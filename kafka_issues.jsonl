{"project": "KAFKA", "issue_id": "KAFKA-1", "title": "The log4j appender still uses the SyncProducer API", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:09.000+0000", "updated": "2024-11-26T08:11:21.000+0000", "description": "The log4j appender still uses the SyncProducer API. Change it to use the Producer API using the StringEncoder instead.", "comments": [], "derived": {"summary": "The log4j appender still uses the SyncProducer API. Change it to use the Producer API using the StringEncoder instead.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "The log4j appender still uses the SyncProducer API - The log4j appender still uses the SyncProducer API. Change it to use the Producer API using the StringEncoder instead."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-2", "title": "a restful producer API", "status": "Resolved", "priority": "Minor", "reporter": null, "assignee": "Dru Panchal", "labels": [], "created": "2011-07-19T21:32:09.000+0000", "updated": "2016-08-26T22:35:11.000+0000", "description": "If Kafka server supports a restful producer API, we can use Kafka in any programming language without implementing the wire protocol in each language.", "comments": ["Since the Kafka APIs are undergoing changes in upcoming versions, this JIRA becomes more important. Kafka customers are asking for it. This is important for increased adoption of Kafka.", "[~anandriyer] I would like to resolve this JIRA because the Kafka REST Proxy covers your desired use case.\n\nhttp://docs.confluent.io/2.0.0/kafka-rest/docs/index.html\n"], "derived": {"summary": "If Kafka server supports a restful producer API, we can use Kafka in any programming language without implementing the wire protocol in each language.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "a restful producer API - If Kafka server supports a restful producer API, we can use Kafka in any programming language without implementing the wire protocol in each language."}, {"q": "What updates or decisions were made in the discussion?", "a": "[~anandriyer] I would like to resolve this JIRA because the Kafka REST Proxy covers your desired use case.\n\nhttp://docs.confluent.io/2.0.0/kafka-rest/docs/index.html"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-3", "title": "Consumer needs a pluggable decoder", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:09.000+0000", "updated": "2011-09-28T00:42:56.000+0000", "description": "Kafka producer allows a user to plug in an encoder (from type T to Message). We need to do the same thing on the consumer side, by allowing a user to plug in a decoder (from Message to type T).", "comments": ["\nHere is a first stab at plugging in a decoder for the consumer API.  The\nhtml docs/quickstart also need to be updated after this is finalized, so\nthis is more of a checkpoint review request.\n\nThe consumer API becomes a bit clunky with the parameterized type so if\nanyone has ideas on making this simpler/cleaner please comment.\n\nThere is an example of the API in action in the new test under\nZookeeperConsumerConnectorTest, but here is a summary:\n\n// create consumerConfig and set config.deserializerClass to\n// MyDecoder (extends Decoder<MyRichType>)\n\nval consumer = new ZookeeperConsumerConnector[MyRichType](consumerConfig)\nval streams = consumer.createMessageStreams(topicMap) // each stream is of type KafkaMessageStream[MyRichType]\nfor (stream : streams) {\n    for (myRichTypeMessage <- stream) {\n        // use myRichTypeMessage: MyRichType\n    }\n}\n\n- With this approach, createMessageStreams can handle at most one decoder\n  class. If the client needs different decoders for different topics, the\n  client will need to create separate consumer connectors with a different\n  deserializer for each one. One possible way to avoid that would be to make\n  the deserializer config a topic:deserializer map, and move the type\n  parameter to the createMessageStreams method. The client will still need\n  to call createMessageStreams for each deserializer type, but will not need\n  multiple connectors objects - not sure if this adds much value though and\n  it makes the config more cumbersome.\n\n- For ConsoleConsumer, the MessageFormatter cannot be parameterized from the\n  config. So that poses a bit of a redundancy problem in that the formatter\n  presumably needs to decode the message. I guess you can have a decoder\n  implementation that implements both the decoder and formatter traits, and\n  have the formatter implementation use the decoder if necessary.\n\nSince this is a client API change, it would be great to discuss and resolve\nany concerns about it at this point - should we post this comment on\nkafka-user as well?\n", "One way to do this would be to make use of type inference off the method arguments. For example, you could make the api be\nval firstStreamConfig = new StreamConfig[String](topic = \"my_topic\", parallelism = 5, decoder=new StringDecoder())\nval secondStreamConfig = new StreamConfig[Integer](topic = \"your_topic\", parallelism = 5, decoder=new IntDecoder())\nval streamSet: StreamSet = consumer.createMessageStreams(firstStreamConfig, secondStreamConfig)\nval streamOne: List[KafkaMessageStream[String]] = streamSet.get(firstStreamConfig)\n\nHere the type of the iterator is actually inferred from the parametrization on the config object passed in. This trick is used a lot in java APIs to help pass through type parameters. The other advantage is that it gives us a general per-topic config object. Currently we don't have a very clean way to do per-topic config.\n\nAnother way to do this would be to ask if there is a way to just make the stream creation be one at a time without causing a \"rebalance storm\" for the clients. In that case you could do \nval consumer = new ZookeeperConsumerConnector[MyRichType](consumerConfig)\nval streamsA = consumer.createMessageStream(\"topic_a\", decoder, parallelism)\nval streamsB = consumer.createMessageStream(\"topic_a\", decoder, parallelism)\nTo my eyes this later approach is much more friendly, especially in the common case where you just have one topic to consume from. I think the reason we didn't do this was to avoid having a sequence of these cause a ton of rebalancing activity. I wonder if there is a direct fix for that.", "Thanks for the tip - that's an interesting idea. I had actually tried using type inference to avoid having to specify the default [Message] when a decoder is not required, but couldn't get it to work - I don't recall exactly why, but now that I read your comment, I think parameterizing the API methods instead of the class makes it more tractable. I'll experiment tomorrow.", "I tried various experiments for this - the difficulty lies in bringing the Java\nAPI up to par with the Scala API mainly due to limitations in (or my\nunderstanding of) the interoperability of Java-Scala generics and default\narguments.\n\nThe current consumer API is:\n\n    createMessageStreams(topicCountMap: Map[String, Int]): Map[String,\n        List[KafkaMessageStream]]\n\nWith consumer decoders, we want to be able to return a Map[String,\nList[KafkaMessageStream[T]]], and if possible have different types bound to\nT in the map. This is doable in Scala, but I don't see how we can support an\nequally flexible API in Java.\n\nSo I think our alternatives are:\n\n1 - Use type inference, and allow disparity between the Scala and Java APIs\n2 - Go with a simpler API that requires an explicit type parameter, but is\n    consistent across Java/Scala\n3 - Other thoughts?\n\nFor the first approach, the API could be:\n\n    createMessageStreams[T](topicCountMap: Map[String, StreamConfig[T]]):\n        Map[String, List[KafkaMessageStream[T]]]\n\nThere are a couple of issues in getting this to work. Although it works well\non the Scala side, Java is more limited. E.g., if the type parameter is\ndeclared covariant (i.e., +T) with Scala it is possible to mix different\ndecoder types in a single call to createMessageStreams, but it is not\npossible do this cleanly in Java. Anyway, I think this use case would be\nrare.  More importantly, the other useful benefit of this approach (default\ntype if decoder is not specified) is also not available to the Java API,\nsince Java doesn't understand default arguments. I tried a wrapper factory\nobject to make StreamConfigs, but ultimately an explicit type and casting is\nrequired on the Java side.\n\nThe second approach would be along the following lines:\n\nConsumer connector provides:\n\n    createMessageStreams[T](topicCountMap: Map[String, Int], decoder:\n        Decoder[T]): Map[String, List[KafkaMessageStream[T]]]\n\nSo pretty much the existing API with the addition of the parameterized type\nand the decoder argument.\n\nScala usage:\n\n// c is a ConsumerConnector\n\nval intStreamMap = c.createMessageStreams(intTopicMap, new IntDecoder)\nval floatStreamMap = c.createMessageStreams(floatTopicMap, new FloatDeocder)\nval aFloatTopicStreams = floatStreamMap.get(\"aFloatTopic\")\n\nJava usage:\n\njava.util.Map<String, List<KafkaMessageStream<Float>>> floatStreamMap =\n    c.createMessageStreams(floatTopicMap, new FloatDecoder());\n\nKafkaMessageStream<Float> aFloatTopicStreams =\n    floatStreamMap.get(\"aFloatTopic\");\n\nThoughts?\n", "I vote for the second approach. The API is simpler and is consistent between java and scala.", "Here is an updated patch. (Btw, I added a new method to the Java consumer connector interface to allow creating streams without having to specify the default decoder each time.)", "Overall, the patch looks good. Some minor comments:\n\n1. Does ZookeeperConsumerConnector.consume need a default decoder? The caller always passes in a decoder.\n\n2. Shouldn't ConsumerShell use StringDecoder?\n\n3. Consumer.java doesn't need to import kafka.serializer.DefaultDecoder.\n\n4. It seems you don't need the default decoder in ZookeeperConsumerConnector.createMessageStreams, since the default decoder is already defined in the trait.", "Made the changes in this patch. For 2, either way works, but makes more sense to use StringDecoder now that we have the decoder in the consumer api.", "Thanks, Joel. Just committed this."], "derived": {"summary": "Kafka producer allows a user to plug in an encoder (from type T to Message). We need to do the same thing on the consumer side, by allowing a user to plug in a decoder (from Message to type T).", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Consumer needs a pluggable decoder - Kafka producer allows a user to plug in an encoder (from type T to Message). We need to do the same thing on the consumer side, by allowing a user to plug in a decoder (from Message to type T)."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks, Joel. Just committed this."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-4", "title": "Confusing Error mesage from producer when no kafka brokers are available", "status": "Resolved", "priority": "Minor", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:09.000+0000", "updated": "2017-11-16T09:11:09.000+0000", "description": "If no kafka brokers are available the producer gives the following error: \n\nException in thread \"main\" kafka.common.InvalidPartitionException: Invalid number of partitions: 0 \nValid values are > 0 \nat kafka.producer.Producer.kafka$producer$Producer$$getPartition(Producer.scala:144) \nat kafka.producer.Producer$$anonfun$3.apply(Producer.scala:112) \nat kafka.producer.Producer$$anonfun$3.apply(Producer.scala:102) \nat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206) \nat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206) \nat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34) \nat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:32) \nat scala.collection.TraversableLike$class.map(TraversableLike.scala:206) \nat scala.collection.mutable.WrappedArray.map(WrappedArray.scala:32) \nat kafka.producer.Producer.send(Producer.scala:102) \nat kafka.javaapi.producer.Producer.send(Producer.scala:101) \nat com.linkedin.nusviewer.PublishTestMessage.main(PublishTestMessage.java:45) \n\nThis is confusing. The problem is that no brokers are available, we should make this more clear.", "comments": ["Current error message when no broker is available is:\r\n\r\n{code}\r\nWARN Connection to node -1 could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)\r\n{code}\r\n\r\nThis message was introduced by KAFKA-5179, so I think it is save to say that we can close this ticket as well with the same fix version. Before that there were other error messages that also improved upon this message, but I don't think we need to provide the entire history here..\r\n"], "derived": {"summary": "If no kafka brokers are available the producer gives the following error: \n\nException in thread \"main\" kafka. common.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Confusing Error mesage from producer when no kafka brokers are available - If no kafka brokers are available the producer gives the following error: \n\nException in thread \"main\" kafka. common."}, {"q": "What updates or decisions were made in the discussion?", "a": "Current error message when no broker is available is:\r\n\r\n{code}\r\nWARN Connection to node -1 could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)\r\n{code}\r\n\r\nThis message was introduced by KAFKA-5179, so I think it is save to say that we can close this ticket as well with the same fix version. Before that there were other error messages that also improved upon this message, but I don't think we need to provide the entire history here.."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-5", "title": "Continuous log flusher", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:10.000+0000", "updated": "2015-02-07T23:48:24.000+0000", "description": "Today, a kafka log is flushed based on either the number of messages or a certain amount of time elapsed. Setting those numbers properly may not be easy. A better way is do have a background thread that keeps flushing dirty logs as faster as the underlying storage allows.", "comments": [], "derived": {"summary": "Today, a kafka log is flushed based on either the number of messages or a certain amount of time elapsed. Setting those numbers properly may not be easy.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Continuous log flusher - Today, a kafka log is flushed based on either the number of messages or a certain amount of time elapsed. Setting those numbers properly may not be easy."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-6", "title": "Support replicating from more than 1 remote kafka cluster", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:10.000+0000", "updated": "2012-09-14T23:22:09.000+0000", "description": "It would be good if KafkaServerStartable can take multiple consumerConfig and replicate data from multiple remote kafka clusters.", "comments": [], "derived": {"summary": "It would be good if KafkaServerStartable can take multiple consumerConfig and replicate data from multiple remote kafka clusters.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support replicating from more than 1 remote kafka cluster - It would be good if KafkaServerStartable can take multiple consumerConfig and replicate data from multiple remote kafka clusters."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-7", "title": "javaapi MessageSet doesn't need to implement writeTo socket", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:10.000+0000", "updated": "2011-07-19T21:32:10.000+0000", "description": "javaapi MessageSet (and its subclasses) doesn't need to implement the following method, since a javaapi.MessageSet is always converted to the scala version before being sent to socket. \ndef writeTo(channel: WritableByteChannel, offset: Long, maxSize: Long): Long", "comments": [], "derived": {"summary": "javaapi MessageSet (and its subclasses) doesn't need to implement the following method, since a javaapi. MessageSet is always converted to the scala version before being sent to socket.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "javaapi MessageSet doesn't need to implement writeTo socket - javaapi MessageSet (and its subclasses) doesn't need to implement the following method, since a javaapi. MessageSet is always converted to the scala version before being sent to socket."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-8", "title": "Upgrade to Zookeeper 3.3.3", "status": "Closed", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:10.000+0000", "updated": "2011-07-19T21:32:10.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Upgrade to Zookeeper 3.3.3"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-9", "title": "Consumer logs ERROR during close", "status": "Closed", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:11.000+0000", "updated": "2011-07-19T21:32:11.000+0000", "description": "When closing the consumer, we sometimes get the following error: \n\n[2011-05-16 13:24:39,203] ERROR consumed offset: 862812384944 doesn't match fetch offset: 862813943889 for PageViewEvent:2-0; consumer may lose data (kafka.consumer.ConsumerIterator)", "comments": [], "derived": {"summary": "When closing the consumer, we sometimes get the following error: \n\n[2011-05-16 13:24:39,203] ERROR consumed offset: 862812384944 doesn't match fetch offset: 862813943889 for PageViewEvent:2-0; consumer may lose data (kafka. consumer.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Consumer logs ERROR during close - When closing the consumer, we sometimes get the following error: \n\n[2011-05-16 13:24:39,203] ERROR consumed offset: 862812384944 doesn't match fetch offset: 862813943889 for PageViewEvent:2-0; consumer may lose data (kafka. consumer."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-10", "title": "Kafka deployment on EC2 should be WHIRR based, instead of current contrib/deploy code based solution", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:11.000+0000", "updated": "2020-05-15T01:46:28.000+0000", "description": "Apache Whirr is a a set of libraries for running cloud services \nhttp://incubator.apache.org/whirr/ \n\nIt is desirable that Kafka's integration with EC2 be Whirr based, rather than the code based solution we currently have in contrib/deploy. \n\nThe code in contrib/deploy will be deleted in 0.6 release", "comments": ["People will use any of whirr, pallet, chef or other CM method to deploy kafka - I went with pallet. Any reason to keep a specific means in trunk ? Maybe the whirr bits should be in whirr directly like other apache projects ?", "You are right. This JIRA is just a placeholder and will be open until someone adds Kafka in Whirr.", "I had started this a while ago, but hadn't finished it.  I'll see if I can find my work, thought it may be outdated.  The code for starting Kafka in Whirr should definitely be hosted within Whirr itself. See WHIRR-212.  Feel free to start if you'd like.", "Jakob let me know if I can help to update the code for latest trunk.", "This should be closed. Whirr is in the Apache Attic.Â ", "Apache Whirr has been retired."], "derived": {"summary": "Apache Whirr is a a set of libraries for running cloud services \nhttp://incubator. apache.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Kafka deployment on EC2 should be WHIRR based, instead of current contrib/deploy code based solution - Apache Whirr is a a set of libraries for running cloud services \nhttp://incubator. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "Apache Whirr has been retired."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-11", "title": "The zookeeper based Producer doesn't remove a dead broker from its list while serving a produce request", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:11.000+0000", "updated": "2011-07-19T21:32:11.000+0000", "description": "The producer registers a watcher on /brokers/ids to detect the new set of brokers in the cluster. It uses that to keep the producer pool connections updated. But, this watcher callback should also remove the dead brokers, if any, from its in memory data structure. This is important so that we don't accidentally pick a dead broker to serve a produce request. T", "comments": [], "derived": {"summary": "The producer registers a watcher on /brokers/ids to detect the new set of brokers in the cluster. It uses that to keep the producer pool connections updated.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "The zookeeper based Producer doesn't remove a dead broker from its list while serving a produce request - The producer registers a watcher on /brokers/ids to detect the new set of brokers in the cluster. It uses that to keep the producer pool connections updated."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-12", "title": "Improve EventHandler in AsyncProducer", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:11.000+0000", "updated": "2011-07-19T21:32:11.000+0000", "description": "There are a few issues with the current EventHandler. \n1. If an EventHandler is specified in a config file, the instantiator requires that the EventHandler has an empty constructor. \n2. Today, a user has to pass in the Encoder twice, once through the Producer and another through the EventHandler. \n3. The default EventHandler is not set (say, for events of String type).", "comments": [], "derived": {"summary": "There are a few issues with the current EventHandler. 1.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Improve EventHandler in AsyncProducer - There are a few issues with the current EventHandler. 1."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-13", "title": "ZKBrokerPartitionInfo doesn't allow load balancing on a new topic", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:12.000+0000", "updated": "2011-07-19T21:32:12.000+0000", "description": "The problem is that initially no broker has registered for a topic in ZK. Once the producer sends a message to a broker, that broker is registered in ZK. After that, the producer sticks with that broker.", "comments": [], "derived": {"summary": "The problem is that initially no broker has registered for a topic in ZK. Once the producer sends a message to a broker, that broker is registered in ZK.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "ZKBrokerPartitionInfo doesn't allow load balancing on a new topic - The problem is that initially no broker has registered for a topic in ZK. Once the producer sends a message to a broker, that broker is registered in ZK."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-14", "title": "ZK exception in consumer when no broker has registered", "status": "Closed", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:12.000+0000", "updated": "2011-07-19T21:32:12.000+0000", "description": "The consumer will get a ZKNoNodeException if no broker has ever registered in ZK.", "comments": [], "derived": {"summary": "The consumer will get a ZKNoNodeException if no broker has ever registered in ZK.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "ZK exception in consumer when no broker has registered - The consumer will get a ZKNoNodeException if no broker has ever registered in ZK."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-15", "title": "SBT release-zip target doesn't include bin and config directories anymore", "status": "Closed", "priority": "Major", "reporter": "Jun Rao", "assignee": "Jun Rao", "labels": [], "created": "2011-07-19T21:32:12.000+0000", "updated": "2013-06-13T16:27:00.000+0000", "description": "SBT release-zip target is responsible for creating a fully deployable release zip containing all the package jars, scripts in the bin directory and config property files. \nCurrently, it packages the kafka jar and the lib directories correctly.", "comments": ["Unless we want to make this a one off, we need to fix this for the release.", "We have someone in our team who can help address this issue as part of https://issues.apache.org/jira/browse/KAFKA-134. Maybe for the 0.7 release, we can just add the bin and config directories manually?", "Moving to 0.8", "This is fixed in 0.8."], "derived": {"summary": "SBT release-zip target is responsible for creating a fully deployable release zip containing all the package jars, scripts in the bin directory and config property files. Currently, it packages the kafka jar and the lib directories correctly.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "SBT release-zip target doesn't include bin and config directories anymore - SBT release-zip target is responsible for creating a fully deployable release zip containing all the package jars, scripts in the bin directory and config property files. Currently, it packages the kafka jar and the lib directories correctly."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is fixed in 0.8."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-16", "title": "Add JMX for changing log4j level dynamically at Kafka server", "status": "Closed", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:12.000+0000", "updated": "2011-07-19T21:32:12.000+0000", "description": "We want to be able to change log4 level at Kafka server without restarting the server.", "comments": [], "derived": {"summary": "We want to be able to change log4 level at Kafka server without restarting the server.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add JMX for changing log4j level dynamically at Kafka server - We want to be able to change log4 level at Kafka server without restarting the server."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-17", "title": "ZookeeperConsumerConnectorMBean needs to close SimpleConsumer when done", "status": "Closed", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:13.000+0000", "updated": "2011-07-19T21:32:13.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "ZookeeperConsumerConnectorMBean needs to close SimpleConsumer when done"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-18", "title": "Create SBT sub projects for perf, examples, contrib that are currently using ant", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:13.000+0000", "updated": "2011-07-19T21:32:13.000+0000", "description": "Some code components in Kafka are using ant. It is convenient to be able to compile all code using sbt and be able to create a release zip, ONLY if all sub components like perf, java examples and contrib compile and run correctly.", "comments": [], "derived": {"summary": "Some code components in Kafka are using ant. It is convenient to be able to compile all code using sbt and be able to create a release zip, ONLY if all sub components like perf, java examples and contrib compile and run correctly.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Create SBT sub projects for perf, examples, contrib that are currently using ant - Some code components in Kafka are using ant. It is convenient to be able to compile all code using sbt and be able to create a release zip, ONLY if all sub components like perf, java examples and contrib compile and run correctly."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-19", "title": "unit test fail on some linux machines", "status": "Closed", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:13.000+0000", "updated": "2011-07-19T21:32:13.000+0000", "description": "This is caused by improper use of scala test.", "comments": [], "derived": {"summary": "This is caused by improper use of scala test.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "unit test fail on some linux machines - This is caused by improper use of scala test."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-20", "title": "Remove the java FileMessageSet API", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:13.000+0000", "updated": "2011-07-19T21:32:13.000+0000", "description": "A java API is not required for FileMessageSet. This is meant to be an internal functionality and hence, it is not required to expose a Java API for it.", "comments": [], "derived": {"summary": "A java API is not required for FileMessageSet. This is meant to be an internal functionality and hence, it is not required to expose a Java API for it.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Remove the java FileMessageSet API - A java API is not required for FileMessageSet. This is meant to be an internal functionality and hence, it is not required to expose a Java API for it."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-21", "title": "Increase sleep timeout in AutoOffsetResetTest", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:14.000+0000", "updated": "2011-07-19T21:32:14.000+0000", "description": "The auto offset reset integration tests in AutoOffsetResetTest.scala, the timeout of 1100 is not enough for some Linux machines. \n\nIncreasing that timeout to 2seconds fixes the issue.", "comments": [], "derived": {"summary": "The auto offset reset integration tests in AutoOffsetResetTest. scala, the timeout of 1100 is not enough for some Linux machines.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Increase sleep timeout in AutoOffsetResetTest - The auto offset reset integration tests in AutoOffsetResetTest. scala, the timeout of 1100 is not enough for some Linux machines."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-22", "title": "Unrequired explicit dependency on specific versions of certain libraries", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:14.000+0000", "updated": "2011-07-19T21:32:14.000+0000", "description": "Currently, kafka explicitly depends on a certain version of cglib - 2.1_3. On linux, this version causes NoSuchMethodErrors. \n\nAlso, the dependency on asm is not required. The correct set of dependencies are - \n\nscalatest \njunit \neasymock 3.0 (this pulls in the correct versions of cglib and objenesis)", "comments": [], "derived": {"summary": "Currently, kafka explicitly depends on a certain version of cglib - 2. 1_3.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Unrequired explicit dependency on specific versions of certain libraries - Currently, kafka explicitly depends on a certain version of cglib - 2. 1_3."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-23", "title": "Add another constructor to ProducerData for the very simple case of sending a single message with default partitioning", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:14.000+0000", "updated": "2011-07-19T21:32:14.000+0000", "description": "Currently, to send a single message, you have to create a list, add a single message to it, wrap it up in a ProducerData object and then send it. \n\nFor the most basic use case of sending a single message, with random partitioning, it is reasonable to provide an api that takes in just the topic and a single message.", "comments": [], "derived": {"summary": "Currently, to send a single message, you have to create a list, add a single message to it, wrap it up in a ProducerData object and then send it. For the most basic use case of sending a single message, with random partitioning, it is reasonable to provide an api that takes in just the topic and a single message.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add another constructor to ProducerData for the very simple case of sending a single message with default partitioning - Currently, to send a single message, you have to create a list, add a single message to it, wrap it up in a ProducerData object and then send it. For the most basic use case of sending a single message, with random partitioning, it is reasonable to provide an api that takes in just the topic and a single message."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-24", "title": "Change the config parameter \"broker.partition.info\" to a more descriptive name", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:14.000+0000", "updated": "2011-07-19T21:32:14.000+0000", "description": "In the producer, if you want to turn off the zookeeper based broker discovery and load balancing, you need to specify the list of kafka brokers in your cluster, through the config parameter \"broker.partition.info\". From this name itself, it is not clear that it expects the kafka broker information.", "comments": [], "derived": {"summary": "In the producer, if you want to turn off the zookeeper based broker discovery and load balancing, you need to specify the list of kafka brokers in your cluster, through the config parameter \"broker. partition.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Change the config parameter \"broker.partition.info\" to a more descriptive name - In the producer, if you want to turn off the zookeeper based broker discovery and load balancing, you need to specify the list of kafka brokers in your cluster, through the config parameter \"broker. partition."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-25", "title": "Make MultiFetchResponse an Iterable instead of an Iterator", "status": "Closed", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:15.000+0000", "updated": "2011-07-19T21:32:15.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Make MultiFetchResponse an Iterable instead of an Iterator"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-26", "title": "Be more defensive in managing offset in the consumer", "status": "Closed", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:15.000+0000", "updated": "2011-07-19T21:32:15.000+0000", "description": "In the high level consumer, we advance the consumed offset as messages are consumed. Periodically, we finish consuming a chunk of fetched data and move on to the next chunk. To be defensive, we need to validate that at the transition point, the consumed offset should be identical to the fetch offset of the next chunk.", "comments": [], "derived": {"summary": "In the high level consumer, we advance the consumed offset as messages are consumed. Periodically, we finish consuming a chunk of fetched data and move on to the next chunk.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Be more defensive in managing offset in the consumer - In the high level consumer, we advance the consumed offset as messages are consumed. Periodically, we finish consuming a chunk of fetched data and move on to the next chunk."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-27", "title": "Provide a default encoder for kafka.producer.Producer", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:15.000+0000", "updated": "2011-07-19T21:32:15.000+0000", "description": "Today, the \"serializer.class\" parameter in Producer is required. It is reasonable, however, to provide a default no-op Encoder<Message>. The client would have to type the Producer accordingly though. \n\nFor example, this would work - \nval producer = new Producer<String, Message>(config) \n\nThis wouldn't - \nval producer = new Producer<String, String>(config", "comments": [], "derived": {"summary": "Today, the \"serializer. class\" parameter in Producer is required.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Provide a default encoder for kafka.producer.Producer - Today, the \"serializer. class\" parameter in Producer is required."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-28", "title": "Improve API docs for all public APIs", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:16.000+0000", "updated": "2011-07-19T21:32:16.000+0000", "description": "Basically, we want to improve the scaladocs/javadocs for all public APIS", "comments": [], "derived": {"summary": "Basically, we want to improve the scaladocs/javadocs for all public APIS.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Improve API docs for all public APIs - Basically, we want to improve the scaladocs/javadocs for all public APIS."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-29", "title": "avoid reimplementing ZookeeperConsumerConnectorMBean in javaapi.consumer.ZookeeperConsumerConnector", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:16.000+0000", "updated": "2011-07-19T21:32:16.000+0000", "description": "Basically, if we do the following in ConsumerConnector: \ndef createJavaConsumerConnector(config: ConsumerConfig): kafka.javaapi.consumer.ConsumerConnector = { \nval consumerConnect = new kafka.javaapi.consumer.ZookeeperConsumerConnector(config) \nUtils.swallow(logger.warn, Utils.registerMBean(consumerConnect.underlying, consumerStatsMBeanName)) \nconsumerConnect \n} \n\nthen kafka.javaapi.consumer.ZookeeperConsumerConnector doesn't need to implement ZookeeperConsumerConnectorMBean any more.", "comments": [], "derived": {"summary": "Basically, if we do the following in ConsumerConnector: \ndef createJavaConsumerConnector(config: ConsumerConfig): kafka. javaapi.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "avoid reimplementing ZookeeperConsumerConnectorMBean in javaapi.consumer.ZookeeperConsumerConnector - Basically, if we do the following in ConsumerConnector: \ndef createJavaConsumerConnector(config: ConsumerConfig): kafka. javaapi."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-30", "title": "Refactor public API into java and Scala API", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:17.000+0000", "updated": "2011-07-19T21:32:17.000+0000", "description": "In release 0.5, some public APIs have more than 1 constructors or other APIs, especially for Java data structures. It is cleaner to have a separate javaapi package and include all public Java APIs there.", "comments": [], "derived": {"summary": "In release 0. 5, some public APIs have more than 1 constructors or other APIs, especially for Java data structures.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Refactor public API into java and Scala API - In release 0. 5, some public APIs have more than 1 constructors or other APIs, especially for Java data structures."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-31", "title": "Handle ZK exception properly in consumer", "status": "Closed", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:17.000+0000", "updated": "2011-07-19T21:32:17.000+0000", "description": "Occasionally, during rebalance, we may hit a ZK exception because a ZK node that we thought is there suddenly disappear. When this happens, we need to reset the consumer state before the next rebalance happens.", "comments": [], "derived": {"summary": "Occasionally, during rebalance, we may hit a ZK exception because a ZK node that we thought is there suddenly disappear. When this happens, we need to reset the consumer state before the next rebalance happens.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Handle ZK exception properly in consumer - Occasionally, during rebalance, we may hit a ZK exception because a ZK node that we thought is there suddenly disappear. When this happens, we need to reset the consumer state before the next rebalance happens."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-32", "title": "consumer picks up wrong offset during rebalance", "status": "Closed", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:17.000+0000", "updated": "2011-07-19T21:32:17.000+0000", "description": "Occasionally, we saw a consumer picks up wrong offset during rebalance.", "comments": [], "derived": {"summary": "Occasionally, we saw a consumer picks up wrong offset during rebalance.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "consumer picks up wrong offset during rebalance - Occasionally, we saw a consumer picks up wrong offset during rebalance."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-33", "title": "Explicitly name all threads", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:17.000+0000", "updated": "2011-07-19T21:32:17.000+0000", "description": "We should name all threads (e.g., ProducerSendThread) that we create.", "comments": [], "derived": {"summary": "We should name all threads (e. g.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Explicitly name all threads - We should name all threads (e. g."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-34", "title": "NotCompliantMBeanException while creating kafka.javaapi.consumer.ZookeeperConsumerConnector", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:18.000+0000", "updated": "2011-07-19T21:32:18.000+0000", "description": "When you try to create a zk consumer through the java api - \n\njavax.management.NotCompliantMBeanException: MBean class kafka.javaapi.consumer.ZookeeperConsumerConnector does not implement DynamicMBean, neither follows the Standard MBean conventions (javax.management.NotCompliantMBeanException: Class kafka.javaapi.consumer.ZookeeperConsumerConnector is not a JMX compliant Standard MBean) nor the MXBean conventions (javax.management.NotCompliantMBeanException: kafka.javaapi.consumer.ZookeeperConsumerConnector: Class kafka.javaapi.consumer.ZookeeperConsumerConnector is not a JMX compliant MXBean) \nat com.sun.jmx.mbeanserver.Introspector.checkCompliance(Introspector.java:160) \nat com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:305) \nat com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:482)", "comments": [], "derived": {"summary": "When you try to create a zk consumer through the java api - \n\njavax. management.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "NotCompliantMBeanException while creating kafka.javaapi.consumer.ZookeeperConsumerConnector - When you try to create a zk consumer through the java api - \n\njavax. management."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-35", "title": "Default Encoder for Kafka log4 appender", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:18.000+0000", "updated": "2011-07-19T21:32:18.000+0000", "description": "The log4j appender requires the client to pass in a custom Encoder. Ideally, we should default to a StringEncoder in the absence of a user-specified one.", "comments": [], "derived": {"summary": "The log4j appender requires the client to pass in a custom Encoder. Ideally, we should default to a StringEncoder in the absence of a user-specified one.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Default Encoder for Kafka log4 appender - The log4j appender requires the client to pass in a custom Encoder. Ideally, we should default to a StringEncoder in the absence of a user-specified one."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-36", "title": "New producer API", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:18.000+0000", "updated": "2011-07-19T21:32:18.000+0000", "description": "We need to introduce a new Producer API that wraps the 2 low-level producer APIs - kafka.producer.SyncProducer and kafka.producer.async.AsyncProducer. The goal is to expose all the producer functionalities through a single API to the client. The new producer should be able to - \n\n1. handle queueing/buffering of multiple producer requests and asynchronous dispatch of the batched data \n2. handle the serialization of data through a user-specified Encoder \n3. provide zookeeper based automatic broker discovery \n4. provide software load balancing through an optionally user-specified Partitioner", "comments": [], "derived": {"summary": "We need to introduce a new Producer API that wraps the 2 low-level producer APIs - kafka. producer.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "New producer API - We need to introduce a new Producer API that wraps the 2 low-level producer APIs - kafka. producer."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-37", "title": "Quick start still references SimpleProducer", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:18.000+0000", "updated": "2011-07-19T21:32:18.000+0000", "description": "The quickstart kafka page still references \"SimpleProducer\". As of the latest github checkout, this no longer exists.", "comments": [], "derived": {"summary": "The quickstart kafka page still references \"SimpleProducer\". As of the latest github checkout, this no longer exists.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Quick start still references SimpleProducer - The quickstart kafka page still references \"SimpleProducer\". As of the latest github checkout, this no longer exists."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-38", "title": "Improve case of multiple logically distinct kafka clusters sharing the same ZooKeeper ensemble", "status": "Closed", "priority": "Minor", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:18.000+0000", "updated": "2011-07-19T21:32:18.000+0000", "description": "Currently all of the data stored in Zookeeper comes directly off of the root. \n\n{noformat} \nval consumersPath = \"/consumers\" \nval brokerIdsPath = \"/brokers/ids\" \nval brokerTopicsPath = \"/brokers/topics\" \n{noformat} \n\nIf for operational reasons if one wants to have logically distinct Kafka clusters share the same Zookeeper ensemble the current setup is a little awkward. You have to make sure that none of the broker ids, topic names, consumer groups etc are all unique. Really you have one giant kafka cluster partitioned by application logic. \n\nOptions: \n(A) A configurable root added as a property \"kafka/my-awesome-cluster\". This could default to \"\" for backwards compatibility. \n(B) Each znode's data element includes what kafka cluster it belongs to. \n\nI think (A) is almost certainly preferable.", "comments": [], "derived": {"summary": "Currently all of the data stored in Zookeeper comes directly off of the root. {noformat} \nval consumersPath = \"/consumers\" \nval brokerIdsPath = \"/brokers/ids\" \nval brokerTopicsPath = \"/brokers/topics\" \n{noformat} \n\nIf for operational reasons if one wants to have logically distinct Kafka clusters share the same Zookeeper ensemble the current setup is a little awkward.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Improve case of multiple logically distinct kafka clusters sharing the same ZooKeeper ensemble - Currently all of the data stored in Zookeeper comes directly off of the root. {noformat} \nval consumersPath = \"/consumers\" \nval brokerIdsPath = \"/brokers/ids\" \nval brokerTopicsPath = \"/brokers/topics\" \n{noformat} \n\nIf for operational reasons if one wants to have logically distinct Kafka clusters share the same Zookeeper ensemble the current setup is a little awkward."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-39", "title": "javaapi does not declare IOException to be thrown", "status": "Resolved", "priority": "Minor", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:19.000+0000", "updated": "2020-05-27T20:46:52.000+0000", "description": "javac it upset that IOException is not in the method signature of the javaapi. This forces un-idiomatic try/catch blocks like: \n{noformat} \n} catch (Exception e) { \nif (e instanceof IOException) { \nlogger.error(\"error communicating with broker: \" + e); \n} else { \nlogger.error(\"unexpected exception: \", e); \n} \n} \n{noformat} \n\n\nThey clearly can be generated (from SimpleComsumer:fetch: \n{noformat} \ncase ioe: java.io.IOException => channel = null; throw ioe; \n{noformat} \n\n\nI wasn't sure how to get a \"proper\" method signature with the java/scale checked exception mismatch.", "comments": [], "derived": {"summary": "javac it upset that IOException is not in the method signature of the javaapi. This forces un-idiomatic try/catch blocks like: \n{noformat} \n} catch (Exception e) { \nif (e instanceof IOException) { \nlogger.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "javaapi does not declare IOException to be thrown - javac it upset that IOException is not in the method signature of the javaapi. This forces un-idiomatic try/catch blocks like: \n{noformat} \n} catch (Exception e) { \nif (e instanceof IOException) { \nlogger."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-40", "title": "Time based log rolling", "status": "Resolved", "priority": "Minor", "reporter": null, "assignee": "Chris Burroughs", "labels": [], "created": "2011-07-19T21:32:19.000+0000", "updated": "2020-05-27T20:50:44.000+0000", "description": "In some cases we know that consumers are interested in data on time boundaries (for example, hourly), and when a new consumer of the type is spun up it wants to consume data since the last boundary (start at 12 noon). \n\nOffsetRequest can do this now, but Log:getOffsetsBefore is \"very approximate\" and it would be nice for the consumers to not have to iterate over unneeded data that (being older) is less likely to be in the page cache. \n\nProposal: Optional argument to roll log file if it contains more than n seconds of data. I think this is reasonable, but wanted to create a ticket for comments in-case I've missed a reason this infeasible or otherwise a bad idea.", "comments": ["It seem theÂ time-index as introduced via KIP-33 ([https://cwiki.apache.org/confluence/display/KAFKA/KIP-33+-+Add+a+time+based+log+index]) addressed this issue."], "derived": {"summary": "In some cases we know that consumers are interested in data on time boundaries (for example, hourly), and when a new consumer of the type is spun up it wants to consume data since the last boundary (start at 12 noon). OffsetRequest can do this now, but Log:getOffsetsBefore is \"very approximate\" and it would be nice for the consumers to not have to iterate over unneeded data that (being older) is less likely to be in the page cache.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Time based log rolling - In some cases we know that consumers are interested in data on time boundaries (for example, hourly), and when a new consumer of the type is spun up it wants to consume data since the last boundary (start at 12 noon). OffsetRequest can do this now, but Log:getOffsetsBefore is \"very approximate\" and it would be nice for the consumers to not have to iterate over unneeded data that (being older) is less likely to be in the page cache."}, {"q": "What updates or decisions were made in the discussion?", "a": "It seem theÂ time-index as introduced via KIP-33 ([https://cwiki.apache.org/confluence/display/KAFKA/KIP-33+-+Add+a+time+based+log+index]) addressed this issue."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-41", "title": "multi-produce and multi-fetch support with replication", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": null, "labels": ["replication"], "created": "2011-07-19T21:32:19.000+0000", "updated": "2012-04-06T17:53:22.000+0000", "description": "We need to figure out how to support multi-produce and multi-fetch smoothly with the replication support. The client has to figure out which partitions are collocated on the same broker and adjust accordingly when some partitions are moved.", "comments": ["Is this ticket still required considering the changes being made to the produce and fetch requests?  If we've agreed on implementing the backwards-incompatible wire-protocol changes, this ticket is uncessary, right?", "Yes, this may be covered by the new wire protocol change and KAFKA-239.", "This is fixed as part of KAFKA-239"], "derived": {"summary": "We need to figure out how to support multi-produce and multi-fetch smoothly with the replication support. The client has to figure out which partitions are collocated on the same broker and adjust accordingly when some partitions are moved.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "multi-produce and multi-fetch support with replication - We need to figure out how to support multi-produce and multi-fetch smoothly with the replication support. The client has to figure out which partitions are collocated on the same broker and adjust accordingly when some partitions are moved."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is fixed as part of KAFKA-239"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-42", "title": "Support rebalancing the partitions with replication", "status": "Closed", "priority": "Blocker", "reporter": "Jun Rao", "assignee": "Neha Narkhede", "labels": ["features"], "created": "2011-07-19T21:32:19.000+0000", "updated": "2014-12-30T00:36:48.000+0000", "description": "As new brokers are added, we need to support moving partition replicas from one set of brokers to another, online.", "comments": ["This is a pretty tricky feature. Since it involves multiple state changes before reassignment can be marked complete, there are many failure conditions to think about and handle recovery correctly\n\n1. Admin tool changes\n1.1 Added a new admin command reassign-partition. Right now, it handles one partition, since I thought the failure/exit conditions and error messages are simpler to handle. But if people think we should add multiple partitions support in the same command invocation, that is fine too.\n1.2 Added a new reassignPartition(topic, partition, RAR) API that registers a data change listener on /admin/reassign_partitions path and then creates the /admin/reassign_partitions={} path in zookeeper. It waits until that path is deleted from zookeeper. Once it is deleted, it checks if AR == RAR. If yes, it reports success otherwise failure.\n1.3 Added a shutdown hook to handle command cancellation by the admin. In this case, it checks if reassignment was completed or not and logs the output accordingly.\n\n2. Controller changes\n\nReassigning replicas for a partition goes through a few stages -\nRAR = Reassigned replicas\nAR = Original list of replicas for partition\n\n1. Register listener for ISR changes to detect when the RAR is a subset of the ISR\n2. Start new replicas RAR - AR.\n3. Wait until new replicas are in sync with the leader\n4. If the leader is not in RAR, elect a new leader from RAR\n5. Stop old replicas AR - RAR\n6. Write new AR\n7. Remove partition from the /admin/reassign_partitions path\n\nThe above state changes steps are inside the onPartitionReassignment() callback in KafkaController.scala\n\n3. Partition Reassignment failure cases\n\nBroadly there are 2 types of failures we need to worry about -\n1. Controller failover\n2. Runtime error at the broker hosting the replica\n\nLet's go through the failure cases and recovery -\n1. If the controller fails over between steps 1 and 2, the new controller on startup will read the non-empty admin path and just restart the partition reassignment process from scratch\n2a. If the controller fails over between steps 2 and 3 above, the new controller will check if the new replicas are in sync with the leader or not. In either case, it will resume partition reassignment for the partitions listed in the admin path\n2b. If, for some reason, the broker is not able to start the replicas, the isr listener for reassigned partitions will not trigger. So, the controller will not resume partition reassignment process for that partition. After some time, the admin command can be killed and it will report failure and delete the admin path so it can be retried.\n3. If the controller fails over between steps 4 and 5, the new controller will realize that the new replicas are already in sync. If the new leader is part of the new replicas and is alive, it will not trigger leader re-election. Else it will re-elect the leader from amongst the live reassigned replicas.\n4a. If the controller fails over between steps 5 and 6, the new controller resumes partition reassignment and repeats steps 4 onwards\n4b. If, for some reason, the broker does not complete the leader state change, the partition after reassignment will be offline. This is a problem we have today even for leader election of newly created partitions. The controller doesn't wait for an acknowledgement from the broker for the make-leader state change. Nevertheless, the broker can fail even after sending a successful ack, so there isn't much value in waiting for an ack. However, I think the leader broker should expose an mbean to signify the availability of a partition. If people think this is a good idea, I can file a bug to fix this.\n5. If the controller fails over between steps 6 and 7, it deletes the partition from the admin path marking the completion of this partition's reassignment. The partition reassignment zookeeper listener should record partition to be reassigned only if RAR not equal AR.\n\n4. PartitionReassignedListener\n\nStarts the partition reassignment process unless -\n1. Partition previously existed\n2. New replicas are the same as existing replicas\n3. Any replica in the new set of replicas are dead\n\nIf any of the above conditions are satisfies, it logs an error and removes the partition from list of reassigned partitions notifying the admin command about the failure/completion.\n\n5. PartitionLeaderSelector\n\nAdded a self transition on the OnlinePartition state change. This is because, with cluster expansion and preferred replica leader election features, we need to move the leader for online partitions as well.\n\nAdded partition leader selector module since we have 3 different ways of selecting the leader for a partition -\n1. Offline leader selector - Pick an alive in sync replica as the leader. Otherwise, pick an alive assigned replica\n2. Reassigned partition leader selector - Pick one of the alive in-sync reassigned replicas as the new leader\n3. Preferred replica leader selector - Pick the preferred replica as the new leader\n4. Testing\n\n6. Replica state machine changes\nAdded 2 new states to the replica state machine -\n1. NewReplica        : The controller can create new replicas during partition reassignment. In this state, a\n                       replica can only get become follower state change request.  Valid previous\n                       state is NonExistentReplica\n2. OnlineReplica     : Once a replica is started and part of the assigned replicas for its partition, it is in this\n                       state. In this state, it can get either become leader or become follower state change requests.\n                       Valid previous state are NewReplica, OnlineReplica or OfflineReplica\n3. OfflineReplica    : If a replica dies, it moves to this state. This happens when the broker hosting the replica\n                       is down. Valid previous state are NewReplica, OnlineReplica\n4. NonExistentReplica: If a replica is deleted, it is moved to this state. Valid previous state is OfflineReplica\n\n7. Added 6 unit test cases to test -\n1. Partition reassignment with leader of the partition in the new list of replicas\n2. Partition reassignment with leader of the partition NOT in the new list of replicas\n3. Partition reassignment with existing assigned replicas NOT overlapping with new list of replicas\n4. Partition reassignment for a non existing partition. This is a negative test case\n5. Partition reassignment for a partition that was completed upto step 6 by previous controller. This tests if after controller failover, it handles marking that partition's reassignment as completed.\n6. Partition reassignment for a partition that was completed upto step 3 by previous controller. This tests if after controller failover, it handles leader re-election correctly and completes rest of the partition reassignment process.\n", "Patch v1 contains a fix for KAFKA-525", "Thanks for patch v1. Looks good overall. Some comments:\n\n1. ReassignPartitionsCommand:\n1.1 If a partition doesn't exist, we should fail the operation immediately without updating ReassignPartitionsPath.\n1.2 I think it would be useful to support migrating multiple topics and partitions. We can just take a JSON file that describes the new replicas as input.\n1.3 If ReassignPartitionsPath already exists, we should quit immediately and not overwrite the path. This means that we will only allow 1 outstanding cluster rebalance at a given point of time, which is ok as long as the admin command allows multiple topic/partition being specified. \n\n2. Currently, we fail the partition reassignment operation if any broker in RAR is down, during initialization. However, brokers in RAR can go down after initialization. So, it would be good if we can handle RAR failures. Probably the only change needed is that when a broker is online, we need to start those replicas in RAR too.\n\n3. The logic is now get more complicated with the reassginment logic. Could we describe how it works in a comment? \n\n4. PartitionLeaderSelector.selectLeader(): describe what the return value is in the comment.\n\n5. ReassignedPartitionsIsrChangeListener.handleDataChange(): The following statement is weird. Only controller can change leaders and the controller always updates the leader cache every time a leader is changed. So, there shouldn't be a need for updating the leader cache on ZK listeners.\n                controllerContext.allLeaders.put((topic, partition), leaderAndIsr.leader)\n\n6. KafkaController.onPartitionReassignment(): Could we put the logic that makes sure all replicas RAR are is ISR in  onPartitionReassignment()? Currently, that logic is duplicated in 2-3 places and that logic is always followed by a call to onPartitionReassignment(). If we do this, do we still need ReassignedPartitionsContext.areNewReplicasInIsr?\n\n7. ReplicaStateChangeMachine:\n7.1 NonExistentReplica: The controller holds on to all replicas in this state. Is this necessary? Can we just remove them from the replicaState map.\n7.2 In the following code, we don't really need to read from ZK and can use the cached data.\n            case _ =>\n              // check if the leader for this partition is alive or even exists\n              // NOTE: technically, we could get the leader from the allLeaders cache, but we need to read zookeeper\n              // for the ISR anyways\n              val leaderAndIsrOpt = ZkUtils.getLeaderAndIsrForPartition(zkClient, topic, partition)\n\n8. AdminTest:\n8.1 testPartitionReassignmentWithLeaderInNewReplicas: How do we make sure that replica 0 is always the leader?\n8.2 testResumePartitionReassignmentThatWasCompleted: Towards the end, the comment says leader should be 2, but there is no broker 2 in the test.\n\n9. ControllerBrokerRequestBatch: Should we rename the two addRequestForBrokers to addLeaderAndIsrRequestForBrokers and addStopReplicaRequestForBrokers respectively?\n\n10. PartitionOfflineException,StateChangeFailedException: We can probably change the implementation to use RuntimeException(message, throwable) directly.\n\n11. LeaderElectionTest.testLeaderElectionAndEpoch(): Not sure if the change is correct. If there is no leadership change, leader epoch shouldn't change, right?\n", "1. ReassignPartitionsCommand:\n1.1  Makes sense, changed that.\n\n1.2 I think that makes sense. Thinking about this more, I guess it is not such a good idea to block the admin command until all the partitions are successfully reassigned. I changed the reassign partitions admin command to issue the partition reassignment request if that path doesn't already exist. This protects accidentally overwriting the zookeeper path. I also added a check reassignment status admin command that will report if the reassignment status of a partition is completed/failed/in progress. Also, another thing to be careful about a batch reassignment API is to avoid piling up important state change requests on the controller while it reassigns multiple partitions. Since reassignment of partitions is not an urgent state change, we should give up the controller lock after each partition is reassigned. That will ensure that other state changes can sneak in, if necessary\n\n1.3 Yes, forgot to include that in v1 patch.\n\n2. Initially, I thought the admin could just re-run the partition reassignment command, but I realize that it involes one manual step.\n\n3, 4 Sure\n\n5. Good point, removed it.\n\n6. This check is not done on every single invocation of onPartitionReassignment, it is done on controller failover and isr change listener. It is not required to be done when the partition reassigned callback triggers. But I think it is a good idea to move it to the callback, just in case we have not covered scenarios when the check should be done.\n\n7.1  While changing the state of a replica to NewReplica, we need to ensure that it was in the NonExistentReplica state. We can remove the replica from the replicaState map after it moves to the NonExistentReplica state explicitly, but there is a chance it will be added back to the map again. This can happen if we re-start the replica after stopping it. But, since this is infrequent, I made this change.\n\n7.2 We do not cache the isr which is required for the controller to be able to send a leader and isr request to the broker\nBesides, this operation is only invoked when a new broker is started or controller fails over. Both of these operations are rare enough that we don't need to worry about optimizing this.\n\n\n8.1 There is a very good chance that it will be. This is because, we always pick the first alive assigned replica as the leader. Since replica 0 is the first assigned replica and is never shut down during the test, it will be the leader. Even if, due to some rare zookeeper session expiration issue, it is not the leader, the test will not fail.\n\n8.2 The comment is redundant there, so I removed it\n\n9, 10. Good point, fixed it\n\n11. It is correct since the controller increments the epoch for isr changes made by itself.\n", "Thanks for patch v2. Some more comments:\n\n20. ReassignPartitionsCommand:\n20.1 Could we add a description of the format of the jaon file in the command line option?\n20.2 If partitionsToBeReassigned is an empty, should we just fail the command?\n20.3 reassignPartitions(): Instead of check the existence of ReassignPartitionsPath and then write in ZK, it's better to use ZkUtils.createPersistentPath(), which throws an exception if node already exists. This will prevent the corner case that the path is created just after the existence check.\n20.4 createReassignedPartitionsPathInZK: It seems that each call to this method just overwrites ReassignPartitionsPath with 1 partition's assignment. So we will lose the assignments of all partitions except the last one?\n\n21. CheckReassignmentStatus: It's better to move checkIfReassignmentSucceeded and checkIfPartitionReassignmentSucceeded from ZkUtils to CheckReassignmentStatus since they are only used here and ZkUtils is getting big.\n\n22. KafkaController.onBrokerStartup() : It seems that we can get partitionsBeingReassigned from the cache in controllerContext, instead of from ZK.\n\n23. PartitionStateMachine.initializeLeaderAndIsrForPartiiton(): When writing the initial leaderAndIsr path for a new partition, there is no need to read the path first to make sure that it doesn't exists. createPersistentPath will throw an exception if the path exists.\n", "20. ReassignPartitionsCommand:\n20.1, 20.2 Sure, that is a good idea\n20.3 For this corner case to happen, another instance of the admin command would have to run at the right time. If that happens, both the admin commands might see that the path doesn't exist and try to create it. At this point, one of the admin commands will get an error and it will exit.\n20.4 Good catch, that is a bug. Initially, I wrote the entire map of all partitions using that API. But later, for per-partition sanity checks, changed it to get invoked for every partition and that probably introduced the bug.\n\n21. They are used in AdminTest as well, but this makes sense.\n\n22, 23. Included these optimizations.\n", "Thanks for patch v3. Looks good to me overall. Just one comment:\n\n20.3 The problem is that reassignPartitions() uses updatePartitionReassignmentData, which in turn uses updatePersistentPath. updatePersistentPath won't throw an exception if a node already exists. So, what could happen is that 2 admin commands are issued at the same time. Both pass the existence test of the ZK path. One command writes its data in the reassignment path first. The other one then overwrites it. Now, both commands appear to have completed successfully. Using ZkUtils.createPersistentPath() instead of updatePersistentPath() would prevent this since the former throws an exception if the path already exists.\n\n", "20.3 Good point, I see what you are saying. Fixed it", "Thanks for patch v4. AdminTest.testResumePartitionReassignmentAfterLeaderWasMoved seems to fail.\n\n[2012-10-08 21:30:06,005] ERROR [PartitionsReassignedListener on 0]: Error completing reassignment of partition [test, 0] (kafka.controller.PartitionsReassignedListener:102)\nkafka.common.KafkaException: Only  replicas out of the new set of replicas 2,3 for partition [test, 0] to be reassigned are alive. Failing partition reassignment\n\tat kafka.controller.PartitionsReassignedListener$$anonfun$handleDataChange$2.liftedTree1$1(KafkaController.scala:512)\n\tat kafka.controller.PartitionsReassignedListener$$anonfun$handleDataChange$2.apply(KafkaController.scala:495)\n\tat kafka.controller.PartitionsReassignedListener$$anonfun$handleDataChange$2.apply(KafkaController.scala:489)\n\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:105)\n\tat kafka.controller.PartitionsReassignedListener.handleDataChange(KafkaController.scala:489)\n\tat org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:547)\n\tat org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)\n[2012-10-08 21:30:06,280] ERROR [PartitionsReassignedListener on 0]: Error completing reassignment of partition [test, 0] (kafka.controller.PartitionsReassignedListener:102)\norg.I0Itec.zkclient.exception.ZkInterruptedException: java.lang.InterruptedException\n\tat org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:687)\n\tat org.I0Itec.zkclient.ZkClient.readData(ZkClient.java:766)\n\tat org.I0Itec.zkclient.ZkClient.readData(ZkClient.java:761)\n\tat kafka.utils.ZkUtils$.readDataMaybeNull(ZkUtils.scala:363)\n\tat kafka.utils.ZkUtils$.getLeaderAndIsrForPartition(ZkUtils.scala:78)\n\tat kafka.controller.KafkaController.areReplicasInIsr(KafkaController.scala:323)\n\tat kafka.controller.KafkaController.onPartitionReassignment(KafkaController.scala:183)\n\tat kafka.controller.PartitionsReassignedListener$$anonfun$handleDataChange$2.liftedTree1$1(KafkaController.scala:509)\n\tat kafka.controller.PartitionsReassignedListener$$anonfun$handleDataChange$2.apply(KafkaController.scala:495)\n\tat kafka.controller.PartitionsReassignedListener$$anonfun$handleDataChange$2.apply(KafkaController.scala:489)\n\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:105)\n\tat kafka.controller.PartitionsReassignedListener.handleDataChange(KafkaController.scala:489)\n\tat org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:547)\n\tat org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)\nCaused by: java.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Object.wait(Object.java:485)\n\tat org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1344)\n\tat org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:925)\n\tat org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:956)\n\tat org.I0Itec.zkclient.ZkConnection.readData(ZkConnection.java:103)\n\tat org.I0Itec.zkclient.ZkClient$9.call(ZkClient.java:770)\n\tat org.I0Itec.zkclient.ZkClient$9.call(ZkClient.java:766)\n\tat org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:675)\n\t... 13 more\n[2012-10-08 21:30:06,377] ERROR [Replica state machine on Controller 3]: Error while changing state of replica 2 for partition [test, 0] to OnlineReplica (kafka.controller.ReplicaStateMachine:102)\njava.lang.AssertionError: assertion failed: Replica 2 for partition [test, 0] should be in the NewReplica,OnlineReplica,OfflineReplica states before moving to OnlineReplica state. Instead it is in NonExistentReplica state\n\tat scala.Predef$.assert(Predef.scala:91)\n\tat kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:194)\n\tat kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:130)\n\tat kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:86)\n\tat kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:86)\n\tat scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)\n\tat scala.collection.immutable.List.foreach(List.scala:45)\n\tat kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:86)\n\tat kafka.controller.KafkaController$$anonfun$onPartitionReassignment$1.apply$mcVI$sp(KafkaController.scala:187)\n\tat kafka.controller.KafkaController$$anonfun$onPartitionReassignment$1.apply(KafkaController.scala:186)\n\tat kafka.controller.KafkaController$$anonfun$onPartitionReassignment$1.apply(KafkaController.scala:186)\n\tat scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)\n\tat scala.collection.immutable.List.foreach(List.scala:45)\n\tat kafka.controller.KafkaController.onPartitionReassignment(KafkaController.scala:186)\n\tat kafka.controller.KafkaController$$anonfun$initializeReassignedPartitionsContext$5.apply(KafkaController.scala:300)\n\tat kafka.controller.KafkaController$$anonfun$initializeReassignedPartitionsContext$5.apply(KafkaController.scala:299)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:631)\n\tat scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:161)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:194)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:80)\n\tat kafka.controller.KafkaController.initializeReassignedPartitionsContext(KafkaController.scala:299)\n\tat kafka.controller.KafkaController.initializeControllerContext(KafkaController.scala:284)\n\tat kafka.controller.KafkaController.onControllerFailover(KafkaController.scala:79)\n\tat kafka.controller.KafkaController$$anonfun$1.apply$mcV$sp(KafkaController.scala:52)\n\tat kafka.server.ZookeeperLeaderElector.elect(ZookeeperLeaderElector.scala:55)\n\tat kafka.server.ZookeeperLeaderElector$LeaderChangeListener.handleDataDeleted(ZookeeperLeaderElector.scala:94)\n\tat org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:549)\n\tat org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)\n[2012-10-08 21:30:06,379] ERROR [Replica state machine on Controller 3]: Error while changing state of replica 3 for partition [test, 0] to OnlineReplica (kafka.controller.ReplicaStateMachine:102)\njava.lang.AssertionError: assertion failed: Replica 3 for partition [test, 0] should be in the NewReplica,OnlineReplica,OfflineReplica states before moving to OnlineReplica state. Instead it is in NonExistentReplica state\n\tat scala.Predef$.assert(Predef.scala:91)\n\tat kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:194)\n\tat kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:130)\n\tat kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:86)\n\tat kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:86)\n\tat scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)\n\tat scala.collection.immutable.List.foreach(List.scala:45)\n\tat kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:86)\n\tat kafka.controller.KafkaController$$anonfun$onPartitionReassignment$1.apply$mcVI$sp(KafkaController.scala:187)\n\tat kafka.controller.KafkaController$$anonfun$onPartitionReassignment$1.apply(KafkaController.scala:186)\n\tat kafka.controller.KafkaController$$anonfun$onPartitionReassignment$1.apply(KafkaController.scala:186)\n\tat scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)\n\tat scala.collection.immutable.List.foreach(List.scala:45)\n\tat kafka.controller.KafkaController.onPartitionReassignment(KafkaController.scala:186)\n\tat kafka.controller.KafkaController$$anonfun$initializeReassignedPartitionsContext$5.apply(KafkaController.scala:300)\n\tat kafka.controller.KafkaController$$anonfun$initializeReassignedPartitionsContext$5.apply(KafkaController.scala:299)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:631)\n\tat scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:161)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:194)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:80)\n\tat kafka.controller.KafkaController.initializeReassignedPartitionsContext(KafkaController.scala:299)\n\tat kafka.controller.KafkaController.initializeControllerContext(KafkaController.scala:284)\n\tat kafka.controller.KafkaController.onControllerFailover(KafkaController.scala:79)\n\tat kafka.controller.KafkaController$$anonfun$1.apply$mcV$sp(KafkaController.scala:52)\n\tat kafka.server.ZookeeperLeaderElector.elect(ZookeeperLeaderElector.scala:55)\n\tat kafka.server.ZookeeperLeaderElector$LeaderChangeListener.handleDataDeleted(ZookeeperLeaderElector.scala:94)\n\tat org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:549)\n\tat org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)\n[2012-10-08 21:30:06,381] ERROR [Replica state machine on Controller 3]: Error while changing state of replica 0 for partition [test, 0] to OfflineReplica (kafka.controller.ReplicaStateMachine:102)\njava.lang.AssertionError: assertion failed: Replica 0 for partition [test, 0] should be in the NewReplica,OnlineReplica states before moving to OfflineReplica state. Instead it is in NonExistentReplica state\n\tat scala.Predef$.assert(Predef.scala:91)\n\tat kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:194)\n\tat kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:156)\n\tat kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:86)\n\tat kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:86)\n\tat scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)\n\tat scala.collection.immutable.List.foreach(List.scala:45)\n\tat kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:86)\n\tat kafka.controller.KafkaController$$anonfun$stopOldReplicasOfReassignedPartition$1.apply$mcVI$sp(KafkaController.scala:363)\n\tat kafka.controller.KafkaController$$anonfun$stopOldReplicasOfReassignedPartition$1.apply(KafkaController.scala:362)\n\tat kafka.controller.KafkaController$$anonfun$stopOldReplicasOfReassignedPartition$1.apply(KafkaController.scala:362)\n\tat scala.collection.immutable.Set$Set2.foreach(Set.scala:101)\n\tat kafka.controller.KafkaController.stopOldReplicasOfReassignedPartition(KafkaController.scala:362)\n\tat kafka.controller.KafkaController.onPartitionReassignment(KafkaController.scala:193)\n\tat kafka.controller.KafkaController$$anonfun$initializeReassignedPartitionsContext$5.apply(KafkaController.scala:300)\n\tat kafka.controller.KafkaController$$anonfun$initializeReassignedPartitionsContext$5.apply(KafkaController.scala:299)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:631)\n\tat scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:161)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:194)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:80)\n\tat kafka.controller.KafkaController.initializeReassignedPartitionsContext(KafkaController.scala:299)\n\tat kafka.controller.KafkaController.initializeControllerContext(KafkaController.scala:284)\n\tat kafka.controller.KafkaController.onControllerFailover(KafkaController.scala:79)\n\tat kafka.controller.KafkaController$$anonfun$1.apply$mcV$sp(KafkaController.scala:52)\n\tat kafka.server.ZookeeperLeaderElector.elect(ZookeeperLeaderElector.scala:55)\n\tat kafka.server.ZookeeperLeaderElector$LeaderChangeListener.handleDataDeleted(ZookeeperLeaderElector.scala:94)\n\tat org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:549)\n\tat org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)\n\nPartition should have been reassigned to 0, 1 expected:<List(2, 3)> but was:<List(0, 1)>\njunit.framework.AssertionFailedError: Partition should have been reassigned to 0, 1 expected:<List(2, 3)> but was:<List(0, 1)>\n\tat junit.framework.Assert.fail(Assert.java:47)\n\tat junit.framework.Assert.failNotEquals(Assert.java:277)\n\tat junit.framework.Assert.assertEquals(Assert.java:64)\n\tat kafka.admin.AdminTest.testResumePartitionReassignmentAfterLeaderWasMoved(AdminTest.scala:361)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat junit.framework.TestCase.runTest(TestCase.java:164)\n", "Also, could you make the two new scripts in bin/ executable?", "The intermittent test failure is due to the partition reassignment failing to complete due to ZkInterruptedException. This is probably due to the test trying to introduce a controller failover. But controller failover takes some time to restart the partition reassignment and the test failed due to a lower value of the timeout. I fixed the comment in the test assertion and increased the wait time. Now all test seem to pass after couple of iterations", "Also, I will make the scripts executable before checking in", "This is blocking work on KAFKA-43, if no one has objections on v5, I will commit this today", "+1 on the patch. Still see AdminTest.testResumePartitionReassignmentAfterLeaderWasMoved fails occasionally due to the same error. However, it seems to be less frequent now. \n\nUnfortunately, you will need to rebase. The patch can be committed after the rebase.", "I fear I might end up rebasing incorrectly and volunteer for applying KAFKA-510 top of this rather than other way around. This is going to require me to revert KAFKA-510, apply KAFKA-42 and then re-apply KAFKA-510. ", "Committed this", "Created reviewboard  against branch origin/trunk", "Updated reviewboard  against branch apache/0.8.1", "This jira is already closed. Is the patch for this jira?", "I think this is just due to the review-tool, which use the magic number \"42\" when no jira number is specified.", "Updated reviewboard https://reviews.apache.org/r/29468/diff/\n against branch origin/trunk"], "derived": {"summary": "As new brokers are added, we need to support moving partition replicas from one set of brokers to another, online.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support rebalancing the partitions with replication - As new brokers are added, we need to support moving partition replicas from one set of brokers to another, online."}, {"q": "What updates or decisions were made in the discussion?", "a": "Updated reviewboard https://reviews.apache.org/r/29468/diff/\n against branch origin/trunk"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-43", "title": "Rebalance to preferred broke with intra-cluster replication support", "status": "Closed", "priority": "Blocker", "reporter": "Jun Rao", "assignee": "Neha Narkhede", "labels": ["features"], "created": "2011-07-19T21:32:20.000+0000", "updated": "2012-10-13T00:45:13.000+0000", "description": "We need to allow the leader to be moved to the preferred broker, for better load balancing.", "comments": ["Added the ability to move the leader to the preferred replica for a partition. This patch contains the following changes -\n\n1. A new admin command PreferredReplicaLeaderElectionCommand and corresponding bin script\n\n2. The admin command takes list of partitions as an input json file and writes it to the zookeeper persistent path /admin/preferred_replica_election\n\n3. The controller registers a data change listener on the admin path. For each such partition, it triggers the OnlinePartition -> OnlinePartition state change with the preferred replica election leader selector. It batches the state changes for all partitions to be able to batch the leader and isr requests sent to the brokers. This is done since there is a very high chance that the admin might use this feature to move some leaders to a particular broker. The controller deletes the admin path once it has finished processing all partitions\n\n4. Added 2 new unit tests to AdminTest", "In addition to the above, I realized there was another bug while using the (String, Int) tuple for topic and partition. So I changed all such usages in controller with TopicAndPartition. Didn't change other places in code to keep this patch contained.", "Patch doesn't apply to 0.8 head so you will need to rebase. Here are some additional comments:\n\nPreferredReplicaLeaderElectionCommand:\n- There's a Utils.checkRequiredArgs helper that you can use.\n- Shutdown hook should close zkClient.\n- updatePreferredReplicaElectionData is unused - or is this meant to support concurrent invocations of the tool in the\n  future?\n- Would be good to have a short --help string for all the admin tools.\n- Spacing conventions: some places have no space between the last character and opening brace, else should be on new\n  line, etc.\n\nKafkaController:\n- Instead of nulls in the controller context, can we switch to Set.empty? I can try that in KAFKA-340 as well.\n- Would be clearer to rename removePartitionsFromPreferredReplicaElection to maybeRemove...\n- Also, the name removePartitionsFromPreferredReplicationElection does not document the side-effect of deleting the\n  PreferredReplicaLeaderElectionPath. The delete can be moved to a finally block of handleDataChange. In fact,\n  it seems removePartitionsFromPreferredReplicaElection can also be moved (from onPreferredReplicaElection) to the\n  finally block.\n- Likewise, the initialize(ReassignedPartitions|PreferredElection)Context have the additional side-effect of actually\n  triggering the operation so can we rename accordingly? e.g., triggerPreferredReplicaLeaderElection would be a\n  sufficient name since these two \"contexts\" really live in ControllerContext.\n\nRevert test log4j.properties\n\nTestUtils.readFileIntoString can be removed.\n", "Overall, patch looks good. It would useful to add an option so that the controller will try to move all partitions to the preferred replica.", "Thanks for the review, Jun and Joel ! Here is a follow up patch to address your review suggestions - \n\nJoel's review\n\n>> - Would be good to have a short --help string for all the admin tools.\nThat is provided by the joptparser when you invoke it without any parameters\n\nKafkaController:\n>> - Instead of nulls in the controller context, can we switch to Set.empty? I can try that in KAFKA-340 as well.\nWould like to do that in a follow up cleanup patch. If you get to it before, that's fine as well.\n\n>> - Would be clearer to rename removePartitionsFromPreferredReplicaElection to maybeRemove...\nDidn't rename this since it *always* removes the partitions\n\nRest of the comments are addressed\n\nJun's review\n\nThat's a great point. Changed the tool to default to all partitions. \n\nAlso, found a bug in the PreferredReplicaPartitionLeaderSelector where it didn't check if the preferred replica was in the isr or not. Fixed that.\n", "Thanks for patch v2.\nThanks for patch v2. Just some minor comments. Once addressed, the patch can be checked in without another review.\n\n20. AdminTest:\n20.1 testBasicPreferredReplicaElection(): remove the println statement\n20.2 remove unused imports\n\n21. KafkaController.removePartitionsFromPreferredReplicaElection(): If we can't move the leader, we should probably log it as warning instead of error.\n\n22. PartitionLeaderSelector.PreferredReplicaPartitionLeaderSelector(): Instead of using match/case, it seems it's simpler if we write it as if/else. If the preferred replica is in isr and is live, then we move the leader to the preferred replica. Otherwise, we just throw an exception.\n\n23. TestUtils.readFileIntoString(): not used.\n\n\n\n", "Addressed v2 review comments and checked in.", "re: removePartitionsFromPreferredREplicaElection - yes I misread that.\n\nFor --help I was referring to a description of what the tool does. On second thoughts, a well-named script + good docs on the arguments is better.\n\nI'll address getting rid of the nulls in KAFKA-340."], "derived": {"summary": "We need to allow the leader to be moved to the preferred broker, for better load balancing.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Rebalance to preferred broke with intra-cluster replication support - We need to allow the leader to be moved to the preferred broker, for better load balancing."}, {"q": "What updates or decisions were made in the discussion?", "a": "re: removePartitionsFromPreferredREplicaElection - yes I misread that.\n\nFor --help I was referring to a description of what the tool does. On second thoughts, a well-named script + good docs on the arguments is better.\n\nI'll address getting rid of the nulls in KAFKA-340."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-44", "title": "Various ZK listeners to support intra-cluster replication", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Neha Narkhede", "labels": [], "created": "2011-07-19T21:32:20.000+0000", "updated": "2012-06-10T20:42:28.000+0000", "description": "We need to implement the new ZK listeners for the new paths registered in ZK.", "comments": ["Hi all,\n\nI spent today working through this (and consequently KAFKA-45 because there's a lot of overlap), hoping to have a patch in mid-week, and had a suggestion regarding how new replicas are assigned to brokers.  Currently, every broker effectively listens to every possible path in /brokers/topic/[topic]/partitions/[partition] because it doesn't know which partition replica will be assigned to it.  This places heavy burden on ZK since it will have to fire an unncecessary high volume of callbacks to every broker.  \n\nThe problem is that the broker doesn't know for which partition it will be assigned a replica for after its up.  I propose an approach whereby a new path in ZK acts as a queue which brokers watch to receive new replica assignments.  This path will exist for every broker in the system, so something like /brokers/[brokerId]/new_partition/[topic:partitionId] or some such thing?  The admin utilities will create these paths along with the replica assignments.  I realize this is additional overhead and somewhat asymmetric with the other partition paths so I'm interested to hear peoples thoughts!", "Prshanth, \n\nThis seems like a good idea to me. Ideally, this new ZK path /brokers/[brokerId]/new_partition/[topic:partitionId] should be created atomically with other ZK paths constructed when a new topic is created. We could leverage the ZK multiple-row transaction support for this when ZK 3.4 is more stable.", "Sounds good.  Another question is what to do with that path should the broker go down with pending assignments.  My vote would be for the broker to recreate it as part of the startup process, thereby deleting previous data.  My thinking is that the broker will discover all assigned replicas naturally as part of startup assuming the topic creation admin tool did its job correctly updating partition replica lists in ZK.", "Not sure if I follow exactly what you are proposing here. In particular, what does the broker recreate? My thinking is the following:\n\n/brokers/[brokerId]/new_partition/[topic:partitionId] is the source of truth about what topic/partitions a broker has and that path is only created during topic creation. A broker listens to that path to pick up new topic/partition assigned to it. When a broker starts up, it simply reads all /brokers/[brokerId]/new_partition/[topic:partitionId] to determine the set of topic/partition that it should have. So the broker never needs to recreate that path.", "Ah, I think I see where you're going with this.  You'd like to use it as a persistent path listing all partitions assigned to a broker.  If so, perhaps the path can be changed slightly to /brokers/[brokerId]/assigned_partitions/[topic:partitionId] to indicate appropriately?  \n\nMy intention was to have that just for new partition assignment notifications and act more like a queue.  So the flow would be:\n\n1. Tool assigns topic X, partition Y to broker Z\n1a. New entry added to /broker/topics/X/partitions/Y/replicas to append broker Z to list.\n1b. New znode created at /broker/Z/new_partition/[X:Y]\n2. Broker listens on /broker/Z/new_partition\n2a. New partition is assigned, bootstrap replicas\n2b After successful bootstrap, remove /broker/Z/new_partition/[X:Y]\n\nI like you're idea much better; it's clear which partitions are assigned to which broker and makes startup simpler as well.\n", "Can we also use this new path for partition reassignment?  When the admin would like to reassign a partition, the broker's ID is appeneded to the list at /brokers/partitions_reassigned/[topic]/[partId] and a znode is created in /brokers/[brokerId]/assigned_partitions/[topic:partId] ?  Then only the leader listens on the reassignment path in order to update the leader replicas RAR and the new brokers become aware of new partitions and bootstrap as they normally would.  Thoughts?", "It may be possible, but this can be a bit tricky. When you move a partition, you want to wait until the partition in the new broker has fully caught up, before deleting the one on the old broker. So, one way to achieve this is to have another ZK path that indicates this transition state. After the transition is done, the new assignment will be added to the assigned_partition path.\n\nIn any case, let's start by just focusing on static partition assignment. We can worry about partition reassignment a bit later when we get to kafka-42.", "Okay, I've attached a very very rough draft of a patch.  I'm really looking for feedback and thoughts because there's just a lot of overlap with KAFKA-45.\n\n1. New classes are Replica, Partition and ReplicaManager.  kafka.replica.Partition will need to be merged with kafka.cluster.Partition which is a little light at the moment.\n2. There are a few placeholders for KAFKA-45 and KAFKA-46 in there.\n3. Does not include the partition reassignment listener.  We'll need to revisit this because the current process suffers from the same problem as the previous partition assignment logic.\n4. Includes the new assigned_partitions path but relies on old replica assignment path creation for testing purposes.  This will need to change once the tools change.\n5. I've tried to make it as unintrusive as possible.\n\nThere is one issue I'm trying to wrap my head around.  Consider a broker A that comes up with no partitions and the admin reassigns a partition X to it.  It properly bootstraps it and catches up.  Upon catch up, the current leader executes a leader election and assume broker A wins.  Broker A then does some bootstrapping before \"switching on\" the partition and serving fetch/produce requests.  Part of the bootstrap is determining which replicas are in ISR (read from ZK) by waiting for the replica to catch up, but because the server isn't responding to fetch requests for the replica and it isn't aware of where every replica is in terms of its HW and LEO, none will ever catch up \"in time\".  Am I missing something?", "This approach of having a queue of state change requests that each replica acts upon, is something I'm leaning towards for all state changes. \n\nThere are 2 ways of making state changes in a system which uses ZK listeners -\n\n1. Each server listens on various ZK paths, registers the same listeners, and follows the same code path to apply state changes to itself. Here, the state machine, is replicated on each server.\n2. A highly-available co-ordinator listens of various ZK paths, registers ZK listeners, verifies system state and state transitions. Then issues state transition requests to the various replicas. Here, only the co-ordinator executes the state machine.\n\nWe have been down approach 1 earlier with the zookeeper consumer, and through experience, found that though, it seems simpler to design and implement at first, it turns into a fairly buggy and high operational overhead system. This is because that approach suffers from \n\n1. herd effect\n2. \"split brain\" problem. \n3. In addition to these, it will be pretty complicated to perform upgrades on the state machine and can leave the cluster in an undefined state during upgrades.  \n4. Monitoring the state machine is a hassle, due to it being distributed in nature\n\nApproach 2 ensures the state machine only on the co-ordinator, which itself is elected from amongst the brokers. This approach ensures that - \n\n1. at any point of time, we can reason about the state of the entire cluster.\n2. Only after the state is verified, can further state changes be applied. If verification fails, alerts can be triggered preventing the system from getting into an indefinite state.\n3. A big advantage of this approach is easier upgrades to the state machine. It is true that, theoretically, state machine logic doesn't change much over time, but in reality, state machine changes would need upgrades, due to improvements in the logic or fixing code bugs. \n4. Monitoring the state machine becomes much simpler\n\nIn general, both approaches are âdoableâ, but we need to weigh the cost of âpatchingâ the code to make it work VS choosing a simple design that will be easy to maintain and monitor.\n\nI would like to see a discussion on this fundamental design choice, before jumping to code and patches on KAFKA-44 and KAFKA-45.  ", "That sounds fair enough.  Can we create a wiki page attached to one created for the overall replication work as mentioned in KAFKA-50?", "Also, mind if I assign this to myself?", "This JIRA can implement the stateChangeListener() as described in the Kafka replication design document, and leave stubs for becomeFollower()/becomeLeader() which are part of KAFKA-302. Also, lets leave out anything about partition reassignment for now. That work is included as part of other JIRAs and can be done when the basic replication functionality is nailed and tested.\n\nPrashanth, I've attempted to reduce dependencies and define scope of KAFKA-44 and KAFKA-45. Hopefully the above clarifies the scope of this JIRA. Assigning it to you, as per your request", "Sounds good to me.  I'm also fine with removing this ticket and rolling this work as another subtask of KAFKA-45 just for clarity.  Your decision :)", "This is very closely related to the broker startup procedure", "Fixed as part of KAFKA-301"], "derived": {"summary": "We need to implement the new ZK listeners for the new paths registered in ZK.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Various ZK listeners to support intra-cluster replication - We need to implement the new ZK listeners for the new paths registered in ZK."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed as part of KAFKA-301"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-45", "title": "Broker startup, leader election, becoming a leader/follower for intra-cluster replication", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Neha Narkhede", "labels": [], "created": "2011-07-19T21:32:20.000+0000", "updated": "2012-06-10T20:40:36.000+0000", "description": "We need to implement the logic for starting a broker with replicated partitions, the leader election logic and how to become a leader and a follower.", "comments": ["Hey everyone.  Since the ZK structures are effectively done as part of KAFKA-47, I thought I'd start on this ticket.  Something that came up was the log cleanup functionality within LogManager will need to be tweaked.  My thinking is that Replica's should manage the cleanup of their local logs (moving that functionality from LogManager) and need to be in-sync with the leader with respect to hw, logEndOffset, and also the logMinOffset; essentially, the amount of data available on all ISR for replica R must be the same reglardless of each individual broker's log cleanup configuration.  Not sure how that information should be propagated, whether through the follower's fetch request or somewhere in ZK, that should be left up to discussion.  Regardless, non-leader replica's can use this minOffset to perform local log cleanups, I suppose.  Please do let me know if I'm missing a peice of the puzzle here or if there's a simpler solution.\n", "Prashanth,\n\nThanks for helping out on replication and getting in the format changes. Before starting on this, we are blocked on KAFKA-49 and KAFKA-44. It seems like it will be most effective to get some help on these JIRAs first. Just a suggestion, comments are welcome!\n\n", "No worries.  The reason I chose to start here was that I felt it required the creation of some of the base entities required for KAFKA-49 and KAFKA-44.  Since 49 is still semi-blocked by 240 on the produce-side, I didn't want to make too many changes there.  44 on the other hand, makes use of some of algorithms used as part of start up.  Thinking about it, 44 is probably a better place to start in terms of complexity.  Thanks!  ", "Prashanth,\n\nThat's a good question. We'd like to make replicas identical with each other. This is easy for size-based log retention, but harder for time-based log retention. What you proposed is to have only the leader delete old log segments and propagate this information to the followers. This seems like a reasonable approach. The question is how should the leader communicate such information to the followers. One possibility is to piggyback on the FetchResponse returned to the followers. This will mean some extra optional fields in FetchResponse.", "I think we are overthinking this. Currently cleanup is not a precise SLA, it is just a guarantee of the form \"we will never delete anything younger than X OR we will always maintain at least Y bytes of messages\". Trying to maintain this in synchronous form across nodes is overkill I think. It is fine if every node acts independently as long as each of them respects the SLA. I think this should be much simpler and more likely to work.", "Here is something to think about wrt to leader election and replica failures -\n\nIf there are 3 replicas for a partition, and the leader acks the produce request once the request is acked by the 2 followers. The produce request doesn't care about the replication factor. So if one of the followers is slow, the leader will receive less than 2 acks from the followers, and it will go ahead and send a success ACK to the producer. The replicas update their HW only on the next replica fetch response. Since the HW committer thread is running independently. it is possible that the checkpointed HW of one of the 3 replicas is lower than the others. \n\nIf at this point, if leader fails, it will trigger the leader election procedure. According to the current design proposal, any replica in the ISR can become the leader. If the replica with the lower HW becomes the leader, then it will truncate its log upto this last checkpointed HW and start taking produce requests from there. The other 2 replicas, will send ReplicaFetchRequests with an offset that doesn't exist on the leader.\n\nEffectively, it seems that we will end up losing some successfully acknowledged produce requests. Probably, the leader election procedure should check the HW of the participating replicas and give preference to replica with highest HW ?", "If there are 2 followers and leader receives ack from only follow 1, but not follower 2 (within timeout), the leader will kick follower 2 from ISR before it can commit the message and ack the producer. So, follower 2 will never get a chance to become the new leader should the current leader fail.", "Resolved as part of KAFKA-46"], "derived": {"summary": "We need to implement the logic for starting a broker with replicated partitions, the leader election logic and how to become a leader and a follower.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Broker startup, leader election, becoming a leader/follower for intra-cluster replication - We need to implement the logic for starting a broker with replicated partitions, the leader election logic and how to become a leader and a follower."}, {"q": "What updates or decisions were made in the discussion?", "a": "Resolved as part of KAFKA-46"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-46", "title": "Commit thread, ReplicaFetcherThread for intra-cluster replication", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Neha Narkhede", "labels": [], "created": "2011-07-19T21:32:20.000+0000", "updated": "2012-06-01T20:58:51.000+0000", "description": "We need to implement the commit thread at the leader and the fetcher thread at the follower for replication the data from the leader.", "comments": ["Should this be broken down into two sub tasks: One for the commiting thread on the leader and one for the fetcher thread on the follower?", "Some notes on the implementation:\n\n1. If required_acks for a produce request is not 0 or 1, the commit thread will put the request in a DelayedQueue and adds it to a watch list for that topic/partition (similar implementation as long poll in kafka-48).\n2. There will be an \"offset watcher\" per topic/partition. The watcher fires everytime a follower in the in-sync set advances the offset. Every time the watcher fires, it checks from the head of watcher list and see if any produce request can be satisfied on this partition. If so, marks this partition as satisfied, if all partitions of the produce requests are satisfied, dequeue the request an send back the ack. \n3. How to add/delete follower from in-sync set? We can run a background insync-check thread that does the following:\nfor each topic/partition\n  get and save the offset of each partition of the leader replica\n  wait for KeepInSyncTime\n  check if any insync replica hasn't caught up to the saved offset, if so, drop it out of insync set (need to fire the corresponding offset watcher) \n  check if any replica (not in insync set) has caught up to the saved offset, if so, add it to insync set.\n\nIf we follow this approach, we can have 2 subtasks that implement \"offset watcher\" and insync-check thread. We can also have 1 separate subtask for the FetcherThread and another for the commit thread.\n", ">> Should this be broken down into two sub tasks: One for the commiting thread on the leader and one for the fetcher thread on the follower? \n\nLets not worry about that for now. I'll take care of this JIRA, once we resolve the other JIRAs. This one has many other dependencies.", "Started work on this, will upload a patch, once KAFKA-45 is resolved", "Attaching a draft patch for message replication. This is just to give a high level overview of the changes involved and is by no means ready to be committed. So no need for a detailed review. There are probably a few bugs lurking around. Since the changes are pretty significant, I was hoping to get some early feedback. \n\n1. Added ReplicaFetcherThread that reads data from the leader and appends to local replica log\n\n2. Added highwatermark maintenance at the leader and the follower. The highwatermark is checkpointed in the partition directory in a file named highwatermark.\n\n3. Added ISR maintenance logic on the leader. This involves possibly expanding the ISR while handling a fetch request from a follower. \n\n4. Also added ISRExpirationThread that tracks the highwatermark update time for all partitions that the leader owns and shrinks it if (hw update time + keep.in.sync.time.ms) < current time. \n\n5. Note that to get this patch going, I had to put in code to cover KAFKA-302. I would encourage a more detailed review for the becomeLeader() and becomeFollower() APIs. We would either like to check it in from this patch, or if Prashanth has some patch, review that one too. \n\nI will probably add v1 patch with unit tests early next week. \n\nAlso, I would like to check this in parts, if possible. Starting with probably KAFKA-302, then the actual message replication logic. But that is open for discussion as well. \n\n", "I can comment on point 5, related to KAFKA-302.  After a first pass this is what I've got, let me know what you think:\n\n1. In KafkaZooKeeper.leaderElection, if the replica isn't elected as the leader, it should issue a becomeFollower after reading who became the leader from ZK.\n2. In KafkaServer.becomeLeader, the leader should probably update the ISR and CUR list by performing the same type of logic as the ISRExpirationThread does.  If the intention was to rely on the ISRET to perform this asynchronously, I think it'll need to be modified to update the partition's CUR list along with the ISR.\n3. In ISRExpirationThread, you'll need to add the slowest partition back into the queue in every case.\n4. It seems like there are two ways to update a leader replica's HW, either through Replica.hw itself or Partition.leaderHW.  To avoid confusion, can we simplify and provide only one API through which all clients to perform this?  The latter seems to do the same thing but just update the hw update time.  \n5. Can the leo and hw methods in Replica make use of the isLocal method?  The logic is a little more clear this way, IMO.\n6. In KafkaApis.handleFetchRequest, it looks the maybeAddReplicaToISR call is unncessary and is rolled into readMessageSets?  Any reason we need it there?\n7. KafkaApis.readMessageSets should probably verify that the leader for the partition exists on the broker.\n\nIn general, I'm not entirely sold on the ISRExpirationThread.  From my point of view, there is a function that, given a partition, determines whether its ISR/CUR list needs to be updated in-memory and in ZK.  Right now, there is a single thread that uses a heap to pick off the partitions with the oldest update times, waits for expiry if necessary, then updates accordingly.  I'm wondering if it's possible instead to leverage a scheduled executor that appropriately schedules the execution of the above function on a given partition based on the same criteria (the partition's HW updated time); when the task actually executes, it's possible the hw would have moved, making the task an no-op.  The benefit there is simplicity, added concurrency and a slightly more accurate/real-time reflection of the ISR list in ZK meaning a possible reduction in message loss during leadership changes?", "Prashanth, \nThanks for reviewing the patch, in detail. Like I mentioned earlier, this is just a draft patch and will clearly miss some details like you've pointed out. I'm looking more for high level feedback on data structures used, any ideas for refactoring etc.\n", "Some comments on the draft.\n\nHigh level:\n1. We should consider whether to have 1 HW checkpoint file per partition vs 1 HW checkpoint file for all partitions. The benefit of the latter is fewer file writes during checkpoint and fewer file reads during broker startup. Also, to avoid corrupting the checkpointed file, we should probably first write the file to a tmp file and rename to the actual checkpointed file. This probably can be done in a separate jira.\n\n2. The benefit of using an ISRExpirationThread is that it's relatively simple since there is 1 thread doing all the ISR expiration. One drawback I can see is that idle partitions are still constantly checked by the thread. This may or may not be a big concern.\n\nLow level:\n3. KafkaApis:\n3.1 Agreed with #6 in Prashanth's comment. Probably don't need to call maybeAddReplicaToISR directly from handlFetchRequest.\n3.2 A subtle issue is that we should probably wait until a (replica) fetch request is successful before updating the follower replica's LEO. This is because during an unclean failover (no live brokers in ISR), the offset of the first fetch request from a follower may not be valid.\n3.3 We need to update ISR in ZK and in memory atomically since the ISR can be expanded and shrunk from different threads.\n\n4. Partition:\n4.1 We probably don't need to add reassignedReplicas in the patch and can add it later when we get to kafka-42, if necessary.\n4.2 We probably don't need both catchUpReplicas and assignedReplicas since we can always derive one from another together with ISR.\n4.3 Do we need to maintain a HashMap of <replica_id., Replica>, instead of a set of replicas for faster lookup? This may not be a big deal since the replica set is small.\n4.4 Should we keep highWatermarkUpdateTime in Log where the HW is stored?\n\n5. Replica:\n5.1 leo(), if log is present, we should return l.leo not l.getHighwaterMark.\n\n6. KafkaConfig: All follower related properties should be probably be prefixed with \"follower\".\n\n7. Log:\n7.1 recoverUptoLastCheckpointedHW(): if there are k+1 log segment files need to be truncated, we should delete the last k and truncate the first one.\n", "Prashanth,\n\nRegarding your high level comment -\n\nIf you use a scheduled thread pool executor to schedule every partition greadily, you would be spinning up a bunch of threads that might not do any work, since the priority of the items in the queue keeps changing. Also, you will need to change the priority of an already scheduled task, which might not be possible to do. What's more, if the threadpool maxes out, a partition that requires immediate ISR expiration might not get scheduled on time. \n\nI'm guessing it is unlikely that all the partitions on a node expire at the same time. Even if they do, it might take maybe a few seconds for the last partition to shrink its ISR, which is not a big deal. In reality, there would be very few partitions, maybe 1 or 2, that need to shrink their ISR due to slower followers. That's why a single thread seems to suffice for handling the ISR expiration for all partitions. \n\nRegarding your detailed review comments -\n\n1. Fixed that\n2. That is a good suggestion. I've attempted a refactoring, let me know if you have more feedback on it. The ISR thread updates the ISR in ZK AND in memory cache, where as the become leader API is passed in the latest ISR read from ZK and it just has to update its cache with that state, on becoming a leader. So, I wrapped up the cache + Zk update in an updateLeaderAndISR API in KafkaServer. This will be used by the ISR expiration thread. The become leader should not be updating anything in Zk, it should just be reading from Zk and updating its cache. \n3. The patch already does it at the end of the while loop. Or do you mean something else ?\n5. Yes\n6. The replica should be added to the ISR as soon as the fetch request is received by the server. I intended to add the replica to the ISR even if it might get ended up in the purgatory waiting for additional data. Ideally, would like to get rid of it from the readMessageSets() API.\n7. Good point. Added that.\n\nJun\n\nRegarding your high level comments -\n\n1. Yes, I kept it simple in this patch, since the main goal is to get the message replication to work. Persisting all the HW in one file would definitely be a better approach and can be another JIRA\n2. Idle partitions will cause one delete and one insert into the priority queue. It doesn't look like an issue, but could be resolved by adding a TTL to items in the queue. Idle partitions will expire from the queue and will be added back to the queue when the leader receives a produce request for that partition. However, I'd like to push that to later, since it does not improve correctness and does not look like a performance issue. \n\nRegarding your detailed review comments -\n\n3.2 Good point. I think that is better too.\n3.3 Yes, this is one of those âdetailsâ that I didn't include in the draft patch.\n4.1 Removed it\n4.2 Removed CUR\n4.3 Kept it simple since replication factors that make sense in production are typically 3-5.\n4.4 Yes, but when will it be used ?\n\n5.1 Good catch !\n\n6. That exists in the patch. Did I miss any ?\n7. Good point. Fixed it\n", "Thanks for the patch. Overall, a very encouraging patch given the complexity of this jira. Some comments:\n\nFrom previous reviews:\nWere 4.1 and 4.2 addressed in the patch? I still see CUR and reassignedReplicas.\nFor 4.4, I think highWatermarkUpdateTime can be used as described in 15.2 below.\nFor 6, I meant that all local variable names should also be prefixed with follower.\n\nNew review comments:\n11. KafkaApis:\n11.1 handleFetchRequest(): if the leader of one partition is not on this broker, we reject the whole request. Ideally, we should just send the error code for that partition in the response and fulfill the rest of the request. \n11.2 handleFetchRequest() and readMessageSets(): If the leader is not on this broker, we should probably return a new type of error like NotLeaderException, instead of using InvalidPartionException or throwing IllegalStateException. \n11.3 readMessageSets(): add a comment of what -1 means for replicaId\n\n12. ReplicaManager:\n12.1 remove unused imports\n12.2 maybeIncrementLeaderHW(): if(newHw < oldHw) should be if(newHw > oldHw)\n12.3 We will need to either synchronize the methods in this class or use ConcurrentHashMap for allReplicas since allReplicas can be read and updated concurrently.\n\n13. Replica:\n13.1 hw(): to be consistent, we should probably throw IllegalStateException, instead of InvalidPartitionException.\n\n14. KafkaServer:\n14.1 There are a couple of TODOs. Will they be addressed in this jira or separate jiras?\n\n15. ISRExpirationThread:\n15.1 It seems that when the time expires, we always update the ISR. We should only update ISR if it actually shrinks.\n15.2 Currently, we take a replica out of ISR if its LEO is less than leaderHW after keepInSync time. We probably should use the following condition:\n  leaderHW - r.leo > keepInSyncBytes || currentTime - r.highWatermarkUpdateTime > keepInSyncTime\n  The first condition handles a slow follower and the second condition handles a stuck follower.\n15.3 I think we can potentially get rid of the inner while loop by putting all the logic when time expires in a if statement and the awaitUtil part in the else clause of the if statement.\n15.4 Also, instead of using a priority queue and keep adding and deleting partitions into the queue, would it be simpler to have the thread just check the isInsyncCondition for each partition every keepInSyncTime?\n\n16. LogDisk: recoverUptoLastCheckpointedHW(): \n16.1 The second condition in\n          segments.view.find(segment =>  lastKnownHW >= segment.start && lastKnownHW < segment.size) \n       seems incorrect. It seems that you want to use \"lastKnownHW < segment.messageSet.getEndOffset\"\n16.2 The files of all deleted segments should be deleted like that in LogManager.cleanupExpiredSegments().\n\n17. LogOffsetTest:\n17.1 There is no need to keep testEmptyLogs(), since we have a test that covers fetching from a non-existing topic using SimpleConsumer.\n\n18. PrimitiveApiTest:\n18.1 testConsumerNotExistTopic(): we probably shouldn't create the topic in this case.\n\n19. ProducerTest:\n19.1 testZKSendToNewTopic(): Which should fix the comment that says \"Available partition ids should be 0, 1, 2 and 3\" since there is only 1 partition created.\n\n20. ReplicaFetchTest:\n20.1 Since the test is already using in-memory log, we can remove TODO in testReplicaFetcherThread().\n20.2 testReplicaFetcherThreadI(): Instead of sleeping and then checking log.get.getLogEndOffset, could we create a utility method that keeps checking until LEO reaches certain value up to a certain max wait time? Maybe we should make a more general util that waits up to a certain amount of time until a condition is satisfied. \n", "Comments\nSome of these we discussed in person but I wanted to post them here for anyone else following along. A number of these are really just structural or naming comments. When we get closer to final form I will do a pass on trying to understand all the logic and see if I can find any corner cases, but I haven't done that yet. As a result I am not really sure if I understand everything I am commenting on, so take it all with a grain of salt.\n \n1. This patch pays really good attention to code structure and testability which is awesome since we are adding gobs of hard logic. Nice to see things getting cleaner as we do this.\n2. For some reason I preferred just having Log instead of DiskLog and MemoryLog. I feel like doing it twice tends to lead to similar logic in both. I do like the idea of lightening unit tests. I wonder if a helper method to set up a log wouldn't be good enough, though? Not sure if this is a rational preference or just inertia on my part, though, so feel free to ignore. If we do separate out a Log trait I feel we should clean up that interface a bit it is kind of a disaster right now (probably we should do that regardless).\n3. Maybe HW mark should move out of Log since now it really indicates something about the state of other servers and Log is meant to be just a simple stand alone log implementation.\n4. I think expanding out some of the acronyms in public methods would be nice: i.e. highWatermark and logEndOffset. Having a concise local variable name is helpful but for the poor person trying to learn the code i think the slightly more verbose name is helpful. If you prefer the more concise naming then just having good javadoc that explains the abbreviations would be good.\n5. Consider making Replica just be a dumb pojos (posos?) and move the ReplicaFetcherThread elsewhere.\n6. We currently have Partition and BrokerPartitionInfo. We should clarify why both of these and make the naming make sense. To me a partition is logically just (topic, partId). I think BrokerPartitionInfo is really a bit hard to understand, though that is unrelated to this patch. Partition is more like the broker's information about replicas of that partition. Also both Partition and Replica are in the cluster package which was originally shared by client and server. Now with all the additional stuff this is really part of the server only, right? Probably we should change the package name...?\n7. I sent a separate email on setters/getters. I think overall there area a lot of setter/getter methods. We should pick a style for these and go with that uniformly (we haven't been consistent so far). \n8. I think we should figure out a general strategy for managing the zookeeper interactions. I think it is wise to wrap up the zookeeper dependency but having everything in one class is too much. Maybe the way to go is to have generic ZkUtils and reusable infra like ZkQueue and then split KafkaZookeeper into the logical functions it covers.\n9. For the config I recommend the prefix \"replication\" or \"replica\" instead of \"follower\" (e.g. replication.socket.timeout.ms), I think this is more clear to someone who hasn't read about the internals of our replication design and doesn't know the terminology.\n10. A bit of internals have spilled into KafkaServer, such as locking, ISR management, etc. I think KafkaServer should just interact with the main subsystems of kafka in a very abstract way. I think the core problem is that we need to think more about the functionality and API of ReplicaManager. To my mind the replica manager should be the one running the ISR maintence, doing its own locking, etc. To me the main subsystems are (1) logs managed by the LogManager, (2) the network layer wrapped up by SocketServer, (3) the request processor pool (4) and now the ReplicaManager or ReplicationManager or whatever we want to call it.\n11. ISRMaintenceThread--I think you are right that we will need to be very prompt about handling ISR expiration since this is effectively part of our failure detection time. It might be good, though to just stick in the polling loop Jun suggested for now, and then come back to optimize it later (even though we almost certain will have to), just to reduce the scope in this iteration. \n12. Also make sure the ISR thread either uses the Utils.newThread helper or handles the common gotchas (thread name, set daemon properly, set uncaught exception handler). Also think through the details of the lifecycle.\n13. We have a lot of threads that basically run in a loop and use an isRunning atomic boolean and count down latch. You added two but I think we had a few others. Consider factoring this out into a helper runnable that these can extend. Verifying the lifecycle details for each is kind of a pain and it pretty easy to either not cleanly shutdown all the threads or block indefinitely or whatever.\n14. The changes in KafkaApis seem kind of brute forced. ensureLeaderOnThisBroker and the massive expansion of logic in readMessageSets seems like we are just brute forcing through this problem. We need to find a way to structure this into methods that make sense and don't reach into the internals of other parts of the system. readMessageSet is already doing crazy funky stuff that needs to be fixed. I think restructuring readMessageSet will help with some of the problems, and the rest can maybe be solved by pushing all the replica/leo/isr logic here into ReplicaManager. Basically the API level should just say ReplicaManager.leaderForPartition(id) as part of the request validation and ReplicaManager.recordFollowerPosition(...) and move all the other details out of the KafkaApis. Not sure if I understand this well enough for that to make sense...\n15. We should always return the hw mark in the PartitionData, right? This way we can do monitoring on the consumers. Currently it looks like we only do this for replicas.\n16. Name for ReplicaManager.makeSurePartitionExists and ReplicaManager.assurePartitionExists doesn't really call out the difference. I would recommend calling them getOrCreatePartition() and ensureExists()\n17. Overall I would think through the public API for ReplicaManager. I think it may be possible to move much more replica/replication/partitioning logic under this classes wrapper and out of other parts of the system which would be good. ", "Jay's comments remind another thing:\n\n21. The follower's HW should be min(follower LEO, leader HW). This is to handle the case that a follower is still catching up.", "Thanks for the great feedback ! This is probably the largest jira for KAFKA-50, I've done my best to include the review changes in this JIRA. I will file separate JIRAs to include other review suggestions. Let me attempt to describe the changes made in this patch -\n\n1. Simplied the ISR maintenance logic to iterate through the partitions every keepInSyncTimeMs ms. I guess the overhead is O(n) for isr expiration and O(1) for replica fetch requests. This seems reasonable since keepInSyncTimeMs is expected to be in the order of several seconds and replica fetch requests are easily more frequent than that.\n2. Fixed ISR expiration logic to remove a slow follower as well as a stuck follower from the ISR\n2. Moved replication specific logic inside ReplicaManager and Partition. So KafkaApis and KafkaServer have minimum replication specific code\n3. Removed InMemoryLog, I guess that was an over optimization\n4. Kept the high watermarks in a separate file, will fix it in a separate JIRA to contain the changes in this JIRA. ", "Regarding Jun's comments -\n\n4, 6: Done\n\nNew review comments:\n11. KafkaApis:\n11.1 Makes sense\n11.2 Added a new exception class NotLeaderForPartitionException. We can improve the naming going forward.\n11.3 Done\n\n12. ReplicaManager:\n12.1 Done\n12.2 Good catch \n12.3 Ideally, I would like to get rid of allReplicas, maybe do it differently. I'm thinking of fixing this in another JIRA. Let me know if you prefer fixing it in this one.\n\n13. Replica:\n13.1 Done\n\n14. KafkaServer:\n14.1 Removed the TODOs. They are addressed.\n\n15. ISRExpirationThread:\n15.1 Done\n15.2 I've included logic for handling slow and stuck followers, and unit tested it.\n15.3 It has completely disappeared now.\n15.4 Agreed\n\n16. LogDisk: recoverUptoLastCheckpointedHW():\n16.1 That's a good point.\n16.2 Done\n\n17. LogOffsetTest:\n17.1 Deleted it\n\n18. PrimitiveApiTest:\n18.1 testConsumerNotExistTopic() Actually I'm not too sure this test makes sense in the replication branch. Is this testing that the server returns some meaningful error code if it receives a request for an unknown topic ? If yes, maybe we don't need a consumer to test that logic. I haven't fixed this test, maybe we can think more on what exactly we want to test here. \n\n19. ProducerTest:\n19.1 testZKSendToNewTopic(): Done\n\n20. ReplicaFetchTest:\n20.1 Right\n20.2 testReplicaFetcherThreadI(): That is a good suggestion. I'd like to clean up unit tests and add related helper APIs, maybe in another JIRA.", "Indeed, this is replication! The rest of it is just a simple matter of handling failures and a little tooling. :-) Very nicely done.", "Jay,\n\nThanks for thinking through the code structure, I've included more refactoring changes in this patch. Some of the suggestions are orthogonal to this patch and I'd prefer to fix it in another JIRA, given the complexity of this patch. Maybe I can create a 'refactoring' JIRA after this one to cover some of these -\n\n2. Makes sense. I guess that was an over optimization.\n3. This is a good suggestion, al though would prefer keeping it to refactoring JIRA\n4. Picked descriptive names\n5. Somehow I like the idea of wrapping up enough logic inside Replica to figure out if it is a follower or leader. ReplicaFetcherThread inside Replica allows that. Al though, I'm not sure that is the best way to achieve it.\n6. Yeah, probably something to think about. Will move it to the refactoring JIRA\n7. I like Option 4 there, hoping that can be fixed in a separate JIRA\n8. Yeah, I moved some zookeeper client access to ReplicaManager so that all replication specific logic can be moved there.\n9. Changed configs to replication.*\n11. Simplified the ISR expiration. Looks better now.\n12. Hmm, Utils.newThread returns Thread, but I think it is useful to use some APIs specific to ReplicaFetcherThread like getIfFollowerAndLeader(). But I see your point here. Given a choice, it is always better to use a helper method. I set the daemon property and the thread handles all Throwables. \n13. Yeah, this is a good suggestion. This also fits in generic refactoring category that can be fixed separately.\n14. This is another great suggestion. Please see the included patch if you like it. \n15. Fixed it\n16. Fixed it\n17. Yeah, this will keep changing with the v3 code. Will be good to keep this in mind though.\n\nOverall, I liked your refactoring suggestions, and I might have been lazy to describe all of the changes I made here. Will really appreciate it if you can read through the new patch and suggest improvements. I'm fine with working through more in this patch itself, if you feel that works better. ", "Filed KAFKA-350 for improving the high watermark maintenance\nFiled KAFKA-351 to cover the refactoring suggestions. \n\nNow, we need some serious system testing for all this code ! :-)", "Thanks for patch v2. To help people review the code, I summarized the logic of handling the produce and fetch request on the server in this wiki: https://cwiki.apache.org/confluence/display/KAFKA/Handling+Produce+and+Fetch+Requests+in+KafkaApi\n\nSome new comments:\n21. ISRExpirationThread: It seems that this class is no longer used. Let's remove it.\n\n22. Partition.getOutOfSyncReplicas(): The first condition doesn't seem to implement what's in the comment. It doesn't check the leader's leo update time. Also, the condition specified in the comment doesn't seem sufficient. Suppose that the leader gets 100 bytes of more data, after which no more data is coming. A follower gets the first 50 bytes and then stopped. The follower's leo has been updated after the leader's leo was last updated. However, we still need to take the follower out of ISR. How about changing the condition to: select replicas whose leo is less than the leo of leader and whose leo hasn't been updated for keepInsyncTime. \n\n23. ReplicaManager:\n23.1 makeLeader(): remove comment \"also add this partition to the ISR expiration priority queue\"\n23.2 makeFollower(): If a follower switches leader, we should stop the old FetchThread before starting the new one.\n\n24. Replica.leoUpdateTime(): use logEndOffsetUpdateTime to be consistent.\n\n25. KafkaConfig: Let's keep the variable name and property name consistent. If we choose to use replication as the prefix for property name, use the same prefix for variable names.\n\n26. KafkaServer: To be consistent, we should probably name becomeLeader and becomeFollower as makeLeader and makeFollower, respectively.\n\n27. Log.recoverUptoLastCheckpointedHW(): not sure if comment 16.2 is addressed. Removed segments are not physically deleted.\n\n28. ISRExpirationTest:\n28.1 testISRExpirationForSlowFollowers(): the comment says set leo of remote replica to sth like 2, but the code set it to 4.\n28.2 testISRExpirationForStuckFollowers() and testISRExpirationForSlowFollowers(): is Thread.sleep() really needed? testISRExpirationForMultiplePartitions() didn't seem to use Thread.sleep().\n\n29. PrimitiveApiTest.testConsumerNotExistTopic(): I think this test is just to make sure that the client can get the error code on a non-existing topic.\n\n30. TestUtils:follower.socket.timeout.ms is now renamed to replication.socket.timeout.ms\n", "Updated patch to address Jun's suggestions -\n\n1. Fixed the ISR expiration for stuck followers case\n2. HW maintenance work is postponed to KAFKA-350\n3. System test (KAFKA-341), that tests message replication without failures, works on this patch\n\nMore detailed comments -\n\n21. Removed it\n\n22. Partition.getOutOfSyncReplicas(): Good catch ! Fixed the logic and added another test case for this.\n\n23. ReplicaManager: Done\n\n24. Replica: Changed the name to logEndOffsetUpdateTime()\n\n25. KafkaConfig: Changed the variable and config names to replica.*\n\n26. KafkaServer: Well, become* makes sense on the entity that is changing its state (Replica), make*, I thought made sense on the actor (KafkaServer). But that is just a matter of personal taste :)\n\n27. There are some nitty gritty details about HW maintenance that I would like to fix as part of KAFKA-350\n\n28. ISRExpirationTest: Done\n\n29. PrimitiveApiTest.testConsumerNotExistTopic(): I think the right fix is to throw a descriptive exception is UnknownTopicException when a client makes a produce/consume request for a topic that has never been created. Filed KAFKA-351 to fix it.\n\n30. TestUtils:Fixed it", "+ 1 from me for patch v3. Let's see if there are more comments from others.\n\nFor 27, it's ok to resolve this in kafka-350. Could you update the jira so that we remember all the changes that need to be made? Ditto for kafka-351.", "Attaching an updated patch that includes the rebase changes from KAFKA-348\n\nAlso, updated the follow up JIRAs - KAFKA-350 and KAFKA-351", "+1 for patch v4."], "derived": {"summary": "We need to implement the commit thread at the leader and the fetcher thread at the follower for replication the data from the leader.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Commit thread, ReplicaFetcherThread for intra-cluster replication - We need to implement the commit thread at the leader and the fetcher thread at the follower for replication the data from the leader."}, {"q": "What updates or decisions were made in the discussion?", "a": "+1 for patch v4."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-47", "title": "Create topic support and new ZK data structures for intra-cluster replication", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": null, "labels": [], "created": "2011-07-19T21:32:20.000+0000", "updated": "2017-09-07T18:20:54.000+0000", "description": "We need the DDL syntax for creating new topics. May need to use things like javaCC. Also, we need to register new data structures in ZK accordingly.", "comments": ["I recommend we just make a command line tool rather than a formal ddl. That will be more sysadmin friendly and easier to script up.\n\nWe should also think through how this will interact with topic auto-creation which is a feature we are currently reliant on.", "Yes, I agree that we can start with just a command line tool. \n\nFor auto-creation of a topic, we can optionally enable the producer to automatically create a non-existing topic (with some preconfigured # partitions, replication factors, etc), using the same underlying logic of the command line tool.", "Some thoughts on the create/delete topic support.\n\n1. What if the create process dies in the middle of the creation?\nThe create process will create 1 ZK node like the following for each partition in the topic. \n/brokers/topics/[topic]/[partition_id]/replicas --> {replica_id : broker_id â¦}\n\nThis means that if the process fails in the middle, some of the partitions may not be created. Ideally, we should probably use the multi-row transaction support feature in ZK (ZOOKEEPER-965), which will be released in ZK 3.4. Since this should be a relatively rare event, for now, we can probably just do this as a best effort. If the create command fails in the middle, we can always delete the topic and create it gain.\n\n2. How to delete a topic?\nWe can simply delete all entries in /brokers/topics/[topic] in ZK. On receiving the delete event, each replica will delete its corresponding local log directory. \n\nA broker could be down when the delete command was issued. When the broker is restarted, it should check if a topic still exists in ZK. If not, it will delete the local log directory.\n\nA more subtle issue can happen when a topic is deleted and recreated while a broker is down. When this broker is restarted, it should recognize that its local log directory is out of date. It should delete everything in the local log directory and create a new one. One way to do this is to store a version id (could be just a timestamp) in /brokers/topics/[topic]. The same version id is also stored in a .version file in the local log directory when it's first created. By comparing the version id in the local log directory and in ZK, a broker can detect when a local log directory is out of date during startup.\n\n3. Where to store the log locally?\nCurrently, a log is stored in {log.dir}/topicname-x for partition x. Now, a partition can have multiple replicas. One possibility is {log.dir}/topicname/x-y for partition x and replica y.\n\n4. What about auto-creation?\nOne possibility is to add an option in the producer so that it can automatically create a topic if the topic doesn't exist in ZK yet.\n\n5. I'd like to break this jira into 2 parts. The first part just adds the create/delete command and the corresponding ZK path. The second part will change the local log directory, add the auto topic creation support, and simply route the produce/consume requests to the first replica of each partition. This can be a separate jira.\n", "1,2. The other approach would be to implement a zk structure representing some kind of queue of actions to be carried out for each node. I think the delete case may be a special case of needing to reliably issue a command to a broker. This could be a CREATE or DELETE command or some kind of other operational command (e.g. MIGRATE-PARTITION). I think this was why Kishore and co took the zk route for communciation for Helix, I think this is one of the problems they were trying to solve.\n\nThe other question is how this is initiated. Do we add some administrative APIs to the brokers or is the communication through zookeeper? Either is fine, but we should be consistent about how we do administrative actions. I recommend we brainstorm a list of admin type actions we might want and try to generalize something that will work for all them.\n\n3. As a matter of taste, I like what you propose for the directory structure, topic/x-y, better then what we currently do. This does mean needing to rename lots of files when there is a mastership change, though.\n\n4. One concern is that auto-create may be really slow if we are blocking on acknowledgement from all the brokers. Not sure if this is a major problem.", "1,2. It seems to me it's simpler to have all brokers read from a single ZK path that stores the source of true, instead of broadcasting messages to every broker. For the latter, one has to further worry about what if only part of but not all messages are posted.\n\nTo be consistent, I think all administrative commands simply create some data structures in ZK and should complete very quickly. Each broker watches those ZK paths and take actions accordingly.\n\n3. The directory name is only tied to replica id and won't change with a mastership change. The mastership info is recorded in ZK, not in directory  names.\n\n4.  Typically, auto-create should complete very quickly since it just writes a few data structures in ZK.", "Does how we store the logs locally still require changes in light of the modifications we've made to the protocol?  A replica for partition X is stored at most once on a broker so with the current naming conventon we'll never run into conflicts.  Perhaps to be explicit that a certain directory is a replica, we could put them into {log.dir}/topicname/replica/partitionId but I don't think it's entirely necessary? ", "Prashanth, good question.\n\nYes, we could continue using the current log structure {log.dir}/topicname-partitionid. The only thing is that we would like to store some per topic metadata on disk, e.g., the version id (creation time) of a topic (to deal with some of the edge cases during topic re-creation). With the current structure, we either have to duplicate the topic metadata in each partition directory or deterministically pick one partition (like the smallest one) to store the metadata. Neither is ideal. It's much cleaner if we use the new structure {log.dir}/topicname/partitionid. Then the topic metadata can be stored under {log.dir}/topicname.", "Hmm, if you guys are considering changing something about the log structure might I request that you consider doing something to ease the pain when there are 1,000's or 10,000's topics? \n\nThe current structure doesn't work well since most filesystems tend to have problems when you store 20k or more directories in one directory.\n\nA hashing scheme is a good solution.  The tradeoff is that it is much more difficult to find the topic directory by hand.  A hash of even 10 top level directories would afford 10x more total topics (currently, the practical limit appears to hover around 20k, so 10x would give us 200k) -- this would probably be sufficient for my needs. ", "Just my two cents here. Hashing (even consistent) is a logical idea, the caveat being that it will require maintaining an upper bound on the number of topic \"buckets\" to avoid renaming and moving a bunch of large files around if we don't enforce such a limit.  To solve the other issue of not being able to quickly determine which bucket a topic falls into by-hand, we could maintain a file at the bucket-level that lists which topic belongs in which bucket, much like the metadata file Jun mentioned earlier.  It's extra overhead on the part of the system, I'm not familiar with a single broker requiring so many topics but it's certainly conceivable.", "By using {log.dir}/topicname/partitionid, we already reduce the # of directories from # total partitions to # total topics. We can think a bit more how to improve it further. This should probably be a separate jira.", "Closing this umbrella JIRA as all tasks are resolved."], "derived": {"summary": "We need the DDL syntax for creating new topics. May need to use things like javaCC.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Create topic support and new ZK data structures for intra-cluster replication - We need the DDL syntax for creating new topics. May need to use things like javaCC."}, {"q": "What updates or decisions were made in the discussion?", "a": "Closing this umbrella JIRA as all tasks are resolved."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-48", "title": "Implement optional \"long poll\" support in fetch request", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Jay Kreps", "labels": [], "created": "2011-07-19T21:32:21.000+0000", "updated": "2012-04-30T21:42:45.000+0000", "description": "Currently, the fetch request is non-blocking. If there is nothing on the broker for the consumer to retrieve, the broker simply returns an empty set to the consumer. This can be inefficient, if you want to ensure low-latency because you keep polling over and over. We should make a blocking version of the fetch request so that the fetch request is not returned until the broker has at least one message for the fetcher or some timeout passes.", "comments": ["This latency issue is important for replication because the latency for the producer will now depend on the replication (fetching) on the followers. This means our current polling mechanism is not going to be good, because we have to either back off for a period of time to avoid busy waiting on the server. In addition with replication we need a similar ability to process requests asynchronously--we do not want to block any threads while waiting for acks from followers. This also breaks our simple request/response model.\n\nThis is also important for the streaming use cases as they involve a large number of stacked topics, and hence the end-to-end latency is a multiple of the single-hop consumer latency.\n\nFixing this is slightly tricky.\n\nThe first thing I think we would need to do is move the execution of request handling out of the socket server threads. This would generally be a good thing to do anyway, as I/O currently blocks request handling for all sockets sharing a thread. This can add unnecessary latency.\n\nThe design for this could be a request BlockingQueue that the SocketServer submits all requests to, and N response BlockingQueues, one for each socket thread. The request processing would happen in a separate threadpool that would feed off the request queue and send responses back to the response queue. For asynchronous requests, no response need be enqueued.\n\nThe request handling would now be an ExecutorService with a fixed number of processes. Each process would poll the request queue, process any request it gets, and send back responses.\n\nLong poll requests from fetchers would either be handled immediately, or, if there is no available data, would add themselves to a list of watchers on the topic. When a request comes in on that topic, it would and responses for all watchers. The request would specify a max timeout after which the request would return empty, this could be implemented with a DelayQueue that was checked periodically for expired requests. A generalization of this would be to have the fetch request provide not only a max_wait_time but also a min_data_size, which would make the request block until the given number of bytes of data have accumulated. This would actually enable the opposite of simple long poll--instead of trying to minimize latency the fetcher would be able to ensure they got a good size chunk of data on each request to ensure good throughput and avoid lots of little requests each fetching only a few small messages.\n\nA similar mechanism would be possible for acknowledgements coming from followers. When a produce request occurs with a min_ack_count > 1, the request would go into a list of waiting requests for that topic/partition. When the ack request comes in from followers, we would check the waiting producers and add responses to the response queue for any newly unblocked request.\n\nI would like to do a round of refactoring on the SocketServer anyway, so let me know if anyone has comments on this before i go do anything too crazy. For example, I want someone else to validate the interaction with the replication design.\n", "Jay, thanks for carefully thinking ahead. I agree that we will need to decouple the socket processor thread from the handler thread, to make it easy for producers to wait for acks from followers, and for the consumers to block until new data is produced. We probably need 1 request queue and 1 response queue per socket processor thread. That way, we can ensure that the response is always handled by the socket thread that has registered the needed socket key for the response.", "This is a draft patch that refactors the socket server to make requests and responses asynchronous. No need for a detailed review, it still needs a lot of cleanup, but I wanted to show people the idea in more detail.", "Hi - please keep in mind the use case where a consumer is interested in more than one topic.\n\nThis feature if implemented only for one topic will not be useful for this use case - assuming it's infeasible to open multiple tcp connections.\n\nThe first proposal I have is to allow the request to contain a list of topics.  However, upon consideration, this would require the response to also be adjusted such that it would contain the name of the topic, otherwise it would be next to impossible to ascertain which topic the response corresponds to - well it could be done such that the response is returned in the same way as the request was requested, and for topics with no messages, an empty response is given, but this seems pretty bad from a network bandwidth standpoint.\n\nSo my final proposal would be to introduce an epoll like request/response.  The consumer would submit a request with a list of interested topics, and the response would be a topic and # of messages available on that topic when the topic(s) have messages.\n\nThe advantage to this solution is that it would be entirely backward compatible, since you would simply introduce a new request/response pair and it would also allow the consumer to decide which topics to poll (or pull) from first, so that it could prioritize, if it wanted.  \n\nFinally, I like the idea of allowing the consumer to specify a min # of messages required to trigger the poll, you might want to copy the pattern you already setup for log flushing, e.g. max time and/or min # of messages.  So the request might look like:\n\nlist-of :  topic-name:min msgs:max time\n\nand the response might be:\n\nlist-of : topic-name:# msgs available\n\n\n", "Yes, these are all good points. The work I have done so far just splits request processing into a separate thread pool and enables asynchronous handling. This is a fairly general thing we need for a few different use cases. Perhaps this should be broken into a separate JIRA.\n\nI have thought a little bit about how to do long poll, though. Logically what I want to do is make it possible to give a minimum byte size for the response and a maximum delay in ms; then have the server delay the response until we have at least min_bytes messages in the response OR we hit the maximum delay time. The goal is both to improve latency (by avoiding waiting in between poll requests), to reduce load on the server (by not polling), and to make it possible to improve throughput. If you set min_bytes = 0 or max_delay_ms = 0 you effectively get the current behavior. The throughput improvement comes if you set the min_bytes > 1; this would give a way to artificially increase the response size for requests to the topic (i.e. avoid fetching only a few messages at a time) while still giving hard latency guarantees. We have seen, the request size is one of the important things for network throughput.\n\nAs you say, the only case to really consider is the multi-fetch case. The single topic fetch can just be seen as a special case of this. I think your first proposal is closer to what I had in mind. Having the response contain an empty message set for the topics that have no data has very little overhead since it is just positionally indexed, so it is like 4 bytes or something. I don't like doing a poll() style interface that just returns ready topics doesn't seem very useful to me because the only logical thing you can do is then initiate a fetch on those topics, right? So might as well just send back the data and have a single request type to worry about?\n\nOne of the tricky questions for multifetch is what does the minimum byte size pertain to? A straight-forward implementation in the current system would be to add the min_bytes and timeout to the fetch request which would effectively bundle it up N times in the multi-fetch (currently multi-fetch is just N fetches glued together). This doesn't really make sense, though. Which of these minimum sizes would cause the single response to be sent? Would it be when all conditions were satisfied or when one was satisfied? I think the only thing that makes sense is to set these things at the request level. Ideally what I would like to do is remove the fetch request entirely because it is redundant and fix multi-fetch to have the following:\n   [(topic1, partitions1), (topic2, partitions2),...], max_total_size, max_wait_ms\nThis also fixes the weird thing in multifetch now where you have to specify the topic with each partition, so a request for 10 partitions on the same topic repeats the topic name 10 times. This is an invasive change, though, since it means request format changes.\n\nI am also not 100% sure how to implement the min_bytes parameter efficiently for multi-fetch. For the single fetch case it is pretty easy, the implementation would be to keep a sort of hybrid priority queue by timeout time (e.g. the unix timestamp at which we owe a response). When a fetch request came in we would try to service it immediately, and if we could meet its requirements we would immediately send a response. If we can't meet its min_bytes requirement then we would calculate the offset for that topic/partition at which the request would be unblocked (e.g. if the current offset is X and the min_bytes is M then the target size is X+M). We would insert new requests into this watchers list maintaining a sort by increasing target size. Each time a produce request is handled we would respond to all the watching requests whose target size is < then new offset, this would just require walking the list until we see a request with a target size greater than the current offset. All the newly unblocked requests would be added to the response queue. So this means the only work added to a produce request is the work of transferring newly unblocked requests to the response queue and at most we only need to examine one blocked request.\n\nThe timeout could be implemented by keeping a priority queue of requests based on the unix timestamp of the latest allowable response (i.e. the ts the request came in, plus the max_wait_ms). We could add a background thread to remove items from this as their timeout occurs, and add them to the response queue with an empty response.\n \nFor the multifetch case, things are harder to do efficiently. The timeouts can still work the same way. However the min_bytes is now over all the topics the request covers. The only way I can see to implement this is to keep a counter associated with each watcher, and have the watcher watch all the requested topics. But now on each produce request we need to increment ALL the watchers on the topic produced to.\n\nDunno, maybe for practical numbers of blocked requests (a few hundred? a thousand?) this doesn't matter. Or maybe there is a more clever approach. Ideas welcome.\n", "I can see how it would be reasonable to do the first approach.  It does limit one use case I was considering, which is to allow the consumer to decide in which order to fetch the topics after the poll is triggered, however, this can be done at request time when the topics are requested.\n\nAs you say, the response is 100% compatible, it's just the request that changes.  Therefore it would make sense I think to go ahead and make a new request type that doesn't yet exist and then the current fetch request remains the same on the wire and the behavior of it is just a degenerate case of this new use case with delay and bytes set to 0.\n\nI think you might consider how useful is it to worry about user specified time/bytes?  It will add a lot of complexity to your implementation, and frankly if I have just the ability to do a multi-fetch that will wait until something has arrived and send me whatever it has at the current moment that will be good enough.  A minimum implementation should also probably provide a simple timeout that will respond with nothing if the timeout expires.\n\nI think the simple implementation by itself a huge win and you might consider -- is that good enough?\n\nFor me it is - I would prefer to get the simple thing in the short term and wait for the harder thing in the long-term.", "Hi Taylor,\n\nCould you give a little more detail on your use case for ordering the fetches? I think you have a use case I haven't thought of, but I don't know if I understand it. Is your motivation some kind of quality of service over the topics?\n\nAs you say, this would definitely be a new request type for compatibility, and we would probably try to deprecate the old format over the next few releases as we can get clients updated.\n\nYour point about complexity is valid. I think for our usage since we use kafka very heavily the pain of grandfathering in new APIs is the hardest part, and the socket server refactoring is next, so I was thinking the difficulty of implementing a few internal data structures is not too bad. I suppose it depends on if I work out a concrete plan there or not. If the best we can do is iterate over the full set of watchers it may not be worth it.", "Actually, I don't have a valid use case for priority fetches, I was just thinking ahead.\n\nI agree that it's painful to have message format upgrades.  On the flip side of course we probably also agree it's bad to have parameters in the message header that don't correspond to real features.  \n\nCan you make a trade-off and reserve some bytes for these two int (or long) parameters and/or a few others but just call the space reserved?", "Just had a chance to look at the patch. Agree in principle this would work. It's probably better to create a separate jira for moving the requesthandler out of socket server. The long poll jira will depend on that jira.", "Cool, moved it.", "I've been staring at the code for a while - and I'm not sure I understand why you need KAFKA-202 to implement this feature.\n\nWhat I am thinking to do is:\n1) Every thread has to open a local socket for read/write\n2) Each thread puts the socket into the poll set for reading\n3) If a read request fails to read any messages, when it comes back to the handler, the handler adds a callback method to the appropriate log and puts the read request into a special queue.  When that log gets messages for write, it calls the callback.  The callback writes a byte into the special thread socket.\n4) The byte wakes up the thread, which sees that the special socket had a byte written to it, and so it goes and re-handles the read requests in the special queue as if they had just come in from the network. Thus if there are any messages available in the log for a given request, they are read just like normal and transferred out onto the channel.  If not, they're re-queued as per step 3.\n\nI think there is some pieces I haven't quite got right - in particular, I think there can only be one active response at a time.  Thus there will have to be some sort of response queue built up as each request generates a response, but I think that's simple - the handler just writes responses with non-zero messages into a response queue and the write logic of the socketserver is updated to drain this queue on write events (at the moment, it only deals with one response at a time, but now it may have many to send out queued up).\n\nSome other work that is probably going to be more difficult is that the binary protocol has to change to include the topic name or else there is no way to disambiguate the responses coming back.", "Taylor,\n\nSorry for the late response. I am not sure that I understand your proposal. \n\na. Why do we need a local socket? It seems that the same thing can be achieved by just turning on the write_interesting bit in the socket key corresponding to a client request.\n\nb. It's not clear to me how you correlate a queued client request with the corresponding client socket.", "This is a very rough draft of long poll support. It appears to work. Here are some remaining issues:\n1. I need the updated request objects to properly get the new fields (min_bytes, max_wait). Currently I am just hard-coding some made-up values.\n2. This patch is very specific to long poll support for fetch requests, it will require more generalization to support our other async case, namely delaying produce requests until a certain number of slaves are caught up.\n3. There are still some unit test problems.\n4. Code is a little rough still.\n\nTake a look if interested, I will discuss with a few people and clean up a little more before asking for a real review.", "Jay - that's great to hear!! Would you mind summarizing the way that the long-poll works?  I know that several different implementations were suggested here on the thread and I wanted to know which one you ultimately decided to go with.", "Hey Taylor, here are the nitty gritty details:\n- When a fetch request comes in we immediately check if we have sufficient data to satisfy it\n   - if so we respond immediately\n   - If not we add a \"watch\" on the topics that the fetch is for, and add it to a delay queue to expire it after the given timeout\n   - There is a background thread that checks the delay queue for expired requests and responds to them with whatever data is available\n- When a produce request comes in we update the watchers for all the topics it produces to, and increment their byte count. Any requests that have been satisfied by this produce, are then executed and responses are sent.\n\nSo one of the earlier questions was how to support polling on a very large number of topics AND wants very low latency, I think as you described it would be possible to implement this by simply multiplexing the requests on the single socket and letting the server respond to these as possible.", "Two other issues with this patch, I forgot to mention:\n- There is a race condition between checking the available bytes, and adding the watchers for the topics. I *think* this is okay since the min_bytes is a minimum not a maximum, so in the rare case that a produce comes in before the watchers are added we will just wait slightly longer than we should have. I think this is probably better than properly synchronizing and locking out all produces on that partition.\n- The other issues is that the delay queue is only emptied right now when the delay expires. If the request is fulfilled before the delay expires, the request is marked completed, but it remains in the delay queue until it expires. This is a problem and needs to be fixed. The problem is that if the client sets a low min_bytes and a high max_wait these requests may accumulate. Currently we would have to do an O(N) walk of the waiting requests to fix this. I am going to try to come up with an improved set of data structures to fix this without requiring that.", "Overall, the patch looks good. Some comments:\n\n1. DelayedItem.compareTo: yourEnd should be delayed.createdMs + delayed.delayMs\n2. Suppose that a client issues MultiFetch requests on a hot topic and a cold topic. What can happen is that the watcher list for the cold topic won't be cleaned up for a long time. One solution is to have a cleaner thread that periodically wakes up to remove satisfied items. The cleaner thread can be used to clean up the DelayQueue too.\n3. MessageSetSend.empty is not used.\n", "This version of the patch updates the code to work with the new request objects and correctly respect the min_bytes and max_fetch_wait settings.\n\nPlease review the new configs and make sure we are happy with the naming.", "Oops, missing about a bazillion files on that last patch.", "Thanks for patch v3. Some comments:\n\n31. DelayedFetch is keyed off topic. It should be keyed off (topic, partition) since a consumer may be interested in only a subset of partitions within a topic.\n\n32. KafkaApis: The following 3 lines are duplicated in 2 places.\n      val topicData = readMessageSets(delayed.fetch.offsetInfo)\n      val response = new FetchResponse(FetchRequest.CurrentVersion, delayed.fetch.correlationId, topicData)\n      requestChannel.sendResponse(new RequestChannel.Response(delayed.request, new FetchResponseSend(response, ErrorMapping.NoError), -1))\nShould we put them in a private method and share the code?\n\n33. ExpiredRequestReaper.purgeExpired(): We need to decrement unsatisfied count here.\n\n34. FetchRequest: Can we have the default constants for correlationId, clientid, etc defined and shared btw the constructor and the request builder?\n\n35. MessageSetSend.empty is unused. Should we remove it?", "Jun, thanks for the feedback. This patch hopefully addresses your comments:\n1. I removed the empty flag, as you suggested from MessageSetSend\n2. I would like to leave the ugly duplicate code for now. Making a seperate method for this doesn't really make sense as it isn't really a stand alone piece of code. I think the root problem is that action you do when the request is satisfied can be done either synchronously (if possible), asynchronously when the criteria are satisfied, or asychronously when the request expires. I think the right way to do this is to refactor RequestPurgatory a bit and somehow always use the same callback for all three cases. I would like to address this as a seperate patch because this idea is not fully baked yet.\n3. The default values are now shared between the builder and constructor.\n4. I changed the key to be (topic, partition) for FetchRequestPurgatory. That was a major oversite.\n5. The purgeExpired method is actually misnamed it is really purgeSatisfied, so it doesn't need to decrement the satisfied count. However there is a major bug in that count, it wasn't getting decremented by the processing thread. I added a new method to cover this.", "I attached a diff that just shows the changes between v3 and v4 for folks who already looked at v3.", "Patch v4 looks good. Just one more comment.\n\n41. RequestPurgatory.update(): if(w == null), could we return a singleton empty array, instead of creating a new one every time?", "Good point Jun, now it is\n    if(w == null)\n      Seq.empty\n    else\n      w.collectSatisfiedRequests(request)\n\nI will wait for more feedback before making a new patch since this is a pretty trivial change.", "This patch looks very good. Here are a few questions - \n\n1. I like the way the expired requests are handled by implementing the logic inside the FetchRequestPurgatory. However, can we not do the same for satisfied requests by providing a satisfy() abstract API in RequestPurgatory ? That gets rid of the handling of fetch requests inside handleProducerRequest() in KafkaApis, which is a little awkward to read. When we have the ProduceRequestPurgatory, the same satisfy() operation can send responses for produce requests once the fetch responses for the followers come in. \n\n2. I gave the RequestPurgatory data structure some thought. Not sure if this buys us anything over the current data structure. How about the following data structure for the RequestPurgatory - \n\n2.1. The watchers would be a priority heap (PriorityQueue), with the head being the DelayedItem with the least delay value (earliest expiration time). So for each (topic, partition), we have a PQ of watchers. \n\n2.2. The expiration data structure is another PQ of size n, where n is the number of keys in RequestPurgatory. This expiration PQ has the heads of each of the watcher lists above. \n\n2.3. The expiration thread will await on a condition variable with a timeout = delay of the head of the expiration PQ. The condition also gets signaled whenever the head of any of the n watcher list changes. \n\n2.4. When the expiration thread gets signaled, it removes its head element, expires it if its ready, ignores if its satisfied, and adds an element from the watch list it came from. It keeps doing this until its head has expiration time in the future. Then it goes back to awaiting on the condition variable. \n\n2.5. The item to be expired gets removed from its watch list as well as expiration PQ in O(1). \n\n2.6. The item that gets satisfied sets a flag and gets removed from its watcher list. If the satisfied item is the head of the watcher list, the expiration thread gets signaled to add new head to its PQ. \n\n2.7 Pros \n2.7.1. The watcher list doesn't maintain expired items, so doesn't need state-keeping for liveCount and maybePurge() \n2.7.2. During a watch operation, items only enter the expiration PQ if they are the head of the watcher list \n2.7.3. The expiration thread does a more informed get operation, instead of polling the queue in a loop. \n\n2.8. Cons \n2.8.1. watch operation is O(logn) where n is the number of DelayedItems for a key \n2.8.2 The forcePurge() operation on the expiration data structure still needs to happen in O(n) \n\nDid I miss something here ? Thoughts ? \n\n3. On the other hand, this is a huge non-trivial patch and you must be pretty tired of rebasing and working through unit tests. We could just discuss the above changes, and maybe file another JIRA to track it, instead of delaying this patch further. But that is your call.", "Hey Neha, yes, my hope is to get the patch evaluated as is, and then take another pass at cleaning up the way we handle the satisfaction action as Jun and you requested and try out other approaches to the purgatory data structure asynchronously. That should take these cleanup/polishing items out of the critical path.\n\nI like your idea of the dual priority queues, but I need to work through it more to fully understand it.", "+1 on the patch. I have a few minor comments:\n\nKafkaRequestHandlers :\n- requestLogger unused.\n\nConsumerConfig:\n- maxFetchWait -> rename the prop to max.fetch.wait.ms and the val to\n  maxFetchWaitMs\n- Can we get rid of fetcherBackoffMs? It says it is deprecated, but had a\n  reference in FetcherRunnable which you removed.\n- May want to have an explicit constraint that consumerTimeoutMs <=\n  maxFetchWait\n\nRequestPurgatory:\n\n- Unused import.\n- The parameterized types and overall tricky nature of this component make\n  it somewhat difficult to follow. I (think) I understood it only after\n  looking at its usage in KafkaApis, so the comments and javadocs (including\n  class' summary on top) can only go so far.  Even so, I think the comments\n  seem slightly out of sync with the code and can be improved a bit. E.g.,\n  what is \"given size\" in the update method's comment? current keys in the\n  comment for watch == the given request's keys. and so on.\n- Also, it may be easier to follow if we do some renaming, but it's a matter\n  of taste and I may have misunderstood the code to begin with:\n  - I find it confusing that there's a map called watchers which is a map\n    from keys to Watcher objects, and the Watcher class itself has a\n    linked-list of delayed requests called watchers. May be unwieldy, but\n    how about renaming:\n    - RequestPurgatory.watchers to watchedRequestsForKey\n    - Watchers to WatchedRequests\n    - Watchers.watchers to requests\n  - Rename DelayedRequest.satisfied to satisfiedOrExpired (I find it weird\n    that the reaper marks expired requests as satisfied.)\n  - update -> maybeNotify?\n- In collectSatisfiedRequests, the comment on \"another thread has satisfied\n  this request\". That can only be the ExpiredRequestReaper thread right?\n- It is slightly odd that we have to call the reaper's satisfyRequest method\n  from Watcher. Would it work to move the unsatisfied counter up to\n  RequestPurgatory?\n", "Joel, this is great feedback. I will address these issues in the commit since most are naming/documentation related.", "Included most of Joel's comments, and fixed a few lagging unit tests (in particular refactored AutoOffsetResetTest).\n\nComments on the general structure of request purgatory I am going to put off until we have our second use case ready to implement--the producer acks. When we have that I am going to look at refactoring so that the \"satisfaction action\" is a function included with the DelayedRequest which is executed regardless of whether the request is satsified or times out. But I want to put this off until we can check it against the specifics of the second use case."], "derived": {"summary": "Currently, the fetch request is non-blocking. If there is nothing on the broker for the consumer to retrieve, the broker simply returns an empty set to the consumer.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Implement optional \"long poll\" support in fetch request - Currently, the fetch request is non-blocking. If there is nothing on the broker for the consumer to retrieve, the broker simply returns an empty set to the consumer."}, {"q": "What updates or decisions were made in the discussion?", "a": "Included most of Joel's comments, and fixed a few lagging unit tests (in particular refactored AutoOffsetResetTest).\n\nComments on the general structure of request purgatory I am going to put off until we have our second use case ready to implement--the producer acks. When we have that I am going to look at refactoring so that the \"satisfaction action\" is a function included with the DelayedRequest which is executed regardless of whether the request is satsified or times out. But I want to put this off until we can check it against the specifics of the second use case."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-49", "title": "Add acknowledgement to the produce request.", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Prashanth Menon", "labels": [], "created": "2011-07-19T21:32:21.000+0000", "updated": "2013-05-02T02:29:54.000+0000", "description": "Currently, the produce request doesn't get acknowledged. We need to have a broker send a response to the producer and have the producer wait for the response before sending the next request.", "comments": ["If there are any other producer-related changes coming we should try to batch these together at the same time to avoid having to update clients too often.", "So I've started a little work on this.  Looks to me like the ProducerRequest is going to need an additional \"acknowledge\" boolean field (default false) which we send along with the rest of the fields.  On the producer side, there are a couple of options:\n\n1. We can leave the producer API and have producers (both sync and async) acknowledge all or non of their messages.  This behaviour will be driven by a configuration field or a class parameter.\n2. We add the acknowledgement parameter (default false) to the producer send API for and leave the remaining behaviour the same.  The producer then handles waiting or not waiting for acks on a per-message case.\n \nI would prefer the second option as it's simple to do and gives the option to clients.  Waiting on the producing end shouldn't be an issue.  On the broker side, it becomes easy as we just send a boolean response after handling the request (ditto for the multi-produce request).  \n\nDid anyone else have any thoughts on this?", "Prshanth, thanks for getting started on this. I agree with your second approach. Basically, add a new parameter in SyncProducer.send/multisend to indicate whether an ack is needed or not. The high level producer can then set that parameter based on ProducerConfig (probably true for sync mode and false for async mode). \n\nAnother question is what kind of ack does the broker send back. A simple approach is to send back a boolean. Another possibility is to return for each partition in the produce request, the latest offset after the request is served. Some clients could potentially make use of the returned offset.", "I second returning the new offset.", "+1 for returning the offset\n\nIf we are changing the request format it would also be good to think it through in some detail to get it right since these kinds of API changes are harder to rollout then to make. Some questions:\n1. Are we trying to maintain compatibility for this change? If so we should bump up the request id number and ignore the new fields for the old request id. This is not too hard, but requires a little extra work.\n2. Currently we have ProduceRequest and MultiProducerRequest and FetchRequest and MultiFetchRequest. I recommend we rename MultiProducerRequest to ProduceRequest and delete the existing ProduceRequest. The current ProduceRequest has no advantages over MultiProducerRequest and having both means each change we make has to be done for both. The two variations are just there for historical reasons--originally there was no multi-* version of the requests and we added that later. I recommend we do the same for the FetchRequest/MultiFetchRequest. This will make our lives simpler going forward.\n3. Both of the MultiProducerRequest has the format [(topic, partition, messages), (topic, partition, messages), ...]. This is because it is just a bunch of repeated ProducerRequests. This is really inefficient, though, as a common case is that we are producing a bunch of messages for different partitions under the same topic (i.e. if we are doing the key-based partitioning). It would be better for the format to be [(topic, [(partition, messages), ...], topic, [(partition, messages), ...], ...]. This would mean that each topic is only given once.\n\nI would like to get a quick consensus on the desired format of the produce and fetch requests up front, then we can break this into appropriate sub tasks so we don't expand the scope of Prasanth's work too much.", "Both 2 and 3 make sense. \n\nIf we do this in a bug fix release in 0.7, we probably need to maintain backward compatibility. If we do this as part of the replication work, we probably can make a non-backward compatible change. My preference is the latter.", "Generally I think it's a good idea to have a version embedded into the protocol.  This allows for backwards and forwards compatibility.  In a sense, the request id works as such, so in some sense it's a matter of semantics, but the only way to identify that there are multiple versions of the same request is to have some kind of external mapping that says id 2 and id 8 are really the same request, just different versions.\n\nOTOH, if you use a version, you can then have:\n\nid 2 version 1\nid 2 veriosn 2\n\netc. and this is imho easier to manage.\n\nUsually, the version should in fact be the first value in the protocol, so that you never have formatting issues that lie outside the realm of the versioned data.\n\nCurrently, all requests are preceded by a header, which contains the length of the data.  This is where I would start, we should either strive for:\n\nversion: 2 bytes\nlength: 4 bytes\n\nor \n\nlength: 4 bytes\nversion: 2 bytes\n\nNote that the message request already has a way to represent versions, using the magic field, but honestly I find it a little bit non explicit for my taste.\n\nI would also include a 64-bit \"flags\" area that will allow for future flags to be set to indicate various things.\n\nSo, if I were to suggest a standard header for requests and responses it would look like:\n\nlength: 2 bytes\nversion: 2 bytes\nreuest id: 2 bytes\nflags: 4 bytes\npayload: n bytes\n", "It could be possible to split the current request id - 2 bytes - into a version and an id field one byte long each.   Assuming there's not much need for a vocabulary greater than 256 verbs, and versions > 256, this would probably work within the current binary protocol and give backwards compatibility to 0.6 and 0.7 clients...", "Hi Taylor, as you say the request id was meant to be the version. However in retrospect I do think I like the idea of separating the request and the version of the request. I agree it would be nice to split this.\n\nI think the open question here is whether we should try to maintain backwards compatibility for the next major release. It would probably be very convenient for us at code-writing time not to, but is more painful at rollout time.", "Well, how many versions do you think you want?  Maybe we could split the request field up into say the first 5 or 6 bits instead of 8 for the versions.", "So guys, my thought is the best plan of attack would be fully think through the protocol changes for all the use cases we currently know about and do those all at once (even if the features the new fields support aren't yet available). This will avoid doing this in lots of little patches that all conflict, and it will make us think things through holistically.\n\nI created a wiki with some of the outstanding ideas, I am going to move this discussion to the main dev and user lists to get broader feedback. It would be good if people could give their thoughts there so we can try to get these changes right.", "KAFKA-240 implements the new wire format for producer and consumer. Since this JIRA requires the new format, it depends on KAFKA-240", "KAFKA-239 is complete and KAFKA-240 is almost there. \n\nPrashanth, in your comment above, you've mentioned you've started work on this. If so, mind assigning this JIRA to yourself ? This looks like the next JIRA to work on after KAFKA-240 is in. (https://issues.apache.org/jira/browse/KAFKA-50?focusedCommentId=13180712&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13180712)", "Sure, but I actually can't change assignments.  Could be a privileges issue?", "Looks like you didn't exist in the Apache Kafka JIRA list. Added you there, also assigned this JIRA to you", "Hey all.  Getting the acknowledgements done was fairly straightforward - I've \"todo-ed\" actually waiting for replica acknowledgements as part of kafka-44 when it introduces true replicas and partitions.  Having the sync producer wait for a response when it requires acknowledgements was trivial.  My question is what to do with the async producer.  Was the intent to perform some logic in the default event handler, or to expect clients to write custom event handlers that deal errors?  Should we bubble up the response to the producer level?", "Prashanth,\n\nThat's a good question. Initially, I was just thinking that for async producers, if we get an error during send (after retries), we will just log the error without telling the client. Currently, the event handler is not really extensible. I can image that we add some kind of callback to return those errors. The question is what will the client do on those errors. Will it resend? If so, we will need to pass the failed requests through callback too. I am curious about how other messaging systems like activeMQ do in the async mode.", "If I remember right, HornetQ allows  you to implement a callback interface to be notified of message acknowledgments.  We could do something similar, passing back the request and response for the erroneous parts.  To your second point, I suppose it depends on the error.  Some may be logged and skipped, some will require a refresh of topic metadata .  The default behaviour should cover the base cases.\n\nCurious to hear other thoughts", "The current defaulteventhandler already refreshes topic metadata on retries. So, if we return any failed request to the client, there is probably not much the client can do, except for logging it. In any case, we should use a separate jira to track if we need any aysnc callback on the producer side.", "Sounds good to me.  Doing some additional work on DefaultEventHandler, I noticed something off in the retry logic that I'd like to get confirmed.  Consider the case where I've got data destined for more than one broker, say three.  \n- Enter handleSerializedData()\n  - Partioning and collating makes a map with three key/value pairs (broker -> topic partition data and messages).  \n  - Enter for loop\n    - Assume the first send works on the first try for broker #1.  \n    - Next iteration, the second send to broker #2 fails on the first try, we fall into the retry loop and recursive into handleSerializedData with requiredRetries = 0.\n      - In handleSerializedData\n      - This time, the partitioned data will one one key/value pair for the single broker (broker #2) we're attempting to resend data to.  \n      - Enter for loop\n        - Attempt to send data to broker #2, the send succeeds\n      - We exhaust the map entries and the for loop condition.\n    - We return to the retry loop for retry=1 on broker #2 in the catch block.  \n    - The previous send succeeded on first try and now there's the \"return\" statement.  This exists the function, but we have one more broker (broker #3) to send data to.\n\nDoes the flow sound about right?  I think what needs to happen is to set a flag and break the retry while loop after a successful retry.  Then we check the flag after the loop and either throw the exception or continue the outer for loop.\n\nAm I crazy?  Am I missing something in my sleep-deprived state here?", "Prashanth,\n\nYes, that's actually a real bug. Instead of returning in retry, we should just set a boolean to indicate that a send has succeeded. At the end, we will throw FailedToSendMessageException if the boolean is not set. Otherwise, we will continue with the for loop. Could you file a separate jira to fix that? Thanks for catching the bug.", "Done, created KAFKA-295.  Expect patch for this jira later tonight.", "I've attached a patch for this.  A few comments:\n\n- Modified ProducerResponse\n- Broker does not actually wait for replica ACKS.  This will be done with KAFKA-44.\n- Sync producer has been modified to wait for response from broker.  Async producer isn't aware of request level errors, this will require a separate ticket.\n- Some general cleanup on producer request, async producer and removal of MultiProduce request key.\n\nOne oddity is since we use Arrays in the request and response, it breaks the case class equality/hashcode logic since Java's arrays are broken.  We should probably make them Seq's (separate JIRA?) or WrappedArrays.\n", "Prashanth,\n\nThanks for the patch. Overall, it looks pretty good. Some comments:\n1. KafkaApis: Even when the produce request requires no ack, we will still need to send error code back. So we should always send a ProduceResponse. When no ack is specified, we probably don't need to send offsets back.\n2. ProduceRequest.getNumMessages: rename it to something like getNumTopicPartitions\n3. AsyncProducerTeest.testDefaultHanlderRetryLogic: doesn't really test retry\n4. SyncProducerTest.testProduceBlocksWhenRequired: Use createTopic instead of creating ZK path directly.\n5. I agree that it's probably better to use Seq in our requests/response, instead of Array. Then we need a java version to convert Seq to java array and vice versa. Please open a separate jira to track this.\n", "Thanks for the pointers!  \n\n1.  Hmmm, do you propose returning an empty offsets array back to the client when no ack is required?  That seems perfectly reasonable since the broker can't make guarantees as to the offsets; but it  does feel somehow incongrous since one would expect the errors and offsets array sizes to be equal.  I'm fine with the idea as long as it's agreed in the wire format.  If I've completely missed the point, forgive me!\n2.  Done.\n3.  Done.  Wow, that wasn't supposed to be included.  It was part of my sanity check for the incorrect retry logic I mentioned earlier.\n4.  Done.\n5.  Done.", "1. Or we can just treat noacks the same as act=1 for now.", "A few more concerns popped up as a result of making the send in syncproducer blocking.  \n\n1. Edit: So it turns out that using the channel in SyncProducer like we are to perform reads won't trigger socket timeouts though we set it and will block forever which is bad news(check http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4614802 and http://stackoverflow.com/questions/2866557/timeout-for-socketchannel for workaround).  The latter post has a simple solution that involves creating a separate ReadableByteChannel instance for timeout-enabled reads.  The other option being using non-blocking sockets with selectors which is more complex.\n2.  It is conceivable that a broker listed in the replica set /brokers/topics/[topic]/partitions/[partition]/replicas is offline or shutdown which means their ephemeral entries are missing in ZK.  A problem then arises when an active broker attempts to pull metadata and node information for topics these brokers host since AdminUtils assumes any broker in AR or ISR must have paths/info in ZK /brokers/ids/[brokerId], but since they don't an NoNodeException is thrown.  A corner case for sure, but something that should probably be fixed.", "I've attached an updated patch.  \n\n1. ProduceRequest always receives a ProducerResponse.  If acks=0, offsets in the response are treated as if acks=1, meaning only the leader has acked.\n2. SyncProducer blocks waiting for a response from a ProducerRequest.\n3. I've commented out ProducerTest.testZKSendWithDeadBroker since it relies on SyncProducer logic that will need to change.  It also will need to be rewritten with the changes coming in as part of KAFKA-45 leader election logic.\n\nLet me know if I've missed anything!\n", "Prashanth,\n\nI am ready to commit your patch. A couple things:\n\n1. Your patch added a new class ResponseHandler, but it's not used. Should that be removed?\n2. Your concern #1 is valid. Could you create another jira to track this?\n3. For your concern #2, it's ok for getMetadataApi to return empty leader in a transition state. The client will simply backoff a bit and call getMetadataApi again.", "Hi Jun, I've attached a new patch.\n\n1. Yes, I've removed it.\n2. Done.\n3. You are correct in the case of leaders, but I believe the problem stands when pulling topic metadata with atleast one offline broker listed in the assigned replicas.\n\n", "Thanks for the patch Prashanth. Just committed to 0.7 branch.\n\nFor 3, if a broker is offline, then eventually it will not be in ISR. In the transition state, we could have an ISR broker without matching host and port.", "Sorry for visiting this late. I have a few questions about producer ACK. \n\n1. In DefaultEventHandler, the producer ACK is not used. Shouldn't it be used to figure out if the send operation needs to be retried ? \n2. In DefaultEventHandler, should the producer wait for ACK and timeout if it doesn't receive one ?\n3. In KafkaApis, why doesn't the broker send an error back to the producer if it received a producer request for a partition that is not hosted on that broker ?", "Better late than never.  A second review is always a plus!  To your points:\n\n1. Absolutely, this was overlooked.  Expect patch later tonight or tomorrow.\n2. The DefaultEventHandler does wait for the ack by waiting for the response.  Unfortunately, the current SyncProduer doesn't timeout correctly for which KAFKA-305 was created.\n3. KafkaApis does not explicitly do the check, instead relying on LogManager which currently does.  It makes sense to move that piece of logic along with the TODO from LogManager into KafkaApis for clarity and to separate ZK from LogManager.\n\nWhat do you think?", "1. I'm refactoring some part of DefaultEventHandler as part of KAFKA-300. I'll upload a patch tonight. You can either choose to work off of the changed code or not. Your call.\n2. Sounds good\n3. In handleProduceRequest, the logManager.append() throws InvalidPartitionException when it receives a request for a partition that it does not own. Does it make sense to send an ACK to the producer with an error code like  NotLeaderForPartitionException ?\n\n", "Reopening this issue to address some review suggestions and to fix KAFKA-305 as part of this JIRA", "Prashanth, thanks for resolving KAFKA-305 ! Would you be up for finishing up the remaining work on this ? It seems like a good idea to complete earlier JIRAs, before moving to the later ones.", "Okay, I've attached a patch that should take care of the outstanding items.  A couple of points:\n\n1. KafkaApis not checks whether a partition has a leader on the broker.  It uses KafkaZooKeeper to check this, for now, but should probably rely on ReplicaManager and the Replica itself to determine this.  KAFKA-46 should take care of this.\n2. I've removed the random partition check on the server-side since the partitioning is done in default event handler.  Producers should know which broker a partition belongs to.\n3. I've added a new NotLeaderForPartitionException and added it to ErrorMapping so clients can receive it.\n4. DefaultEventHandler.send now returns a list of topic/partition tuples that represents those messages that need to be resent due to an error.\n5. Due to the changes in KafkaApis produce check, some of the tests have been modified to ensure topics are in ZK and to wait for leadership.\n\nI think that covers all, please point out any issues!", "+1. Thanks for incorporating the review suggestions !", "Prashanth,\n\nPatch looks good: Just one minor thing.\n\n6. Unused imports: KafkaZookeeper\n\nWhile looking at the patch, I realized that there are a couple of other things that we will need to follow up.\n\na. In DefaultEventHandler, it seems that we rely on the fact that broker.id is a non-negative integer. However, we don't enforce that in broker startup.\nb. With the create topic ddl, some of the broker configs like topic.partition.count.map probably don't make sense anymore.\n\nI will create a jira for each item to follow up.", "Thanks for the review!  I've attached newest patch for #6 resolved.", "Prashanth,\n\nThanks for the patch. It seems that kafka-48 is almost ready. Since that's a relatively large patch, I will commit your patch after kafka-48 is committed.", "Prashanth,\n\nNow that kafka-48 is committed to 0.8, we can commit your patch. Since you are a committer now, could you commit this yourself?", "Sorry for the delay everyone.  I'm planning to block off some time this weekend to commit this patch, and hoping I don't run into any access/permissions issues :)", "Excellent, committed this to 0.8."], "derived": {"summary": "Currently, the produce request doesn't get acknowledged. We need to have a broker send a response to the producer and have the producer wait for the response before sending the next request.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add acknowledgement to the produce request. - Currently, the produce request doesn't get acknowledged. We need to have a broker send a response to the producer and have the producer wait for the response before sending the next request."}, {"q": "What updates or decisions were made in the discussion?", "a": "Excellent, committed this to 0.8."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-50", "title": "kafka intra-cluster replication support", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Jun Rao", "labels": [], "created": "2011-07-19T21:32:21.000+0000", "updated": "2013-06-13T15:34:25.000+0000", "description": "Currently, Kafka doesn't have replication. Each log segment is stored in a single broker. This limits both the availability and the durability of Kafka. If a broker goes down, all log segments stored on that broker become unavailable to consumers. If a broker dies permanently (e.g., disk failure), all unconsumed data on that node is lost forever. Our goal is to replicate every log segment to multiple broker nodes to improve both the availability and the durability. \n\nWe'd like to support the following in Kafka replication: \n\n1. Configurable synchronous and asynchronous replication \n2. Small unavailable window (e.g., less than 5 seconds) during broker failures \n3. Auto recovery when a failed broker rejoins \n4. Balanced load when a broker fails (i.e., the load on the failed broker is evenly spread among multiple surviving brokers)\n\nHere is a complete design proposal for Kafka replication - https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Replication", "comments": ["Sorry for coming late to this.\n\nI read the design document and it looked like it is quite a complex feature to implement and maintain going forward. The sheer amount of complexity in managing the replicas and partition just bothers me. I am wondering why can't we use HDFS which is hardened over a number of years. Obviously I may be missing some subtle things. Would be great if folks shed light on this.", "Sharad,\n\nGood question. HDFS is a great system and is the very first thing that we thought about when looking at Kafka replication. The pros are that (1) we can offload the replication complexity to another system and (2) HDFS can recover various data failure very effectively. Some of the cons are:\n\n1. HDFS only provides data redundancy, but not computational redundancy. If at any given point of time, there is only one broker that can serve the data, the availability is not going to be high. When a broker is down, we need to elect another broker to take over its data. Even though data doesn't have to be physically moved, this process may require a little a bit of recovery of each partition and may take some time to complete. In that window, some partitions become unavailable. Further, data logically moved to the new broker is initially cold.\n\n2. HDFS currently is not a highly available system. The namenode is a SPOF and you need something like Avatar namenode to make it HA. It's not clear when this feature is going to be generally available and used.\n\n3. Using HDFS brings in another complex system, it's not clear how easy it is to operate, especially for an online system.\n\n4. HDFS is not a true POSIX file system. The append/truncate support is relatively new. This may force us to redesign some of the things that currently require in-place update (e.g., during recovery).\n\n5. HDFS manages its data at block level. Kafka replication can manage data at the partition level (a partition can be 3 orders of magnitude bigger than a block). This means we can manage much less meta data and therefore, potentially have a simpler design and implementation.\n\n", "I must agree completely with Jun here.  The beauty of Kafka lies in it's simplicity.  To add another piece to the puzzle such as HDFS would break this and diminish the value.  I would even argue that if possible Kafka might consider removing Zookeeper as a dependency - or at least make it optional.  \n\nI would also add that it's not clear that HDFS would actually exhibit the write/read performance that Kafka achieves using NIO.  And because there is basically zero copy, Kafka's memory and cpu overhead is incredibly low for what it does.\n\nThese two main factors - it's incredible performance with very low overhead using commodity components are Kafka's strengths.  Adding HDFS would eliminate them.\n\nI would suggest that any replication strategy should focus on non-guaranteed delivery.  The Kafka clients already do not provide a guaranteed delivery mechanism, and as such Kafka should only be used in a setting where it's tolerable to have some amount of message loss during a failure event.  Minimizing this loss is a reasonable goal to achieve, but it should not compromise the simplicity and performance of Kafka in any way.", "To put it another way, in terms of what I want from replication - currently if I have a failure event I will currently lose the history of all of my messages.  I would like Kafka to preserve as much of those messages as possible in a failure event.  It's ok if not every message that appeared to be delivered was delivered.\n\nThis is a classic CAP tradeoff - does Kafka provide C or A?  I propose it continue to focus on A.", "> I would even argue that if possible Kafka might consider removing Zookeeper as a dependency - or at least make it optional. \n\nIt's already optional (enable.zookeeper=false), but you loose a lot if you disable it.  Taylor, maybe you could elaborate in the mailing list or another ticket what subset of functionality you would be willing to give up to not use ZK?", "We do plan to offer both async replication and sync replication. In async mode, the latency should still remain low since the client doesn't wait for the data to hit all replicas. However, a small amount data may not be replicated to the followers during a failure and will be lost. Sync mode gives you more or less the opposite. This could be useful for people who want to use Kafka as traditional messaging systems like ActiveMQ.\n\nThis is another difference with HDFS, which only has a sync replication mode.", "Thanks Jun for the comments.\n\n> HDFS only provides data redundancy, but not computational redundancy. \n\nIf data resides in HDFS, theoretically it can be served by any broker. The default could be being served from the broker which has hot data. (the data being written to)\n\n> The namenode is a SPOF \nTrue, but namenode going down doesn't let you loose the data, yes the cluster is not accessible for that period. However if kafka has acks and producer side spooling (which anyway should be there IMO for data durability), no data would be lost.\n\n> The append/truncate support is relatively new.\nIts been used by Hbase folks for quite sometime.\n\n> HDFS manages its data at block level. \nDoesn't really matter as users of hdfs care least about blocks. They have a file view of things.\n\n\nThat all said, I don't want to derail this work with any kind of debate here. I was just thinking to get production quality replication quickly. Look forward to having the replication in. Thanks!\n\n\n\n", "Hey Sharad, your comments are all correct. I think using HDFS would certainly require the least implementation effort and contains a mature replication system tested at large scale. The downside is that HDFS is fairly complex in its own right, and has a number of drawbacks for high-availability, low-latency cases (spof is one but not the only one). Also many use cases do not need replication, but supporting hdfs and local fs efficiently probably means two pretty different implementations. We felt that this kind of multi-subscriber log is a really important abstraction in its own right for systems of all kinds and so our thought was to just kind of suck it up and do the full implementation since we thought the end result would be better.", "make sense. Thanks Jay and Jun.", "The dependencies of the sub-jiras look like the following:\n\n48\n49\n47 <-- 46,44/45  <-- 43,42,41\n  \nThis means that initially, 47,48,49 can be worked on independently.", "Attach v2 of the detailed design doc. Made 2 minor changes:\n\n1. V1 design has 2 separate ZK paths for a broker, one registered and one alive. Simplify it to have just 1 ZK path for live broker. The implication is that if a topic is created while a broker is down, no partition will be assigned to that broker. Since topic creation is infrequent, this is likely not a big issue.\n\n2. Use broker id as replica id for each partition, instead of using an explicit replica id.\n", "Here is a breakdown of all the jiras and their dependencies:\n\n1. kafka-47: create/delete data structures in ZK, automatically create topic, and use them (making partitions logical, supporting only 1 replica, no failure support). ï»¿ï»¿(L1)\n  1.1 kafka-237: create/delete ZK path for a topic in an admin tool (L0)\n  1.2 kafka-238: add a getTopicMetaData method in broker and expose it to producer\n  1.3 kafka-239: Wire existing producer and consumer to use the new ZK data structure\n2. kafka-202: decouple request handler and socket sever; enabling long poll in the consumer (L1)\n3. kafka-240: implement new producer and consumer request format (L1) \n4. kafka-49: add ack to ProduceRequest (L2). depending on #3\n5. kafka-48: long poll in consumer (L2). depending on #2\n6. kafka-44: commit thread, replica fetcher thread (L3). depending on #1, #4, #5\n7. kafka-45: broker starting up, leader election (L3). depending on #1\n8. kafka-46: various ZK listeners (L3). depending on #1\n9. kafka-43: move master to preferred replica when possible (L4). optimization\n10. kafka-42: rebalance partition with new brokers (L4). extra feature\n", "Can the design be moved to the Kafka wiki instead of a non-editable pdf attached here ? It will make it much easier to discuss missing details in the current design.", "Yes, we can start a new wiki for each of the sub-jiras, if needed.", "Cool, so I was thinking if the original design was on a wiki too, it will be much easier to point to sections of it, to make discussions easier. If you have a text copy of it, would you mind pasting it in a Kafka JIRA page ? It would be very useful.", "Attach the word doc of the design.", "Moving the kafka replication design docs to a wiki. This includes both the high-level as well as the low level design details. The following changes are made on the wiki -\n\n1. The state machine will be maintained and changed only by the leader for a partition. The leader co-ordinates each state change by requesting the followers to act on state change requests. This ensures that we don't have a split-brain problem during state changes amongst the replicas for a partition.\n2. More details are added for the various algorithms\n", "I have updated the replication V3 design wiki: https://cwiki.apache.org/confluence/display/KAFKA/kafka+Detailed+Replication+Design+V3 by incorporating the content from V2. We plan to do the implementation based on the V3 design. If there are concerns, please add your commend to the design wiki.\n\nThe following is a list of open jiras, roughly in order of dependency and importance.\n\nPhase 2 (Basic message replication, with testing and tools and minimum fancy features)\nKAFKA-335: Embedded controller (1w)\n\nKAFKA-336: Admin RPC between controller and broker (1.5w)\n\nKAFKA-337: Upgrade ZKClient to allow conditional updates through ZK (0.5w)\n\nKAFKA-342: Broker startup (revisit based on v3.E design, 1.5w)\n\nKAFKA-343: Leader election, become leader, become follower (revisit based on v3. A,C; 2.5w)\n   Depends on KAFKA-301\n\nKAFKA-329: Create topic support (revisit based on v3 design, 1w)\n\nKAFKA-46:  Replica fetch, leader commit (v3.G design, 2.5w)\n  Depends on KAFKA-301\n  Depends on KAFKA-302\n\nKAFKA-338: controller failover (V3. D 2w)\n\nKAFKA-339: Multifetch for follower (1.5w)\n\nKAFKA-306: Fix broker failure test on 0.8 branch (1w)\n\nKAFKA-330: Delete topic support (1w)\n  Depends on KAFKA-301\n\nKAFKA-327 Monitoring and tooling support (2w)\n\nPhase 3 (System tests and more advanced features like preferred replica leadership transfer, online partition reassignment)\nKAFKA-174 Performance suite for Kafka (2w)\nKAFKA-42 Online partition reassignment (3w)\nKAFKA-43 Preferred replica leadership transfer (1w)\n"], "derived": {"summary": "Currently, Kafka doesn't have replication. Each log segment is stored in a single broker.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "kafka intra-cluster replication support - Currently, Kafka doesn't have replication. Each log segment is stored in a single broker."}, {"q": "What updates or decisions were made in the discussion?", "a": "I have updated the replication V3 design wiki: https://cwiki.apache.org/confluence/display/KAFKA/kafka+Detailed+Replication+Design+V3 by incorporating the content from V2. We plan to do the implementation based on the V3 design. If there are concerns, please add your commend to the design wiki.\n\nThe following is a list of open jiras, roughly in order of dependency and importance.\n\nPhase 2 (Basic message replication, with testing and tools and minimum fancy features)\nKAFKA-335: Embedded controller (1w)\n\nKAFKA-336: Admin RPC between controller and broker (1.5w)\n\nKAFKA-337: Upgrade ZKClient to allow conditional updates through ZK (0.5w)\n\nKAFKA-342: Broker startup (revisit based on v3.E design, 1.5w)\n\nKAFKA-343: Leader election, become leader, become follower (revisit based on v3. A,C; 2.5w)\n   Depends on KAFKA-301\n\nKAFKA-329: Create topic support (revisit based on v3 design, 1w)\n\nKAFKA-46:  Replica fetch, leader commit (v3.G design, 2.5w)\n  Depends on KAFKA-301\n  Depends on KAFKA-302\n\nKAFKA-338: controller failover (V3. D 2w)\n\nKAFKA-339: Multifetch for follower (1.5w)\n\nKAFKA-306: Fix broker failure test on 0.8 branch (1w)\n\nKAFKA-330: Delete topic support (1w)\n  Depends on KAFKA-301\n\nKAFKA-327 Monitoring and tooling support (2w)\n\nPhase 3 (System tests and more advanced features like preferred replica leadership transfer, online partition reassignment)\nKAFKA-174 Performance suite for Kafka (2w)\nKAFKA-42 Online partition reassignment (3w)\nKAFKA-43 Preferred replica leadership transfer (1w)"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-51", "title": "getOffsetsBefore() returns wrong offset when given a specific timestamp", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:23.000+0000", "updated": "2011-07-19T21:32:23.000+0000", "description": "When a specific timestamp is specified, getOffsetsBefore() always returns the current HW, which is incorrect.", "comments": [], "derived": {"summary": "When a specific timestamp is specified, getOffsetsBefore() always returns the current HW, which is incorrect.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "getOffsetsBefore() returns wrong offset when given a specific timestamp - When a specific timestamp is specified, getOffsetsBefore() always returns the current HW, which is incorrect."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-52", "title": "Consumer Code documentation", "status": "Resolved", "priority": "Minor", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:23.000+0000", "updated": "2011-07-19T21:32:23.000+0000", "description": "The example code for the \"Consumer Code\" section on http://sna-projects.com/kafka/quickstart.php seems to contain a couple of errors. \n\nHere's the working code: \n\n{code} \n// specify some consumer properties \nProperties props = new Properties(); \nprops.put(\"zk.connect\", \"localhost:2181\"); \nprops.put(\"zk.connectiontimeout.ms\", \"1000000\"); \nprops.put(\"groupid\", \"test_group\"); \n\n// Create the connection to the cluster \nConsumerConfig consumerConfig = new ConsumerConfig(props); \nConsumerConnector consumerConnector = Consumer.create(consumerConfig); \n\n// create 4 partitions of the stream for topic \"test\", to allow 4 threads to consume \nMap<String, List<KafkaMessageStream>> topicMessageStreams = \nconsumerConnector.createMessageStreams(ImmutableMap.of(\"test\", 4)); \n// create list of 4 threads to consume from each of the partitions \nList<KafkaMessageStream> streams = topicMessageStreams.get(\"test\"); \nExecutorService executor = Executors.newFixedThreadPool(4); \n\n// consume the messages in the threads \nfor (final KafkaMessageStream stream : streams) { \nexecutor.submit(new Runnable() { \n//final KafkaMessageStream stream = topicStream.getValue(); \npublic void run() { \nfor (Message message : stream) { \n// process message \n} \n} \n}); \n} \n{code} \n\nIt might also be worth specifying the imports: \n\n{code} \nimport kafka.consumer.*; \nimport kafka.message.Message; \n\nimport java.util.Properties; \nimport java.util.Map; \nimport java.util.List; \nimport java.util.concurrent.ExecutorService; \nimport java.util.concurrent.Executors; \n\nimport com.google.common.collect.ImmutableMap; \n{code}", "comments": [], "derived": {"summary": "The example code for the \"Consumer Code\" section on http://sna-projects. com/kafka/quickstart.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Consumer Code documentation - The example code for the \"Consumer Code\" section on http://sna-projects. com/kafka/quickstart."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-53", "title": "close() in SimpleConsumer should be synchronized", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:24.000+0000", "updated": "2011-07-19T21:32:24.000+0000", "description": "Similar to KAFKA-12, the close method in SimpleConsumer doesn't hold the lock while closing the channel and setting it to null, potentially creating a race condition if a message is being received.", "comments": [], "derived": {"summary": "Similar to KAFKA-12, the close method in SimpleConsumer doesn't hold the lock while closing the channel and setting it to null, potentially creating a race condition if a message is being received.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "close() in SimpleConsumer should be synchronized - Similar to KAFKA-12, the close method in SimpleConsumer doesn't hold the lock while closing the channel and setting it to null, potentially creating a race condition if a message is being received."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-54", "title": "Propagate server all exceptions to consumer", "status": "Closed", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:24.000+0000", "updated": "2011-07-19T21:32:24.000+0000", "description": "Currently, we only propagate a few known exceptions to the consumer. We should propagate all exceptions to the consumer.", "comments": [], "derived": {"summary": "Currently, we only propagate a few known exceptions to the consumer. We should propagate all exceptions to the consumer.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Propagate server all exceptions to consumer - Currently, we only propagate a few known exceptions to the consumer. We should propagate all exceptions to the consumer."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-55", "title": "shutdown kafka when there is any disk IO error", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:24.000+0000", "updated": "2011-07-19T21:32:24.000+0000", "description": "Currently, if we encounter any IO error while writing to a kafka log, we simply log the error and continue. However, this kind of errors could leave the log in a corrupted state (e.g., only part of a message is added to the log). When this happens, we should stop accepting new requests and force kafka to shutdown. Once kafka is restarted, log recovery can clean up any corrupted log.", "comments": [], "derived": {"summary": "Currently, if we encounter any IO error while writing to a kafka log, we simply log the error and continue. However, this kind of errors could leave the log in a corrupted state (e.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "shutdown kafka when there is any disk IO error - Currently, if we encounter any IO error while writing to a kafka log, we simply log the error and continue. However, this kind of errors could leave the log in a corrupted state (e."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-56", "title": "Speed up recovery", "status": "Closed", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:24.000+0000", "updated": "2011-07-19T21:32:24.000+0000", "description": "Currently we run recovery on the last segment of each log. This ensures that the log is correct, but requires reading the complete segment which can be slow. \n\nOne approach would be to record graceful shutdown using some .no_recovery_needed file, and on the next startup remove this file and open logs without running recovery. This will speed up the common case of a graceful recovery. \n\nThere may be some approach that writes known-valid offsets to a checkpoint file and allows recovery to proceed from there.", "comments": [], "derived": {"summary": "Currently we run recovery on the last segment of each log. This ensures that the log is correct, but requires reading the complete segment which can be slow.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "Speed up recovery - Currently we run recovery on the last segment of each log. This ensures that the log is correct, but requires reading the complete segment which can be slow."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-57", "title": "Add an optional acknowledgement in the producer", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:25.000+0000", "updated": "2015-02-07T23:49:57.000+0000", "description": "Currently the producer will flush the socket buffer but does not wait for an answer. This is good for tracking-like use cases but bad for queue-like cases. We should add an optional acknowledgement to the protocol and have the producer await this.", "comments": [], "derived": {"summary": "Currently the producer will flush the socket buffer but does not wait for an answer. This is good for tracking-like use cases but bad for queue-like cases.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add an optional acknowledgement in the producer - Currently the producer will flush the socket buffer but does not wait for an answer. This is good for tracking-like use cases but bad for queue-like cases."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-58", "title": "Combine fetch/multifetch and send/multisend", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:25.000+0000", "updated": "2015-02-07T23:49:43.000+0000", "description": "We should expose only the more general multisend and multi fetch. The overhead of these is not high, and it would be good to reduce the number of network APIs.", "comments": [], "derived": {"summary": "We should expose only the more general multisend and multi fetch. The overhead of these is not high, and it would be good to reduce the number of network APIs.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Combine fetch/multifetch and send/multisend - We should expose only the more general multisend and multi fetch. The overhead of these is not high, and it would be good to reduce the number of network APIs."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-59", "title": "ByteBufferMessageSet logs error about fetch size", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:25.000+0000", "updated": "2011-07-19T21:32:25.000+0000", "description": "Not sure how this happened, but someone added an error message about fetch size being too small in ByteBufferMessageSet. This obviously makes no sense since this class is used in the producer and broker as well as in the consumer, neither of which have a fetch size. This error needs to be properly handled (say by throwing an error), and each user needs to be modified to handle it appropriately.", "comments": [], "derived": {"summary": "Not sure how this happened, but someone added an error message about fetch size being too small in ByteBufferMessageSet. This obviously makes no sense since this class is used in the producer and broker as well as in the consumer, neither of which have a fetch size.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "ByteBufferMessageSet logs error about fetch size - Not sure how this happened, but someone added an error message about fetch size being too small in ByteBufferMessageSet. This obviously makes no sense since this class is used in the producer and broker as well as in the consumer, neither of which have a fetch size."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-60", "title": "If the producer sends an invalid MessageSet the broker will append it, corrupting the log", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:25.000+0000", "updated": "2011-07-19T21:32:25.000+0000", "description": "It appears our producer request handling is actually a little buggy. We allow messagesets to be ragged on the right (i.e. contain a trailing partial message), but when we append() we ultimately do \nmessages.writeTo(channel, 0, messages.sizeInBytes) \nwhich i believe would append not just the valid messages, but also the partial message, thus corrupting the log. \n\nWe need to set limit() on the ByteBuffer to truncate off invalid trailing messages before writing, or something like that.", "comments": [], "derived": {"summary": "It appears our producer request handling is actually a little buggy. We allow messagesets to be ragged on the right (i.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "If the producer sends an invalid MessageSet the broker will append it, corrupting the log - It appears our producer request handling is actually a little buggy. We allow messagesets to be ragged on the right (i."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-61", "title": "SimpleProducer lose messages when socket gets an io exception", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:25.000+0000", "updated": "2011-07-19T21:32:25.000+0000", "description": "Currently, if we get any io exception while sending a message, SimpleProducer reestablishes the socket connection without resending the message. Thus the message is lost. \n\nOne way to fix this is to only reset socket channel to null when there is io exception during send and throw the exception back to the caller. The caller can capture the exception and resend the message.", "comments": [], "derived": {"summary": "Currently, if we get any io exception while sending a message, SimpleProducer reestablishes the socket connection without resending the message. Thus the message is lost.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "SimpleProducer lose messages when socket gets an io exception - Currently, if we get any io exception while sending a message, SimpleProducer reestablishes the socket connection without resending the message. Thus the message is lost."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-62", "title": "Ephemeral nodes continue to exist after consumer exit", "status": "Resolved", "priority": "Minor", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:26.000+0000", "updated": "2011-07-19T21:32:26.000+0000", "description": "You can get an error when restarting kafka if the zookeeper timeout hasn't elapsed. The reason is likely that the ephemeral nodes don't immediately disappear when the client disconnects but seem to persist until the timeout (30 secs or so). It would be good to find a way to forcefully terminate the connection or manually delete the znodes on a clean shutdown to avoid this issue. \n\n[2010-12-11 13:08:56,164] FATAL \norg.I0Itec.zkclient.exception.ZkNodeExistsException: \norg.apache.zookeeper.KeeperException$NodeExistsException: \nKeeperErrorCode = NodeExists for /brokers/ids/0 \n(kafka.server.KafkaServer) \n[2010-12-11 13:08:56,166] FATAL \norg.I0Itec.zkclient.exception.ZkNodeExistsException: \norg.apache.zookeeper.KeeperException$NodeExistsException: \nKeeperErrorCode = NodeExists for /brokers/ids/0 \nat org.I0Itec.zkclient.exception.ZkException.create(ZkException.java: \n55) \nat org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java: \n685) \nat org.I0Itec.zkclient.ZkClient.create(ZkClient.java:304) \nat org.I0Itec.zkclient.ZkClient.createEphemeral(ZkClient.java:328) \nat kafka.utils.ZkUtils$.createEphemeralPath(ZkUtils.scala:63) \nat kafka.utils.ZkUtils \n$.createEphemeralPathExpectConflict(ZkUtils.scala:79) \nat \nkafka.server.KafkaZooKeeper.registerBrokerInZk(KafkaZooKeeper.scala: \n54) \nat kafka.log.LogManager.startup(LogManager.scala:119) \nat kafka.server.KafkaServer.startup(KafkaServer.scala:62) \nat kafka.Kafka$.main(Kafka.scala:59) \nat kafka.Kafka.main(Kafka.scala) \nCaused by: org.apache.zookeeper.KeeperException$NodeExistsException: \nKeeperErrorCode = NodeExists for /brokers/ids/0 \nat org.apache.zookeeper.KeeperException.create(KeeperException.java: \n110) \nat org.apache.zookeeper.KeeperException.create(KeeperException.java: \n42) \nat org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:637) \nat org.I0Itec.zkclient.ZkConnection.create(ZkConnection.java:87) \nat org.I0Itec.zkclient.ZkClient$1.call(ZkClient.java:308) \nat org.I0Itec.zkclient.ZkClient$1.call(ZkClient.java:304) \nat org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:", "comments": [], "derived": {"summary": "You can get an error when restarting kafka if the zookeeper timeout hasn't elapsed. The reason is likely that the ephemeral nodes don't immediately disappear when the client disconnects but seem to persist until the timeout (30 secs or so).", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Ephemeral nodes continue to exist after consumer exit - You can get an error when restarting kafka if the zookeeper timeout hasn't elapsed. The reason is likely that the ephemeral nodes don't immediately disappear when the client disconnects but seem to persist until the timeout (30 secs or so)."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-63", "title": "MessageSet does not implement Java iterable", "status": "Resolved", "priority": "Minor", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:26.000+0000", "updated": "2011-07-19T21:32:26.000+0000", "description": "Which makes the for each loop in java not compatible. \nThis is fine in Scala however, the Scala iterable is implemented.", "comments": [], "derived": {"summary": "Which makes the for each loop in java not compatible. This is fine in Scala however, the Scala iterable is implemented.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "MessageSet does not implement Java iterable - Which makes the for each loop in java not compatible. This is fine in Scala however, the Scala iterable is implemented."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-64", "title": "KafkaAppender should to Encoder build from LoggingEvent", "status": "Closed", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:26.000+0000", "updated": "2011-07-19T21:32:26.000+0000", "description": "Currently in KafkaAppender: \n\nval message = encoder.toMessage(event.getMessage) \n\nWe would lose lots of important data, e.g. time, level, class, thread id ... These are important and already parsed data fields that can logged for process. \n\nEncoder should take LoggingEvent instead.", "comments": [], "derived": {"summary": "Currently in KafkaAppender: \n\nval message = encoder. toMessage(event.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "KafkaAppender should to Encoder build from LoggingEvent - Currently in KafkaAppender: \n\nval message = encoder. toMessage(event."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-65", "title": "Consumer client should warn when creating streams for partitions that do not exist", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:26.000+0000", "updated": "2011-07-19T21:32:26.000+0000", "description": "Currently if num.partitions is configured for n partitions and the client attempts to write to partition n+1, the send will succeed. However, if the consumer attempts to create a stream for partition n+1, that stream will never yield any information. It would at least be nice to get a warning about the partition impedance on the consumer (and possibly on the server). Alternatively, maybe update the quickstart docs to indicate that the server config change is necessary to achieve partitioning.", "comments": [], "derived": {"summary": "Currently if num. partitions is configured for n partitions and the client attempts to write to partition n+1, the send will succeed.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Consumer client should warn when creating streams for partitions that do not exist - Currently if num. partitions is configured for n partitions and the client attempts to write to partition n+1, the send will succeed."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-66", "title": "Poison Message", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:27.000+0000", "updated": "2011-07-19T21:32:27.000+0000", "description": "Unfortunately I wasn't able to figure out exactly how I accomplished this, but somehow I managed to get one or more poison messages in my test topic using fairly simple single threaded SimpleProducer/SimpleConsumer code. In this state, I could see several hundreds of thousands of messages in the topic via JMX and adding new messages appeared to succeed from the API calls, however each add resulted in the following error: \n\n[2010-12-08 14:56:04,952] ERROR kafka.message.InvalidMessageException (kafka.network.Processor) \nkafka.message.InvalidMessageException \nat kafka.log.Log$$anonfun$append$1.apply(Log.scala:187) \nat kafka.log.Log$$anonfun$append$1.apply(Log.scala:185) \nat scala.collection.Iterator$class.foreach(Iterator.scala:631) \nat kafka.utils.IteratorTemplate.foreach(IteratorTemplate.scala:29) \nat scala.collection.IterableLike$class.foreach(IterableLike.scala:79) \nat kafka.message.MessageSet.foreach(MessageSet.scala:63) \nat kafka.log.Log.append(Log.scala:185) \nat kafka.server.KafkaRequestHandlers.handleProducerRequest(KafkaRequestHandlers.scala:56) \nat kafka.server.KafkaRequestHandlers$$anonfun$handlerFor$1.apply(KafkaRequestHandlers.scala:40) \nat kafka.server.KafkaRequestHandlers$$anonfun$handlerFor$1.apply(KafkaRequestHandlers.scala:40) \nat kafka.network.Processor.handle(SocketServer.scala:268) \nat kafka.network.Processor.read(SocketServer.scala:291) \nat kafka.network.Processor.run(SocketServer.scala:202) \nat java.lang.Thread.run(Thread.java:662) \n\nAdditionally, no consumers were functional, they were all blocked and not draining the existing messages. Stopping the service did not get things healthy, but stopping and removing the logs fixed the problem. \n\nNot sure what the design intent is, but it would seem that failure to process a message internally within the server should cause the message to be rejected at the client layer.", "comments": [], "derived": {"summary": "Unfortunately I wasn't able to figure out exactly how I accomplished this, but somehow I managed to get one or more poison messages in my test topic using fairly simple single threaded SimpleProducer/SimpleConsumer code. In this state, I could see several hundreds of thousands of messages in the topic via JMX and adding new messages appeared to succeed from the API calls, however each add resulted in the following error: \n\n[2010-12-08 14:56:04,952] ERROR kafka.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Poison Message - Unfortunately I wasn't able to figure out exactly how I accomplished this, but somehow I managed to get one or more poison messages in my test topic using fairly simple single threaded SimpleProducer/SimpleConsumer code. In this state, I could see several hundreds of thousands of messages in the topic via JMX and adding new messages appeared to succeed from the API calls, however each add resulted in the following error: \n\n[2010-12-08 14:56:04,952] ERROR kafka."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-67", "title": "cleanup junit test", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:27.000+0000", "updated": "2011-07-19T21:32:27.000+0000", "description": "hide known log4j error during unit test.", "comments": [], "derived": {"summary": "hide known log4j error during unit test.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "cleanup junit test - hide known log4j error during unit test."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-68", "title": "This is a test to see if the kafka-dev mailing list gets this notification", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:27.000+0000", "updated": "2011-07-19T21:32:27.000+0000", "description": "Hi guys!", "comments": [], "derived": {"summary": "Hi guys!.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "This is a test to see if the kafka-dev mailing list gets this notification - Hi guys!."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-69", "title": "trait Serializer should be broken down to WriterSerializer and ReadSerializer", "status": "Closed", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:27.000+0000", "updated": "2011-07-19T21:32:27.000+0000", "description": "Create 2 extra traits: ReadSerializer and WriterSerializer and have Serializer be empty traits depending on both. \n\nThis is a backward compatible change and would make the api much more flexible, here is an example: \n\nKafkaAppender should only take WriteSerializer which makes using KafkaAppender much simpler.", "comments": [], "derived": {"summary": "Create 2 extra traits: ReadSerializer and WriterSerializer and have Serializer be empty traits depending on both. This is a backward compatible change and would make the api much more flexible, here is an example: \n\nKafkaAppender should only take WriteSerializer which makes using KafkaAppender much simpler.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "trait Serializer should be broken down to WriterSerializer and ReadSerializer - Create 2 extra traits: ReadSerializer and WriterSerializer and have Serializer be empty traits depending on both. This is a backward compatible change and would make the api much more flexible, here is an example: \n\nKafkaAppender should only take WriteSerializer which makes using KafkaAppender much simpler."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-70", "title": "Introduce retention setting that depends on space", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": null, "labels": ["log", "roll"], "created": "2011-07-19T21:32:27.000+0000", "updated": "2011-09-21T16:44:30.000+0000", "description": "Currently there is this setting: \n\nlog.retention.hours \n\nintroduce: \n\nlog.retention.size \n\nSemantic would be, either size is reached or time is reached, oldest messages will be deleted. \n\nThis does not break back-ward compatibility and would make the system robust under scenarios where message size is not deterministic over time.", "comments": ["Prashanth,\n\nNot sure if you still have those questions in the deleted comment. In any case, for your first question, the size limit can be checked every time we roll the log. For the second question, the default value can be no size limit to keep it consistent with the old behavior.", "Hi Jun,\n\nNope, answered them myself (had a brain-blank moment), hence why I deleted it.  In the mean time, I've attached a patch I think should enable this feature.  I originally did this on GitHub with a pull request but was redirected here :)  \n\nLet me know what you think or if this isn't quite what was expected.  \n\n- Prashanth", "Prashanth,\n\nThanks a lot for the patch. cleanUpSegmentsToMaintainSize will walk through the segments in whatever order they were loaded. That is done in loadSegments which (due to java.io.File.listFiles) does not guarantee any particular order. We need to do a numeric sort of the segments and then start deleting from the segment with the smallest offset.\n\nJoel\n", "Hi Joel,\n\nLooking through Log.loadSegments it looks like the segments (if any were found) are already sorted in order of ascending offset.  This effectively solves the problem, no?\n\n- Prashanth", "I agree the delete logic is correct. Log.scala keeps all segments in sorted order, and enforces the behavior that segments can only be deleted in reverse order (it uses takeWhile on the list) so this should guarantee that segments are deleted from oldest to newest. When we take the patch we should add a comments in our existing code to the effect that the list is guaranteed to be in sorted order, since that is kind of implicit now but it is important.\n\nPrashanth, one corner case here is when the segment size < log.retention.size. One thing to note is that if the currently active segment is marked for deletion, even if that segment is not full, it will dutifully be deleted and a new, empty segment created. The cleanup based on modified time always guarantees that the last message happened AT LEAST log.retention.hours ago because it uses the modification time of the file. This means the client always has at least that much time to pick up their messages. This gets weird, though, when we delete the active segment based on size. It looks to me that if I have a segment size of 100MB but a log.retention.size of 50MB, then when my single-segment log gets to 50MB we will delete the active segment, effectively all messages in the log, even though likely it contains as-yet undelivered messages which may have arrived only ms ago. I think this will be confusing for people. Two ways to fix it: one is to require that log.retention.size > log.file.size, the other is to change the delete logic so that it deletes all but the most recent segment. Another way to say this is that since our granularity of cleanup is log.file.size we have to either overshoot or undershoot their given retention size by as much as log.file.size-1. It is probably safer to overshoot rather than undershoot what they give. Another alternative would be to phrase the retention configuration in terms of the number of files rather than the size in bytes which would be more exact, but I think the way you have done it is more intuitive.\n\nAnother feature request: would it be possible to add a per-topic config similar to topic.log.retention.hours (say topic.log.retention.size) to allow configuring the log size at the topic level? If not I think we can take a patch that fixes the above issue and just open a bug for that.", "Haha, this is quite uncanny.  I had the same discussion regarding log.file.size and log.retention.size after I made the patch and concluded (I believe incorrectly) that the retention size should always be larger than the segment size.  I see both sides of the argument but I'm not really in a position to make a decision (due to me being rather new and lacking the knowledge of the breadth of use cases it's being used under).  From my point of view having the retention size be larger than the log file size is the natural solution; implementing logic to trim the log to log.retention.size but leaving the active segment undeleted is confusing since it doesn't really live by the rule its namesake configuration suggests.  As a user, I will still see the log file greater than the retention size and be confused (perhaps this can be solved with a simple rename of the configuration?).  Just my two cents.  I'd agree with you though, dealing with bytes rather than files is a more intuitive way.\n\nIn any case, let me know which solution you would like to go with.  As it is right now, I made the retention size > log size rule implicit, but I can add it into the code and throw an error or warning.\n\nI'd definitely like to work on the per-topic retention feature request, it'll give me a better chance to dig through the code :)  My preference would be to create a new bug for it (assign it to me?).\n\nCheers,\nPrashanth  ", "Prashanth, thanks for the patch.\n\nI suggest that we enforce retention size > log size and disallow the broker to start if the rule is violated.", "Okay, expect a patch with the added rule enforcement later today.", "Actually, now that I think about it, not sure if just limiting the config to the size of one segment is enough. Intuitively here is what I think we want the config to mean: \"Keep the least amount of data possible to guarantee a buffer for the consumer of at least N bytes\". If we don't give that guarantee of that form there is no way to reason about the SLA for the consumer to pick up their data. For example if retention_size is 50*1024*1024+10 and segment size is 5050*1024*1024, then the log may actually be trimmed as small as 10 bytes.\n\nSo Jun, I think the config approach doesn't work. I think we need to tweak the delete rule slightly.", "One possibility is to have the following semantic: delete segment files as much as possible with at least retention_size amount of data remaining (instead of bringing the total data size to below the retention_size).", "Jun, yeah, exactly, I think that is what I was trying to say but said more clearly.", "Hi all, I've attached an updated patch for your consideration.  It now deletes segments leaving at least logRetentionSize space in the log.", "Hi guys, had a chance to look at the newest patch?", "+1. Thanks for the patch !", "Applied, thanks! I will open a JIRA to add per-topic overrides for the retention size.", "Trunk == 0.7 not 0.8, right?", "You are right Chris. Trunk is on 0.7"], "derived": {"summary": "Currently there is this setting: \n\nlog. retention.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Introduce retention setting that depends on space - Currently there is this setting: \n\nlog. retention."}, {"q": "What updates or decisions were made in the discussion?", "a": "You are right Chris. Trunk is on 0.7"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-71", "title": "embed offset inside Message class", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:28.000+0000", "updated": "2012-11-15T03:59:57.000+0000", "description": "introduce Message.getOffst() that returns the offset this message is read from. \n\nI am using SimpleConsumer to manage offset myself, I would need to do: \n\noffset+=MessageSet.entrySize(msg); \n\nand keep track of the offset myself. \n\nThis would my life easier if Message contains offset information.", "comments": ["In 0.8 the iterator returns the offset."], "derived": {"summary": "introduce Message. getOffst() that returns the offset this message is read from.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "embed offset inside Message class - introduce Message. getOffst() that returns the offset this message is read from."}, {"q": "What updates or decisions were made in the discussion?", "a": "In 0.8 the iterator returns the offset."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-72", "title": "Redo config objects with default arguments", "status": "Resolved", "priority": "Minor", "reporter": null, "assignee": null, "labels": [], "created": "2011-07-19T21:32:28.000+0000", "updated": "2020-05-27T20:51:39.000+0000", "description": "It would be nice to experiment with redoing the config objects we use for the various clients using scala 2.8 default arguments in addition to the java.util.Properties arguments.", "comments": ["Closing this old ticket as abandoned."], "derived": {"summary": "It would be nice to experiment with redoing the config objects we use for the various clients using scala 2. 8 default arguments in addition to the java.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Redo config objects with default arguments - It would be nice to experiment with redoing the config objects we use for the various clients using scala 2. 8 default arguments in addition to the java."}, {"q": "What updates or decisions were made in the discussion?", "a": "Closing this old ticket as abandoned."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-73", "title": "SyncProducer sends messages to invalid partitions without complaint", "status": "Resolved", "priority": "Minor", "reporter": "Jonathan Herman", "assignee": "Dru Panchal", "labels": [], "created": "2011-07-20T19:19:19.000+0000", "updated": "2016-08-26T22:47:33.000+0000", "description": "The SyncProducer class will send messages to invalid partitions without throwing an exception or otherwise alerting the user.\n\nReproduction:\nRun the kafka-simple-consumer-shell.sh script with an invalid partition number. An exception will be thrown and displayed. Run the kafka-producer-shell.sh with the same partition number. You will be able to send messages without any errors.", "comments": ["This is because SyncProducer doesn't get any ACK from the broker right now. This will change as part of https://issues.apache.org/jira/browse/KAFKA-49.", "Marking this JIRA closed as the bug was solved with KAFKA-49."], "derived": {"summary": "The SyncProducer class will send messages to invalid partitions without throwing an exception or otherwise alerting the user. Reproduction:\nRun the kafka-simple-consumer-shell.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "SyncProducer sends messages to invalid partitions without complaint - The SyncProducer class will send messages to invalid partitions without throwing an exception or otherwise alerting the user. Reproduction:\nRun the kafka-simple-consumer-shell."}, {"q": "What updates or decisions were made in the discussion?", "a": "Marking this JIRA closed as the bug was solved with KAFKA-49."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-74", "title": "Kafka mirror (corp replica): auto-discovery of topics", "status": "Resolved", "priority": "Major", "reporter": "Joel Jacob Koshy", "assignee": "Jun Rao", "labels": [], "created": "2011-07-20T20:16:42.000+0000", "updated": "2011-10-24T03:06:14.000+0000", "description": "The corp replica's kafka embedded consumer requires a whitelist of topics to be \nspecified in its configuration. This does not scale very well as more and more\ntopics are added. Instead, it can keep track of the current topics in zookeeper.\nWith this approach, there should be a blacklist configuration as well if the user\nwishes to omit designated topics in the replica.\n\nFurthermore, the \"replica/replication\" terms can become confusing when we start\nworking on the replication feature. So, as part of this issue, we can address this\nambiguity as well:\n\nReplication vs. Mirroring:\n\nKafka's roadmap includes a \"replication\" feature \n(https://issues.apache.org/jira/browse/KAFKA-50) that will improve its\ndurability and availability guarantees. In the past, we have also used the\nterm \"replication\" to describe the process of building a replica of a Kafka\ncluster. This is done by providing a consumer configuration when starting up\na kafka server. The configuration should contain a parameter\n(embeddedconsumer.topics) which is a whitelist of topics that the user\nwishes to replicate. The kafka server then instantiates an embedded consumer\nto fetch the corresponding logs from the source cluster.  The messages that\nthe embedded consumer consumes are written to local kafka logs.\n\nIn order to avoid any confusion between the two features going forward, I\nthink it will be good to make a clearer distinction. We can call the former\nfeature \"replication\", and the latter feature (i.e., building a replica)\n\"mirroring\". So, if the user provides an (embedded) consumer configuration\nto the Kafka server, then it will implicitly run as a \"mirror\". We can also\nimprove the clarity of the related config parameters as described below.\n\nConfig change - Default topic whitelists for mirroring:\n\nThe embedded consumer's whitelist is currently specified as part of\nConsumerConfig. E.g.,embeddedconsumer.topics=topic1:3,topic2:1. However, the\ncommon case is to mirror all topics. Therefore, it may be more convenient to\ndiscover topics through the source cluster's ZooKeeper, mirror all topics by\ndefault and provide a new blacklist configuration option. If you wish to\nmirror only a few topics, the whitelist option is still available.\n\nAt most one of the following options can be present in the embedded\nconsumer's configuration. If neither option is present, all topics will be\nmirrored.\n\nmirror.topics.blacklist: (topics to skip for mirroring)\nmirror.topics.whitelist: (alias for embeddedconsumer.topics, which can\neventually be deprecated)\n", "comments": ["Patch for KAFKA-74 and KAFKA-75. It includes an enhanced embedded consumer system test that was used to verify the changes. The patch also addresses a small bug in the generation of consumer-id's which caused an occasional collision when instantiating multiple consumer connectors on the same host at the same time.", "Got a compilation error with the patch:\n[error] /Users/jrao/intelligj_workspace/kafka_svn/trunk/core/src/test/scala/unit/kafka/consumer/ZookeeperTopicEventWatcherTest.scala:34: value brokerTopicsPath is not a member of object kafka.utils.ZkUtils\n[error]     \"%s/%s-%d\".format(ZkUtils.brokerTopicsPath, \"topic\", i)\n", "Here's the updated patch. The reason the older patch did not compile for you is that there was a change in the name for ZkUtils.brokerTopicsPath -> BrokerTopicsPath in the compression feature. I did not catch this, because package does not depend on test-compile. I modified this as well (please see KafkaProject.scala\"). Also, there was another small issue with the config checking: the invalid config exception should only be thrown if both whitelist/blacklist are *nonempty*. The previous patch threw an exception if they were both present as keys in the config props.", "One comment:\nIn KafkaServerStartable, in handleTopicEvent, it's better to log only filtered events.\n", "Some more comments:\n1. In ZookeeperTopicEventWatcher.ZkTopicEventListener.handleChildChange(), there is a subtle bug. Since isBootstrapping is set outside of the synchronization block, it can happen that the caller with isBootstrapping set to true may not be the one that enters the synchronized block first. A simple solution is to synchronize the whole body of this method.\n\n2. This may be nit-picking. In KafkaServerStartable.EmbeddedConsumer.handleTopicEvent(), isTopicAllowed may be tested twice on a particular topic, once through added/deleted topics and another through allTopics. Can you restructure the code so that each topic is only tested once?\n\n3. There is a conflict when applying your patch. Could you rebase?", "Updated patch:\n- Based on offline discussion with Jun, a simpler API for the topic event listener is sufficient and more efficient. Should we need more fine-grained events we can incorporate pieces of the earlier patch.\n- There was another corner-case race condition in the shutdown function, so added appropriate synchronization and got rid of the shutdown flag in ZookeeperTopicEventWatcher. Please double-check this logic in case I missed something.\n- Addressed the other relevant comments.\n", "Sorry, but I overlooked this earlier. We need to re-register all watchers when ZK session expires. ZkClient doesn't seem to re-register watches when a new session is created.", "Thanks for pointing out the session expiration issue. Here is an updated patch, along with the other small edits we discussed:\n- logging when handling zkclient exceptions\n- consumer connector: moved session expire listener registration prior to rebalance listener registration.\n", "system_test/embedded_consumer breaks on mac.", "Also, please make sure that the new files have the correct Apache header.", "This was tested on a linux box. The test script used GNU options in the ps command. See if this works on your mac now.\n\nThanks,\n\nJoel", "Thanks, Joel. Just committed this with a minor fix that uses awk instead of cut in the testing script (so that it works on mac).", "Closing as this appears to have been completed."], "derived": {"summary": "The corp replica's kafka embedded consumer requires a whitelist of topics to be \nspecified in its configuration. This does not scale very well as more and more\ntopics are added.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Kafka mirror (corp replica): auto-discovery of topics - The corp replica's kafka embedded consumer requires a whitelist of topics to be \nspecified in its configuration. This does not scale very well as more and more\ntopics are added."}, {"q": "What updates or decisions were made in the discussion?", "a": "Closing as this appears to have been completed."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-75", "title": "Kafka mirror does not shutdown on IO error", "status": "Closed", "priority": "Major", "reporter": "Joel Jacob Koshy", "assignee": null, "labels": [], "created": "2011-07-26T18:27:15.000+0000", "updated": "2011-08-03T22:17:47.000+0000", "description": "The embedded consumer in the kafka mirror implementation uses the low-level Log api to create the replica. The append operation may fail and result in a corrupt log file, due to an IO error which is currently caught and ignored.\n\nThe proposed fix is to switch to using the high-level producer API to create the replica. Not only would this avoid the above issue, but it would also fit better with the current design of the replication enhancement for kafka (http://linkedin.jira.com/browse/KAFKA-23), since the low-level Log api is not replication-aware. Another advantage is that compression is exposed at the producer API-level. One caveat in this approach would be the following: the async producer drops events when its queue is full. This behavior is unsuitable for the embedded consumer, so we can expose a configuration option in the producer to allow for (queue-level) blocking semantics.\n", "comments": ["If we use a producer to mirror the source cluster, then that brings in the need for a producer configuration in addition to the embedded consumer configuration. There are some alternatives:\n\n- One ConsumerConfig and one ProducerConfig. i.e., only allow mirroring one source cluster at a time. If you need mirrors of multiple source clusters, you have to set up one target kafka cluster per source cluster.\n- Only one (global) ConsumerConfig and allow multiple values (one per source cluster) for the zk.connect property. However, with this approach you cannot easily override global configs such as socket.buffersize/timeout (and such overrides are often useful for cross-DC mirrors), topic blacklists (when we do auto-discovery of topics - SNA-6887), etc.\n- Array of ConsumerConfig (one per source cluster to mirror) and one ProducerConfig. We can either instantiate multiple embedded consumers or just multiple consumer connectors.\n\nMore considerations for ProducerConfig:\n\n- With only one ProducerConfig, identical topics from different clusters will get mixed on the target cluster. We could have an array of ConsumerConfig/ProducerConfig pairs to avoid this.\n- If it makes sense, we can also allow ProducerConfig to be optional to aid in smoother deployments of the next release (with the above changes in place). In this case, the ProducerConfig will inherit the ZkConfig of the local kafka broker.\n", "This issue has been subsumed by the fix for KAFKA-74."], "derived": {"summary": "The embedded consumer in the kafka mirror implementation uses the low-level Log api to create the replica. The append operation may fail and result in a corrupt log file, due to an IO error which is currently caught and ignored.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Kafka mirror does not shutdown on IO error - The embedded consumer in the kafka mirror implementation uses the low-level Log api to create the replica. The append operation may fail and result in a corrupt log file, due to an IO error which is currently caught and ignored."}, {"q": "What updates or decisions were made in the discussion?", "a": "This issue has been subsumed by the fix for KAFKA-74."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-76", "title": "Move/ migrate Kafka source code to ASF Maven repository", "status": "Resolved", "priority": "Major", "reporter": "Henry Saputra", "assignee": "Jay Kreps", "labels": ["maven", "migrate", "repository"], "created": "2011-07-28T21:22:19.000+0000", "updated": "2011-08-01T23:49:22.000+0000", "description": "This JIRA task is logged to track open task to move/ migrate Kafka podling source code from Github to ASF Maven repository.", "comments": ["Code is checked in. svn trunk https://svn.apache.org/repos/asf/incubator/kafka/trunk @ r1152970 correpsonds to https://github.com/kafka-dev/kafka/commit/709afe4ec75489bc00a44335de8821fa726bb97e"], "derived": {"summary": "This JIRA task is logged to track open task to move/ migrate Kafka podling source code from Github to ASF Maven repository.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Move/ migrate Kafka source code to ASF Maven repository - This JIRA task is logged to track open task to move/ migrate Kafka podling source code from Github to ASF Maven repository."}, {"q": "What updates or decisions were made in the discussion?", "a": "Code is checked in. svn trunk https://svn.apache.org/repos/asf/incubator/kafka/trunk @ r1152970 correpsonds to https://github.com/kafka-dev/kafka/commit/709afe4ec75489bc00a44335de8821fa726bb97e"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-77", "title": "Implement \"group commit\" for kafka logs", "status": "Resolved", "priority": "Major", "reporter": "Jay Kreps", "assignee": "Jay Kreps", "labels": [], "created": "2011-07-30T17:41:14.000+0000", "updated": "2012-10-22T03:19:32.000+0000", "description": "The most expensive operation for the server is usually going to be the fsync() call to sync data in a log to disk, if you don't flush your data is at greater risk of being lost in a crash. Currently we give two knobs to tune this trade--log.flush.interval and log.default.flush.interval.ms (no idea why one has default and the other doesn't since they are both defaults). However if you flush frequently, say on every write, then performance is not that great.\n\nOne trick that can be used to improve this worst case of continual flushes is to allow a single fsync() to be used for multiple writes that occur at the same time. This is a lot like \"group commit\" in databases. It is unclear which cases this would improve and by how much but it might be worth a try.", "comments": ["A patch that implements group commit for kafka. This implementation is a little complex, the append() method is now a little scary, maybe someone sees a way to do it more simply.\n\nA couple of notes:\n\n1. I don't use any separate threads, the actual write is done by one of the writing threads involved in the commit (essentially it is a race, whomever gets there first does it).\n\n2. I only try to batch the flush, I don't try to batch the write() call though. Batching writes could be done as well but it would require either working around the MessageSet.writeTo interface since you now want to write multiple message sets at once in a single call, which breaks the current abstraction. Also the write call gives time for more writes to accumulate in the group so that might not help.\n\n3. I try to limit the group size to some fixed upper limit (50) which I just hard code. In practice I could not produce groups of more than 3, but I want to guarantee that you can't block the commit forever by queuing up writes under high load.\n\nThis whole idea is really only worth it if there are non-pathological cases where performance gets significantly better and performance doesn't get worse anywhere else.\n\nI haven't really done any performance testing yet as my laptop seems to get CPU bound by the producer perf test process which means I am having trouble producing an I/O-bound load on one machine. I think I need to run an experiment with more than one producer machine which is separate from the kafka machine and perhaps with more than one topic to force seeks when we do flushes (sequentially flushes should be much cheaper but that would only happen if you had one topic). I will update this bug when i have some real benchmarking.\n", "Thanks for the patch. The logic in the patch looks correct. I don't know how much benefit we can gain from this. Will wait for the performance number.\n\nMy main concern with this patch is that it doesn't remove either of the two existing log flush configs and potentially adds a third one, MaxGroupCommitSize. This seems to complicate the configs further.\n\nMy preference would be to implement a separate flush thread that constantly obtains dirty file segments from a blocking queue and flushes each of them as fast as possible. We can replace the two existing flush configs with a new one that controls the queue size (i.e., # of outstanding flushes).", "bq. My preference would be to implement a separate flush thread that constantly obtains dirty file segments from a blocking queue and flushes each of them as fast as possible.\n\nflush == FileChannel.force == fsync, right?  Isn't the the point to limit fsync to a reasonable (not too many per scond) rate, not to issue them as fast as possible? ", "Chris, I am not if if we gain much by limiting the # of fsync. In a typically scenario in Kafka, most reads are served from pagecache. So the real I/O load to the underlying storage system is fsync. If at a given point of time, there is a pending write and there is no ongoing fsync, we are not fully utilizing the available resource of the storage system. I think a more effective way is to keep flushing in a separate thread. If there are multiple additional writes accumulated during one flush, the next flush will fsync more data to the storage media in a single call, essentially getting the benefit of group commit. If there is only 1 more write accumulated, syncing it immediately doesn't hurt since otherwise the storage system will likely be idle.\n", "Yeah, fwiw, I consider this a proof of concept to understand the perf impact.\n\nChris, yes, flush == FileChannel.force == fsync.\n\nTo clarify the performance case I am going after is not the case where you have tuned back sync() to a reasonable level, this case we already handle optimally i think. The case I was targeting was the case where you need to sync on every write for durability (or to reduce consumer latency). In this case I suspect we could batch 30-50% of the flushes, which could help a lot. Since this is a performance optimization I agree it is only maybe worth it if the performance is quite good in the target case and not worse elsewhere. If not then we will at least have explored the possibility.\n\nOne thing that would greatly help this kind of thing would be to have a sort of canonical performance suite to run against, but making that is more work then this patch...\n\nI agree that it could be cleaner to have a separate I/O thread pool that handled all writes for each Log in a single threaded manner off its own write queue. If the performance pans out i will consider this.", "I tried running the new producer perf tests on this patch, measuring the producer throughput with and without this patch. Here are the findings -\n\ngroup.commit, MB/sec, messages/sec, broker.num.partitions, broker.flush.interval, broker.num.threads, num.producer.threads\nyes, 1.8903, 9910.4099, 1, 1, 8, 8\nno, 1.1037, 5786.3673, 1, 1, 8, 8\nyes, 6.0624, 31784.5769, 1, 1000, 8, 8\nno, 4.9943, 26184.3166, 1, 1000, 8, 8\n\nvarying the number of threads on the server (affects the number of writes that can be batched)\n\ngroup.commit, MB/sec, messages/sec, broker.num.partitions, broker.flush.interval, broker.num.threads, num.producer.threads\n\nyes, 2.0313, 10649.6273, 1, 1, 32, 8\nno, 1.1499, 6028.5997, 1, 1, 32, 8\nyes, 6.2151, 32584.9653, 1, 1000, 32, 8\nno, 4.8507, 25431.4445, 1, 1000, 32, 8\n\n\nTo summarize, it shows at least 16% improvement (with flush interval 1000 and 8 server threads) and 38% improvement (with flush interval 1 and 8 server threads)\n\n", "This is not really a good idea post 0.8 as we no longer have much dependence on the disk flush.", "> This is not really a good idea post 0.8 as we no longer have much dependence on the disk flush.\n\nJay, would you mind explaining a bit more? Is there a new feature in Kafka >0.8 that improves durability without the the needs for disk flushes? Or is there perhaps a new feature that decreases the performance penalty of flushing after every message?\n\n", "I think what Jay meant is that in 0.8, a message is considered as committed as long as it's written in memory in f brokers (f being the replication factor). This is probably as good or better than forcing data to disk, assuming failures are rare. Therefore, flushing to disk does not need to be optimized for durability guarantees."], "derived": {"summary": "The most expensive operation for the server is usually going to be the fsync() call to sync data in a log to disk, if you don't flush your data is at greater risk of being lost in a crash. Currently we give two knobs to tune this trade--log.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Implement \"group commit\" for kafka logs - The most expensive operation for the server is usually going to be the fsync() call to sync data in a log to disk, if you don't flush your data is at greater risk of being lost in a crash. Currently we give two knobs to tune this trade--log."}, {"q": "What updates or decisions were made in the discussion?", "a": "I think what Jay meant is that in 0.8, a message is considered as committed as long as it's written in memory in f brokers (f being the replication factor). This is probably as good or better than forcing data to disk, assuming failures are rare. Therefore, flushing to disk does not need to be optimized for durability guarantees."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-78", "title": "add optional mx4j support to expose jmx over http", "status": "Resolved", "priority": "Minor", "reporter": "Chris Burroughs", "assignee": "Chris Burroughs", "labels": [], "created": "2011-07-31T16:09:11.000+0000", "updated": "2011-10-06T05:12:22.000+0000", "description": "In the fun world of operations and monitoring sometimes HTTP is a better choice than 'real' jmx for alerting.  mx4j exposes jmx beans over HTTP (sadly with xml).  Attached patch is a scala port of CASSANDRA-1068 that optionally loads mx4j if it is present on the classpath.", "comments": ["This is nice. Couple of things:\n- Do we have anyone who would directly make use of this? Without that these things tend to rot...\n- Jun/Neha, we are already using mx4j what happens to someone already using mx4j if this code runs too?\n- We have tried to make the kafka broker easily embeddable. I wonder if we should not just let people add additional \"container\" functionality that way rather than us trying to add it all into the kafka itself. What I mean is you can easily make an XyzKafkaWrapper.java that does whatever custom logic you want in addition to starting the Kafka broker.", "I would use it. Right now all of our production JVM code exposes stats over HTTP, we mostly avoid JMX.\n\nOn the embeddable broker, I'd sign up to help spec/dev it if needed. I asked if that was possible in the IRC channel last week and the short answer was, \"not really\".", "The kafka broker is just an object that takes a java.util.Properties in its constructor so it should be very easy to embed. Was there a reason it couldn't be embedded? We do this in tests and indeed this is how linkedin does it so i think it does work...\n\nI think this is an important use case, the question is do we want to try to build this in or let other people create their own wrappers. I could see going either way, but my experience is that a lot of this packaging stuff tends to be fairly environment specific so even if someone else is using mx4j they may have a different incompatible version so it is just better to let people embed and do their own thing. (Actually LinkedIn is using mx4j in this very way and I am a little concerned this would conflict with out usage...so it would be good if there was an option so it could be disabled at the least).\n\nSo I think we should either:\n1. Document how to embed the broker to make it easy for people to do these kinds of things OR\n2. Make it so that this can be disabled", "I think if this is disabled by default through a config, then it should be fine. Some minor comments on the patch:\n1. please remove unused import javax.management.MBeanServer\n2. fix typo scale -> scala", "I like having a clean separation between \"this is the kafka class\"  and \"this is the class for the kafka daemon (and will instantiate the kafka server class)\".\n\nIf you try to start mx4j twice you will get something like this in the logs, but I think everything will function correctly.\n\n[2011-08-08 15:34:43,470] WARN Could not start register mbean in JMX (kafka.utils.Mx4jLoader$)\njavax.management.InstanceAlreadyExistsException: system:name=http\n\tat com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:453)\n\tat com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(DefaultMBeanServerInterceptor.java:1484)\n\tat com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:963)\n\tat com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:917)\n\tat com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:312)\n\tat com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:482)\n\tat kafka.utils.Mx4jLoader$.maybeLoad(Mx4jLoader.scala:47)\n\tat kafka.server.KafkaServer.startup(KafkaServer.scala:74)\n\tat kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:40)\n\tat kafka.Kafka$.main(Kafka.scala:56)\n\tat kafka.Kafka.main(Kafka.scala)\n", "v2 fixes typos and imports", "Jay, are you satisfied with the multiple invitations of mx4j case?", "Shouldn't the mx4j port come from kafka config, why make it a system property?\n\nWhat happens if two kafka servers are instantiated in the same jvm? We do this for testing quite a lot.\n\nWe should also have a config to disable this feature.", "It doesn't looks like that we have a patch that addresses all the concerns here. Perhaps we can defer this to 0.8?", "Sorry for the delay.\n\n - License header format updated.\n - Added -Dmx4jdisable to disable mx4j even if it's on the classpath.\n\nThey are system properties because it only makes sense to have one mx4j port per JVM (like jmx ports), not one per embedded broker.  If there is a case there you have a lot of embedded brokers, and mx4j (ie not a test), and the log messages are annoying I can add some more complicated \"do only once\" code.\n\nUnit tests should be unaffected since we don't have a hard dep on mx4j.", "Chris, thanks for the patch. Do you think that we can change mx4jdisable to mx4jenable so that if the user doesn't specify the property, the mx4j part is not touched? This way, users who already use mx4j but don't want mx4j to be loaded in the embedded Kafka broker are not affected by default.", "v4 defaults to off (even with mx4j is on the classpath).", "+1. Thanks for the patch, Chris.", "Committed this with a minor change (rename property mx4jenable to kafka_mx4jenable). Thanks for the patch, Chris."], "derived": {"summary": "In the fun world of operations and monitoring sometimes HTTP is a better choice than 'real' jmx for alerting. mx4j exposes jmx beans over HTTP (sadly with xml).", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "add optional mx4j support to expose jmx over http - In the fun world of operations and monitoring sometimes HTTP is a better choice than 'real' jmx for alerting. mx4j exposes jmx beans over HTTP (sadly with xml)."}, {"q": "What updates or decisions were made in the discussion?", "a": "Committed this with a minor change (rename property mx4jenable to kafka_mx4jenable). Thanks for the patch, Chris."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-79", "title": "Introduce the compression feature in Kafka", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "labels": [], "created": "2011-08-02T00:17:33.000+0000", "updated": "2011-10-06T21:18:04.000+0000", "description": "With this feature, we can enable end-to-end block compression in Kafka. The idea is to enable compression on the producer for some or all topics, write the data in compressed format on the server and make the consumers compression aware. The data will be decompressed only on the consumer side. Ideally, there should be a choice of compression codecs to be used by the producer. That means a change to the message header as well as the network byte format. On the consumer side, the state maintenance behavior of the zookeeper consumer changes. For compressed data, the consumed offset will be advanced one compressed message at a time. For uncompressed data, consumed offset will be advanced one message at a time. ", "comments": ["- Have you done any performance comparisons?\n- Do you think you could produce a diff (link to github is fine) that shows all of the compression changes? \n- My reading of CompressionCodec is that getCompressionCodec would have to be edited to add a new codec (so it would not be user plugable).  Do you think thats something we should support (and in that case I guess a separate ticket)", "We have some performance comparisons, we should include that information on the performance page at least by the time this is released. Of course our primary concern is interdatacenter bandwidth rather than performance per se. We see a ~30% compression ratio on our Avro tracking data.\n\nNeha should be able to give a diff. I think it was the last checkin on github before the cutover.\n\nIt is important that decompression always happen with the codec used for compression, so it can't just be the case that there is some property compression.codec=org.apache.kafka.GzipCompressor in the config because a mismatch on producer and consumer would lead to unreadable data, and if two people send messages with different codecs you would be totally screwed. This means the codec used must be maintained with the message set. We do this by having a compression id where 0=none, 1=gzip, etc. This doesn't lend itself to extensability since that list has to be predetermined, but we could reserve a codec id for \"user defined\" codec and leave it up to the user to configure it right.\n\nMy intuition is that most people just want a good compression implementation included out of the box and don't want to fiddle with it so i think it would be best to get that right. I think even in the long run there are really only 2-3 algorithms that have a reasonable cpu/size compression/decompression tradeoff so it makes sense to just implement and fully test those for perf and correctness and include those in a way that can't break.", "The github diff URL is here - https://github.com/nehanarkhede/kafka/compare/8db50143d7362750225e...4c49c4e2a9490255c35e21f89a6f4545de87cc5a\n\nThis diff url will capture the major changes, though some more minor fixes went in the Apache SVN repo after this. \n\nI started writing a wiki page for the compression feature here - https://cwiki.apache.org/confluence/display/KAFKA/Compression\n\nComments and suggestions are welcome.", "This looks like an excellent feature, Neha - thanks for working on it. We push a lot of highly compressible data into Kafka. Trading a bit of CPU for reduced disk and network activity sounds excellent.\n\nWould you be willing to accept a patch that implements support for http://code.google.com/p/snappy in addition to (or instead of) GZip? When consuming high-data-rate streams, we quickly peg the core on GZip decoding and have switched to Snappy (specifically, this implementation: http://code.google.com/p/snappy-java/) as a result.\n\nIf you have a chance, take a quick look at this JVM de/compressor throughput comparison: https://github.com/ning/jvm-compressor-benchmark/wiki -- these results mirror ours pretty closely. On a 36GB dataset of serialized data, we see an 89% compression ratio out of Snappy and 95% out of GZip. At least in our case, the slightly lower compression ratio still left us with a huge win in terms of codec throughput (and reducing the CPU burden on consuming / producing applications).\n\nâ Scott", "Scott, we would gladly take that feature! The compression is pluggable so adding that should not be too hard, we just need another compression id for snappy (gzip=1, so snappy=2). We should still keep gzip, I think it has a place, for example in our usage our biggest bottleneck is inter-datacenter bandwidth, so we will probably stay with gzip I think, plus it has no native depenencies so it is a little better \"out-of-the-box\" experience.\n\nOne thing we should think through is how this is packaged. Typically we make these things contrib/ packages to avoid adding to the main dependencies. It doesn't appear that the snappy jar has any external dependencies though and it looks like it packages up the .so/.dll files for all platforms in its jar, which is nice. I think it might be better just to add it in the main code, and ensure that none of the snappy classes are loaded unless the user selects snappy as the compression type (this will make the snappy jar optional for people--you only need it if you use it).\n\nAny other thoughts from people?\n", "- I think we should have a clear convention for ids. For example: core < 10000, contrib < 20000, HERE-BE-DRAGONS > 20000. \n- I think there is room for gzip, and something else in the  LZF/Snappy area in the default kafka install.\n- I'm mildly uncomfortable with native code dependencies, but the Hadoop guys seem to have gotten something working.", "Chris, which ids are you referring to?", "What Jay called \"compression ids\" (ie 1==gzip).", "Scott,\n\nThanks for pointing us to Snappy. I took a brief look at the benchmarks for Snappy, and it does look promising to me. As Jay mentioned, GZIP buys us increased throughput and better utilization of the network bandwidth, due to relatively high compression ratio. Though, its decompression cost, in terms of both TPS and CPU usage is not very low. According to preliminary Kafka compression performance benchmarks, with fetch size of 1MB, the consumer throughput doubled, while consuming a GZIP compressed topic. When the consumer is fully caught up, the CPU usage is ~45%, as compared to ~12% when the same consumer is consuming uncompressed data. On the producer side, for a batch size of 200, message size of 200, the producer throughput for generating compressed data is 1/2 the throughput when producing uncompressed data. That is the cost of compression for GZIP. Though this is tolerable for inter-DC replication, we could do better for more real-time applications that care about TPS more than the compression ratio. I see Snappy fitting well here (http://ning.github.com/jvm-compressor-benchmark/results/canterbury-roundtrip-2011-07-28/index.html).\n\nThe compression ratio that we see (for a producer batch size of 200) is 3x for GZIP on our typical tracking data set. I wonder how low this will be for Snappy. It will be good to check. \n\nIt will be great to see a Snappy integration patch with some Kafka performance benchmarks that measure compression/decompression overhead, compression ratio, effect on producer/consumer throughput. \n\n- Neha", "Jay and Neha,\n\nFantastic - thanks for taking a look. Your GZip choice makes sense - for inter-DC transport, totally agree with pushing the CPU a little harder to drive down the wire size. Most of our current use is within the same rack, so we've been optimizing for codec throughput. That said, I agree and like the idea of pluggable compressors.\n\nI'll clone, apply Neha's patch, and drop that in this weekend. I don't expect that it will be too complicated (it's just an InputStream; our local use is just a couple lines).\n\n(Apologies for the slow reply; I had mistakenly disabled e-mail notifications).\n\nâ Scott"], "derived": {"summary": "With this feature, we can enable end-to-end block compression in Kafka. The idea is to enable compression on the producer for some or all topics, write the data in compressed format on the server and make the consumers compression aware.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Introduce the compression feature in Kafka - With this feature, we can enable end-to-end block compression in Kafka. The idea is to enable compression on the producer for some or all topics, write the data in compressed format on the server and make the consumers compression aware."}, {"q": "What updates or decisions were made in the discussion?", "a": "Jay and Neha,\n\nFantastic - thanks for taking a look. Your GZip choice makes sense - for inter-DC transport, totally agree with pushing the CPU a little harder to drive down the wire size. Most of our current use is within the same rack, so we've been optimizing for codec throughput. That said, I agree and like the idea of pluggable compressors.\n\nI'll clone, apply Neha's patch, and drop that in this weekend. I don't expect that it will be too complicated (it's just an InputStream; our local use is just a couple lines).\n\n(Apologies for the slow reply; I had mistakenly disabled e-mail notifications).\n\nâ Scott"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-80", "title": "Fork or create link to node kafka client", "status": "Closed", "priority": "Trivial", "reporter": "Marcus Westin", "assignee": null, "labels": ["client", "node", "nodejs"], "created": "2011-08-03T17:32:08.000+0000", "updated": "2013-04-04T20:42:51.000+0000", "description": "https://github.com/marcuswestin/node-kafka has a working implementation of simple client in nodejs. It would be nice if it can be listed among the known kafka clients to avoid reinventing the wheel if someone needs a node implementation.\n\nCheers!\nMarcus", "comments": ["Marcus I'm not sure what you mean by fork. Are you requesting that we include this in the main Kafka tree (in which case ASF policy would require you to attach it at a patch and click the legal checkbox), or would you prefer to have just a link in the README & wiki and manage the code yourself  on github?", "Hey Chris,\n\nWhichever is the project maintainers' preference. If you like to have the clients in the kafka tree then I'll submit it as a patch; if not then a link in the README sounds great too.\n\nCheers!", "Hi Marcus, thanks for the great contribution!\n\nI think if the code is in reasonably good shape we prefer to keep it with the main code base unless you are doing very active development and would prefer to have your own repo. This helps people find the code, send us patches, and will help us to add integration tests that run all the clients against the server.\n\nOne thing we need to be able to do this though is make sure we have someone who will maintain the code on an ongoing basis (adapt to any binary format changes, fix bugs, etc). If you are up for doing that then I would vote we keep it with the main code.\n\n", "Yeah, if protocol changes are described well and there's working example code I'm happy to keep the node client up to date. Ok, I'll get a formal patch together.\n\nCheers!\nMarcus", "There is a link to the Node.JS client on the wiki page.\nhttps://cwiki.apache.org/confluence/display/KAFKA/Clients#Clients-Node.js\n\nThis bug should be closed as fixed."], "derived": {"summary": "https://github. com/marcuswestin/node-kafka has a working implementation of simple client in nodejs.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Fork or create link to node kafka client - https://github. com/marcuswestin/node-kafka has a working implementation of simple client in nodejs."}, {"q": "What updates or decisions were made in the discussion?", "a": "There is a link to the Node.JS client on the wiki page.\nhttps://cwiki.apache.org/confluence/display/KAFKA/Clients#Clients-Node.js\n\nThis bug should be closed as fixed."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-81", "title": "wrong path in bin/kafka-run-class.sh ", "status": "Resolved", "priority": "Major", "reporter": "Chris Burroughs", "assignee": null, "labels": [], "created": "2011-08-03T18:07:49.000+0000", "updated": "2013-06-14T03:56:39.000+0000", "description": "https://github.com/kafka-dev/kafka/issues/28\n\n{{monospace}}\nI just downloaded the official 0.6 archive:\nhttps://github.com/downloads/kafka-dev/kafka/kafka-0.6.zip\n\nand tried starting zookeeper / kafka.\n\nThe above archive will extract the deps into a dir called \"libs\", but in bin/kafka-run-class.sh there's a loop to add the jars in \"lib\" to the classpath:\n\nfor file in $base_dir/lib/*.jar;\ndo\n  CLASSPATH=$CLASSPATH:$file\ndone\n\nIt's a little more complicated then that. The tarball also places kafka-0.6.jar in the root of the directory, where no scripts look. config does not seem to have the log4j properties files, which makes zookeeper sad.\n{{monospace}}\n", "comments": ["It will be great to improve the release-zip target in sbt. The following should be the contents of the zip file, created by the ./sbt release-zip file. \n\nkafka\n|_bin            (all scripts that are required to get kafka up and running, also includes various scripts hidden in contrib, java-examples and perf)\n|_config       (all related config files )\n|_lib             (all dependent libraries required by various scripts in bin)\n|_kafka-x.x.jar\n|_kafka-java-examples-x.x.jar\n|_kafka-perf-x.x.jar\n|_hadoop-consumer_x.x.jar\n|_hadoop-producer-x.x.jar\n", "I have not had a change to work on this.  I still intended to, but anyone else should feel free to grab it since we can't release with this.", "Since, we haven't received a patch yet, we will hand create the release zip for 0.7. Moving this to 0.8", "This is fixed in 0.8."], "derived": {"summary": "https://github. com/kafka-dev/kafka/issues/28\n\n{{monospace}}\nI just downloaded the official 0.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "wrong path in bin/kafka-run-class.sh  - https://github. com/kafka-dev/kafka/issues/28\n\n{{monospace}}\nI just downloaded the official 0."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is fixed in 0.8."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-82", "title": "upgrade zkclient library as of 2011/04/12", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Jun Rao", "labels": [], "created": "2011-08-03T20:57:00.000+0000", "updated": "2011-08-10T05:13:45.000+0000", "description": "to pickup the following useful change:\n\nZkEventThread shouldn't stop on errors - catch throwables as well ", "comments": ["new jars build as of 2011/04/12 from https://github.com/sgroschupf/zkclient", "+1"], "derived": {"summary": "to pickup the following useful change:\n\nZkEventThread shouldn't stop on errors - catch throwables as well.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "upgrade zkclient library as of 2011/04/12 - to pickup the following useful change:\n\nZkEventThread shouldn't stop on errors - catch throwables as well."}, {"q": "What updates or decisions were made in the discussion?", "a": "new jars build as of 2011/04/12 from https://github.com/sgroschupf/zkclient"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-83", "title": "Options in SyncProducerConfig and AsyncProducerConfig can leak", "status": "Resolved", "priority": "Minor", "reporter": "Joel Jacob Koshy", "assignee": null, "labels": [], "created": "2011-08-04T20:31:13.000+0000", "updated": "2011-11-12T21:13:24.000+0000", "description": "There is the high-level producer api, and then there is the sync producer and then there is the async producer. Some config options are shared across these which leads to a small degree of confusion. I have a diagram lying around that I can attach to this jira. Anyway, due to this sharing the ProducerPool code copies around properties which is unsafe, because a developer who adds a new option may forget to copy it over. The fix is simple and should make the preceding summary a bit more clear.", "comments": ["Config hierarchy.", "Patch for KAFKA-83", "Looks great. \n+1", "Thanks, Joel. Just committed this.", "Closing as I believe this has been applied."], "derived": {"summary": "There is the high-level producer api, and then there is the sync producer and then there is the async producer. Some config options are shared across these which leads to a small degree of confusion.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Options in SyncProducerConfig and AsyncProducerConfig can leak - There is the high-level producer api, and then there is the sync producer and then there is the async producer. Some config options are shared across these which leads to a small degree of confusion."}, {"q": "What updates or decisions were made in the discussion?", "a": "Closing as I believe this has been applied."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-84", "title": "commit offset before consumer shutdown", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Jun Rao", "labels": [], "created": "2011-08-05T16:02:33.000+0000", "updated": "2011-09-13T01:26:51.000+0000", "description": "To avoid duplicated messages, the consumer should do a final offset commit before shutting itself down, if autocommit is enabled.", "comments": ["patch ready for review.", "Looks good.\n+1", "+1 \nNice catch!.\nFor consistency it would be good if it was commitOffsets() instead of commitOffsets since the convention we (kind of) have is that things with side effects get the method style call and things with no side effects get the variable-access style call."], "derived": {"summary": "To avoid duplicated messages, the consumer should do a final offset commit before shutting itself down, if autocommit is enabled.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "commit offset before consumer shutdown - To avoid duplicated messages, the consumer should do a final offset commit before shutting itself down, if autocommit is enabled."}, {"q": "What updates or decisions were made in the discussion?", "a": "+1 \nNice catch!.\nFor consistency it would be good if it was commitOffsets() instead of commitOffsets since the convention we (kind of) have is that things with side effects get the method style call and things with no side effects get the variable-access style call."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-85", "title": "Enhancements to .NET client", "status": "Resolved", "priority": "Major", "reporter": "Eric Hauser", "assignee": null, "labels": [], "created": "2011-08-05T17:33:35.000+0000", "updated": "2011-09-22T02:12:52.000+0000", "description": "* Refactoring original .NET client to mimic low-level and high-level Java APIs\n* ZooKeeper support for producers / consumers\n* Partitioner support for producers\n* Custom encoders for producers\n* Integration tests", "comments": ["Create using:\n\ngit diff --no-prefix a6ea8b16a11ad3fff6a283bface5c14975caec2d..be7bc445becd945be84365589574936db66d9399 > KAKFA-85.patch\n\nagainst:\n\ngit@github.com:ExactTargetDev/kafka.git - dotnet-client-enhancements", "Could you rebase your patch off the Apache repository? I can't seem to apply the patch.\n\npatching file .gitignore\nHunk #1 FAILED at 9.\n1 out of 1 hunk FAILED -- saving rejects to file .gitignore.rej\nThe next patch would delete the file clients/csharp/lib/StyleCop/Microsoft.StyleCop.Targets,\nwhich does not exist!  Assume -R? [n] ^C\n", "Will do as soon as there is a GitHub mirror.", "Eric, git://git.apache.org/kafka.git mirror is now up. ", "I went ahead and created a patch against SVN to make sure it applies correctly.  Attached is the new version.", "Just wanted to add the following relevant task:\n\nhttps://issues.apache.org/jira/browse/INFRA-3842\n\nThe ZooKeeper project has asked for Windows build support for Jenkins so that the ZK C# client can be built with CI.  The C# ZK client used in the Kafka patch has been contributed to the ZooKeeper project as well:\n\nhttps://issues.apache.org/jira/browse/ZOOKEEPER-1158", "Thanks for the patch, Eric.\n\nThis is a big patch and it's a lot of work. It raised a good question: what's the right approach to support client bindings for languages other than java. I will start a discussion in our mailing list.", "Eric, are you serious about maintaining the .net client going forward? If so, I can commit your patch for now. Thanks,", "We are certainly using it, and have already implemented compression and a number of performance fixes that I'll be submitting in additional patches.", "Do you really need clients/csharp/lib/log4Net/log4net.xml? It's a pretty big file.", "No, that does not need to be committed.", "Committed without the xml file. Thanks Eric.", "Eric, could you fix all the headers to comply with Apache format? Thanks,", "Attached a patch that adds the headers.  This is the previous full patch minus the XML file.", "Eric, I already committed your first patch. Could you make a patch that includes just the header change? Please make the header identical to other scala files. For example, please don't include \"Copyright 2011 LinkedIn\". Thanks,", "Sure, will do.  Apparently the GitHub sync is a little behind.  I didn't see your commit out there so I just assumed you hadn't committed it yet.", "What is the \"GitHub\" sync?", "All ASF repositories are synced to GitHub as read only mirrors:\n\nhttp://git.apache.org/\nhttps://github.com/apache/kafka\n"], "derived": {"summary": "* Refactoring original. NET client to mimic low-level and high-level Java APIs\n* ZooKeeper support for producers / consumers\n* Partitioner support for producers\n* Custom encoders for producers\n* Integration tests.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Enhancements to .NET client - * Refactoring original. NET client to mimic low-level and high-level Java APIs\n* ZooKeeper support for producers / consumers\n* Partitioner support for producers\n* Custom encoders for producers\n* Integration tests."}, {"q": "What updates or decisions were made in the discussion?", "a": "All ASF repositories are synced to GitHub as read only mirrors:\n\nhttp://git.apache.org/\nhttps://github.com/apache/kafka"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-86", "title": "the \"design\" link on the home page 404's", "status": "Resolved", "priority": "Minor", "reporter": "Robert J Berger", "assignee": "Jay Kreps", "labels": [], "created": "2011-08-06T21:32:39.000+0000", "updated": "2011-08-07T01:37:07.000+0000", "description": "The link for \"design\" in See our design page for more details. is broken (http://incubator.apache.org/kafka/design.php)", "comments": ["Nice catch! I fixed both links. Should be fixed on the site in a few hours as soon as the weird delayed apache sync thing happens."], "derived": {"summary": "The link for \"design\" in See our design page for more details. is broken (http://incubator.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "the \"design\" link on the home page 404's - The link for \"design\" in See our design page for more details. is broken (http://incubator."}, {"q": "What updates or decisions were made in the discussion?", "a": "Nice catch! I fixed both links. Should be fixed on the site in a few hours as soon as the weird delayed apache sync thing happens."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-87", "title": "time based segment index", "status": "Resolved", "priority": "Major", "reporter": "Chris Burroughs", "assignee": "Chris Burroughs", "labels": [], "created": "2011-08-08T13:40:08.000+0000", "updated": "2016-12-05T12:54:40.000+0000", "description": "A time index that:\n - Has minimal performance impact (such as by being append only)\n - Is suitable for\n - Works with getOffsetsBefore\n - Can have it's granularity configured.  With numbers >= 1 minute being \"normal\".\n - Can be disabled\n\nSee mailing list discussion : http://mail-archives.apache.org/mod_mbox/incubator-kafka-dev/201107.mbox/%3C4E2F678E.6060500@gmail.com%3E\n\n", "comments": ["As a related thing, to make this really useful we should integrate it into the consumer API. I.e. give an API along the lines of consumer.resetTo(long).", "Note to self: This came up on the user list and there was also significant interest in also indexing by message number (\"I would like to go back 10,000 messages, please give me that offset\").", "Duplicate of KAFKA-3163"], "derived": {"summary": "A time index that:\n - Has minimal performance impact (such as by being append only)\n - Is suitable for\n - Works with getOffsetsBefore\n - Can have it's granularity configured. With numbers >= 1 minute being \"normal\".", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "time based segment index - A time index that:\n - Has minimal performance impact (such as by being append only)\n - Is suitable for\n - Works with getOffsetsBefore\n - Can have it's granularity configured. With numbers >= 1 minute being \"normal\"."}, {"q": "What updates or decisions were made in the discussion?", "a": "Duplicate of KAFKA-3163"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-88", "title": "Producer perf test fails against localhost with > 10 threads", "status": "Resolved", "priority": "Minor", "reporter": "Jay Kreps", "assignee": "Jay Kreps", "labels": [], "created": "2011-08-08T17:36:19.000+0000", "updated": "2011-08-08T19:09:58.000+0000", "description": "The perf test starts producing errors when it is run with --threads 11 (or higher). The cause is that we create a zookeeper connection per thread, and zookeeper recently added a feature which limits the number of connections per ip in ZOOKEEPER-336. This setting is set to 10 by default. I recommend we bump this up in our packaged zk config, since it is hard to figure this out and makes it look like the client itself is having issues.\n\n", "comments": ["Change zk config to allow unlimited connections.", "Thanks, Jay. Looks good. Just committed this.", "Is this a zk conn per thead only for perf code,  Or do the producers/consumers do this?", "There is effectively one producer per test thread, this is good as it is more realistic i think."], "derived": {"summary": "The perf test starts producing errors when it is run with --threads 11 (or higher). The cause is that we create a zookeeper connection per thread, and zookeeper recently added a feature which limits the number of connections per ip in ZOOKEEPER-336.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Producer perf test fails against localhost with > 10 threads - The perf test starts producing errors when it is run with --threads 11 (or higher). The cause is that we create a zookeeper connection per thread, and zookeeper recently added a feature which limits the number of connections per ip in ZOOKEEPER-336."}, {"q": "What updates or decisions were made in the discussion?", "a": "There is effectively one producer per test thread, this is good as it is more realistic i think."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-89", "title": "consumer should initialize to a valid offset", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Jun Rao", "labels": [], "created": "2011-08-09T18:03:58.000+0000", "updated": "2011-10-12T01:21:28.000+0000", "description": "Currently, when a consumer starts up for the very first time, the server will log kafka.common.OffsetOutOfRangeException. To avoid that, the consumer should pick up a valid offset if offset is not available in ZK.", "comments": ["Patch available for review.", "The second case should be OffsetRequest.Latest\n\nAlso, the latestOffset val in getEarliestOrLatestOffset would be better renamed to earliestOrLatestOffset.", "Good catch. Uploaded v2 patch.", "Also, in the function earliestOrLatestOffset, the variable still says latestOffset. Can you rename it to say earliestOrLatestOffset ?", "Attached v3. Fixed variable name and message in the exception.", "+1\n\nLooks great.", "+1. "], "derived": {"summary": "Currently, when a consumer starts up for the very first time, the server will log kafka. common.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "consumer should initialize to a valid offset - Currently, when a consumer starts up for the very first time, the server will log kafka. common."}, {"q": "What updates or decisions were made in the discussion?", "a": "+1\n\nLooks great."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-90", "title": "remove in tree zk jar", "status": "Resolved", "priority": "Minor", "reporter": "Chris Burroughs", "assignee": "Chris Burroughs", "labels": [], "created": "2011-08-09T18:19:52.000+0000", "updated": "2011-08-24T03:18:20.000+0000", "description": "To get around some pulling in an undesirable log4j version we placed the zk jar in tree: http://mail-archives.apache.org/mod_mbox/incubator-kafka-dev/201108.mbox/%3CCAOG_4QbUq20gG9K34nsuq_5WnCn3mF1ionYyRXAKZs9VTyAkUA@mail.gmail.com%3E\n\nWe can do the same thing with exclusions.", "comments": ["This compiles and the contents of lib after running ./sbt update look right to me.", "Looks good. \n+1", "LGTM\n\nShip It!"], "derived": {"summary": "To get around some pulling in an undesirable log4j version we placed the zk jar in tree: http://mail-archives. apache.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "remove in tree zk jar - To get around some pulling in an undesirable log4j version we placed the zk jar in tree: http://mail-archives. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "LGTM\n\nShip It!"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-91", "title": "zkclient does not show up in pom", "status": "Resolved", "priority": "Minor", "reporter": "Chris Burroughs", "assignee": "Chris Burroughs", "labels": [], "created": "2011-08-09T18:27:22.000+0000", "updated": "2011-10-30T21:42:49.000+0000", "description": "The pom from created by `make-pom`. Does not include zkclient, which is  of course a key dependency.  Not sure yet how to pull in zkclient while excluding sbt itself.\n\n$ cat core/target/scala_2.8.0/kafka-0.7.pom  | grep -i zkclient | wc -l\n0\n", "comments": ["One way to do this would be to override the make-pom command in KafkaProject.scala and customize the output pom ", "Your right that's probably the best way instead of trying to do something clever and automatic with the contents of lib.  Some examples I found: http://vasilrem.com/blog/software-development/from-sbt-to-maven-in-one-move/ , https://gist.github.com/878462", "Moving it to 0.8", "This magic incantation appears to work.", "Could I get a review?", "Have we verified what other dependent jars this zkClient pulls in ? We've hit the multiple jar versions problem before.", "Our custom build of zkclient isn't in any repo and does not have a pom, so it can not pull in any dependencies.", "Oh, I missed that. I tried to apply the patch, but it doesn't apply on a clean checkout of the repository. Could you rebase and upload it again ?", "Sorry missed your reply, rebased as of 76957e5e3a748b59525e5e7934f93721eb8f4c38", "+1. Just committed this, but realized you could've done it yourself."], "derived": {"summary": "The pom from created by `make-pom`. Does not include zkclient, which is  of course a key dependency.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "zkclient does not show up in pom - The pom from created by `make-pom`. Does not include zkclient, which is  of course a key dependency."}, {"q": "What updates or decisions were made in the discussion?", "a": "+1. Just committed this, but realized you could've done it yourself."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-92", "title": "upgrade to latest stable 0.7.x sbt", "status": "Resolved", "priority": "Minor", "reporter": "Chris Burroughs", "assignee": "Chris Burroughs", "labels": [], "created": "2011-08-09T19:15:02.000+0000", "updated": "2011-10-06T03:25:40.000+0000", "description": "None of the changes look particularly important, but there isn't a reason to stay with an older version.\n\nhttp://code.google.com/p/simple-build-tool/wiki/Changes\n\nOld google group thread: http://groups.google.com/group/kafka-dev/browse_thread/thread/1c7b06a8c550fe23/08f3b2f9cd2059b0?lnk=gst&q=sbt#08f3b2f9cd2059b0", "comments": ["Diff doesn't include the binary:\nhttp://code.google.com/p/simple-build-tool/downloads/list", "So I think this is a totally safe backwards compatible upgrade, but has anyone else had a chance to give it a test?", "Tried it and was able to build package, test and idea. +1\n", "Chris, do you want to get this committed for 0.7 release? If so, we need this to be done by today.", "It turns out that this is the trigger for KAFKA-147, as it causes junit-3.8.1 to get pulled in.\n\nCan we revert to 0.7.5? We are anyway going to upgrade to 0.10.1 in KAFKA-134.\n", "This bug fix is causing another blocker bug - https://issues.apache.org/jira/browse/KAFKA-147\n\nIt seems like we will need to stick to 0.7.5 for now, and later upgrade to the latest SBT, which apparently fixes the issue that KAFKA-147 runs into.\n\nWe will have to revert this checkin - 1178669\n\n", "FWIW I can't reproduce KAFKA-134, but if you have confirmed that reverting fixes it I'm cool with going back to 0.7.5. (Sorry for the crazy bug hunt!)", "Ah, I see Neha already did."], "derived": {"summary": "None of the changes look particularly important, but there isn't a reason to stay with an older version. http://code.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "upgrade to latest stable 0.7.x sbt - None of the changes look particularly important, but there isn't a reason to stay with an older version. http://code."}, {"q": "What updates or decisions were made in the discussion?", "a": "Ah, I see Neha already did."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-93", "title": "Change code header to follow standard ASF source header ", "status": "Resolved", "priority": "Minor", "reporter": "Henry Saputra", "assignee": "Henry Saputra", "labels": [], "created": "2011-08-10T14:44:31.000+0000", "updated": "2011-08-12T01:13:47.000+0000", "description": "Need to update Kafka code to include ASF standard source header: http://www.apache.org/legal/src-headers.html\n\nI think most Kafka code already have the ASF header. Just need to remove the copyrights section of the comment per guideline.", "comments": ["Committed as revision 1156232.", "Hi Henry, there are a few more files:\n\ngrep -irH linkedin * | grep -v \\.svn\nLICENSE:   Copyright 2010 LinkedIn\nclients/clojure/LICENSE:Copyright 2011 LinkedIn\nclients/cpp/LICENSE:Copyright 2011 LinkedIn\nclients/csharp/LICENSE:Copyright 2011 LinkedIn\nclients/php/LICENSE:Copyright 2011 LinkedIn\nclients/python/kafka.py:# Copyright 2010 LinkedIn\nclients/python/setup.py:    author='LinkedIn.com',\nclients/ruby/LICENSE:Copyright 2011 LinkedIn\ncore/src/main/scala/kafka/tools/GetOffsetShell.scala:* Copyright 2010 LinkedIn\ncore/src/test/scala/unit/kafka/producer/ProducerMethodsTest.scala: * Copyright 2011 LinkedIn\nperf/.classpath:\t<classpathentry kind=\"lib\" path=\"/Users/femekci/linkedInSrc/kafka-multifetch/lib/scala-library-2.8.0.jar\"/>\nperf/.classpath:\t<classpathentry kind=\"lib\" path=\"/Users/femekci/linkedInSrc/kafka-multifetch/dist/kafka-0.04.6.0.jar\"/>\n\n", "Also, do test bash scripts (e.g., the system tests) need the license. How about config files? Do these fall under \"source\"? I don't see any comment on this in http://www.apache.org/legal/src-headers.html ", "Re-open for missing source and script files.", "Thanks Joel, apparently my update script somehow miss some scala files. I need to make sure it covers clients file too.", "Remove LinkedIn copyrights line and add ASF to some files.\n\nWe will just add changes directly for missing ASF license header. Closing this as resolved.", "@Henry - I believe, per the user mailing list, we're using RTC for patches in Kafka, so you need to upload the patch you'd like to commit and get a +1 before actually applying it.  Can you upload the patch you've committed?", "Hi Jakob, I think for this case not much review needed. I just change the header comment.\n\nYou can take a look at list of file changed in the Subversion Commits tab for this Jira.", "@Henry - the LICENSE files you've committed are incorrect in that you didn't replace the copyright year or name of copyright holder from the boiler text.  Perhaps, had the patch been uploaded for review as part of the RTC process that's been agreed upon, this would have been caught before the commit.", "You are talking about Copyright [yyyy] [name of copyright owner]?\n\nI thought this is just template that need to fill in by someone including Kafka in their project?\n\nI will follow RTC process next time.", "@Jakob From the ASF license file itself:\n\nAPPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n....\n\n\nThe boiler plate is used to add Apache license to you own source which LinkedIn has done before moving Kafka to ASF incubator. Since Kafka is moving under ASF umbrella I modified the header comment to follow ASF guideline.\n\nPlease let me know if I have interpreted this incorrectly.", "Are all of the clients from LinkedIn?  I thought some were from external pull requests.", "Looks like all of them has LinkedIn as Copyright holder in the LICENSE.", "But previously LinkedIn has applied LICENSE file incorrectly in Kafka source tree. AFAIK, the Copyright [yyyy] [name of copyright owner] template is only replaced if you want to add ASF 2.0 license to your source code.\n\nYou dont actually change the LICENSE file. Again please correct me if I am wrong.", "Hi @Jakob, so are you ok with the checkins, especially the copyright issue you brought up before?", "It's done now and we've clarified the correct procedure going forward, so it's fine."], "derived": {"summary": "Need to update Kafka code to include ASF standard source header: http://www. apache.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Change code header to follow standard ASF source header  - Need to update Kafka code to include ASF standard source header: http://www. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "It's done now and we've clarified the correct procedure going forward, so it's fine."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-94", "title": "KafkaServer can throw a NullPointerException during startup if zookeeper is down", "status": "Resolved", "priority": "Minor", "reporter": "Joel Jacob Koshy", "assignee": null, "labels": [], "created": "2011-08-12T01:17:20.000+0000", "updated": "2012-07-18T00:00:01.000+0000", "description": "Starting up KafkaServer when zookeeper is down can lead to a NullPointerException as shown below. The LogManager throws a ZkTimeoutException from startup if zookeeper is down. This is caught and we call shutdown. socketServer is uninitialized at this point, and hence the NPE. Will upload patch in a bit.\n\n2011/08/12 00:22:36.194 FATAL [KafkaServer] [pool-2-thread-1] [kafka] java.lang.NullPointerException\n\tat kafka.server.KafkaServer.shutdown(KafkaServer.scala:98)\n\tat kafka.server.KafkaServer.startup(KafkaServer.scala:84)\n\tat kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:40)\n\tat com.linkedin.kafka.KafkaStartable.start(KafkaStartable.java:49)\n\tat com.linkedin.spring.servlet.ComponentsContextLoaderListener.setServletContextAttributes(ComponentsContextLoaderListener.java:113)\n\tat com.linkedin.spring.servlet.ComponentsContextLoaderListener.contextInitialized(ComponentsContextLoaderListener.java:50)\n\tat org.mortbay.jetty.handler.ContextHandler.startContext(ContextHandler.java:549)\n\tat org.mortbay.jetty.servlet.Context.startContext(Context.java:136)\n\tat org.mortbay.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1282)\n\tat org.mortbay.jetty.handler.ContextHandler.doStart(ContextHandler.java:518)\n\tat org.mortbay.jetty.webapp.WebAppContext.doStart(WebAppContext.java:499)\n\tat org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)\n\tat com.linkedin.emweb.ContextBasedHandlerImpl.doStart(ContextBasedHandlerImpl.java:123)\n\tat org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)\n\tat com.linkedin.emweb.WebappDeployerImpl.start(WebappDeployerImpl.java:324)\n\tat com.linkedin.emweb.WebappDeployerImpl.deploy(WebappDeployerImpl.java:178)\n\tat com.linkedin.emweb.StateKeeperWebappDeployer.deploy(StateKeeperWebappDeployer.java:73)\n\tat com.linkedin.emweb.mbeans.WebappDeployerAdmin.deploy(WebappDeployerAdmin.java:86)\n\tat com.linkedin.emweb.mbeans.WebappDeployerAdmin$1.call(WebappDeployerAdmin.java:130)\n\tat com.linkedin.emweb.mbeans.WebappDeployerAdmin$1.call(WebappDeployerAdmin.java:127)\n\tat com.linkedin.emweb.mbeans.WebappDeployerAdmin$3.call(WebappDeployerAdmin.java:171)\n\tat java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:138)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n\tat java.lang.Thread.run(Thread.java:619)\n", "comments": ["Since the logmanager is also uninitialized, we should check for that as well.\n\n2011/08/12 00:22:36.191 FATAL [KafkaServer] [pool-2-thread-1] [kafka] org.I0Itec.zkclient.exception.ZkTimeoutException: Unable to connect to zookeeper server within timeout: 300000\n\tat org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)\n\tat org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)\n\tat org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)\n\tat kafka.server.KafkaZooKeeper.startup(KafkaZooKeeper.scala:45)\n\tat kafka.log.LogManager.<init>(LogManager.scala:86)\n\tat kafka.server.KafkaServer.startup(KafkaServer.scala:59)\n\tat kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:40)\n\tat com.linkedin.kafka.KafkaStartable.start(KafkaStartable.java:49)\n\tat com.linkedin.spring.servlet.ComponentsContextLoaderListener.setServletContextAttributes(ComponentsContextLoaderListener.java:113)\n\tat com.linkedin.spring.servlet.ComponentsContextLoaderListener.contextInitialized(ComponentsContextLoaderListener.java:50)\n\tat org.mortbay.jetty.handler.ContextHandler.startContext(ContextHandler.java:549)\n\tat org.mortbay.jetty.servlet.Context.startContext(Context.java:136)\n\tat org.mortbay.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1282)\n\tat org.mortbay.jetty.handler.ContextHandler.doStart(ContextHandler.java:518)\n\tat org.mortbay.jetty.webapp.WebAppContext.doStart(WebAppContext.java:499)\n\tat org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)\n\tat com.linkedin.emweb.ContextBasedHandlerImpl.doStart(ContextBasedHandlerImpl.java:123)\n\tat org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)\n\tat com.linkedin.emweb.WebappDeployerImpl.start(WebappDeployerImpl.java:324)\n\tat com.linkedin.emweb.WebappDeployerImpl.deploy(WebappDeployerImpl.java:178)\n\tat com.linkedin.emweb.StateKeeperWebappDeployer.deploy(StateKeeperWebappDeployer.java:73)\n\tat com.linkedin.emweb.mbeans.WebappDeployerAdmin.deploy(WebappDeployerAdmin.java:86)\n\tat com.linkedin.emweb.mbeans.WebappDeployerAdmin$1.call(WebappDeployerAdmin.java:130)\n\tat com.linkedin.emweb.mbeans.WebappDeployerAdmin$1.call(WebappDeployerAdmin.java:127)\n\tat com.linkedin.emweb.mbeans.WebappDeployerAdmin$3.call(WebappDeployerAdmin.java:171)\n\tat java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:138)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n\tat java.lang.Thread.run(Thread.java:619)", "+1. Thanks for the patch.", "Thanks, Joel. Just committed this.", "Should this be closed?", "Yes - this has been committed."], "derived": {"summary": "Starting up KafkaServer when zookeeper is down can lead to a NullPointerException as shown below. The LogManager throws a ZkTimeoutException from startup if zookeeper is down.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "KafkaServer can throw a NullPointerException during startup if zookeeper is down - Starting up KafkaServer when zookeeper is down can lead to a NullPointerException as shown below. The LogManager throws a ZkTimeoutException from startup if zookeeper is down."}, {"q": "What updates or decisions were made in the discussion?", "a": "Yes - this has been committed."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-95", "title": "Create Jenkins readable test output", "status": "Resolved", "priority": "Major", "reporter": "Chris Burroughs", "assignee": null, "labels": [], "created": "2011-08-12T13:55:07.000+0000", "updated": "2015-12-17T17:39:03.000+0000", "description": "Jenkinds likes XML.  See http://henkelmann.eu/2010/11/14/sbt_hudson_with_test_integration", "comments": ["Not a blocker.", "This was fixed a while back:\n\nhttps://builds.apache.org/job/kafka-trunk-jdk7/909/testReport/"], "derived": {"summary": "Jenkinds likes XML. See http://henkelmann.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Create Jenkins readable test output - Jenkinds likes XML. See http://henkelmann."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was fixed a while back:\n\nhttps://builds.apache.org/job/kafka-trunk-jdk7/909/testReport/"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-96", "title": "Add a style guide for kafka site", "status": "Closed", "priority": "Minor", "reporter": "Jay Kreps", "assignee": "Jay Kreps", "labels": [], "created": "2011-08-13T18:48:52.000+0000", "updated": "2011-08-16T16:33:42.000+0000", "description": "We have actual contributors now (yay!) so it might be good to consider having a style/coding guide that covers what is expected for new development or submitted patches. In my experience these guidelines are always a bit personal, and only work if there is real agreement amongst contributors. I have written up the guide which reflects at least the early code. I would love to get people's feedback on whether anything is missing or questionable and see if we could adopt these.", "comments": ["Here are the guidelines.", "That's a good list. I agree with most of the items. I want to add 1 more:\n* avoid duplicating code when possible.\n\nAbout enforcing backward compatibility. I agree that we should try to do that as much as we can. However, for major changes like replication. This is going to be hard. For example, an old client will not understand the concept of logical partitions, replicas of a partition, the leader of a partition, etc. One benefit of a streaming engine is that one can upgrade by creating a new instance of the brokers using the new version, upgrade the producer and point it to the new brokers, drain the old consumer and finally upgrade the consumer and point it to the new brokers. This should still allow minimal down time. Also, should backward compatibility be maintained just for the previous version or for all old versions? If it's the latter, the code may become harder and harder to maintain going forward.", "Cool, I will add that.\n\nI wanted to have the section on compatibility just because my experience previously is that people don't think about it at all. This is not to say we will never break compatibility, just that it is something that has to be thought through with each feature. I will try to clarify that.", "Patch is attached.", "We've not made much use of Scala's Option and instead have relied on Java's null.  Would it be worth adding that Option should be prefered in an effort to push the code to more idiomatic Scala?", "+1 on using Scala Option. It forces you to handle null values in a cleaner fashion\n\nI'd also like to add this in the context of logging -\n\n* prefer using string.format() instead of several string concatenations", "The issue with Option is that we heavily make use of java libraries which use null, so at best we will just get a mix of styles. I am fine recommending it.\n\nString.format is a good recommendation.", "Err, maybe not: http://stackoverflow.com/questions/513600/should-i-use-javas-string-format-if-performance-is-important\nSounds like the String.format doesn't get the same built in optimization that the + operator does so it is about 5x slower.", "Cool, I will take that to be consensus. I added all suggestions but am going to hold off on String.format since it sounds like there are pros and cons.", "I created KAFKA-105 to split off the logging discussion. I think we can have cleaner and faster code with slf4j.\n\nIs there a canonical scala guide you want to reference for indentation and stuff like that?\n\nBackwards compatibility:  I read that as one release alone the lines of 0.x.y and 0.x+1.y must be backwards compatibility.  But every 0.x.y and 0.y.y+n release must be.\n\nWe have some work to do if we want to eliminate all sleeps in tests:\n $ grep -rni sleep core/src/test/ | wc -l\n89\n\nExcited to see @threadsafe and friends enforced.\n\nNits:\n * There are four levels of logging TRACE, DEBUG, INFO, WARN, ERROR, and FATAL (That's more than 4)\n\n\n", "I found this as a sort of semi-standard scala style guide:\n  http://davetron5000.github.com/scala-style/ScalaStyleGuide.pdf\nI thought it was quite good. The only place I think we differ is that we do\n  while(true) {...}\ninstead of \n  while (true) {\n\nI don't particularly care one way or another on that one, but it would be good to have consistency.\n\nI recommend we adopt this guide.\n\nThe meaning of compatibility is really: there should always be a no-downtime upgrade path from release to release. I will clarify that.\n\nYes, the sleeps crept in. Overall i think we have added a lot of tests (good), but the quality has gone down a lot (lots of cut and paste, lots of hardcoded values, and lots of non-deterministic sleeps which fail about 25% of the time on my machine and are slow). I think we have some work to do. We can start by just not adding to them.\n\n", "Sounds good to me.  Thanks for putting this together.\n\nI'll try to start identifying the tests that fail spuriously most often in Jenkins.", "Updated the site with everyones suggestions. Thanks all.", "Page is up, we can do further suggestions as one-offs. Closing this issue."], "derived": {"summary": "We have actual contributors now (yay!) so it might be good to consider having a style/coding guide that covers what is expected for new development or submitted patches. In my experience these guidelines are always a bit personal, and only work if there is real agreement amongst contributors.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add a style guide for kafka site - We have actual contributors now (yay!) so it might be good to consider having a style/coding guide that covers what is expected for new development or submitted patches. In my experience these guidelines are always a bit personal, and only work if there is real agreement amongst contributors."}, {"q": "What updates or decisions were made in the discussion?", "a": "Page is up, we can do further suggestions as one-offs. Closing this issue."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-97", "title": "SocketServer.scala refers to Handler-specific variables", "status": "Closed", "priority": "Minor", "reporter": "Jay Kreps", "assignee": "Jun Rao", "labels": [], "created": "2011-08-14T04:28:35.000+0000", "updated": "2013-03-04T17:58:44.000+0000", "description": "There is meant to be a separation between the socket server and the kafka server. The socket server doesn't know anything about kafka, it is a set of classes that take a request handler implementation, reads requests and writes responses. It is like a 300 line version of netty. Some logging slipped in that is specific to the KafkaHandler implementation. This is a violation of concerns and should be fixed, it is kind of like the map/reduce framework referencing stuff in a particular map/reduce job. If we want to refactor the handlers to better be able to give the name to a particular request that is good, but nothing in kafka.network should depend on anything but kafka.utils. Having worked on projects where we screwed this up i am a little sensitive--the network stuff is complex enough even if we don't mix it in with kafka logic, we want it to be testable on its own.\n\nHere is the code (beginning on SocketServer.scala, line 267):\n    if(requestLogger.isTraceEnabled) {\n      requestTypeId match {\n        case RequestKeys.Produce =>\n          requestLogger.trace(\"Handling produce request from \" + channelFor(key).socket.getRemoteSocketAddress())\n        case RequestKeys.Fetch =>\n          requestLogger.trace(\"Handling fetch request from \" + channelFor(key).socket.getRemoteSocketAddress())\n        case RequestKeys.MultiFetch =>\n          requestLogger.trace(\"Handling multi-fetch request from \" + channelFor(key).socket.getRemoteSocketAddress())\n        case RequestKeys.MultiProduce =>\n          requestLogger.trace(\"Handling multi-produce request from \" + channelFor(key).socket.getRemoteSocketAddress())\n        case RequestKeys.Offsets =>\n          requestLogger.trace(\"Handling offset request from \" + channelFor(key).socket.getRemoteSocketAddress())\n        case _ => throw new InvalidRequestException(\"No mapping found for handler id \" + requestTypeId)\n      }\n    }", "comments": ["The code referenced no longer exists in 0.8. On quick inspection the only Kafka reference in SocketServer.scala is on line 185-186:\n      case e: SocketException =>\n        throw new KafkaException(\"Socket server failed to bind to %s:%d: %s.\".format(socketAddress.getHostName, port, e.getMessage), e)\n\nThis bug should be closed. Perhaps a new bug should be opened to fix the reference to KafkaException."], "derived": {"summary": "There is meant to be a separation between the socket server and the kafka server. The socket server doesn't know anything about kafka, it is a set of classes that take a request handler implementation, reads requests and writes responses.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "SocketServer.scala refers to Handler-specific variables - There is meant to be a separation between the socket server and the kafka server. The socket server doesn't know anything about kafka, it is a set of classes that take a request handler implementation, reads requests and writes responses."}, {"q": "What updates or decisions were made in the discussion?", "a": "The code referenced no longer exists in 0.8. On quick inspection the only Kafka reference in SocketServer.scala is on line 185-186:\n      case e: SocketException =>\n        throw new KafkaException(\"Socket server failed to bind to %s:%d: %s.\".format(socketAddress.getHostName, port, e.getMessage), e)\n\nThis bug should be closed. Perhaps a new bug should be opened to fix the reference to KafkaException."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-98", "title": "Unit tests hard code ports", "status": "Resolved", "priority": "Minor", "reporter": "Jay Kreps", "assignee": "Jay Kreps", "labels": [], "created": "2011-08-14T05:36:10.000+0000", "updated": "2011-08-15T17:34:35.000+0000", "description": "This doesn't work on a test server or on your own box if you have kafka running. Patch removes this.", "comments": ["Also check out the awesome assertion I fixed:\n\nassert(x - x < 300)\n\nWe had better hope that is true or we have bigger problems.", "+1. The patch looks good to me.", "Applied."], "derived": {"summary": "This doesn't work on a test server or on your own box if you have kafka running. Patch removes this.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Unit tests hard code ports - This doesn't work on a test server or on your own box if you have kafka running. Patch removes this."}, {"q": "What updates or decisions were made in the discussion?", "a": "+1. The patch looks good to me."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-99", "title": "SocketServer.scala does not enforce a maximum request size", "status": "Resolved", "priority": "Major", "reporter": "Jay Kreps", "assignee": "Jay Kreps", "labels": [], "created": "2011-08-14T07:30:42.000+0000", "updated": "2011-08-20T04:08:20.000+0000", "description": "The socket server should enforce a max request size to avoid running out of memory if a large request is sent. I see code in BoundedByteBufferReceive and in KafkaConfig to specify this, but it doesn't seem to be getting used. I added this in and added a stand-alone test for the socket server to start handling some of this stuff.", "comments": ["Shouldn't we instantiate SocketServer with the max size from KafkaConfig? Other than that, the patch looks good.", "Wups, somehow that got dropped from my patch, but should have been there. Here is the full patch.", "This includes the stuff i dropped from my original patch.", "+1"], "derived": {"summary": "The socket server should enforce a max request size to avoid running out of memory if a large request is sent. I see code in BoundedByteBufferReceive and in KafkaConfig to specify this, but it doesn't seem to be getting used.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "SocketServer.scala does not enforce a maximum request size - The socket server should enforce a max request size to avoid running out of memory if a large request is sent. I see code in BoundedByteBufferReceive and in KafkaConfig to specify this, but it doesn't seem to be getting used."}, {"q": "What updates or decisions were made in the discussion?", "a": "This includes the stuff i dropped from my original patch."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-100", "title": "ProducerShell should use high-level producer instead of SyncProducer", "status": "Closed", "priority": "Major", "reporter": "Jun Rao", "assignee": null, "labels": [], "created": "2011-08-14T15:34:19.000+0000", "updated": "2013-06-13T16:28:04.000+0000", "description": "We should change ProducerShell to use high-level producer instead of SyncProducer.", "comments": ["Moving to 0.8", "This is fixed since we consolidated on ConsoleProducer."], "derived": {"summary": "We should change ProducerShell to use high-level producer instead of SyncProducer.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "ProducerShell should use high-level producer instead of SyncProducer - We should change ProducerShell to use high-level producer instead of SyncProducer."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is fixed since we consolidated on ConsoleProducer."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-101", "title": "Avoid creating a new topic by the consumer", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": null, "labels": ["patch"], "created": "2011-08-15T03:44:03.000+0000", "updated": "2011-11-23T23:11:38.000+0000", "description": "Currently, if  a consumer consumes a topic and the topic doesn't exist, the topic is created automatically. Sometimes this can be confusing. Often, an ad hoc user may put a wrong topic name in the consumer and thus create many unnecessary topics.", "comments": ["Fixes KAFKA-101 but the core tests for ZK fail - I think because other tests are expecting a getOrCreate call to connect to ZK and it no longer does that.", "Thanks for the patch.\n\nThe test failures seem to be transient and are the results of time dependencies, which we should fix separately.\n\nCould you add a unit test on SimpleConsumer to consume a non-existing topic/partition? It should return an empty messageSet. Other than that, the patch looks good. ", "I added a test for the empty log in LogManager.  You want a similar test in SimpleConsumer also?", "Yes, since a test in SimpleConsumer will further test the wire protocol. Thanks,", "sorry, I am not seeing a SimpleConsumer test.  Do you have a specific file in mind?  Maybe log/LogOffsetTest.scala?", "We can add it in PrimitiveApiTest.", "Actually another thing. If a topic doesn't already exist, the consumer shouldn't create the ZK path /broker/topics/topic. Currently, we do the following in ZookeeperConsumerConnector.consume\n\n      ZkUtils.makeSurePersistentPathExists(zkClient, partitionPath)\n\nWe should remove that line. There are a couple of things to check.\n(1) If the consumer can still get the watcher triggered when the topic is created (by a producer). I think this should happen since ZKClient registers an Exists watcher if path doesn't exist. But we should check.\n(2) There are probably places in the rebalance code where we should treat missing the ZK path  /broker/topics/topic properly.", "Attache patch v2. Took Taylor's patch and added (1) a SimpleConsumer level test in PrimitiveApiTest (2) don't let consumer create the topic path under /brokers/topics (only producer can create it).", "Taylor, could you review patch v2? Thanks,", "I realized on Friday that I had neglected to fix getOffsets.  So calling getOffsets on a non-existent topic creates a file too.  I wrote the fix yesterday - I will see if I can merge the two together and post an updated patch with tests.  \n", "Ok - your commit looks good.  I've added the get offsets patch which should be applied after yours.", "Merged the v2 patch and the getoffset patch into a v3 patch.\n1. get rid of unused method LogManager.getLogOrGenerateFake\n2. If the topic hasn't been created, probably only getting the latest or the earliest offset make sense. Otherwise, we should return an empty array.\n3. Optimized the import on several files.", "lgtm - I tried to make Log.getLog(...) more scala like and have it return Option[Log] but that somehow caused other tests to fail, my scala fu is not yet strong.", "Thanks, Taylor. Just committed this.", "great! :)", "So - I rolled out my version of this patch which was applied to 0.6.  In dev and stage, it worked just great.   In production, we it appears we ended up with a very mysterious occurrence.  \n\nTopics appeared to be created and the first message delivered was corrupted.  Instead of having the usual binary data at the start of a topic file, the beginning of the file held the partial contents of the first message.\n\nI'm going to start looking into how that might even be possible - I thought to post here in case in other stages of development maybe you saw this problem and/or fixed it which might help me pinpoint which part of the code could be responsible that I mistakenly broken.\n\nTo be clear, if the first message delivered was let's say \"One fine day in the middle of the night\" then the contents of our topic files appeared as so:\n\n-----------------------------------\nay in the middle of the night\n------------------------------------\n\nWhen it should have been something like:\n---------------------------------\n[binary stuff]One fine day in the middle of the night\n------------------------------------", "Taylor, this would suggest a synchronization issue during append to the log. Looking at the code, the append is always synchronized on the lock of the segment file. Could you reproduce the problem on 0.7? ", "Unfortunately, I cannot.  I have gone ahead and updated our clients to speak 0.7 properly so we are going to move soon.  However it's going to be hard to justify to roll out just 0.7 with this patch without any changes \"just to see\" if the problem resurfaces.  We might go ahead and do it on one machine.  Anyway that will have to wait until at best sometime next week since we will have to rollout updated clients that can speak 0.6 and 0.7 first."], "derived": {"summary": "Currently, if  a consumer consumes a topic and the topic doesn't exist, the topic is created automatically. Sometimes this can be confusing.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Avoid creating a new topic by the consumer - Currently, if  a consumer consumes a topic and the topic doesn't exist, the topic is created automatically. Sometimes this can be confusing."}, {"q": "What updates or decisions were made in the discussion?", "a": "Unfortunately, I cannot.  I have gone ahead and updated our clients to speak 0.7 properly so we are going to move soon.  However it's going to be hard to justify to roll out just 0.7 with this patch without any changes \"just to see\" if the problem resurfaces.  We might go ahead and do it on one machine.  Anyway that will have to wait until at best sometime next week since we will have to rollout updated clients that can speak 0.6 and 0.7 first."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-102", "title": "Shutting down Kafka should be FATAL, not ERROR", "status": "Resolved", "priority": "Minor", "reporter": "Blake Matheny", "assignee": null, "labels": [], "created": "2011-08-15T16:45:35.000+0000", "updated": "2011-08-15T17:32:04.000+0000", "description": "When Kafka encounters an unrecoverable error it generates an error level log record and then calls Runtime.getRuntime.halt(1). This should really be fatal, not an error.", "comments": ["Thanks for catching this! While we are cleaning up we should fix the error messages. The capitalization is wrong and they reference internal method names, which makes them sysadmin unfriendly.", "Updated the patch to at least cleanup the error/fatal logging in those two methods according to the kafka style guide.", "+1", "Applied."], "derived": {"summary": "When Kafka encounters an unrecoverable error it generates an error level log record and then calls Runtime. getRuntime.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Shutting down Kafka should be FATAL, not ERROR - When Kafka encounters an unrecoverable error it generates an error level log record and then calls Runtime. getRuntime."}, {"q": "What updates or decisions were made in the discussion?", "a": "Updated the patch to at least cleanup the error/fatal logging in those two methods according to the kafka style guide."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-103", "title": "Make whitelist/blacklist mirror configs more consistent", "status": "Resolved", "priority": "Minor", "reporter": "Joel Jacob Koshy", "assignee": "Jun Rao", "labels": [], "created": "2011-08-15T19:12:56.000+0000", "updated": "2011-11-14T00:40:04.000+0000", "description": "The blacklist config for kafka mirrors is a comma separated list of topics. However, the whitelist config is a comma-separated list of \"topics:numthreads\" pairs, which allows for a multi-threaded consumer in the mirror. It will be good to keep the two configs consistent in format. So, we can make the whitelist config a comma-separated list of topics and provide a config (say, kafka.mirror.consumer.threads) that will specify the number of threads to use for all topics in the whitelist (if present).\n", "comments": ["Hmm.. looks like the mirror topics patch actually ignored the numthreads in the whitelist. In any event, this patch allows a global numthreads for the embedded consumer.", "Sorry about deleting that.. I forgot to update the property in system test.", "Thanks, Joel. Just committed this.", "The behavior of the mirror.topics.whitelist is backwards incompatible. If some users currently have a whitelist setup with the old format (topic1:numPartitions1, topic2:numPartitions2), it is ideal to just rename \"embedded.consumer.topics\" to \"mirror.topics.whitelist\". Instead, this change forces those users to change their white-listed topics list to strip off the number of partitions. It would be ideal if the number of partitions is just ignored, with a little warning message. That way the change is backwards compatible.\n\nIf a user just renames \"embedded.consumer.topics\" to \"mirror.topics.whitelist\" and leaves the number of partitions, currently the corp replica behavior is to not mirror anything at all. That is unintuitive. ", "Note that this is an incremental patch.\n\nThat is a good point - i.e., we have told clients that we are changing the old whitelist config to mirror.topics.whitelist. If they simply do a rename then nothing is going to get mirrored, so I agree that backward-compatibility is important.\n\nbtw, I think a better fix (that would also help KAFKA-104) is to disallow invalid topics, but it would be better to think about that more carefully.\n", "I actually don't think that we need to make this backward compatible because the # of threads specified in the old format will be ignored in the new config. If we support the old format, users will assume that the number of consumer threads is in effect. It's better to make this incompatible so the users can realize there is a problem.", "At least per kafka-103-patch.v1 it is not totally ignored. So if the whitelist config contains: SomeTopic:1 then SomeTopic will not get mirrored because it will fail the isTopicAllowed filter in KafkaServerStartable. i.e., no topics will get mirrored.", "Jun, even if we don't want to support the older format, the behavior of silently not mirroring any data is also not acceptable. What would be helpful is to either log it as a warning or throw an InvalidConfigException. ", "However, we haven't officially released anything that exposes whitelist in the config. We are just iterating in trunk.", "In summary, I think it's simpler if we enforce a single format for any property value. In this case, to help users identify problems, we can probably log the whitelisted and blacklisted topics, if any.", "Is this complete?", "I think the concerns raised here can be addressed with clear documentation on how mirroring is done."], "derived": {"summary": "The blacklist config for kafka mirrors is a comma separated list of topics. However, the whitelist config is a comma-separated list of \"topics:numthreads\" pairs, which allows for a multi-threaded consumer in the mirror.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Make whitelist/blacklist mirror configs more consistent - The blacklist config for kafka mirrors is a comma separated list of topics. However, the whitelist config is a comma-separated list of \"topics:numthreads\" pairs, which allows for a multi-threaded consumer in the mirror."}, {"q": "What updates or decisions were made in the discussion?", "a": "I think the concerns raised here can be addressed with clear documentation on how mirroring is done."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-104", "title": "Invalid topics prevent broker start-up", "status": "Closed", "priority": "Minor", "reporter": "Joel Jacob Koshy", "assignee": null, "labels": [], "created": "2011-08-15T23:03:09.000+0000", "updated": "2013-03-06T00:48:00.000+0000", "description": "Excerpt from a trace we saw today. If there is a topic directory with an invalid name, then the registerMBean call throws an exception which eventually results in a start failure. Also, if you try sending messages with an invalid topic to a broker, the broker's logs get flooded with these exceptions. Easy fix is to wrap the registerMBean with Utils.swallow; and probably prevent ProducerRequest from accepting invalid topics.\n\n2011/08/15 20:43:50.834 FATAL [KafkaServer] [main] [kafka] javax.management.MalformedObjectNameException: Invalid character '\"' in value part of property\n        at javax.management.ObjectName.construct(ObjectName.java:602)\n        at javax.management.ObjectName.<init>(ObjectName.java:1403)\n        at kafka.utils.Utils$.registerMBean(Utils.scala:372)\n        at kafka.log.Log.<init>(Log.scala:120)\n        at kafka.log.LogManager$$anonfun$2.apply(LogManager.scala:70)\n        at kafka.log.LogManager$$anonfun$2.apply(LogManager.scala:65)\n        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)\n        at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:34)\n        at kafka.log.LogManager.<init>(LogManager.scala:65)\n        at kafka.server.KafkaServer.startup(KafkaServer.scala:60)\n        at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:46)\n        at com.linkedin.kafka.KafkaStartable.start(KafkaStartable.java:54)", "comments": ["I am unable to reproduce this issue.\n\nIt is likely this was fixed in another issue such as: https://issues.apache.org/jira/browse/KAFKA-495\n"], "derived": {"summary": "Excerpt from a trace we saw today. If there is a topic directory with an invalid name, then the registerMBean call throws an exception which eventually results in a start failure.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Invalid topics prevent broker start-up - Excerpt from a trace we saw today. If there is a topic directory with an invalid name, then the registerMBean call throws an exception which eventually results in a start failure."}, {"q": "What updates or decisions were made in the discussion?", "a": "I am unable to reproduce this issue.\n\nIt is likely this was fixed in another issue such as: https://issues.apache.org/jira/browse/KAFKA-495"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-105", "title": "switch to using slf4j", "status": "Resolved", "priority": "Major", "reporter": "Chris Burroughs", "assignee": "Chris Burroughs", "labels": [], "created": "2011-08-16T13:32:10.000+0000", "updated": "2013-03-05T17:14:10.000+0000", "description": "Breaking out discussion from KAFKA-96.  \n\nslf4j has a cleaner (and faster) way of not logging: http://www.slf4j.org/faq.html#logging_performance that avoid both the redundant if's and string con-catting.  slf4j is just an api, we would still use log4j for the actual logging in the standalone server.  This also makes it easier on downstream projects who are not using log4j.  The downside is that java logging is notoriously complicated, and now we would have two logging frameworks that can break.\n\nDiscussion from other projects; ZOOKEEPER-850, SOLR-560 HBASE-2608, CASSANDRA-625, THRIFT-558", "comments": ["For my part I have always felt that since everyone uses log4j, a wrapper around that was simply a bridge too far. The inline arguments are nice, but for some reason it only extends to 2 arguments and more than that allocates an array (as varargs would). I could go either way on this.\n\nAnother approach would be to just use String.format and omit the Logger.isDebugEnabled check.", "It appears that due to lack of discussion this bug should be closed as wont fix.", "Since we now have the Logger class, which gives us most of the benefits of slf4j api, we can resolve this jira for now."], "derived": {"summary": "Breaking out discussion from KAFKA-96. slf4j has a cleaner (and faster) way of not logging: http://www.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "switch to using slf4j - Breaking out discussion from KAFKA-96. slf4j has a cleaner (and faster) way of not logging: http://www."}, {"q": "What updates or decisions were made in the discussion?", "a": "Since we now have the Logger class, which gives us most of the benefits of slf4j api, we can resolve this jira for now."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-106", "title": "Include rolledup segment size stats via jmx", "status": "Open", "priority": "Minor", "reporter": "Chris Burroughs", "assignee": "UTKARSH BHATNAGAR", "labels": [], "created": "2011-08-16T16:24:45.000+0000", "updated": "2020-05-27T20:54:56.000+0000", "description": "We already have size for a each topic-partition pair.  From the user list it looks like it would be helpful to also include entire size for a topic, and size for all topics.", "comments": ["[~junrao] - I would like to submit a PR for this. Can you please assign this to me?\n\nAlso, I have written KafkaWriter for JMXTrans:\nhttps://github.com/jmxtrans/jmxtrans/tree/master/jmxtrans-output/jmxtrans-output-kafka", "[~utkarshcmu], just added you to the contributor list and you should be able to assign the jira to yourself.", "Is this old ticket still valid? Can/should we close it?"], "derived": {"summary": "We already have size for a each topic-partition pair. From the user list it looks like it would be helpful to also include entire size for a topic, and size for all topics.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Include rolledup segment size stats via jmx - We already have size for a each topic-partition pair. From the user list it looks like it would be helpful to also include entire size for a topic, and size for all topics."}, {"q": "What updates or decisions were made in the discussion?", "a": "Is this old ticket still valid? Can/should we close it?"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-107", "title": "Bug in serialize and collate logic in the DefaultEventHandler", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": null, "labels": [], "created": "2011-08-17T03:02:09.000+0000", "updated": "2011-09-13T01:27:19.000+0000", "description": "There is a bug in the serialize and collate in the DefaultEventHandler, that uses the map() API on a hashmap to convert a sequence of messages to a ByteBufferMessageSet, based on the compression configs. The usage of the zip() API after the map() API on a hashmap, has the side effect of reordering the mapping between the keys and the values. ", "comments": ["This patch refactors the code in the DefaultEventHandler serialize API to fix the reordering bug", "Ok, so the problem was that the code was trying to zip a map (topicsAndPartitions) and a list (messages). Even though they are derived from the same map, there is no guarantee that the map is going to be iterated in the same order as the list. So, in the future, we need to be careful about using zip. We need to make sure that the two collections to be zipped have well defined ordering (e.g., list) for iteration and the ordering is what we want. Could you double check other usage of zip introduced in the compression patch?\n\nThe patch looks fine. However, could you also add a unit test (probably just at the serialize() level) that exposes this problem?", "Another thing, please remove unreferenced package imports.", "Jun, the code is not  zipping a map with a list. That won't even compile. The code is just zipping a sequence of tuples with a sequence of ByteBufferMessageSet. The problem is that one sequence is generated using the map API on the original HashMap and another sequence is generated using the map API on another HashMap. So ordering of data across those sequences is not guaranteed. Hence, a zip on those sequences will not associate the keys with the correct values.\n\nI will add a unit test to cover this, and remove the unreferenced package imports.", "Adding a unit test to the test suite for AsyncProducer that catches the reordering bug in serialize() API of DefaultEventHandler", "+1"], "derived": {"summary": "There is a bug in the serialize and collate in the DefaultEventHandler, that uses the map() API on a hashmap to convert a sequence of messages to a ByteBufferMessageSet, based on the compression configs. The usage of the zip() API after the map() API on a hashmap, has the side effect of reordering the mapping between the keys and the values.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Bug in serialize and collate logic in the DefaultEventHandler - There is a bug in the serialize and collate in the DefaultEventHandler, that uses the map() API on a hashmap to convert a sequence of messages to a ByteBufferMessageSet, based on the compression configs. The usage of the zip() API after the map() API on a hashmap, has the side effect of reordering the mapping between the keys and the values."}, {"q": "What updates or decisions were made in the discussion?", "a": "Adding a unit test to the test suite for AsyncProducer that catches the reordering bug in serialize() API of DefaultEventHandler"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-108", "title": "A new unit test for ByteBufferMessageSet iterator", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Jun Rao", "labels": [], "created": "2011-08-17T17:39:53.000+0000", "updated": "2011-09-12T16:43:46.000+0000", "description": "We need to add a unit test that iterates ByteBufferMessageSet with compression and without. Also, test that ByteBufferMessageSet can be iterated multiple times.", "comments": ["Patch attached.", "Attaching patch v2. The unit test fails and exposes a bug on handling compressed empty ByteBufferMessageSet.", "+1. All tests look good."], "derived": {"summary": "We need to add a unit test that iterates ByteBufferMessageSet with compression and without. Also, test that ByteBufferMessageSet can be iterated multiple times.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "A new unit test for ByteBufferMessageSet iterator - We need to add a unit test that iterates ByteBufferMessageSet with compression and without. Also, test that ByteBufferMessageSet can be iterated multiple times."}, {"q": "What updates or decisions were made in the discussion?", "a": "+1. All tests look good."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-109", "title": "CompressionUtils introduces a GZIP header while compressing empty message sets", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": null, "labels": [], "created": "2011-08-18T09:10:53.000+0000", "updated": "2011-09-13T01:27:39.000+0000", "description": "The CompressionUtils helper class takes in a sequence of messages and compresses those, using the appropriate codec. But even if it receives an empty sequence, it still ends up adding a GZIP compression header to the data, efffectively \"adding\" data to the resulting ByteBuffer. This doesn't match with the behavior for uncompressed empty message sets. CompressionUtils should be fixed by removing this side-effect.", "comments": ["This patch handles the behavior of ByteBufferMessageSet for compression of empty list of messages. This modifies the ByteBufferMessageSet to create an empty byte buffer, in this case, instead of attaching a GZIP header to it. There are a couple of reasons to do this -\n\n1. To maintain consistent behavior between an empty uncompressed message set and an empty compressed message set\n2. To avoid attaching extraneous header information to non-existing data, effectively occupying space on disk", "+1. The patch looks good.", "Actually, this doesn't cover javaapi.ByteBufferMessageSet. It seems that javaapi.ByteBufferMessageSet duplicates some of the constructor code in ByteBufferMessageSet. We should avoid doing that.", "This is a revised patch that refactors the constructors of both java and scala ByteBufferMessageSet into a common API in MessageSet. This ensures that the bug fix exists both in the Java API as well as the Scala API", "+1"], "derived": {"summary": "The CompressionUtils helper class takes in a sequence of messages and compresses those, using the appropriate codec. But even if it receives an empty sequence, it still ends up adding a GZIP compression header to the data, efffectively \"adding\" data to the resulting ByteBuffer.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "CompressionUtils introduces a GZIP header while compressing empty message sets - The CompressionUtils helper class takes in a sequence of messages and compresses those, using the appropriate codec. But even if it receives an empty sequence, it still ends up adding a GZIP compression header to the data, efffectively \"adding\" data to the resulting ByteBuffer."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is a revised patch that refactors the constructors of both java and scala ByteBufferMessageSet into a common API in MessageSet. This ensures that the bug fix exists both in the Java API as well as the Scala API"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-110", "title": "Bug in the collate logic of the DefaultEventHandler dispatches empty list of messages using the producer", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": null, "labels": [], "created": "2011-08-18T10:39:44.000+0000", "updated": "2011-09-13T01:27:58.000+0000", "description": "The collate logic in the DefaultEventHandler is designed to batch together requests for a single topic and partition in order to send it to the server in a single request. In this collate logic, the use of the partition API might give back an empty sequence of data for a particular topic,partition pair. It is useless to add it to the list of data to be sent, and it should avoid making network requests.", "comments": ["This patch corrects the behavior of the collate API to avoid adding empty sequences of data to the list of events to be sent using the producer.\n\nAlso, it modifies the ProducerSendThread so that it never dispatches an empty list of events to the event handler.", "+1 (please remove unreferenced imports before checking in)"], "derived": {"summary": "The collate logic in the DefaultEventHandler is designed to batch together requests for a single topic and partition in order to send it to the server in a single request. In this collate logic, the use of the partition API might give back an empty sequence of data for a particular topic,partition pair.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Bug in the collate logic of the DefaultEventHandler dispatches empty list of messages using the producer - The collate logic in the DefaultEventHandler is designed to batch together requests for a single topic and partition in order to send it to the server in a single request. In this collate logic, the use of the partition API might give back an empty sequence of data for a particular topic,partition pair."}, {"q": "What updates or decisions were made in the discussion?", "a": "+1 (please remove unreferenced imports before checking in)"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-111", "title": "A bug in the iterator of the ByteBufferMessageSet returns incorrect offsets when it encounters a compressed empty message set", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": null, "labels": [], "created": "2011-08-18T10:56:56.000+0000", "updated": "2011-09-13T01:28:52.000+0000", "description": "The deep iterator logic in the ByteBufferMessageSet returns incorrect offsets when it encounters empty compressed data. Ideally, it should be able to decompress the data, figure out that it is somehow empty, skip it and proceed to decoding rest of the data. To make this possible, the manner in which we update the offset to be returned by the iterator, needs to be tweaked.", "comments": ["This patch corrects the offset management in the deep iterator of the ByteBufferMessageSet, to handle the case of compressed empty message sets.", "1. Please remove unreferenced imports before checking in.\n2. The following unit test now hangs.\nTest Starting: testCompression(kafka.consumer.ZookeeperConsumerConnectorTest)", "The patch in kafka-108 also identifies this problem. The problem is that we need to set the new offset in 2 cases: (1) if the compressed unit is empty; (2) otherwise, the last message of the compressed unit is being iterated. This patches covers (1), but not (2). One solution is probably to use the code that we had before, but treat case (1) specially, i.e., we won't even create an inner iterator and just advance the offset.", "How about patch v2? It currently breaks PrimitiveApiTest. However, the test probably should be fixed since it includes an empty message set.", "Yes, looks like patch v2 is a better way of handling the above 2 cases. We should check what breaks with PrimitiveApiTest though, maybe just a test bug.", "Patch v3. Fix test in kafka.javaapi.message.ByteBufferMessageSetTest.\n\nPrimitiveApiTest for javaapi still fails. However, it should pass after kafka-109 is fixed."], "derived": {"summary": "The deep iterator logic in the ByteBufferMessageSet returns incorrect offsets when it encounters empty compressed data. Ideally, it should be able to decompress the data, figure out that it is somehow empty, skip it and proceed to decoding rest of the data.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "A bug in the iterator of the ByteBufferMessageSet returns incorrect offsets when it encounters a compressed empty message set - The deep iterator logic in the ByteBufferMessageSet returns incorrect offsets when it encounters empty compressed data. Ideally, it should be able to decompress the data, figure out that it is somehow empty, skip it and proceed to decoding rest of the data."}, {"q": "What updates or decisions were made in the discussion?", "a": "Patch v3. Fix test in kafka.javaapi.message.ByteBufferMessageSetTest.\n\nPrimitiveApiTest for javaapi still fails. However, it should pass after kafka-109 is fixed."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-112", "title": "Improve the command line tools in the bin directory to use the compression feature correctly", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": null, "labels": [], "created": "2011-08-18T20:35:39.000+0000", "updated": "2011-09-13T01:29:42.000+0000", "description": "This ticket is filed to improve various command line tools -\n\n1. All tools need to use the new Iterator API for ByteBufferMessageSet that returns MessageAndOffset\n2. A bug in ProducerPerformance does not set the compression codec correctly.\n3. The simple consumer shell lacks some useful command line options and needs better logging\n\n", "comments": ["1. DumpLogSegments currently returns the absolute offset, which is ideal. With the patch, it returns relative offset within a file. This will be confusing if a file starts at an offset other than 0.\n2. The patch doesn't seem to apply on trunk. Could you rebase?", "Jun,\n\nThat makes sense. I reverted that change to the DumpLogSegments utility.\n\nThanks,\nNeha", "The patch looks good. The patch doesn't apply because you moved DumpLogSegment from the util to tools (which is good)."], "derived": {"summary": "This ticket is filed to improve various command line tools -\n\n1. All tools need to use the new Iterator API for ByteBufferMessageSet that returns MessageAndOffset\n2.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Improve the command line tools in the bin directory to use the compression feature correctly - This ticket is filed to improve various command line tools -\n\n1. All tools need to use the new Iterator API for ByteBufferMessageSet that returns MessageAndOffset\n2."}, {"q": "What updates or decisions were made in the discussion?", "a": "The patch looks good. The patch doesn't apply because you moved DumpLogSegment from the util to tools (which is good)."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-113", "title": "test log4j properties shouldn't be turned off", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Chris Burroughs", "labels": [], "created": "2011-08-19T22:49:35.000+0000", "updated": "2011-08-31T16:57:03.000+0000", "description": "core/src/test/resources/log4j.properties is set to off. We should change it to error. Also, this file doesn't seem to be picked up in unit test since there are lots of info level logging in the output.", "comments": ["- Turns logging for tests on by default\n- I used WARN, that appears to result in very little output\n- Adds line numbers\n-  For me setting log4j.logger.kafka=TRACE results in a torrent of output.", "+1. Thanks for the patch, Chris."], "derived": {"summary": "core/src/test/resources/log4j. properties is set to off.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "test log4j properties shouldn't be turned off - core/src/test/resources/log4j. properties is set to off."}, {"q": "What updates or decisions were made in the discussion?", "a": "+1. Thanks for the patch, Chris."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-114", "title": "Replace tabs with spaces", "status": "Resolved", "priority": "Trivial", "reporter": "Jay Kreps", "assignee": "Jay Kreps", "labels": [], "created": "2011-08-20T04:44:35.000+0000", "updated": "2011-10-04T02:26:57.000+0000", "description": "Because I am obsessive compulsive.", "comments": ["Patch was generated with the following script:\n\n#!/bin/bash\nfiles=`grep -lR '\t' $1 | grep -v svn`\nfor file in $files;\ndo\n  echo \"Fixing $file\";\n  mv $file $file.bak;\n  expand -t 2 $file.bak > $file;\n  rm $file.bak\ndone", "+1"], "derived": {"summary": "Because I am obsessive compulsive.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Replace tabs with spaces - Because I am obsessive compulsive."}, {"q": "What updates or decisions were made in the discussion?", "a": "Patch was generated with the following script:\n\n#!/bin/bash\nfiles=`grep -lR '\t' $1 | grep -v svn`\nfor file in $files;\ndo\n  echo \"Fixing $file\";\n  mv $file $file.bak;\n  expand -t 2 $file.bak > $file;\n  rm $file.bak\ndone"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-115", "title": "Kafka server access log does not log request details coming from a MultiProduceRequest", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": null, "labels": [], "created": "2011-08-23T01:51:02.000+0000", "updated": "2011-09-13T01:30:19.000+0000", "description": "the access logger logic on the kafka server has a bug, that doesn't log the individual produce request that are part of a MultiProduceRequest. ", "comments": ["Please ignore the previous patch. That could lead to redundant logging.", "V2 looks good. +1"], "derived": {"summary": "the access logger logic on the kafka server has a bug, that doesn't log the individual produce request that are part of a MultiProduceRequest.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Kafka server access log does not log request details coming from a MultiProduceRequest - the access logger logic on the kafka server has a bug, that doesn't log the individual produce request that are part of a MultiProduceRequest."}, {"q": "What updates or decisions were made in the discussion?", "a": "V2 looks good. +1"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-116", "title": "AsyncProducer shutdown logic causes data loss", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": null, "labels": [], "created": "2011-08-23T07:55:04.000+0000", "updated": "2011-09-13T01:30:52.000+0000", "description": "The current shutdown logic of the AsyncProducer allows adding events to the queue, after adding the shutdown command to it. The ProducerSendThread drains all the data in the queue until it hits the shutdown command. Hence, all data added after the shutdown command is lost.", "comments": ["1. Fixed the shutdown bug by setting the closed queue flag before adding the shutdown command to the async producer queue\n2. Added more logging at the trace level \n3. Refactored the blocking async producer logic to use case match instead of nested if else", "+1", "IllegalQueueStateException was missed in the commit.", "thanks for pointing that out. I checked it in now."], "derived": {"summary": "The current shutdown logic of the AsyncProducer allows adding events to the queue, after adding the shutdown command to it. The ProducerSendThread drains all the data in the queue until it hits the shutdown command.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "AsyncProducer shutdown logic causes data loss - The current shutdown logic of the AsyncProducer allows adding events to the queue, after adding the shutdown command to it. The ProducerSendThread drains all the data in the queue until it hits the shutdown command."}, {"q": "What updates or decisions were made in the discussion?", "a": "thanks for pointing that out. I checked it in now."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-117", "title": "The FetcherRunnable busy waits on empty fetch requests ", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": null, "labels": [], "created": "2011-08-23T21:42:40.000+0000", "updated": "2011-09-13T01:30:02.000+0000", "description": "The FetcherRunnable busy waits on empty fetch requests by skipping the backoff logic. This can fill up the disk space due to the public access log being filled up. Also the CPU usage shoots up to 100%. ", "comments": ["The bug was in the shallowValidBytes logic where it returns a negative value when dealing with an empty fetch request.\n\nCorrected the logic of shallowValidBytes to return 0 when there is no data in the ByteBufferMessageSet created from an empty fetch request.", "We probably should just have a method validBytes that returns the valid bytes in the original message (ie, if the message is compressed, the bytes refer to the compressed bytes). There is no real need for knowing the total valid bytes on a compressed message set.", "I think your suggestion makes sense. The deepValidBytes method is useless and never used, even by the unit tests. It is better to have just a single API for calculating valid bytes.", "Deleted the deepValidBytes API. Made the shallowValidBytes API private and had the validBytes API point to shallowValidBytes.\n\nvalidBytes() API will always give you the number of compressed bytes in the ByteBufferMessageSet, if it is compressed. ", "Includes all changes from v2 patch + a unit test to cover the valid bytes for empty message sets", "+1 on v3."], "derived": {"summary": "The FetcherRunnable busy waits on empty fetch requests by skipping the backoff logic. This can fill up the disk space due to the public access log being filled up.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "The FetcherRunnable busy waits on empty fetch requests  - The FetcherRunnable busy waits on empty fetch requests by skipping the backoff logic. This can fill up the disk space due to the public access log being filled up."}, {"q": "What updates or decisions were made in the discussion?", "a": "Includes all changes from v2 patch + a unit test to cover the valid bytes for empty message sets"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-118", "title": "Producer performance tool should use the new blocking async producer instead of the sleep timeout hack", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": null, "labels": [], "created": "2011-08-24T00:13:01.000+0000", "updated": "2011-09-13T01:31:09.000+0000", "description": "Before the blocking async producer feature, if you add new data to the async producer queue faster than it can handle, then it throws QueueFullException. To get around this in the producer performance tool, we had added a custom sleep option. This ends up reducing the producer throughput and  also slows down the system test. After KAFKA-74, we have the option of blocking on the async producer, instead of throwing QueueFullException. This is useful to run stress and performance tests.", "comments": ["Modified the perf and system tools to use the blocking async producer, so that all system tests run successfully and fast.", "+1 for the patch. The code in ProducerPerformance probably can be refactored better. Will create a separate jira."], "derived": {"summary": "Before the blocking async producer feature, if you add new data to the async producer queue faster than it can handle, then it throws QueueFullException. To get around this in the producer performance tool, we had added a custom sleep option.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Producer performance tool should use the new blocking async producer instead of the sleep timeout hack - Before the blocking async producer feature, if you add new data to the async producer queue faster than it can handle, then it throws QueueFullException. To get around this in the producer performance tool, we had added a custom sleep option."}, {"q": "What updates or decisions were made in the discussion?", "a": "+1 for the patch. The code in ProducerPerformance probably can be refactored better. Will create a separate jira."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-119", "title": "Avoid duplicated code in ProducerPerformance", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": null, "labels": [], "created": "2011-08-24T00:29:03.000+0000", "updated": "2016-04-18T05:14:57.000+0000", "description": "ProducerPerformance has a SyncProducerThread and an AsyncProducerThread. Much of the code is duplicated. We should look at how to refactor the code to avoid duplication.", "comments": ["ProducerPerformance.scala is deprecated.  New ProducerPerformance tool is introduced to use new Producer Client. "], "derived": {"summary": "ProducerPerformance has a SyncProducerThread and an AsyncProducerThread. Much of the code is duplicated.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "Avoid duplicated code in ProducerPerformance - ProducerPerformance has a SyncProducerThread and an AsyncProducerThread. Much of the code is duplicated."}, {"q": "What updates or decisions were made in the discussion?", "a": "ProducerPerformance.scala is deprecated.  New ProducerPerformance tool is introduced to use new Producer Client."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-120", "title": "Code clean up in FetcherRunnable and ZookeeperConsumerConnector", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Jun Rao", "labels": [], "created": "2011-08-24T00:55:43.000+0000", "updated": "2011-09-13T01:31:41.000+0000", "description": "Add more logging info and remove unused code.", "comments": ["patch attached.", "There is a problem with using the logger.warn(Throwable) API. It won't log the entire stack trace of the error. You will need to use the logger.warn(\"custom message\", Throwable) API to be able to log the entire stack trace", "Fixed the issue. Loaded patch v2.", "+1.\nLooks good. "], "derived": {"summary": "Add more logging info and remove unused code.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Code clean up in FetcherRunnable and ZookeeperConsumerConnector - Add more logging info and remove unused code."}, {"q": "What updates or decisions were made in the discussion?", "a": "+1.\nLooks good."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-121", "title": "pom should include standard maven niceties", "status": "Resolved", "priority": "Minor", "reporter": "Chris Burroughs", "assignee": null, "labels": [], "created": "2011-08-24T03:29:34.000+0000", "updated": "2014-02-12T17:01:22.000+0000", "description": "* license info, name, description, etc\n* webpage link\n* parent tags for sub-modules\n* groupid pending mailing list discussion\n\nOne refernce: http://vasilrem.com/blog/software-development/from-sbt-to-maven-in-one-move/", "comments": ["will keep this open for the 0.8.1 release in case anything is missed", "[~joestein] What do we need to do here before the 0.8.1 release?", "attached is the pom generated by the gradle build\n\nwe can use this ticket to decide if the pom is looking how we want and if not make changes appropriately... or not if it is good to go\n\nall pom specific issues I think should be encompassed in this ticket", "These have all been taken care."], "derived": {"summary": "* license info, name, description, etc\n* webpage link\n* parent tags for sub-modules\n* groupid pending mailing list discussion\n\nOne refernce: http://vasilrem. com/blog/software-development/from-sbt-to-maven-in-one-move/.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "pom should include standard maven niceties - * license info, name, description, etc\n* webpage link\n* parent tags for sub-modules\n* groupid pending mailing list discussion\n\nOne refernce: http://vasilrem. com/blog/software-development/from-sbt-to-maven-in-one-move/."}, {"q": "What updates or decisions were made in the discussion?", "a": "These have all been taken care."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-122", "title": "Add more info in output of DumpLogSegments", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Jun Rao", "labels": [], "created": "2011-08-24T21:50:33.000+0000", "updated": "2011-09-13T01:31:55.000+0000", "description": "Need to show the magic byte, compression codec and whether the crc matches for each message.", "comments": ["Patch attached.", "+1. Looks good"], "derived": {"summary": "Need to show the magic byte, compression codec and whether the crc matches for each message.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add more info in output of DumpLogSegments - Need to show the magic byte, compression codec and whether the crc matches for each message."}, {"q": "What updates or decisions were made in the discussion?", "a": "+1. Looks good"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-123", "title": "Rubygem gemspec doesn't build correctly", "status": "Closed", "priority": "Trivial", "reporter": "John Le", "assignee": null, "labels": ["ruby"], "created": "2011-08-26T21:31:11.000+0000", "updated": "2014-02-10T23:03:01.000+0000", "description": "The gemspec file doesn't build correctly because lib/kafka/error_codes.rb is not included in the gemspec files.\n\nI've created a pull request in github that fixes the problem.  It's really simple.  Simpler to fix than to write this ticket.\n\nhttps://github.com/kafka-dev/kafka/pull/41", "comments": ["John, \n\nThanks for the fix. Would you mind attaching the patch directly here? You need to grant Apache the permission to use your patch and I can't do that for you. Thanks again.", "This was a duplicate of Kafka-135 and is already fixed. Please close this bug."], "derived": {"summary": "The gemspec file doesn't build correctly because lib/kafka/error_codes. rb is not included in the gemspec files.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Rubygem gemspec doesn't build correctly - The gemspec file doesn't build correctly because lib/kafka/error_codes. rb is not included in the gemspec files."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was a duplicate of Kafka-135 and is already fixed. Please close this bug."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-124", "title": "Console consumer does not exit if consuming process dies", "status": "Resolved", "priority": "Major", "reporter": "Jay Kreps", "assignee": "Jay Kreps", "labels": [], "created": "2011-08-26T22:40:36.000+0000", "updated": "2011-09-01T05:48:23.000+0000", "description": "Running the kafka console consumer it should be the case that if the consuming subprocess dies the java process dies as well. Instead it continues consuming messages even though there is no one to give them to.", "comments": ["Can a brother get a +1?", "Appreciate the comment cleanup. The System.out.flush() line looks like it has a different indentation level than the other lines in the same block. Everything else looks good. Not sure if a +1 is required from Kafka devs or not but it has my vote.", " the patch looks good to me. +1 please fix the indentation before checking in.", "+1\n\nPrintStream javadoc suggests that System.out.flush() is redundant, since checkError also does a flush. Per our coding conventions since checkError has that side-effect, optional parentheses would be appropriate for that call.\n"], "derived": {"summary": "Running the kafka console consumer it should be the case that if the consuming subprocess dies the java process dies as well. Instead it continues consuming messages even though there is no one to give them to.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Console consumer does not exit if consuming process dies - Running the kafka console consumer it should be the case that if the consuming subprocess dies the java process dies as well. Instead it continues consuming messages even though there is no one to give them to."}, {"q": "What updates or decisions were made in the discussion?", "a": "+1\n\nPrintStream javadoc suggests that System.out.flush() is redundant, since checkError also does a flush. Per our coding conventions since checkError has that side-effect, optional parentheses would be appropriate for that call."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-125", "title": "tooBigRequestIsRejected fails with unexpected Exceptoin", "status": "Resolved", "priority": "Minor", "reporter": "Chris Burroughs", "assignee": "Jay Kreps", "labels": [], "created": "2011-08-31T17:04:14.000+0000", "updated": "2011-10-03T20:12:17.000+0000", "description": "Commit: http://svn.apache.org/viewvc?view=revision&revision=1159837\n\n[info] Test Starting: tooBigRequestIsRejected\n[error] Test Failed: tooBigRequestIsRejected\njava.lang.Exception: Unexpected exception, expected<java.io.EOFException> but was<java.net.SocketException>\n\tat org.junit.internal.runners.TestMethodRunner.runUnprotected(TestMethodRunner.java:91)\n\tat org.junit.internal.runners.BeforeAndAfterRunner.runProtected(BeforeAndAfterRunner.java:34)\n\tat org.junit.internal.runners.TestMethodRunner.runMethod(TestMethodRunner.java:75)\n\tat org.junit.internal.runners.TestMethodRunner.run(TestMethodRunner.java:45)\n\tat org.junit.internal.runners.TestClassMethodsRunner.invokeTestMethod(TestClassMethodsRunner.java:71)\n\tat org.junit.internal.runners.TestClassMethodsRunner.run(TestClassMethodsRunner.java:35)\n\tat org.junit.internal.runners.TestClassRunner$1.runUnprotected(TestClassRunner.java:42)\n\tat org.junit.internal.runners.BeforeAndAfterRunner.runProtected(BeforeAndAfterRunner.java:34)\n\tat org.junit.internal.runners.TestClassRunner.run(TestClassRunner.java:52)\n\tat org.junit.internal.runners.CompositeRunner.run(CompositeRunner.java:29)\n\tat org.junit.runner.JUnitCore.run(JUnitCore.java:121)\n\tat org.junit.runner.JUnitCore.run(JUnitCore.java:100)\n\tat org.junit.runner.JUnitCore.run(JUnitCore.java:91)\n\tat org.scalatest.junit.JUnitSuite$class.run(JUnitSuite.scala:261)\n\tat kafka.network.SocketServerTest.run(SocketServerTest.scala:32)\n\tat org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:40)\n\tat sbt.TestRunner.run(TestFramework.scala:53)\n\tat sbt.TestRunner.runTest$1(TestFramework.scala:67)\n\tat sbt.TestRunner.run(TestFramework.scala:76)\n\tat sbt.TestFramework$$anonfun$10$$anonfun$apply$11.runTest$2(TestFramework.scala:194)\n\tat sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)\n\tat sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)\n\tat sbt.NamedTestTask.run(TestFramework.scala:92)\n\tat sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)\n\tat sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)\n\tat sbt.TaskManager$Task.invoke(TaskManager.scala:62)\n\tat sbt.impl.RunTask.doRun$1(RunTask.scala:77)\n\tat sbt.impl.RunTask.runTask(RunTask.scala:85)\n\tat sbt.impl.RunTask.sbt$impl$RunTask$$runIfNotRoot(RunTask.scala:60)\n\tat sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)\n\tat sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)\n\tat sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)\n\tat sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)\n\tat sbt.Control$.trapUnit(Control.scala:19)\n\tat sbt.Distributor$Run$Worker.run(ParallelRunner.scala:131)\nCaused by: java.net.SocketException: Connection reset\n\tat java.net.SocketInputStream.read(SocketInputStream.java:168)\n\tat java.net.SocketInputStream.read(SocketInputStream.java:182)\n\tat java.io.DataInputStream.readInt(DataInputStream.java:370)\n\tat kafka.network.SocketServerTest.sendRequest(SocketServerTest.scala:56)\n\tat kafka.network.SocketServerTest.tooBigRequestIsRejected(SocketServerTest.scala:78)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.junit.internal.runners.TestMethodRunner.executeMethodBody(TestMethodRunner.java:99)\n\tat org.junit.internal.runners.TestMethodRunner.runUnprotected(TestMethodRunner.java:81)\n\tat org.junit.internal.runners.BeforeAndAfterRunner.runProtected(BeforeAndAfterRunner.java:34)\n\tat org.junit.internal.runners.TestMethodRunner.runMethod(TestMethodRunner.java:75)\n\tat org.junit.internal.runners.TestMethodRunner.run(TestMethodRunner.java:45)\n\tat org.junit.internal.runners.TestClassMethodsRunner.invokeTestMethod(TestClassMethodsRunner.java:71)\n\tat org.junit.internal.runners.TestClassMethodsRunner.run(TestClassMethodsRunner.java:35)\n\tat org.junit.internal.runners.TestClassRunner$1.runUnprotected(TestClassRunner.java:42)\n\tat org.junit.internal.runners.BeforeAndAfterRunner.runProtected(BeforeAndAfterRunner.java:34)\n\tat org.junit.internal.runners.TestClassRunner.run(TestClassRunner.java:52)\n\tat org.junit.internal.runners.CompositeRunner.run(CompositeRunner.java:29)\n\tat org.junit.runner.JUnitCore.run(JUnitCore.java:121)\n\tat org.junit.runner.JUnitCore.run(JUnitCore.java:100)\n\tat org.junit.runner.JUnitCore.run(JUnitCore.java:91)\n\tat org.scalatest.junit.JUnitSuite$class.run(JUnitSuite.scala:261)\n\tat kafka.network.SocketServerTest.run(SocketServerTest.scala:32)\n\tat org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:40)\n\tat sbt.TestRunner.run(TestFramework.scala:53)\n\tat sbt.TestRunner.runTest$1(TestFramework.scala:67)\n\tat sbt.TestRunner.run(TestFramework.scala:76)\n\tat sbt.TestFramework$$anonfun$10$$anonfun$apply$11.runTest$2(TestFramework.scala:194)\n\tat sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)\n\tat sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)\n\tat sbt.NamedTestTask.run(TestFramework.scala:92)\n\tat sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)\n\tat sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)\n\tat sbt.TaskManager$Task.invoke(TaskManager.scala:62)\n\tat sbt.impl.RunTask.doRun$1(RunTask.scala:77)\n\tat sbt.impl.RunTask.runTask(RunTask.scala:85)\n\tat sbt.impl.RunTask.sbt$impl$RunTask$$runIfNotRoot(RunTask.scala:60)\n\tat sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)\n\tat sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)\n\tat sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)\n\tat sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)\n\tat sbt.Control$.trapUnit(Control.scala:19)\n\tat sbt.Distributor$Run$Worker.run(ParallelRunner.scala:131)\n", "comments": ["Jay,\n\nWe are getting ready to cut the first RC early next week. It would be good to fix this before the RC is published. Do you think that can happen ?\n\n", "+1 for the patch."], "derived": {"summary": "Commit: http://svn. apache.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "tooBigRequestIsRejected fails with unexpected Exceptoin - Commit: http://svn. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "+1 for the patch."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-126", "title": "Log flush should complete upon broker shutdown", "status": "Resolved", "priority": "Major", "reporter": "Joel Jacob Koshy", "assignee": null, "labels": [], "created": "2011-08-31T23:58:46.000+0000", "updated": "2011-10-05T23:05:58.000+0000", "description": "Broker shutdown currently forces the flush scheduler to shutdown(Now). This leads to an unclean shutdown. cleanupLogs may be affected by a similar scenario.\n\n2011/08/31 09:45:34.833 ERROR [LogManager] [kafka-logflusher-0] [kafka] Error flushing topic MyTopic\njava.nio.channels.ClosedByInterruptException\n        at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:184)\n        at sun.nio.ch.FileChannelImpl.force(FileChannelImpl.java:362)\n        at kafka.message.FileMessageSet.flush(FileMessageSet.scala:174)\n        at kafka.log.Log.flush(Log.scala:306)\n        at kafka.log.LogManager$$anonfun$kafka$log$LogManager$$flushAllLogs$1.apply(LogManager.scala:274)\n        at kafka.log.LogManager$$anonfun$kafka$log$LogManager$$flushAllLogs$1.apply(LogManager.scala:263)\n        at scala.collection.Iterator$class.foreach(Iterator.scala:631)\n        at kafka.utils.IteratorTemplate.foreach(IteratorTemplate.scala:30)\n        at kafka.log.LogManager.kafka$log$LogManager$$flushAllLogs(LogManager.scala:263)\n        at kafka.log.LogManager$$anonfun$startup$1.apply$mcV$sp(LogManager.scala:129)\n        at kafka.utils.Utils$$anon$2.run(Utils.scala:58)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)\n        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)\n        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:619)\n\nA possible fix this would be to use shutdown() instead of shutdownNow() in the scheduler.\n", "comments": ["This is pretty much the simple fix that I suggested in the description. i.e., change the shutdown behavior of KafkaScheduler to use shutdown() instead of shutdownNow(). I also added a shutdownNow() method to KafkaScheduler for access to the immediate shutdown behavior.\n\nFor testing: I changed the flush settings to:\nlog.flush.interval=10000\nlog.default.flush.interval.ms=0\nlog.default.flush.scheduler.interval.ms=20\nran ProducerPerformance and shut down the broker a couple of times. These settings consistently reproduce the exception without this fix.\n\nThe other usages of shutdown (now called shutdownNow) are by the zookeeper offset committer, and the LogManager's log cleanup. I think these schedulers should also switch to using shutdown() and that is accomplished by this patch.\n\nFinally, I'm piggy backing an small unrelated warning message in the producer config - i.e. when both zk.connect and broker.list are specified, zk.connect takes precedence. (Let me know if you prefer this to be removed from this patch.)\n", "Using the executor shutdown API, the process would block until the currently executing thread finishes execution. I guess that works fine unless for some odd reason, the thread blocks on one action forever, in which case the process would have to be killed manually. I can't imagine of a concrete example of when it could block forever though. \n\nOn the other hand, I guess that if/when that kind of blocking happens, it is a serious problem/bug that needs attention anyways. So using the shutdown() API looks like a good approach. \n\n+1", "Thanks Joel for the patch. The zookeeper offset committer should use shutdownNow. If there is anything wrong with ZK server, we still want to be able to shut down a consumer immedidately.", "That makes sense. Here is the updated patch.", "Oops - forgot to rebase.", "Thanks, Joel. Just committed this."], "derived": {"summary": "Broker shutdown currently forces the flush scheduler to shutdown(Now). This leads to an unclean shutdown.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Log flush should complete upon broker shutdown - Broker shutdown currently forces the flush scheduler to shutdown(Now). This leads to an unclean shutdown."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks, Joel. Just committed this."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-127", "title": "Tool to watch consumer offsets and lag", "status": "Resolved", "priority": "Minor", "reporter": "Joel Jacob Koshy", "assignee": null, "labels": [], "created": "2011-09-01T00:01:13.000+0000", "updated": "2011-09-10T00:37:04.000+0000", "description": "I have a simple script to watch consumer offsets and lag that was useful to us in production debugging. Unfortunately, it is slow and does not work on Macs due to some non-portable awk. I will write up a scala implementation.", "comments": ["Running this tool spits out some zk/zkclient logging. Not sure if we want to default to warn level for org.apache.zookeeper and org.I0Itec", "After trying this out a bit more I decided to incorporate a small formatting improvement.", "+1. Thanks, Joel. Just committed this."], "derived": {"summary": "I have a simple script to watch consumer offsets and lag that was useful to us in production debugging. Unfortunately, it is slow and does not work on Macs due to some non-portable awk.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Tool to watch consumer offsets and lag - I have a simple script to watch consumer offsets and lag that was useful to us in production debugging. Unfortunately, it is slow and does not work on Macs due to some non-portable awk."}, {"q": "What updates or decisions were made in the discussion?", "a": "+1. Thanks, Joel. Just committed this."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-128", "title": "DumpLogSegments outputs wrong offsets", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Jun Rao", "labels": [], "created": "2011-09-01T01:24:20.000+0000", "updated": "2011-09-01T07:50:37.000+0000", "description": null, "comments": ["Patch attached.", "\nThe start offset is printed incorrectly before the inner loop. Looks good otherwise.\n\n+1\n\n"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "DumpLogSegments outputs wrong offsets"}, {"q": "What updates or decisions were made in the discussion?", "a": "The start offset is printed incorrectly before the inner loop. Looks good otherwise.\n\n+1"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-129", "title": "ZK-based producer can throw an unexpceted exception when sending a message", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": null, "labels": [], "created": "2011-09-01T01:36:06.000+0000", "updated": "2011-10-04T23:05:59.000+0000", "description": "Here is a log trace when that happens.\n\n2011/08/26 11:25:20.104 FATAL [EmbeddedConsumer] [kafka-embedded-consumer-firehoseActivity-0] [kafka] java.util.NoSuchElementException: None.getjava.util.NoSuchElementException: None.get\n        at scala.None$.get(Option.scala:185)\n        at scala.None$.get(Option.scala:183)\n        at kafka.producer.Producer$$anonfun$3.apply(Producer.scala:115)\n        at kafka.producer.Producer$$anonfun$3.apply(Producer.scala:101)\n        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)\n        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)\n        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)\n        at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:32)\n        at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)\n        at scala.collection.mutable.WrappedArray.map(WrappedArray.scala:32)\n        at kafka.producer.Producer.send(Producer.scala:101)\n        at kafka.server.EmbeddedConsumer$$anonfun$startNewConsumerThreads$1$$anonfun$apply$1$$anon$1$$anonfun$run$1.apply(KafkaServerStartable.scala:136)\n        at kafka.server.EmbeddedConsumer$$anonfun$startNewConsumerThreads$1$$anonfun$apply$1$$anon$1$$anonfun$run$1.apply(KafkaServerStartable.scala:134)\n        at scala.collection.Iterator$class.foreach(Iterator.scala:631)\n        at kafka.utils.IteratorTemplate.foreach(IteratorTemplate.scala:30)\n        at scala.collection.IterableLike$class.foreach(IterableLike.scala:79)\n        at kafka.consumer.KafkaMessageStream.foreach(KafkaMessageStream.scala:29)\n        at kafka.server.EmbeddedConsumer$$anonfun$startNewConsumerThreads$1$$anonfun$apply$1$$anon$1.run(KafkaServerStartable.scala:134)\n        at java.lang.Thread.run(Thread.java:619)\n", "comments": ["The producer using the zookeeper software load balancer maintains a ZK cache that gets updated by the zookeeper watcher listeners. During some events like a broker bounce, the producer ZK cache can get into an inconsistent state, for a small time period. In this time period, it could end up picking a broker partition that is unavailable, to complete the send operation. \n\nWhen this happens, the ZK cache needs to be updated, and the process of picking a broker partition for the current event should be repeated. This is repeated for a configurable number of retries, defaulting to 3. Arguably, if it takes more than 1-2 retries to get consistent data from zookeeper, something is really wrong, and we should throw an exception back to the user and fail the send operation.\n\nThis patch also adds a check around the debug and trace level logging in the producer", "1. import collection.SortedSet is not used in Producer.\n\n2. In ZKBrokerPatitionInfo, it seem that we need to synchronize on zkWatcherLock in getBrokerInfo, to prevent seeing inconsistent info between allBrokers and the syncProducer list in ProducerPool.\n\n3. In ProducerTest.testSendSingleMessage, the comment says that we want to send the request to a random partition, why is the partition number changed from -1 to 0? ", "2. Good catch. Updated the patch to include this.\n3. While I was making this change, I found a bug in the partitioning approach of the producer when broker.list option is used. Previously, it choose a random broker, and then created a produce request with -1 as the partition. This is not, however, we intend to do partitioning. We let the default partitioner pick the right broker partition from amongst all available. So we never end up with a request with -1 as the partition. That test was also written with this buggy logic. It is using the StaticPartitioner, so the broker partition is deterministically selected. I fixed the bug as well as the tests to expose the right behavior.", "OK, for item 3, could you change the comment accordingly?", "Yeah, the comment in the test didn't quite match the new behavior. Fixed it", "This patch fixes the comments that didn't match the expected behavior in the code", "+1. Thanks for the patch."], "derived": {"summary": "Here is a log trace when that happens. 2011/08/26 11:25:20.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "ZK-based producer can throw an unexpceted exception when sending a message - Here is a log trace when that happens. 2011/08/26 11:25:20."}, {"q": "What updates or decisions were made in the discussion?", "a": "+1. Thanks for the patch."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-130", "title": "Provide a default producer for receiving messages from STDIN", "status": "Resolved", "priority": "Minor", "reporter": "Felix GV", "assignee": "Jay Kreps", "labels": [], "created": "2011-09-01T20:19:58.000+0000", "updated": "2011-10-12T01:22:43.000+0000", "description": "It would be useful to provide a default producer we can fire up that reads from STDIN and sends one message per line to the broker.\n\nThe most obvious use case for this is to pipe a tail -f command into it, to tail log files as they're generated. Making it depend on STDIN seems more flexible than a producer that just tails files though.", "comments": ["Add a console producer for kafka that reads from standard in.", "Usage is the following:\n\njkreps-mn:kafka-git jkreps$ bin/kafka-run-class.sh kafka.producer.ConsoleProducer \nMissing required argument \"[topic]\"\nOption                                  Description                            \n------                                  -----------                            \n--batch-size <Integer: size>            Number of messages to send in a single \n                                          batch if they are not being sent     \n                                          synchronously. (default: 200)        \n--compress                              If set, messages batches are sent      \n                                          compressed                           \n--line-reader <reader_class>            The class name of the class to use for \n                                          reading lines from standard in. By   \n                                          default each line is read as a       \n                                          seperate message. (default: kafka.   \n                                          producer.                            \n                                          ConsoleProducer$LineMessageReader)   \n--message-encoder <encoder_class>       The class name of the message encoder  \n                                          implementation to use. (default:     \n                                          kafka.serializer.StringEncoder)      \n--property <prop>                                                              \n--sync                                  If set message send requests to the    \n                                          brokers are synchronously, one at a  \n                                          time as they arrive.                 \n--timeout <Long: timeout_ms>            If set and the producer is running in  \n                                          asynchronous mode, this gives the    \n                                          maximum amount of time a message     \n                                          will queue awaiting suffient batch   \n                                          size. The value is given in ms.      \n                                          (default: 1000)                      \n--topic <topic>                         REQUIRED: The topic id to produce      \n                                          messages to.                         \n--zookeeper <connection_string>         REQUIRED: The zookeeper connection     \n                                          string for the kafka zookeeper       \n                                          instance in the form HOST:PORT       \n                                          [/CHROOT].\n\nIt reads from standard input so it can take interactive input or else you can pipe stuff to it:\ncat ~/some_log_file.log| bin/kafka-run-class.sh kafka.producer.ConsoleProducer --topic test --zookeeper localhost:2181\n\nBy default it reads lines from the log and sends them as string messages to Kafka.\n\nBoth the class used to read objects and the Kafka producer serializer can be customized from the arguments. The serializer is the normal kafka.serializer.Encoder interface, and the class used to read messages from System.in should extend the MessageReader abstract class (trait), which has the following methods:\n\n  trait MessageReader { \n    def init(inputStream: InputStream, props: Properties) {}\n    def readMessage(): AnyRef\n    def close() {}\n  }\n\nAKA:\n\ninterface MessageReader {\n  public void init(InputStream in, Properties props);\n  public Object readMessage();\n  public void close();\n}", "Thanks for the patch Jay. It looks good. About the propertyOpt, could you add a description about its purpose and format? Also, it's not used in the init method of LineMessageReader.", "Hey Jun, the idea of the properties option is this: we want to give a way for the user to plug in code for parsing the input file, and that code may require some options of its own. For example you might need to give the charset of the input file or some other info. To make this possible there needs to be a way to pass config through this code to the user's plug-in message reader. Obviously for our usage, the thing I had in mind was being able to pass in an avro schema or url to get one. This is all fairly advanced functionality, so the user doesn't need to mess with it unless they want to read in messages a particular way.\n\nLineMessageReader is a default implementation of this interface that just turns one line into an input to the producer as a string, used in combination with the StringEncoder it does the normal logging thing of sending one line as a message.\n\nHopefully that makes sense...", "Ok, that makes sense. Just document the format in the description of the property and we can commit the patch.", "I will add some docs, can I get a +1 on this?", "Added to the --help info on --property.", "+1"], "derived": {"summary": "It would be useful to provide a default producer we can fire up that reads from STDIN and sends one message per line to the broker. The most obvious use case for this is to pipe a tail -f command into it, to tail log files as they're generated.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Provide a default producer for receiving messages from STDIN - It would be useful to provide a default producer we can fire up that reads from STDIN and sends one message per line to the broker. The most obvious use case for this is to pipe a tail -f command into it, to tail log files as they're generated."}, {"q": "What updates or decisions were made in the discussion?", "a": "Added to the --help info on --property."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-131", "title": "Hadoop Consumer goes into an infinite loop when  kafka.request.limit is set to -1", "status": "Resolved", "priority": "Major", "reporter": "Sam William", "assignee": null, "labels": ["patch"], "created": "2011-09-02T19:20:19.000+0000", "updated": "2011-09-29T18:39:11.000+0000", "description": "There is a bug in  KafkaETLContext.java  where in a new Iterator instance is being created every time. This causes endless loops.", "comments": ["Adding an instance variable _respIterator , so that response.iterator() isn't called multiple times", "Fix for the infinite loop bug in KafkaETLContext.java", "Just one thing's missing.\nOn line 201 (of original file):\n_offset += msgAndOffset.offset();\n\nThat's incorrect. The msgAndOffset returns the offset, not the offset increment. So it should be:\n_offset = msgAndOffset.offset();\n", "Adding the fix for offsets (Line 201)", "Great. I'm good with this patch.", "Thanks Sam and Richard. I just committed this.", "I just wanted to point out that this bug seems to happen whether kafka.request.limit is set to -1 or not.\n\nThe current 0.6 release that is available on the site is not very usable because of this bug...\n\nThe current trunk does fix this problem though, which is great. Thanks :) !", "We have run into this as well Felix. I'd like to backport whatever change fixed this in trunk into our 0.6.1 branch. Any idea where I should look?", "Blake,\n   You could apply the attached patch  (https://issues.apache.org/jira/secure/attachment/12493181/KAFKA-131.patch)  to the file KafkaETLContext.java", "Sorry I should have been more clear. We actually run into this issue not using the KafkaETL. We occasionally see regular consumers go into a loop (continue to fetch the same offset), I thought the comment from Felix was referring to that specifically."], "derived": {"summary": "There is a bug in  KafkaETLContext. java  where in a new Iterator instance is being created every time.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Hadoop Consumer goes into an infinite loop when  kafka.request.limit is set to -1 - There is a bug in  KafkaETLContext. java  where in a new Iterator instance is being created every time."}, {"q": "What updates or decisions were made in the discussion?", "a": "Sorry I should have been more clear. We actually run into this issue not using the KafkaETL. We occasionally see regular consumers go into a loop (continue to fetch the same offset), I thought the comment from Felix was referring to that specifically."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-132", "title": "Reduce Unnecessary Filesystem Writes for Logs Without Unflushed Events", "status": "Resolved", "priority": "Minor", "reporter": "C. Scott Andreas", "assignee": null, "labels": [], "created": "2011-09-13T21:09:53.000+0000", "updated": "2011-09-21T16:46:13.000+0000", "description": "We noticed a fair amount of stress on our filesystem in an environment with a large number of topics but low message activity. After some investigation, we realized that a short log.flush.interval coupled with a large number of topics resulted in a lot of unnecessary disk activity, even without events to be written.\n\nThis activity occurs because FileChannel.force(true) is called on the underlying FileMessageSet for each log, even when there are no messages to be written. This call forces unproductive writes to the underlying filesystem.\n\nThis case is especially stressed in an environment with a large number of low-activity topics for which low latency is still important. Here is the before-and-after output of `iostat -x 2` on a system with 1044 topics and a timed flush interval of 100ms. Note the reduction in %util and writes/second. In the \"before\" output, we see 40-80% util and ~260 writes/second. In the \"after\" output, we see 10-15% util and ~65 writes/second.\n\nPre-patch output: https://raw.github.com/gist/54d0f4c62753a6e2de1f/7ee1982bfa8e5c088bcf9ba953f01956443bd31e/iostat-pre-kafka-patch.txt\n\nPost-patch output:\nhttps://raw.github.com/gist/54d0f4c62753a6e2de1f/b939973c7fed642480856d9bdeb2e4cb0ada445b/iostat-post-kafka-patch.txt\n\nThe proposed patch (see attached) skips calling the underlying FileMessageSet flush operation if the log's atomic counter indicates that there are no messages to be written.", "comments": ["Patch applied to core/src/main/scala/kafka/log/Log.scala in version 0.6.1", "Ack, nice catch! Nothing like a 4x improvement from one line of code.\n+1", "Applied!", "Trunk == 0.7 not 0.8, right?"], "derived": {"summary": "We noticed a fair amount of stress on our filesystem in an environment with a large number of topics but low message activity. After some investigation, we realized that a short log.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "Reduce Unnecessary Filesystem Writes for Logs Without Unflushed Events - We noticed a fair amount of stress on our filesystem in an environment with a large number of topics but low message activity. After some investigation, we realized that a short log."}, {"q": "What updates or decisions were made in the discussion?", "a": "Trunk == 0.7 not 0.8, right?"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-133", "title": "Publish kafka jar to a public maven repository", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": null, "labels": ["patch"], "created": "2011-09-15T15:48:52.000+0000", "updated": "2013-06-24T23:12:10.000+0000", "description": "The released kafka jar must be download manually and then deploy to a private repository before they can be used by a developer using maven2.\n\nSimilar to other Apache projects, it will be nice to have a way to publish Kafka releases to a public maven repo. \n\nIn the past, we gave it a try using sbt publish to Sonatype Nexus maven repo, but ran into some authentication problems. It will be good to revisit this and get it resolved.", "comments": ["I did this last night for one of our other projects, Norbert.\n\nIf I upgrade Kafka to sbt 0.10.1, I should be able to do this. Will also write a blog post so other projects know how to do this with maven.", "Thanks for picking this up. Also, do you mind filing another JIRA for the sbt 0.10.1 upgrade ?\n", "I need to state the obvious, because I am a mentor, but while it's good to get this machinery working, this project cannot actually publish any artifacts until its first official release.", "Josh, you are my hero.", "I think it would also be nice to publish the resulting clojure client jar to a maven repo, so client clojure code could benefit easily.", "Hey Guys,\n\nIs there a maven repo for Kafka at this point? :)\n\nI sure could use it if there is one.\n\nCheers,\nChris", "commit e6f067d538d4902f64f8397256fe8d967e036bc6\nAuthor: Joshua Hartman <jhartman@linkedin.com>\nDate:   Mon Oct 24 10:56:12 2011 -0700\n\n    Initial attempt at sbt upgrade.\n\ndiff --git a/lib/sbt-launch.jar b/lib/sbt-launch.jar\ndeleted file mode 100644\nindex 67ee369..0000000\nBinary files a/lib/sbt-launch.jar and /dev/null differ\ndiff --git a/project/Build.scala b/project/Build.scala\nnew file mode 100644\nindex 0000000..085809b\n--- /dev/null\n+++ b/project/Build.scala\n@@ -0,0 +1,147 @@\n+import sbt._\n+import Keys._\n+import xml.NodeSeq\n+\n+object BuildSettings {\n+  val exclusions = Seq(\"javax\", \"jmxri\", \"jmxtools\", \"mail\", \"jms\")\n+  val ivyExclude = <dependencies>{exclusions.map ( e => <exclude module={e}/>)}</dependencies>\n+\n+  val buildSettings = Defaults.defaultSettings ++ Seq (\n+    organization := \"kafka\",\n+    version      := \"0.7\",\n+    crossScalaVersions := Seq(\"2.8.1\", \"2.9.1\"),\n+    ivyXML := ivyExclude\n+  )\n+\n+  def buildPomExtra(pom: NodeSeq, name: String, desc: String) = {\n+    pom ++ Seq(\n+      <name>\n+        {name}\n+      </name>,\n+      <description>\n+        {desc}\n+      </description>,\n+      <url>http://incubator.apache.org/kafka</url>,\n+      <licenses>\n+        <license>\n+          <name>Apache</name>\n+          <url>http://svn.apache.org/repos/asf/incubator/kafka/trunk/LICENSE</url>\n+          <distribution>repo</distribution>\n+        </license>\n+      </licenses>,\n+      <scm>\n+        <url>http://svn.apache.org/repos/asf/incubator/kafka/trunk/</url>\n+        <connection>scm:svn:http://svn.apache.org/repos/asf/incubator/kafka/trunk</connection>\n+        <developerConnection>scm:svn:https://foo.googlecode.com/svn/trunk/</developerConnection>\n+        <connection>scm:git:git://github.com/linkedin-sna/norbert.git</connection>\n+      </scm>,\n+      <developers>\n+        <developer>\n+          <id>jkreps</id>\n+          <name>Jay Kreps</name>\n+          <url>http://www.linkedin.com/in/jaykreps</url>\n+        </developer>\n+        <developer>\n+          <id>junrao</id>\n+          <name>Jun Rao</name>\n+          <url>http://www.linkedin.com/in/junrao</url>\n+        </developer>\n+        <developer>\n+          <id>nehanarkhede</id>\n+          <name>Joshua Hartman</name>\n+          <url>http://www.linkedin.com/in/nehanarkhede</url>\n+        </developer>\n+      </developers>\n+    )\n+  }\n+}\n+\n+object Resolvers {\n+  val oracleRepo = \"Oracle Maven 2 Repository\" at \"http://download.oracle.com/maven\"\n+  val jBossRepo = \"JBoss Maven 2 Repository\" at \"http://repository.jboss.com/maven2\"\n+  val kafkaResolvers = Seq(oracleRepo, jBossRepo)\n+}\n+\n+object CoreDependencies {\n+//  TODO jhartman: When sbt 0.11.1 is ready, we can use the following code instead of ivy xml\n+//  val exclusions = Seq(\"javax\", \"jmxri\", \"jmxtools\", \"mail\", \"jms\") map (n => ExclusionRule(name = n))\n+//  val log4j = (\"log4j\" % \"log4j\" % \"1.2.15\") excludeAll (exclusions :_*)\n+\n+  val log4j = (\"log4j\" % \"log4j\" % \"1.2.15\")\n+  val jopt = \"net.sf.jopt-simple\" % \"jopt-simple\" % \"3.2\"\n+  val deps = Seq(log4j, jopt)\n+}\n+\n+object HadoopProducerDependencies {\n+  val avro = \"org.apache.avro\" % \"avro\" % \"1.4.1\"\n+  val jacksonCore = \"org.codehaus.jackson\" % \"jackson-core-asl\" % \"1.5.5\"\n+  val jacksonMapper = \"org.codehaus.jackson\" % \"jackson-mapper-asl\" % \"1.5.5\"\n+  val deps = Seq(avro, jacksonCore, jacksonMapper)\n+}\n+\n+object HadoopConsumerDependencies {\n+  val jodaTime = \"joda-time\" % \"joda-time\" % \"1.6\"\n+  val httpclient = \"commons-httpclient\" % \"commons-httpclient\" % \"3.1\"\n+  val deps = Seq(jodaTime, httpclient)\n+}\n+\n+object TestDependencies {\n+  val easymock = \"org.easymock\" % \"easymock\" % \"3.0\" % \"test\"\n+  val junit = \"junit\" % \"junit\" % \"4.1\" % \"test\"\n+  val scalaTest = \"org.scalatest\" % \"scalatest\" % \"1.2\" % \"test\"\n+  val deps = Seq(easymock, junit, scalaTest)\n+}\n+\n+object KafkaBuild extends Build {\n+  import BuildSettings._\n+\n+  lazy val core = Project(\"core\", file(\"core\"),\n+    settings = buildSettings ++ Seq(\n+      libraryDependencies ++= CoreDependencies.deps ++ TestDependencies.deps,\n+      resolvers := Resolvers.kafkaResolvers\n+    )\n+  )\n+\n+  lazy val examples = Project(\"examples\", file(\"examples\"),\n+   settings = buildSettings\n+  ) dependsOn (core)\n+\n+\n+  lazy val perf = Project(\"perf\", file(\"perf\"),\n+    settings = buildSettings\n+  ) dependsOn (core)\n+\n+  lazy val hadoopProducer = Project(\"hadoop-producer\", file(\"hadoop-producer\"),\n+    settings = buildSettings ++ Seq(\n+      libraryDependencies ++= HadoopProducerDependencies.deps\n+    )\n+  ) dependsOn (core)\n+\n+  lazy val hadoopConsumer = Project(\"hadoop-consumer\", file(\"hadoop-consumer\"),\n+    settings = buildSettings ++ Seq(\n+      libraryDependencies ++= HadoopConsumerDependencies.deps\n+    )\n+  ) dependsOn (core)\n+\n+  lazy val contrib = Project(\"contrib\", file(\"contrib\"), settings = buildSettings) aggregate(hadoopConsumer, hadoopProducer)\n+\n+  lazy val root = Project(\"root\", file(\".\"),\n+    settings = buildSettings ++ Seq(\n+      pomExtra <<= (pomExtra, name, description) { buildPomExtra }\n+  )) aggregate(core, examples, perf, contrib)\n+\n+  lazy val full = Project(\n+    id           = \"kafka\",\n+    base         = file(\"full\"),\n+    settings     = buildSettings ++ Seq(\n+      description := \"Includes all of kafka project in one\",\n+      libraryDependencies ++= CoreDependencies.deps ++ TestDependencies.deps ++ HadoopProducerDependencies.deps ++ HadoopConsumerDependencies.deps,\n+\n+      (unmanagedJars in Compile) <<= (projects.map(unmanagedJars in Compile in _).join).map(_.flatten),\n+      (unmanagedSourceDirectories in Compile) <<= projects.map(unmanagedSourceDirectories in Compile in _).join.apply(_.flatten),\n+      (managedSourceDirectories in Compile) <<= projects.map(managedSourceDirectories in Compile in _).join.apply(_.flatten),\n+\n+      pomExtra <<= (pomExtra, name, description) { buildPomExtra }\n+    )\n+  )\n+}\ndiff --git a/project/build.properties b/project/build.properties\ndeleted file mode 100644\nindex 7c14cce..0000000\n--- a/project/build.properties\n+++ /dev/null\n@@ -1,10 +0,0 @@\n-#Project properties\n-#Mon Feb 28 11:55:49 PST 2011\n-project.name=Kafka\n-sbt.version=0.7.5\n-project.version=0.7\n-build.scala.versions=2.8.0\n-contrib.root.dir=contrib\n-lib.dir=lib\n-target.dir=target/scala_2.8.0\n-dist.dir=dist\ndiff --git a/project/build/KafkaProject.scala b/project/build/KafkaProject.scala\nindex ba682d5..77aafba 100644\n--- a/project/build/KafkaProject.scala\n+++ b/project/build/KafkaProject.scala\n@@ -1,190 +1,190 @@\n-import sbt._\n-\n-class KafkaProject(info: ProjectInfo) extends ParentProject(info) with IdeaProject {\n-  lazy val core = project(\"core\", \"core-kafka\", new CoreKafkaProject(_))\n-  lazy val examples = project(\"examples\", \"java-examples\", new KafkaExamplesProject(_), core)\n-  lazy val perf = project(\"perf\", \"perf\", new KafkaPerfProject(_), core)\n-  lazy val contrib = project(\"contrib\", \"contrib\", new ContribProject(_))\n-\n-  lazy val releaseZipTask = core.packageDistTask\n-\n-  val releaseZipDescription = \"Compiles every sub project, runs unit tests, creates a deployable release zip file with dependencies, config, and scripts.\"\n-  lazy val releaseZip = releaseZipTask dependsOn(core.corePackageAction, core.test, examples.examplesPackageAction, perf.perfPackageAction,\n-    contrib.producerPackageAction, contrib.consumerPackageAction) describedAs releaseZipDescription\n-\n-  class CoreKafkaProject(info: ProjectInfo) extends DefaultProject(info)\n-     with IdeaProject with CoreDependencies with TestDependencies {\n-   val corePackageAction = packageAllAction\n-\n-  //The issue is going from log4j 1.2.14 to 1.2.15, the developers added some features which required\n-  // some dependencies on various sun and javax packages.\n-   override def ivyXML =\n-    <dependencies>\n-      <exclude module=\"javax\"/>\n-      <exclude module=\"jmxri\"/>\n-      <exclude module=\"jmxtools\"/>\n-      <exclude module=\"mail\"/>\n-      <exclude module=\"jms\"/>\n-    </dependencies>\n-\n-    override def repositories = Set(ScalaToolsSnapshots, \"JBoss Maven 2 Repository\" at \"http://repository.jboss.com/maven2\",\n-      \"Oracle Maven 2 Repository\" at \"http://download.oracle.com/maven\", \"maven.org\" at \"http://repo2.maven.org/maven2/\")\n-\n-    override def artifactID = \"kafka\"\n-    override def filterScalaJars = false\n-\n-    // build the executable jar's classpath.\n-    // (why is it necessary to explicitly remove the target/{classes,resources} paths? hm.)\n-    def dependentJars = {\n-      val jars =\n-      publicClasspath +++ mainDependencies.scalaJars --- mainCompilePath --- mainResourcesOutputPath\n-      if (jars.get.find { jar => jar.name.startsWith(\"scala-library-\") }.isDefined) {\n-        // workaround bug in sbt: if the compiler is explicitly included, don't include 2 versions\n-        // of the library.\n-        jars --- jars.filter { jar =>\n-          jar.absolutePath.contains(\"/boot/\") && jar.name == \"scala-library.jar\"\n-        }\n-      } else {\n-        jars\n-      }\n-    }\n-\n-    def dependentJarNames = dependentJars.getFiles.map(_.getName).filter(_.endsWith(\".jar\"))\n-    override def manifestClassPath = Some(dependentJarNames.map { \"libs/\" + _ }.mkString(\" \"))\n-\n-    def distName = (artifactID + \"-\" + projectVersion.value)\n-    def distPath = \"dist\" / distName ##\n-\n-    def configPath = \"config\" ##\n-    def configOutputPath = distPath / \"config\"\n-\n-    def binPath = \"bin\" ##\n-    def binOutputPath = distPath / \"bin\"\n-\n-    def distZipName = {\n-      \"%s-%s.zip\".format(artifactID, projectVersion.value)\n-    }\n-\n-    lazy val packageDistTask = task {\n-      distPath.asFile.mkdirs()\n-      (distPath / \"libs\").asFile.mkdirs()\n-      binOutputPath.asFile.mkdirs()\n-      configOutputPath.asFile.mkdirs()\n-\n-      FileUtilities.copyFlat(List(jarPath), distPath, log).left.toOption orElse\n-              FileUtilities.copyFlat(dependentJars.get, distPath / \"libs\", log).left.toOption orElse\n-              FileUtilities.copy((configPath ***).get, configOutputPath, log).left.toOption orElse\n-              FileUtilities.copy((binPath ***).get, binOutputPath, log).left.toOption orElse\n-              FileUtilities.zip(((\"dist\" / distName) ##).get, \"dist\" / distZipName, true, log)\n-      None\n-    }\n-\n-    val PackageDistDescription = \"Creates a deployable zip file with dependencies, config, and scripts.\"\n-    lazy val packageDist = packageDistTask dependsOn(`package`, `test`) describedAs PackageDistDescription\n-\n-    val cleanDist = cleanTask(\"dist\" ##) describedAs(\"Erase any packaged distributions.\")\n-    override def cleanAction = super.cleanAction dependsOn(cleanDist)\n-\n-    override def javaCompileOptions = super.javaCompileOptions ++\n-      List(JavaCompileOption(\"-source\"), JavaCompileOption(\"1.5\"))\n-  }\n-\n-  class KafkaExamplesProject(info: ProjectInfo) extends DefaultProject(info)\n-     with IdeaProject\n-     with CoreDependencies {\n-    val examplesPackageAction = packageAllAction\n-    val dependsOnCore = core\n-  //The issue is going from log4j 1.2.14 to 1.2.15, the developers added some features which required\n-  // some dependencies on various sun and javax packages.\n-   override def ivyXML =\n-    <dependencies>\n-      <exclude module=\"javax\"/>\n-      <exclude module=\"jmxri\"/>\n-      <exclude module=\"jmxtools\"/>\n-      <exclude module=\"mail\"/>\n-      <exclude module=\"jms\"/>\n-    </dependencies>\n-\n-    override def artifactID = \"kafka-java-examples\"\n-    override def filterScalaJars = false\n-  }\n-\n-  class KafkaPerfProject(info: ProjectInfo) extends DefaultProject(info)\n-      with IdeaProject\n-      with CoreDependencies {\n-    val perfPackageAction = packageAllAction\n-    val dependsOnCore = core\n-  //The issue is going from log4j 1.2.14 to 1.2.15, the developers added some features which required\n-  // some dependencies on various sun and javax packages.\n-   override def ivyXML =\n-    <dependencies>\n-      <exclude module=\"javax\"/>\n-      <exclude module=\"jmxri\"/>\n-      <exclude module=\"jmxtools\"/>\n-      <exclude module=\"mail\"/>\n-      <exclude module=\"jms\"/>\n-    </dependencies>\n-\n-    override def artifactID = \"kafka-perf\"\n-    override def filterScalaJars = false\n-  }\n-\n-  class ContribProject(info: ProjectInfo) extends ParentProject(info) with IdeaProject {\n-    lazy val hadoopProducer = project(\"hadoop-producer\", \"hadoop producer\",\n-                                      new HadoopProducerProject(_), core)\n-    lazy val hadoopConsumer = project(\"hadoop-consumer\", \"hadoop consumer\",\n-                                      new HadoopConsumerProject(_), core)\n-\n-    val producerPackageAction = hadoopProducer.producerPackageAction\n-    val consumerPackageAction = hadoopConsumer.consumerPackageAction\n-\n-    class HadoopProducerProject(info: ProjectInfo) extends DefaultProject(info)\n-      with IdeaProject\n-      with CoreDependencies {\n-      val producerPackageAction = packageAllAction\n-      override def ivyXML =\n-       <dependencies>\n-         <exclude module=\"netty\"/>\n-           <exclude module=\"javax\"/>\n-           <exclude module=\"jmxri\"/>\n-           <exclude module=\"jmxtools\"/>\n-           <exclude module=\"mail\"/>\n-           <exclude module=\"jms\"/>\n-       </dependencies>\n-\n-      val avro = \"org.apache.avro\" % \"avro\" % \"1.4.1\"\n-      val jacksonCore = \"org.codehaus.jackson\" % \"jackson-core-asl\" % \"1.5.5\"\n-      val jacksonMapper = \"org.codehaus.jackson\" % \"jackson-mapper-asl\" % \"1.5.5\"\n-    }\n-\n-    class HadoopConsumerProject(info: ProjectInfo) extends DefaultProject(info)\n-      with IdeaProject\n-      with CoreDependencies {\n-      val consumerPackageAction = packageAllAction\n-      override def ivyXML =\n-       <dependencies>\n-         <exclude module=\"netty\"/>\n-           <exclude module=\"javax\"/>\n-           <exclude module=\"jmxri\"/>\n-           <exclude module=\"jmxtools\"/>\n-           <exclude module=\"mail\"/>\n-           <exclude module=\"jms\"/>\n-       </dependencies>\n-\n-      val jodaTime = \"joda-time\" % \"joda-time\" % \"1.6\"\n-      val httpclient = \"commons-httpclient\" % \"commons-httpclient\" % \"3.1\"\n-    }\n-  }\n-\n-  trait TestDependencies {\n-    val easymock = \"org.easymock\" % \"easymock\" % \"3.0\" % \"test\"\n-    val junit = \"junit\" % \"junit\" % \"4.1\" % \"test\"\n-    val scalaTest = \"org.scalatest\" % \"scalatest\" % \"1.2\" % \"test\"\n-  }\n-\n-  trait CoreDependencies {\n-    val log4j = \"log4j\" % \"log4j\" % \"1.2.15\"\n-    val jopt = \"net.sf.jopt-simple\" % \"jopt-simple\" % \"3.2\"\n-  }\n-\n-}\n+//import sbt._\n+//\n+//class KafkaProject(info: ProjectInfo) extends ParentProject(info) with IdeaProject {\n+//  lazy val core = project(\"core\", \"core-kafka\", new CoreKafkaProject(_))\n+//  lazy val examples = project(\"examples\", \"java-examples\", new KafkaExamplesProject(_), core)\n+//  lazy val perf = project(\"perf\", \"perf\", new KafkaPerfProject(_), core)\n+//  lazy val contrib = project(\"contrib\", \"contrib\", new ContribProject(_))\n+//\n+//  lazy val releaseZipTask = core.packageDistTask\n+//\n+//  val releaseZipDescription = \"Compiles every sub project, runs unit tests, creates a deployable release zip file with dependencies, config, and scripts.\"\n+//  lazy val releaseZip = releaseZipTask dependsOn(core.corePackageAction, core.test, examples.examplesPackageAction, perf.perfPackageAction,\n+//    contrib.producerPackageAction, contrib.consumerPackageAction) describedAs releaseZipDescription\n+//\n+//  class CoreKafkaProject(info: ProjectInfo) extends DefaultProject(info)\n+//     with IdeaProject with CoreDependencies with TestDependencies {\n+//   val corePackageAction = packageAllAction\n+//\n+//  //The issue is going from log4j 1.2.14 to 1.2.15, the developers added some features which required\n+//  // some dependencies on various sun and javax packages.\n+//   override def ivyXML =\n+//    <dependencies>\n+//      <exclude module=\"javax\"/>\n+//      <exclude module=\"jmxri\"/>\n+//      <exclude module=\"jmxtools\"/>\n+//      <exclude module=\"mail\"/>\n+//      <exclude module=\"jms\"/>\n+//    </dependencies>\n+//\n+//    override def repositories = Set(ScalaToolsSnapshots, \"JBoss Maven 2 Repository\" at \"http://repository.jboss.com/maven2\",\n+//      \"Oracle Maven 2 Repository\" at \"http://download.oracle.com/maven\", \"maven.org\" at \"http://repo2.maven.org/maven2/\")\n+//\n+//    override def artifactID = \"kafka\"\n+//    override def filterScalaJars = false\n+//\n+//    // build the executable jar's classpath.\n+//    // (why is it necessary to explicitly remove the target/{classes,resources} paths? hm.)\n+//    def dependentJars = {\n+//      val jars =\n+//      publicClasspath +++ mainDependencies.scalaJars --- mainCompilePath --- mainResourcesOutputPath\n+//      if (jars.get.find { jar => jar.name.startsWith(\"scala-library-\") }.isDefined) {\n+//        // workaround bug in sbt: if the compiler is explicitly included, don't include 2 versions\n+//        // of the library.\n+//        jars --- jars.filter { jar =>\n+//          jar.absolutePath.contains(\"/boot/\") && jar.name == \"scala-library.jar\"\n+//        }\n+//      } else {\n+//        jars\n+//      }\n+//    }\n+//\n+//    def dependentJarNames = dependentJars.getFiles.map(_.getName).filter(_.endsWith(\".jar\"))\n+//    override def manifestClassPath = Some(dependentJarNames.map { \"libs/\" + _ }.mkString(\" \"))\n+//\n+//    def distName = (artifactID + \"-\" + projectVersion.value)\n+//    def distPath = \"dist\" / distName ##\n+//\n+//    def configPath = \"config\" ##\n+//    def configOutputPath = distPath / \"config\"\n+//\n+//    def binPath = \"bin\" ##\n+//    def binOutputPath = distPath / \"bin\"\n+//\n+//    def distZipName = {\n+//      \"%s-%s.zip\".format(artifactID, projectVersion.value)\n+//    }\n+//\n+//    lazy val packageDistTask = task {\n+//      distPath.asFile.mkdirs()\n+//      (distPath / \"libs\").asFile.mkdirs()\n+//      binOutputPath.asFile.mkdirs()\n+//      configOutputPath.asFile.mkdirs()\n+//\n+//      FileUtilities.copyFlat(List(jarPath), distPath, log).left.toOption orElse\n+//              FileUtilities.copyFlat(dependentJars.get, distPath / \"libs\", log).left.toOption orElse\n+//              FileUtilities.copy((configPath ***).get, configOutputPath, log).left.toOption orElse\n+//              FileUtilities.copy((binPath ***).get, binOutputPath, log).left.toOption orElse\n+//              FileUtilities.zip(((\"dist\" / distName) ##).get, \"dist\" / distZipName, true, log)\n+//      None\n+//    }\n+//\n+//    val PackageDistDescription = \"Creates a deployable zip file with dependencies, config, and scripts.\"\n+//    lazy val packageDist = packageDistTask dependsOn(`package`, `test`) describedAs PackageDistDescription\n+//\n+//    val cleanDist = cleanTask(\"dist\" ##) describedAs(\"Erase any packaged distributions.\")\n+//    override def cleanAction = super.cleanAction dependsOn(cleanDist)\n+//\n+//    override def javaCompileOptions = super.javaCompileOptions ++\n+//      List(JavaCompileOption(\"-source\"), JavaCompileOption(\"1.5\"))\n+//  }\n+//\n+//  class KafkaExamplesProject(info: ProjectInfo) extends DefaultProject(info)\n+//     with IdeaProject\n+//     with CoreDependencies {\n+//    val examplesPackageAction = packageAllAction\n+//    val dependsOnCore = core\n+//  //The issue is going from log4j 1.2.14 to 1.2.15, the developers added some features which required\n+//  // some dependencies on various sun and javax packages.\n+//   override def ivyXML =\n+//    <dependencies>\n+//      <exclude module=\"javax\"/>\n+//      <exclude module=\"jmxri\"/>\n+//      <exclude module=\"jmxtools\"/>\n+//      <exclude module=\"mail\"/>\n+//      <exclude module=\"jms\"/>\n+//    </dependencies>\n+//\n+//    override def artifactID = \"kafka-java-examples\"\n+//    override def filterScalaJars = false\n+//  }\n+//\n+//  class KafkaPerfProject(info: ProjectInfo) extends DefaultProject(info)\n+//      with IdeaProject\n+//      with CoreDependencies {\n+//    val perfPackageAction = packageAllAction\n+//    val dependsOnCore = core\n+//  //The issue is going from log4j 1.2.14 to 1.2.15, the developers added some features which required\n+//  // some dependencies on various sun and javax packages.\n+//   override def ivyXML =\n+//    <dependencies>\n+//      <exclude module=\"javax\"/>\n+//      <exclude module=\"jmxri\"/>\n+//      <exclude module=\"jmxtools\"/>\n+//      <exclude module=\"mail\"/>\n+//      <exclude module=\"jms\"/>\n+//    </dependencies>\n+//\n+//    override def artifactID = \"kafka-perf\"\n+//    override def filterScalaJars = false\n+//  }\n+//\n+//  class ContribProject(info: ProjectInfo) extends ParentProject(info) with IdeaProject {\n+//    lazy val hadoopProducer = project(\"hadoop-producer\", \"hadoop producer\",\n+//                                      new HadoopProducerProject(_), core)\n+//    lazy val hadoopConsumer = project(\"hadoop-consumer\", \"hadoop consumer\",\n+//                                      new HadoopConsumerProject(_), core)\n+//\n+//    val producerPackageAction = hadoopProducer.producerPackageAction\n+//    val consumerPackageAction = hadoopConsumer.consumerPackageAction\n+//\n+//    class HadoopProducerProject(info: ProjectInfo) extends DefaultProject(info)\n+//      with IdeaProject\n+//      with CoreDependencies {\n+//      val producerPackageAction = packageAllAction\n+//      override def ivyXML =\n+//       <dependencies>\n+//         <exclude module=\"netty\"/>\n+//           <exclude module=\"javax\"/>\n+//           <exclude module=\"jmxri\"/>\n+//           <exclude module=\"jmxtools\"/>\n+//           <exclude module=\"mail\"/>\n+//           <exclude module=\"jms\"/>\n+//       </dependencies>\n+//\n+//      val avro = \"org.apache.avro\" % \"avro\" % \"1.4.1\"\n+//      val jacksonCore = \"org.codehaus.jackson\" % \"jackson-core-asl\" % \"1.5.5\"\n+//      val jacksonMapper = \"org.codehaus.jackson\" % \"jackson-mapper-asl\" % \"1.5.5\"\n+//    }\n+//\n+//    class HadoopConsumerProject(info: ProjectInfo) extends DefaultProject(info)\n+//      with IdeaProject\n+//      with CoreDependencies {\n+//      val consumerPackageAction = packageAllAction\n+//      override def ivyXML =\n+//       <dependencies>\n+//         <exclude module=\"netty\"/>\n+//           <exclude module=\"javax\"/>\n+//           <exclude module=\"jmxri\"/>\n+//           <exclude module=\"jmxtools\"/>\n+//           <exclude module=\"mail\"/>\n+//           <exclude module=\"jms\"/>\n+//       </dependencies>\n+//\n+//      val jodaTime = \"joda-time\" % \"joda-time\" % \"1.6\"\n+//      val httpclient = \"commons-httpclient\" % \"commons-httpclient\" % \"3.1\"\n+//    }\n+//  }\n+//\n+//  trait TestDependencies {\n+//    val easymock = \"org.easymock\" % \"easymock\" % \"3.0\" % \"test\"\n+//    val junit = \"junit\" % \"junit\" % \"4.1\" % \"test\"\n+//    val scalaTest = \"org.scalatest\" % \"scalatest\" % \"1.2\" % \"test\"\n+//  }\n+//\n+//  trait CoreDependencies {\n+//    val log4j = \"log4j\" % \"log4j\" % \"1.2.15\"\n+//    val jopt = \"net.sf.jopt-simple\" % \"jopt-simple\" % \"3.2\"\n+//  }\n+//\n+//}\ndiff --git a/project/plugins/Plugins.scala b/project/plugins/Plugins.scala\ndeleted file mode 100644\nindex 6938e30..0000000\n--- a/project/plugins/Plugins.scala\n+++ /dev/null\n@@ -1,6 +0,0 @@\n-import sbt._\n-\n-class Plugins(info: ProjectInfo) extends PluginDefinition(info) {\n-  val repo = \"GH-pages repo\" at \"http://mpeltonen.github.com/maven/\"\n-  val idea = \"com.github.mpeltonen\" % \"sbt-idea-plugin\" % \"0.1-SNAPSHOT\"\n-}\ndiff --git a/sbt b/sbt\nindex 7d8b41e..657e990 100755\n--- a/sbt\n+++ b/sbt\n@@ -1 +1,2 @@\n-java -Xmx1024M -XX:MaxPermSize=512m -jar `dirname $0`/lib/sbt-launch.jar \"$@\"\n+java -Xmx1024M -XX:MaxPermSize=512m -jar `dirname $0`/sbt-launch.jar \"$@\"\n+\n", "Josh, you are seriously my hero. Drinks are owed.", "Thanks for the patch Josh ! Well in time for 0.7 release.\n\nPlease could you attach the diff file here and also select the \"grant it to Apache\" option ? That way we can apply, test and accept the patch.", "I came into Mountain View today on a loooong train ride and my internet died right after I posted this. I wanted to say that it wasn't quite ready, but that we should sit down today and go through this so we can perfect it for kafka's long term needs.\n\nChris Riccomini reminded me about this on Friday and I promised myself I would finish it over the weekend - I think I'm about 3 weeks overdue.", "Hi, has there been any progress on this ticket ?\n\na pom file in place will be very useful to set up a project.\n\nThanks for the effort here.", "Patricio,\n\nThis ticket is blocked on KAFKA-134. Once that is resolved, I believe this ticket will be easier to resolve.\n\nThanks,\nNeha", "The pom.xml contained in this gist, coupled with the artifact built by the source contents of the kafka-0.7-incubating-src archive can be deployed to a maven repository and ease things for java clients.", "Also, instead of having sbt (and pom) sections that exclude log4j 1.2.15's pulled-in dependency, you could simply update to log4j 1.2.16", "Fwiw, this is the pom I currently use for uploading to our internal maven repo.", "+1 for this.  Funny, it looks like it has been exactly 365 days since the issue was opened and a bit over 6 months since the last comment.  KAFKA-134 is still blocking... :(  [~jkreps], [~charmalloc], & co, any chance of committing KAFKA-134 to unblock this issue (9 votes)?\n", "bump... +1\n\nCan this be done? We're looking to consume Kafka...\n\nAt least to the Sonatype OSS repo or somewhere?\nhttps://docs.sonatype.org/display/Repository/Sonatype+OSS+Maven+Repository+Usage+Guide", "One issue with doing this for the 0.8 branch is that it contains ~3 dependencies that are checked into the kafka source itself...", "Maybe the thing to do then is to help owners of those projects Kafka depends on get their stuff into Maven, starting by opening tickets in their issue tracker.\n[~meder@mcs.anl.gov] or [~smeder] - which dependencies are those specifically?\n", "The third party dependencies that I am aware of are:\n\ncom.yammer.metrics, metrics-core\ncom.yammer.metrics, metrics-annotation\n\nand a custom zkclient jar \n", "Coda Hale metrics does seem to be in Maven repo already, see: http://mvnrepository.com/artifact/com.yammer.metrics\nThough from KAFKA-585 it looks like there may be a dependency on Metrics 3.*, which is not in any Maven repo yet.\n\nCustom zkclient jar... maybe a Kafka developer in the know can jump in?\nMaybe this jar could be included/hidden in one of the Kafka jars, so it is not treated as external dependencies?\nOh, is that zkclient Stefan's zkclient? This one: http://mvnrepository.com/artifact/com.github.sgroschupf/zkclient/0.1 but locally modified, jarred, and stuck in svn?\nMaybe we could instead provide a patch for zkclient and get zkclient maintainer(s) to apply it?  I knows Stefan personally and could bug him about it.... he is a busy CEO these days, but maybe somebody else on his team could get this done.\n", "I noticed Fix Version is not set, but if you look at https://issues.apache.org/jira/browse/KAFKA#selectedTab=com.atlassian.jira.plugin.system.project%3Apopularissues-panel you'll see this is THE most popular Kafka issue.\n\nShouldn't this be set to 0.8, so it gets done for/with the upcoming release?\n", "the zkclient dep moved to maven a while back it is not modified IIRC\n\nOn Tue, Nov 13, 2012 at 1:45 AM, Otis Gospodnetic (JIRA)\n", ">> Custom zkclient jar... maybe a Kafka developer in the know can jump in?\n\nThat's because we modified one of the zk client APIs to expose the stat object returned from the underlying zookeeper APIs. But after that, zkclient was never published to Maven.", "Thanks Neha.\nSo is the blocker for this issue the fact that there is a custom zkclient?\nShould the modified API be provided as a patch to the original zkclient?\nShould the zkclient maintainers be nudged to publish the new zkclient to a Maven repo?\n", ">> So is the blocker for this issue the fact that there is a custom zkclient?\nI believe so.\n\n>> Should the modified API be provided as a patch to the original zkclient?\nIt is already checked into zkclient trunk\n\n>> Should the zkclient maintainers be nudged to publish the new zkclient to a Maven repo?\nSure, please feel free to ping them.", "FYI just emailed Stefan asking if zkclient 0.2 could be cut and published in Maven.  He replied a few minutes later from a cab and CCed his colleague asking him to do this, so if stars align, you may see zkclient 0.2 out and you can remove it from Kafka.\n\nAnd then I really really hope you can publish Kafka to a Maven repo! :)\n\nQuestions/comments:\n* How come this is not Fix Version 0.8?  Should it not be?  Is it not planned for the upcoming 0.8?\n* Nobody is assigned.  Is it known who will do this?\n* It looks like KAFKA-134 is marked as blocker, but it's also not assigned and is not set for 0.8...?\n", "* How come this is not Fix Version 0.8? Should it not be? Is it not planned for the upcoming 0.8?\nJust changed the affected version to 0.8\n\n* Nobody is assigned. Is it known who will do this?\nAnyone who is interested can pick this up. Feel free to assign this to yourself. \n\n* It looks like KAFKA-134 is marked as blocker, but it's also not assigned and is not set for 0.8...?\nKAFKA-134 is required only to be able to publish to maven using an sbt command. For 0.8, if that is not ready, we can manually publish it. All the assignee would do in this case, is figure out the procedure to get this done and post it here. One of the committers can help roll the ball.", "Here is the procedure that one of Sematext engineers shared with the rest of our team when he set up one of our OSS projects in Sonatype's Maven repo (so ignore references to ActionGenerator):\n\n1. The first thing is to create an account at \nhttps://issues.sonatype.org/. \n\n2. Second thing is to create a ticket at \nhttps://issues.sonatype.org/browse/OSSRH with 'Project' type and \nsubtype of 'Support - Open Source Project Repository Hosting' and \nissue type of 'New Project'. You need to fill all the information \nthere and submit issue. After that you'll have to wait from a few \nhours to two days for it to be ready. \n\n3. After 2) is ready you need to install GPG client, create your key \nand publish it: \n * gpg --gen-key (to generate key) \n * gpg --list-keys (to list keys) \n * gpg --keyserver hkp://pool.sks-keyservers.net --send-keys YOUR_KEY_ID (to upload it) \n * gpg --keyserver hkp://pool.sks-keyservers.net --recv-keys YOUR_KEY_ID (to check if it was uploaded) \n  \n4. Add GPG maven plugin to your project. The best way is to create a \nnew profile, not to sign artifacts during local builds. With an \nprofile like the one below, you use if by adding -DperformRelease=true \nto maven command. It will ask for your key password and sign artifacts \nwith your key. This is needed to publish artifacts to maven \nrepository. I've created the following profile: \n\n <profiles> \n  <profile> \n    <id>release-sign-artifacts</id> \n    <activation> \n      <property> \n        <name>performRelease</name> \n        <value>true</value> \n      </property> \n    </activation> \n    <build> \n      <plugins> \n        <plugin> \n          <groupId>org.apache.maven.plugins</groupId> \n          <artifactId>maven-gpg-plugin</artifactId> \n          <version>1.1</version> \n          <executions> \n            <execution> \n              <id>sign-artifacts</id> \n              <phase>verify</phase> \n              <goals> \n                <goal>sign</goal> \n              </goals> \n            </execution> \n          </executions> \n        </plugin> \n      </plugins> \n    </build> \n  </profile> \n </profiles> \n  \n 5. You need to ensure that you have the following in your POM files: \n  * <modelVersion> \n  * <groupId> \n  * <artifactId> \n  * <version> \n  * <packaging> \n  * <name> \n  * <description> \n  * <url> \n  * <licenses> \n  * <scm><url> \n  * <scm><connection> \n  * <developers> \n  \n 6. Add Sonatype parent to your parent POM: \n  <parent> \n    <groupId>org.sonatype.oss</groupId> \n    <artifactId>oss-parent</artifactId> \n    <version>7</version> \n  </parent> \n  \n 7. Add SCM information to your project. For example like this: \n  <scm> \n    <connection>scm:git:git@github.com:sematext/ActionGenerator.git</connection> \n    <developerConnection>scm:git:git@github.com:sematext/ActionGenerator.git</developerConnection> \n    <url>https://github.com/sematext/ActionGenerator</url> \n  </scm>   \n  \n8. Modify your settings.xml file (you should have it in your maven \nhome directory) and add the following: \n <servers> \n    <server> \n      <id>sonatype-nexus-snapshots</id> \n      <username>your-jira-id</username> \n      <password>your-jira-pwd</password> \n    </server> \n    <server> \n      <id>sonatype-nexus-staging</id> \n      <username>your-jira-id</username> \n      <password>your-jira-pwd</password> \n    </server> \n  </servers> \n  \nWhere: \n * your-jira-id is the ID of your account created in step 1 \n * your-jira-pwd is the password for account created in step 1 \n  \n9. Build your project, sign artifacts and deploy: \n $mvn clean install deploy -DperformRelease=true \n  \n10. After that, your artifcts should be uloaded to staging sonatype. \nYou now should login into https://oss.sonatype.org/ with the \npreviously created account, choose repository and click \"Close\". If \nyou did everything right, close will be successful. \n\n11. Now click \"Release\" on the closed repository and comment on the \nissue created in step 2) that you released the artifacts. Someone at \nSonatype will review the artifacts and accept it. The review process \nwill only happen once and the initial release. Later releases will be \nreleased automaticaly. \n\nThis is what I did. Now I'm waiting for ActionGenerator to be \nreleased. In case you would like to go deeper into that process, there \nis a nice documentation available at \nhttps://docs.sonatype.org/display/Repository/Sonatype+OSS+Maven+Repository+Usage+Guide\n", "Update on the zklient front - it's getting Sonatyped: https://issues.sonatype.org/browse/OSSRH-4783 and Johannes Zillmann says it should be in there in 2-3 days.\n", "Hey Otis, that's awesome. Thanks for taking point on that and keeping the focus on this issue.", "There you have it:\n\nzkclient 0.2:\nhttp://search.maven.org/#artifactdetails%7Ccom.101tec%7Czkclient%7C0.2%7Cjar\n\nmetrics 2.2.0:\nhttp://search.maven.org/#artifactdetails%7Ccom.yammer.metrics%7Cmetrics-parent%7C2.2.0%7Cpom\n\nThis is now the most popular Kafka issue by far!\n", "That's great. Thanks a lot Otis for seeing this through. We will look into the process of publishing to Maven now. ", "Taking Otis notes into account, Here is the simplest patch I could make, that applies cleanly on 0.8:\n\nHere are the changes: \n\n* Version is 0.8.0-SNAPSHOT until 0.8.0 becomes stable\n* You need to change the publish URL and credentials location\n* Use com.101tec.zkclient 0.2\n* Use com.yammer.metrics.metrics-* 3.0.0-SNAPSHOT (I had to use snapshot because there is no 3.0.0 stable)\n* Use org.slf4j.slf4j-simple 1.6.4 (\"latest.release\" was used)\n\nTo publish: \n\n* ./sbt publish-local && ./sbt publish\n\nThis will create a pom for kafka that depends on all the subprojects. The \"main\" project that you generally want to use is core-kafka_2.8.0 like this:\n\n<dependency>\n  <groupId>kafka</groupId>\n  <artifactId>core-kafka_2.8.0</artifactId>\n  <version>0.8.0-SNAPSHOT</version>\n</dependency>", "this looks really good so far I was able to get ./sbt publish-local to run just fine and integrate the result loca .iv2 with another sbt project I have using \"kafka\" % \"core-kafka_2.8.0\" % \"0.8.0-SNAPSHOT\"\n\ncouple things\n\n1) the groupID should be = org.apache  \n2) do you see any reason this would not run on the 0.7.2 branch? I am going to propose a 0.7.3 release with this change also in there (less the metrics as that is only in 0.8 and changes to version all minor details). I don't but figure I would ask \n\nnotes for self: http://www.apache.org/dev/publishing-maven-artifacts.html", "I committed this with 2 minor changes \n\n1) removed the commented out line of code that is not needed anymore \n2) overrode the organization to be org.apache so the groupId shows up as such\n\n\nsbt works for using this locally now\n\n\"org.apache\" % \"core-kafka_2.8.0\" % \"0.8.0-SNAPSHOT\"\n\nit is a great step forward toward for helping folks to use the core code.  thanks!\n\nany other issues that come up I will follow-up accordingly with them and will shepherd that we publish once released (since we are not there yet it is premature but saw no reason to not get these changes up in trunk and resolve this ticket)\n", "Joe, do you plan to merge the commit to the 0.8 branch?", "Just to clarify -- this is marked as resolved, but I don't see any artifacts in maven central (search.maven.org).  Is there some other repo to which the artifacts have been released?", "<< Just to clarify -- this is marked as resolved, but I don't see any artifacts in maven central (search.maven.org). Is there some other repo to which the artifacts have been released?\n\n0.8 is not released yet.  once it is the artifacts for 0.8 will be published", "committed to 0.8 branch also", "I'm new to maven, so this may be a dumb question: would it be reasonable/easy to publish nightly jars (via Apache?), under the 0.8.0-SNAPSHOT tag?  I have an automated build system that currently git's and builds the whole thing, which I would love to replace with simple jar files.", "Are we expecting  0.7.3 release with feature to publish to maven repository..? If so do you know the time frame.. ? Also publish-local publish it to local ivy repository.. Is there a way to change it to publish to local maven repository..?\n\nRaja.", "0.8.x needs to depend on a released version of metrics.  \n\nI currently use yammer-metrics in my code, and was excited to see that kafka was going to use metrics so that we could clean up and standardize things there.  However, since it depends essentially on trunk (3.0.0-SNAPSHOT) including Kafka 0.8 breaks and there is no release plan or date for when 3.0.0 will even exist.\n\nEven worse, if we try and compile against the CURRENT yammer-metrics head 3.0.0-SNAPSHOT kafka fails to compile because it is ... trunk and things change.\n\n0.8.x needs to depend on a released version of metrics.", "Additionally, 0.8.x can not be an official Apache release in this form because the source tarball would not compile or provide all of the tools necessary to build or reference publicly available libraries (i.e. the metrics snapshot jar is ephemeral).", "This ticket is marked as fixed, but the latest on the 0.8 branch still depends on an unpublished version of Yammer - this is from Build.scala:\n\n <dependency>\n        <groupId>com.yammer.metrics</groupId>\n        <artifactId>metrics-core</artifactId>\n        <version>3.0.0-c0c8be71</version>\n        <scope>compile</scope>\n      </dependency>\n      <dependency>\n        <groupId>com.yammer.metrics</groupId>\n        <artifactId>metrics-annotations</artifactId>\n        <version>3.0.0-c0c8be71</version>\n        <scope>compile</scope>\n      </dependency>\n\nMaven Central doesn't have a version anything like the one mentioned above: http://repo2.maven.org/maven2/com/yammer/metrics/metrics-core/", "I filed KAFKA-826 to track the metrics 2.2.0 change. Patches are welcome !", "Silly question, but I just downloaded 0.8 branch and I see no pom.xml.\nI would like to locally build kafka and publish to our private local maven repo.\nIs this currently possible? Do I have to do something for the pom.xml to get generated.\nSorry for the newbie question.\n\nI noticed that when I run ./sbt publish-local, the artifacts get pushed to my local repo:\n~/.ivy2/local/org.apache/contrib_2.8.0\n~/.ivy2/local/org.apache/kafka_2.8.0\n\nI would like to publish to our private local maven repo (which just so happens to be in S3), but I'm used to modifying a pom.xml to add the required plugins/repoID's/etc/etc/, hence the question on where the pom.xml is.\n\nThanks!", "Have you tried the following as listed in earlier comments?\n\n./sbt publish-local && ./sbt publish ", "The build is using SBT, whic is using Ivy behind the scenes, so there is no pom.xml.  One is generated during the build and can be published to a maven repo if SBT is configured correctly.  I'm not familiar with how to do that.\n\nThe best I can find quickly with a search are:\nhttps://groups.google.com/forum/?fromgroups=#!topic/simple-build-tool/CJS8GvXO4j0\nhttps://groups.google.com/forum/?fromgroups=#!topic/simple-build-tool/HaEv4P9Mxkw\n", "Thanks Scott, for the links. Just searching for \"sbt s3\" does not get too many hits, but the best one by far is:\nhttp://stackoverflow.com/questions/9958103/how-to-publish-to-amazon-s3-with-sbt\nSo far, from what I can tell, either copy the ivy jar and do the following (the million dollar question is how to configure the setters):\n{\n  val s3r = new org.springframework.aws.ivy.S3Resolver\n  // call its setters to configure it, see api docs above etc.\n  publishTo := Some(new sbt.RawRepository(s3r)\n}\nor use sbt-s3, which I haven't had any success in.\nNonetheless, I think I have enough to experiment with.\nThanks!", "When I type: ./sbt actions, I see one to create a POM.xml\n\nWe're having to build kafka and host it on S3 ourselves, which is unfortunate."], "derived": {"summary": "The released kafka jar must be download manually and then deploy to a private repository before they can be used by a developer using maven2. Similar to other Apache projects, it will be nice to have a way to publish Kafka releases to a public maven repo.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Publish kafka jar to a public maven repository - The released kafka jar must be download manually and then deploy to a private repository before they can be used by a developer using maven2. Similar to other Apache projects, it will be nice to have a way to publish Kafka releases to a public maven repo."}, {"q": "What updates or decisions were made in the discussion?", "a": "When I type: ./sbt actions, I see one to create a POM.xml\n\nWe're having to build kafka and host it on S3 ourselves, which is unfortunate."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-134", "title": "Upgrade Kafka to sbt 0.11.3", "status": "Resolved", "priority": "Major", "reporter": "Joshua Hartman", "assignee": null, "labels": [], "created": "2011-09-15T18:46:34.000+0000", "updated": "2013-01-21T04:00:08.000+0000", "description": "Upgrading to sbt 0.10.1 is a nice to have as sbt moves forward. Plus, it's a requirement for me to help publish Kafka to maven :)", "comments": ["Better late than never.", "So Neha, can you review this patch. I would love to get this applied so we can get the eclipse support working again.", "Is 0.10 still the most recent version, or would it be easier to just jump to 0.11?", "Well either way. The problem is I am totally lost in SBT with any version, but I feel the longer this sits the more it gets out of sync with all the various build files changes we are making. I tried to review this but basically failed after about 45 minutes of training to grok the weirdness of SBT, so i am hoping someone more experienced can take a shot.", "This patch doesn't apply cleanly and has at least one bug -\n\nnnarkhed-ld:kafka-134 nnarkhed$ cat sbt\njava -Xmx1024M -XX:MaxPermSize=512m -jar `dirname $0`/sbt-launch.jar \"$@\"\n\nEven when the patch was uploaded sbt-launch.jar was never in the top-level directory. Was this patch developed against trunk ?\nCan you upload a patch that works against trunk ?\n", "Attaching a patch that will apply cleanly against trunk. This is a draft patch, as I just took the previous patch and corrected -\n\n1. The sbt file to point to the lib directory for the sbt-launch.jar\n2. The sbt-launch.jar for 0.10.0\n3. svn delete on the old KafkaProject.scala and Plugins.scala files\n4. changed build.properties to change the SBT version to 0.10.0\n\nAfter these changes, the patch will apply cleanly against trunk, will pull SBT 0.10.0, but I still get the following error on the new Build.scala file -\n\nnnarkhed-ld:kafka-134 nnarkhed$ ./sbt\n[info] Set current project to default (in build file:/home/nnarkhed/Projects/kafka-134/project/plugins/)\n[info] Compiling 1 Scala source to /home/nnarkhed/Projects/kafka-134/project/target/scala_2.8.1/classes...\n[error] /home/nnarkhed/Projects/kafka-134/project/Build.scala:130: not found: value description\n[error] Error occurred in an application involving default arguments.\n[error]       pomExtra <<= (pomExtra, name, description) { buildPomExtra }\n[error]                                     ^\n[error] /home/nnarkhed/Projects/kafka-134/project/Build.scala:137: not found: value description\n[error] Error occurred in an application involving default arguments.\n[error]       description := \"Includes all of kafka project in one\",\n[error]       ^\n[error] two errors found\n\nI probably won't be able to work on this before KAFKA-198. Uploading this patch in case someone else wants to take a look.", "we should look at going up to 11.2 now once we catch up maintence of it should not be so bad.\n\nif anyone else can take a look that would be great I can review it.    I will check out what was done already maybe I can see where the issue is because I had similar problems to Neha when I tried it needs a clean once through", "The patch applied for me\n\nJosephs-MacBook-Pro:kafka_134 josephstein$ patch -p0 -i ~/Downloads/kafka-134-v2-draft.latest.patch \npatching file sbt\npatching file project/build/KafkaProject.scala\nReversed (or previously applied) patch detected!  Assume -R? [n] y\npatching file project/plugins/Plugins.scala\npatching file project/Build.scala\npatching file project/build.properties\n\nand then I downloaded sbt 11.3 and when i do ./sbt compile i get this error (updated not required anymore)\n\n\nJosephs-MacBook-Pro:kafka_134 josephstein$ ./sbt compile\n[warn] Using project/plugins/ is deprecated for plugin configuration (/opt/apache/kafka_134/project/plugins).\n[warn] Put .sbt plugin definitions directly in project/,\n[warn]   .scala plugin definitions in project/project/,\n[warn]   and remove the project/plugins/ directory.\n[info] Loading project definition from /opt/apache/kafka_134/project/plugins\n[info] Set current project to root (in build file:/opt/apache/kafka_134/)\n[info] Compiling 137 Scala sources and 3 Java sources to /opt/apache/kafka_134/core/target/scala-2.9.1/classes...\n[error] /opt/apache/kafka_134/core/src/main/scala/kafka/consumer/ConsoleConsumer.scala:22: object I0Itec is not a member of package org\n[error] import org.I0Itec.zkclient._\n[error]            ^\n[error] /opt/apache/kafka_134/core/src/main/scala/kafka/consumer/Fetcher.scala:22: object I0Itec is not a member of package org\n[error] import org.I0Itec.zkclient.ZkClient\n[error]            ^\n[error] /opt/apache/kafka_134/core/src/main/scala/kafka/consumer/FetcherRunnable.scala:24: object I0Itec is not a member of package org\n[error] import org.I0Itec.zkclient.ZkClient\n[error]            ^\n[error] /opt/apache/kafka_134/core/src/main/scala/kafka/utils/ZkUtils.scala:20: object I0Itec is not a member of package org\n[error] import org.I0Itec.zkclient.ZkClient\n[error]            ^\n[error] /opt/apache/kafka_134/core/src/main/scala/kafka/consumer/TopicCount.scala:22: object I0Itec is not a member of package org\n[error] import org.I0Itec.zkclient.ZkClient\n[error]            ^\n[error] /opt/apache/kafka_134/core/src/main/scala/kafka/consumer/ZookeeperConsumerConnector.scala:26: object I0Itec is not a member of package org\n[error] import org.I0Itec.zkclient.exception.ZkNodeExistsException\n[error]            ^\n[error] /opt/apache/kafka_134/core/src/main/scala/kafka/consumer/ZookeeperConsumerConnector.scala:28: object I0Itec is not a member of package org\n[error] import org.I0Itec.zkclient.{IZkStateListener, IZkChildListener, ZkClient}\n[error]            ^\n[error] /opt/apache/kafka_134/core/src/main/scala/kafka/consumer/ZookeeperConsumerConnector.scala:29: object zookeeper is not a member of package org.apache\n[error] import org.apache.zookeeper.Watcher.Event.KeeperState\n[error]                   ^\n[error] /opt/apache/kafka_134/core/src/main/scala/kafka/consumer/ZookeeperTopicEventWatcher.scala:22: object I0Itec is not a member of package org\n[error] import org.I0Itec.zkclient.{IZkStateListener, IZkChildListener, ZkClient}\n[error]            ^\n[error] /opt/apache/kafka_134/core/src/main/scala/kafka/consumer/ZookeeperTopicEventWatcher.scala:23: object zookeeper is not a member of package org.apache\n[error] import org.apache.zookeeper.Watcher.Event.KeeperState\n[error]                   ^\n[error] /opt/apache/kafka_134/core/src/main/scala/kafka/message/CompressionUtils.scala:57: object xerial is not a member of package org\n[error]   import org.xerial.snappy.SnappyInputStream\n[error]              ^\n[error] /opt/apache/kafka_134/core/src/main/scala/kafka/message/CompressionUtils.scala:58: object xerial is not a member of package org\n[error]   import org.xerial.snappy.SnappyOutputStream\n[error]              ^\n[error] /opt/apache/kafka_134/core/src/main/scala/kafka/producer/ZKBrokerPartitionInfo.scala:25: object zookeeper is not a member of package org.apache\n[error] import org.apache.zookeeper.Watcher.Event.KeeperState\n[error]                   ^\n[error] /opt/apache/kafka_134/core/src/main/scala/kafka/producer/ZKBrokerPartitionInfo.scala:26: object I0Itec is not a member of package org\n[error] import org.I0Itec.zkclient.{IZkStateListener, IZkChildListener, ZkClient}\n[error]            ^\n[error] /opt/apache/kafka_134/core/src/main/scala/kafka/server/KafkaZooKeeper.scala:22: object I0Itec is not a member of package org\n[error] import org.I0Itec.zkclient.{IZkStateListener, ZkClient}\n[error]            ^\n[error] /opt/apache/kafka_134/core/src/main/scala/kafka/server/KafkaZooKeeper.scala:23: object I0Itec is not a member of package org\n[error] import org.I0Itec.zkclient.exception.ZkNodeExistsException\n[error]            ^\n[error] /opt/apache/kafka_134/core/src/main/scala/kafka/server/KafkaZooKeeper.scala:24: object zookeeper is not a member of package org.apache\n[error] import org.apache.zookeeper.Watcher.Event.KeeperState\n[error]                   ^\n[error] /opt/apache/kafka_134/core/src/main/scala/kafka/tools/ConsumerOffsetChecker.scala:22: object I0Itec is not a member of package org\n[error] import org.I0Itec.zkclient.ZkClient\n[error]            ^\n[error] /opt/apache/kafka_134/core/src/main/scala/kafka/tools/ExportZkOffsets.scala:23: object I0Itec is not a member of package org\n[error] import org.I0Itec.zkclient.ZkClient\n[error]            ^\n[error] /opt/apache/kafka_134/core/src/main/scala/kafka/tools/ImportZkOffsets.scala:24: object I0Itec is not a member of package org\n[error] import org.I0Itec.zkclient.ZkClient\n[error]            ^\n[error] /opt/apache/kafka_134/core/src/main/scala/kafka/utils/Utils.scala:31: object I0Itec is not a member of package org\n[error] import org.I0Itec.zkclient.ZkClient\n[error]            ^\n[error] /opt/apache/kafka_134/core/src/main/scala/kafka/tools/ReplayLogProducer.scala:29: object I0Itec is not a member of package org\n[error] import org.I0Itec.zkclient._\n[error]            ^\n[error] /opt/apache/kafka_134/core/src/main/scala/kafka/tools/VerifyConsumerRebalance.scala:21: object I0Itec is not a member of package org\n[error] import org.I0Itec.zkclient.ZkClient\n[error]            ^\n[error] /opt/apache/kafka_134/core/src/main/scala/kafka/utils/UpdateOffsetsInZK.scala:20: object I0Itec is not a member of package org\n[error] import org.I0Itec.zkclient.ZkClient\n[error]            ^\n[error] /opt/apache/kafka_134/core/src/main/scala/kafka/utils/ZkUtils.scala:21: object I0Itec is not a member of package org\n[error] import org.I0Itec.zkclient.serialize.ZkSerializer\n[error]            ^\n[error] /opt/apache/kafka_134/core/src/main/scala/kafka/utils/ZkUtils.scala:25: object I0Itec is not a member of package org\n[error] import org.I0Itec.zkclient.exception.{ZkNodeExistsException, ZkNoNodeException, ZkMarshallingError}\n[error]            ^\n[error] 26 errors found\n[error] {file:/opt/apache/kafka_134/}core/compile:compile: Compilation failed\n[error] Total time: 14 s, completed May 14, 2012 9:33:22 PM\nJosephs-MacBook-Pro:kafka_134 josephstein$ \n\n\nnoticed we should/could move some things into build.sbt probably some other linking issue now that lib_managed is in the ivy cache causing this error...\n\nseems something simple might be wrong here wanted to post my results in case anyone had any ideas? I will bang on this ticket some ... lots of community benefit to get a more update version going here IMHO", "I have updated a new patch v3 https://gist.github.com/2699027 to resolve the issue I commented about when trying the previous patch\n\nI checked it compiles with SBT 0.11.3 http://typesafe.artifactoryonline.com/typesafe/ivy-releases/org.scala-sbt/sbt-launch/0.11.3/sbt-launch.jar\n\nnew issue is that tests are not running at all just error\n\nJosephs-MacBook-Pro:kafka_134 josephstein$ ./sbt test\n[warn] Using project/plugins/ is deprecated for plugin configuration (/opt/apache/kafka_134/project/plugins).\n[warn] Put .sbt plugin definitions directly in project/,\n[warn]   .scala plugin definitions in project/project/,\n[warn]   and remove the project/plugins/ directory.\n[info] Loading project definition from /opt/apache/kafka_134/project/plugins\n[info] Set current project to root (in build file:/opt/apache/kafka_134/)\n[info] No tests to run for root/test:test\n[info] No tests to run for contrib/test:test\n[info] Compiling 45 Scala sources to /opt/apache/kafka_134/core/target/scala-2.9.1/test-classes...\n[info] No tests to run for perf/test:test\n[info] No tests to run for examples/test:test\n[info] No tests to run for hadoop-consumer/test:test\n[info] No tests to run for hadoop-producer/test:test\n[warn] there were 16 deprecation warnings; re-run with -deprecation for details\n[warn] one warning found\n[error] Could not run test kafka.message.ByteBufferMessageSetTest: java.lang.ClassCastException: scala.collection.immutable.Set$EmptySet$ cannot be cast to scala.collection.generic.Addable\n[error] Could not run test kafka.log.LogCorruptionTest: java.lang.ClassCastException: scala.collection.immutable.Set$EmptySet$ cannot be cast to scala.collection.generic.Addable\n[error] Could not run test kafka.integration.AutoOffsetResetTest: java.lang.ClassCastException: scala.collection.immutable.Set$EmptySet$ cannot be cast to scala.collection.generic.Addable\n[error] Could not run test kafka.message.CompressionUtilTest: java.lang.ClassCastException: scala.collection.immutable.Set$EmptySet$ cannot be cast to scala.collection.generic.Addable\n[error] Could not run test kafka.integration.BackwardsCompatibilityTest: java.lang.ClassCastException: scala.collection.immutable.Set$EmptySet$ cannot be cast to scala.collection.generic.Addable\n[error] Could not run test kafka.consumer.ZookeeperConsumerConnectorTest: java.lang.ClassCastException: scala.collection.immutable.Set$EmptySet$ cannot be cast to scala.collection.generic.Addable\n[error] Could not run test kafka.producer.AsyncProducerTest: java.lang.ClassCastException: scala.collection.immutable.Set$EmptySet$ cannot be cast to scala.collection.generic.Addable\n[error] Could not run test kafka.consumer.TopicFilterTest: java.lang.ClassCastException: scala.collection.immutable.Set$EmptySet$ cannot be cast to scala.collection.generic.Addable\n[error] Could not run test kafka.zk.ZKLoadBalanceTest: java.lang.ClassCastException: scala.collection.immutable.Set$EmptySet$ cannot be cast to scala.collection.generic.Addable\n[error] Could not run test kafka.message.MessageTest: java.lang.ClassCastException: scala.collection.immutable.Set$EmptySet$ cannot be cast to scala.collection.generic.Addable\n[error] Could not run test kafka.message.FileMessageSetTest: java.lang.ClassCastException: scala.collection.immutable.Set$EmptySet$ cannot be cast to scala.collection.generic.Addable\n[error] Could not run test kafka.log.SegmentListTest: java.lang.ClassCastException: scala.collection.immutable.Set$EmptySet$ cannot be cast to scala.collection.generic.Addable\n[error] Could not run test kafka.log.LogOffsetTest: java.lang.ClassCastException: scala.collection.immutable.Set$EmptySet$ cannot be cast to scala.collection.generic.Addable\n[error] Could not run test kafka.utils.UtilsTest: java.lang.ClassCastException: scala.collection.immutable.Set$EmptySet$ cannot be cast to scala.collection.generic.Addable\n[error] Could not run test kafka.producer.SyncProducerTest: java.lang.ClassCastException: scala.collection.immutable.Set$EmptySet$ cannot be cast to scala.collection.generic.Addable\n[error] Could not run test kafka.javaapi.integration.PrimitiveApiTest: java.lang.ClassCastException: scala.collection.immutable.Set$EmptySet$ cannot be cast to scala.collection.generic.Addable\n[error] Could not run test unit.kafka.producer.ProducerMethodsTest: java.lang.ClassCastException: scala.collection.immutable.Set$EmptySet$ cannot be cast to scala.collection.generic.Addable\n[error] Could not run test kafka.javaapi.consumer.ZookeeperConsumerConnectorTest: java.lang.ClassCastException: scala.collection.immutable.Set$EmptySet$ cannot be cast to scala.collection.generic.Addable\n[error] Could not run test kafka.javaapi.producer.SyncProducerTest: java.lang.ClassCastException: scala.collection.immutable.Set$EmptySet$ cannot be cast to scala.collection.generic.Addable\n[error] Could not run test kafka.consumer.FetcherTest: java.lang.ClassCastException: scala.collection.immutable.Set$EmptySet$ cannot be cast to scala.collection.generic.Addable\n[error] Could not run test kafka.log4j.KafkaLog4jAppenderTest: java.lang.ClassCastException: scala.collection.immutable.Set$EmptySet$ cannot be cast to scala.collection.generic.Addable\n[error] Could not run test kafka.integration.LazyInitProducerTest: java.lang.ClassCastException: scala.collection.immutable.Set$EmptySet$ cannot be cast to scala.collection.generic.Addable\n[error] Could not run test kafka.network.SocketServerTest: java.lang.ClassCastException: scala.collection.immutable.Set$EmptySet$ cannot be cast to scala.collection.generic.Addable\n[error] Could not run test kafka.log.LogManagerTest: java.lang.ClassCastException: scala.collection.immutable.Set$EmptySet$ cannot be cast to scala.collection.generic.Addable\n[error] Could not run test kafka.integration.PrimitiveApiTest: java.lang.ClassCastException: scala.collection.immutable.Set$EmptySet$ cannot be cast to scala.collection.generic.Addable\n[error] Could not run test kafka.zk.ZKEphemeralTest: java.lang.ClassCastException: scala.collection.immutable.Set$EmptySet$ cannot be cast to scala.collection.generic.Addable\n[error] Could not run test kafka.server.ServerShutdownTest: java.lang.ClassCastException: scala.collection.immutable.Set$EmptySet$ cannot be cast to scala.collection.generic.Addable\n[error] Could not run test kafka.log.LogTest: java.lang.ClassCastException: scala.collection.immutable.Set$EmptySet$ cannot be cast to scala.collection.generic.Addable\n[error] Could not run test kafka.javaapi.message.ByteBufferMessageSetTest: java.lang.ClassCastException: scala.collection.immutable.Set$EmptySet$ cannot be cast to scala.collection.generic.Addable\n[error] Could not run test kafka.javaapi.producer.ProducerTest: java.lang.ClassCastException: scala.collection.immutable.Set$EmptySet$ cannot be cast to scala.collection.generic.Addable\n[error] Could not run test kafka.producer.ProducerTest: java.lang.ClassCastException: scala.collection.immutable.Set$EmptySet$ cannot be cast to scala.collection.generic.Addable\n[error] Error: Total 0, Failed 0, Errors 0, Passed 0, Skipped 0\n[error] Error during tests:\n[error] \tkafka.log.LogManagerTest\n[error] \tkafka.log.LogTest\n[error] \tkafka.consumer.FetcherTest\n[error] \tkafka.integration.AutoOffsetResetTest\n[error] \tunit.kafka.producer.ProducerMethodsTest\n[error] \tkafka.javaapi.integration.PrimitiveApiTest\n[error] \tkafka.message.MessageTest\n[error] \tkafka.producer.ProducerTest\n[error] \tkafka.zk.ZKLoadBalanceTest\n[error] \tkafka.network.SocketServerTest\n[error] \tkafka.message.FileMessageSetTest\n[error] \tkafka.javaapi.producer.SyncProducerTest\n[error] \tkafka.javaapi.producer.ProducerTest\n[error] \tkafka.javaapi.consumer.ZookeeperConsumerConnectorTest\n[error] \tkafka.zk.ZKEphemeralTest\n[error] \tkafka.log.LogCorruptionTest\n[error] \tkafka.integration.LazyInitProducerTest\n[error] \tkafka.log.SegmentListTest\n[error] \tkafka.consumer.TopicFilterTest\n[error] \tkafka.consumer.ZookeeperConsumerConnectorTest\n[error] \tkafka.producer.AsyncProducerTest\n[error] \tkafka.log4j.KafkaLog4jAppenderTest\n[error] \tkafka.message.ByteBufferMessageSetTest\n[error] \tkafka.server.ServerShutdownTest\n[error] \tkafka.message.CompressionUtilTest\n[error] \tkafka.log.LogOffsetTest\n[error] \tkafka.javaapi.message.ByteBufferMessageSetTest\n[error] \tkafka.integration.BackwardsCompatibilityTest\n[error] \tkafka.integration.PrimitiveApiTest\n[error] \tkafka.utils.UtilsTest\n[error] \tkafka.producer.SyncProducerTest\n[error] {file:/opt/apache/kafka_134/}core/test:test: Tests unsuccessful\n[error] Total time: 31 s, completed May 15, 2012 12:05:45 AM\n", "If you're interested, I have also had a stab at porting Kafka to SBT 0.11.3 and Scala 2.9.2. The code is available here:\n\nhttps://github.com/cb372/kafka-1\n\nIt's all based on Kafka trunk, forked yesterday at revision 1339944.\n\nThere are 3 branches:\n\n* sbt0.11.3  <-- SBT updated to 0.11.3, Scala still at 2.8.0\n* scala2.9.2 <-- SBT 0.11.3, Scala updated to 2.9.2\n* break2.8compatibility  <-- SBT 0.11.3, Scala 2.9.2, all deprecation warnings fixed, code is now incompatible with Scala 2.8.x\n\nA few comments:\n\n* I'm no SBT guru so there may some craziness inside Build.scala. Hopefully it's understandable.\n* There is no fix for KAFKA-15 because I wasn't really sure of how it was supposed to work in the first place.\n* Worringly, after upgrading Scala to 2.9.2 about 1/3 of the tests started failing!\n* The reason that all the tests fail in Joe Stein's comment above is that ScalaTest has not been updated. Updating it to scalatest_2.9.2 v1.8 will fix this.\n* The run-rat.sh script had the jar's path hardcoded into it. Because the jar is now somewhere in the Ivy cache, rather than the lib_managed folder, I changed this so that SBT passes the jar's path to the script.\n\nFeel free to use any of this code, if it's of any help to you.\n", "This is great Chris. Guys, is there any objection to taking the scala2.9.2 branch? We should be able to cross-compile for 2.8 compatibility, right? I think it would be great to get this update done. It should probably be done one the 0.8 branch, though, right?", "I think if we can get the latest scala and sbt working on trunk for a 0.7.2 release we would open the project to more of the community", "<< The reason that all the tests fail in Joe Stein's comment above is that ScalaTest has not been updated. Updating it to scalatest_2.9.2 v1.8 will fix this. \n\nok, the tests are running now (great) but 75 tests are failing ( :( )\n\nlooks like some issues \n\n[info] - testBasic(kafka.javaapi.consumer.ZookeeperConsumerConnectorTest) *** FAILED ***\n[info]   java.net.BindException: Address already in use\n[info]   at sun.nio.ch.Net.bind(Native Method)\n[info]   at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:126)\n[info]   at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:59)\n[info]   at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:52)\n[info]   at org.apache.zookeeper.server.NIOServerCnxn$Factory.<init>(NIOServerCnxn.java:144)\n[info]   at org.apache.zookeeper.server.NIOServerCnxn$Factory.<init>(NIOServerCnxn.java:125)\n[info]   at kafka.zk.EmbeddedZookeeper.<init>(EmbeddedZookeeper.scala:31)\n[info]   at kafka.zk.ZooKeeperTestHarness$class.setUp(ZooKeeperTestHarness.scala:30)\n[info]   at kafka.javaapi.consumer.ZookeeperConsumerConnectorTest.setUp(ZookeeperConsumerConnectorTest.scala:34)\n[info]   at junit.framework.TestCase.runBare(TestCase.java:128)\n[info]   ...\n\n\nand\n\n[info] - testCollateAndSerializeEvents *** FAILED ***\n[info]   java.lang.AssertionError: Expectation failure on verify:\n[info]     multiSend([ProducerRequest(test1$topic,1,75), ProducerRequest(test-topic,0,70), ProducerRequest(test-topic,1,70), ProducerRequest(test1$topic,0,75)]): expected: 1, actual: 0\n[info]   at org.easymock.internal.MocksControl.verify(MocksControl.java:184)\n[info]   at org.easymock.EasyMock.verify(EasyMock.java:2038)\n[info]   at kafka.producer.AsyncProducerTest.testCollateAndSerializeEvents(AsyncProducerTest.scala:292)\n[info]   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n[info]   at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n[info]   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n[info]   at java.lang.reflect.Method.invoke(Method.java:597)\n[info]   at org.junit.internal.runners.TestMethodRunner.executeMethodBody(TestMethodRunner.java:99)\n[info]   at org.junit.internal.runners.TestMethodRunner.runUnprotected(TestMethodRunner.java:81)\n[info]   at org.junit.internal.runners.BeforeAndAfterRunner.runProtected(BeforeAndAfterRunner.java:34)\n[info]   ...\n\n\nare the two biggest culprits, guessing something with the scalatest updates from where we were to what is current.\n\ntrunk on my same machine runs fine (so the address in use is not localized it is something in how the tests are running causing it)\n\nI tried the scala2.9.2 branch of Chris's github fork but got the same issue I had before (tests not even running)\n \n\n", "when i use scalatest2.9.2 - 1.6.1 only 50 tests fail compared to the 74 failing with 1.8", "down to 1 test failing on trunk and a different 1 test failing on branch\n\ngot things working better now by adding\n\nparallelExecution in Test := false \n\ninto Build.scala 's buildSettings\n\non trunk failure is\n\n[info] - testCollateAndSerializeEvents *** FAILED ***\n[info]   java.lang.AssertionError: Expectation failure on verify:\n[info]     multiSend([ProducerRequest(test1$topic,1,75), ProducerRequest(test-topic,0,70), ProducerRequest(test-topic,1,70), ProducerRequest(test1$topic,0,75)]): expected: 1, actual: 0\n[info]   at org.easymock.internal.MocksControl.verify(MocksControl.java:184)\n[info]   at org.easymock.EasyMock.verify(EasyMock.java:2038)\n[info]   at kafka.producer.AsyncProducerTest.testCollateAndSerializeEvents(AsyncProducerTest.scala:292)\n[info]   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n[info]   at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n[info]   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n[info]   at java.lang.reflect.Method.invoke(Method.java:597)\n[info]   at org.junit.internal.runners.TestMethodRunner.executeMethodBody(TestMethodRunner.java:99)\n[info]   at org.junit.internal.runners.TestMethodRunner.runUnprotected(TestMethodRunner.java:81)\n[info]   at org.junit.internal.runners.BeforeAndAfterRunner.runProtected(BeforeAndAfterRunner.java:34)\n[info]   ...\n\non 0.8 branch, testFailedSendRetryLogic is failing ... \n\n[info]   Not expected (AsyncProducerTest.scala:483)", "\"parallelExecution in Test := false\" only prevents tests in the same subproject from running simultaneously. Is this what is intended? Unfortunately, SBT 0.10 and 0.11 are limited when it comes to controlling parallel execution. parallelExecution := false disables _all_ parallel execution (including compilation). This is all explained in the wiki (https://github.com/harrah/xsbt/wiki/Parallel-Execution) along with details in the new and experimental mechanism in SBT 0.12.", "Hmmm, I've already done work for upgrading to 0.11.3 on #139 (https://issues.apache.org/jira/browse/KAFKA-139). The test error you're seeing is because of a change in how EasyMock is used (see my notes in the ticket). Perhaps we should join forces (and tickets?) to get this pushed along.", "The main server does not seem to start after including Chris patch : scala dep is missing (no more scala-library file in ../boot/ folder) and class path is still referencing scala 2.8.0.", "all of the work has moved into KAFKA-139 to cross compile and upgrade to later SBT too"], "derived": {"summary": "Upgrading to sbt 0. 10.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Upgrade Kafka to sbt 0.11.3 - Upgrading to sbt 0. 10."}, {"q": "What updates or decisions were made in the discussion?", "a": "all of the work has moved into KAFKA-139 to cross compile and upgrade to later SBT too"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-135", "title": "the ruby kafka gem is not functional", "status": "Closed", "priority": "Major", "reporter": "Pierre-Yves Ritschard", "assignee": "Pierre-Yves Ritschard", "labels": [], "created": "2011-09-20T17:10:08.000+0000", "updated": "2013-03-04T17:02:50.000+0000", "description": "The gem spec is missing a file declaration, the resulting gem is thus unusable", "comments": ["Thanks for the patch, Pierre-Yves. Just committed this."], "derived": {"summary": "The gem spec is missing a file declaration, the resulting gem is thus unusable.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "the ruby kafka gem is not functional - The gem spec is missing a file declaration, the resulting gem is thus unusable."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for the patch, Pierre-Yves. Just committed this."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-136", "title": "A JMX bean that reports #message/sec in consumer", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Jun Rao", "labels": [], "created": "2011-09-22T17:34:55.000+0000", "updated": "2011-10-03T04:57:26.000+0000", "description": "We need to add a jmx bean that shows # messages consumed by a consumer. The question is whether we want to break this down by topic. If not, the change is pretty straightforward by adding a counter in ConsumerIterator.", "comments": ["Thanks for filing this JIRA Jun. It will be very helpful to have this data available. I do think that a per-topic JMX would be helpful.", "Attach a patch for adding a jmx bean that collects cumulated # of messages per topic in the consumer.\n\nI was trying to add a similar bean in the producer. The best place to add that seems to be in SyncProducer. However, we are forced to iterate all messages in the produce request. This probably will add non-trivial overhead, especially if the request is compressed. Any suggest on how to do this better is welcomed.\n", "Patch looks good. Having this information exposed as a JMX bean is useful. I was wondering if we should expose this metric for the SimpleConsumer, since that API is in use too ? ", "SimpleConsumer is a bit different. We only know that the user get a fetch response. However, we don't know if the messages in the response have been iterated over. So, for SimpleConsumer, we can only collect request level stats."], "derived": {"summary": "We need to add a jmx bean that shows # messages consumed by a consumer. The question is whether we want to break this down by topic.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "A JMX bean that reports #message/sec in consumer - We need to add a jmx bean that shows # messages consumed by a consumer. The question is whether we want to break this down by topic."}, {"q": "What updates or decisions were made in the discussion?", "a": "SimpleConsumer is a bit different. We only know that the user get a fetch response. However, we don't know if the messages in the response have been iterated over. So, for SimpleConsumer, we can only collect request level stats."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-137", "title": "Fix ASF license headers for C# client", "status": "Resolved", "priority": "Minor", "reporter": "Eric Hauser", "assignee": null, "labels": [], "created": "2011-09-22T18:00:32.000+0000", "updated": "2011-09-23T01:45:04.000+0000", "description": "Correct license headers for all .cs files.", "comments": ["Thanks, Eric. Just committed this."], "derived": {"summary": "Correct license headers for all. cs files.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Fix ASF license headers for C# client - Correct license headers for all. cs files."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks, Eric. Just committed this."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-138", "title": "Bug in the queue timeout logic of the async producer", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": null, "labels": [], "created": "2011-09-25T22:59:58.000+0000", "updated": "2011-09-28T00:52:18.000+0000", "description": "There is a bug in the queue timeout logic of the async producer. This bug shows up when the producer is very low throughput. The behavior observed by such very low throughput producers is delayed dispatching of the events. There is no observed data loss though.", "comments": ["This patch corrects the queue.poll logic to respect the queue timeout. Before this, under very low traffic, the producer ended up waiting for a timeout proportional to number of events added to the queue, the upper bound being the batch size. ", "+1", "Thanks, Neha. Just committed this."], "derived": {"summary": "There is a bug in the queue timeout logic of the async producer. This bug shows up when the producer is very low throughput.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Bug in the queue timeout logic of the async producer - There is a bug in the queue timeout logic of the async producer. This bug shows up when the producer is very low throughput."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks, Neha. Just committed this."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-139", "title": "cross-compile multiple Scala versions and upgrade to SBT 0.12.1", "status": "Resolved", "priority": "Major", "reporter": "Chris Burroughs", "assignee": null, "labels": ["build"], "created": "2011-09-27T13:02:19.000+0000", "updated": "2013-01-23T20:13:14.000+0000", "description": "Since scala does not maintain binary compatibly between versions, organizations tend to have to move all of there code at the same time.  It would thus be very helpful if we could cross build multiple scala versions.\n\nhttp://code.google.com/p/simple-build-tool/wiki/CrossBuild\n\nUnclear if this would require KAFKA-134 or just work.", "comments": ["Assigning to Josh Hartman", "This is actually a very important issue that needs to be thought about and is closely related to mavenizing the project. First some background info:\n\nScala is not backwards compatible, except for bug fix releases like scala 2.8.0 -> scala 2.8.1. Furthermore, the library jar needs to match the jar it was compiled against. Fortunately, source code has been forwards compatible since scala 2.8, which means that if you build against 2.8, you should be able to build against 2.9.\n\nThe standard way to handle this issue in the scala world is to publish separate artifacts for their jars. So for instance, we could publish kafka-core_2.8.0, kafka-core_2.8.1, kafka-core_2.9.0, and kafka-core_2.9.1. You can configure sbt to do this with one line of code. It's a bit gross, but it's the only way I know of to solve this problem. Without doing this, we'll force users of the library to depend on a specific version of scala. Please read https://github.com/harrah/xsbt/wiki/Cross-Build to learn a bit more.\n\nI'd really appreciate input from the developer team about this. Kafka is on a really old version of scala and it will probably be an issue for non-linkedin users very soon. (Plus if I have artifacts built against 2.9.1 for all LinkedIn scala jars, I can upgrade LinkedIn to 2.9.1 in about a day).", "Agreed that the need for project_scaleVersion suffixes for everything is obnoxious, but is the least bad option and would be appreciated by everyone. Internally at Clearspring we wrapped kafka with a pom with a dep on 2.8.1, since that was the version we were using.  But several projects will soon want to use 2.9.x *and* kafka, and it would be nice to have the right fix upstream.\n\nSo as far as input goes, +1!", "+1 from me as well. In the interim I've created some custom builds, but that's not ideal.", "It's been a while. When can we expect a Scala 2.9.x supported Kafka version?\n\nSum of us need this urgently.", "Sure. Would you be up for describing the set of changes required to make this possible ? We are open to accepting patches. ", "I pulled the latest from trunk and even before attempting to build against 2.9, there is one test failing.  Is this expected?\n\n[info] \n[info] == core-kafka / test-finish ==\n[error] Failed: : Total 137, Failed 1, Errors 0, Passed 136, Skipped 0\n[info] == core-kafka / test-finish ==\n[info] \n[info] == core-kafka / Test cleanup 1 ==\n[info] Deleting directory /var/folders/s4/5vy31m2n0ng7h5qbn4575myr00021m/T/sbt_bd845ec9\n[info] == core-kafka / Test cleanup 1 ==\n[info] \n[info] == core-kafka / test-cleanup ==\n[info] == core-kafka / test-cleanup ==\n[error] Error running kafka.zk.ZKLoadBalanceTest: Test FAILED\n[error] Error running test: One or more subtasks failed\n[info] \n\nAlso, Kafka is using a really old version of sbt (0.7.x), but upgrading to xsbt is really involved.", "This is a preliminary patch to upgrade the build to SBT 0.11.3 and cross-compile for 2.8.0, 2.9.1 and 2.9.2. Minor tweak required to AsyncProducerTest.scala to ignore order of Map pair traversal, but otherwise it's a generally straightforward translation of the existing build. Haven't had time to fix/update the release-zip task, but hopefully I can get that done in the next few weeks.", "Thanks for the patch. Can't apply the patch to 0.8 cleanly. Could you rebase?", "This patch is against trunk, not 0.8. Let me work out what changes need to be made", "I have the build seemingly working, but in my branch (and 0.8 HEAD) some of the tests just hang forever. Is 0.8 HEAD supposed to be stable?", "0.8 is under development but the unit tests should not hang, and there are intermittent test failures that need to be fixed. Which tests do you see hanging? Can you add a comment in KAFKA-384 with your list?\n", "OK, the Map traversal ordering issue I hit before is causing the same issue on the 0.8 branch, but whereas before I could ignore it with a special EasyMock matcher (because it was against an array), now the order of results from DefaultEventHandler.partitionAndCollate is reflected by the order of calls against the EasyMock, something that I cannot work around as far as I can tell. A simple fix would be to have partitionAndCollate use a SortedMap (I've tested this and it works). I can include this change as part of the patch if that's OK.", "Derek,\n\nFor the 0.8 branch, is AsyncProducerTest.testPartitionAndCollateEvents the only test that fails? Will it help if we create expectedResult using HashMap instead of Map (which is what partitionAndCollate does)?", "Yes, that's the only test that fails. I don't think that using HashMap is going to help, either, because IIRC between Scala 2.8.0 and 2.9.x the implementations for Map and HashMap both changed such that ordering of pair traversal differs, which is the underlying problem. Because testPartitionAndCollateEvents doesn't look at the order but rather a higher-level usage of that ordering (e.g. calls to getAnyProducer, send, etc), I think it may be difficult to thread that logic through to allow both the 2.8.0 and 2.9.x behaviors. Having said that, I'm only marginally familiar with the internals, so there may be some approach other than using a SortedMap.", "Also, I tried the patch for 0.7. What's the right way to build now? If I do \"sbt update\", it prompts me for Name:.", "Ok, maybe you can submit a patch with the sortedMap first and we can see if there are other ways of addressing the unit test issue.", "Here's the patch against 0.8 with the included change to use SortedMap. This will require a new version of SBT. Here we use PaulP's excellent sbt-extras script (https://github.com/paulp/sbt-extras) instead of bundling the sbt-launcher.jar in the project, but you could also just update that JAR. To build, simple run \"sbt +clean +package\"", "Sorry, submit patch didn't do what I thought it would. Here's the patch against 0.8", "I applied the patch and did ./sbt +clean +package. It prompted me for Name:. What should I put in and how do I find out what names are valid?", "I'm not even sure where \"Name:\" is coming from. I definitely don't get that locally building with sbt 0.11.3. Are you sure you're not using an older version of SBT?", "Derek, thanks for the patch. Some comments:\n\n1. It would be good if we can use the new sbt out of a refresh kafka checkout without requiring any external download. Can we update both the sbt script and the sbt-launch jar to include the new sbt?\n2. ./sbt package seems to default to scala 2.9.1. Could we default it to 2.8? Also, the -28 option uses scala 2.8.2. Is it possible to support 2.8.0?\n3. ./sbt idea stopped working.", "1. You should be able to simply update the sbt-launch.jar under lib/ to get SBT working on a fresh kafka checkout without having to use PaulP's script.\n2. I've fixed it so the build defaults to 2.8.0. The -28 option isn't what you want. I believe that runs SBT under 2.8.2, not the compilation, etc\n3. The idea plugin changed a little. The new syntax is \"./sbt gen-idea\"\n\nI'm attaching a revised patch with some minor tweaks to fix a bug in the cross build and to make 2.8.0 the default. Please let me know if you have further concerns/questions", "Revised patch against 0.8", "Derek, thanks for the new patch. A few more comments:\n\n10. ./sbt gen-idea doesn't seem to work for me. Got the following:\n[error] Not a valid command: gen-idea\n[error] Not a valid project ID: gen-idea\n[error] Not a valid configuration: gen-idea\n[error] Not a valid key: gen-idea\n[error] gen-idea\n\n11. What's the syntax for building other versions of scala, say 2.9? Could you update the readme file?", "10. sbt gen-idea works fine for me when I update lib/ with the sbt-launch.jar from http://typesafe.artifactoryonline.com/typesafe/ivy-releases/org.scala-sbt/sbt-launch/0.11.3-2/sbt-launch.jar. Can you try again and email me the full log?\n\n11. To select a specific version of scala, use the \"++<version>\" syntax. For example:\n\n./sbt \"++ 2.8.0 package\"\n\nI'm attaching a 3rd revision of the patch with an updated README.md.\n\n", "Revised patch (updated README)", "I did a new checkout of 0.8 branch, applied patch v3 and copied the sbt-launch.jar to lib. I still got the following error:\n\n[jrao@jrao-ld kafka_0.8_temp]$ ./sbt gen-idea\n[warn] Using project/plugins/ is deprecated for plugin configuration (/home/jrao/Intellij_workspace/kafka_0.8_temp/project/plugins).\n[warn] Put .sbt plugin definitions directly in project/,\n[warn]   .scala plugin definitions in project/project/,\n[warn]   and remove the project/plugins/ directory.\n[info] Loading project definition from /home/jrao/Intellij_workspace/kafka_0.8_temp/project/plugins\n[info] Set current project to Kafka (in build file:/home/jrao/Intellij_workspace/kafka_0.8_temp/)\n[error] Not a valid command: gen-idea\n[error] Not a valid project ID: gen-idea\n[error] Not a valid configuration: gen-idea\n[error] Not a valid key: gen-idea\n[error] gen-idea\n[error]         ^\n", "Also, does DefaultEventHandler.partitionAndCollate fail at the following test?\n    assertEquals(expectedResult, actualResult)\nIf so, we can probably write a customized equal test that sorts the sequence of ProducerData by value first and then compare. This way, we may not need to change HashMap to SortedMap in DefaultEventHandler.partitionAndCollate().", "No, unfortunately. It used to do that (on trunk), but in 0.8 the failure is within AsyncProducerTest.testFailedSendRetryLogic due to a change in ordering of underlying send calls:\n\n[2012-07-09 17:53:25,663] ERROR \n  Unexpected method call getAnyProducer():\n    getAnyProducer(): expected: 3, actual: 4\n    close(): expected: 1, actual: 0 (kafka.utils.Utils$:102)\njava.lang.AssertionError: \n  Unexpected method call getAnyProducer():\n    getAnyProducer(): expected: 3, actual: 4\n    close(): expected: 1, actual: 0\n\tat org.easymock.internal.MockInvocationHandler.invoke(MockInvocationHandler.java:45)\n\tat org.easymock.internal.ObjectMethodsFilter.invoke(ObjectMethodsFilter.java:73)\n\tat org.easymock.internal.ClassProxyFactory$MockMethodInterceptor.intercept(ClassProxyFactory.java:92)\n\tat kafka.producer.ProducerPool$$EnhancerByCGLIB$$5e7236c2.getAnyProducer(<generated>)\n\tat kafka.producer.BrokerPartitionInfo.updateInfo(BrokerPartitionInfo.scala:73)\n\tat kafka.producer.async.DefaultEventHandler$$anonfun$handle$1.apply$mcV$sp(DefaultEventHandler.scala:53)\n\tat kafka.utils.Utils$.swallow(Utils.scala:401)\n\tat kafka.utils.Logging$class.swallowError(Logging.scala:102)\n\tat kafka.utils.Utils$.swallowError(Utils.scala:38)\n\tat kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:53)\n\tat kafka.producer.AsyncProducerTest.testFailedSendRetryLogic(AsyncProducerTest.scala:475)\n\nBased on a brief review of the code involved I don't see a simple way around this for the 0.8 branch, but I'm open to suggestions.", "Strange, gen-idea works for me. I get some warnings about NOT FOUND jars, but otherwise it creates the project.iml file. The SHA hash on my sbt-launch.jar file is:\n\ncfdb2c9d63776a5faaf681b06b9cf5c077375d43  lib/sbt-launch.jar\n\nAnd I haven't modified the \"sbt\" script in the project root at all.", "Any update on status? I can't reproduce the gen-idea issue.", "Actually, I think I may know what's going on. The patch may not be deleting some of the dirs under project/. In particular, there should no longer be a project/plugins/ dir, but from your log above it appears that you still have it there:\n\n[jrao@jrao-ld kafka_0.8_temp]$ ./sbt gen-idea\n[warn] Using project/plugins/ is deprecated for plugin configuration (/home/jrao/Intellij_workspace/kafka_0.8_temp/project/plugins).\n[warn] Put .sbt plugin definitions directly in project/,\n[warn] .scala plugin definitions in project/project/,\n[warn] and remove the project/plugins/ directory. \n...\n\nCan you make sure that project/plugins/ is gone and try again?", "I checked out 0.8 code, applied v3 patch, downloaded the sbt jar (link above) in the lib directory and ran ./sbt gen-idea. It fails and here is the complete trace - http://pastebin.com/suUZ9wnw", "Can you remove your project/plugins/ dir and see if that does it? For some reason the patch doesn't seem to delete that directory", "After deleting the project/plugins/ directory, ./sbt gen-idea works, so does ./sbt package. Please can you include this change and upload a patch ?", "As far as I know patch can't remove directories, but if you know otherwise please point me to docs", "Ok. A couple of comments -\n\n1. What is the difference between ./sbt +package and ./sbt package ?\n2. Please remove Release.scala since the release-zip target does not work and is not useful until fixed\n3. Why do some unit tests have their parent test name printed in the test output, while some don't. See below - \n\n[info] Test Starting: testReachableServer(kafka.producer.SyncProducerTest)\n[info] Test Passed: testReachableServer(kafka.producer.SyncProducerTest)\n[info] Test Starting: testSingleMessageSizeTooLarge(kafka.producer.SyncProducerTest)\n[info] Test Passed: testSingleMessageSizeTooLarge(kafka.producer.SyncProducerTest)\n[info] Test Starting: testCompressedMessageSizeTooLarge(kafka.producer.SyncProducerTest)\n[info] Test Passed: testCompressedMessageSizeTooLarge(kafka.producer.SyncProducerTest)\n[info] Test Starting: testProduceCorrectlyReceivesResponse(kafka.producer.SyncProducerTest)\n[info] Test Passed: testProduceCorrectlyReceivesResponse(kafka.producer.SyncProducerTest)\n[info] Test Starting: testProducerCanTimeout(kafka.producer.SyncProducerTest)\n[info] Test Passed: testProducerCanTimeout(kafka.producer.SyncProducerTest)\n[info] Test Starting: testFieldValues\n[info] Test Passed: testFieldValues\n[info] Test Starting: testChecksum\n[info] Test Passed: testChecksum\n[info] Test Starting: testEquality\n[info] Test Passed: testEquality\n[info] Test Starting: testIsHashable\n[info] Test Passed: testIsHashable\n\n4. Why does test-only testName try to run test-only in other projects. Earlier, after you selected a project, you could run test-only and it just executed that one test, without causing delay -\n\n> projects\n[info] In file:/home/nnarkhed/Projects/kafka-139/\n[info]   * Kafka\n[info]     contrib\n[info]     core\n[info]     hadoop consumer\n[info]     hadoop producer\n[info]     java-examples\n[info]     perf\n> test-only kafka.integration.LazyInitProducerTest\n[info] No tests to run for contrib/test:test-only\n[info] No tests to run for Kafka/test:test-only\n[info] No tests to run for java-examples/test:test-only\n[info] No tests to run for hadoop producer/test:test-only\n[info] No tests to run for hadoop consumer/test:test-only\n[info] Test Starting: testProduceAndFetch(kafka.integration.LazyInitProducerTest)\n[info] Test Passed: testProduceAndFetch(kafka.integration.LazyInitProducerTest)\n[info] Test Starting: testProduceAndMultiFetch(kafka.integration.LazyInitProducerTest)\n[info] Test Passed: testProduceAndMultiFetch(kafka.integration.LazyInitProducerTest)\n[info] Test Starting: testMultiProduce(kafka.integration.LazyInitProducerTest)\n[info] Test Passed: testMultiProduce(kafka.integration.LazyInitProducerTest)\n[info] Test Starting: testMultiProduceResend(kafka.integration.LazyInitProducerTest)\n[info] Test Passed: testMultiProduceResend(kafka.integration.LazyInitProducerTest)\n[info] Passed: : Total 4, Failed 0, Errors 0, Passed 4, Skipped 0\n[success] Total time: 11 s, completed Jul 20, 2012 5:51:06 PM\n", "1. +package will compile, test, and package for all cross-build Scala versions (2.8.0, 2.9.1), while package simply runs it for the default Scala version (2.8.0). See https://github.com/harrah/xsbt/wiki/Cross-Build for more details.\n\n2. I'll upload a new patch with Release.scala removed\n\n3. I'm not sure. As part of the patch I had to upgrade scalatest, so this may be some change in behavior there\n\n4. With a multi-module project, SBT 0.10+ will run a given command for all subprojects. If you want to run a command for a particular sub-project you can prefix it with \"projectname/\". For example, to run the test above, just do:\n\ncore/test-only kafka.integration.LazyInitProducerTest ", "Patch with \"Release\" task removed", "Just checking to see what I can do to move this ticket along, thanks!", "Sorry for reviewing this late and thanks for the patch. I applied it and try ./sbt idea, but it fails with the following error message -\n\nnnarkhed-ld:kafka-139 nnarkhed$ ./sbt idea\nGetting org.scala-tools.sbt sbt_2.7.7 0.11.3 ...\n\n:: problems summary ::\n:::: WARNINGS\n\t\tmodule not found: org.scala-tools.sbt#sbt_2.7.7;0.11.3\n\n\t==== local: tried\n\n\t  /home/nnarkhed/.ivy2/local/org.scala-tools.sbt/sbt_2.7.7/0.11.3/ivys/ivy.xml\n\n\t  -- artifact org.scala-tools.sbt#sbt_2.7.7;0.11.3!sbt_2.7.7.jar:\n\n\t  /home/nnarkhed/.ivy2/local/org.scala-tools.sbt/sbt_2.7.7/0.11.3/jars/sbt_2.7.7.jar\n\n\t==== Maven2 Local: tried\n\n\t  file:///home/nnarkhed/.m2/repository/org/scala-tools/sbt/sbt_2.7.7/0.11.3/sbt_2.7.7-0.11.3.pom\n\n\t  -- artifact org.scala-tools.sbt#sbt_2.7.7;0.11.3!sbt_2.7.7.jar:\n\n\t  file:///home/nnarkhed/.m2/repository/org/scala-tools/sbt/sbt_2.7.7/0.11.3/sbt_2.7.7-0.11.3.jar\n\n\t==== sbt-db: tried\n\n\t  http://databinder.net/repo/org.scala-tools.sbt/sbt_2.7.7/0.11.3/ivys/ivy.xml\n\n\t  -- artifact org.scala-tools.sbt#sbt_2.7.7;0.11.3!sbt_2.7.7.jar:\n\n\t  http://databinder.net/repo/org.scala-tools.sbt/sbt_2.7.7/0.11.3/jars/sbt_2.7.7.jar\n\n\t==== Maven Central: tried\n\n\t  http://repo1.maven.org/maven2/org/scala-tools/sbt/sbt_2.7.7/0.11.3/sbt_2.7.7-0.11.3.pom\n\n\t  -- artifact org.scala-tools.sbt#sbt_2.7.7;0.11.3!sbt_2.7.7.jar:\n\n\t  http://repo1.maven.org/maven2/org/scala-tools/sbt/sbt_2.7.7/0.11.3/sbt_2.7.7-0.11.3.jar\n\n\t==== Scala-Tools Maven2 Repository: tried\n\n\t  http://scala-tools.org/repo-releases/org/scala-tools/sbt/sbt_2.7.7/0.11.3/sbt_2.7.7-0.11.3.pom\n\n\t  -- artifact org.scala-tools.sbt#sbt_2.7.7;0.11.3!sbt_2.7.7.jar:\n\n\t  http://scala-tools.org/repo-releases/org/scala-tools/sbt/sbt_2.7.7/0.11.3/sbt_2.7.7-0.11.3.jar\n\n\t==== Scala-Tools Maven2 Snapshots Repository: tried\n\n\t  http://scala-tools.org/repo-snapshots/org/scala-tools/sbt/sbt_2.7.7/0.11.3/sbt_2.7.7-0.11.3.pom\n\n\t  -- artifact org.scala-tools.sbt#sbt_2.7.7;0.11.3!sbt_2.7.7.jar:\n\n\t  http://scala-tools.org/repo-snapshots/org/scala-tools/sbt/sbt_2.7.7/0.11.3/sbt_2.7.7-0.11.3.jar\n\n\t\t::::::::::::::::::::::::::::::::::::::::::::::\n\n\t\t::          UNRESOLVED DEPENDENCIES         ::\n\n\t\t::::::::::::::::::::::::::::::::::::::::::::::\n\n\t\t:: org.scala-tools.sbt#sbt_2.7.7;0.11.3: not found\n\n\t\t::::::::::::::::::::::::::::::::::::::::::::::\n\n\n\n:: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\nunresolved dependency: org.scala-tools.sbt#sbt_2.7.7;0.11.3: not found\nError during sbt execution: Error retrieving required libraries\n  (see /home/nnarkhed/Projects/kafka-139/project/boot/update.log for complete log)\nError: Could not retrieve sbt 0.11.3\n", "I replied via email but it doesn't look like it went through.\n\nThis has been somewhat of a complicated patch to submit :). Besides the obvious application of the patch file itself, you'll need to downoad the latest SBT jar and replace the existing one in lib/, and you'll also need to remove the project/plugins/ dir. \"Patch\", per se, doesn't support either of these operations, so I apologize for any confusion. The error you show above appears to be an issue with the SBT jar not being updated.\n\nI saw rumblings on the list about moving to Git from SVN. If that happens I can host my own repo, make the changes there, and then just submit a pull request if that would be simpler. Alternatively, I could submit a script that applies the patch, downloads the latest SBT, and removes the extraneous directories. Which would be preferable from your standpoint?", "Derek,\n\nI understand that this patch is pretty complicated to submit. I'd like to help out to expedite this. I don't mind downloading the sbt JAR and removing the project/plugins directory, just missed it last time I reviewed the patch. Even after doing that, I see the following problems, some I resolved already, some I'm not so sure about -\n\n1. the perf sub-project is missing a build.sbt. So I added one\n2. The scripts are pointing to */target/scala_2.8.0 while the new path is */target/scala-2.8.0. I changed all scripts to incorporate this new path\n\nI need some help with the following -\n\n3. ./sbt package doesn't always re-create the jars for all sub-projects. \n4. I couldn't find the path where all the downloaded dependency jars are stored. Our scripts need to have that in the classpath in order to run properly. \n\nFor this patch to be checked in, we need to ensure that unit tests as well as system tests pass. Right now, unit test pass, but system tests fail due to (4). \n\nI think we are pretty close to accepting this patch. Appreciate your help in getting this completed!\n", "Thanks for looking into it! For what it's worth, my comment on this being a complicated patch was meant to convey my understanding that I'm probably doing some things here well outside the norm for submitted patches, not a complaint about the process :)\n\nFor #3 do you mean that changes in code don't result in a newly built JAR for certain subprojects? If that's the case I'm probably missing a dependency setting on those subprojects. For #4, this is a change in the behavior of SBT for 0.10+. Now, SBT uses Ivy, so none of the downloaded deps will be in the project dir. Rather, they will be in the user's Ivy home (~/.ivy2/cache by default). In the case of packaging, this is something that may (and I stress *may*) be simpler within SBT, as in the Release.scala I had originally, or by using the sbt-assembly plugin (https://github.com/sbt/sbt-assembly/) to bundle all JARs together into one. Please let me know if I can help with that. If you'd like me to take a look at it, would it be possible to check in what you *do* have working on a branch so that I can simply point git-svn at it and work on your exact codebase?\n\nThanks,\n\nDerek", "3. In the earlier SBT version, if you run ./sbt package over and over, it re-created the jars for all subprojects each time. Now, I'm not so sure it does that ?\n4. Like I said, all our scripts (*.sh) need to add the dependencies to the Java CLASSPATH. Right now, they point to the lib_managed directory and hence the system tests fail. \n5. You might want to run system tests before submitting the patch -\n./system_test/*/bin/run-test.sh\n6. Why does the patch change DefaultEventHandler to use a SortedMap ?\n\nWould you be up for incorporating the changes suggested in review comments 1-6 above and submitting a patch ? Rest of the patch looks good.\n", "3. SBT now does dependency management all the way to the produced artifacts. If nothing has changed, it won't rebuild the JAR. There may be a setting to override this behavior, but I would have to look into it if that's a requirement.\n\n4. Is the intention that the system tests are run outside of SBT, or would it be OK to execute them as part of the test phase on the master project? If the latter is OK, I think this is done fairly easily. If we need it outside there may be a way to run SBT to interrogate the classpath for a given project. I'll look at the scripts.\n\n5. Let me know if running #4 is OK within SBT\n\n6. This was discussed earlier in the ticket. Due to a change in Scala's Map iteration ordering between 2.8.0 and 2.9.x, EasyMock fails in Scala > 2.8.x. Changing to a SortedMap preserves the ordering, allowing the test to pass. The new tests use an indirect measure via EasyMock of call order, and as far as I can tell it would be difficult to work around it because the change required is several layers below where EasyMock interfaces.", "Sorry, to answer your last question I'd be happy to work out the remaining issues to get this in place.", "3. Sounds good, let's not change anything here. It is fine to not re-create the jars, if nothing has changed.\n4. Yes, the system tests are just some shell scripts that run out of SBT. It will be good to change them to include the dependent jars on the classpath. \n5. No, it's not. The way to run system tests is as shown above.\n6. Thanks for the explanation, makes sense. ", "Updated the patch for current 0.8. Just added the yammer metrics dependency and fixed merge conflicts. That said, I am not familiar with Scala or SBT so you may also just want to disregard the patch update...", "Noticed I had screwed up the build.properties part of the patch. Again, feel free to disregard...", "Since we're on Git now, I've pushed a branch with the changes here:\n\nhttps://github.com/dchenbecker/kafka-sbt\n\nI haven't had a chance to look at Sam's patches yet, but I'll get those applied as well. The branch is taken from the latest 0.8 head, using SBT 0.12.1.", "FYI\n\nI have been reviewing\n\ngit remote add dchenbecker git://github.com/dchenbecker/kafka-sbt.git\ngit fetch dchenbecker\ngit merge dchenbecker/topic/0.8-sbt-0.12\n\n********\n\nso far the only changes I have made where minor in in Build.scala to reflect recent updates for mavenification\n\n-    version := \"0.8.0\",\n-    organization := \"org.apache.kafka\",\n+    version := \"0.8-SNAPSHOT\",\n+    organization := \"org.apache\",\n\nstil working on reviewing a few more areas of change but this is looking really good.  so far I am a +1 on this\n\nunless I find anything or anyone else does that might be hacking on this ticket too I will commit the changes tomorrow\n", "Joe, that's great! Let me know if you run into any issues beyond the minor ones you found. I was also thinking that perhaps the cross build should include 2.8.2 (latest in that line) and 2.10.0, since it just came out. I'm running a test build tonight across those versions and if all looks well I'll push the update to my branch.", "OK, 2.10.0 is going to be a non-starter at this point due to a number of removed methods in the Scala collections libs (they've been deprecated since 2.8.1/2.9.0), but 2.8.2 seems fine.", "added 2.8.2 and committed changes to 0.8 branch", "Joe, after your check in. I saw a couple of changes: (1) ./sbt package received a bunch of warnings during compilation; (2) bin/kafka-server-start.sh config/server.properties stopped working with the following error\n\nbin/kafka-server-start.sh config/server.properties \nException in thread \"main\" java.lang.NoClassDefFoundError: kafka/Kafka\nCaused by: java.lang.ClassNotFoundException: kafka.Kafka\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:202)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:190)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:306)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:247)\nCould not find the main class: kafka.Kafka.  Program will exit.\n\nIt seems that kafka/kafka.class is no longer in the Kafka jar.", "there are 2 issues I am fixing now in kafka-run-class.sh to get this working again on branch 0.8\n\n1 is minor the changes one dir is - instead of _ now\n\nthe other issue is it looks like since SBT 0.10.0 all of the project/boot jars are now in ~/ivy2 no longer localized\n\nI am changing the bash script now to look in the ~/iv2 directory we could (could) consider using assembly for stand alone jar\n\nfirst step is getting it working again, we have to decide if the fix I am making right now to get it running pointing to ~/iv2 is what we want or if we want to start to use assembly for standalone jar and not looping through the directories in ", "i committed the changes so that the start script looks in the right ~/.ivy2 directories and related changes so that jars are matching up\n\ntested quick start script and looks good now on 0.8 branch", "I checked out 0.8 this morning and this breaks the Mavenization from KAFKA-133\nThe dependencies for com.yammer.metrics is nested in the resulting pom.xml, which is not valid.", "Also, when running \"bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test\", I saw the following:\nSLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\nSLF4J: Defaulting to no-operation (NOP) logger implementation\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n\nThis seems to be new and I am not sure how critical it is. ", "It also looks like package no longer builds a test jar(KAFKA-676)? How do we run the various non-unit tests? Previous you did\n$./sbt\n> package\nand then from the command line you could do \n$ ./bin/kafka-run-class.sh kafka.TestLinearWriteSpeed\n\nBut now I get class not found.", "Also is it right that kafka-run-class is hardcoding ~/.ivy2/cache as the lib directory? Not sure that that came with this patch, but can't that directory be anywhere?", "ok so first let me review KAFKA-728 and commit it because that was caused by this also.\n\nI will open a sub ticket to make ~/ivy2/cache a variable in the bash script and upload a patch after that commit\n\nlet me look into what is causing the KAFKA-133 to break and create a sub ticket, I will take a look to see what might be causing it and upload a patch or hola for assistance \n\nlet me look into what is causing the KAFKA-676 to break and create a sub ticket, I will take a look to see what might be causing it and upload a patch or hola for assistance \n\nlet me reproduce the SLF4J error because I could have sworn I ran into that and resolved it but maybe that error is different than the one I already fixed (likely) and sub ticket for that with a patch there too it should be trivial\n\ni am on it\n\n"], "derived": {"summary": "Since scala does not maintain binary compatibly between versions, organizations tend to have to move all of there code at the same time. It would thus be very helpful if we could cross build multiple scala versions.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "cross-compile multiple Scala versions and upgrade to SBT 0.12.1 - Since scala does not maintain binary compatibly between versions, organizations tend to have to move all of there code at the same time. It would thus be very helpful if we could cross build multiple scala versions."}, {"q": "What updates or decisions were made in the discussion?", "a": "ok so first let me review KAFKA-728 and commit it because that was caused by this also.\n\nI will open a sub ticket to make ~/ivy2/cache a variable in the bash script and upload a patch after that commit\n\nlet me look into what is causing the KAFKA-133 to break and create a sub ticket, I will take a look to see what might be causing it and upload a patch or hola for assistance \n\nlet me look into what is causing the KAFKA-676 to break and create a sub ticket, I will take a look to see what might be causing it and upload a patch or hola for assistance \n\nlet me reproduce the SLF4J error because I could have sworn I ran into that and resolved it but maybe that error is different than the one I already fixed (likely) and sub ticket for that with a patch there too it should be trivial\n\ni am on it"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-140", "title": "Expose total metrics through MBeans as well", "status": "Resolved", "priority": "Major", "reporter": "Pierre-Yves Ritschard", "assignee": null, "labels": [], "created": "2011-09-29T07:02:51.000+0000", "updated": "2011-09-30T19:37:20.000+0000", "description": "Mathias Herberts suggested on the ML that Kafka get the following additional metrics exposed through a bean\n\ntotalflushms\ntotalbytesread\ntotalbyteswritten\ntotalfetchrequestms\ntotalproducerequestms", "comments": ["The attached files fixes the issue", "fix for kafka-140", "Pierre-Yves, thanks for creating the jira and submitting the patch.\n\nI wonder if SnapshotStats.getTotalMetric really gives what you want. This method currently gives you the aggregated metric for the latest 30-second window. My understanding is that you want to have an aggregated metric since the server is started. Is that right? If so, what you need to do is to create a new Stats in SnapshotStats that keeps tracks of the aggregated metric for the whole history.", "I indeed got that wrong, will update the patch.", "Updated patch", "Thanks Pierre-Yves, I just committed this patch."], "derived": {"summary": "Mathias Herberts suggested on the ML that Kafka get the following additional metrics exposed through a bean\n\ntotalflushms\ntotalbytesread\ntotalbyteswritten\ntotalfetchrequestms\ntotalproducerequestms.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Expose total metrics through MBeans as well - Mathias Herberts suggested on the ML that Kafka get the following additional metrics exposed through a bean\n\ntotalflushms\ntotalbytesread\ntotalbyteswritten\ntotalfetchrequestms\ntotalproducerequestms."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks Pierre-Yves, I just committed this patch."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-141", "title": "Check and make sure that the files that have been donated have been updated to reflect the new ASF copyright", "status": "Resolved", "priority": "Blocker", "reporter": "Alan Cabrera", "assignee": "Neha Narkhede", "labels": [], "created": "2011-10-02T18:32:22.000+0000", "updated": "2011-10-12T09:36:44.000+0000", "description": "Check and make sure that the files that have been donated have been updated to reflect the new ASF copyright", "comments": ["For ASF copyright, I guess we only care about java/scala source files, not other configuration, script files. Is that correct?", "I think have changed the old style of ASF license but have not added one to the files that didnt have one before.", "You'd be surprised at what has to have the ASF header.  We need to run rat.", "Could not get the excludes to work, but a sample RAT run is attached.", "Ran RAT on a fresh checkout of Kafka. It complains the following didn't have the Apache header -\n\n1. most clients\n2. all files in config and bin\n3. Some tests. Attaching a patch for that\n4. perf directory. https://issues.apache.org/jira/browse/KAFKA-149 is filed to delete this.\n\nWe still haven't heard back about whether 1 & 3 should have the header or not ?", "+1 on the patch that fixes the tests.", "Attaching an updated patch, where I've added the ASF header to all files, except a few ones in the clients directory. \n\nAlso, attaching an updated rat output.\n\nI hope owners of the clients can step up and clean their files to replace their custom headers with ASF headers", "That's a big patch :)\n\nThe hashbangs do need to be on the first line of the file, so we'll need to move the header after #! if present.\n\nThere is something weird with the cs files: the original files have a BOM (http://en.wikipedia.org/wiki/Byte_order_mark) in them, which is visible with a hexdump of the original file. If you apply this patch, and open the patched cs file with vim/more/less there is a visible <FEFF>, which is another BOM. Since it is not at the beginning of the file it is visible and may cause compilation issues.\n", "Good catch, Joel. I corrected the .sh files and reverted the csharp files. I think those client owners can help cleaning up their code, since they best understand it.", "Added patch to fix the Apache license for the C++ client. Alternatively to the patch, you can apply the following steps (comments have *** before them):\n\n\n*** When in the kafka root\n*** Remove autogen files\n\nrm -rfv clients/cpp/{build-aux,aclocal.m4,Makefile.in,configure}\n\n\n*** Apply patch for readme\n\ndiff --git a/clients/cpp/README.md b/clients/cpp/README.md\nindex e0bcd50..33f69f1 100644\n--- a/clients/cpp/README.md\n+++ b/clients/cpp/README.md\n@@ -10,6 +10,7 @@ http://gcc.gnu.org/\n http://www.boost.org/\n \n ```bash\n+./autoconf.sh\n ./configure\n ```\n\n\n*** And then make a file called clients/cpp/autoconf.sh with the contents\n\n#!/bin/sh\n\n# We need libtool for ./configure && make && make install stage\ncommand -v libtool\nif  [ $? -ne 0 ]; then\n    echo \"autoconf.sh: error: unable to locate libtool\"\n    exit 1\nfi\n\n# We need autoreconf to build the ./configure script\ncommand -v autoreconf\nif  [ $? -ne 0 ]; then\n    echo \"autoconf.sh: error: unable to locate autoreconf\"\n    exit 1\nfi\n\nmkdir -p ./build-aux/m4\nautoreconf --verbose --force --install\n\n\n*** Finally make the file executable\n\nchmod +x clients/cpp/autoconf.sh", "-1 \n\nThe license header is missing in *atleast* the following files -\n\nclients/cpp/src/tests/encoder_helper_tests.cpp\nclients/cpp/src/tests/encoder_tests.cpp\nclients/cpp/src/tests/producer_tests.cpp\nclients/cpp/src/encoder_helper.hpp\nclients/cpp/src/producer.cpp\nclients/cpp/src/encoder.hpp\nclients/cpp/src/producer.hpp\nclients/cpp/src/example.cpp\n\nPlease make sure all the files in the cpp directory have the Apache license header.", "Taking Lorenzo's patch and fixing the remaining files to add the Apache license header to it.", "+1 on KAFKA-141-cpp.patch", "Thanks. Just committed this and resolved the ticket", "Patch to add license to all files", "oh, I didn't notice you already fixed the files. Ignore the latest patch."], "derived": {"summary": "Check and make sure that the files that have been donated have been updated to reflect the new ASF copyright.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Check and make sure that the files that have been donated have been updated to reflect the new ASF copyright - Check and make sure that the files that have been donated have been updated to reflect the new ASF copyright."}, {"q": "What updates or decisions were made in the discussion?", "a": "oh, I didn't notice you already fixed the files. Ignore the latest patch."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-142", "title": "Check and make sure that for all code included with the distribution that is not under the Apache license, we have the right to combine with Apache-licensed code and redistribute", "status": "Closed", "priority": "Blocker", "reporter": "Alan Cabrera", "assignee": null, "labels": [], "created": "2011-10-02T18:32:59.000+0000", "updated": "2011-10-10T13:41:41.000+0000", "description": "Check and make sure that for all code included with the distribution that is not under the Apache license, we have the right to combine with Apache-licensed code and redistribute", "comments": ["Alan,\n\nIs this ticket for all the external jars that we used in Kafka?", "Anything that is not under AL 2.0.", "If this is for external jars, I think we are good to go. All external jar dependencies of Kafka can be distributed under Apache license.", "Since all external jars have AL 2.0 License, I am closing this ticket.", "Thanks for looking into this.", "jopt-simple is MIT, but that's still Category A.\n\nAlan, don't we have to clear everything under contrib/ and clients/ as well?", "Chris, for contrib and clients, I added a patch to KAFKA-143. ", "Sorry, I meant the *dependencies* of clients/ and contrib/"], "derived": {"summary": "Check and make sure that for all code included with the distribution that is not under the Apache license, we have the right to combine with Apache-licensed code and redistribute.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Check and make sure that for all code included with the distribution that is not under the Apache license, we have the right to combine with Apache-licensed code and redistribute - Check and make sure that for all code included with the distribution that is not under the Apache license, we have the right to combine with Apache-licensed code and redistribute."}, {"q": "What updates or decisions were made in the discussion?", "a": "Sorry, I meant the *dependencies* of clients/ and contrib/"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-143", "title": "Check and make sure that all source code distributed by the project is covered by one or more approved licenses", "status": "Resolved", "priority": "Blocker", "reporter": "Alan Cabrera", "assignee": "Neha Narkhede", "labels": [], "created": "2011-10-02T18:34:15.000+0000", "updated": "2011-10-14T01:35:08.000+0000", "description": "Check and make sure that all source code distributed by the project is covered by one or more of the following approved licenses: Apache, BSD, Artistic, MIT/X, MIT/W3C, MPL 1.1, or something with essentially the same terms", "comments": ["We also need to run Rat to make sure that all the files have the appropriate headers.  You'd be surprised at how many superfluous files need to be \"stamped\".", "Fixed some contrib and clients to have the Apache License header, as well as the LICENSE file. With this, I think we have made sure all code distributed by Kafka is Apache 2.0 compatible", "+1 on the patch.", " - LICENSE files need to have the \"Copyright [yyyy] [name of copyright owner]\" line filled in.\n - We can't remove the FSF copyright notice that says  we can redistribute \"as long as this notice is preserved.\"  I'm not sure how m4/autoconf type files are supposed to be handled.\n", "I'll fix the LICENSE files. \nFor the FSF copyright, if that is really required, why don't any other files have it ? like other clients, contrib etc ?", "Fixed LICENSE files in contrib and clients.\n\nFor the \"Copyright [yyyy] [name of copyright owner]\" part, it is the same in almost all incubator projects, and also in top level Apache projects like Zookeeper. \n\nSo leaving it as is. ", "Still have lots of Rat failures.", "Uploading the rat output. Only \n\n1. README files and \n2. some make files and \n3. some package.html files don't have the header. \n4. System test .out files \n\nI wonder if the above files need the header ? Please advise!", "Attaching a patch that corrects the remaining files that rat complains about. Now rat output passes with the attached .rat-excludes file. This .rat-excludes file is pending review on KAFKA-151. ", "+1 with the new patch.", "Rat test passes with 0 failures"], "derived": {"summary": "Check and make sure that all source code distributed by the project is covered by one or more of the following approved licenses: Apache, BSD, Artistic, MIT/X, MIT/W3C, MPL 1. 1, or something with essentially the same terms.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Check and make sure that all source code distributed by the project is covered by one or more approved licenses - Check and make sure that all source code distributed by the project is covered by one or more of the following approved licenses: Apache, BSD, Artistic, MIT/X, MIT/W3C, MPL 1. 1, or something with essentially the same terms."}, {"q": "What updates or decisions were made in the discussion?", "a": "Rat test passes with 0 failures"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-144", "title": "kafka-server-start.sh ignores JMX_PORT", "status": "Resolved", "priority": "Trivial", "reporter": "Mathias Herberts", "assignee": null, "labels": [], "created": "2011-10-03T10:08:11.000+0000", "updated": "2011-10-03T15:50:05.000+0000", "description": "The launch script kafka-server-start.sh forces the JMX_PORT environment variable to the value '9999' thus ignoring any previous value.", "comments": ["Changes the forces assignment to JMX_PORT to a conditional one, thus taking into consideration the previous value of JMX_PORT if it is set.", "+1. Thanks Mathias for the patch. I just committed this."], "derived": {"summary": "The launch script kafka-server-start. sh forces the JMX_PORT environment variable to the value '9999' thus ignoring any previous value.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "kafka-server-start.sh ignores JMX_PORT - The launch script kafka-server-start. sh forces the JMX_PORT environment variable to the value '9999' thus ignoring any previous value."}, {"q": "What updates or decisions were made in the discussion?", "a": "+1. Thanks Mathias for the patch. I just committed this."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-145", "title": "Kafka server mirror shutdown bug", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "labels": [], "created": "2011-10-03T23:25:53.000+0000", "updated": "2011-10-05T23:03:16.000+0000", "description": "When a machine that is mirroring data off of another Kafka broker is shutdown, it runs into the following exception, effectively dropping data. The shutdown API needs to be fixed to first shutdown the consumer threads, drain all the data to the producer, and only then shutdown the producer. \n\nFATAL kafka.server.EmbeddedConsumer  - kafka.producer.async.QueueClosedException: Attempt to add event to a closed queue.kafka.producer.async.QueueClosedException: Attempt to add event to a closed queue.\n        at kafka.producer.async.AsyncProducer.send(AsyncProducer.scala:87)\n        at kafka.producer.ProducerPool$$anonfun$send$1$$anonfun$apply$mcVI$sp$1$$anonfun$apply$2.apply(ProducerPool.scala:131)\n        at kafka.producer.ProducerPool$$anonfun$send$1$$anonfun$apply$mcVI$sp$1$$anonfun$apply$2.apply(ProducerPool.scala:131)\n        at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)\n        at scala.collection.immutable.List.foreach(List.scala:45)\n        at kafka.producer.ProducerPool$$anonfun$send$1$$anonfun$apply$mcVI$sp$1.apply(ProducerPool.scala:131)\n        at kafka.producer.ProducerPool$$anonfun$send$1$$anonfun$apply$mcVI$sp$1.apply(ProducerPool.scala:130)\n        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)\n        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)\n        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:130)\n        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:102)\n        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:102)\n        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)\n        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)\n        at kafka.producer.ProducerPool.send(ProducerPool.scala:102)\n        at kafka.producer.Producer.zkSend(Producer.scala:144)\n        at kafka.producer.Producer.send(Producer.scala:106)\n        at kafka.server.EmbeddedConsumer$$anonfun$startNewConsumerThreads$1$$anonfun$apply$1$$anon$1$$anonfun$run$1.apply(KafkaServerStartable.scala:136)\n        at kafka.server.EmbeddedConsumer$$anonfun$startNewConsumerThreads$1$$anonfun$apply$1$$anon$1$$anonfun$run$1.apply(KafkaServerStartable.scala:134)\n        at scala.collection.Iterator$class.foreach(Iterator.scala:631)\n        at kafka.utils.IteratorTemplate.foreach(IteratorTemplate.scala:30)\n", "comments": ["This patch corrects the shutdown behavior of Kafka mirroring, i.e. EmbeddedConsumer. It first shuts down the new topic watcher, then the zookeeper consumer connector. After this, it stops the mirroring threads. At this point, all mirroring threads have finished mirroring all data that they've ever consumed. Only then, it shuts down the producer. \n\nThis ensures that the producer is not shutdown, before the mirroring threads have finished their work, thereby avoiding data loss caused due to QueueClosedException.", "In MirroringThread.run, it's probably better to do the countdown even when we hit an exception.", "Uploading an updated patch, in which the shutdown latch is decremented in a finally block, to make sure the mirroring threads will shutdown even when they run into an error/exception.", "+1 on the new patch.", "Thanks. Just committed the patch."], "derived": {"summary": "When a machine that is mirroring data off of another Kafka broker is shutdown, it runs into the following exception, effectively dropping data. The shutdown API needs to be fixed to first shutdown the consumer threads, drain all the data to the producer, and only then shutdown the producer.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Kafka server mirror shutdown bug - When a machine that is mirroring data off of another Kafka broker is shutdown, it runs into the following exception, effectively dropping data. The shutdown API needs to be fixed to first shutdown the consumer threads, drain all the data to the producer, and only then shutdown the producer."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks. Just committed the patch."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-146", "title": "testUnreachableServer sporadically fails", "status": "Resolved", "priority": "Minor", "reporter": "Chris Burroughs", "assignee": "Neha Narkhede", "labels": [], "created": "2011-10-04T02:09:57.000+0000", "updated": "2011-10-05T20:29:38.000+0000", "description": "(If anyone can tell me how to convince Jira to do verbatim output,  I would be grateful)\n\nThis seems to fail about 50% of the time on builds.apache.org, also reported by Bao Thai Ngo on the -dev list.  I have not had success reproducing it locally on my Ubuntu laptop.\n\n[0m[[0minfo[0m] [34m[0m\n[0m[[0minfo[0m] [34m== core-kafka / kafka.javaapi.producer.SyncProducerTest ==[0m\n[0m[[0minfo[0m] [0mTest Starting: testUnreachableServer[0m\nFirst message send retries took 365 ms\n[0m[[31merror[0m] [0mTest Failed: testUnreachableServer[0m\njunit.framework.AssertionFailedError: null\n\tat junit.framework.Assert.fail(Assert.java:47)\n\tat junit.framework.Assert.assertTrue(Assert.java:20)\n\tat junit.framework.Assert.assertTrue(Assert.java:27)\n\tat kafka.javaapi.producer.SyncProducerTest.testUnreachableServer(SyncProducerTest.scala:75)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.junit.internal.runners.TestMethodRunner.executeMethodBody(TestMethodRunner.java:99)\n\tat org.junit.internal.runners.TestMethodRunner.runUnprotected(TestMethodRunner.java:81)\n\tat org.junit.internal.runners.BeforeAndAfterRunner.runProtected(BeforeAndAfterRunner.java:34)\n\tat org.junit.internal.runners.TestMethodRunner.runMethod(TestMethodRunner.java:75)\n\tat org.junit.internal.runners.TestMethodRunner.run(TestMethodRunner.java:45)\n\tat org.junit.internal.runners.TestClassMethodsRunner.invokeTestMethod(TestClassMethodsRunner.java:71)\n\tat org.junit.internal.runners.TestClassMethodsRunner.run(TestClassMethodsRunner.java:35)\n\tat org.junit.internal.runners.TestClassRunner$1.runUnprotected(TestClassRunner.java:42)\n\tat org.junit.internal.runners.BeforeAndAfterRunner.runProtected(BeforeAndAfterRunner.java:34)\n\tat org.junit.internal.runners.TestClassRunner.run(TestClassRunner.java:52)\n\tat org.junit.internal.runners.CompositeRunner.run(CompositeRunner.java:29)\n\tat org.junit.runner.JUnitCore.run(JUnitCore.java:121)\n\tat org.junit.runner.JUnitCore.run(JUnitCore.java:100)\n\tat org.junit.runner.JUnitCore.run(JUnitCore.java:91)\n\tat org.scalatest.junit.JUnitSuite$class.run(JUnitSuite.scala:261)\n\tat kafka.javaapi.producer.SyncProducerTest.run(SyncProducerTest.scala:33)\n\tat org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:40)\n\tat sbt.TestRunner.run(TestFramework.scala:53)\n\tat sbt.TestRunner.runTest$1(TestFramework.scala:67)\n\tat sbt.TestRunner.run(TestFramework.scala:76)\n\tat sbt.TestFramework$$anonfun$10$$anonfun$apply$11.runTest$2(TestFramework.scala:194)\n\tat sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)\n\tat sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)\n\tat sbt.NamedTestTask.run(TestFramework.scala:92)\n\tat sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)\n\tat sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)\n\tat sbt.TaskManager$Task.invoke(TaskManager.scala:62)\n\tat sbt.impl.RunTask.doRun$1(RunTask.scala:77)\n\tat sbt.impl.RunTask.runTask(RunTask.scala:85)\n\tat sbt.impl.RunTask.sbt$impl$RunTask$$runIfNotRoot(RunTask.scala:60)\n\tat sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)\n\tat sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)\n\tat sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)\n\tat sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)\n\tat sbt.Control$.trapUnit(Control.scala:19)\n\tat sbt.Distributor$Run$Worker.run(ParallelRunner.scala:131)\n", "comments": ["This test keeps failing on and off on Mac boxes as well. Basically, it is written to test the producer reconnect timeout, so it is timing dependent. I think we should clean all unit tests that have a timing dependency. \n\nFor this ticket, I've attached a patch that deletes these tests from the unit test suite. ", "It seems that the problem is caused by this assertion: Assert.assertTrue((firstEnd-firstStart) < 300). However, I don't see too much value of these 2 tests and would agree that they can be removed. We should also remove the same tests in javaapi too.\n.", "Deleted those tests from kafka.producer.SyncProducerTest as well as kafka.javaapi.producer.SyncProducerTest", "+1", "Committed this patch. Thanks!"], "derived": {"summary": "(If anyone can tell me how to convince Jira to do verbatim output,  I would be grateful)\n\nThis seems to fail about 50% of the time on builds. apache.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "testUnreachableServer sporadically fails - (If anyone can tell me how to convince Jira to do verbatim output,  I would be grateful)\n\nThis seems to fail about 50% of the time on builds. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "Committed this patch. Thanks!"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-147", "title": "kafka integration tests fail on a fresh checkout", "status": "Resolved", "priority": "Blocker", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "labels": [], "created": "2011-10-05T01:28:52.000+0000", "updated": "2011-10-06T01:59:39.000+0000", "description": "On a fresh checkout and with an empty .ivy2 and .m2 cache, if you execute ./sbt update test, the integration tests will fail with this error - \n\njava.lang.NoSuchMethodError: junit.framework.TestSuite.<init>([Ljava/lang/Class;)V\n\tat org.scalatest.junit.JUnit3Suite.run(JUnit3Suite.scala:309)\n\tat org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:40)\n\tat sbt.TestRunner.run(TestFramework.scala:53)\n\tat sbt.TestRunner.runTest$1(TestFramework.scala:67)\n\tat sbt.TestRunner.run(TestFramework.scala:76)\n\tat sbt.TestFramework$$anonfun$10$$anonfun$apply$11.runTest$2(TestFramework.scala:194)\n\tat sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)\n\tat sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)\n\tat sbt.NamedTestTask.run(TestFramework.scala:92)\n\tat sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)\n\tat sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)\n\tat sbt.TaskManager$Task.invoke(TaskManager.scala:62)\n\tat sbt.impl.RunTask.doRun$1(RunTask.scala:77)\n\tat sbt.impl.RunTask.runTask(RunTask.scala:85)\n\tat sbt.impl.RunTask.sbt$impl$RunTask$$runIfNotRoot(RunTask.scala:60)\n\tat sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)\n\tat sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)\n\tat sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)\n\tat sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)\n\tat sbt.Control$.trapUnit(Control.scala:19)\n\tat sbt.Distributor$Run$Worker.run(ParallelRunner.scala:131)\n\nThe reason being 2 versions of the junit jar on the test classpath that SBT uses to run the \"test\" command. The KafkaProject.scala file corrects defines one of the test dependencies to be junit-4.1, since it uses a JUnit api in some of the tests. The problem is that there is another junit jar (v3.8.1) which gets downloaded as a transitive dependency on Scala 2.8.0. The cause of the above error is an incorrect test classpath, that includes both v4.1 as well as v3.8.1.\n\nOne of the possible fixes is to override the \"testClasspath\" variable in SBT to explicitly exclude junit from directories other than core/lib_managed/test\n", "comments": ["Reverting the checkin (r1178669) for KAFKA-92 to resolve this bug.", "+1. We can upgrade to sbt 10.0 later.", "Thanks. Committed the patch"], "derived": {"summary": "On a fresh checkout and with an empty. ivy2 and.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "kafka integration tests fail on a fresh checkout - On a fresh checkout and with an empty. ivy2 and."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks. Committed the patch"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-148", "title": "Shutdown During a Log Flush Throws Exception, Triggers Startup in Recovery Mode", "status": "Resolved", "priority": "Minor", "reporter": "C. Scott Andreas", "assignee": null, "labels": ["patch"], "created": "2011-10-05T22:42:35.000+0000", "updated": "2011-11-12T18:25:50.000+0000", "description": "If a log is being flushed when Kafka is shut down, KafkaScheduler immediately terminates the execution of the flush tasks, causing exceptions to be thrown and aborting the clean shutdown process. When restarted, the instance will launch in recovery mode, scanning the most recent log segment for each topic before launching.\n\nThis issue is particularly highlighted by environments which have a large number of topics and a low flush threshold. In our case, nearly every deploy to our Kafka service in which the app is restarted triggers an unclean shutdown followed by startup in recovery mode. The real pain here is reading through every log file on each restart, but this can be avoided entirely.\n\nThe behavior is caused by the way the KafkaScheduler threadpool (responsible for log flushing) is shut down. Currently, KafkaScheduler.shutdown() calls \"executor.shutdownNow().\" executor.shutdownNow()'s behavior immediately cancels all running tasks in a threadpool and terminates. This causes each Log to throw a java.nio.channels.ClosedByInterruptException as the flush to disk is interrupted, aborting the clean shutdown. Here's a sample log of the exceptions we see thrown when restarting Kafka: https://gist.github.com/1265940\n\nThe fix for this is pretty simple. Rather than calling executor.shutdownNow() in KafkaScheduler.shutdown(), we can call executor.shutdown() (compare Javadoc: http://bit.ly/mZUSaD). This method \"initiates an orderly shutdown in which previously submitted tasks are executed, but no new tasks will be accepted.\" Currently-running flushes are allowed to complete (generally very quick), future flush tasks are cancelled, no exceptions are thrown, and the shutdown finishes cleanly within about a second.\n\nI've attached the patch we've applied and deployed. It applies cleanly to 0.6.1.\n\n- Scott", "comments": ["This was fixed in https://issues.apache.org/jira/browse/KAFKA-126 However, that patch may not apply to 0.6.1.", "Cool, thanks Joel!"], "derived": {"summary": "If a log is being flushed when Kafka is shut down, KafkaScheduler immediately terminates the execution of the flush tasks, causing exceptions to be thrown and aborting the clean shutdown process. When restarted, the instance will launch in recovery mode, scanning the most recent log segment for each topic before launching.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Shutdown During a Log Flush Throws Exception, Triggers Startup in Recovery Mode - If a log is being flushed when Kafka is shut down, KafkaScheduler immediately terminates the execution of the flush tasks, causing exceptions to be thrown and aborting the clean shutdown process. When restarted, the instance will launch in recovery mode, scanning the most recent log segment for each topic before launching."}, {"q": "What updates or decisions were made in the discussion?", "a": "Cool, thanks Joel!"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-149", "title": "Current perf directory has buggy perf tests", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": null, "labels": [], "created": "2011-10-06T18:25:50.000+0000", "updated": "2011-10-09T21:53:23.000+0000", "description": "The scripts in the current perf directory are buggy and not useful to run any reliable Kafka performance tests. The performance tools that work correctly are -\n\nProducerPerformance.scala\nSimpleConsumerPerformance.scala\nConsumerPerformance.scala\n\nCurrently, the above are in the tools directory. Ideally, a Kafka performance suite should repackage these tools with some sample performance load and output data in csv format that can be graphed. \n\nI suggest deleting the perf directory and redoing this cleanly.", "comments": ["+1", "I'm not following what was wrong with the perf/ tools, or what changed to make them unreliable.", "The perf directory had quite a few problems \n\n1. Our perf tools which are tested well were not used.\n2. No clear separation between perf tools and definition of performance load\n3. A lot of custom code written to plot some custom graphs, that cannot be easily understood by existing and new developers\n4. The custom code was in Java, and was buggy (the perf numbers that it gave wasn't exactly indicative of real Kafka performance)\n\nIdeally, our thought is to improve and bundle up the perf tools that we have and write some scripts that clearly define the performance load used and purpose of the perf test.\n\n", "Thanks, that makes sense.  Is there a ticket yet for the New and Improved perf scripts?"], "derived": {"summary": "The scripts in the current perf directory are buggy and not useful to run any reliable Kafka performance tests. The performance tools that work correctly are -\n\nProducerPerformance.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Current perf directory has buggy perf tests - The scripts in the current perf directory are buggy and not useful to run any reliable Kafka performance tests. The performance tools that work correctly are -\n\nProducerPerformance."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks, that makes sense.  Is there a ticket yet for the New and Improved perf scripts?"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-150", "title": "Confusing NodeExistsException failing kafka broker startup", "status": "Closed", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Jay Kreps", "labels": [], "created": "2011-10-07T16:51:19.000+0000", "updated": "2014-10-20T14:08:24.000+0000", "description": "Sometimes, broker startup fails with the following exception \n\n[2011-10-03 15:33:22,193] INFO Awaiting connections on port 9092\n(kafka.network.Acceptor)\n[2011-10-03 15:33:22,193] INFO Registering broker /brokers/ids/0\n(kafka.server.KafkaZooKeeper)\n[2011-10-03 15:33:22,229] INFO conflict in /brokers/ids/0 data:\n10.98.20.109-1317681202194:10.98.20.109:9092 stored data:\n10.98.20.109-1317268078266:10.98.20.109:9092 (kafka.utils.ZkUtils$)\n[2011-10-03 15:33:22,230] FATAL\norg.I0Itec.zkclient.exception.ZkNodeExistsException:\norg.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode =\nNodeExists for /brokers/ids/0 (kafka.server.KafkaServer)\n[2011-10-03 15:33:22,231] FATAL\norg.I0Itec.zkclient.exception.ZkNodeExistsException:\norg.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode =\nNodeExists for /brokers/ids/0\n   at org.I0Itec.zkclient.exception.ZkException.create(ZkException.java:55)\n   at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:685)\n   at org.I0Itec.zkclient.ZkClient.create(ZkClient.java:304)\n   at org.I0Itec.zkclient.ZkClient.createEphemeral(ZkClient.java:328)\n   at kafka.utils.ZkUtils$.createEphemeralPath(ZkUtils.scala:55)\n   at\nkafka.utils.ZkUtils$.createEphemeralPathExpectConflict(ZkUtils.scala:71)\n   at\nkafka.server.KafkaZooKeeper.registerBrokerInZk(KafkaZooKeeper.scala:54)\n   at kafka.log.LogManager.startup(LogManager.scala:122)\n   at kafka.server.KafkaServer.startup(KafkaServer.scala:77)\n   at\nkafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:40)\n   at kafka.Kafka$.main(Kafka.scala:56)\n   at kafka.Kafka.main(Kafka.scala)\nCaused by: org.apache.zookeeper.KeeperException$NodeExistsException:\nKeeperErrorCode = NodeExists for /brokers/ids/0\n   at org.apache.zookeeper.KeeperException.create(KeeperException.java:110)\n   at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)\n   at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:637)\n   at org.I0Itec.zkclient.ZkConnection.create(ZkConnection.java:87)\n   at org.I0Itec.zkclient.ZkClient$1.call(ZkClient.java:308)\n   at org.I0Itec.zkclient.ZkClient$1.call(ZkClient.java:304)\n   at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:675)\n   ... 10 more\n (kafka.server.KafkaServer)\n[2011-10-03 15:33:22,231] INFO Shutting down... (kafka.server.KafkaServer)\n[2011-10-03 15:33:22,232] INFO shutdown scheduler kafka-logcleaner-\n(kafka.utils.KafkaScheduler)\n[2011-10-03 15:33:22,239] INFO shutdown scheduler kafka-logflusher-\n(kafka.utils.KafkaScheduler)\n[2011-10-03 15:33:22,481] INFO zkActor stopped (kafka.log.LogManager)\n[2011-10-03 15:33:22,482] INFO Closing zookeeper client...\n(kafka.server.KafkaZooKeeper)\n[2011-10-03 15:33:22,482] INFO Terminate ZkClient event thread.\n(org.I0Itec.zkclient.ZkEventThread)\n\nThere could be 3 things that might have happened \n(1) you restarted kafka within the zk timeout, in which case as far as zk is concerned your old broker still exists...this is weird but actually correct behavior, \n(2) you have two brokers with the same id, \n(3) zk has a bug and is not deleting ephemeral nodes.\n\nInstead of just throwing the ZK NodeExistsException, we should include the above information in a well-named Kafka exception, for clarity.", "comments": ["Now produces the following error:\n\n[2011-10-23 21:16:53,635] FATAL Fatal error during startup. (kafka.server.KafkaServer)\njava.lang.RuntimeException: A broker is already registered on the path /brokers/ids/0. This probably indicates that you either have configured a brokerid that is already in use, or else you have shutdown this broker and restarted it faster than the zookeeper timeout so it appears to be re-registering.\n\tat kafka.server.KafkaZooKeeper.registerBrokerInZk(KafkaZooKeeper.scala:60)\n\tat kafka.log.LogManager.startup(LogManager.scala:123)\n\tat kafka.server.KafkaServer.startup(KafkaServer.scala:79)\n\tat kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:46)\n\tat kafka.Kafka$.main(Kafka.scala:60)\n\tat kafka.Kafka.main(Kafka.scala)\n", "In KafkaZooKeeper.registerBrokerInZk, in the case statement, we need to rethrow exceptions other than ZkNodeExistsException.", "Hmm, good thinking. But actually I don't think that is how try/catch works in scala. It just catches the exception you specify. So what I have there is equivalent to \ntry {\n  ...\n} catch (ZkNodeExistsException e) {...}\n\nI tested this out just to be sure, and it does seem to work that way (which is good because otherwise every single try/catch would have to remember to rethrow).\n\nscala> jkreps-mn:kafka-trunk jkreps$ scala\nWelcome to Scala version 2.8.0.final (Java HotSpot(TM) 64-Bit Server VM, Java 1.6.0_22).\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala> try {                                     \n     |   throw new Exception(\"\")               \n     | } catch {                                 \n     |   case e: RuntimeException => println(\"gotcha\")\n     | }\njava.lang.Exception: \n\tat .liftedTree1$1(<console>:7)\n\tat .<init>(<console>:6)\n\tat .<clinit>(<console>)\n\tat RequestResult$.<init>(<console>:9)\n\tat RequestResult$.<clinit>(<console>)\n\tat RequestResult$scala_repl_result(<console>)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat scala.tools.nsc.Interpreter$Request$$anonfun$loadAndRun$1$$anonfun$apply$18.apply(Interpreter.scala:981)\n\tat scala.tools.nsc.Interpreter$Request$$anonfun$loadAndRun$1$$anonfun$apply$18.apply(Interpreter.scala:981)\n\tat scala.util.control.Exception$Catch.apply(Exception.scala:...\n", "+1 on the patch. \n\nI thought the try/catch thing in scala is just a special case of match/case.", "trunk is still 0.7, right?", "Good point. Changed the fixed version to 0.7"], "derived": {"summary": "Sometimes, broker startup fails with the following exception \n\n[2011-10-03 15:33:22,193] INFO Awaiting connections on port 9092\n(kafka. network.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Confusing NodeExistsException failing kafka broker startup - Sometimes, broker startup fails with the following exception \n\n[2011-10-03 15:33:22,193] INFO Awaiting connections on port 9092\n(kafka. network."}, {"q": "What updates or decisions were made in the discussion?", "a": "Good point. Changed the fixed version to 0.7"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-151", "title": "Standard .rat-excludes file", "status": "Resolved", "priority": "Trivial", "reporter": "Chris Burroughs", "assignee": "Neha Narkhede", "labels": [], "created": "2011-10-09T21:55:30.000+0000", "updated": "2011-10-14T01:35:33.000+0000", "description": "We should have a standard list of excludes for when we run RAT from the command line (other projects do this with the RAT ant or maven plugins).  As far as I know there isn't a nice way to do that yet so I created  RAT-99\n\nAssuming something like that is excepted, what I have so far is:\n.rat-excludes\nREADME.md\n.gitignore\n.git/.*\n.svn/.*\ntarget.*\nclients/target.*\ncore/target.*\ncontrib/target.*\nproject/plugins/target.*\nboot.*\nTODO$\nexpected.out$", "comments": ["Modified the .rat-excludes that Chris uploaded here, to include READMEs, Makefile, .git, .svn and more.\n\nAdded a run-rat.sh script that reads the .rat-excludes and pipes the rat output to kafka/rat.out\n\nAdded a task to sbt that will run the above mentioned rat script.\n\nAdded the rat jar to lib folder, since somehow it does't get pulled from Maven", "+1", "Kafka has a rat test that passes"], "derived": {"summary": "We should have a standard list of excludes for when we run RAT from the command line (other projects do this with the RAT ant or maven plugins). As far as I know there isn't a nice way to do that yet so I created  RAT-99\n\nAssuming something like that is excepted, what I have so far is:.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Standard .rat-excludes file - We should have a standard list of excludes for when we run RAT from the command line (other projects do this with the RAT ant or maven plugins). As far as I know there isn't a nice way to do that yet so I created  RAT-99\n\nAssuming something like that is excepted, what I have so far is:."}, {"q": "What updates or decisions were made in the discussion?", "a": "Kafka has a rat test that passes"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-152", "title": "Include payload size in DumpLogSegments", "status": "Resolved", "priority": "Minor", "reporter": "Chris Burroughs", "assignee": "Chris Burroughs", "labels": [], "created": "2011-10-10T23:35:57.000+0000", "updated": "2011-10-11T01:11:10.000+0000", "description": "It's handy for debugging large and unprintable messages.", "comments": ["+1 for the patch. Thanks Chris. Just committed this."], "derived": {"summary": "It's handy for debugging large and unprintable messages.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Include payload size in DumpLogSegments - It's handy for debugging large and unprintable messages."}, {"q": "What updates or decisions were made in the discussion?", "a": "+1 for the patch. Thanks Chris. Just committed this."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-153", "title": "Add compression to C# client", "status": "Resolved", "priority": "Major", "reporter": "Eric Hauser", "assignee": null, "labels": [], "created": "2011-10-11T03:19:28.000+0000", "updated": "2011-10-18T20:01:11.000+0000", "description": "Compression support to the C# client.  Configuration was also refactored to support changes along with various performance fixes.", "comments": ["Patch doesn't apply cleanly on a fresh checkout of kafka svn -\n\npatching file clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/ZooKeeperClient.Watcher.cs\nReversed (or previously applied) patch detected!  Assume -R? [n] n\nApply anyway? [n] y\nHunk #1 FAILED at 1.\nHunk #2 succeeded at 14 with fuzz 2 (offset 1 line).\nHunk #3 succeeded at 108 (offset 2 lines).\nHunk #4 succeeded at 207 (offset 2 lines).\nHunk #5 succeeded at 223 (offset 2 lines).\nHunk #6 succeeded at 242 (offset 2 lines).\nHunk #7 succeeded at 265 (offset 2 lines).\nHunk #8 succeeded at 287 (offset 2 lines).\nHunk #9 succeeded at 315 (offset 2 lines).\nHunk #10 succeeded at 359 (offset 2 lines).\nHunk #11 succeeded at 390 (offset 2 lines).\n1 out of 11 hunks FAILED -- saving rejects to file clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/ZooKeeperClient.Watcher.cs.rej\n\n", "Rebased against trunk.", "Thanks for the patch Eric. Just committed this.", "Is this something we want to re-spin the release on, start work on a 0.7.1?\n\n\n"], "derived": {"summary": "Compression support to the C# client. Configuration was also refactored to support changes along with various performance fixes.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add compression to C# client - Compression support to the C# client. Configuration was also refactored to support changes along with various performance fixes."}, {"q": "What updates or decisions were made in the discussion?", "a": "Is this something we want to re-spin the release on, start work on a 0.7.1?"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-154", "title": "ZK consumer may lose a chunk worth of message during rebalance in some rare cases", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Jun Rao", "labels": [], "created": "2011-10-11T17:34:07.000+0000", "updated": "2011-10-15T07:33:38.000+0000", "description": "Occasionally, we have see errors with the following message in the consumer log after a rebalance happens.\n   consumed offset: xxx doesn't match fetch offset: yyy for topicz\n\nThe consumer offset xxx should always match the fetch offset yyy.", "comments": ["This problem is caused by a very subtle bug. When a fetcher calls PartitionTopicInfo.enqueue, we first advance the fetch offset and then enqueue the fetched chunk into a blocking queue. When the fetcher thread is interrupted (because we are shutting down the fetcher after a rebalance), it can happen that we just advanced the fetch offset to xxx, but got interrupted while trying to add the fetched chunk into the queue (so the chunk is not added to the queue). Then a new fetcher gets created to start fetching from xxx. This causes a chunk worth of data just before xxx to be lost to the consumer.  ", "Patch is ready for review.\n\nWe just need to make sure that we enqueue first and then advance the fetch offset. This way, if the enqueue operation gets interrupted, the fetch offset is not advanced. If the enqueue operation succeeds, the offset is guaranteed to be advanced since it can't be interrupted.", "+1. Good catch !", "Thanks for the review. Committed the patch.", "While looking at this area of the code, I was wondering about this patch: wouldn't it permit the mirror issue of enqueue and not advancing the offset, since the interrupt could occur just before the fetch offset update? So the new fetcher may fetch the same offset again. It seems to me that the interrupt and PartitionTopicInfo's enqueue method itself should be mutually exclusive - or perhaps provide suitable handling for chunks with the same fetch offset in the consumer iterator. Or I must be missing something obvious :)", "Ok nm - it's late.. I see that the queues are cleared out."], "derived": {"summary": "Occasionally, we have see errors with the following message in the consumer log after a rebalance happens. consumed offset: xxx doesn't match fetch offset: yyy for topicz\n\nThe consumer offset xxx should always match the fetch offset yyy.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "ZK consumer may lose a chunk worth of message during rebalance in some rare cases - Occasionally, we have see errors with the following message in the consumer log after a rebalance happens. consumed offset: xxx doesn't match fetch offset: yyy for topicz\n\nThe consumer offset xxx should always match the fetch offset yyy."}, {"q": "What updates or decisions were made in the discussion?", "a": "Ok nm - it's late.. I see that the queues are cleared out."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-155", "title": "Support graceful Decommissioning of Broker", "status": "Resolved", "priority": "Major", "reporter": "Sharad Agarwal", "assignee": null, "labels": [], "created": "2011-10-14T05:40:44.000+0000", "updated": "2013-03-20T16:25:16.000+0000", "description": "There should be a graceful way of decommissioning the broker so that there is absolutely 0 data loss. Decommissioning is not necessarily related to replication (Kafka-50).\n\nThere should be a way to get the broker out of the cluster only from the produce side. Consumers should be able to continue keep pulling data. When the administrator is sure that all data has been consumed by consumers, broker node can be removed permanently.\n\nSame would be useful for rolling upgrades without any message loss.\n", "comments": ["I think this is done in 0.8 branch with the ShutdownBroker admin command (which goes through JMX command KafkController.shutdownBroker())", "ShutdownBroker admin command does not completely achieve this. It merely moves the leader from that broker and then shuts the broker down. But now, some of the partitions could be under replicated. What really solves the problem is preferred replica election admin command. This allows you to add new brokers to an existing cluster, move some partition off of the brokers to be decommissioned and then shutdown the broker by killing it.", "You mean the partition reassignment tool, not preferred replication election tool, right?", "Oh, right, i meant the partition reassignment tool :)"], "derived": {"summary": "There should be a graceful way of decommissioning the broker so that there is absolutely 0 data loss. Decommissioning is not necessarily related to replication (Kafka-50).", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support graceful Decommissioning of Broker - There should be a graceful way of decommissioning the broker so that there is absolutely 0 data loss. Decommissioning is not necessarily related to replication (Kafka-50)."}, {"q": "What updates or decisions were made in the discussion?", "a": "Oh, right, i meant the partition reassignment tool :)"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-156", "title": "Messages should not be dropped when brokers are unavailable", "status": "Resolved", "priority": "Major", "reporter": "Sharad Agarwal", "assignee": "Dru Panchal", "labels": [], "created": "2011-10-15T13:28:47.000+0000", "updated": "2016-10-05T09:30:14.000+0000", "description": "When none of the broker is available, producer should spool the messages to disk and keep retrying for brokers to come back.\n\nThis will also enable brokers upgrade/maintenance without message loss.", "comments": ["This is more or less what we do with SyncProducer .  But I'm not sure there is one right choice for durability,  shedding load, exponential vs linear backoff, etc. ", "Chris,\n\nWe currently don't do this with any Producer in Kafka. The scenario that Sharad has specified is when no broker is available. Today, in Kafka, the producers would just throw ConnectionRefused/NoBrokerPartitionsAvailable exceptions back to the caller, instead of spooling data to disk and then retrying later.\n\n", "Sorry, wrong \"we\".  I meant this is what the company I work for does when calling SyncProducer.", "If we spool, when the broker comes back, should we deliver the spool messages first, or deliver them in parallel with new incoming messages?", "This would be a good feature to have. I think we will probably not use it at LinkedIn, because managing disk space on all machines is something we would like to avoid. I think likely we will just use replication in cases where we want to guarantee delivery. However many people have asked for this.\n\nThere are a lot of implementation details to work out. I think the sanest thing to do would be to use the Log.scala class to create a single log for the client. In the case of the async producer, it should really log the event to this log immediately and background thread should pull from this log instead of using an in-memory blocking queue. This would \"guarantee\" delivery (assuming nothing happens to the filesystem of the sender machine). The sync producer could probably just log events that fail? That kind of changes the semantics though. It would be good to see a concrete proposal of how all this would plug together, what guarantees it would give, etc.", "To clarify what I was saying, the single log would likely have the message content as well as the key (if any), and the topic...essentially all the metadata about the send. I think there is no reason to keep more than one log since this is effectively just the \"unsent messages\" log.", "Perhaps the producers themselves should just take some sort of MessageSendErrorHandler.  And implementations could queue, retry with backoff, writ to disk, drop but log, etc.\n\nFWIW Clearspring has a pipeline with: ConcurrentQueue --> spill to disk queue with max size (then drops messages) --> SyncProducer with retry/backoff.", "bq. because managing disk space on all machines is something we would like to avoid. \ntrue. but I assume that disk buffer will be bounded which will keep things simple to manage the disk space.\n\nIMO this feature is critical to have critical data pipelines to move to kafka. Without this:\n- Incase of temporary network glitches messages would be lost\n- Rolling upgrades is required which may not be that easy operability wise. Otherwise we can simply restart the whole cluster.\n\nThe other systems (flume, scribe) do manage this via producer/agent side spooling.\n\nbq. If we spool, when the broker comes back, should we deliver the spool messages first, or deliver them in parallel with new incoming messages?\n\nWe should always deliver the messages in the order of being produced. So it should always be from the spool first.\n\n\n", "I agree it is a good feature! It overlaps somewhat with broker replication which makes the partitions themselves more highly available, but they solve slightly different problems. The weakness of the spooling is that you can lose the spooled data if you lose the client filesystem, but it has the advantage of lower resource requirements overall, I think. It would be good to support both. I think the next step would be to have a more detailed design of how this would impact the producer code.", "This is feature is very important for me. I am intrested in migrating from Scribe to Kafka and this feature is the only thing keeping me from starting now. \n\nIt looks like isse https://issues.apache.org/jira/browse/KAFKA-789 is a duplicate of this.\n", "I was led to believe this is most probably not planned for 0.8 now. \nJust to comment that currently in 0.8 a message set asynchronously sent may partially fail, in the sense that only some of its destinations (determined by topic & partition) will fail while others succeed. That seems to mean a 'one log spool' for all messages (if all messages are spooled before attempting to send them) requires non-sequential management of such a log, or overheads over-proportional to the proportion of failed messages. Perhaps a log per topic would simplify the algorithm but then there's partitions as well...", "Yes, this is likely a post 0.8 item. ", "I am positive that the producer wire protocol has to have built-in features to support the ability to prevent dropped messages when brokers are unavailable.  There is no way to achieve 'optimal transmission' without two-phase commit or idempotence between the producer and broker.  I define 'optimal transmission' as the guarantee that data is not duplicated or lost after some well known point has been reached as viewed by the producer.   Prior to this point (for example, when the message is in a in memory queue), there can be no guarantees from any system.\n\n{quote}\n\"FWIW Clearspring has a pipeline with: ConcurrentQueue --> spill to disk queue with max size (then drops messages) --> SyncProducer with retry/backoff. \"\n{quote}\nSuch a system can get as close as only losing or duplicating one 'batch' of messages, where that batch size is >= 1 message.  At best, when reading form the data spilled from disk, between sending a batch and recieving acknowledgement, a crash at either end will leave that batch in limbo.   The batch needs an identifier that both sides can persist or generate to identify the batch in case one side has to recover from a crash.(two phase commit).  Many database systems have this (see http://www.postgresql.org/docs/9.2/static/sql-prepare-transaction.html), where as a client you can name a transaction so that after you get an acknowledgement from the prepare commit, the client can log that it has been prepared, send the commit command, and if it crashes before getting the acknowledgement, upon recovery it can look up the identifier for the in flight commit, and check with the system to see if it succeeded or not.\n\n\nWe have an internal system that we are attempting to replace Kafka with, but it does not guarantee delivery as we do.  We spool data on our producers into batches (a file per batch), and then transfer these batches into the downstream system.  This system stores these batches in a staging area, so that if either side crashes before the batch transfer completes recovery is simple.  Upon validating that the batch (which is uniquely  named) is identical on both sides, the producer can remove it locally and promote from the staging area to the completed area (atomically).  This again is safe if either side crashes, since an item in the staging area that does not exist on the producer indicates it has successfully been moved.\n\nKafka will have to mimic this sort of safety at each stage.  On the consumer side, batch offsets + partition and topic information serve as unique identifiers for a batch that allow only-once semantics.  On the producer side, is there something equivalent? \n\nReplication mitigates the problem significantly, but there is still the possibility that an item is dropped or duplicated if there is a transient network issue that TCP/IP does not handle, if the broker does not hand out unique batch ids for each batch (I am unsure of this). \n\n If messages are spooled to disk when a broker is unavailable, the process of reading back items from that log and sending them to the broker without loss or duplication is tricky.  Each batch of messages will need an identifier shared between the broker and producer, and the batch will need to be marked with the identifier safely to disk prior to sending the batch to the broker.  After acknowledgement the producer can delete the batch or mark it complete.  If it crashes between sending the batch to the broker and receiving a response (or otherwise fails to get acknowledgement) it must be able to ask the broker whether the batch with the given identifier was received, or alternatively, it can send the batch twice and the broker will ignore the duplicate send based on the identifier.\n\nDoes the producer wire protocol include batch ids generated by the broker so that this can be implemented?  It does not seem to be the case here https://cwiki.apache.org/KAFKA/a-guide-to-the-kafka-protocol.html#AGuideToTheKafkaProtocol-ProduceAPI\nThis protocol does not seem to support the ability to support \"only once\" message semantics.\n", "To support lossless transmission, the producer protocol will need to change to have a two-phase exchange.\n\nRather than sending a message batch, and receiving a response with the offset of the first message as described: https://cwiki.apache.org/KAFKA/a-guide-to-the-kafka-protocol.html#AGuideToTheKafkaProtocol-ProduceAPI as the Producer Request and Producer Response, the process would have two exchanges per batch:\n\nOption 1: Broker assigns batch ids as the batches come in.  Drawback: broker must track UUIDs and hold state on them for potentially a long time.\n\n1.  \"Batch Prepare\": Producer send batch to broker\n2.  \"Acknowledge Batch Prepare\": Broker commits batch to staging area, and assigns a UUID (or similar unique id) to the batch, and returns the UUID to the producer\n3.  \"Batch Commit\": Producer sends message to broker to commit the batch, with the UUID token to identify the batch;\n4.  \"Acknowledge Batch Commit\": Broker commits batch from staging area to topic atomically (or idempotently and non-atomically), and returns an acknowledgement with the offset\n\nIf the producer crashes or loses connection between steps 1 and 2 or 2 and 3 (or the network breaks, and restores), it can send the batch again, get a new UIUD, and start over, orphaning the first batch.  The client needs to be able to clear out orphaned batches it created, or they must expire after a long time.\nIf the producer crashes or has network issues between steps 3 and 4, then upon restore it will attempt step 3 again, which is idempotent and safe.  The broker has to keep the in flight UUIDs and used UUIDs for a while because a client may have some large time lag in recovery between a failed step 3 to 3 exchange, and step 3 and 4 may occur multiple times as a result.\n\nOption 2:  Pre-assigned batch ids.  Benefit:  failure between steps 1 and 3 does not orphan a batch.\n0a.  \"Request Batch IDs\": Producer requests a set of batch ids that are unique for use \n0b.  \"Receive Batch IDs\": Broker returns UUIDs for use as batch ids later.\n1.  \"Batch Prepare\": Producer send batch to broker with one of the UUIDs.\n2.  \"Acknowledge Batch Prepare\": Broker commits batch to staging area tagged with the UUID\n3.  \"Batch Commit\": Producer sends message to broker to commit the batch, with the UUID token to identify the batch;\n4.  \"Acknowledge Batch Commit\": Broker commits batch from staging area to topic atomically (or idempotently and non-atomically), and returns an acknowledgement with the offset\n\nIf the producer crashes or loses connection between steps 1 and 2 or 2 and 3, it can attempt step 3, optimistically assuming the broker got the batch and has been staged.  If it did not and step 3 fails, it can start over with step 1 using the same UUID.\nIf the producer crashes or loses connection between steps 3 and 4, then upon restore it will attempt step 3 again, which is idempotent and safe.  If step 3 fails it can assume the batch has already been committed.  The broker has to track in flight UUIDs and recently committed UUIDs and the corresponding offsets for a while because a client may have some large time lag in recovery after a failure between steps 3 and 4, and steps 3 and 4 may occur more than once for a given batch.\n\nIf it is tolerable to lose or duplicate up to one batch per failure (network, consumer, or broker), none of the above is required, and the described protocol is sufficient.  Since there is always the possibility of message loss if the producer crashes, this may be acceptable, however it would be nice to not worry about data loss due to broker or network failure, and leave the only loss window at the producer side.\n\nWith two phase batches, a producer can safely spool data to disk in the event of serious error, and recover, or spool to disk in all conditions prior to sending downstream to the broker.  Each batch from disk would be tagged with a UUID and a log can be kept on what the current state is relative to the four steps above so that recovery can initiate at the right spot and no batches missed or duplicated.", "is there any code for this or the SyncProducer available?  I like the idea of just having a log to write and read from.  Perhaps try and send an event to a Broker manager.  Are there any projects with replicated broker/consumer/producer management out there?\n\nthanks,\nrob", "Scott--I am interested in implementing a deduplication scheme similar to what you propose. I think this would have several uses. This would definitely be post 0.8.\n\nI do think we are conflating the storage location (disk versus memory) with deduplication/commit mechanism. I claim the scheme you propose just avoids duplicates and is unrelated to writing data to disk.\n\nI have to say I am a little skeptical of the \"fall back to disk thing\". In our usage we have many thousands of servers and a small number of kafka servers with nice disks--I think this is fairly standard. The idea that involving thousands of crappy local disks in the data pipeline will decrease the empirical frequency of data loss seems dubious to me.\n\nBut regardless of whether you buffer on disk or buffer in memory (as the client currently does). As long as the client has sufficient space to buffer until the server is available again there is no data loss. And indeed the replication fail-over is very fast so this really does work. As you point out, though that does lead to the possibility of duplicate messages. Which is where you proposal comes in.\n\nI had thought of a similar thing. Here was my idea:\n1. Client provides a unique instance id for itself.\n2. Each message contains the instance id and a per-client sequence number\n3. Broker maintains a per-client highwater mark on the sequence number, periodically checkpointed to disk\n4. In the event of a hard crash the broker rebuilds the highwater marks from the last checkpoint and the log\n5. Broker discards any request containing a message from a client that has a sequence number less than or equal to the high-water mark.\n\nThe advantage of this approach would be that it doesn't require a multi-phase produce, the disadvantage is that it requires assigning client ids.\n\nOne question about your proposal. Let's say that the broker fails before sending the \"Acknowledge Batch Commit\", ownership of that partition fails over to another broker but that broker but the client doesn't know if the transaction was committed (and the broker died just before sending the ack) or was not committed. How can the producer then send to the other broker which won't have the same UUID info?", "Jay --\n\nI agree, the duplication issue does not depend on whether there is a disk or memory queue.  However, in both cases one can choose to dither duplicate messages or drop them on failures.  In the in memory case, biasing it to drop a message rather than duplicate on a failure is more acceptable than the on disk case.  This is because an in memory queue is more likely to suffer loss than a disk queue.  For example, a producer may crash or be kill-9'd and we would expect in flight, in memory data to be lost.\nMy thoughts on this issue are biased by our legacy system -- each producer-equivalent would log locally and then the equivalent of the broker would 'harvest' these logs with no possible duplication.  Loss is possible if the disks failed on the client, but that would take down the whole app anyway.  Furthermore, we use SSD's on those servers (since late 2008!) and have not had a single SSD drive failure where data was lost (we had a couple have their performance degrade to abysmal levels, but the data was still there). \nAdditionally, we are able to restart / service the nodes that collect the data without data loss because of the local spooling.   Replication in Kafka will allow us to do rolling restarts of brokers and achieve similar operational utility.  The need for 'spill to disk' is certainly less with replication active.  However, it doesn't take us long to fill our entire memory buffer up full of messages on some of our clients -- even a 10 second window of unavailability means losing messages unless we can spill to disk.\n\nOn your proposal:\n* What happens if there is a 'bubble' in sequence ids from the broker perspective?  What does the broker do?  How does the client know to re-send?\n* What happens when two clients assign themselves the same id?\n\nAnswer to question on my proposal:\n* It is assumed that the final batch commit is idempotent, so if the client fails to get the final ACK (step 4, \"Acknowledge Batch Commit\" it will go back to step 3 and send the batch commit message again.  If it is the same broker, it can simply acknowledge since it already committed it.  If it is a replica, then there are two cases:\n  a) The other broker has the UUID info (which is replicated?) and can restart the process at the right point.\n  b) Failover to another broker starts the process over at step 1 with the same UUID, and when the broker that crashed comes online the brokers in the replica set reconcile to remove the duplicate.  There are a limited number of in flight or recently in flight batches.\n\nI think b will work, but I don't know enough about how a broker replica set reconciles in 0.8 when one fails.  If we assume strict ordering on whether the replica or the client gets the ACK for a batch commit first, a repair process should be consistent.\n\nA two-phase produce doesn't have to be serial from batch to batch -- a few pipelined requests could be supported, but too many could be used for DOS.  A high-water-mark approach is more difficult to pipeline, but probably does not need it.\n\n\nOne idea I had is far more radical.  It boils down to these questions:\nWhy even have a separate producer protocol at all?  Why isn't the consumer protocol good enough for getting data to the brokers?\n\nI admit, this is tricky and I have not thought through it well; but I think it is worth sharing.  The consumer protocol is highly reliable and easy to enforce once-only semantics.   If there was some sort of client-initiated broker 'pull' with the consumer protocol, there might be some opportunities for overall simplification in the protocol and more sharing in the code.\nA producer would be required to assign an offset id and increment per message.  The producer would trigger the broker to begin initiate a request to read all of the batches from that starting ID to the \"end\" , commit it, then start from the last offset to the \"end\", and repeat.  This makes a producer like a broker -- except that it wants to drop data a lot faster, and therefore needs to know how far along the broker is in pulling data down.  Perhaps it can safely assume that if batch \"1 to 50\" was requested, and subsequently batch \"51 to 100\" is requested, that the request for the latter batch indicates that the first has successfully been committed, but that serializes batches and prevents pipelining.  Alternatively the \"1 to 50 is committed\" message can ride with the \"get 51 to 100\" request.\nWhat I find useful here is the same thing that is great about the consumer protocol: putting the burden on the one obtaining the data to track progress is cleaner in the face of failure.\nThis bears similarity to your proposal, but with inversion of control -- the broker asks for the next batch when it is ready.   If there is a broker failure, the replica can pull in messages, and duplicate removal can occur when they reconcile (and the offset id in the topic will be consistent, since both sides use offsets).  Producers are then responsible for buffering up to the threshold they can tolerate, and can spool to disk if they please (perhaps re-using some broker code to do so).\n\n", "Scott Carey - completely agree with your use-case here.  The ability todo rolling restarts and have processes spill to disk when there is outages so that we can in some-case withstand hours in stead of minutes in the case of a failure of the cluster or a strange network outage.", "This JIRA is duplicated by KAFKA-789 which has provided the requested solution in Kafka 0.10.1.0. Marking this JIRA resolved.", "[~dpnchl] KAFKA-789 is already resolved as a duplicate of this issue itself. Also, as far as I know, there are no current plans to include this feature in 0.10.1", "To clarify, the issue where are tracking this is now https://issues.apache.org/jira/browse/KAFKA-1955 since it includes a WIP patch."], "derived": {"summary": "When none of the broker is available, producer should spool the messages to disk and keep retrying for brokers to come back. This will also enable brokers upgrade/maintenance without message loss.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Messages should not be dropped when brokers are unavailable - When none of the broker is available, producer should spool the messages to disk and keep retrying for brokers to come back. This will also enable brokers upgrade/maintenance without message loss."}, {"q": "What updates or decisions were made in the discussion?", "a": "To clarify, the issue where are tracking this is now https://issues.apache.org/jira/browse/KAFKA-1955 since it includes a WIP patch."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-157", "title": "Message may be delivered to incorrect partition incase of semantic partitioning", "status": "Resolved", "priority": "Major", "reporter": "Sharad Agarwal", "assignee": null, "labels": [], "created": "2011-10-15T14:38:24.000+0000", "updated": "2015-02-07T23:49:12.000+0000", "description": "Incase the broker hosting the partition is down, messages are currently repartitioned with the number of available brokers. This may lead to void the partitioning contract.\n\nhttp://bit.ly/oEz2fT", "comments": ["I think the only fix for this is the replication work that makes the partitions highly available. I think with the current non-HA partitions you have only two choices: non-availablility or sloppy partitioning."], "derived": {"summary": "Incase the broker hosting the partition is down, messages are currently repartitioned with the number of available brokers. This may lead to void the partitioning contract.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Message may be delivered to incorrect partition incase of semantic partitioning - Incase the broker hosting the partition is down, messages are currently repartitioned with the number of available brokers. This may lead to void the partitioning contract."}, {"q": "What updates or decisions were made in the discussion?", "a": "I think the only fix for this is the replication work that makes the partitions highly available. I think with the current non-HA partitions you have only two choices: non-availablility or sloppy partitioning."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-158", "title": "go consumer & producer to support compression", "status": "Resolved", "priority": "Minor", "reporter": "Jeffrey Damick", "assignee": null, "labels": ["go-client"], "created": "2011-10-17T13:52:52.000+0000", "updated": "2011-10-27T14:23:33.000+0000", "description": "As related to KAFKA-79, the go consumer and producer needs to support the compression attribute per https://cwiki.apache.org/confluence/display/KAFKA/Compression.\n\nCan someone assign this to me, i'll add support and create a patch.\n\nthanks ", "comments": ["Jeffrey, I tried assigning this to you, but your name doesn't show up in the JIRA list for Kafka. ", "tested against the kafka/bin/producershell and consumer.  ", "ok, but the tests weren't updated and no new ones added, it doesnt support either compressed or uncompressed messages, and it doesnt transparently decompress the messages or have a way to plugin different compression codecs.. Let me just finish up this patch...", "Take 2 on the patch, with updated tests ", "patch to add ability to compressed messages\n\nSeeing strange behavior from kafka when sending the compressed messages:\n\ngo client sends this: \n\n00 00 00 36 00 00 00 04 74 65 73 74 00 00 00 00 00 00 00 26 00 00 00 22 01 01 F0 17 43 A5 1F 8B 08 00 00 00 00 00 04 FF 4A CE CF 2D 00 04 00 00 FF FF 3A 6F 0A CB 04 00 00 00\n\nand servers gives:\n\n[2011-10-20 11:51:50,106] DEBUG Listening to new connection from /127.0.0.1:59861 (kafka.network.Processor)\n[2011-10-20 11:51:50,107] TRACE 54 bytes read from /127.0.0.1:59861 (kafka.network.Processor)\n[2011-10-20 11:51:50,115] TRACE Handling produce request from /127.0.0.1:59861 (kafka.request.logger)\n[2011-10-20 11:51:50,119] TRACE Producer request ProducerRequest(test,0,38) (kafka.request.logger)\n[2011-10-20 11:51:50,119] DEBUG makeNext() in deepIterator: innerDone = true (kafka.message.ByteBufferMessageSet)\n[2011-10-20 11:51:50,119] TRACE Remaining bytes in iterator = 34 (kafka.message.ByteBufferMessageSet)\n[2011-10-20 11:51:50,119] TRACE size of data = 34 (kafka.message.ByteBufferMessageSet)\n[2011-10-20 11:51:50,120] DEBUG Message is compressed. Valid byte count = 0 (kafka.message.ByteBufferMessageSet)\n[2011-10-20 11:51:50,133] DEBUG makeNext() in deepIterator: innerDone = true (kafka.message.ByteBufferMessageSet)\n[2011-10-20 11:51:50,133] TRACE Remaining bytes in iterator = 0 (kafka.message.ByteBufferMessageSet)\n[2011-10-20 11:51:50,133] TRACE size of data = 1668246896 (kafka.message.ByteBufferMessageSet)\n[2011-10-20 11:51:50,134] ERROR Error processing ProduceRequest on test:0 (kafka.server.KafkaRequestHandlers)\nkafka.common.InvalidMessageSizeException: invalid message size: 1668246896 only received bytes: 0 at 0( possible causes (1) a single message larger than the fetch size; (2) log corruption )\n\n", "It seems that you are sending a message marked as compressed. However, after the payload is decompressed, the content is not well-formatted. Likely causes include: (1) the message is actually not compressed; (2) the compression codec is not gzip. ", "the curious part is that the scala producer writes this:\n\n0000003c0000000474657374000000000000002c000000280101891b70861f8b0800000000000000636060e0626438cd956f959c9f5b0000dce317a70e000000\n\nAnd after breaking it down, the message part is:\n\n1f 8b 08 00 00 00 00 00 00 00 63 60 60 e0 62 64 38 cd 95 6f 95 9c 9f 5b 00 00 dc e3 17 a7 0e 00 00 00\n\nafter ungzip'ing it becomes (gzip cmd line to hexdump):\n\n00 00 00 0a 01 00 cb 0a  6f 3a 63 6f 6d 70        |........o:comp|\n\nDid something else in the message format change when it's compressed?\n\nI see the same result when i decompress it in go in my consumer.. \n\n", "i should add the text i send from the producer was:  'comp'", "The go producer and the scala producer should send the same bytes for the same message, right?", "not necessarily, the gzip implementation is different. But I'm wondering why when I use gzip (cmd line, not from go) to ungzip that message it has this on the front: (as seen above) 00 00 00 0a 01 00 cb 0a 6f 3a\n\nThis happens to match what i see in go (the extra bytes on the front of 'comp') .. so i must be parsing it wrong somehow...\n", "In the broker, we need to unzip compressed messages to verify the crc. Can unzip decoder messages encoded by compress?", "Are you saying you don't use gzip to decompress the messages in the broker?  i don't understand the question?", "I think I figured out my mistake, inside the gzip is yet another message... i misunderstood and thought it was only the payload that compressed.  So it's a compressed message with an uncompressed message inside it, is that really what it's supposed to be? \n\n", "That's right. We take one or more uncompressed messages and compress them using gzip and store the compressed bytes in the payload of a single message. This way, we can recursively iterate into a compressed message. See CompressionUtils.compress for details.", "thanks, i'll make the appropriate updates.  But it seems like this compression flag would be a better fit on the 'message set' and then use that to encapsulate all messages.. ", "There is a compression flag on ByteBufferMessageSet, with the following signature:\n\n  def this(compressionCodec: CompressionCodec, messages: Message*)\n", "nevermind, i see.", "Working patch for dealing with compressed & uncompressed messages.  Updated tests & added a pluggable interface for future other payload codecs (compression or other)", "Jeffrey, thanks for the patch. It looks good, though I wasn't able to build the go code and run the unit tests. The instructions in the README seem to be outdated ? ", "I get the following - are these files missing from your patch?\n\nThanks,\n\nJoel\n\nclients/go]$ GOROOT=. make install\nMakefile:1: src/Make.inc: No such file or directory\nMakefile:14: src/Make.pkg: No such file or directory\nmake: *** No rule to make target `src/Make.pkg'.  Stop.", "joel: just a guess but it looks like you goroot isn't set right.  It needs to point to the location where you installed go, mine points to /opt/go for example.  May want to double check: http://golang.org/doc/install.html#install\n\nNeha: i'm glad to help, what error did you get?", "I saw the same error that Joel mentioned. Let me follow the installation for go and see if that helps.", "Yes - that was the issue. It is probably obvious to go users. Otherwise, it would be good to mention this in the readme.\n\n+1\n\n", "+1 on updating the README. Thanks for the patch !", "who commits the patch then?  i would if i could, can i have access to that part of the tree?", "One of the committers can accept the patch. Please can you update the README and upload an updated patch ?", "updated the README, including links to the incubator website & go installation.", "Thanks for being responsive. Just committed this !"], "derived": {"summary": "As related to KAFKA-79, the go consumer and producer needs to support the compression attribute per https://cwiki. apache.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "go consumer & producer to support compression - As related to KAFKA-79, the go consumer and producer needs to support the compression attribute per https://cwiki. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for being responsive. Just committed this !"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-159", "title": "Php Client support for compression attribute", "status": "Resolved", "priority": "Minor", "reporter": "AaronR", "assignee": null, "labels": [], "created": "2011-10-18T04:44:49.000+0000", "updated": "2011-10-18T17:57:58.000+0000", "description": "The php client didn't support the new compression attribute \n\nhttps://cwiki.apache.org/confluence/display/KAFKA/Compression", "comments": ["Not entirely sure about this but it allowed the producer/consumer examples to work with other clients better.  ", "Thanks for the patch Aaron. Just committed this."], "derived": {"summary": "The php client didn't support the new compression attribute \n\nhttps://cwiki. apache.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Php Client support for compression attribute - The php client didn't support the new compression attribute \n\nhttps://cwiki. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for the patch Aaron. Just committed this."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-160", "title": "ZK consumer gets into infinite loop if a message is larger than fetch size", "status": "Resolved", "priority": "Blocker", "reporter": "Jun Rao", "assignee": "Jun Rao", "labels": [], "created": "2011-10-18T17:11:16.000+0000", "updated": "2011-10-19T23:53:34.000+0000", "description": null, "comments": ["There were a couple of problems.\n1. We used to throw an exception while iterating messages in a ByteBufferMessageSet, if we can't iterate a single message out of the buffer. This indicates that either the fetch size is too small or if there is corruption in the data. Throwing exception is a good way to notify the consumer that something is wrong. After the compression patch, such an exception is no longer thrown.\n\n2. The constructor of ByetBufferMessageSet recently added a new parameter in the middle and some of the callers didn't get changed accordingly. So, some input parameters are misaligned.\n\nAttach a patch that fixes both problems.", "Hey did I break this?", "I think this was introduced when compression was added. I reviewed the patch, but didn't catch this.", "Submitted patch v2, adding a unit test that uncovers the problem.", "+1 on patch v2", "Committed."], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "ZK consumer gets into infinite loop if a message is larger than fetch size"}, {"q": "What updates or decisions were made in the discussion?", "a": "+1 on patch v2"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-161", "title": "Producer using broker list does not load balance requests across multiple partitions on a broker", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "labels": [], "created": "2011-10-18T21:58:20.000+0000", "updated": "2011-10-25T04:59:40.000+0000", "description": "https://issues.apache.org/jira/browse/KAFKA-129 introduced a bug in the load balancing logic of the Producer using broker.list.\nSince the broker.list doesn't specify the number of partitions in total, it should ideally pick a broker randomly, and then send the produce request with partition id -1, so that the EventHandler routes the request to a random partition.\nInstead of that, it defaults to 1 partition on each broker and ends up using the Partitioner to pick a partitions amongst the available ones.", "comments": ["Fixing the load balancing strategy for the broker.list option on the Producer. With this change, when broker.list is used, the producer will pick a random broker and send request to partition -1 on that broker.", "1. We should guard logger.debug with the isDebugEnabled check.\n2. The method name getNumPartitionsForTopic is misleading since the return value is a list of partitions, instead of number of partitions. Can we rename it to sth like getPartitionListForTopic? Ditto for the name of the variable being assigned to.\n", "1. Guarded the logger.debug\n2. Changed the names of variables and functions to make more sense", "+1 on the new patch."], "derived": {"summary": "https://issues. apache.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Producer using broker list does not load balance requests across multiple partitions on a broker - https://issues. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "+1 on the new patch."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-162", "title": "Upgrade python producer to the new message format version", "status": "Resolved", "priority": "Major", "reporter": "Jay Kreps", "assignee": "Jay Kreps", "labels": [], "created": "2011-10-18T22:17:50.000+0000", "updated": "2011-10-19T22:15:40.000+0000", "description": "The python producer is still on the old message format version (pre-compression). This can cause some issues when integrating with consumers that don't support this. It would be good to bump the python producer up to the current version.", "comments": ["+1"], "derived": {"summary": "The python producer is still on the old message format version (pre-compression). This can cause some issues when integrating with consumers that don't support this.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Upgrade python producer to the new message format version - The python producer is still on the old message format version (pre-compression). This can cause some issues when integrating with consumers that don't support this."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-163", "title": "Ruby client needs to support new compression byte", "status": "Closed", "priority": "Minor", "reporter": "AaronR", "assignee": null, "labels": [], "created": "2011-10-19T02:10:44.000+0000", "updated": "2013-04-04T20:43:14.000+0000", "description": "Ruby client updates", "comments": ["tested against the consumer/producer shell (scala clients) as well as go producer/consumers.", "This seems to be a patch for 158, not 163.", "Try2", "-1. Rat fails. Attaching the output here.", "Hi Aaron, I think there is missing licensing info. Can you update that and trying running the rat tool under bin/ to verify it is all apache compliant?", "Old bug. Client libraries, such as this Ruby library, are no longer being maintained in the main project.\n\nPlease close. Won't Fix."], "derived": {"summary": "Ruby client updates.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Ruby client needs to support new compression byte - Ruby client updates."}, {"q": "What updates or decisions were made in the discussion?", "a": "Old bug. Client libraries, such as this Ruby library, are no longer being maintained in the main project.\n\nPlease close. Won't Fix."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-164", "title": "Config should default to a higher-throughput configuration for log.flush.interval", "status": "Resolved", "priority": "Major", "reporter": "Jay Kreps", "assignee": "Jay Kreps", "labels": [], "created": "2011-10-20T15:36:59.000+0000", "updated": "2011-10-24T17:37:42.000+0000", "description": "Currently we default the flush interval to log.flush.interval=1. This is very slow as it immediately flushes each message. I recommend we change this to 20000 and drop the time-based flush to 1 second. This should be a good default trade-off between latency and throughput.", "comments": ["Maybe shrink flush time to 500ms?", "There are good things to be said about having the default configuration be the \"overly paranoid do not loose your data\" one, even though once they think about what that means no one actually wants that.  I'd stick with round numbers and a note elaborating on the trade-offs.", "I added documentation on the tradeoffs for flush policy. I also went back and improved the documention across the board for the options we have and added in commented out stubs for options that were missing but are available (such as size-based retention policy).\n\nI find no one ever reads the code to find out about configs so the config file is a pretty good place to document them.", "Err, ignore the random Jmx file in the patch...left over from another commit.", "+1 on changes in server.properties"], "derived": {"summary": "Currently we default the flush interval to log. flush.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "Config should default to a higher-throughput configuration for log.flush.interval - Currently we default the flush interval to log. flush."}, {"q": "What updates or decisions were made in the discussion?", "a": "+1 on changes in server.properties"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-165", "title": "Add helper script for zkCli.sh", "status": "Resolved", "priority": "Minor", "reporter": "Jay Kreps", "assignee": "Jay Kreps", "labels": [], "created": "2011-10-20T15:50:01.000+0000", "updated": "2011-10-23T22:02:24.000+0000", "description": "It would be nice to have a helper script for starting zkCli packaged in so we could include it's usage in the docs.", "comments": ["This patch adds the start script. It does not include jline support since we don't have jline right now. When we add jline we should add it for all interactive scripts.", "+1"], "derived": {"summary": "It would be nice to have a helper script for starting zkCli packaged in so we could include it's usage in the docs.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Add helper script for zkCli.sh - It would be nice to have a helper script for starting zkCli packaged in so we could include it's usage in the docs."}, {"q": "What updates or decisions were made in the discussion?", "a": "This patch adds the start script. It does not include jline support since we don't have jline right now. When we add jline we should add it for all interactive scripts."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-166", "title": "Create a tool to jump JMX data to a csv file to help build out performance tests", "status": "Resolved", "priority": "Major", "reporter": "Jay Kreps", "assignee": "Jay Kreps", "labels": [], "created": "2011-10-22T20:58:08.000+0000", "updated": "2011-10-24T18:52:25.000+0000", "description": "In order to get sane performance stats we need to be able to integrate the values we keep in JMX. To enable this it would be nice to have a generic tool that dumped JMX stats to a csv file. We could use this against the producer, consumer, and broker to collect kafka metrics while the tests were running.\n\n", "comments": ["Added draft patch.\n\njkreps-mn:kafka-trunk jkreps$ bin/kafka-run-class.sh kafka.tools.JmxTool --help\nOption                                  Description                            \n------                                  -----------                            \n--help                                  Print usage information.               \n--jmx-url <service-url>                 The url to connect to to poll JMX      \n                                          data. See http://http://download.    \n                                          oracle.                              \n                                          com/javase/7/docs/api/javax/management/remote/JMXServiceURL.\n                                          html for details. (default: service: \n                                          jmx:rmi:///jndi/rmi://:9999/jmxrmi)  \n--object-name <name>                    A JMX object name to query. If no      \n                                          objects are specified all objects    \n                                          will be queried.                     \n--reporting-interval <Integer: ms>      Interval in MS with which to poll jmx  \n                                          stats. (default: 5000) \n\nExample:\njkreps-mn:kafka-trunk jkreps$ bin/kafka-run-class.sh kafka.tools.JmxTool --object-name 'kafka:*' --reporting-interval 1000\n\"time\", \"kafka:type=kafka.KafkaLog4j:priority\", \"kafka:type=kafka.SocketServerStats:NumFetchRequests\", \"kafka:type=kafka.SocketServerStats:NumProduceRequests\", \"kafka:type=kafka.SocketServerStats:FetchRequestsPerSecond\", \"kafka:type=kafka.SocketServerStats:TotalBytesRead\", \"kafka:type=kafka.KafkaLog4j:appender=stdout\", \"kafka:type=kafka.SocketServerStats:BytesWrittenPerSecond\", \"kafka:type=kafka.SocketServerStats:AvgProduceRequestMs\", \"kafka:type=kafka.KafkaLog4j:name\", \"kafka:type=kafka.SocketServerStats:MaxFetchRequestMs\", \"kafka:type=kafka.SocketServerStats:BytesReadPerSecond\", \"kafka:type=kafka.SocketServerStats:TotalProduceRequestMs\", \"kafka:type=kafka.SocketServerStats:ProduceRequestsPerSecond\", \"kafka:type=kafka.SocketServerStats:MaxProduceRequestMs\", \"kafka:type=kafka.SocketServerStats:TotalFetchRequestMs\", \"kafka:type=kafka.SocketServerStats:TotalBytesWritten\", \"kafka:type=kafka.SocketServerStats:AvgFetchRequestMs\"\n1319317539516, INFO, 0, 0, -0.0, 0, log4j:appender=stdout, 0.0, 0.0, root, 0.0, 0.0, 0, -0.0, 0.0, 0, 0, 0.0\n1319317540523, INFO, 0, 0, -0.0, 0, log4j:appender=stdout, 0.0, 0.0, root, 0.0, 0.0, 0, -0.0, 0.0, 0, 0, 0.0\n1319317541523, INFO, 0, 0, -0.0, 0, log4j:appender=stdout, 0.0, 0.0, root, 0.0, 0.0, 0, -0.0, 0.0, 0, 0, 0.0\n1319317542522, INFO, 0, 0, -0.0, 0, log4j:appender=stdout, 0.0, 0.0, root, 0.0, 0.0, 0, -0.0, 0.0, 0, 0, 0.0\n1319317543521, INFO, 0, 0, -0.0, 0, log4j:appender=stdout, 0.0, 0.0, root, 0.0, 0.0, 0, -0.0, 0.0, 0, 0, 0.0\n...", "1. If object-name is not specified, should we include all jmx beans?\n2. Maybe we should order the output by bean names.\n3. The description of jmx-url has double http: in it. ", "Coda Hale's metrics package has built in support for export:\n\nhttps://github.com/codahale/metrics/commit/fce41d75046129a72e3582ae24ac698812d0fe53\n\nSince we will inevitable want more than just average throughput (such as percentiles for latency to judge jitter).  We might want to consider switching to that.", "Yeah, I am not opposed to moving to that metrics package if it doesn't bring in a bunch of dependencies.\n\nI think that is a separate issue, though. Right now I am just trying to support scripting up performance tests using our existing jmx metrics. The goal is to be able to dump out perf statistics in CSV for some post analysis and graphs. I wrote a strawman wiki on the goal here: \n  https://cwiki.apache.org/confluence/display/KAFKA/Performance+testing\n\nThe goal is be able to do nightly performance and integration runs. Neha had done the majority of the work so far, and I was just helping to gather more stats and do the R graphs.", "Updated version of the patch. Allows date formatting for the time stamp.\n\nJun--I implemented your idea to sort the columns. I am leaving time as the first column always, though, for readability. I also fixed the doc string for --jmx-url. The default was always to query all jmx beans, though the formatting for some of the more complex java.lang beans is a little crazy.", "Minor things -\n\nIs it HH:mm:ss:SSS or HH:mm:ss.SSS ?\nAlso, can we take the TODO off since the date formatting is in ?\n\n+1. Good to see this patch.", "Yeah, I can change the default to use a dot, I think that is prolly more standard. Good point about the TODO."], "derived": {"summary": "In order to get sane performance stats we need to be able to integrate the values we keep in JMX. To enable this it would be nice to have a generic tool that dumped JMX stats to a csv file.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "Create a tool to jump JMX data to a csv file to help build out performance tests - In order to get sane performance stats we need to be able to integrate the values we keep in JMX. To enable this it would be nice to have a generic tool that dumped JMX stats to a csv file."}, {"q": "What updates or decisions were made in the discussion?", "a": "Yeah, I can change the default to use a dot, I think that is prolly more standard. Good point about the TODO."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-167", "title": "Move partition assignment to the broker", "status": "Resolved", "priority": "Major", "reporter": "Jay Kreps", "assignee": null, "labels": [], "created": "2011-10-24T17:27:02.000+0000", "updated": "2014-07-12T19:56:16.000+0000", "description": "Currently partitions are assigned to consumers for consumption via a co-ordination algorithm by the consumers. This means there is effectively no master broker node, which is nice in a way, but makes the consumer logic complex to implement. It would be good to move the co-ordination to the brokers. This would make implementation of a consumer client much much easier, since only minimal zk interaction would be required. There are a number of details that would  have to be worked out to finalize the protocol. Discussion is here:\n\nhttp://mail-archives.apache.org/mod_mbox/incubator-kafka-dev/201109.mbox/%3cCAFbh0Q2ADjXcbCYqGh8TB4jXBAs3Hq5RLxk9o9w76we-bWanVw@mail.gmail.com%3e", "comments": ["Hi,\n\nSo I would suggest this: for each consumer group, one broker can be elected as partition/replica assignment HA coordinator using a standard election mechanism on zookeeper already available in Curator.\n\nI think it is nicer to distribute as much as possible, and, while recognizing that the algorithm is more deterministic, stable and convergent when using a centralized coordinator, there is no reason to pick the same coordinator for different consumer groups.\n\nApart from the mail discussion thread, can you please indicate any discussion on design, implementation, so that I can try to pick a small part and code something for it?\n\nThanks,\nNicu Marasoiu", "This is being done (finally) in the new consumer implementation."], "derived": {"summary": "Currently partitions are assigned to consumers for consumption via a co-ordination algorithm by the consumers. This means there is effectively no master broker node, which is nice in a way, but makes the consumer logic complex to implement.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Move partition assignment to the broker - Currently partitions are assigned to consumers for consumption via a co-ordination algorithm by the consumers. This means there is effectively no master broker node, which is nice in a way, but makes the consumer logic complex to implement."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is being done (finally) in the new consumer implementation."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-168", "title": "Support locality in consumer partition assignment algorithm", "status": "Resolved", "priority": "Major", "reporter": "Jay Kreps", "assignee": null, "labels": [], "created": "2011-10-24T17:30:09.000+0000", "updated": "2016-06-08T22:15:41.000+0000", "description": "There are some use-cases where it makes sense to co-locate brokers and consumer processes. In this case it would be nice to optimize the assignment of partitions to consumers so that the consumer preferentially consumes from the broker with which it is co-located.\n\nIf we are going to do KAFKA-167, moving the assignment to the broker, it would make sense to do that first so we only have to change the logic in one place.", "comments": ["Ideally we would also support rack locality in the partition assignment so that the brokers would have both a hostname and a rack name in their config. We would prefer partitions from an identical host, then from the same rack, then any partition.", "Today, In the rebalancing assignment, we don't use broker names. Instead we use broker_id:partition_id. So in addition to attaching the rack name to the broker name, we would need to use broker name (host-rack) in the rebalancing assignment algorithm.\n\nMaybe, even the consumer id should contain the rack name ?", "\nPutting in the discussion from the thread which started this JIRA for more context.\n\n@Inder\nUse-Case\n------------\n1. I have a topic T spread across two brokers (B1, B2)running on different machines, each having 2 partitions configured for T. Totally 4 partitions (1-0, 1-1, 2-0, 2-1)\n2. Consumer C1 is part of group g1 and is consuming from from B1, B2 for T\n3. Add a new consumer C2 part of g1\n\nThis is triggering a re balance across C1 & C2 and eventually C1 gets 1-0, 1-1 and C2 gets 2-0, 2-1.\nP.S. - B1, C1 are sharing the same machine, same is the case with B2,C2\n\nBehavior\n---------\nboth consumers are getting partitions which are hosted on the same boxes. Is this a coincidence or an optimization w.r.t locality of data and will always be applied.\n\n@Jun\nDuring rebalance, we simply sort all partitions and consumers by name and\ngive each consumer an even range of partitions. Since partitions on the same\nbroker sort together, they tend to be given out to the same consumer, as in\nthis case.\n\n@Inder\nSorted partitions grouped together might go to same consumer but it may not be going to the consumer where its local instead of NIO, if consumers are hosted on brokers.\n\nFor example :\nIn this case my partitions were like 1-0, 1-1 hosted on a machine named proudarmburn and 2-0, 2-1,  later on trainingarmburn so we got the localization as sorting followed the same behavior, however if my second machine name would had been leadergather then after sorting it would had gotten partitions starting with 1 series which are remote to it.\n\nSince we know the brokers hosting the partitions and consumers which are subscribed if we add affinity to broker while re balancing it could save quite a bit of nio.\n\nOf course I am not sure if its a recommended configuration to be able to have consumers share hardware with brokers. What do you guys think\n", "looking at the code of rebalance() in ZooKeeperConsumerConnector.scala\nto figure if i could get Broker Names of partitions of a topic and then consumer names which could be grouped with brokers to leverage locality.\n\nLooks like we store brokerid for broker and consumerid for consumer which may not be same even if we are hosting them on the same box.\nThis makes life tough. Any thoughts.", "It's possible to specify the consumer id in ConsumerConfig. Not sure if that's the right approach though.", "The broker must have an abstract id not tied to host:port, to make it possible to change the hostname or port or move the data. The consumer id could contain host (there is no port), though I don't know if this is a good idea.\n\nWhat I had in mind was just adding this metadata. E.g. programmatically getting the hostname and adding a rack config parameter and adding this in zk as metadata.", "Jay i agree with you that getting the hostname programatically will abstract out things. I'll do some research.", "Jay, I'd really appreciate knowing why you closed this issue. I've read everything I can find on the web and watched loads of your videos, but still can't figure out why co-locating your processing next to the data it's going to process isn't a more widely requested feature. Is there some fundamental reason why this is a bad idea, is it just not technically possible due to Kafka's architecture, or is still something you'd still accept a patch for?"], "derived": {"summary": "There are some use-cases where it makes sense to co-locate brokers and consumer processes. In this case it would be nice to optimize the assignment of partitions to consumers so that the consumer preferentially consumes from the broker with which it is co-located.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support locality in consumer partition assignment algorithm - There are some use-cases where it makes sense to co-locate brokers and consumer processes. In this case it would be nice to optimize the assignment of partitions to consumers so that the consumer preferentially consumes from the broker with which it is co-located."}, {"q": "What updates or decisions were made in the discussion?", "a": "Jay, I'd really appreciate knowing why you closed this issue. I've read everything I can find on the web and watched loads of your videos, but still can't figure out why co-locating your processing next to the data it's going to process isn't a more widely requested feature. Is there some fundamental reason why this is a bad idea, is it just not technically possible due to Kafka's architecture, or is still something you'd still accept a patch for?"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-169", "title": "Layering violations in Kafka code", "status": "Resolved", "priority": "Minor", "reporter": "Jay Kreps", "assignee": null, "labels": [], "created": "2011-10-25T04:31:47.000+0000", "updated": "2014-07-17T21:49:27.000+0000", "description": "I am noticing  lot of layering violations creeping into the code.\n\nFor example the log implementation depends on zookeeper code now, the network server depends on the kafka api, etc. This stuff is messy and makes it hard to test or reason about the pieces in isolation.\n\nI have run a quick analysis on the imports to look at problems and there are a few. Let's try to keep this graph in good shape and think about the layering in the code.", "comments": ["Here is a script to create a graphviz graph of the package dependencies. I also attached the output graph in svg form. This graph is slightly filtered (I removed things like utils and javaapi which are natural integration points).\n\nHere is the command I used to generate this:\ncd kafka-trunk/core/src/main/scala\negrep -R '(import|package)' . | python ~/Desktop/draw_deps.py kafka | grep -v javaapi | grep -v utils | grep -v kafka.tools > ~/Desktop/kafka_deps.dot; dot -o ~/Desktop/kafka_deps.svg -Tsvg ~/Desktop/kafka_deps.dot"], "derived": {"summary": "I am noticing  lot of layering violations creeping into the code. For example the log implementation depends on zookeeper code now, the network server depends on the kafka api, etc.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Layering violations in Kafka code - I am noticing  lot of layering violations creeping into the code. For example the log implementation depends on zookeeper code now, the network server depends on the kafka api, etc."}, {"q": "What updates or decisions were made in the discussion?", "a": "Here is a script to create a graphviz graph of the package dependencies. I also attached the output graph in svg form. This graph is slightly filtered (I removed things like utils and javaapi which are natural integration points).\n\nHere is the command I used to generate this:\ncd kafka-trunk/core/src/main/scala\negrep -R '(import|package)' . | python ~/Desktop/draw_deps.py kafka | grep -v javaapi | grep -v utils | grep -v kafka.tools > ~/Desktop/kafka_deps.dot; dot -o ~/Desktop/kafka_deps.svg -Tsvg ~/Desktop/kafka_deps.dot"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-170", "title": "Support for non-blocking polling on multiple streams", "status": "Resolved", "priority": "Critical", "reporter": "Jay Kreps", "assignee": null, "labels": ["replication"], "created": "2011-10-26T06:08:58.000+0000", "updated": "2015-02-07T22:35:44.000+0000", "description": "Currently we provide a blocking iterator in the consumer. This is a good mechanism for consuming data from a single topic, but is limited as a mechanism for polling multiple streams.\n\nFor example if one wants to implement a non-blocking union across multiple streams this is hard to do because calls may block indefinitely. A similar situation arrises if trying to implement a streaming join of between two streams.\n\nI would propose two changes:\n1. Implement a next(timeout) interface on KafkaMessageStream. This will easily handle some simple cases with minimal change. This handles certain limited cases nicely and is easy to implement, but doesn't actually cover the two cases above.\n2. Add an interface to poll streams.\n\nI don't know the best approach for the later api, but it is important to get it right. One option would be to add a ConsumerConnector.drainTopics(\"topic1\", \"topic2\", ...) which blocks until there is at least one message and then returns a list of triples (topic, partition, message).", "comments": ["Hi - I solved this problem in the NodeJS client here: https://github.com/tagged/node-kafka\n\nYou may or may not like the approach - but at least you can see which solution I went with.  Node of course is an event based system so it's more natural to use callbacks which may not necessarily be appropriate for Java.", "1. KafkaMessageStream currently does have a timeout controlled by consumer.timeout.ms. If the next call on the iterator doesn't get any message within that time, an exception is thrown during the next call. This is probably not what you want since the iterator may never timeout if one topic has new messages constantly.\n\n2. Do we really need to return the triple? Do users care about the topic/partition that a messages comes from or do they just want to simply consumer messages from all topics? If they don't, we can probably implement a special fetcher that put messages from different topics into a shared in-memory queue for the end user to iterate. The interface for KafkaMessageStream may not need to be changed.\n\nTaylor, \n\nCould you elaborate your approach a bit more here?", "> Do users care about the topic/partition\n\nI think yes. If we allow users to provide an arbitrary partitioner, the partitioning may be meaningful through their pipeline.", "Hi Jun.\n\nMy approach works like this:\n\n1)  There is a low-level API that was created by Marcus Westin.  The low-level API contains a Client.js (the reader) and Producer.js (the writer).  Client.js contains the following APIs:\n  - connect(args)\n  - fetchTopic(topicName, callback)\n  - fetchOffsets(topicName, arg, callback)\n\n2)  I created the Consumer.js implementation which extends the low-level Client.js and provides 2 additional APIs:\n\n  - subscribeTopic(topicName, callback)\n  - unsubscribeTopic(topicName, callback)\n\nIt is simply the job of the Consumer.js to keep track of the topics that the caller has so far subscribed to, and whenever it polls for messages, it simply writes a request using the low-level API for every subscribed topic.  So, if for example the caller has subscribed to A, B, C, then the Consumer will call the low-level API and request A, B, C.  Note that order is not defined, but is important for the retreival.\n\nNext, Kafka will write responses for A, B, C, and the messages that arrive will then each be individually returned to the caller.  The callback allows for its args to contain a topic name, so the callback that is passed can be the same callback for all topics.  Once that is done, there is a configurable timeout.  If that timeout has passed, then the Consumer simply sends a poll request for all the subscribed topics again.  If the timeout has not passed, the Consumer waits for that timeout to pass and then polls for all subscribed topics.", "Taylor, \n\nI see. Yes, if you use the low-level API, you can implement this. However, there is other work you have to do. It would be good if there is a simple way to extend the high-level API to do the same thing.", "Since this is proposing an API change, it will be good to think about this as part of 0.8", "I've been browsing for a non-blocking simple consumer library and I came across this issue. It so happens that I had modified the high-level consumer to use callbacks instead of blocking queues. I don't know if using non-blocking polling would be a better approach but here is a code example for those who are interested: \n\n{code:java}\nval partitionSize = 4\nval topicCountMap = new util.HashMap[EventHandler[String, String], Integer]()\nval consumer = Consumer.create(...)\nval cb = (messageHolder: MessageAndMetadata[String, String]) => {\n   println(messageHolder.message)\n}\n\ntopicCountMap.put(new EventHandler(\"MyTopic\", cb), partitionSize)\nconsumer.createMessageStreams(topicCountMap, new StringDecoder(), new StringDecoder())\n{code}\n\nThe code is part of the Kafka Web Console project and it can be found here: https://github.com/claudemamo/kafka-web-console/tree/master/app/kafka.", "Claude, in 0.9 we are going to rewrite our consumer client and it will use non-blocking poll APIs, do you want to check its design and see if it satisfies your use case?\n\nhttps://cwiki.apache.org/confluence/display/KAFKA/Kafka+0.9+Consumer+Rewrite+Design", "This is being solved in the new consumer api."], "derived": {"summary": "Currently we provide a blocking iterator in the consumer. This is a good mechanism for consuming data from a single topic, but is limited as a mechanism for polling multiple streams.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support for non-blocking polling on multiple streams - Currently we provide a blocking iterator in the consumer. This is a good mechanism for consuming data from a single topic, but is limited as a mechanism for polling multiple streams."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is being solved in the new consumer api."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-171", "title": "Kafka producer should do a single write to send message sets", "status": "Resolved", "priority": "Major", "reporter": "Jay Kreps", "assignee": "Jay Kreps", "labels": [], "created": "2011-10-26T17:52:10.000+0000", "updated": "2011-11-23T07:30:02.000+0000", "description": "From email thread: \nhttp://mail-archives.apache.org/mod_mbox/incubator-kafka-dev/201110.mbox/%3cCAFbh0Q1PYUj32thBaYQ29E6J4wT_mrG5SuUsfdeGWj6rmEx9Gw@mail.gmail.com%3e\n> Before sending an actual message, kafka producer do send a (control) message of 4 bytes to the server. Kafka producer always does this action before send some message to the server.\n\nI think this is because in BoundedByteBufferSend.scala we do essentially\n channel.write(sizeBuffer)\n channel.write(dataBuffer)\n\nThe correct solution is to use vector I/O and instead do\n channel.write(Array(sizeBuffer, dataBuffer))", "comments": ["Attached is a draft patch which turns the request into a single write. This is just a draft if this actually improves performance we should change Receive to use ScatteringByteChannel for consistency and also clean up a few more files with the same trick.\n\nOn my mac laptop I do see a change in tcpdump which seems to eliminate the 4 byte send. However I don't see any positive result in performance for synchronous single-threaded sends of 10 byte messages (which should be the worst case for this). I think this may just be because I am testing over localhost.\n\nHere are the details on the results I have:\n\nTRUNK:\njkreps-mn:kafka-git jkreps$ sudo tcpdump -i lo0 port 9093 \ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on lo0, link-type NULL (BSD loopback), capture size 96 bytes\n10:32:30.128938 IP jkreps-mn.linkedin.biz.56953 > jkreps-mn.linkedin.biz.9093: S 323648854:323648854(0) win 65535 <mss 16344,nop,wscale 3,nop,nop,timestamp 377871870 0,sackOK,eol>\n10:32:30.129004 IP jkreps-mn.linkedin.biz.9093 > jkreps-mn.linkedin.biz.56953: S 526915069:526915069(0) ack 323648855 win 65535 <mss 16344,nop,wscale 3,nop,nop,timestamp 377871870 377871870,sackOK,eol>\n10:32:30.129013 IP jkreps-mn.linkedin.biz.56953 > jkreps-mn.linkedin.biz.9093: . ack 1 win 65535 <nop,nop,timestamp 377871870 377871870>\n10:32:30.129022 IP jkreps-mn.linkedin.biz.9093 > jkreps-mn.linkedin.biz.56953: . ack 1 win 65535 <nop,nop,timestamp 377871870 377871870>\n10:32:30.129306 IP jkreps-mn.linkedin.biz.56953 > jkreps-mn.linkedin.biz.9093: P 1:5(4) ack 1 win 65535 <nop,nop,timestamp 377871870 377871870>\n10:32:30.129319 IP jkreps-mn.linkedin.biz.9093 > jkreps-mn.linkedin.biz.56953: . ack 5 win 65535 <nop,nop,timestamp 377871870 377871870>\n10:32:30.129339 IP jkreps-mn.linkedin.biz.56953 > jkreps-mn.linkedin.biz.9093: P 5:41(36) ack 1 win 65535 <nop,nop,timestamp 377871870 377871870>\n10:32:30.129350 IP jkreps-mn.linkedin.biz.9093 > jkreps-mn.linkedin.biz.56953: . ack 41 win 65535 <nop,nop,timestamp 377871870 377871870>\n10:32:30.151892 IP jkreps-mn.linkedin.biz.56953 > jkreps-mn.linkedin.biz.9093: F 41:41(0) ack 1 win 65535 <nop,nop,timestamp 377871870 377871870>\n10:32:30.151938 IP jkreps-mn.linkedin.biz.9093 > jkreps-mn.linkedin.biz.56953: . ack 42 win 65535 <nop,nop,timestamp 377871870 377871870>\n10:32:30.151946 IP jkreps-mn.linkedin.biz.56953 > jkreps-mn.linkedin.biz.9093: . ack 1 win 65535 <nop,nop,timestamp 377871870 377871870>\n10:32:30.152554 IP jkreps-mn.linkedin.biz.9093 > jkreps-mn.linkedin.biz.56953: F 1:1(0) ack 42 win 65535 <nop,nop,timestamp 377871870 377871870>\n10:32:30.152571 IP jkreps-mn.linkedin.biz.56953 > jkreps-mn.linkedin.biz.9093: . ack 2 win 65535 <nop,nop,timestamp 377871870 377871870>\n\nPATCHED:\njkreps-mn:kafka-git jkreps$ sudo tcpdump -i lo0 port 9093 \ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on lo0, link-type NULL (BSD loopback), capture size 96 bytes\n10:35:40.637220 IP jkreps-mn.linkedin.biz.56993 > jkreps-mn.linkedin.biz.9093: S 1456363353:1456363353(0) win 65535 <mss 16344,nop,wscale 3,nop,nop,timestamp 377873772 0,sackOK,eol>\n10:35:40.637287 IP jkreps-mn.linkedin.biz.9093 > jkreps-mn.linkedin.biz.56993: S 1260172914:1260172914(0) ack 1456363354 win 65535 <mss 16344,nop,wscale 3,nop,nop,timestamp 377873772 377873772,sackOK,eol>\n10:35:40.637296 IP jkreps-mn.linkedin.biz.56993 > jkreps-mn.linkedin.biz.9093: . ack 1 win 65535 <nop,nop,timestamp 377873772 377873772>\n10:35:40.637306 IP jkreps-mn.linkedin.biz.9093 > jkreps-mn.linkedin.biz.56993: . ack 1 win 65535 <nop,nop,timestamp 377873772 377873772>\n10:35:40.657848 IP jkreps-mn.linkedin.biz.56993 > jkreps-mn.linkedin.biz.9093: P 1:41(40) ack 1 win 65535 <nop,nop,timestamp 377873773 377873772>\n10:35:40.657886 IP jkreps-mn.linkedin.biz.9093 > jkreps-mn.linkedin.biz.56993: . ack 41 win 65535 <nop,nop,timestamp 377873773 377873773>\n10:35:40.711399 IP jkreps-mn.linkedin.biz.56993 > jkreps-mn.linkedin.biz.9093: F 41:41(0) ack 1 win 65535 <nop,nop,timestamp 377873773 377873773>\n10:35:40.711430 IP jkreps-mn.linkedin.biz.9093 > jkreps-mn.linkedin.biz.56993: . ack 42 win 65535 <nop,nop,timestamp 377873773 377873773>\n10:35:40.711437 IP jkreps-mn.linkedin.biz.56993 > jkreps-mn.linkedin.biz.9093: . ack 1 win 65535 <nop,nop,timestamp 377873773 377873773>\n10:35:40.762640 IP jkreps-mn.linkedin.biz.9093 > jkreps-mn.linkedin.biz.56993: F 1:1(0) ack 42 win 65535 <nop,nop,timestamp 377873774 377873773>\n10:35:40.762678 IP jkreps-mn.linkedin.biz.56993 > jkreps-mn.linkedin.biz.9093: . ack 2 win 65535 <nop,nop,timestamp 377873774 377873774>\n\nTRUNK:\nbin/kafka-producer-perf-test.sh --topic test --brokerinfo zk.connect=localhost:2181 --messages 300000 --message-size 10 --batch-size 1 --threads 1\n...\n[2011-10-26 10:33:58,458] INFO Total Num Messages: 300000 bytes: 3000000 in 13.636 secs (kafka.tools.ProducerPerformance$)\n[2011-10-26 10:33:58,459] INFO Messages/sec: 22000.5867 (kafka.tools.ProducerPerformance$)\n[2011-10-26 10:33:58,459] INFO MB/sec: 0.2098 (kafka.tools.ProducerPerformance$)\n\nPATCHED:\njkreps-mn:kafka-git jkreps$ bin/kafka-producer-perf-test.sh --topic test --brokerinfo zk.connect=localhost:2181 --messages 300000 --message-size 10 --batch-size 1 --threads 1\n...\n[2011-10-26 10:38:03,965] INFO Total Num Messages: 300000 bytes: 3000000 in 13.254 secs (kafka.tools.ProducerPerformance$)\n[2011-10-26 10:38:03,965] INFO Messages/sec: 22634.6763 (kafka.tools.ProducerPerformance$)\n[2011-10-26 10:38:03,966] INFO MB/sec: 0.2159 (kafka.tools.ProducerPerformance$)\n\n", "This is a good change to make. A couple of comments -\n\n1. Since we are changing WritableByteChannel to GatheringByteChannel, it is better to change the return type of writeTo and writeCompletely to return long, instead of int. This will avoid the coercion to Int in BoundedByteBufferSend.scala.\n\n2. There are a couple of other places, where we do these double writes, e.g. OffsetArraySend, MessageSetSend etc.  We might as well fix those ? ", "Moving off localhost between my mac laptop and dev workstation (linux) I see similar results:\n\nTRUNK:\njkreps-mn:kafka-git jkreps$ bin/kafka-producer-perf-test.sh --topic test --brokerinfo zk.connect=jkreps-ld:2181 --messages 500000 --message-size 10 --batch-size 1 --threads 1\n[2011-10-26 11:59:51,795] INFO Total Num Messages: 500000 bytes: 5000000 in 13.046 secs (kafka.tools.ProducerPerformance$)\n[2011-10-26 11:59:51,795] INFO Messages/sec: 38325.9237 (kafka.tools.ProducerPerformance$)\n[2011-10-26 11:59:51,795] INFO MB/sec: 0.3655 (kafka.tools.ProducerPerformance$)\n\nPATCHED:\njkreps-mn:kafka-git jkreps$ bin/kafka-producer-perf-test.sh --topic test --brokerinfo zk.connect=jkreps-ld:2181 --messages 500000 --message-size 10 --batch-size 1 --threads 1\n[2011-10-26 11:58:42,335] INFO Total Num Messages: 500000 bytes: 5000000 in 13.125 secs (kafka.tools.ProducerPerformance$)\n[2011-10-26 11:58:42,335] INFO Messages/sec: 38095.2381 (kafka.tools.ProducerPerformance$)\n[2011-10-26 11:58:42,335] INFO MB/sec: 0.3633 (kafka.tools.ProducerPerformance$)", "Even if this doesn't measurably improve node to node performance (and I'm not sure we should expect it to since we don't have to wait for an ACK to send the next packet), isn't it definitely making life better for network engineer?", "Yes, I think we should do it. My concern was just that I might be misunderstanding tcpdump or something since I find this a little counter-intuitive..", "Okay this patch completes the conversion to GatheringByteChannel. I recommend we take this even though there doesn't seem to be real perf difference just because the network profile is better.", "Since we are changing WritableByteChannel to GatheringByteChannel, would it be better to change the return type of writeTo and writeCompletely to return long, instead of int. This will avoid the coercion to Int in BoundedByteBufferSend.scala.\n\n", "I actually don't think we should make the writeTo method return a long since we use 4 byte ints in the protocol to delimit size of request so we really can't take a buffer > Int.MaxValue. I added a check on this to avoid an overflow if the size is > Int.MaxValue - 4, which is unlikely but possible.", "nnarkhed-mn:kafka-171 nnarkhed$ find . -name \"*scala\" -exec grep -Hi \"asInstanceOf\\[Int\\]\" {} \\;\n./core/src/main/scala/kafka/api/OffsetRequest.scala:  header.putInt(size.asInstanceOf[Int] + 2)\n./core/src/main/scala/kafka/api/ProducerRequest.scala:  def sizeInBytes(): Int = 2 + topic.length + 4 + 4 + messages.sizeInBytes.asInstanceOf[Int]\n./core/src/main/scala/kafka/network/BoundedByteBufferSend.scala:    written.asInstanceOf[Int]\n./core/src/main/scala/kafka/producer/SyncProducer.scala:    val setSize = messages.sizeInBytes.asInstanceOf[Int]\n./core/src/main/scala/kafka/server/MessageSetSend.scala:  header.putInt(size.asInstanceOf[Int] + 2)\n./core/src/main/scala/kafka/server/MessageSetSend.scala:      written += fileBytesSent.asInstanceOf[Int]\n./core/src/main/scala/kafka/server/MessageSetSend.scala:  def sendSize: Int = size.asInstanceOf[Int] + header.capacity\n./core/src/main/scala/kafka/utils/Utils.scala:    buffer.putInt((value & 0xffffffffL).asInstanceOf[Int])\n./core/src/main/scala/kafka/utils/Utils.scala:    buffer.putInt(index, (value & 0xffffffffL).asInstanceOf[Int])\n\nIts not great that we have so many places where we need to worry about coercion, but we can clean this up the next time we change the on wire protocol.\n\n+1 on the latest patch", "MessageSet has a couple of unused imports. Other than that, the patch looks good. ", "Cool, will clean up imports before checking in. I am going to hold off on this until after 0.7 goes out.", "You can check it into trunk. 0.7 is going off its own branch", "Committed."], "derived": {"summary": "From email thread: \nhttp://mail-archives. apache.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Kafka producer should do a single write to send message sets - From email thread: \nhttp://mail-archives. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "You can check it into trunk. 0.7 is going off its own branch"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-172", "title": "The existing perf tools are buggy", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "labels": [], "created": "2011-10-26T17:53:06.000+0000", "updated": "2013-06-14T04:02:05.000+0000", "description": "The existing perf tools - ProducerPerformance.scala, ConsumerPerformance.scala and SimpleConsumerPerformance.scala are buggy. It will be good to -\n\n1. move them to a perf directory, along with helper scripts\n2. fix the bugs, so that they measure throughput correctly", "comments": ["What bugs are you seeing?"], "derived": {"summary": "The existing perf tools - ProducerPerformance. scala, ConsumerPerformance.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "The existing perf tools are buggy - The existing perf tools - ProducerPerformance. scala, ConsumerPerformance."}, {"q": "What updates or decisions were made in the discussion?", "a": "What bugs are you seeing?"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-173", "title": "Support encoding for non ascii characters", "status": "Closed", "priority": "Minor", "reporter": "Alejandro", "assignee": null, "labels": [], "created": "2011-10-26T22:47:23.000+0000", "updated": "2013-04-04T20:43:48.000+0000", "description": "See attached patch.\n", "comments": ["Anyone who can review this?", "Ping. Can anybody help review this ?", "Old bug. Client libraries, such as this Ruby library, are no longer being maintained in the main project.\n\nPlease close. Won't Fix."], "derived": {"summary": "See attached patch.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support encoding for non ascii characters - See attached patch."}, {"q": "What updates or decisions were made in the discussion?", "a": "Old bug. Client libraries, such as this Ruby library, are no longer being maintained in the main project.\n\nPlease close. Won't Fix."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-174", "title": "Add performance suite for Kafka", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": null, "labels": ["replication"], "created": "2011-10-26T23:15:38.000+0000", "updated": "2016-05-24T20:53:16.000+0000", "description": "This is a placeholder JIRA for adding a perf suite to Kafka. The high level proposal is here -\nhttps://cwiki.apache.org/confluence/display/KAFKA/Performance+testing\n\nThere will be more JIRAs covering smaller tasks to fully implement this. They will be linked to this JIRA. ", "comments": [" - If you are willing to ask OS tools. iostat has some relevant stuff (such as await)\n\n - Coda Hale's metrics have some nice tools for latency, I'd be happy to take that as a sub task.", "Chris, following up on this, I need to redo the JMX for the network server for KAFKA-202, let's move the discussion on internal metrics and JMX to KAFKA-203.", "It will be good to think about performance tests relevant to replication", "was done ages ago..."], "derived": {"summary": "This is a placeholder JIRA for adding a perf suite to Kafka. The high level proposal is here -\nhttps://cwiki.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add performance suite for Kafka - This is a placeholder JIRA for adding a perf suite to Kafka. The high level proposal is here -\nhttps://cwiki."}, {"q": "What updates or decisions were made in the discussion?", "a": "was done ages ago..."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-175", "title": "Add helper scripts to wrap the current perf tools", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "labels": [], "created": "2011-10-26T23:17:31.000+0000", "updated": "2016-05-24T20:52:58.000+0000", "description": "We have 3 useful tools to run producer and consumer perf tests - \n\nProducerPerformance.scala, SimpleConsumerPerformance.scala and ConsumerPerformance.scala.\n\nThese tests expose several options that allows you to define the load for each perf run. It will be good to expose some helper scripts that will cover some single node perf testing scenarios.", "comments": ["This is a draft version for the helper scripts. By no means, is this a full set of single node perf scenarios. My feeling is that the list will evolve. We can start with some basic wrapper scripts and add more scenarios as other JIRAs. \n\nThe following scripts are included in the patch -\n\n1. Plot producer throughput over batch size\n2. Plot producer throughput over time\n3. Plot simple and zk consumer throughput over fetch size\n\nKeep in mind that this JIRA depends on KAFKA-176 and hence is an incremental patch on top of it.", "Since the scripts need working perf tools", "A few comments:\n1. Testing on batch size is a bit confusing since how big a batch is really depends on message size. What really matters is the size of each ProduceRequest. So, maybe we should choose the request size as the variable instead.\n2. Can we merge producer-throughput-over-batch-size.sh and zk-producer-throughput-over-batch-size.sh into a single script and add an option to allow the caller to choose either broker list or zk connect?\n3. Not sure how producer-throughput-over-data-accumulated.sh is different from producer-throughput-over-batch-size.sh. They both seem to vary batch sizes.", "The previous patch had some malformed lines. This patch should apply cleanly. Note to self: Switch to git !", "Previous comments 1 and 3 are not addressed.\n\n4. consumer-throughput-over-fetch-size.sh calls kafka-simple-consumer-perf-test.sh inconsistently, some using zk.connect and some using broker.list, some with fetch-size and some without. Also, instead of duplicating cals tol kafka-simple-consumer-perf-test.sh, can we use a for loop instead? Ditto for all other tests.\n\n\n\n", "This was done ages ago..."], "derived": {"summary": "We have 3 useful tools to run producer and consumer perf tests - \n\nProducerPerformance. scala, SimpleConsumerPerformance.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add helper scripts to wrap the current perf tools - We have 3 useful tools to run producer and consumer perf tests - \n\nProducerPerformance. scala, SimpleConsumerPerformance."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was done ages ago..."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-176", "title": "Fix existing perf tools", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "labels": [], "created": "2011-10-27T00:32:29.000+0000", "updated": "2013-05-02T02:29:45.000+0000", "description": "The existing perf tools - ProducerPerformance.scala, ConsumerPerformance.scala and SimpleConsumerPerformance.scala are slightly buggy. It will be good to -\n\n1. move them to a perf directory from the existing kafka/tools location\n2. fix the bugs, so that they measure throughput correctly\n", "comments": ["This patch fixes a bunch of problems in the perf tools. I'll try to explain the changes at a high level, since I don't remember the exact list of little bugs that existed -\n\n1. Refactoring ProducerPerformance to include just one code path for both sync and async producer\n2. Fix bugs in ProducerPerformance to fix the throughput calculation and the logic for having multiple threads\n3. Fix various bugs in the consumer performance tools\n4. Fix the output of the tools to include useful info in the csv format\n5. Move all of the above to a perf sub project", "ProducerPerformance:\n1. The following code seems to be used to enable/disable the header. Is it better to control that in config instead using debug logging (so that it's not mixed with other debug logging)? Also, the header info is not complete, missing the first few fields. The header is probably useful for stats printed out periodically in each thread. So it should be printed out early, if enabled.  \n    if(logger.isDebugEnabled)\n        logger.debug(\"message size, batch size, total data sent in MB, MB/sec, total data sent in nMsg, nMsg/sec\")\n2. Whether avgPerf is specified or not, the user is probably always interested in the aggregated numbers across all threads. How about we always print it out and have a config option \"showDetails\" to enable/disable periodic reporting in each thread. Ditto in other perf tools.\n3. ProducerThread has multiple bugs:\n  3.1. Variable-sized messages are not picked up in Async mode\n  3.2. In Sync mode, messageSet needs to be reset for each batch, if messages are of variable size (seems to be an existing bug)\n4. It's better not to duplicate the following code. Defining it once in a static method seems better. \n            println((\"%s, %d, %d, %d, %d, %.2f, %.4f, %d, %.4f\").format(formattedReportTime, config.compressionCodec.codec,\n              threadId, config.messageSize, config.batchSize, (bytesSent*1.0)/(1024 * 1024), mbPerSec, nSends, numMessagesPerSec))\n\n", "This is a draft patch, so things that I wasn't sure off, are left incomplete\n\n1. The header being in debug logging is clearly not ideal. There are 2 ways of exposing the header - \na. In the code\nb. In the scripts\n\nSince some graphing tools might need the headers in a certain format, I initially thought its better left in the scripts (see KAFKA-175). However, since the code controls the display of the data, it might as well be left there.\n\n2. The --avg option already controls that. We could probably rename that to -aggregate instead ?\n\n3.1 & 3.2 The reason I haven't touched the variable size message path is because I don't think it is well implemented. When selecting that option, the throughput actually reduces due to the way it is implemented. Not sure having a variable message size option actually helps conclude anything in this perf test ?\n\n4. Good point. \n\nI will update the patch, once we have a better idea about the questions above.\n\n", "1. It can be in the code. Maybe we just need a command line option to control whether to display header or not?\n\n2. I am suggesting that we alway show aggregated output. So there won't be a -aggregate option. Instead, have a --showDetails option to enable/disable detailed stats in each thread, probably with default set to false.\n\n3. Some system tests can use a producer that generates variable-sized random messages. Instead of having another producer tool, it would be good if we can just allow such option in this tool.\n", "1. I think we can add a command line option --header that will control the display of the header\n\n2. Good suggestion. I think what you are saying is that aggregate stats should be the default, and instead of having --aggregate option, we should have --showDetailedStats option. I think that is a good idea. \n\n3. OK. I'll take a stab at that, and upload a new patch.\n\n4. KAFKA-175 will need to be updated as well. It would be great if people can look at that too.\n", "tryCleanupZookeeper looks cut-and-pasted from place to place. It shows up in ConsoleConsumer.scala, ReplayLogProducer.scala, and ConsumerPerformance.scala. We should not do that. Can we make some kind of utility function for that?\n\nAlso there is a PerfConfig class, which is a great idea as a way to normalize some of the config options we are using between all the tools. But it looks like the class is just duplicated between the tools. Can this be shared? \n\nI would like to cleanup the scripts used to run these things so that we get rid of all the silly ancient ones (*simple-perf-test.sh and *shell.sh), but I think I will open a seperate ticket for that since it is unrelated to your changes.", "1. tryCleanupZookeeper can definitely be a Util. \n2. This is also a good suggestion. There is definitely a lot of overlap amongst PerfConfig in all the perf tests.\n3. For scripts, please can you also take a look at KAFKA-175 ?\n\nThanks. I will incorporate these suggestions and submit an updated patch tomorrow. ", "1. Header moved to perf tools\n2. Default is aggregated stats. Added a show-detailed-stats option for finer grained stats per reporting-interval\n3. Fixed the variable size message bugs in producer performance tool\n4. Abstracted out common perf configs in a top-level PerfConfig class.\n5. Moved tryCleanupZookeeper to Utils\n", "+1", "Thanks. Just committed this. I'm thinking there will be more improvements to these perf tools as part of KAFKA-175"], "derived": {"summary": "The existing perf tools - ProducerPerformance. scala, ConsumerPerformance.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Fix existing perf tools - The existing perf tools - ProducerPerformance. scala, ConsumerPerformance."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks. Just committed this. I'm thinking there will be more improvements to these perf tools as part of KAFKA-175"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-177", "title": "Remove the clojure client until it is correctly implemented and refactored", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "labels": [], "created": "2011-10-28T19:54:36.000+0000", "updated": "2011-10-30T21:18:30.000+0000", "description": "The related conversation on the mailing list is here - http://mail-archives.apache.org/mod_mbox/incubator-kafka-dev/201110.mbox/ajax/%3CCAFbh0Q0xMBWXK0gnKZyKkwwUBjoyxJ=QcR1hu9V55qh7dE8JdA@mail.gmail.com%3E\n\nThe clojure client can be brought back when it is in better shape and ownership.", "comments": ["This patch deletes the clojure client code", "+1", "Just committed this."], "derived": {"summary": "The related conversation on the mailing list is here - http://mail-archives. apache.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Remove the clojure client until it is correctly implemented and refactored - The related conversation on the mailing list is here - http://mail-archives. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "Just committed this."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-178", "title": "Simplify the brokerinfo argument to the producer-perf-test.sh", "status": "Resolved", "priority": "Major", "reporter": "Jay Kreps", "assignee": null, "labels": [], "created": "2011-10-31T05:26:35.000+0000", "updated": "2015-02-07T23:47:12.000+0000", "description": "Currently:\njkreps-mn:kafka-git jkreps$ bin/kafka-producer-perf-test.sh \nMissing required argument \"[brokerinfo]\"\nOption                                  Description                            \n------                                  -----------                            \n...        \n--brokerinfo <broker.list=brokerid:     REQUIRED: broker info (either from     \n  hostname:port or zk.connect=host:       zookeeper or a list.                 \n  port>      \n...\n\nThis is kind of confusing and doesn't match the other scripts. I would like to change it to \n  --zookeeper zk_connect_string\n  --broker-list id1:host1:port1,id2:host2:port2,...\nand require that one of these be specified.", "comments": [], "derived": {"summary": "Currently:\njkreps-mn:kafka-git jkreps$ bin/kafka-producer-perf-test. sh \nMissing required argument \"[brokerinfo]\"\nOption                                  Description                            \n------                                  -----------.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Simplify the brokerinfo argument to the producer-perf-test.sh - Currently:\njkreps-mn:kafka-git jkreps$ bin/kafka-producer-perf-test. sh \nMissing required argument \"[brokerinfo]\"\nOption                                  Description                            \n------                                  -----------."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-179", "title": "Log files always touched when broker is bounced", "status": "Closed", "priority": "Minor", "reporter": "Joel Jacob Koshy", "assignee": "Raul Castro Fernandez", "labels": ["newbie"], "created": "2011-10-31T22:15:53.000+0000", "updated": "2014-06-12T22:02:28.000+0000", "description": "It looks like the latest log segment is always touched when the broker upon start-up regardless of whether it has corrupt data or not, which fudges the segment's mtime. Minor issue, but I found it a bit misleading when trying to verify a log cleanup setting in production. I think it should be as simple as adding a guard in FileMessageSet's recover method to skip truncate if validUpTo == the length of the segment. Will test this later.\n", "comments": ["The function recoverLog() in Log.scala checks whether the log has the CleanShutdownMarker. If it exists, then it returns without touching any MessageSet (lines 173-176). Does the description of the JIRA refers to this function?", "It might have been fixed in KAFKA-615. Maybe you can do a quick test to confirm and close this as resolved-implemented.", "Just tried it. It seems that it is already fixed. After creating a couple of topics and reinitialising the brokers a couple of times, the mtime of the log and index files does not change.", "Should I mark it as solved?", "Cool - thanks for checking."], "derived": {"summary": "It looks like the latest log segment is always touched when the broker upon start-up regardless of whether it has corrupt data or not, which fudges the segment's mtime. Minor issue, but I found it a bit misleading when trying to verify a log cleanup setting in production.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Log files always touched when broker is bounced - It looks like the latest log segment is always touched when the broker upon start-up regardless of whether it has corrupt data or not, which fudges the segment's mtime. Minor issue, but I found it a bit misleading when trying to verify a log cleanup setting in production."}, {"q": "What updates or decisions were made in the discussion?", "a": "Cool - thanks for checking."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-180", "title": "Clean up shell scripts", "status": "Resolved", "priority": "Major", "reporter": "Jay Kreps", "assignee": "Jay Kreps", "labels": [], "created": "2011-11-01T04:08:10.000+0000", "updated": "2014-02-09T23:51:30.000+0000", "description": "Currently it is a bit of a mess:\njkreps-mn:kafka-git jkreps$ ls bin\nkafka-console-consumer-log4j.properties\tkafka-producer-perf-test.sh\t\tkafka-server-stop.sh\t\t\tzookeeper-server-stop.sh\nkafka-console-consumer.sh\t\tkafka-producer-shell.sh\t\t\tkafka-simple-consumer-perf-test.sh\tzookeeper-shell.sh\nkafka-console-producer.sh\t\tkafka-replay-log-producer.sh\t\tkafka-simple-consumer-shell.sh\nkafka-consumer-perf-test.sh\t\tkafka-run-class.sh\t\t\trun-rat.sh\nkafka-consumer-shell.sh\t\t\tkafka-server-start.sh\t\t\tzookeeper-server-start.sh\n\nI think all the *-shell.sh scripts and all the *-simple-perf-test.sh scripts should die. If anyone has a use for these test classes we can keep them around and use the via kafka-run-class, but they are clearly not made for normal people to use. The *-shell.sh scripts are obsolete now that we have the *-console-*.sh scripts, since these do everything the old scripts did and more. I recommend we also delete the code for these.\n\nI would like to change each tool so that it produces a usage line explaining what it does when run without arguments. Currently I actually had to go read the code to figure out what some of these are.\n\nI would like to clean up places where the arguments are non-standard. Argument names should be the same across all the tools.\n\nI would also like to rename kafka-replay-log-producer.sh to kafka-copy-topic.sh. I think this tool should also accept two zookeeper urls, the url of the input cluster and the url of the output cluster so this tool can be used to copy between clusters. I think we can have a --zookeeper a --input-zookeeper and a --output-zookeeper where --zookeeper is equivalent to setting both the input and the output zookeeper. Also confused why the options for this list --brokerinfo which can be either a zk url or brokerlist AND also --zookeeper which must be a zk url.\n\nAny objections to all this? Any other gripes people have while I am in there?", "comments": ["SimpleConsumeShell is still useful for debugging purpose. I'd like to keep the code. The script can go.", "We mostly did this."], "derived": {"summary": "Currently it is a bit of a mess:\njkreps-mn:kafka-git jkreps$ ls bin\nkafka-console-consumer-log4j. properties\tkafka-producer-perf-test.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Clean up shell scripts - Currently it is a bit of a mess:\njkreps-mn:kafka-git jkreps$ ls bin\nkafka-console-consumer-log4j. properties\tkafka-producer-perf-test."}, {"q": "What updates or decisions were made in the discussion?", "a": "We mostly did this."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-181", "title": "Log errors for unrecognized config options", "status": "Closed", "priority": "Major", "reporter": "Joel Jacob Koshy", "assignee": "Jun Rao", "labels": ["optimization"], "created": "2011-11-01T19:45:15.000+0000", "updated": "2012-08-25T06:24:29.000+0000", "description": "Currently, unrecognized config options are silently ignored. Notably, if a config has a typo or if a deprecated config is used, then there is no warning issued and defaults are assumed. One can argue that the broker or a consumer or a producer with an unrecognized config option should not even be allowed to start up especially if defaults are silently assumed, but it would be good to at least log an error.\n", "comments": ["Yes, please yes.\n\nI recommend we create a Config object that wraps java.util.Properties. It should include all the random Utils helpers we have for parsing ints and stuff. Whenever a get() is called for a property string we should record that property in a set. We can add a method that intersects the requested properties with the provided properties to get unused properties.\n\nThis config can be used in KafkaConfig and other configs.\n\nAs a side note, there are many places where we need to be able let the user provide pluggins that implement an interface. Examples are the EventHandler and Serializer interfaces in the producer, and you could imagine us making other things such as offset storage pluggable. One requirement to make this work is that it needs to be possible for the user to set properties for their plugin. For example to create an AvroSerializer you need to be able to pass in a schema.registry.url parameter which needs to get passed through unmolested to the AvroSerializerImpl to use. To enable the config objects like KafkaConfig that parse out their options should retain the original Config instance. The general contract for pluggins should be that they must provide a constructor that takes a Config so that these configs can be passed through.", "Here are a few other facilities it would be nice to have if we were adding a generic config helper class:\n# It would also be nice to have a facility to support aliases for backwards compatibility. Say we introduce a property log.retention.size and later realize that is inconsistent with the other properties which explicitly give the units. It would be nice to change the name to log.retention.size.bytes as the official name but still retain the old name for compatability. Of course this can be done manually, but maybe could be done better in the helper class.\n# It would be nice to require an official documentation field.\n# It would be nice to have a way to dump out all configs used, their defaults, and their documentation strings as a way to automatically build the site documentation and keep it in sync.", "+1, these are good ideas", "+1", "That sounds like a good idea. However, how do we know the config initialization has finished so that we can report unused properties? ", "The assumption is that the properties are only used during initialization and are passed into other components as simple constructor args. I notice this isn't universally true, but is generally a good pattern so we could refactor the cases where it isn't true.\n\nSo basically it should be safe to check at the end of kafka server startup and warn about any unused properties.", "That's reasonable for broker configs. What about configs for clients?", "Attach patch v1. Added a wrapper class VeririableProperties. Verification is done at the end of the constructor of each public-facing configuration class. It reports overridden property values and non-used property values.", "Can we move the various Utils.get* into VerifiableProperties? This way you just say\n  prop.getInt('prop.name')\n\nThe only reason for all those utility functions was because Properties doesn't have them and I didn't know about implicits at that time.", "Thanks for the review. Attach patch v2 that addresses the review comments. Also rebased.", "+1", "+1\nA couple nit picks:\n1. VerifiableProperties is a \"generic\" utility class, so I don't think it should contain getCompressionCodec() which is pretty application specific.\n2. It would be nice to put it in its own file kafka/utils/VerifiableProperties.scala", "Thanks for the review. Addressed the 2 nits. Committed to 0.8."], "derived": {"summary": "Currently, unrecognized config options are silently ignored. Notably, if a config has a typo or if a deprecated config is used, then there is no warning issued and defaults are assumed.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Log errors for unrecognized config options - Currently, unrecognized config options are silently ignored. Notably, if a config has a typo or if a deprecated config is used, then there is no warning issued and defaults are assumed."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for the review. Addressed the 2 nits. Committed to 0.8."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-182", "title": "Set a TCP connection timeout for the SimpleConsumer", "status": "Resolved", "priority": "Major", "reporter": "Jay Kreps", "assignee": null, "labels": [], "created": "2011-11-01T20:19:39.000+0000", "updated": "2015-02-07T23:46:24.000+0000", "description": "Currently we use SocketChannel.open which I *think* can block for a long time. We should make this configurable, and we may have to create the socket in a different way to enable this.", "comments": [], "derived": {"summary": "Currently we use SocketChannel. open which I *think* can block for a long time.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Set a TCP connection timeout for the SimpleConsumer - Currently we use SocketChannel. open which I *think* can block for a long time."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-183", "title": "Expose offset vector to the consumer", "status": "Resolved", "priority": "Major", "reporter": "Jay Kreps", "assignee": "Jay Kreps", "labels": [], "created": "2011-11-01T20:27:04.000+0000", "updated": "2014-07-17T23:35:44.000+0000", "description": "We should enable consumers to save their position themselves. This would be useful for consumers that need to store consumed data so they can store the data and the position together, this gives a poor man's \"transactionality\" since any data loss on the consumer will also rewind the position to the previous position so the two are always in sync.\n\nTwo ways to do this:\n1. Add an OffsetStorage interface and have the zk storage implement this. The user can override this by providing an OffsetStorage implementation of their own to change how values are stored.\n2. Make commit() return the position offset vector and add a setPosition(List<Long>) method to initialize the position.\n\nLet's figure out any potential problems with this, and work out the best approach.", "comments": ["This is being done in the new consumer."], "derived": {"summary": "We should enable consumers to save their position themselves. This would be useful for consumers that need to store consumed data so they can store the data and the position together, this gives a poor man's \"transactionality\" since any data loss on the consumer will also rewind the position to the previous position so the two are always in sync.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Expose offset vector to the consumer - We should enable consumers to save their position themselves. This would be useful for consumers that need to store consumed data so they can store the data and the position together, this gives a poor man's \"transactionality\" since any data loss on the consumer will also rewind the position to the previous position so the two are always in sync."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is being done in the new consumer."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-184", "title": "Log retention size and file size should be a long", "status": "Resolved", "priority": "Minor", "reporter": "Joel Jacob Koshy", "assignee": null, "labels": [], "created": "2011-11-02T05:37:51.000+0000", "updated": "2013-07-03T22:08:10.000+0000", "description": "Realized this in a local set up: the log.retention.size config option should be a long, or we're limited to 2GB. Also, the name can be improved to log.retention.size.bytes or Mbytes as appropriate. Same comments for log.file.size. If we rename the configs, it would be better to resolve KAFKA-181 first.\n", "comments": ["The first part of this appears to be changed in the 0.8 branch. \n\n /* the maximum size of the log before deleting it */\n  val logRetentionSize = Utils.getLong(props, \"log.retention.size\", -1)", "Added patch to change log.retention.size to log.retention.size.bytes, and updated included server.properties", "Thanks! I am going to hold off on this. As you mention, changing the name of the properties is kind of undesirable as people would likely not notice and silently set the wrong thing. Fixing KAFKA-181 would make this less painful. In particular it would be nice to have aliases so we can fix config names but still conveniently allow the old name. Will add a comment to that ticket.", "We have already fixed all the config naming and types in 0.8. We can resolve this."], "derived": {"summary": "Realized this in a local set up: the log. retention.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Log retention size and file size should be a long - Realized this in a local set up: the log. retention."}, {"q": "What updates or decisions were made in the discussion?", "a": "We have already fixed all the config naming and types in 0.8. We can resolve this."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-185", "title": "The files in the kafka\\clients\\csharp\\src\\Kafka\\Kafka.Client\\cfg folder of the csharp client are out of date.", "status": "Resolved", "priority": "Major", "reporter": "Mike Wilkerson", "assignee": null, "labels": [], "created": "2011-11-03T16:04:33.000+0000", "updated": "2015-02-07T23:46:05.000+0000", "description": "When I open the Kafka.sln there are many files missing. When I remove the missing and add the files that are in the cfg folder I get many other errors. I think these files are from a newer version and the latest files have not been added.\n\nThis is the complete list of missing files.\n\nCfg\\AsyncProducerConfiguration.cs\nCfg\\BrokerConfiguration.cs\nCfg\\BrokerConfigurationElement.cs\nCfg\\BrokerConfigurationElementCollection.cs\nCfg\\ConsumerConfigurationSection.cs\nCfg\\ConsumerConfiguration.cs\nCfg\\ProducerConfiguration.cs\nCfg\\ProducerConfigurationSection.cs\nCfg\\ZooKeeperConfigurationElement.cs\nCfg\\ZooKeeperServerConfigurationElement.cs\nCfg\\ZooKeeperServerConfigurationElementCollection.cs\nCfg\\SyncProducerConfiguration.cs\nCfg\\ZooKeeperConfiguration.cs\n\nExceptions\\IllegalStateException.cs\nExceptions\\InvalidMessageSizeException.cs\nExceptions\\NoSuchElementException.cs\nExceptions\\UnknownCodecException.cs\n\nMessages\\CompressionCodec.cs\nMessages\\CompressionCodecs.cs\nMessages\\CompressionUtils.cs\nMessages\\MessageAndOffset.cs\n\nKafkaStopWatch.cs\n\n\n", "comments": ["We are switching our analytics platform over to a message queuing solution and I'm a big fan of the architecture of kafka.  Unfortunately our web code is c# and this is a complete blocker for any implementation we do.", "I'm also a big fan of your architecture, but as Brent we also need the c# web code.", "I am also looking for a way to use Kafka CSharp client... any updates on missing files?", "Has anyone found the missing files?  Has anyone instead rolled back to the previous check-in and re-built?\n\n\nRevision: 1185772\nAuthor: junrao\nDate: Tuesday, October 18, 2011 1:52:13 PM\nMessage:\nAdd compression to C# client; patched by Eric Hauser; KAFKA-153"], "derived": {"summary": "When I open the Kafka. sln there are many files missing.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "The files in the kafka\\clients\\csharp\\src\\Kafka\\Kafka.Client\\cfg folder of the csharp client are out of date. - When I open the Kafka. sln there are many files missing."}, {"q": "What updates or decisions were made in the discussion?", "a": "Has anyone found the missing files?  Has anyone instead rolled back to the previous check-in and re-built?\n\n\nRevision: 1185772\nAuthor: junrao\nDate: Tuesday, October 18, 2011 1:52:13 PM\nMessage:\nAdd compression to C# client; patched by Eric Hauser; KAFKA-153"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-186", "title": "no clean way to getCompressionCodec from Java-the-language", "status": "Resolved", "priority": "Major", "reporter": "Chris Burroughs", "assignee": null, "labels": [], "created": "2011-11-04T17:26:10.000+0000", "updated": "2017-08-17T11:41:22.000+0000", "description": "The obvious thing fails:\n\nCompressionCodec.getCompressionCodec(1) results in cannot find symbol\nsymbol  : method getCompressionCodec(int)\nlocation: interface kafka.message.CompressionCodec\n\nWriting a switch statement with  kafka.message.NoCompressionCodec$.MODULE$ and duplicating the logic in CompressionCodec.getCompressionCodec is no fun, nor is creating a Hashtable just to call Utils.getCompressionCodec.  I'm not sure if there is a magic keyword to make it easy for javac to understand which CompressionCodec I'm referring to.\n\n", "comments": ["We could just use kafka.message.NoCompression(). It's a static method.\n\nAlso, it doesn't look like there is a constant defined for GzipCompressionCode in the message class.", "Actually, we should use kafka.message.NoCompressionCodec.codec() and  kafka.message.GZIPCompressionCodec.codec(). Both are static methods.", "That requires everyone to write their own switch statement though.", "You only need do deal with the codec directly if you use SyncProducer. We recommend most people use Producer, in which compress level can be configured in the producer property.", "Yes, agreed, I think we can definitely change this but the SyncProducer should be considered an internal class now. We should remove it from the docs with the 0.7 release as I think the Producer api can be either sync or async.", "Chris,\n\nBased on the discussion above, do you feel like the code needs improvement ? If not, can we close this JIRA ?\n", "CompressionType Java class added in newer Kafka version."], "derived": {"summary": "The obvious thing fails:\n\nCompressionCodec. getCompressionCodec(1) results in cannot find symbol\nsymbol  : method getCompressionCodec(int)\nlocation: interface kafka.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "no clean way to getCompressionCodec from Java-the-language - The obvious thing fails:\n\nCompressionCodec. getCompressionCodec(1) results in cannot find symbol\nsymbol  : method getCompressionCodec(int)\nlocation: interface kafka."}, {"q": "What updates or decisions were made in the discussion?", "a": "CompressionType Java class added in newer Kafka version."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-187", "title": "Add Snappy Compression as a Codec and refactor CompressionUtil and option on startup to select what the default codec", "status": "Resolved", "priority": "Major", "reporter": "Joe Stein", "assignee": "Joe Stein", "labels": [], "created": "2011-11-05T18:03:35.000+0000", "updated": "2013-05-02T02:29:46.000+0000", "description": "My thoughts are a new trait CompressionDependencies for KafkaProject.scala, adding snappy as the first library.\n\nrefactor CompressionUtil for better code reuse and provide a way on startup to select what the default codec is instead of the default always gziping", "comments": ["Leaving a comment here, as Joe didn't appear to be in the JIRA list. Assigning this JIRA to Joe.", "Thanks for the patch Joe! A couple of comments", "1. There is quite a lot of overlap in the code for the GZIP and Snappy codec in CompressionUtils. Wonder if you were up for refactoring it so that they use the same code path ?\n2. One thing to think about is whether Snappy should be a compile and run time dependency in the core kafka project. Especially since, GZIP will be default and Snappy will only be used if it is explicitly configured. I wonder if there is any way of defining optional run time dependencies ?\n3. I think we will have to wait for the unit test to get fixed before accepting this patch. Have you tried running the system and performance tests yet ? ", "{quote}\n\n1. There is quite a lot of overlap in the code for the GZIP and Snappy codec in CompressionUtils. Wonder if you were up for refactoring it so that they use the same code path ? \n\n{quote}\n\nAgreed, I am.  Should I open a new ticket for refactoring CompressionUtils or just part of this ticket?\n\n{quote}\n\n2. One thing to think about is whether Snappy should be a compile and run time dependency in the core kafka project. Especially since, GZIP will be default and Snappy will only be used if it is explicitly configured. I wonder if there is any way of defining optional run time dependencies ? \n\n{quote}\n\nYeah, we could so something at startup that changes the default behavior to be a specific codec.  Incorporating this into refactoring that class should not be a big deal where the default case will check an object instead of implementing gzip (like it does now) and depending on that object call either the gzip or snappy function i create (which will also get used by the case match).  same JIRA as this?  a new JIRA for refactoring and put this into that?  a third JIRA?\n\n{quote}\n\n3. I think we will have to wait for the unit test to get fixed before accepting this patch. Have you tried running the system and performance tests yet ?\n\n{quote}\n\nSounds good, I should be able to chip away at that tomorrow or early this week.  I did try running the performance tests but ran into some errors.  it is possible something I was doing wrong so I want to go through and set it up again before sending email about that.  same, will try that in the next few days too.\n", "1. I think we might as well do the refactoring it as part of this patch. Copy pasting code doesn't seem like a good idea. \n2. Yes. Same JIRA as this one. We might as well think through all the issues with supporting multiple codecs cleanly now, rather than later.\n3. I've updated the perf patch. Do you want to submit a patch for the unit test ?", "1) sounds good to me, I opened another JIRA but will comment there that it is dead and will just do the work here, not a problem.\n2) sounds good\n3) done KAFKA-192\n\nthe changes were simple enough for me to just redo them after refactoring as such", ">> provide a way on startup to select what the default codec is instead of the default always gziping\n \nToday, we have a config named \"compression.codec\" that picks the compression codec. Today compression.codec is a numeric value. It is 0 for no compression and 1 for GZIP. Now we are supporting multiple codecs. It makes sense for this to be a string value, which can be one of \"none, gzip, snappy\".  Default is still \"none\".\n\nAs part of 2., I was raising a different question. In the general case, should snappy always be a core dependency of Kafka or not ? I don't know the right answer here. Maybe we need to think more.", "re-factored CompressionUtil, added snappy compression and test case for its use.  I have not run the perf test to compare gzip vs snappy yet.   \n", "It would be good if the snappy jar is only a compile time dependency, but not a runtime dependency. This way, people not using snappy doesn't have to include the jar. Could we verify this?", "Yes, this patch allows for that.  Here is how to verify.\n\napply the patch\n./sbt update\n./sbt package\n\nthen remove the jar\n\nrm -f core/lib_managed/scala_2.8.0/compile/snappy-java-1.0.4.1.jar\n\nthen launch up\n\nbin/zookeeper-server-start.sh config/zookeeper.properties\nbin/kafka-server-start.sh config/server.properties\nbin/kafka-producer-shell.sh --props config/producer.properties --topic test\nbin/kafka-consumer-shell.sh --topic test --props config/consumer.properties\n\nsend messages, things are good\n\nshutdown down bin/kafka-producer-shell.sh --props config/producer.properties --topic test\n\nthen in config/producer.properties change the codec to 1\n\nstartup  bin/kafka-producer-shell.sh --props config/producer.properties --topic test\n\nsend messages, things are good to go\n\nshutdown  bin/kafka-producer-shell.sh --props config/producer.properties --topic test\n\nthen  in config/producer.properties change the codec to 2\n\nstartup  bin/kafka-producer-shell.sh --props config/producer.properties --topic test\n\nstarts up fine, then try to send a message.....\n\nException in thread \"main\" java.lang.NoClassDefFoundError: org/xerial/snappy/SnappyOutputStream\n\tat kafka.message.SnappyCompression.<init>(CompressionUtils.scala:61)\n\tat kafka.message.CompressionFactory$.apply(CompressionUtils.scala:82)\n\tat kafka.message.CompressionUtils$.compress(CompressionUtils.scala:111)\n\tat kafka.message.MessageSet$.createByteBuffer(MessageSet.scala:71)\n\tat kafka.message.ByteBufferMessageSet.<init>(ByteBufferMessageSet.scala:45)\n\tat kafka.producer.ProducerPool$$anonfun$send$1$$anonfun$3.apply(ProducerPool.scala:108)\n\tat kafka.producer.ProducerPool$$anonfun$send$1$$anonfun$3.apply(ProducerPool.scala:107)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:206)\n\tat scala.collection.mutable.ArrayBuffer.map(ArrayBuffer.scala:43)\n\tat kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:107)\n\tat kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:102)\n\tat kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:102)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)\n\tat kafka.producer.ProducerPool.send(ProducerPool.scala:102)\n\tat kafka.producer.Producer.configSend(Producer.scala:167)\n\tat kafka.producer.Producer.send(Producer.scala:106)\n\tat kafka.tools.ProducerShell$.main(ProducerShell.scala:68)\n\tat kafka.tools.ProducerShell.main(ProducerShell.scala)\nCaused by: java.lang.ClassNotFoundException: org.xerial.snappy.SnappyOutputStream\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:202)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:190)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:307)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:248)\n\t... 23 more\n", "Thanks, Joe. That's what I was looking for.", "So, this raises a bigger question, how do clients signal that they can't handle codec 'xyz' ?  Is it up to the consumer or could / should the broker re-encode it? \nGranted this probably isn't an issue for snappy since there are appear to be several implementations - but in general is there a need for an 'accept' style header?  \n\nMy thought is that if you leave it up to the client then you could run into an issue where you have client A and client B, and neither support the same compression codecs so you are stuck with uncompressed..  \n", "This is not a problem for java/scala clients. But it could be a problem for non-java clients. I think this is tied to some of the discussions that we had on non-java language support (see \"different language binding support\" thread in http://mail-archives.apache.org/mod_mbox/incubator-kafka-dev/201109.mbox/thread). Ideally, we'd rather each language not re-implement a thick client.", "Right I agree it overlaps that thread, but is compression support a thick or thin attribute? Maybe we should move this to the mailing list?", "Patch v2 looks good to me. Attaching Patch v3 with minor changes in CompressionCodec to avoid duplicating constants. If nobody objects, I will commit this patch later today.", "What is the difference between Joe's v2 patch and the v3 patch ? Can you please describe the changes when uploading a new patch ?", "The following are the changes that I made. \n\nIndex: core/src/main/scala/kafka/message/CompressionCodec.scala\n===================================================================\n--- core/src/main/scala/kafka/message/CompressionCodec.scala\t(revision 1200967)\n+++ core/src/main/scala/kafka/message/CompressionCodec.scala\t(working copy)\n@@ -20,8 +20,9 @@\n object CompressionCodec {\n   def getCompressionCodec(codec: Int): CompressionCodec = {\n     codec match {\n-      case 0 => NoCompressionCodec\n-      case 1 => GZIPCompressionCodec\n+      case NoCompressionCodec.codec => NoCompressionCodec\n+      case GZIPCompressionCodec.codec => GZIPCompressionCodec\n+      case SnappyCompressionCodec.codec => SnappyCompressionCodec\n       case _ => throw new kafka.common.UnknownCodecException(\"%d is an unknown compression codec\".format(codec))\n     }\n   }\n", "+1 on the latest patch. I like the changes.", "Thanks, Joe. Just committed this."], "derived": {"summary": "My thoughts are a new trait CompressionDependencies for KafkaProject. scala, adding snappy as the first library.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add Snappy Compression as a Codec and refactor CompressionUtil and option on startup to select what the default codec - My thoughts are a new trait CompressionDependencies for KafkaProject. scala, adding snappy as the first library."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks, Joe. Just committed this."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-188", "title": "Support multiple data directories", "status": "Closed", "priority": "Major", "reporter": "Jay Kreps", "assignee": "Jay Kreps", "labels": [], "created": "2011-11-05T23:39:11.000+0000", "updated": "2021-06-06T00:09:12.000+0000", "description": "Currently we allow only a single data directory. This means that a multi-disk configuration needs to be a RAID array or LVM volume or something like that to be mounted as a single directory.\n\nFor a high-throughput low-reliability configuration this would mean RAID0 striping. Common wisdom in Hadoop land has it that a JBOD setup that just mounts each disk as a separate directory and does application-level balancing over these results in about 30% write-improvement. For example see this claim here:\n  http://old.nabble.com/Re%3A-RAID-vs.-JBOD-p21466110.html\n\nIt is not clear to me why this would be the case--it seems the RAID controller should be able to balance writes as well as the application so it may depend on the details of the setup.\n\nNonetheless this would be really easy to implement, all you need to do is add multiple data directories and balance partition creation over these disks.\n\nOne problem this might cause is if a particular topic is much larger than the others it might unbalance the load across the disks. The partition->disk assignment policy should probably attempt to evenly spread each topic to avoid this, rather than just trying keep the number of partitions balanced between disks.", "comments": ["Here are a few additional thoughts on this:\n1. This is actually a lot more valuable after kafka 0.8 is out since we will already allow replication at a higher level so the raid is less desirable. Patch should definitely be on 0.8 branch, though it will likely be the same for 0.7.\n2. It is worth deciding if we want to support unbalanced disk sizes and speeds. E.g. if you have a 7.2k RPM drive and a 10k rpm drive will we allow you to balance over these? I recommend we skip this for now, we can always do it later. So like with RAID, we will treat all drives equally.\n3. I think the only change will be in LogManager. Instead of a single config.logDir parameter we will need logDirs, an array of directories. In createLog() we will need a policy that chooses the best disk on which to place the new log-partition.\n4. There may be a few other places that assume a single log directory, may have to grep around and check for that. I don't think their is to much else, as everything else should interact through LogManager and once the Log instance is created it doesn't care where it's home directory is.\n\nOne approach to placement would be to always create new logs on the \"least loaded\" directory. The definition of \"least loaded\" is likely to be a heuristic. There are two things we want to balance (1) data size, (2) i/o throughput. If the retention policy is based on time, then size is a good proxy for throughput. However you could imagine having one log with very small retention size but very high throughput. Another problem is that the usage may change over time, and migration is not feasable. For example a new feature going through a ramped rollout might produce almost no data at first and then later produce gobs of data. Furthermore you might get odd results in the case where you manually pre-create many topics all at once as they would all end up on whichever directory had the least data.\n\nI think a better strategy would be to not try to estimate the least-loaded partition and instead just do round-robin assignment (e.g. logDirs(counter.getAndIncrement() % logDirs.length)). The assumption would be that the number of partitions is large enough that  each topic has one partition on each disk.\n\nEither of these strategies has a corner case if all data goes to a single topic (or one topic dominates the load distribution), and that topic has (say) 5 local partitions and 4 data directories. In this case one directory will get 2x the others. However this corner case could be worked around by carefully aligning the partition count and the total number of data directories, so I don't think we need to handle it here.", "I started the implementation and my code looks much like you have described. I am now to the point of determining which data location to use. I am planning on doing a round-robin assignment for each partition. \n\nSo, with 4 data dirs and the following topic/partition scheme:\n\ntopic1 - 2 partitions\ntopic2 - 4 partitions\ntopic3 - 1 partition\ntopic4 - 2 partitions\n\ndisk1 = topic1/1, topic2/3, topic4/2\ndisc2 = topic1/2, topic2/4\ndisc3 = topic2/1, topic3/1\ndisc4 = topic2/2, topic4/1\n\nThis is a good first step, we may want to later add re-balancing code based on metrics so that the \"produced/consumed messages per second\" are roughly balanced per disk. This may or may not be feasible and valuable and isn't really that important in this initial implementation.", "Sorry, I meant to have this patch in sooner but got quite busy, I expect to have it soon.", "Hey Jonathan, I am working on some log stuff, mind if I take a shot at this too while I am in there?", "Yes please, I have the code written, and I even tested it a little, but I've been able to spend no time on it in a few weeks. It's probably not done yet, but it's close.", "Do you want to just upload whatever you've got in whatever state its in and I can use that as a starting point?", "Okay, took a shot at this. Attached is a draft patch. It needs some tests specific to the log selection.\n\nThere is a hitch, which I discuss below.\n\nThe basic change is\n1. Change KafkaConfig.logDir to logDirectories and take a CSV of paths (I still support the old setting as a fallback)\n2. Move the whole clean shutdown marker file thing into the LogManager. I feel it should have always been inside LogManager since it is an implementation detail of log recovery. Now there is one such file per log directory.\n3. Create new logs in the directories in a round-robin fashion. To do this I keep a counter in LogManager that controls the index of the dir in which we will next create a new log. Initialize this to a random dir index. Each time we create a new index use the dir this points at and then increment it.\n\nThe hitch is the high watermark file. Currently we keep it in log.dir. But what directory should we keep it in when there are multiple log directories? I hackily just use the first. However this creates a dependency on the order of the log dirs in the config, which is not ideal. If we sort them and use the dir that is alphabetically first then if we add new directories that will mess it up (whereas if we hadn't sorted we could have tacked them on the end).\n\nSome options:\n0. What I currently do, just use the first directory for the hwmark file.\n1. Add a new configuration property, metadataDir and use this for the highwatermark file and the clean shutdown marker and any future persistent thing. Downside of this is that it requires a new config parameter the user has to set up.\n2. Require a top level directory for all log directories. e.g. \n   all_logs/\n      log_dir_1/\n        my_topic-1/\n        my_topic-2\n      log_dir_2/\n        ...\nObviously since the goal is to support multiple independently mounted disks and you might not control where they mount to you might have to use soft links. From a previous life I am remembering lots of pain related to java and softlinks.\n\nI would like to get people's feedback on this.", "Thanks for the patch. A couple of comments:\n\n1. It's probably better to have a separate high watermark file per dir for partitions assigned to it. That way, if a disk is damaged, we only lose the high watermarks and the data for partitions on that disk.\n\n2. About data balancing, in the normal case, assigning partitions in a round-robin way to log dirs is fine. However, if a disk is damaged and is replaced, initially there is no data on it. The round-robin approach would mean that the newly replaced disk will always have fewer partitions that other disks. The same issue can occur if a new dir is added in the configuration. An alternative approach is to assign a new partition to the dir with the fewest partitions, which would alleviate this issue.", "Yeah, I think I agree. It's a little more complex, be we can do a \"least loaded\" thing.", "+1 on maintaining one highwatermark file per log directory. This file can contain highwatermarks for partitions that live in that log directory. One way of doing this is by maintaining the partition->highwatermark mapping per log directory inside the ReplicaManager and then just dumping that to the respective log directory's .highwatermark file by the checkpointing thread.", "Updated patch:\n1. Split HWM file per data directory\n2. Move to a \"least partitions\" partition assignment strategy\n3. Add a unit test for the assignment strategy\n\nI think I may have also fixed the transient failure in LogManager.testTimeBasedFlush, though it remains a time-bomb due to its reliance on the scheduler and wall-clock time.\n\nOne thing to think about is that the use of \"least loaded\" does have a few corner cases of its own. In general it won't differ much from round robin. The case where it will differ is the case where we add a new data directory to an existing server or lose a single data directory on a server. In this case ALL new partitions will be created in the empty data directory until it becomes full. The problem this could create  is that any new topics created during this time period will have all partitions assigned to the empty data dir. This may lead to imbalance of load. I think despite this, this strategy is better than (1) round robin, (2) RAID, or (3) something more complicated we might think of now.\n\nThis patch is ready for review.", "Wups, missed two minor changes in that last patch.", "Attached an updated patch with a few very minor changes:\n1. If there is only a single log directory I skip the least loaded calculation. This calculation iterates over all logs so it could be a bit expensive in cases where we have very many logs (though it should be rare). This makes it so that the case where we have only one directory is no more expensive then it is now.\n2. Fixed a deprecation warning\n3. Fixed an outdated scaladoc comment", "Took a quick look at patch v4. Here are few review comments -\n\n1. KafkaConfig\n\nWe should probably raise an error if the same log directory name was specified more than once.\n\n2. LogManager\n\n2.1. I see you have a check in createLogIfNotExists to return if the log is already created. I guess this will happen if two threads execute the following at the same time and enter createLogIfNotExists one after the other.\n    logs.get(topicAndPartition) match {\n      case null => createLogIfNotExists(topicAndPartition)\n\nI wonder if it is useful to move the lock to getOrCreateLog instead ? Also, shouldn't we use the same lock to protect other accesses to the \"logs\" data structure (getLog(), allLogs() and topics()) ?\n\n2.2. Fix typo on nextLogDir \"chose the\" -> \"choose the\"\n\n3. ReplicaManager\n\n3.1 Does it make sense to handle the absence of a matching Log for a topic partition correctly, instead of assuming the presence of one through the get API on an Option ?\n\n3.2 Nit pick -> \"highwater marks\" -> \"high watermarks\" or \"highwatermarks\" ? :)\n\n4. HighwatermarkCheckpoint\n\n4.1 While you're in there, do you mind changing the following API to take in a map of TopicAndPartition->Long instead ? We've been bitten by scala bugs that don't handle equality on tuples very well.\n  def write(highwaterMarksPerPartition: Map[(String, Int), Long])\n", "New patch, rebased and addresses Neha's comments:\n\n1. Good thought. Added a check in LogManager to detect duplicate data directories. This is not the only bad possibility though. It is possible to have another kafka process that has opened using the same data directory. I am not sure what would happen, but something bad. I added a per data-directory file lock to check for this. This adds a new file .lock to each data directory and uses it to do the equivalent of flock/funlock. This will lock access across processes or within a process.\n\n2.1. I agree this is a bit roundabout. The reason is that the logs are in a Pool, which is really a ConcurrentHashMap. These are nice as they don't lock the whole hash table on each lookup. So I think although it is a little more verbose it is better how it is because in the common case (fetching a log) there is no global lock needed. This should also make the other accesses threadsafe.\n\n2.2. Learned to spell Choose. :-)\n\n3.1. I don't really understand this code that well, so I am not sure. If it is a programming error to for there not to be a log present then I would rather leave it (I think you would get the NoSuchElementException and it would be clear what happened). The reason is that adding a match/case statement in the middle of that groupby is going to make it awfully hard to understand.\n\n3.2. Fixed, nice catch.\n\n4.1. Done.\n\nAlso:\n1. Re-arranged methods in LogManager to make a little more sense.", "Thanks for patch v5. Some more comments:\n\n50. LogManager.nextLogDir(): zeros should only include dirs not already used, right? Currently, it seems to include all log dirs.\n\n51. ReplicaManager.checkpointHighWatermarks(): When handling a leaderAndIsr request, we first create a partition and then create a local replica (which creates the local log). So, there is a slight possibility that a partition in allPartitions may not have a local log. The simplest way is to ignore such partition when checkpointing HW.\n\n52. VerifiableProperties:  The following constructor doesn't seem to be used.\ndef this() = this(new Properties)\n", "Okay, you and Neha are giving conflicting advice on ReplicaManager. Can I omit the case where there is no replica? That is what is complicating this method, because if there is no replica then will there be a log to get the parent directory from?\n\n If so then I am left with:\n\n  def checkpointHighWatermarks() {\n    val replicas = allPartitions.values.map(_.getReplica(config.brokerId)).collect{case Some(replica) => replica}\n    val replicasByDir = replicas.filter(_.log.isDefined).groupBy(_.log.get.dir.getParent)\n    for((dir, reps) <- replicasByDir) {\n      val hwms = reps.map(r => (TopicAndPartition(r.topic, r.partitionId) -> r.highWatermark)).toMap\n      highWatermarkCheckpoints(dir).write(hwms)\n    }\n  }\n\nwhich is much simpler. lmk.\n", "Okay this patch addresses Jun's comments:\n\n50. The zeros are actually correct, basically I am initializing to 0 and ten overwriting with the count if there is one. The goal is to ensure that there is an entry for each directory even if it has no logs (otherwise it would never get any logs assigned). It is possible to do this with some kind of case statement, but I think this is more readable.\n\n51. Okay I used the logic above. The logic is now slightly different from what was there before. Now I filter any partition which has no replica from the file. I also filter any replica which has no log, though my understanding is that that shouldn't happen.\n\n52. Left this. The idea is that previously for tests you could do new Properties so it makes sense to be able to do new VerifiableProperties. Not essential so happy either way.", "Our system tests fail with the latest patch. \n\npython -B system_test_runner.py 2>&1 | tee test.out\n\nSaw the following in broker log.\n\n[2012-10-30 07:40:19,682] FATAL Fatal error during KafkaServerStable startup. Prepare to shutdown (kafka.server.KafkaServerStartable)\njava.io.IOException: No such file or directory\n        at java.io.UnixFileSystem.createFileExclusively(Native Method)\n        at java.io.File.createNewFile(File.java:883)\n        at kafka.utils.FileLock.<init>(FileLock.scala:12)\n        at kafka.log.LogManager$$anonfun$10.apply(LogManager.scala:64)\n        at kafka.log.LogManager$$anonfun$10.apply(LogManager.scala:64)\n        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)\n        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)\n        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)\n        at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:34)\n        at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)\n        at scala.collection.mutable.ArrayOps.map(ArrayOps.scala:34)\n        at kafka.log.LogManager.<init>(LogManager.scala:64)\n        at kafka.server.KafkaServer.startup(KafkaServer.scala:60)\n        at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:34)\n        at kafka.Kafka$.main(Kafka.scala:46)\n        at kafka.Kafka.main(Kafka.scala)\n[2012-10-30 07:40:19,683] INFO [Kafka Server 1], shutting down (kafka.server.KafkaServer)\n", "Looks like previously if you configured a data directory that didn't exist we created it for you. This behavior was broken since now we try to acquire a lock first.\n\nThis patch addresses that problem with the following changes:\n0. Rebased\n1. Cleaned up the LogManager initialization so that we create and validate directories before locking them. This fixes the issue.\n2. It looks like the nio FileLock.tryLock has different behvaiors depending on whether the lock is held by your process or another process. This could lead to getting a bad error message if we failed to lock the data directory. Fixed this in our wrapper class.", "Now, some unit tests fail with exceptions like the following. Most of them seem to be transient, but they show up more frequently now.\n\n\u001b[0m[\u001b[31merror\u001b[0m] \u001b[0mTest Failed: testCleanShutdown(kafka.server.ServerShutdownTest)\u001b[0m\nkafka.common.KafkaException: Failed to acquire lock on file .lock in /tmp/kafka-246675. A Kafka instance in another process or thread is using this directory.\n", "This failing test was due to a real bug in LogManager.shutdown that lead to the locks not being released. I fixed this bug. In addition:\n1. Rebased again\n2. Cleaned up ServerShutdownTest and added comments since I spent a lot of time puzzling over this test to figure out what it was doing.\n3. Add Utils.swallow to each shutdown call in KafkaServer.shutdown--otherwise any failure prevents the rest of the shutdown.", "+1 on patch v8. Thanks,", "Checked in rebased version of v8. I did see one failure in the system tests, but chased it down and it was due to KAFKA-593.", "assign a new partition to the dir with the fewest partitions, it works fine if all of the partitions have most or less the same size. But if partition size varies quite a lot, it will cause disk usage imbalance. So it's better to take the disk usage into account.", "[~chenshangan521@163.com] see my comment earlier on this ticket on why assigning to the partitions with the smallest data size has a couple of really bad gotchas. That was why we went with fewest partitions.", "[~jkreps]  I think we could provide an alternative, user can choose either one: Partitions determined or segments determined.", "@chenshangan The issue with using data size was that it is very very common to create a bunch of topics as once. When you do this all new partitions will be put on the same least full partition. Then when data starts being written that partition will be totally overloaded.\n\nWe can make this configurable, but I think almost anyone who chooses that option will get bit by it.\n\nI recommend we instead leave this as it is for initial placement and implement \"rebalancing\" option that actively migrates partitions to balance data between directories. This is harder to implement but I think it is what you actually want.", "[~jkreps] \n\"I recommend we instead leave this as it is for initial placement and implement \"rebalancing\" option that actively migrates partitions to balance data between directories. This is harder to implement but I think it is what you actually want.\"\n\nExactly, this is what I really want, but it's pretty hard to implement. And in our use case, we seldom create a bunch of topics at the same time, topics are increasing day by day.\n\nCommon use case:\n1. a new kafka cluster setup, lots of topics from other kafka cluster or system dump data into this new cluster. segments determined policy works well as all topics are started from zero, so segments are consistent with partitions.\n\n2. an existing  kafka cluster, topics are added day by day. This is the ideal case, segments policy will work well. \n\n3. an existing kafka cluster, topics are added in bunch. It might cause all new topics being put on the same least directory, of course it will cause bad consequence. But if the cluster is big enough and disk counts and capacity of a broker is big enough, and this is not a common use case, the consequence will not be so serious. Users use this option should consider how to avoid such situation.\n\nAbove all, it's worthy providing such an option. But If we can implement a \"rebalancing\" option, it would be perfect.   \n", "[~jkreps]Â please look at KAFKA-12900\r\n\r\nIt seems that we do not have fair data/IO spread now."], "derived": {"summary": "Currently we allow only a single data directory. This means that a multi-disk configuration needs to be a RAID array or LVM volume or something like that to be mounted as a single directory.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support multiple data directories - Currently we allow only a single data directory. This means that a multi-disk configuration needs to be a RAID array or LVM volume or something like that to be mounted as a single directory."}, {"q": "What updates or decisions were made in the discussion?", "a": "[~jkreps]Â please look at KAFKA-12900\r\n\r\nIt seems that we do not have fair data/IO spread now."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-189", "title": "Create a script for dump segment tool", "status": "Resolved", "priority": "Minor", "reporter": "Taylor Gautier", "assignee": null, "labels": ["newbie", "patch"], "created": "2011-11-06T00:01:02.000+0000", "updated": "2018-03-17T06:25:37.000+0000", "description": "In the kafka jar there is a useful segment dump tool.  It would be useful to expose this as a script.  The current usage is:\n\nbin/kafka-run-class.sh kafka.tools.DumpLogSegments logfilename -noprint\n\nChange this to:\n\nbin/kafka-dump-log-segments.sh logfilename <opts>\n", "comments": ["Hi, here's a patch against the trunk. Thanks for Kafka, it rocks!", "Any interest in this; been open a while.", "Fixed inÂ Â KAFKA-6615"], "derived": {"summary": "In the kafka jar there is a useful segment dump tool. It would be useful to expose this as a script.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Create a script for dump segment tool - In the kafka jar there is a useful segment dump tool. It would be useful to expose this as a script."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed inÂ Â KAFKA-6615"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-190", "title": "Parallelize time-based flushes", "status": "Resolved", "priority": "Major", "reporter": "Jay Kreps", "assignee": null, "labels": [], "created": "2011-11-06T02:21:37.000+0000", "updated": "2014-03-20T22:10:59.000+0000", "description": "Currently all time based flushes are done sequentially. If the time-based flush is the only policy that is used this may limit throughput as a single thread may not saturate all the disks. It would be good to make the number of flush threads be configurable. We can probably default it to two threads (threads are cheap).", "comments": ["I think this is less relevant with the new fsync management and replication code."], "derived": {"summary": "Currently all time based flushes are done sequentially. If the time-based flush is the only policy that is used this may limit throughput as a single thread may not saturate all the disks.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Parallelize time-based flushes - Currently all time based flushes are done sequentially. If the time-based flush is the only policy that is used this may limit throughput as a single thread may not saturate all the disks."}, {"q": "What updates or decisions were made in the discussion?", "a": "I think this is less relevant with the new fsync management and replication code."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-191", "title": "Investigate removing the synchronization in Log.flush", "status": "Resolved", "priority": "Major", "reporter": "Jay Kreps", "assignee": "Neha Narkhede", "labels": [], "created": "2011-11-06T02:29:15.000+0000", "updated": "2012-07-12T21:42:08.000+0000", "description": "Currently we have the following synchronization in Log.scala:\n1. append, roll, and flush all share a write lock\n2. read is non-blocking\n\nUnfortunately this means that flush time latency is added to appends (even if the flush is done by a background thread). To fix this we should investigate a scheme to make append and flush not block each other.", "comments": ["I'm interested in investigating this and run some perf tests to see change in IO performance", "Neha--Sounds good. I have a patch (since it is just deleting and reordering, the code change itself is trivial), I will attach. Here are my thoughts. I think we can just remove the synchronization and re-order things so that the unflushed counter and lastFlushTime both remain valid lower bounds. It is possible that the time we set could get overwritten by another thread but it is unlikely to make any practical difference. See if you agree with that logic, I am not 100% positive.\n\nI am not at all sure that this will actually help performance though for two reasons. First, I think it is possible that the file itself may be synchronized. Either at the java level or the OS level. So I am not sure if one can write to the file while a flush is occurring in another thread. This may take some research to understand. \n\nSecond, if it is possible to do parallel write and flush, I think this still may not be ideal (though maybe a good short term hack). My reasoning is that this patch only fixes the blocking behavior for the time-based flush, but my question is why would I ever want to block?\n\nI really see two use cases:\n1. I want every message I write immediately flushed to disk in a blocking fashion before the append() is considered completed. This corresponds to flush.interval=1 in the current system.\n2. I want to periodically flush data, which could be based on the number of messages, or time, (or theoretically based on unflushed bytes, though we haven't implemented that).\n\nSo what I am thinking is that case (1) clearly needs to be blocking to make sense. But for any periodic flush I don't see a reason to block appends. It is true that this makes the intervals inexact, but I think that is probably fine.. For example, even if I set flush.interval=5, it is unlikely that I could actually care that it is exactly 5, I just want to flush often, say ~5 messages. (Even if I did want it exact, since we always write the full messageset, I still might not get that). So I am thinking a better long-term approach might be to have a central threadpool that handles all flushing and have that always be asynchronous. So if I set flush.interval=5, then that means the background thread is triggered every 5 messages BUT no one blocks on this. In addition to this we add an immediate.commit=true/false option to force data to be flushed in a blocking way as part of the append.\n\nObviously the above only works if a parallel append and flush are possible.", "Looks like fsync does block writes, which is kind of weak. This thread is really helpful:\n  http://antirez.com/post/fsync-different-thread-useless.html\n\nThe comments do mention that fsyncdata() (aka FileChannel.force(false)) somehow doesn't interfere, which doesn't make sense to me, as for use since we always append, FileChannel.force should do the same thing regardless of whether we force metadata or not because we are changing the file size which must be flushed. ", "So, I guess If append and fsync can't run in parallel, the patch won't help much. ", "This is not needed in post 0.8 world since flush becomes less important with replication."], "derived": {"summary": "Currently we have the following synchronization in Log. scala:\n1.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Investigate removing the synchronization in Log.flush - Currently we have the following synchronization in Log. scala:\n1."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is not needed in post 0.8 world since flush becomes less important with replication."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-192", "title": "CompressionUtilTest does not run and fails when it does", "status": "Resolved", "priority": "Major", "reporter": "Joe Stein", "assignee": "Joe Stein", "labels": [], "created": "2011-11-06T06:51:38.000+0000", "updated": "2011-11-14T21:55:37.000+0000", "description": "CompressionUtilTest does not run the functions inside of it during ./sbt test\n\nif you change CompressionUtilTest to extend JUnitSuite then the existing functions run (once you adorne them with @Test) but then fail ...\n\nI suspect the TestUtils.checkEquals(messages.iterator, decompressedMessages.iterator) is failing in testSimpleCompressDecompress because all of the messages are serialized into byte arrays and the entire set of messages compressed and that new compressed messages is what is returned as one message instead of the List[Message] and therefor are not interpreted within TestUtil.checkEquals to see this nuance.\n\ne.g.\n\n[error] Test Failed: testSimpleCompressDecompress\njunit.framework.AssertionFailedError: expected:<message(magic = 1, attributes = 0, crc = 3819140844, payload = java.nio.HeapByteBuffer[pos=0 lim=8 cap=8])> but was:<MessageAndOffset(message(magic = 1, attributes = 0, crc = 3819140844, payload = java.nio.HeapByteBuffer[pos=0 lim=8 cap=8]),18)>\n\nand\n\n[error] Test Failed: testComplexCompressDecompress\njunit.framework.AssertionFailedError: expected:<2> but was:<3>\n", "comments": ["made the tests run and fixed them so they succeed (as they should have to compare messages to messages)", "+1. Excellent ! thanks for the patch. Just committed this."], "derived": {"summary": "CompressionUtilTest does not run the functions inside of it during. /sbt test\n\nif you change CompressionUtilTest to extend JUnitSuite then the existing functions run (once you adorne them with @Test) but then fail.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "CompressionUtilTest does not run and fails when it does - CompressionUtilTest does not run the functions inside of it during. /sbt test\n\nif you change CompressionUtilTest to extend JUnitSuite then the existing functions run (once you adorne them with @Test) but then fail."}, {"q": "What updates or decisions were made in the discussion?", "a": "+1. Excellent ! thanks for the patch. Just committed this."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-193", "title": "use by name parameter helper for logging and trait to include lazy logging and refactor code to use the new LogHelper", "status": "Resolved", "priority": "Major", "reporter": "Joe Stein", "assignee": "Jay Kreps", "labels": [], "created": "2011-11-06T17:52:47.000+0000", "updated": "2012-01-19T02:12:17.000+0000", "description": "1) New tait to include logging and helper methods so if (log.isDebugEnabled()) is not required because it is in the helper and log paramaters are passed by name so not executed to tidy up the code\n\n2) refactor all occurrence of logging to use the log helper\n\n3/4 (possibly to be handled in to tickets) the \"lint\" affect from this for changes patched but not on trunk and new patches moving forward until this is baked in", "comments": ["new trait in utils/Logging.scala\n\nscoured trunk and re-factored to use trait\n\nhandling the incremental updates for patches that go to trunk before this patch can maybe just be more patches on this ticket.  after this patch goes to trunk it would be great that any new patches start to use the trait.  ", "This looks good to me.\n\nOne thing I notice is that the type of the argument is, for example,\n  debug(m: =>String)\nbut the type in log4j is \n  void debug(Object m)\n\nIs this intentional? There are two impacts from this:\n\nGood: people will no longer do\n  error(e)\nThe problem with this is that I think it uses e.toString which doesn't print the stack trace but just prints the message.\n\nBad(?): I can no longer print a non-string without calling toString, e.g.\n  debug(kafkaRequestObj)\n\n", "Well, error and fatal have overloaded for throwable since existing code used this... error(e) can still be done the old way (I was coding to keep consistent but we can improve some too)\n\nYes, toString() on the throwable object just is a short description, the message, yup. Bad (my opinion)\n\ndebug(m: => String) did not get debug(m: => Throwable) because no one was doing that yet in the code.\n\nas far as good, my preference/opinion is if you are going to log something to make it useful and informational. stack traces are best\n\nif we accept throwable for every level we could always take that object and do getStackTrace().toString() http://download.oracle.com/javase/6/docs/api/java/lang/Throwable.html#getStackTrace() and then log that result underneath them\n\ne.g.\n\ndef debug(e: => Throwable): Any = {\n    logger.debug(e.getStackTrace().toString())\n}\n\nso folks can do error(e) and we can still get what we want/need in the logs\n\nso how about I change all of the levels in LogHelper to accept throwable as a paramater and to use this stacktrace extraction and to string that for logging?  \n\nanything else?", "Yeah so my recommendations.\n1. Let's have the same variants for each level for consistency. Sounds like that is info(m:=>String), info(m:=>String, t: Throwable) and info(t: Throwable).\n2. I am cool with either String or Object as the message type (e.g. info(m:=>String) or info(m:=>Object). I don't see a great benefit of the Object type in log4j.\n3. Let's definitely make the form that takes only a throwable print the stack trace. I think t.getStackTrace() won't quite do it because the return value is just an array. We can probably have that do info(\"\", t), which I think would do what we want (?) or we have a utility function which I think formats the stack trace.", "ok, I redid the LogHelper to have \n\n1) consistent functions for all levels\n2) format for the throwable object (using toString() on the StackTraceElement http://download.oracle.com/javase/6/docs/api/java/lang/StackTraceElement.html#toString()) \n3) formated both when a string and throwable are passed at the same time.\n4) helper functions for 2 and 3", "Hmm, have you tried this out? Not sure if that works. You are doing\n  def throwableString(e: => Throwable) = e.getStackTrace.toString\n\nBut getStackTrace gives a java array primitive, so that doesn't print a stack trace:\n\njkreps-mn:~ jkreps$ scala\nscala> val e = new RuntimeException(\"Hello\")\ne: java.lang.RuntimeException = java.lang.RuntimeException: Hello\n\nscala> e.getStackTrace.toString\nres8: java.lang.String = [Ljava.lang.StackTraceElement;@4439bc82\n\nWhat if instead we stick with log4j and do the following variants (for info/debug/trace/error/fatal):\ndef info(m: =>String) = if(logger.isInfoEnabled) logger.info(m)\ndef info(m: =>String, e: Throwable) = if(logger.isInfoEnabled) logger.info(m, e)\ndef info(e: Throwable) = if(logger.isInfoEnabled) logger.info(\"\", e)\n\nI think this does what we want with respect to exceptions.", "I tested your approach and that works great just like we want.\n\nAttached patch \"kafka-193-consistent-level-throwable.fix.patch\" to coincide with that.", "Cool, here is the same patch updated against trunk to fix a few new breakages due to new checkins. I also made two minor changes:\n1. Changed LogHelper trait to Logging and made the file name match the trait name. Motivation here is just to make the name trait-like...\n2. Made Utils.registerMbean not throw an exception. Utils.swallow was depending on log4j and this lead to a lot of unnecessary use of log4j directly. I think it is better for jmx registration to not be a fatal exception (JMX shouldn't kill the server or client, just log an error).\n\nIf no objections to these I am going to apply the updated patch.\n", "+1 on the latest patch.", "Epic patch committed.", "We are seeing a weird issue related to this patch. The following java class won't compile.\n\npublic class MyTest extends kafka.consumer.storage.sql.OracleOffsetStorage\n{\n    public MyTest() {\n        super(null);\n    }\n}\n\nThe compilation error is:\n\nfatal(scala.Function0) in kafka.consumer.storage.sql.OracleOffsetStorage cannot implement fatal(scala.Function0<java.lang.String>) in kafka.utils.Logging; attempting to use incompatible return type\nfound   : java.lang.Object\nrequired: void\n\nA similar scala class like the following compiles fine.\n\nclass MyTest() extends kafka.consumer.storage.sql.OracleOffsetStorage(null) {\n}\n\nAnyone knows the issue?", "The issue looks like it is with Scala 2.8.0\n\nI build and use a Kafka jar against 2.9.1 and with that Kafka jar your MyTest.java compiles fine\n\nI also just tried 2.8.2 and that also worked, can you bump your scala version build a jar and use that?"], "derived": {"summary": "1) New tait to include logging and helper methods so if (log. isDebugEnabled()) is not required because it is in the helper and log paramaters are passed by name so not executed to tidy up the code\n\n2) refactor all occurrence of logging to use the log helper\n\n3/4 (possibly to be handled in to tickets) the \"lint\" affect from this for changes patched but not on trunk and new patches moving forward until this is baked in.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "use by name parameter helper for logging and trait to include lazy logging and refactor code to use the new LogHelper - 1) New tait to include logging and helper methods so if (log. isDebugEnabled()) is not required because it is in the helper and log paramaters are passed by name so not executed to tidy up the code\n\n2) refactor all occurrence of logging to use the log helper\n\n3/4 (possibly to be handled in to tickets) the \"lint\" affect from this for changes patched but not on trunk and new patches moving forward until this is baked in."}, {"q": "What updates or decisions were made in the discussion?", "a": "The issue looks like it is with Scala 2.8.0\n\nI build and use a Kafka jar against 2.9.1 and with that Kafka jar your MyTest.java compiles fine\n\nI also just tried 2.8.2 and that also worked, can you bump your scala version build a jar and use that?"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-194", "title": "refactor CompressionUtil for better code reuse and provide a way on startup to select what the default codec is instead of the default always gziping", "status": "Resolved", "priority": "Major", "reporter": "Joe Stein", "assignee": null, "labels": [], "created": "2011-11-06T18:04:30.000+0000", "updated": "2013-05-02T02:29:46.000+0000", "description": null, "comments": ["will refactor the patch with all the current CompressionUtil code", "going to redo the work in KAFKA-187 and put these changes in there also"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "refactor CompressionUtil for better code reuse and provide a way on startup to select what the default codec is instead of the default always gziping"}, {"q": "What updates or decisions were made in the discussion?", "a": "going to redo the work in KAFKA-187 and put these changes in there also"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-195", "title": "change ProducerShell to use high level producer", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Jun Rao", "labels": [], "created": "2011-11-07T06:38:32.000+0000", "updated": "2011-11-07T23:40:34.000+0000", "description": "We need to change ProducerShell to use high level producer and provide a default producer property file", "comments": ["With this patch, we also need to change quick start in our web site to the following:\n\nbin/kafka-producer-shell.sh --props config/producer.properties --topic test", "Attache v2 of the patch. Include more properties in producer property file.", "+1. Lets include this in the next RC", "Committed to trunk. Need to merge into 0.7 branch if we plan to include this in the latest 0.7 RC.", "Please also commit the diff to the 0.7 branch.", "Why are we doing this? KAFKA-180 is to get rid of this code entirely--it is totally replaced by kafka-console-producer.sh, right? It is weird that we have two of these, and I am not sure why we are coding on the old one.", "The main reason is that we don't have an example producer property file and an actual use case. With this patch, there is such a property file that a user can refer to."], "derived": {"summary": "We need to change ProducerShell to use high level producer and provide a default producer property file.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "change ProducerShell to use high level producer - We need to change ProducerShell to use high level producer and provide a default producer property file."}, {"q": "What updates or decisions were made in the discussion?", "a": "The main reason is that we don't have an example producer property file and an actual use case. With this patch, there is such a property file that a user can refer to."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-196", "title": "Topic creation fails on large values", "status": "Resolved", "priority": "Major", "reporter": "Pierre-Yves Ritschard", "assignee": null, "labels": [], "created": "2011-11-07T09:08:14.000+0000", "updated": "2017-08-17T11:38:50.000+0000", "description": "Since topic logs are stored in a directory holding the topic's name, creation of the directory might fail for large strings.\nThis is not a problem per-se but the exception thrown is rather cryptic and hard to figure out for operations.\n\nI propose fixing this temporarily with a hard limit of 200 chars for topic names, it would also be possible to hash the topic name.\n\nAnother concern is that the exception raised stops the broker, effectively creating  a simple DoS vector, I'm concerned about how tests or wrong client library usage can take down the whole broker.", "comments": ["There are a lot of things that do this. In particular lots of different characters can cause problems but especially filesystem ones like '/' and ' ' etc.", "Yep, the thing is, I think the message makes it hard to figure out what just happened, at lest I thought so.", "Thanks for the patch. Where is the limit 200 coming from? Is that the file name limit in most file systems?", "I tried coming up with a sensible and not limitative number which will\nnot clash with MAXPATHLEN when prefixing with the log directory. Some\nold systems are rumored to have it as low as 256, but I don't see many\nuse cases where very large topic strings are relevant. It could safely\nbe bumped to 1000 (which would leave 23 chars for the prefix path).\n\nJava has no way of accessing MAXPATHLEN unfortunately, since it is OS-specific\n\nOn Mon, Nov 7, 2011 at 5:12 PM, Jun Rao (Commented) (JIRA)\n", "Do you think we can make it configurable and default it to 200? Also, can we create a method like verifyTopicName and put all the checkings there?", "You're taking me away from one-liner territory, but i'll take a look\nat it. as for configurable, a property in server.properties is enough\n?\n\nOn Mon, Nov 7, 2011 at 5:28 PM, Jun Rao (Commented) (JIRA)\n", "Yes, a new server property should work.", "Not sure if you will try to address my comment, but verifyTopicName wouldn't suffice - topic names should either be encoded to protect against special characters causing problems, or hashed as Ritschard suggests.", "Yes, we can use verifyTopicName to capture all constraints on topic names. We probably don't want to make it too complicated. How big is the character list that we should disallow?", "Not sure why the server would hang when it couldn't create a log directory. Our socket server processors capture all throwable. So this shouldn't kill any of the processors. Could you take a thread dump and see why the server hangs?", "I think it would be good just to fully think through escaping and validating topic names. Currently we do essentially nothing, and we could potentially file infinite number of bugs against all the individual corner cases. If that is out of scope for what you are trying to do Pierre-Yves we can open a separate ticket, but I think we need to define the acceptable set of strings in a topic name and check that. For example, should we allow spaces? slashes? semicolons? etc. We need to do this escaping against both the unix fs and zookeeper. We are super permissive now, which leads to all kinds of corner cases. The larger solution might just be to make a KafkaTopic class that has the validation logic in the constructor and includes an escapedForFs() and escapedForZk() methods. We probably won't use this everywhere up front, but at least it begins to centralize the logic and makes it easy to reason if a name has already been escaped or not.", "'/' is particularly complicated since we want to eventually have support for  hierarchical topics, in which case '/' (or whatever we choose) will have special meaning to us, ZK, and the local filesystem.  I'd also prefer to have one way to represent topics as strings and not have separate ZK and local fs escaping schemes.\n\nThat said, unless Pierre-Yves feels like biting off a big patch lets keep this one for a configurable max topic length so that the problem users are running into now is fixed.", "Topic MAX_NAME_LENGTH is set to 249 is newer Kafka verions."], "derived": {"summary": "Since topic logs are stored in a directory holding the topic's name, creation of the directory might fail for large strings. This is not a problem per-se but the exception thrown is rather cryptic and hard to figure out for operations.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Topic creation fails on large values - Since topic logs are stored in a directory holding the topic's name, creation of the directory might fail for large strings. This is not a problem per-se but the exception thrown is rather cryptic and hard to figure out for operations."}, {"q": "What updates or decisions were made in the discussion?", "a": "Topic MAX_NAME_LENGTH is set to 249 is newer Kafka verions."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-197", "title": "Embedded consumer doesn't shut down if the server can't start", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Jun Rao", "labels": [], "created": "2011-11-09T16:45:25.000+0000", "updated": "2011-12-13T03:04:19.000+0000", "description": "If a broker embeds a consumer and the broker itself doesn't start (e.g., conflicting broker id in ZK), the embedded consumer is still running. In this case, we should probably shut down the embedded consumer too.\n\nTo do this, we need to either throw an exception or return an error in KafkaServer.startup and act accordingly in KafkaServerStartable.startup.", "comments": ["Patch attached.", "I don't think we want to call halt(), that is like kill -9 the process. I think we want the logs to flush and shutdown gracefully. Can't we just do a graceful shutdown on both the server and the embedded consumer?", "Would it be reasonable to have KafkaServerStartable register a callback with KafkaServer, and have the shutdown API of KafkaServer invoke that callback ? That way, we can ensure that KafkaServerStartable can cleanly shutdown the embedded consumer when the server is shutdown for some reason.", "A less invasive way would just be to have the embedded consumer register a shutdown hook and use System.exit.\n\nI am a little concerned about this whole embedded consumer thing, though. The original approach where we wrote to the local log in process was pretty fool proof. I think sending to a remote broker is actually riddled with issues. The producer send buffer is vulnerable to quite a large loss on any unclean shutdown or indeed any shutdown bugs. And also any condition that leads to a broker being unable to take requests but still registered in zk will lead to unbounded data loss. I wonder if this issue isn't just a special case of many many bad things that could happen.\n\nWith the current approach I actually don't see any benefits at all to bundling the replication process with the kafka broker. It would actually be better to have that run independently it seems to me.", "The main reason that we moved away from writing to local log is to pick up the compression support in the high level producer. Decoupling the embedded consumer from the broker may not be a bad idea. There is one more service/process that one has to manage. However, it's probably more flexible (to support things like consuming from multiple sources and plugging in logic for consumer-side auditing) and is less intrusive to the core Kafka code.", "Attach patch v2. Consolidate all error handling in KafkaServerStarble.", "+1 on v2", "Just committed this."], "derived": {"summary": "If a broker embeds a consumer and the broker itself doesn't start (e. g.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Embedded consumer doesn't shut down if the server can't start - If a broker embeds a consumer and the broker itself doesn't start (e. g."}, {"q": "What updates or decisions were made in the discussion?", "a": "Just committed this."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-198", "title": "Avoid duplicated message during consumer rebalance", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Neha Narkhede", "labels": [], "created": "2011-11-10T16:59:37.000+0000", "updated": "2012-06-14T15:30:08.000+0000", "description": "Currently, a consumer can get duplicated messages when a rebalance is triggered. It would be good if we can eliminate those duplicated messages.", "comments": ["This is a bit tricky to get right. The following is one possible design:\n1. In ConsumerIterator, add a method clearCurrentChunk(). This method will clear the current chunk being iterated. This method has to by synchronized with makeNext().\n\n2. In KafaMessageStream,  add a method clear() which calls consumerIterator.clearCurrentChunk\n\n3. In Fetcher, break initConnections() into 2 methods: stopConnections() and startConnections().\n\n4. In ZookeeperConsumerConnector.updateFetcher. Do the following:\n   a. fetcher.stopConnections\n   b. for each new Fetcher to be created\n      b1. clear fetcher queue\n      b2. call KafkaMessageStream.clear\n   c. call commitOffsets\n   d. fetcher.startConnections", "KAFKA-228 resolve this same issue"], "derived": {"summary": "Currently, a consumer can get duplicated messages when a rebalance is triggered. It would be good if we can eliminate those duplicated messages.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Avoid duplicated message during consumer rebalance - Currently, a consumer can get duplicated messages when a rebalance is triggered. It would be good if we can eliminate those duplicated messages."}, {"q": "What updates or decisions were made in the discussion?", "a": "KAFKA-228 resolve this same issue"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-199", "title": "Add a how-to-mirror document", "status": "Resolved", "priority": "Major", "reporter": "Joel Jacob Koshy", "assignee": null, "labels": [], "created": "2011-11-11T02:27:43.000+0000", "updated": "2011-11-11T21:54:56.000+0000", "description": "I owe kafka-dev a mirroring document, and someone on the user mailing list had questions on mirroring which reminded me to work on this.", "comments": ["Here is a draft of a mirroring document. Any comments? I'm not sure if it is\nat the right level of detail. Does the diagram help much? How about the\nquickstart/demo? Would a wiki work better than a patch for this?\n\nSome other notes: the socket buffer size setting on the server is currently\nignored, but someone at LinkedIn is experimenting with it. Maybe we should\ncomment that out until that is done.\n\nAs discussed on the kafka-user mailing list, there is an interest in open\nsourcing LinkedIn's ETL and audit system - the audit component is very\nuseful in checking the mirror. When that is available, this doc can be\nupdated.\n", "Thanks, Joel. A couple of quick comments:\n\n1. I feel that it's better to put this in our wiki. \n\n2. We probably don't need to include new property files. We just need to list those properties unique to mirroring in the wiki.", "This is great! Personally I really prefer the site html to the wiki for reading, though the wiki is easier to edit.", "Also, one comment, the bit about using mirroring for scalability doesn't quite make sense if you think about it. A single cluster of N machines should be able to handle more reads then two clusters of N/2 (even if you don't count the reads the mirror needs to do). It does help isolation for different SLAs (i.e. ad hoc usage), and multi data center stuff.", "That's right - doesn't help scalability, but facilitates isolation.\n\nI also prefer the website to wiki. I moved it to a wiki for now (https://cwiki.apache.org/confluence/display/KAFKA/Kafka+mirroring). We can let it sit there until it stabilizes. After the ETL stuff has been open-sourced, will add a section on auditing and then consider moving it to the html docs.", "Holding in wiki for now."], "derived": {"summary": "I owe kafka-dev a mirroring document, and someone on the user mailing list had questions on mirroring which reminded me to work on this.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add a how-to-mirror document - I owe kafka-dev a mirroring document, and someone on the user mailing list had questions on mirroring which reminded me to work on this."}, {"q": "What updates or decisions were made in the discussion?", "a": "Holding in wiki for now."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-200", "title": "Support configurable send / receive socket buffer size in server", "status": "Resolved", "priority": "Major", "reporter": "John Fung", "assignee": null, "labels": [], "created": "2011-11-11T21:25:44.000+0000", "updated": "2012-01-06T22:52:17.000+0000", "description": "* Make the send / receive socket buffer size configurable in server.\n\n* KafkaConfig.scala already has the following existing variables to support send / receive buffer:\n    socketSendBuffer\n    socketReceiveBuffer\n\n* The patch attached to this ticket will read the following existing settings in <kafka>/config/server.properties and set the corresponding socket buffers\n    . . .\n    # The send buffer (SO_SNDBUF) used by the socket server\n    socket.send.buffer=1048576\n    # The receive buffer (SO_RCVBUF) used by the socket server\n    socket.receive.buffer=1048576\n", "comments": ["Hey John, can you validate that this setting actually works for large values (i.e. the value you set takes effect)? Values larger than 64k require special negotiation at socket creation time, so that can be trickier to make happen. This setting seems to be made on sockets already accepted so i wonder if it would take effect since the tcp window has already been negotiated at that point. Having been bruised by this before it is best to be careful.\n\nSome docs on this are here: http://docs.oracle.com/javase/1.4.2/docs/api/java/net/Socket.html#setReceiveBufferSize%28int%29\n\nKey point is that to get the buffer setting to take effect on the sockets the server accepts you actually need to set this on the ServerSocket not the accepted sockets. I think setting it on the socket that is accepted won't have any effect. But don't trust anything I say, best to try it out and see.", "Hi Jay,\n\nValues greater than 64k is taking effect in a throughput test carried out recently. Please refer to the following comments for more details.\n\n===================\n\nA throughput test had been carried out to observe the throughput gain by varying the socket buffer sizes at both the sending and receiving ends.\n\nThe following is the layout of the components to test:\n\n|                Sending Colo                 |     Receiving Colo     |\n|------------------  L.A. --------------------|-------- Chicago -------|\n|  Producer (500MB data) --> Source Kafka <------- ConsoleConsumer     |\n\nEnvironments\n============\n- Sending Colo   : L.A.\n- Receiving Colo : Chicago\n- Send Buffer Sizes for Source Kafka server : 100K, 500K, 1M\n- Receive Buffer Sizes for ConsoleConsumer  : 100K, 500K, 1M\n- Hardware for both colos: CPU (2 physical, 16 virtual), 24GB RAM, Linux x86_64\n\nTesting Steps\n=============\n1. Add the property \"socket.send.buffer=102400\" (eg. 100K) in \"kafka.properties\" file for source kafka server\n2. Start Zookeeper and Kafka in sending colo\n3. Start ProducerPerformance class to produce 500MB of data to source Kafka server and wait until all data is produced.\n4. ConsoleConsumer is modified to timeout after 5 sec without incoming messages to get the total time for consuming 500MB of data.\n5. In receiving colo, specify the argument \"--socket-buffer-size 102400\" (100K in this case) for ConsoleConsumer\n6. Start ConsoleConsumer to connect to the zookeeper in sending colo and consume the data\n\nResults\n=======\n\nKafka   Cons    Cons    Cons\nSend    Recv    Fetch   Time\nBuff    Buff    Size    Taken   Min\n=====   =====   =====   =====   =====\n100K    100K    1M      313     5.20\n100K    500K    1M      305     5.10\n100K    1M      1M      307     5.12\n500K    100K    1M      311     5.20\n500K    500K    1M      120     2.00\n500K    1M      1M      121     2.00\n1M      100K    1M      311     5.20\n1M      500K    1M      121     2.00\n1M      1M      1M      98      1.63\n\n1st col: Kafka Send Buffer Size - (\"socket.send.buffer\") configured in kafka.properties file\n2nd col: Consumer Receive Buffer Size \"--socket-buffer-size\" - which is a command line argument for ConsoleConsumer class\n3rd col: Consumer Fetch Size\n4th col: Seconds taken by ConsoleConsumer to consume all data\n5th col: Corresponding minutes taken\n\nObservations\n============\nThe results indicate that the overall throughput is related to the smaller socket buffer size at either the sending or the receiving end. In other words, the smaller socket buffer size acts as a \"bottleneck\" against the throughput of the pipeline.\n", "Interesting. Can we validate the setting directly by checking getReceiveBufferSize()?\n\nI think the reason it may be working is that linux defaults tcp window scaling on...I think the current approach would not work on Solaris because it defaults tcp window scaling off. Or maybe I am wrong. But I think we would be better off doing it the way they recommend and setting the buffer size in the way the documentation encourages.", "But yes, the way TCP is supposed to work is that it takes the minimum of the size the server or client can support.", "Hi Jay,\n\nThe followings are the debug messages from the throughput test and the set values are effective:\n\n1. Kafka log messages at the sending side:\n. . .\n[2011-12-13 01:54:57,572] INFO Closing socket connection to /172.17.166.122. (kafka.network.Processor)\n[2011-12-13 01:54:59,390] INFO #### sendBufferSize : set[1048576] get[1048576] (kafka.network.Acceptor)\n[2011-12-13 01:54:59,390] INFO #### sendBufferSize : set[1048576] get[1048576] (kafka.network.Acceptor)\n[2011-12-13 01:54:59,391] INFO #### sendBufferSize : set[1048576] get[1048576] (kafka.network.Acceptor)\n. . .\n\nThe code which prints the messages is from core/src/main/scala/kafka/network/SocketServer.scala:\n\n  def accept(key: SelectionKey, processor: Processor) {\n    val socketChannel = key.channel().asInstanceOf[ServerSocketChannel].accept()\n    if(logger.isDebugEnabled)\n      logger.info(\"Accepted connection from \" + socketChannel.socket.getInetAddress() + \" on \" + socketChannel.socket.getLocalSocketAddress)\n    socketChannel.configureBlocking(false)\n    socketChannel.socket().setTcpNoDelay(true)\n    socketChannel.socket().setSendBufferSize(sendBufferSize)\n    socketChannel.socket().setReceiveBufferSize(receiveBufferSize)\n  \n    logger.info(\"#### sendBufferSize : set[\" + sendBufferSize + \"] get[\" + socketChannel.socket().getSendBufferSize() + \"]\")\n   \n2. ConsoleConsumer has some debug messages printing from SimpleConsumer at the receiving side:\n. . .\n[2011-12-13 01:58:15,099] DEBUG Connected to /172.17.166.122:10251 for fetching. (kafka.consumer.SimpleConsumer) \n[2011-12-13 01:58:15,160] TRACE requested receive buffer size=512000 actual receive buffer size= 512000 (kafka.consumer.SimpleConsumer)\n. . .\n", "Hi Jay,\n\nI am going to change it to the way recommended by the doc you have posted above.\n\nThanks,\nJohn", "===============\nBackground Informations\n===============\n\n1. Socket.setReceiveBufferSize() has restriction specified in Java API doc : \"For client sockets, setReceiveBufferSize() must be called before connecting the socket to its remote peer.\"\n- http://docs.oracle.com/javase/6/docs/api/java/net/Socket.html#setReceiveBufferSize%28int%29\n\n2. Socket.setSendBufferSize() does not have corresponding restriction as setReceiveBufferSize()\n- http://docs.oracle.com/javase/6/docs/api/java/net/Socket.html#setSendBufferSize%28int%29\n\n3. SocketChannel.socket() returns Socket object which is needed to provide both setReceiveBufferSize() and setSendBufferSize() methods (However, SocketChannel is available only after the connection is accepted)\n\n4. ServerSocketChannel.socket() returns ServerSocket object which provides only setReceiveBufferSize() but *no* setSendBufferSize() method (ServerSocketChannel is available *before* the connection is accepted)\n\n5. Hence, ServerSocketChannel object is available to provide setReceiveBufferSize before calling ServerSocketChannel.accept() which would satisfy the restriction specified in item 1\n\nThe above 5 items are background informations which lead to the following 3 different approaches to set send/receive buffer sizes in Kafka (tested in Linux).\n\nIt appears that Approach 3 is the most appropriate in this scenario.\n\n============================\nApproach 1 - KAFKA-200.patch (existing patch)\n\n1. call setReceiveBufferSize after accept()\n2. call setSendBufferSize after accept()\n============================\n\ndef accept(key: SelectionKey, processor: Processor) {\n    val socketChannel = key.channel().asInstanceOf[ServerSocketChannel].accept()\n    socketChannel.configureBlocking(false)\n    socketChannel.socket().setTcpNoDelay(true)\n    socketChannel.socket().setSendBufferSize(sendBufferSize)\n    socketChannel.socket().setReceiveBufferSize(receiveBufferSize)\n     \n    processor.accept(socketChannel)\n}\n\n=====\nResults:\n=====\n1. In Linux machine, seeing both send / receive buffer socket sizes set to the expected values.\n2. It doesn't comply with the restriction specified by Java API doc\n\n============================\nApproach 2:\n\n1. call setReceiveBufferSize before accept()\n2. call setSendBufferSize before accept()\n============================\n\ndef accept(key: SelectionKey, processor: Processor) {\n      val socketChannel = key.channel().asInstanceOf[SocketChannel]\n      socketChannel.configureBlocking(false)\n      socketChannel.socket().setTcpNoDelay(true)\n      socketChannel.socket().setSendBufferSize(sendBufferSize)\n      socketChannel.socket().setReceiveBufferSize(receiveBufferSize)\n      socketChannel.asInstanceOf[ServerSocketChannel].accept()\n     \n      processor.accept(socketChannel)\n}\n\n=====\nResults:\n=====\n1. Compilation OK.\n2. Runtime error for casting:\n\n[2011-12-15 21:31:52,986] ERROR Error in acceptor (kafka.network.Acceptor)\njava.lang.ClassCastException\n[2011-12-15 21:31:52,986] ERROR Error in acceptor (kafka.network.Acceptor)\njava.lang.ClassCastException\n. . .\n\n\n============================\nApproach 3:\n\n1. call setReceiveBufferSize before accept()\n    (complying with restriction specified in http://docs.oracle.com/javase/6/docs/api/java/net/Socket.html#setReceiveBufferSize%28int%29)\n\n2. call setSendBufferSize after accept()\n    (No corresponding restriction to call before accept() )\n============================\n\ndef accept(key: SelectionKey, processor: Processor) {\n    val serverSocketChannel = key.channel().asInstanceOf[ServerSocketChannel]\n    serverSocketChannel.configureBlocking(false)\n    serverSocketChannel.socket().setReceiveBufferSize(receiveBufferSize)\n   \n    val socketChannel = serverSocketChannel.accept()\n    socketChannel.socket().setTcpNoDelay(true)\n    socketChannel.socket().setSendBufferSize(sendBufferSize)\n\n    processor.accept(socketChannel)\n}\n\n=====\nResults:\n=====\n1. Comply with the restriction specified in Java API doc\n2. Both sendBufferSize and receiveBufferSize are set to the expected values:\n    [2011-12-15 21:46:46,525] DEBUG sendBufferSize: [1048576] receiveBufferSize: [1048576] (kafka.network.Acceptor)", "John,\n\nThanks for the detailed update. From TCP illustrated section 13.3.3, last paragraph: \"The shift count is automatically chosen by TCP, based on the size of the receive buffer.\" So, to set a TCP window size larger than 64K, we just need to make sure the receive buffer size is set properly before the connection is established. So, Approach 3 as you listed is the right thing to do. Could you upload a new patch?", "The KAFKA-200-v3.patch contains the following changes:\n\n1. Updated ConsoleConsumer to take the argument --socket-buffer-size\n2. Implemented Approach 3 as mentioned above. But the code is changed by calling configBlocking(false) after accept( ). Otherwise, it won't work properly.", "Thanks, John. Just committed this patch."], "derived": {"summary": "* Make the send / receive socket buffer size configurable in server. * KafkaConfig.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Support configurable send / receive socket buffer size in server - * Make the send / receive socket buffer size configurable in server. * KafkaConfig."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks, John. Just committed this patch."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-201", "title": "Support for mirroring from multiple sources", "status": "Closed", "priority": "Major", "reporter": "Paul Querna", "assignee": "Joel Jacob Koshy", "labels": [], "created": "2011-11-11T23:08:04.000+0000", "updated": "2012-12-06T18:44:01.000+0000", "description": "Currently the EmbeddedConsumer is configured against a single source mirror.\n\nWe have a use case of consuming from multiple sources clusters.\n\nSimple example, we have 3 datacenters which are collecting data: A, B, C.\n\nWe want all 3 to get full copies of the data eventually, by using a  on a whitelist of topics, and having the topics include the source data centers.\n\nSo, we would like to be able to:\n\nConfigure A to mirror B, and C with a whitelist of topics B,C\nConfigure B to mirror A, and C with a whitelist of topics A,C\nConfigure C to mirror A, and B with a whitelist of topics A,B\n\n", "comments": ["This would be a great feature, we would gladly take a patch!", "This should be relatively easy.  can just extend KafkaServerStartble to take an array of ConsumerConfig and create multiple instances of EmbeddedConsumer.", "Mirror maker does this, no?", "Yes - this should be addressed by MirrorMaker although the user should ensure that the consumption graph for every topic is a DAG."], "derived": {"summary": "Currently the EmbeddedConsumer is configured against a single source mirror. We have a use case of consuming from multiple sources clusters.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support for mirroring from multiple sources - Currently the EmbeddedConsumer is configured against a single source mirror. We have a use case of consuming from multiple sources clusters."}, {"q": "What updates or decisions were made in the discussion?", "a": "Yes - this should be addressed by MirrorMaker although the user should ensure that the consumption graph for every topic is a DAG."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-202", "title": "Make the request processing in kafka asynchonous", "status": "Resolved", "priority": "Major", "reporter": "Jay Kreps", "assignee": "Neha Narkhede", "labels": [], "created": "2011-11-12T03:20:48.000+0000", "updated": "2012-01-13T22:41:23.000+0000", "description": "We need to handle long-lived requests to support replication. To make this work we need to make the processing mechanism asynchronous from the network threads.\n\nTo accomplish this we will retain the existing pool of network threads but add a new pool of request handling threads. These will do all the disk I/O. There will be a queue mechanism to transfer requests to and from this secondary pool.", "comments": ["This patch is ready for review. Note the new config I added for controlling the number of I/O threads.\n\nAlso, this patch removes all JMX monitoring from the socket server. Temporarily ignore this fact. The issue was our JMX was tangling together the socket server and the request handling. I want to generalize our stat collection and will open a second JIRA for that.", "Unfortunately it seems this patch doesn't apply cleanly against the latest code.", "Updated patch to current trunk.", "Overall, the patch looks good. Some comments:\n\n1. KafkaServer.startup doesn't have to capture exception and shutdown. The caller in KafkaServerStarable already does that. Plus, it shuts down embedded consumer appropriately if needed.\n\n2. There is KafkaRequestHandlers.scala.rej in the patch.\n\n3. Unit test seems to fail occasionally, giving the following error.\n[info] == core-kafka / kafka.integration.LazyInitProducerTest ==\n[2012-01-05 21:57:38,773] ERROR Error processing MultiProducerRequest on test:0 (kafka.server.KafkaApis:82)\njava.nio.channels.ClosedChannelException\n\tat sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:88)\n\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:184)\n\tat kafka.message.ByteBufferMessageSet.writeTo(ByteBufferMessageSet.scala:75)\n\tat kafka.message.FileMessageSet.append(FileMessageSet.scala:161)\n\tat kafka.log.Log.append(Log.scala:215)\n\tat kafka.server.KafkaApis.kafka$server$KafkaApis$$handleProducerRequest(KafkaApis.scala:71)\n\tat kafka.server.KafkaApis$$anonfun$handleMultiProducerRequest$1.apply(KafkaApis.scala:64)\n\tat kafka.server.KafkaApis$$anonfun$handleMultiProducerRequest$1.apply(KafkaApis.scala:64)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)\n\tat scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:34)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:206)\n\tat scala.collection.mutable.ArrayOps.map(ArrayOps.scala:34)\n\tat kafka.server.KafkaApis.handleMultiProducerRequest(KafkaApis.scala:64)\n\tat kafka.server.KafkaApis.handle(KafkaApis.scala:43)\n\tat kafka.server.KafkaServer$$anonfun$startup$2.apply(KafkaServer.scala:70)\n\tat kafka.server.KafkaServer$$anonfun$startup$2.apply(KafkaServer.scala:70)\n\tat kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:35)\n\tat java.lang.Thread.run(Thread.java:662)\n[2012-01-05 21:57:38,773] ERROR Error processing ProduceRequest on test:0 (kafka.server.KafkaApis:82)\njava.nio.channels.ClosedChannelException\n\tat sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:88)\n\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:184)\n\tat kafka.message.ByteBufferMessageSet.writeTo(ByteBufferMessageSet.scala:75)\n\tat kafka.message.FileMessageSet.append(FileMessageSet.scala:161)\n\tat kafka.log.Log.append(Log.scala:215)\n\tat kafka.server.KafkaApis.kafka$server$KafkaApis$$handleProducerRequest(KafkaApis.scala:71)\n\tat kafka.server.KafkaApis.handleProducerRequest(KafkaApis.scala:55)\n\tat kafka.server.KafkaApis.handle(KafkaApis.scala:40)\n\tat kafka.server.KafkaServer$$anonfun$startup$2.apply(KafkaServer.scala:70)\n\tat kafka.server.KafkaServer$$anonfun$startup$2.apply(KafkaServer.scala:70)\n\tat kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:35)\n\tat java.lang.Thread.run(Thread.java:662)\n[2012-01-05 21:57:38,775] FATAL Halting due to unrecoverable I/O error while handling producer request: null (kafka.server.KafkaApis:92)\njava.nio.channels.ClosedChannelException\n\tat sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:88)\n\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:184)\n\tat kafka.message.ByteBufferMessageSet.writeTo(ByteBufferMessageSet.scala:75)\n\tat kafka.message.FileMessageSet.append(FileMessageSet.scala:161)\n\tat kafka.log.Log.append(Log.scala:215)\n\tat kafka.server.KafkaApis.kafka$server$KafkaApis$$handleProducerRequest(KafkaApis.scala:71)\n\tat kafka.server.KafkaApis.handleProducerRequest(KafkaApis.scala:55)\n\tat kafka.server.KafkaApis.handle(KafkaApis.scala:40)\n\tat kafka.server.KafkaServer$$anonfun$startup$2.apply(KafkaServer.scala:70)\n\tat kafka.server.KafkaServer$$anonfun$startup$2.apply(KafkaServer.scala:70)\n\tat kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:35)\n\tat java.lang.Thread.run(Thread.java:662)\n[2012-01-05 21:57:38,775] FATAL Halting due to unrecoverable I/O error while handling producer request: null (kafka.server.KafkaApis:92)\njava.nio.channels.ClosedChannelException\n\tat sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:88)\n\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:184)\n\tat kafka.message.ByteBufferMessageSet.writeTo(ByteBufferMessageSet.scala:75)\n\tat kafka.message.FileMessageSet.append(FileMessageSet.scala:161)\n\tat kafka.log.Log.append(Log.scala:215)\n\tat kafka.server.KafkaApis.kafka$server$KafkaApis$$handleProducerRequest(KafkaApis.scala:71)\n\tat kafka.server.KafkaApis$$anonfun$handleMultiProducerRequest$1.apply(KafkaApis.scala:64)\n\tat kafka.server.KafkaApis$$anonfun$handleMultiProducerRequest$1.apply(KafkaApis.scala:64)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)\n\tat scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:34)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:206)\n\tat scala.collection.mutable.ArrayOps.map(ArrayOps.scala:34)\n\tat kafka.server.KafkaApis.handleMultiProducerRequest(KafkaApis.scala:64)\n\tat kafka.server.KafkaApis.handle(KafkaApis.scala:43)\n\tat kafka.server.KafkaServer$$anonfun$startup$2.apply(KafkaServer.scala:70)\n\tat kafka.server.KafkaServer$$anonfun$startup$2.apply(KafkaServer.scala:70)\n\tat kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:35)\n\tat java.lang.Thread.run(Thread.java:662)\n[info] Test Starting: testProduceAndFetch(kafka.integration.LazyInitProducerTest)\n", "Fixed random jvm halts in tests. Removed stray file in patch.", "Improved version of patch. Fixes a bug where a response for a closed socket can throw an uncaught exception.", "I tried to apply the v5 patch and couldn't find RequestChannel.scala in there. Would you mind uploading a patch with that included ?\n\nAlso, if the kafka cluster is experiencing either network or IO bottleneck, it slows processing down and backs up the producer queue causing QueueFullException. To detect this, seems like it will be helpful to expose the number of queued requests in the SocketServerStats ? This could either be a separate JIRA or part of this one, your call.", "Missed some files in the last patch, checked that this one actually builds correctly on a clean checkout.", "unit tests now pass. +1 on the patch.\n\nWe should probably commit this patch to an 0.8 branch.", "Created the 0.8 branch and committed this on the branch. ", "Hey Neha, 0.8 doesn't build for me and is missing a few files in the patch. Did those get missed in the checkin?\n\n[info] == core-kafka / compile ==\n[info]   Source analysis: 134 new/modified, 0 indirectly invalidated, 0 removed.\n[info] Compiling main sources...\n[error] /Users/jkreps/work/kafka-git/core/src/main/scala/kafka/network/SocketServer.scala:45: not found: type RequestChannel\n[error]   val requestChannel = new RequestChannel(numProcessorThreads, maxQueuedRequests)\n[error]                            ^\n[error] /Users/jkreps/work/kafka-git/core/src/main/scala/kafka/network/SocketServer.scala:190: not found: type RequestChannel\n[error]                                val requestChannel: RequestChannel,\n[error]                                                    ^\n[error] /Users/jkreps/work/kafka-git/core/src/main/scala/kafka/network/SocketServer.scala:301: not found: value RequestChannel\n[error]       val req = RequestChannel.Request(processor = id, requestKey = key, request = request, start = time.nanoseconds)\n[error]                 ^\n[error] /Users/jkreps/work/kafka-git/core/src/main/scala/kafka/server/KafkaServer.scala:26: RequestChannel is not a member of kafka.network\n[error] import kafka.network.{SocketServerStats, SocketServer, RequestChannel}\n[error]        ^\n[error] /Users/jkreps/work/kafka-git/core/src/main/scala/kafka/server/KafkaServer.scala:40: not found: type KafkaRequestHandlerPool\n[error]   var requestHandlerPool: KafkaRequestHandlerPool = null\n[error]                           ^\n[error] /Users/jkreps/work/kafka-git/core/src/main/scala/kafka/server/KafkaServer.scala:69: not found: type KafkaRequestHandlerPool\n[error]     requestHandlerPool = new KafkaRequestHandlerPool(socketServer.requestChannel, new KafkaApis(logManager).handle, config.numIoThreads)\n[error]                              ^\n[error] 6 errors found\n[info] == core-kafka / compile ==\n[error] Error running compile: Compilation failed\n[info] \n[info] Total time: 21 s, completed Jan 13, 2012 12:46:07 PM\n[info] \n[info] Total session time: 24 s, completed Jan 13, 2012 12:46:07 PM\n[error] Error during build", "My bad. Forgot to do the svn add on those. Fixed that, should build fine now. ", "Thanks Neha! Working great now!"], "derived": {"summary": "We need to handle long-lived requests to support replication. To make this work we need to make the processing mechanism asynchronous from the network threads.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Make the request processing in kafka asynchonous - We need to handle long-lived requests to support replication. To make this work we need to make the processing mechanism asynchronous from the network threads."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks Neha! Working great now!"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-203", "title": "Improve Kafka internal metrics", "status": "Closed", "priority": "Major", "reporter": "Jay Kreps", "assignee": "Jun Rao", "labels": ["tools"], "created": "2011-11-14T18:52:20.000+0000", "updated": "2012-09-13T04:32:25.000+0000", "description": "Currently metrics in kafka are using old-school JMX directly. This makes adding metrics a pain. It would be good to do one of the following:\n1. Convert to Coda Hale's metrics package (https://github.com/codahale/metrics)\n2. Write a simple metrics package\n\nThe new metrics package should make metrics easier to add and work with and package up the common logic of keeping windowed gauges, histograms, counters, etc. JMX should be just one output of this.\n\nThe advantage of the Coda Hale package is that it exists so we don't need to write it. The downsides are (1) introduces another client dependency which causes conflicts, and (2) seems a bit heavy on design. The good news is that the metrics-core package doesn't seem to bring in a lot of dependencies which is nice, though the scala wrapper seems to want scala 2.9. I am also a little skeptical of the approach for histograms--it does sampling instead of bucketing though that may be okay.\n\n\n", "comments": ["FYI we are using the JMX metrics currently.", "I *think* metrics is source compatible with 2.8,  but I would need to investigate more.", "I have implemented Coda Hale's Metric package and have it running in production.\n\npros:\n- good coverage for typical needs with not much extra effort once it is integrated\n- ganglia context \n- graphite context (I don't use but it is a pro for someone)\n\ncons:\n- package causes a dependency\n- the MetricsServlet is only JSON so some extra effort to have a version that also can be better human readable (better for debugging something I found often)\n- builds are 2.9.1 (I run kafka in 2.9.1 build with it and it works fine though porting it back to 2.8 may get hair and tricky)\n\n\nIMHO \n\n- since there is no compatible Scala version in maven we would basically have to checkout or fork the code, build it and include it (or publish the build ourselves to maven). \n- we should have our own version of MetricsServlet ... if someone wants to implement the http embedded jetty server it should be configurable which then makes the jetty jar not required\n\nSo I am +1 in using source from Coda Hale and keeping that layer as the layer for metrics (we could check in the specific files we need/want to a package kafka.metrics and then make changes as we need (license = ASL 2.0 https://github.com/codahale/metrics/blob/master/LICENSE)\n\nI am comfortable with codahale/metrics source and I would contribute this by pulling the parts in we are looking to use with an initial implementation for using them allowing (replacing anything existing JMX as this would handle that and other context we include like the servlet, ganglia, whatever else we want to make) others to-do so for other things moving forward.  ", "I also have metrics all over the place in production as well.\n\nThe scala compatibility problem only exists if we use metrics-scala, which is just a  pretty wrapper around the pure java stuff http://metrics.codahale.com/manual/scala/.  If we don't use that all of the dependency complications go away.  Is using the java interface palatable?\n\nFor mx4j we made it optional on the classpath.  We can follow the same pattern with metrics reporters to minimize dependency complications for kafak-as-library.\n\nOff Topic:  Json is totally human readable https://addons.mozilla.org/EN-us/firefox/addon/jsonview/", "This probably is unblocked by KAFKA-385. It will be good to convert all existing Kafka metrics to the new metrics package", "This ticket blocks metrics collection and graphing.", "I propose that we add/keep the following set of metrics. Anything missed?\n\nServer side:\nA. Requests:\nA1. produceRequestRate (meter, total)\nA2. fetchRequestRate (meter, follower/non-follower)\nA3. getMetadataRate (meter, total)\nA4. getOffsetRate (meter, total)\nA5. leaderAndISRRate (meter, total)\nA6. stopReplicaRate (meter, total)\nA7. produceRequestSizeHist (hist, total)\nA8. fetchResponseSizeHist (hist, total)\nA9. produceFailureRate (meter, topic/total)\nA10. fetchFailureRate (meter, topic/total)\nA11. produceRequestTime (timer, total)\nA12. fetchRequestTime (timer, total)\nA13. messagesInRate (meter, topic/total)\nA14. messagesOutRate (meter, topic/total)\nA15. messagesBytesInRate (meter, topic/total)\nA16. messagesBytesOutRate (meter, topic/total)\n\nB. Log:\nB1. logFlushTime (timer, total)\n\nC. Purgatory:\nProduce:\nC1. expiredRequestMeter (meter, partition/total)\nC2. satisfactionTimeHist (hist, total)\n\nFetch:\nC3. expiredRequestMeter (meter, follower/non-follower)\nC4. satisfactionTimeHist (hist, follower/non-follower)\n\nBoth:\nC5. delayedRequests (gauge, Fetch/Produce)\n\nD. ReplicaManager:\nD1. leaderPartitionCounts (gauge, total)\nD2. underReplicatedPartitionCounts (|ISR| < replication factor, gauge, total)\nD3. ISRExpandRate (meter, partition/total)\nD4. ISRShrinkRate (meter, partition/total)\n\nE. Controller:\nE1. requestRate (meter, total)\nE2. requestTimeHist (hist, total)\nE3. controllerActiveCount (gauge, total)\n\nClients:\nF. Producer:\nF1. messageRate (meter, topic/total)\nF2. byteRate (meter, topic/total)\nF3. droppedEventRate (meter, total)\nF4. requestRate (meter, total)\nF5. requestSizeHist (hist, total)\nF6. requestTimeHist (hist, total)\nF7. resendRate (meter, total)\nF8. failedSendRate (meter, total)\nF9. getMetadataRate (meter, total) \n\nG. Consumer:\nG1. messageRate (meter, topic/total)\nG2. byteRate (meter, topic/total)\nG3. requestRate (meter, total)\nG4. requestSizeHist (hist, total)\nG5. requestTimeHist (hist, total)\nG6. lagInBytes (gauge, partition)\n\nAlso, I propose that we remove the following metrics since they are either not very useful or are redundant.\nPurgatory:\nProduce:\n* caughtUpFollowerFetchRequest (meter, partition/total): not very useful\n* followerCatchupTime (hist, total): not very useful\n* throughputMeter (meter, partition/total): same as bytesIn\n* satisfiedRequestMeter (meter, total): not very useful\n\nFetch:\n* satisfiedRequestMeter (meter, total): not very useful\n* throughputMeter (meter, partition/total): same as bytesOut\n\nBoth\n* satisfactionRate (meter, Fetch/Produce): not very useful\n* expirationRate (meter, Fetch/Produce/topic): already at Produce/Fetch leve\n", "There are some details around how we track request-level stats. The idea was to begin and end request measurement at the socket server so it includes the full lifecycle. Each request would have the following phases:\n  read time - time spent reading data off the network\n  queue time - time spent waiting for a processing thread to handle the request\n  api time - time spent in the api layer\n  queue time - time spent waiting for a network thread to handle the request\n  write time - time spent writing data (which would include the sendfile time)\n\nThe implementation could just be to add to the Request object we have so that it has something like\n   request.begin(\"read\")\n   // do some reading\n   request.end(\"read\")\n\nIt would be nice to have these stats at the client id level, but there are two problems:\n1. That would be a lot of histogram data\n2. Currently the socket server is not aware of the client id.\n\nSo I recommend we just track this info on a per-api basis for now. We can revisit in the future to get more in-depth instrumentation.", "Attach patch v1. \n\nPatch overview:\n1. Added general support to collect time breakdown (queueTime, localTime, remoteTime, sendTime, totalTime) for all types of requests. Need to refactor RequestChannel.Request a bit to include deserialized request object.\n2. removed some metrics in DelayedRequestMetrics since they are now covered by #1.\n3. Fixed Pool.getMaybePut() to make sure that the new object is only created once.\n4. Converted all existing metrics to use coda hale and added some new metrics.\n\nThe list of new and converted metrics is the following.\nServer side:\nA. Requests \nfor each request type: \nA1. requestRate (meter, total)\nA2. queueTime (hist, total)\nA3. localTime (hist, total)\nA4. remoteTime (hist, total)\nA5. sendTime (hist, total)\nA6. totalTime (hist, total)\nFor Fetch/Produce\nA7. produceFailureRate (meter, topic/total)\nA8. fetchFailureRate (meter, topic/total)\nA9. messagesInRate (meter, topic/total)\nA10. messagesOutRate (meter, topic/total)\nA11. messagesBytesInRate (meter, topic/total)\nA12. messagesBytesOutRate (meter, topic/total)\nAll\nA13. requestQueueSize (gauge, total)\n\nB. Log:\nB1. logFlushTime (timer, total)\nB2. logSegments (gauge, per log)\nB3. logEndOffset (gauge, per log)\n\nC. Purgatory:\nProduce:\nC1. expiredRequestMeter (meter, partition/total)\n\nFetch:\nC2. expiredRequestMeter (meter, follower/non-follower)\n\nBoth:\nC3. delayedRequests (gauge, Fetch/Produce)\n\nD. ReplicaManager:\nD1. leaderPartitionCounts (gauge, total)\nD2. underReplicatedPartitionCounts (|ISR| < replication factor, gauge, total)\nD3. ISRExpandRate (meter, partition/total)\nD4. ISRShrinkRate (meter, partition/total)\n\nE. Controller:\nE1. controllerActiveCount (gauge, total)\n\nClients:\nF. Producer:\nF1. messageRate (meter, topic/total)\nF2. byteRate (meter, topic/total)\nF3. droppedEventRate (meter, total)\nF4. producerQueueSize (gauge, per send thread)\nF5. requestSizeHist (hist, total)\nF6. requestTimeAndRate (timer, total)\nF7. resendRate (meter, total)\nF8. failedSendRate (meter, total)\n\nG. Consumer:\nG1. messageRate (meter, topic/total)\nG2. byteRate (meter, topic/total)\nG3. requestSizeHist (hist, total)\nG4. requestTimeAndRate (timer, total)\nG5. lagInBytes (gauge, partition)", "I think this patch looks great and this list of stats is a good start. I\nhave some minor comments:\n\n1) Rebase - the latest patch applies cleanly to r1378264.\n\n2) The following are just my preferences on naming. What you have should be\n   fine, but we should make sure the stat names are as intuitive as\n   possible. We should come up with a naming convention for stats and add it\n   to our coding convention.\n\n   a) Some timer stats may be better named. E.g., SimpleConsumer\n   ConsumerRequestTime will include both request rate and request duration\n   which is not very intuitive. OTOH I'm having trouble thinking of a naming\n   convention: I would suggest just ConsumerRequestStats - but the size stat\n   would be outside then.\n\n   b) Partition.scala:\n      ISRExpandRate -> ISRExpandEventRate\n      ISRShrinkRate -> ISRShrinkEventRate\n\n   c) Log.scala:\n      \"LogSegments\" -> \"NumLogSegments\"\n\n   d) ConsumerTopicStat.scala:\n      \"Total\" -> \"AllTopics\" Also, what if there's a topic called \"Total\"?\n      :) We may want to name this label such that it is an illegal topic\n      name (KAFKA-495) - say, \"All/Topics\".\n\n   e) SimpleConsumer.scala:\n      \"ConsumerRequestTime\" -> see above.\n\n   f) FileMessageSet.scala:\n      \"LogFlush\" -> \"LogFlushStats\"\n\n   g) RequestChannel.scala:\n\n      i) Instead of \"regular\" and \"follower\" how about \"consumer\" and\n      \"replica\"?\n\n      ii) endRequestTracking -> updateRequestMetrics\n\n      iii) responseComplet (typo)\n\n      iv) For timing stats, may be better to include the unit as part of the\n      metric names (e.g., TotalTimeNs).\n\n      v) SendTime -> ResponseSendTime(Ns)\n\n      vi) May be useful to add a comment that simply lays out the phases to\n      make the code clearer:\n      /* received (start time) -> in queue (queue time) -> dequeued for\n      api-local processing -> [api remote processing] -> send response */\n\n   h) AsyncProducerStats.scala:\n      DroppedEvent -> DroppedEventsPerSec\n      Resentevent -> ResendEventsPerSec\n      resents -> resends\n      FailedSend -> FailedSendsPerSec\n      (or maybe we should just follow a convention: <stat>Rate which\n      defaults to <stat> per sec)\n      FailedSendtRate (typo)\n\n   i) KafkaApis.scala\n      byteInRate -> bytesInRate; byteOutRate -> bytesOutRate\n      ExpiresPerSecond -> ExpirationsPerSec\n\n   j) KafkaRequestHandlers.scala\n      MessageInPerSec -> IncomingMessagesPerSec\n\n3) There are some places (SimpleConsumer, FileMessageSet, SyncProducer)\n   where you use metrics timers. Instead of this:\n\n   val timer = newTimer(...)\n   ...\n   val ctx = timer.time()\n   try {\n     // do something\n   }\n   finally {\n     ctx.stop()\n   }\n\n   You can use the following equivalent pattern:\n   val timer = new KafkaTimer(underlying)\n   timer.time {\n     // do something\n   }\n\n4) ZookeeperConsumerConnector: These JMX operations are actually useful to\n   consumers right?\n\n5) DefaultEventHandler: should byte rate be updated here or only after\n   sending? Although it does seem useful to have the global byte rate even\n   for those that are subsequently dropped.\n\n6) SyncProducer.scala: use KafkaTimer. Also, same comment on naming for\n   timers described above.\n\n7) AbstractFetcherThread.scala: FetcherLagMetrics.lock unused.\n\n8) KafkaApis.scala:\n   a) Line 108 unused\n   b) One caveat in removing the per key ProducerRequestPurgatory stats is\n   if there is a key that has an intermittently slow follower you won't be\n   able to narrow it down very easily (since the entire request will\n   expire). OTOH you will have that stat available from the follower - it's\n   just that you will need to \"search\" for the follower that is causing the\n   expirations. So I think it's fine to remove it as it makes the code a lot\n   simpler.\n\n9) Pool.scala: good idea.\n", "It's great to see a patch that fixes metrics. \n\n1. Partition\nIn the isUnderReplicated API, shouldn't the in sync replicas size be compared to the replication factor for that partition and not the default replciation factor ?\n\n2. ZookeeperConsumerConnector\nThere are a bunch of interesting metrics here that are very useful while troubleshooting. For example,\n2.1 Queue size per topic and consumer thread id: When the consumer client's processing slows down, the consumer's queues back up. Currently, to troubleshoot this issue, we need to take thread dumps. If we had the right monitoring on the\nconsumer, we could just look at the metrics to figure out the problem.\n2.2 Fetch requests per second per fetcher: Useful to know the progress of the fetcher thread. In this, the bean might probably be named after the broker id that the fetcher is connected to, somewhere along the lines of per key purgatory metrics.\n \n3. KafkaController\n3.1 We need a way to tell if a partition is offline. If all replicas of a partition go offline, no leader can be elected for that partition and an alert would have to be raised.\n3.2 We also need to be able to measure - \n3.2.1 leader election latency\n3.2.2 Leader election rate \n\n4. ReplicaManager\n4.1 Rename ISRExpandRate to isrExpandRate\n4.2 Rename ISRShrinkRate to isrShrinkRate\n4.3 I'm not sure how useful it is to have a count for leaders and under replicated partitions. We however, do need a per partition status that tells if the partition is offline or under replicated.\n\n5. TopicMetadataTest\nWrap the long line\n\n6. system_test\nWe have to add the new metrics to the metrics.json file to that we can view the metrics on every test run. Not sure if you want to push that to a separate JIRA or not ?\n", "Attache patch v2. Overview of additional changes.\n\n1. rebased.\n\n2. DefaultEventHandler: Added a metric for serlializationErrorRate. Also changed the serialization error handling a bit depending on whether the send is sync or async.\n\n3. Use the following convention to distinguish AllTopic metrics data and the per topic one: for metrics X, AllTopics => AllTopicsX; topic Y => Y-X.\n\n4. Made a pass of metrics names and tried to keep them consistent.\n\n5. Updated metrics.json with the new metrics. Right now we start a separate jmx tool for each jmx bean. Too many beans will slow down the system test. So, I only exposed a subset of the metrics. Once the jmx tool issue is resolved, we can add more beans for collection and graphing. \n\nReview comments:\nJoel:\n4) Consumer lag is the most useful metrics, on which an alert can be set. LogEndOffset and ConsumerOffset are less useful and can be obtained from tools. \n\n5) The problem is that we only know the size of a message after it is serialized, since message itself can be of any type.\n\n8) If a follower is slow, it will be eventually dropped out of ISR and we have metrics on both ISRShrink rate and underreplicated partitions to track this.\n\nNeha:\n1. For that, the broker needs to know the replication factor of a partition. This information needs to be sent from broker on LeaderAndISRRequest. I will leave that in kafka-340.\n\n4.3 LeaderCount is useful to see if client loads are balanced among brokers and a global \"under replicated partition count\" is convenient for setting up a alert (otherwise, one has to do that on each partition).\n\nThe rest of the review comments are all addressed.\n", "+1 on v2.\n\nThe following are all minor comments.\n\n2.1 - For the TODO in Partition.scala, can you add a comment in the jira\n  where it will be addressed (or create a separate jira) so we don't lose\n  track of it.\n\n2.2 - SimpleConsumer.scala: FetchRequestRateAndTimeMs ->\n  FetchRequestRateAndDurationMs. Similar edits in FileMessageSet.scala for\n  LogFlushRateAndTimeMs; SyncProducer.scala for\n  ProduceRequestRateAndTimeMs\n\n2.3 - ProducerTopicStat.scala: resents -> resends\n\n2.4 - DefaultEventHandler.scala:\n      a - val isSync = \"sync\".equals(config.producerType)\n      b - So exceptions are no longer thrown for async producers on a\n        serialization error. This will have an impact on KAFKA-496. Can you\n        add a comment there after check-in?\n\n2.5 - ReplicaManager: the meter name is inconsistent with the convention\n  used elsewhere.\n\n", "Thanks for the review. Committed to 0.8.\n\n2.1 created kafka-510 to track it.\n\n2.2 left the name as it is since it's shorter\n\nAddressed the rest of the comments."], "derived": {"summary": "Currently metrics in kafka are using old-school JMX directly. This makes adding metrics a pain.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Improve Kafka internal metrics - Currently metrics in kafka are using old-school JMX directly. This makes adding metrics a pain."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for the review. Committed to 0.8.\n\n2.1 created kafka-510 to track it.\n\n2.2 left the name as it is since it's shorter\n\nAddressed the rest of the comments."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-204", "title": "BoundedByteBufferReceive hides OutOfMemoryError", "status": "Resolved", "priority": "Critical", "reporter": "Chris Burroughs", "assignee": "Chris Burroughs", "labels": [], "created": "2011-11-15T16:31:36.000+0000", "updated": "2011-11-23T23:46:49.000+0000", "description": "  private def byteBufferAllocate(size: Int): ByteBuffer = {\n    var buffer: ByteBuffer = null\n    try {\n      buffer = ByteBuffer.allocate(size)\n    }\n    catch {\n      case e: OutOfMemoryError =>\n        throw new RuntimeException(\"OOME with size \" + size, e)\n      case e2 =>\n        throw e2\n    }\n    buffer\n  }\n\nThis hides the fact that an Error occurred, and will likely result in some log handler printing a message, instead of exiting with non-zero status.  Knowing how large the allocation was that caused an OOM is really nice, so I'd suggest logging in byteBufferAllocate and then re-throwing OutOfMemoryError", "comments": ["Uh...does anyone know what the motivation of this was originally? Catching OutOfMemoryError is a bit unorthodox...", "Agree with Jay - if you get an OOME all bets are off.  Best to just exit.", "Tiny patch for this one case, created KAFKA-205 to follow up.", "The main reason this was added is to show the requested size and the caller that triggered such a request. It would be nice if both pieces are logged together. With the new patch, those two pieces are logged separately (although should be close) and someone has to link them together manually.", "Another possibility is to rethrow a new RuntimeException with OOME wrapped as the cause.", "If you rethrow, then rethrow a new OOME with the original OOME wrapped.", "Hmm, it doesn't look like OutOfMemoryError allow one to specify a cause during initialization.", "I understand the intent, but the important thing here is not to swallow the OOM exception, right? I mean once you hit that all bets are off, you need to restart your process...basically I think we shouldn't be messing with that.", "We have request limits in the server and consumer that should provide protection against this so i think that is the appropriate way to handle it. If it does happen I think we should just break everything and then the person running things should set the configs correctly to limit the max request size the server will accept and the max fetch size for the client.", "> I mean once you hit that all bets are off, you need to restart your process...basically I think we shouldn't be messing with that. \n\nYeah, the most important thing to do is get out of the way and let the process exit with a non-zero status code.\n\nSo the options as I see it are:\n (1) Do something ugly (like pass the original fetch request to byteBufferAllocate) for the purposes of a valiant but possible futile logging attempt (there is no guarantee we will be able to allocate the logging Strings we are already asking for, everything we ad makes that less likely).\n (2)  Just rethrow e after a logging attempt in byteBufferAllocate.\n\nMy preference is (2), but if someone prefers (1) that's a reasonable trade off.", "Ok, I am fine with the patch then. Any objection to commit it?", "+1", "Just committed this."], "derived": {"summary": "private def byteBufferAllocate(size: Int): ByteBuffer = {\n    var buffer: ByteBuffer = null\n    try {\n      buffer = ByteBuffer. allocate(size)\n    }\n    catch {\n      case e: OutOfMemoryError =>\n        throw new RuntimeException(\"OOME with size \" + size, e)\n      case e2 =>\n        throw e2\n    }\n    buffer\n  }\n\nThis hides the fact that an Error occurred, and will likely result in some log handler printing a message, instead of exiting with non-zero status.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "BoundedByteBufferReceive hides OutOfMemoryError - private def byteBufferAllocate(size: Int): ByteBuffer = {\n    var buffer: ByteBuffer = null\n    try {\n      buffer = ByteBuffer. allocate(size)\n    }\n    catch {\n      case e: OutOfMemoryError =>\n        throw new RuntimeException(\"OOME with size \" + size, e)\n      case e2 =>\n        throw e2\n    }\n    buffer\n  }\n\nThis hides the fact that an Error occurred, and will likely result in some log handler printing a message, instead of exiting with non-zero status."}, {"q": "What updates or decisions were made in the discussion?", "a": "Just committed this."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-205", "title": "Audit swallowing of exceptions/throwables", "status": "Resolved", "priority": "Major", "reporter": "Chris Burroughs", "assignee": null, "labels": ["newbie"], "created": "2011-11-16T14:15:58.000+0000", "updated": "2020-05-27T20:55:32.000+0000", "description": "There are several cases like KAFKA-204 where we swallow Error, or even Throwable.  This is probably not the right thing to do (is java.lang.StackOverflowError\never recoverable?).  ", "comments": ["Chris,\n\nDo you think we still have this problem in our code ? ", "Still a need for this audit?", "Closing this old ticket as abandoned."], "derived": {"summary": "There are several cases like KAFKA-204 where we swallow Error, or even Throwable. This is probably not the right thing to do (is java.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Audit swallowing of exceptions/throwables - There are several cases like KAFKA-204 where we swallow Error, or even Throwable. This is probably not the right thing to do (is java."}, {"q": "What updates or decisions were made in the discussion?", "a": "Closing this old ticket as abandoned."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-206", "title": "no DISCLAIMER, NOTICE needs cleanup", "status": "Resolved", "priority": "Major", "reporter": "Chris Burroughs", "assignee": "Chris Burroughs", "labels": [], "created": "2011-11-16T14:18:57.000+0000", "updated": "2011-12-01T23:36:54.000+0000", "description": "Followup from the incubator vote.  We need a DISCLAIMER and to clean up the notice.\n\nhttp://mail-archives.apache.org/mod_mbox/incubator-general/201111.mbox/%3CCAOGo0VbZd23mxtVFMCuMkHN9fVhiekBUg1x7mxtH7oAzqgp9mQ%40mail.gmail.com%3E", "comments": ["Still not sure which things actually need to be mentioned in the NOTICE.  Other projects don't seem to be entirely consistent, or at least the pattern is lost on me.", "Thanks Chris for the original patch. I modified the NOTICE file to include sbt", "+1", "Thanks for the patch Chris ! Just committed this to publish a new RC "], "derived": {"summary": "Followup from the incubator vote. We need a DISCLAIMER and to clean up the notice.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "no DISCLAIMER, NOTICE needs cleanup - Followup from the incubator vote. We need a DISCLAIMER and to clean up the notice."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for the patch Chris ! Just committed this to publish a new RC"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-207", "title": "AsyncProducerStats is not a singleton", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Jun Rao", "labels": [], "created": "2011-11-16T20:52:28.000+0000", "updated": "2011-11-22T00:58:51.000+0000", "description": "AsyncProducerStats is not a singleton. This means that if a client instantiates multiple producers, the stat is collected for only 1 instance, instead of all instances.", "comments": ["Make AsyncProducerStats a singleton that tracks dropped events from all AsyncProducer instances. Add a separate jmx bean that reports the queue size for each AsyncProducer instance.", "+1"], "derived": {"summary": "AsyncProducerStats is not a singleton. This means that if a client instantiates multiple producers, the stat is collected for only 1 instance, instead of all instances.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "AsyncProducerStats is not a singleton - AsyncProducerStats is not a singleton. This means that if a client instantiates multiple producers, the stat is collected for only 1 instance, instead of all instances."}, {"q": "What updates or decisions were made in the discussion?", "a": "Make AsyncProducerStats a singleton that tracks dropped events from all AsyncProducer instances. Add a separate jmx bean that reports the queue size for each AsyncProducer instance."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-208", "title": "Efficient polling mechanism for many topics", "status": "Resolved", "priority": "Major", "reporter": "Taylor Gautier", "assignee": null, "labels": [], "created": "2011-11-17T01:05:36.000+0000", "updated": "2013-02-20T17:27:37.000+0000", "description": "Currently, the way to poll many topics is to submit a request for each one in turn, and read the responses.  Especially if there are few messages delivered on many topics, the network overhead to implement this scheme can far outweigh the bandwidth of the actual messages delivered.\n\nEffectively, for topics A, B, C the request/response scheme is the following:\n\n-> Request A offset a\n-> Request B offset b\n-> Request C offset c\n<- no messages\n<- 1 message offset b\n<- no messages\n-> Request A offset a\n-> Request B offset b'\n-> Request C offset c\n<- no messages\n<- no messages\n<- no messages\netc.\n\nI propose a more efficient mechanism which works a bit like epoll in that the client can register interest in a particular topic.  There are many options for the implementation, but the one I suggest goes like so:\n\n-> Register interest A offset a\n-> Register interest B offset b\n-> Register interest C offset c\n-> Next message (null)\n<- messages for B (1 message)\n-> Next message (topic B offset b')\n<- no messages\n-> Unregister Interest C\n...\n\nIt is possible to implement the \"Next Message\" request as either blocking or non-blocking.  I suggest that the request format include space for the timeout, which if set to 0 will be a nonblocking response, and if set to anything other than 0, will block for at most timeout ms. ", "comments": ["Hey Taylor how does this relate to KAFKA-48 and KAFKA-170? I think you are proposing something slightly different but I am not sure the exact relationship.", "Hi Jay.  Good question.\n\nKAFKA-48 in its current description would allow a long poll for one topic at a time.  If a client is interested in only a few topics, it is conceivable that the client could use one TCP connection per topic, and issue a blocking request for each topic it is interested in, one topic per TCP connection.  But this strategy rapidly becomes undesirable as the number of topics one is interested in increases.\n\nAs we discussed in the comments for KAFKA-48 it's possible to consider the use case for multi-fetch and long polling together.  However there wasn't a conclusion to that discussion in terms of a direction.  The proposal that we discussed would allow for an interest set to be requested, and a long poll to then take place over all of the topics.  This is a reasonable solution to the problem, but it's not really going to address my needs - and thus why I submitted this feature request.\n\nThe reasoning is simple - if I have 10,000 topics I am interested in, and only a handful of those topics get messages, then KAFKA-48 is not going to address my signal to noise problem.  I will still have to submit - rather poll - for 10,000 topics and then get a few messages at a time - this means I have something like - depending on the numbers - 10kx20 bytes (assuming average topic name length of 20) 200kbyte requests which maybe return a handful of messages - lets say 10 at at time averaging say 200 bytes which means 200kbytes to receive 2kbytes or a 1:200 signal to noise ratio.  Ack.\n\nKAFKA-170 only addresses the issue of non-blocking consumption in the consumer which is an implementation issue in the client only.  I know that KAFKA-170 can be done in the client using the current TCP protocol because I've already done it using a NodeJS client.\n\n", "All that being said, if you'd like to combine KAFKA-48 and KAFKA-208 together I am fine with that - as long as we address the network efficiency issue at the same time in KAFKA-48.", "Jay,\n\nI think I just realized how the solution you proposed in KAFKA-48 can be used efficiently for my use case.  I suppose this was the solution you had proposed but maybe I did not understand it fully.  \n\nIf I understand it right, your proposal is that long poll requests can be pipelined in the server, and once they are satisified they do not stick around, they should be polled again.  So to use my example from above, the way it would work would be:\n\n-> request messages topic A offset a\n-> request messages topic B offset b\n-> request messages topic C offset c\n(time passes)\n<- messages topic B \n-> request messages topic B offset b'\n(time passes)\n-> request messages topic D offset D\n(time passes)\n<- messages topic C\n-> request messages topic C offset c'\n\netc.\n\nIf this is the vision - then let's close this topic and fold it into KAFKA-48 - I think this will suffice for everything I need.\n", "Yes, but I was actually hoping to even do a multifetch version. So it would be\n\nrequest A:a, B:b, C:c, timeout:500ms\nimmediately get data:B:b', C:c'\nrequest A:a, B:b', C:c', timeout:500ms\nno data for any topics, request blocks\nmessage on C\nget data for C:c''\nrequest A:a, B:b', C:c'', timeout:500ms\n\nEtc.\n\nI think this multifetch approach simplifies things for the consumer implementation. So I think that might cover what you want, I think the primary differences is that there is no separate registration of interest and polling for data, which i think makes sense.\n\nThe only question I think was left open was whether, in addition to a timeout, we could also support a min_bytes parameter so that the response would wait for at least that much data to accumulate. This is slightly more complex to implement on the server side but could improve efficiency.", "Hmm,\n\nYour version seems that it would be inefficient for large # of topics with a (relative to the overall #) small # of topics that receive messages.  The version I posted is much better in this regard, the amount of data actually transmitted is relative not to the # of topics requested, but to the # that receive traffic.\n\nI don't think my version is very hard to implement at all, in fact it would be some very small changes to the current code I have running in Node (which admittedly is event based and a natural fit for async behavior)\n\nI don't personally have any current need for min bytes nor does it seem that I would need it in upcoming use cases.", "Ah, I see, now. So if I understand your need you actually have a very large number of topics (say a thousand) so the reason for splitting the register and poll calls is to avoid sending the topic names each time. I think the problem with a separate register call is how to manage that metadata on the server. I would be concerned about something that had to be manually deallocated because it is possible for the client to leak server resources. Presumably with enough support it could somehow be tied to the life of the socket that created it...so maybe each socket could only register at most one interest set and that would be deallocated if that socket closed.", "That's right :)  And yes, I was assuming that a single socket would retain an interest set and de-allocate it on close.\n\nHowever, it might be easier to use my other proposal most recently stated.   It just builds on the existing polling mechanism but allows for requests to be blocking.  \n\nTo implement it, responses have to be able to come back out of order compared to the requests order, and that requires that the response must contain either a request id, or the topic name (but this is pretty much required no matter what we do).  \n\nIn this case, the client simply re-polls for any responses that are received (or any new topics, if the client so desires)\n\nSo to re-iterate and add a timeout parameter to illustrate how that also works (where C and S are client and server):\n\nC->S fetch messages topic A offset a timeout 500\nC->S fetch messages topic B offset b timeout 500\nC->S request messages topic C offset c timeout 500\n(250 ms passes, server receives 3 messages for topic B) \nC<-S topic B messages size b' (3) messages\nC->S request messages topic B offset (b+b') timeout 500\n(200 ms passes) \nC->S request messages topic D offset d timeout 500\n(50 ms passes, no messages received for A or C) \nC<-S topic A messages size 0 (0) messages\nC<-S topic C messages size 0 (0) messages\nC->S fetch messages topic A offset a timeout 500\nC->S request messages topic C offset c timeout 500\n\n\n... and so on.\n\nBoth of these solutions are pretty similar to one another, in the above case there is still pretty much an \"interest set\" however it's implementation might differ in that it might be considered as outstanding requests. \n\nWhat I like about the above solution is that there is no magic going on at the server side.  In the registered interests implementation, the server also has to keep the current offset of the registered interest set to continue advancing it on the client's behalf, and (other than the automatic storage of the offset in ZK by the high level consumer) this idea goes against the design principles of Kafka as I understand them (move the tracking of offsets to the client to make Kafka simple).\n\n", "Btw, much like KAFKA-101, I may need to implement this in a very short time.", "Polling for many topics in one request seems to be handled by the new 0.8 protocol. Should this bug be closed?", "This is fixed in the new 0.8 protocol \nhttps://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol"], "derived": {"summary": "Currently, the way to poll many topics is to submit a request for each one in turn, and read the responses. Especially if there are few messages delivered on many topics, the network overhead to implement this scheme can far outweigh the bandwidth of the actual messages delivered.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Efficient polling mechanism for many topics - Currently, the way to poll many topics is to submit a request for each one in turn, and read the responses. Especially if there are few messages delivered on many topics, the network overhead to implement this scheme can far outweigh the bandwidth of the actual messages delivered."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is fixed in the new 0.8 protocol \nhttps://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-209", "title": "Remove empty directory when no log segments remain", "status": "Resolved", "priority": "Major", "reporter": "Taylor Gautier", "assignee": null, "labels": [], "created": "2011-11-18T04:59:03.000+0000", "updated": "2012-01-03T20:28:12.000+0000", "description": "When the log cleaner runs it deletes segments from the log directory.  However, if all the segments are cleaned, the log directory itself is retained.  There doesn't seem to be any need for this as the directory will be re-created on write if necessary.\n\nThis JIRA would improve the log cleaner to also remove unused topic directories from the log directory.", "comments": ["Actually, when all log segments are deleted, we automatically create an empty log file whose name is the latest offset. So the log directory will never be empty.", "This is something that started happening after 0.6?  It doesn't seem to be this way in 0.6.  But it would entirely make sense.", "It should behave that way in both 0.6 and 0.7. If not, it's a bug.", "It definitely does not in 0.6.  Don't know about 0.7, haven't tried it.  Now you see the source of my discussion on the mailing list ;-)", "Taylor,\n\nDo you see this behavior with trunk or the 0.7 branch ? ", "No, we have upgraded to 0.7 and this behavior does not exist in 0.7"], "derived": {"summary": "When the log cleaner runs it deletes segments from the log directory. However, if all the segments are cleaned, the log directory itself is retained.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Remove empty directory when no log segments remain - When the log cleaner runs it deletes segments from the log directory. However, if all the segments are cleaned, the log directory itself is retained."}, {"q": "What updates or decisions were made in the discussion?", "a": "No, we have upgraded to 0.7 and this behavior does not exist in 0.7"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-210", "title": "javaapi ZookeeperConsumerConnectorTest duplicates many tests in the scala version", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Jun Rao", "labels": [], "created": "2011-11-21T04:43:38.000+0000", "updated": "2011-12-05T20:28:03.000+0000", "description": "Since javaapi.ZookeeperConsumerConnector is just a thin wrapper over the scala version, we only need to test the basic functionality there.", "comments": ["Remove unnecessary duplicated test cases.", "Is this approach better than trying to abstract over the actual operations and having the test work against the abstraction? E.g. have BaseConsumerConnectorTest and have JavaConsumerConnectorTest and ScalaConsumerConnectorTest extend that to run the same operations against both clients. Or are you saying since we already test this stuff on the scala consumer there is no point doing the same thing on the java one since it is just a wrapper?", "Yes, I am saying that there is no need to stress test the javaapi since it's just a wrapper. All we need to test is that the java <-> scala conversion works. So one basic test should be enough for this purpose.", "Sounds good. +1", "committed"], "derived": {"summary": "Since javaapi. ZookeeperConsumerConnector is just a thin wrapper over the scala version, we only need to test the basic functionality there.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "javaapi ZookeeperConsumerConnectorTest duplicates many tests in the scala version - Since javaapi. ZookeeperConsumerConnector is just a thin wrapper over the scala version, we only need to test the basic functionality there."}, {"q": "What updates or decisions were made in the discussion?", "a": "Sounds good. +1"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-211", "title": "Fix LICENSE file to include MIT and SCALA license", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "labels": [], "created": "2011-11-21T19:36:46.000+0000", "updated": "2011-11-22T18:24:37.000+0000", "description": "See here for reference - http://markmail.org/search/?q=kafka+0.7.0#query:kafka%200.7.0%20list%3Aorg.apache.incubator.general+page:1+mid:fiatvda3uyx2lbzb+state:results\n\nLooks like the LICENSE file should include MIT and SCALA license too", "comments": ["+1", "Committed this."], "derived": {"summary": "See here for reference - http://markmail. org/search/?q=kafka+0.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Fix LICENSE file to include MIT and SCALA license - See here for reference - http://markmail. org/search/?q=kafka+0."}, {"q": "What updates or decisions were made in the discussion?", "a": "Committed this."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-212", "title": "IllegalThreadStateException in topic watcher for Kafka mirroring", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "labels": [], "created": "2011-11-23T19:43:26.000+0000", "updated": "2011-12-01T01:04:02.000+0000", "description": "If the kafka mirroring embedded consumer receives a new topic watcher notification, it runs into the following exception \n\n[2011-11-23 02:49:15,612] FATAL java.lang.IllegalThreadStateException (kafka.consumer.ZookeeperTopicEventWatcher)\n[2011-11-23 02:49:15,612] FATAL java.lang.IllegalThreadStateException\n        at java.lang.Thread.start(Thread.java:595)\n        at kafka.server.EmbeddedConsumer$$anonfun$startNewConsumerThreads$3.apply(KafkaServerStartable.scala:142)\n        at kafka.server.EmbeddedConsumer$$anonfun$startNewConsumerThreads$3.apply(KafkaServerStartable.scala:142)\n        at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)\n        at scala.collection.immutable.List.foreach(List.scala:45)\n        at kafka.server.EmbeddedConsumer.startNewConsumerThreads(KafkaServerStartable.scala:142)\n        at kafka.server.EmbeddedConsumer.handleTopicEvent(KafkaServerStartable.scala:109)\n        at kafka.consumer.ZookeeperTopicEventWatcher$ZkTopicEventListener.liftedTree2$1(ZookeeperTopicEventWatcher.scala:83)\n        at kafka.consumer.ZookeeperTopicEventWatcher$ZkTopicEventListener.handleChildChange(ZookeeperTopicEventWatcher.scala:78)\n        at org.I0Itec.zkclient.ZkClient$7.run(ZkClient.java:568)\n        at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)\n (kafka.consumer.ZookeeperTopicEventWatcher)\n\nThis happens since it tries to start a thread which has finished executing", "comments": ["This patch clears the threadList that holds the older thread references, before adding newer threads to it. This avoids trying to start an already finished thread, thus avoiding IllegalThreadStateException ", "Any thread that doesn't shut down cleanly will leak, is that a problem? Can that happen?", "the thread shutdown is guarded by a countdown latch. It will finish shutdown only after the thread exists the run() method. The problem here is that we keep older shutdown thread references in that list and end up calling start on those, which leads to the exception", "Can somebody help review this ?", "I don't think that answers my question, though, which is how do we know if we are leaking threads? I guess the patch doesn't make it better or worse, since we definitely don't want to keep them in the list, but can you assess what happens if shutdown fails? Can that happen? Do we log it? Or is there a guarantee that the thread must shutdown in some bounded period of time?", ">> which is how do we know if we are leaking threads? \n\nGood question. From what I see, the entire thread run() method is guarded by a try-catch-finally block. In the finally block, we count down the latch. So if the thread itself runs into some exception/error, it will exit the run() method and shut itself down. The other case of thread shutdown is when the mirroring thread itself calls the shutdown API. Here, we wait until the current producer send operation succeeds to count down the latch. In both cases, I don't see how we can leak threads\n\n>> I guess the patch doesn't make it better or worse, since we definitely don't want to keep them in the list, but can you assess what happens if shutdown fails?\n\nNot true. The patch fixes the bug filed here. Your concerns are about the shutdown logic of the thread, which if you suspect is a bug, can go in a separate JIRA.\n\n>> Do we log it? \n\nYes, in any case of shutdown, it gets logged as a FATAL error.\n\n>> Or is there a guarantee that the thread must shutdown in some bounded period of time? \n\nMaybe. If the producer send operation hangs indefinitely, which is a serious bug in the producer send logic.\n", "We shouldn't be leaking threads. If we can get to the code that creates new MirrorThreads, the old threads should have finished since shutdown is blocking. If the shutdown blocks forever, we won't be able to create new threads. Again, there is no thread leak. Although the latter would suggest another serious bug somewhere else.\n\n+1 on the patch.", "Fix is committed"], "derived": {"summary": "If the kafka mirroring embedded consumer receives a new topic watcher notification, it runs into the following exception \n\n[2011-11-23 02:49:15,612] FATAL java. lang.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "IllegalThreadStateException in topic watcher for Kafka mirroring - If the kafka mirroring embedded consumer receives a new topic watcher notification, it runs into the following exception \n\n[2011-11-23 02:49:15,612] FATAL java. lang."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fix is committed"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-213", "title": "make # of consumer rebalance retries configurable", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": null, "labels": [], "created": "2011-11-24T01:06:16.000+0000", "updated": "2011-11-28T21:56:36.000+0000", "description": null, "comments": ["Patch attached.", "A couple of comments -\n\n1. There is a typo in the name of the config - \"rebalance,retries.max\"\n2. There is a typo in the log message (from existing code, but we might as well fix it now) - \"retires\". Lets fix it to \"retries\"\n\n", "Committed after fixing the typo."], "derived": {"summary": "", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "make # of consumer rebalance retries configurable"}, {"q": "What updates or decisions were made in the discussion?", "a": "Committed after fixing the typo."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-214", "title": "SyncProducer should log host and port if can't connect.", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Jun Rao", "labels": [], "created": "2011-11-28T18:44:48.000+0000", "updated": "2011-11-28T21:56:56.000+0000", "description": null, "comments": ["patch attached.", "+1", "committed."], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "SyncProducer should log host and port if can't connect."}, {"q": "What updates or decisions were made in the discussion?", "a": "patch attached."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-215", "title": "Improve system tests for the mirroring code", "status": "Closed", "priority": "Critical", "reporter": "Neha Narkhede", "assignee": null, "labels": ["newbie"], "created": "2011-11-28T19:24:43.000+0000", "updated": "2014-06-19T05:15:33.000+0000", "description": "Improve the system tests for the mirroring code to *add* the following testing scenarios -\n\n1. Bounce the mirroring kafka cluster during the test\n2. Add new topics to the source kafka cluster while the mirror is copying data for existing topics\n3. Bounce the source Kafka cluster during the test\n\nThe point of this improvement is to catch any bugs in the consumer rebalancing or the detection of new topics logic. Also, verification code of this part of the system test can report on the % of duplicates/ % data loss in the mirror cluster. Since Kafka guarantees at least once delivery, small percentage of duplicates is ok, but any amount of data loss is a critical bug", "comments": ["Should be covered by other jiras in 0.8", "We probably need a JIRA to merge the existing broker_failure and mirroring system test. Do we have another JIRA or should we use this one ?"], "derived": {"summary": "Improve the system tests for the mirroring code to *add* the following testing scenarios -\n\n1. Bounce the mirroring kafka cluster during the test\n2.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Improve system tests for the mirroring code - Improve the system tests for the mirroring code to *add* the following testing scenarios -\n\n1. Bounce the mirroring kafka cluster during the test\n2."}, {"q": "What updates or decisions were made in the discussion?", "a": "We probably need a JIRA to merge the existing broker_failure and mirroring system test. Do we have another JIRA or should we use this one ?"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-216", "title": "Add nunit license to the NOTICE file", "status": "Resolved", "priority": "Blocker", "reporter": "Neha Narkhede", "assignee": "Jakob Homan", "labels": [], "created": "2011-11-29T03:20:19.000+0000", "updated": "2011-11-30T23:43:53.000+0000", "description": "According to yet some more feedback from general@, we need to add NUnit (http://www.nunit.org/) to the NOTICE file.", "comments": ["Patch adds nunit verbage to NOTICE and text of zlib/libpng to LICENSE.", "Could you add something like \"For Nunit used in clients/foo/bar\" to the LICENSE text so this is easier to make sense of?  Otherwise looks good to me.", "Yeah, I think it will be useful to add \"For Nunit used in clients/csharp\" to the LICENSE file.", "Updated with new nunit verbiage.", "+1. ", "This should be double committed to the 0.7 branch, in addition to trunk", "Committed to trunk and 0.7.  Resolving as fixed."], "derived": {"summary": "According to yet some more feedback from general@, we need to add NUnit (http://www. nunit.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add nunit license to the NOTICE file - According to yet some more feedback from general@, we need to add NUnit (http://www. nunit."}, {"q": "What updates or decisions were made in the discussion?", "a": "Committed to trunk and 0.7.  Resolving as fixed."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-217", "title": "Client test suite", "status": "Resolved", "priority": "Major", "reporter": "Jay Kreps", "assignee": null, "labels": [], "created": "2011-11-29T18:41:53.000+0000", "updated": "2019-02-04T10:36:42.000+0000", "description": "It would be great to get a comprehensive test suite that we could run against clients to certify them.\n\nThe first step here would be work out a design approach that makes it easy to certify the correctness of a client.", "comments": ["Thread is here: http://mail-archives.apache.org/mod_mbox/incubator-kafka-dev/201111.mbox/%3CCAOeJiJjRHB2c1T-OCre-ZwpDhg+HkVBo2TMyVaYKXsTvEL9PnQ@mail.gmail.com%3E\n\nSome thoughts:\n- It would be good to break out specific features to get a level of compliance: producer, compression types, consumer, etc\n- It would be good to test clients against various protocol versions to test forwards and backwards compatibility\n", "I'd like to see packet dumps of all the message types (from all versions) so that they can be used int unit tests of the clients and with the packet dumps that definition should include information about what to expect inside the data once it has been parsed, for example the CRC value, the value of a message that is compressed.  Then the implementation can assert that it is in fact parsing and generating the packets correctly.  These could be encapsulated into a test harness or test mode of the broker that the client could run a full integration test against as well. \n\n\n", "I like the packet dumps idea from a simplicity point of view. We could have a set of YAML files that represent types of compliance (e.g. simpleconsumer_0.7.yaml), maybe even put the message data in fields as base64 encoded strings. We don't have to worry about invoking clients on the command line -- each client would just know to read the file and the file would specify what the raw data is and the intended results. It also saves us from having to bring up older versions of Kafka to live test against.\n\nThat being said, is there any simple way to test a ZooKeeper Consumer/Producer? The only thing I can think of is to specify a standard command-line interface that clients need to implement, and then have a test script that spins up ZooKeeper and a few Kafka brokers, calls these client commands, and then inspects the output and the ZK state.", "While I fully appreciate the idea of having formal test procedures to validate client implementations I am doubtful, whether this is something that is feasible to implement.Â \r\nSince this ticket has been around for a long time without any activity (more than 7 years) I think we could close this.", "I'll close this for now since no-one objected here or on the mailing list. If we decide to do something like this later on we can always reopen or create a new issue."], "derived": {"summary": "It would be great to get a comprehensive test suite that we could run against clients to certify them. The first step here would be work out a design approach that makes it easy to certify the correctness of a client.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Client test suite - It would be great to get a comprehensive test suite that we could run against clients to certify them. The first step here would be work out a design approach that makes it easy to certify the correctness of a client."}, {"q": "What updates or decisions were made in the discussion?", "a": "I'll close this for now since no-one objected here or on the mailing list. If we decide to do something like this later on we can always reopen or create a new issue."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-218", "title": "ZOOKEEPER-961 is nasty, upgrade to zk 3.3.4", "status": "Closed", "priority": "Critical", "reporter": "Chris Burroughs", "assignee": "Pierre-Yves Ritschard", "labels": ["newbie"], "created": "2011-11-29T20:40:26.000+0000", "updated": "2014-06-19T05:15:41.000+0000", "description": "3.3.4 is out with ZOOKEEPER-961, which I think is our most reported issue.\n\nhttp://www.cloudera.com/blog/2011/11/apache-zookeeper-3-3-4-has-been-released/\n\nShould be a one char changes, but the jar hasn't hit the maven repos yet.", "comments": ["it has now, here's the patch", "+1. Thanks for the patch !", "Committed this patch"], "derived": {"summary": "3. 3.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "ZOOKEEPER-961 is nasty, upgrade to zk 3.3.4 - 3. 3."}, {"q": "What updates or decisions were made in the discussion?", "a": "Committed this patch"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-219", "title": "NOTICE file should be minimal", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Jakob Homan", "labels": [], "created": "2011-11-29T22:23:18.000+0000", "updated": "2012-01-03T19:12:57.000+0000", "description": "Context of filing this bug is here - http://markmail.org/message/cvyzt3hrc74wsyo6?q=NOTICE+minimal+list:org%2Eapache%2Eincubator%2Egeneral\n\n", "comments": ["This has been resolved in 0.7 release"], "derived": {"summary": "Context of filing this bug is here - http://markmail. org/message/cvyzt3hrc74wsyo6?q=NOTICE+minimal+list:org%2Eapache%2Eincubator%2Egeneral.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "NOTICE file should be minimal - Context of filing this bug is here - http://markmail. org/message/cvyzt3hrc74wsyo6?q=NOTICE+minimal+list:org%2Eapache%2Eincubator%2Egeneral."}, {"q": "What updates or decisions were made in the discussion?", "a": "This has been resolved in 0.7 release"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-220", "title": "LogManager test fails on linux", "status": "Closed", "priority": "Critical", "reporter": "Neha Narkhede", "assignee": "Jun Rao", "labels": ["newbie"], "created": "2011-12-01T01:45:34.000+0000", "updated": "2014-06-19T05:15:47.000+0000", "description": "On Linux, LogManagerTest fails on each and every run\n[info] Test Starting: testCleanupExpiredSegments\n[error] Test Failed: testCleanupExpiredSegments\njunit.framework.AssertionFailedError: Now there should only be only one segment. expected:<1> but was:<12>\n        at junit.framework.Assert.fail(Assert.java:47)\n        at junit.framework.Assert.failNotEquals(Assert.java:277)\n        at junit.framework.Assert.assertEquals(Assert.java:64)\n        at junit.framework.Assert.assertEquals(Assert.java:195)\n        at kafka.log.LogManagerTest.testCleanupExpiredSegments(LogManagerTest.scala:87)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.junit.internal.runners.TestMethodRunner.executeMethodBody(TestMethodRunner.java:99)\n        at org.junit.internal.runners.TestMethodRunner.runUnprotected(TestMethodRunner.java:81)\n        at org.junit.internal.runners.BeforeAndAfterRunner.runProtected(BeforeAndAfterRunner.java:34)\n        at org.junit.internal.runners.TestMethodRunner.runMethod(TestMethodRunner.java:75)\n        at org.junit.internal.runners.TestMethodRunner.run(TestMethodRunner.java:45)\n        at org.junit.internal.runners.TestClassMethodsRunner.invokeTestMethod(TestClassMethodsRunner.java:71)\n        at org.junit.internal.runners.TestClassMethodsRunner.run(TestClassMethodsRunner.java:35)\n        at org.junit.internal.runners.TestClassRunner$1.runUnprotected(TestClassRunner.java:42)\n        at org.junit.internal.runners.BeforeAndAfterRunner.runProtected(BeforeAndAfterRunner.java:34)\n        at org.junit.internal.runners.TestClassRunner.run(TestClassRunner.java:52)\n        at org.junit.internal.runners.CompositeRunner.run(CompositeRunner.java:29)\n        at org.junit.runner.JUnitCore.run(JUnitCore.java:121)\n        at org.junit.runner.JUnitCore.run(JUnitCore.java:100)\n        at org.junit.runner.JUnitCore.run(JUnitCore.java:91)\n        at org.scalatest.junit.JUnitSuite$class.run(JUnitSuite.scala:261)\n        at kafka.log.LogManagerTest.run(LogManagerTest.scala:28)\n        at org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:40)\n        at sbt.TestRunner.run(TestFramework.scala:53)\n        at sbt.TestRunner.runTest$1(TestFramework.scala:67)\n        at sbt.TestRunner.run(TestFramework.scala:76)\n        at sbt.TestFramework$$anonfun$10$$anonfun$apply$11.runTest$2(TestFramework.scala:194)\n        at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)\n        at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)\n        at sbt.NamedTestTask.run(TestFramework.scala:92)\n        at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)\n        at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)\n        at sbt.TaskManager$Task.invoke(TaskManager.scala:62)\n        at sbt.impl.RunTask.doRun$1(RunTask.scala:77)\n        at sbt.impl.RunTask.runTask(RunTask.scala:85)\n        at sbt.impl.RunTask.sbt$impl$RunTask$$runIfNotRoot(RunTask.scala:60)\n        at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)\n        at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)\n        at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)\n        at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)\n        at sbt.Control$.trapUnit(Control.scala:19)\n        at sbt.Distributor$Run$Worker.run(ParallelRunner.scala:131)\n[info] Test Starting: testCleanupSegmentsToMaintainSize\n\n", "comments": ["Attach a patch by setting lastmodifiedtime manually. Also, tuned flush interval to speed up the test.", "+1", "committed to trunk."], "derived": {"summary": "On Linux, LogManagerTest fails on each and every run\n[info] Test Starting: testCleanupExpiredSegments\n[error] Test Failed: testCleanupExpiredSegments\njunit. framework.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "LogManager test fails on linux - On Linux, LogManagerTest fails on each and every run\n[info] Test Starting: testCleanupExpiredSegments\n[error] Test Failed: testCleanupExpiredSegments\njunit. framework."}, {"q": "What updates or decisions were made in the discussion?", "a": "committed to trunk."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-221", "title": "LICENSE and NOTICE problems in Kafka 0.7", "status": "Resolved", "priority": "Major", "reporter": "Kevan Lee Miller", "assignee": "Jakob Homan", "labels": [], "created": "2011-12-01T21:54:40.000+0000", "updated": "2011-12-15T18:35:02.000+0000", "description": "The source LICENSE file for Kafka is incomplete. The LICENSE file needs to accurately reflect the Kafka source and included artifacts.\n\nSimilarly, the NOTICE file is likely to be missing information. I'll attach a file with some information that I created. It's incomplete and will need additional work...", "comments": ["This is a partial analysis of Kafka LICENSE/NOTICE requirements. I use emacs to look inside jars for license/notice files. if you don't find one, then you need to search for a license for the artifact.\n\nOnce you have this information pulled together, it's not too hard to pull it all into a LICENSE/NOTICE file.\n\nUnfortunately, there's just not a good automated way to generate this information. If it's any comfort, Geronimo has *way* more embedded jar files than Kafka... ;-)", "Patch that re-does the LICENSE and NOTICE file assuming KAFKA-222 goes in.  NOTICE now just has entries for LinkedIn's contribution, zkclient and sbt.  The other remaining jars are from Apache projects and so don't need to be included here (per my understanding).  The License file has Scala, sbt's license (copied from its LICENSE file in the release we use). zkclient is Apache licensed and as far as I can tell, it therefore doesn't need to be included here.  Is this correct?", "I've tried to have a look but i don't have git and can't work out what SVN revision that patch is against so its hard for me to tell what the resultant files will look like, would you be able to attach the complete license and notice files here?\n\nWhat is the reasoning behind keeping the NOTICE file entries for LinkedIn, zkclient and sbt? For example, looking at the sbt license at https://github.com/harrah/xsbt/blob/0.11/LICENSE if you include that complete license text in the Kafka LICENSE file then I don't think there is a need to mention sbt in the Kafka NOTICE.\n\n", "OK, updated based on comments from Ant and the other thread.  NOTICE (new version: http://dl.dropbox.com/u/565949/NOTICE ) only contains the Apache notice based on:\n* as Ant mentioned, the sbt license is included in full in LICENSE so not necessary in NOTICE\n* zkclient is Apache 2.0 licensed (category A), so no need to mention it in LICENSE\nNew LICENSE (new version: http://dl.dropbox.com/u/565949/LICENSE ):\n* Removed Scala license, since we're not distributing the Scala runtime (it's pulled in via sbt)\n* Has sbt license since it is being included and requires inclusion (although I still don't understand why including it in its jar isn't enough to satisfy this condition)\n* Doesn't include anything for zkclient since it's ASL2.0\n* Doesn't include anything for the pig stuff since they're sister ASF projects.\n\nI think this is enough for a source release.\n", "re-submitting patch.", "here's a new version without the final line of dashes in LICENSE.", "+1 on the latest patch.", "I think we missed the nunit entry in the LICENSE and the NOTICE files. We have the nunit.dll checked into the source repository under clients/csharp", "Thanks a bunch for helping out on this patch, Jakob ! \nJust committed this."], "derived": {"summary": "The source LICENSE file for Kafka is incomplete. The LICENSE file needs to accurately reflect the Kafka source and included artifacts.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "LICENSE and NOTICE problems in Kafka 0.7 - The source LICENSE file for Kafka is incomplete. The LICENSE file needs to accurately reflect the Kafka source and included artifacts."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks a bunch for helping out on this patch, Jakob ! \nJust committed this."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-222", "title": "Mavenize contrib", "status": "Resolved", "priority": "Blocker", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "labels": [], "created": "2011-12-05T04:29:53.000+0000", "updated": "2011-12-08T19:08:31.000+0000", "description": "To reduce the overhead of maintaining the NOTICE and LICENSE files, we need to mavenize most checked-in jars. Out of the current checked-in jars, the following ones cannot be mavenized -\n\n1. lib/sbt-launch.jar: This is required to fire up the build system of Kafka (SBT)\n2. core/lib/zkclient-20110412.jar: Kafka uses a patched zkclient jar\n3. contrib/hadoop-consumer/lib/piggybank.jar: This is not available through Maven\n4. contrib/hadoop-consumer/lib/pig-0.8.0-core.jar: This is also not available through Maven\n", "comments": ["This patch mavenizes most jars, except the ones mentioned in the description of the jira", "Don't we need to do the same thing now on Josh's patch? The longer we hold off on that patch the more it will drift.", "Josh's patch has been reviewed. We found that it wasn't created off the latest trunk and has a bug.", "+1. Verified can still test and build after this.  BTW, here's a script to rm away the newly deleted jars from git, since patch application doesn't do that.", "+1"], "derived": {"summary": "To reduce the overhead of maintaining the NOTICE and LICENSE files, we need to mavenize most checked-in jars. Out of the current checked-in jars, the following ones cannot be mavenized -\n\n1.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Mavenize contrib - To reduce the overhead of maintaining the NOTICE and LICENSE files, we need to mavenize most checked-in jars. Out of the current checked-in jars, the following ones cannot be mavenized -\n\n1."}, {"q": "What updates or decisions were made in the discussion?", "a": "+1. Verified can still test and build after this.  BTW, here's a script to rm away the newly deleted jars from git, since patch application doesn't do that."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-223", "title": "Add metadata requests to Kafka broker", "status": "Resolved", "priority": "Minor", "reporter": "David Ormsbee", "assignee": null, "labels": ["features"], "created": "2011-12-06T17:59:35.000+0000", "updated": "2012-12-06T16:24:50.000+0000", "description": "It would be useful to be able to query metadata from Kafka for the purposes of monitoring and discovery:\n\n1. Version: Just a quick way to get the broker version so that we know not to send it a version of a request or response that it can't handle.\n\n2. Topic information: Return the default number of partitions per topic, a list of all topics, number of partitions per topic if there are overrides, and start/end offsets for those topics (assuming it's cheap to do so). This would allow Consumers and Producers to discover topics and partitions on a single broker without involving ZooKeeper, and would also make it easier to know how many partitions can be sent to when making a new topic with a ZK-aware Producer.\n\nDiscussed in thread:\n http://mail-archives.apache.org/mod_mbox/incubator-kafka-dev/201112.mbox/%3CCALStshY02xT7b2n-%3DDjTF_QifEzjxWnCvmgRhSu8v8-ibbSGyQ%40mail.gmail.com%3E\n\n", "comments": ["From Neha's email:\n\nToday, we don't have such APIs. But I was thinking we can have something\nsimilar to the 4-letter commands in zookeeper.\nFor example,\n\necho topics | nc broker-host broker-port      (Returns all topics with\nnumber of partitions on each broker)\necho srvr | nc broker-host broker-port         (Returns basic broker stats\nlike version, number of topics etc)\n", "To clarify, these would be programming API requests added to the wire protocol, alongside FETCH, PRODUCE, and OFFSETS (say VERSION and TOPICS). I didn't really intend this for monitoring purposes.", "We have added an API like this in 0.8. It is described here:\n https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol"], "derived": {"summary": "It would be useful to be able to query metadata from Kafka for the purposes of monitoring and discovery:\n\n1. Version: Just a quick way to get the broker version so that we know not to send it a version of a request or response that it can't handle.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add metadata requests to Kafka broker - It would be useful to be able to query metadata from Kafka for the purposes of monitoring and discovery:\n\n1. Version: Just a quick way to get the broker version so that we know not to send it a version of a request or response that it can't handle."}, {"q": "What updates or decisions were made in the discussion?", "a": "We have added an API like this in 0.8. It is described here:\n https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-224", "title": "Shouw throw exception when serializer.class if not configured for Producer", "status": "Resolved", "priority": "Minor", "reporter": "Stone Gao", "assignee": null, "labels": ["newbie"], "created": "2011-12-08T03:33:40.000+0000", "updated": "2014-11-11T19:24:56.000+0000", "description": "  val props = new Properties();\n  props.put(\"zk.connect\", \"127.0.0.1:2181\");\n  props.put(\"producer.type\", \"async\");\n  props.put(\"batch.size\", \"50\")\n  props.put(\"serializer.class\", \"kafka.serializer.StringEncoder\");\n  props.put(\"compression.codec\", \"1\") //gzip\n  val config = new ProducerConfig(props);\n\nIf remove the serializer.class config : props.put(\"serializer.class\", \"kafka.serializer.StringEncoder\"); The consumer-shell can no longer get the messages published by producer, so it's like there's something wrong, but no exception got. \n\n", "comments": ["If you remove the serializer.class, the DefaultEncoder - kafka.serializer.DefaultEncoder is used. This is described in http://kafka.apache.org/quickstart.html. Is this an issue? ", "Closing. Doesn't look like there's an issue here since we have a default."], "derived": {"summary": "val props = new Properties();\n  props. put(\"zk.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Shouw throw exception when serializer.class if not configured for Producer - val props = new Properties();\n  props. put(\"zk."}, {"q": "What updates or decisions were made in the discussion?", "a": "Closing. Doesn't look like there's an issue here since we have a default."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-225", "title": "Bug in mirroring code causes mirroring to halt", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Jun Rao", "labels": [], "created": "2011-12-10T00:09:36.000+0000", "updated": "2011-12-14T00:09:28.000+0000", "description": "The mirroring code has an API that restarts the consumer connector when a new topic watcher fires. This triggers a rebalancing operation in the consumer connector. But if this rebalancing operation fails, the mirroring code simply throws an exception and never recovers. Ideally, if the rebalancing operation fails due to n retries, we should shut down the mirror", "comments": ["Patch attached.", "+1", "just committed this."], "derived": {"summary": "The mirroring code has an API that restarts the consumer connector when a new topic watcher fires. This triggers a rebalancing operation in the consumer connector.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Bug in mirroring code causes mirroring to halt - The mirroring code has an API that restarts the consumer connector when a new topic watcher fires. This triggers a rebalancing operation in the consumer connector."}, {"q": "What updates or decisions were made in the discussion?", "a": "just committed this."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-226", "title": "SyncProducer connect may return failed connection on reconnect", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Jun Rao", "labels": [], "created": "2011-12-16T02:07:05.000+0000", "updated": "2011-12-16T02:21:15.000+0000", "description": null, "comments": ["Patch attached.", "+1", "committed"], "derived": {"summary": "", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "SyncProducer connect may return failed connection on reconnect"}, {"q": "What updates or decisions were made in the discussion?", "a": "Patch attached."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-227", "title": "Add Broker Failure Test for a Single Host", "status": "Resolved", "priority": "Major", "reporter": "John Fung", "assignee": "John Fung", "labels": [], "created": "2011-12-16T21:06:36.000+0000", "updated": "2012-01-06T00:45:20.000+0000", "description": "Add this test suite under \"system_test\" folder. It performs broker failure test with a mirroring setup in a single machine.\n\nThe setup contains a source brokers cluster and a mirror brokers cluster. The brokers in both clusters will be bounced alternately.\n\nMessage checksum is printed to the corresponding producer and consumer log files. The checksum will be validated at the end of the test. A simple logic will be used to determine how many messages are lost or duplicated.", "comments": ["KAFKA-227-v3.patch has the following changes:\n\n1. Terminating processes cleanly\n2. Restarted Kafka broker log messages are appended to the log file of a previously terminated broker (instead of overwritten to)\n3. Report producer and source / mirror consumer message count for duplicate and missing messages\n4. Producer is running in the background\n5. Calling ConsumerOffsetChecker to wait for zero consumer offset lagging.", "This is a great patch ! Just one comment. The *sh scripts in the system_test/broker_failure need to be given execute permissions. Apart from that, it works well.", "I made the svn propset changes and committed this patch. This system test has been instrumental in testing KAFKA-228 and I see how this can be useful in testing more Kafka bug fixes and features"], "derived": {"summary": "Add this test suite under \"system_test\" folder. It performs broker failure test with a mirroring setup in a single machine.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Add Broker Failure Test for a Single Host - Add this test suite under \"system_test\" folder. It performs broker failure test with a mirroring setup in a single machine."}, {"q": "What updates or decisions were made in the discussion?", "a": "I made the svn propset changes and committed this patch. This system test has been instrumental in testing KAFKA-228 and I see how this can be useful in testing more Kafka bug fixes and features"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-228", "title": "Reduce duplicate messages served by the kafka consumer for uncompressed topics", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "labels": [], "created": "2011-12-17T18:39:35.000+0000", "updated": "2012-01-07T00:11:18.000+0000", "description": "Kafka guarantees at-least once delivery of messages.The high level consumer provides highly available partitioned consumption of data within the same consumer group. In the event of broker failures or consumer failures within a group, the high level consumer rebalances and redistributes the topic partitions evenly amongst the consumers in a group. With the current design, during this rebalancing operation, Kafka introduces duplicates in the consumed data. \n\nThis JIRA improves the rebalancing operation and the consumer iterator design to guarantee 0 duplicates while consuming uncompressed topics. There will be a small number of duplicates while serving compressed data, but it will be bound by the compression batch size.  ", "comments": ["The following things caused duplicates in the current rebalancing design in the kafka consumer -\n\n1. The rebalancing logic comprised of the following steps - release partition ownership, commit offsets, assign partitions, own partitions, restart fetchers. The sequence of the above operations caused duplicate messages to be delivered. For example, since partitions were released before stopping fetchers, some partitions could be owned by another consumer, and hence two consumers in the same group would continue delivering the same messages to the end user. This would change only after the fetchers were stopped and restarted with the new partition ownership decision.\n2. Since fetchers were not stopped after releasing partition ownership and until a successful rebalancing operation was completed, even if the consumer failed to rebalance after n retries, the consumer iterator would continue serving messages assuming the old partition ownership decision.\n3. In the current logic, if a rebalancing operation fails, all partition ownership was being released, the internal state was being reset before starting another attempt of rebalancing. Since the internal state was reset before committing offsets, no consumer offsets were being written to zookeeper. Due to this, until the rebalancing operation succeeded, another consumer could own some partitions and start consuming data from an older offset. This lead to duplicate messages.\n4. The consumer iterator had no knowledge of the rebalancing operation. So even if the fetcher queues are cleared before restarting the fetchers to fetch fro\nm the newly assigned partitions, the consumer iterator would continue returning messages from the current data chunk it is iterating upon. This lead to a few\n more duplicates.\n\nThe above causes are listed in the descending order of the amount of duplicates introduced.\n \nThis patch reduces the duplicate messages served by the kafka consumer to 0, for uncompressed topics. Changes include -\n\n1. Re factor the rebalancing logic to - stop fetchers, clear the fetcher queues, commit consumer offsets, clear the current data chunk in the consumer iterato\nr, release partition ownership, assign and own partitions, restart fetchers\n2. Fix the consumer iterator to synchronize the consumption of data with clearing the current data chunk and committing offsets. \n", "Thanks for the patch. Some comments below:\n\n1. This jira is a duplicate of kafka-198. We need to mark it.\n2. ZookeeperConsumerConnector\n2.1 commitOffsets(): the logging \"committing all offsets\" should be in trace or debug level\n2.2 The patch commits all offsets for clearing every stream. This is unnecessary. We just need to commit all offsets once after we cleared all streams. Also, we don't need to clear all streams. We only need to clear a stream whose fetch queue needs to be cleared. Here is one way of doing that. We keep a reference to stream in each fetch queue (there is a 1:1 mapping btw stream and fetch queue). In closeFetchers(), we (1) stop the current fetcher; (2) for each fetch queue to be cleared: clear the queue and clear the corresponding stream; (3) commit all offsets. Then, we don't need Fetcher.clearFetcherQueues and we don't need to pass in kafkaMessageStreams in Fetcher.startConnections. Also, ConsumerIterator.clearCurrentChunk doesn't need to take any input parameters.\n\n3. ConsumerIterator:\nThe logic is a bit complicated and I am not sure if it's really necessary. To me, it seems all we need is to make currentDataChunk an AtomticReference. clearCurrentChunk() simply sets the reference to null. This will be the only synchronization that we need (no need for lock). Because of the ordering in 2.2, this will make sure the next hasNext() call on the stream blocks until the Fetcher is started again.\n", "Thanks for reviewing the patch ! Here are my comments -\n\n1. Missed the fact that there was a bug. Marked this as a duplicate of the bug. Technically, this is an improvement, not really a bug fix.\n\n2.1 This info level logging is just moved from its previous location (rebalance() API) to its new location (inside commitOffsets() API). I think it is important to have this at info level to be able to understand the flow of the rebalancing operation and also debug any future issues with data duplication/data loss. Since this is not done too often, I thought it wouldn't pollute the logs.\n\n>>  Also, we don't need to clear all streams\n2.2 According to causes 1. & 2. in my list above, you need to stop the fetchers, clear queues, clear streams BEFORE releasing the partition ownership, to avoid data duplication. Since you release all the partitions, you need to stop all the fetchers and clear all the queues. \n\n3. To reduce the duplicates to 0, returning a message through makeNext() needs to be exclusive to committing offsets and clearing the current iterator. In addition to this, each of those needs to execute atomically. This is why we need a lock. Also, we need to clear the 'current' iterator, not set the currentDataChunk to null. ", "2.1. The problem is that commitOffset is called by auto commit too. We don't want to add info level logging for every auto commit.\n\n2.2. Well, in general, rebalance is selective. Not every rebalance affects all topics. Suppose that there are 2 topics X and Y. X is on broker 1,2 and 3 and Y is only on broker 2 and 3. Consider a consumer group with 2 consumers c1 and c2 that consume both topics X and Y. Initially, if all brokers are alive, after rebalance, the assignment can be something like, c1 <- X (broker1, broker2), Y(broker2); c2<- X (broker3), Y(broker3). Suppose now broker1 is down. After rebalance, the assignment can be something like, c1 <- X (broker2), Y(broker2); c2<- X (broker3), Y(broker3). As you can see, topic Y doesn't really need to rebalance since the partition assignment doesn't change. Our current code handles this by not clearing the queue for topic Y in this case. The fetcher will still be restarted. It just picks up from where it left.\n\n3. First of all, it's not clear if the synchronization in the patch gives the exclusive access that you want. This is mainly because the lock is temporarily released in the makeNext() call. This allows other concurrent callers to sneak in. For example, it could be that makeNext() call gets the next chunk in getNextDataChunk, but hasn't updated currentDataChunk yet. At this moment, the fetcher queue is cleared and clearCurrentChunk is called. The latter saves the current offset (which will be used to set the fetch offset after rebalance) and sets currentDataChunk to null. After that, the makeNext() call continues in getNextDataChunk and sets currentDataChunk to a non-null value. This chunk will be fetched again after rebalance and thus introduce duplicates.\n\nSecond, calling commitOffsets inside consumerIterator seems a bit complicated. I am wondering if we can commit offsets outside of consumerIterator.", "2.1 Fair enough. I'll move it out of commitOffsets() API\n\n>> Well, in general, rebalance is selective\n2.2. There is one case of rebalancing (which you've listed), where not clearing the queue would help. However, the code gets slightly more complicated. But, I will make the changes and include it in the next patch.\n\n>> First of all, it's not clear if the synchronization in the patch gives the exclusive access that you want. This is mainly because the lock is temporarily released in the makeNext() call. This allows other concurrent callers to sneak in. For example, it could be that makeNext() call gets the next chunk in getNextDataChunk, but hasn't updated currentDataChunk yet. At this moment, the fetcher queue is cleared and clearCurrentChunk is called. The latter saves the current offset (which will be used to set the fetch offset after rebalance) and sets currentDataChunk to null. After that, the makeNext() call continues in getNextDataChunk and sets currentDataChunk to a non-null value. This chunk will be fetched again after rebalance and thus introduce duplicates. \n3. The purpose of adding the locking is to do our best to safely reduce the number of duplicates served by the consumer iterator. Locking needs to be done in such a way that the lock is always released before entering a potentially blocking operation. The case you've pointed out seems very corner case, at least from the test runs (using KAFKA-227). For example, over hundreds of iterations of that test, no duplicates were reported. If you try to \"fix\" this case, you will risk a potential deadlock situation, which we must avoid. Given that, this amount of locking seems reasonable to me.\n\n>> Second, calling commitOffsets inside consumerIterator seems a bit complicated. I am wondering if we can commit offsets outside of consumerIterator.\n3. That protects against the duplication of the last data chunk. The best place to move this, out of the consumer iterator, is in ZookeeperConsumerConnector's closeFetchers(), after clearing after clearing the queue and clearing the current iterator. If this change is made, the number of times we write to zookeeper will also reduce\n\nIn our offline chat, you mentioned you want to try to refactor the patch to simplify the consumer iterator code, by removing the lock altogether, and only depend on the atomic references. I was wondering if you'd like to give that a try and upload another patch ?\n", "Attach patch v2. It implements ConsumerIterator in an alternative way that doesn't require locking.\n\nTo apply, apply patch v1, revert ConsumerIterator, then apply patch v2.", "kafka-228-v2.patch had a bunch of compilation errors since it didn't use the AtomicReference's correctly. But after fixing that, I feel I like the simplicity of the ConsumerIterator in the patch. I used the new test added as part of KAFKA-227 to test both patches, and it seems like v2 is only slightly worse than v1. Thinking more about it, v2 runs a risk of duplicating an entire data chunk only if a new chunk is fetched from the queue and before setting the iterator to the data in the newly fetched chunk, the iterator is set to null. So the consumer continues to serve this newly fetched data potentially in parallel with another consumer. \n\nBut, since fetching a new data chunk is fairly rare with a large enough fetch size, it is less probable that we hit this case. \n", "This patch addresses the comments on patch v1 and includes the changes suggested in v2 -\n\n1. The info level statement is moved out of the commitOffsets() API\n2. Only certain fetch queues are cleared, not all\n3. Offsets are not committed in the clearCurrentChunk() API, but in closeFetchers() instead.", "Thanks for the new patch. Overall, looks pretty good. A few extra comments:\n\n4. ConsumerIterator: Could we make currentDataChunk a local variable in makeNext. This is an existing problem, but it would be good if we can fix it in this patch. Also, currentTopicInfo doesn't have to atomic since it's never updated concurrently. \n\n5. It's better to set the KafkaMessageStreams in the constructor of ZKRebalanceListener. This way, even if the ZK listener gets triggered before the ZookeeperConsumerConnector.consume completes, KafkaMessageStreams is available in the rebalance call in the listener.\n\n6. Fetcher: remove shutdown and keep only stopConnectionsToAllBrokers\n", "4. currentDataChunk is a local variable in makeNext now. \n\n5. KafkaMessageStreams are set in the constructor of ZKRebalanceListener. \n\n6. Removed the shutdown() API and changed references to stopConnectionsToAllBrokers \n\n7. currentDataChunk doesn't have to be an AtomicReference."], "derived": {"summary": "Kafka guarantees at-least once delivery of messages. The high level consumer provides highly available partitioned consumption of data within the same consumer group.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Reduce duplicate messages served by the kafka consumer for uncompressed topics - Kafka guarantees at-least once delivery of messages. The high level consumer provides highly available partitioned consumption of data within the same consumer group."}, {"q": "What updates or decisions were made in the discussion?", "a": "4. currentDataChunk is a local variable in makeNext now. \n\n5. KafkaMessageStreams are set in the constructor of ZKRebalanceListener. \n\n6. Removed the shutdown() API and changed references to stopConnectionsToAllBrokers \n\n7. currentDataChunk doesn't have to be an AtomicReference."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-229", "title": "SimpleConsumer is not logging exceptions correctly so detailed stack trace is not coming in the logs", "status": "Resolved", "priority": "Major", "reporter": "Joe Stein", "assignee": "Jay Kreps", "labels": [], "created": "2011-12-29T15:57:32.000+0000", "updated": "2012-01-02T20:55:24.000+0000", "description": null, "comments": ["+1 committed."], "derived": {"summary": "", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "SimpleConsumer is not logging exceptions correctly so detailed stack trace is not coming in the logs"}, {"q": "What updates or decisions were made in the discussion?", "a": "+1 committed."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-230", "title": "Update website documentation to include changes in 0.7.0 release", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "labels": [], "created": "2012-01-03T01:26:50.000+0000", "updated": "2012-01-04T16:59:28.000+0000", "description": "We need to update the website to reflect the changes to the code in the 0.7.0 release. ", "comments": ["Changes include -\n\n1. Update downloads page to link 0.7 release tar, release notes, KEYS file\n2. Update API docs to link the 0.7 API docs\n3. Update configuration to add the compression and other configs added in 0.7\n4. Update quick start to add examples for compression, cleanup stuff that doesn't work\n\n", "More fixes to the quickstart document", "I saw some other incubator projects put their downloadable under www.apache.org/dist/incubator (e.g. www.apache.org/dist/incubator/jena/). Anybody knows how to get access to that location?", "See http://incubator.apache.org/guides/releasemanagement.html#understanding-upload for mirrors", "Patch to update the design document for 0.7. This does the following:\n- Adds discussion of mirroring with link to wiki\n- Adds discussion of compression with link to wiki\n- Removes references to 0.6", "Updated patch that includes Neha's changes with a few additions:\n1. Fix configuration to not refer to SyncProducer and AsyncProducer and to not separate SyncProducer configs (since they are also async producer configs).\n2. Fix up the formatting on the downloads page.\n3. Change the quickstart to use console consumer/producer since it is much more useful.\n4. There are lots more things it would be nice to do for documentation but it probably has to wait for now.", "Jay,\n\nThanks for the updated patch. Applied your patch, and made the following changes -\n\n1. Corrected the downloads link from the quickstart \n2. Corrected the Consumer example in the quickstart to include typed KafkaMessageStream\n3. Added a link to the newbie jira list in the projects page", "Committed the latest patch, which includes previous patches", "The download link goes directly to people.a.o instead of the mirror system.", "Chris, Kafka currently does not have a directory at www.apache.org/dist/incubator. I'm filing an infrastructure ticket to track that. Once its available, we can modify the download link to point there. "], "derived": {"summary": "We need to update the website to reflect the changes to the code in the 0. 7.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Update website documentation to include changes in 0.7.0 release - We need to update the website to reflect the changes to the code in the 0. 7."}, {"q": "What updates or decisions were made in the discussion?", "a": "Chris, Kafka currently does not have a directory at www.apache.org/dist/incubator. I'm filing an infrastructure ticket to track that. Once its available, we can modify the download link to point there."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-231", "title": "avoid logging stacktrace directly", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Jun Rao", "labels": [], "created": "2012-01-03T01:28:28.000+0000", "updated": "2012-01-03T01:55:47.000+0000", "description": "There are several places where we log the stacktrace directly. This can be avoided by using the proper log4j method.\n", "comments": ["Patch attached.", "+1", "Thanks for the review. Just committed this."], "derived": {"summary": "There are several places where we log the stacktrace directly. This can be avoided by using the proper log4j method.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "avoid logging stacktrace directly - There are several places where we log the stacktrace directly. This can be avoided by using the proper log4j method."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for the review. Just committed this."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-232", "title": "ConsumerConnector has no access to \"getOffsetsBefore\" ", "status": "Resolved", "priority": "Minor", "reporter": "Edward Capriolo", "assignee": null, "labels": [], "created": "2012-01-03T18:09:50.000+0000", "updated": "2017-10-29T09:13:27.000+0000", "description": "kafka.javaapi.SimpleConsumer has \"getOffsetsBefore\". I would like this ability in KafkaMessageStream or in ConsumerConnector. In this way clients can access their current position.", "comments": ["Could you explain in a bit more detail how you plan to use this method? Today, ConsumerConnector doesn't support consuming from an arbitrary offset.", "Sure. I am using Kafka to build aggregations of logs. These aggregations are going to be minutely. I want to record clientid and my log position externally. If my consumer goes down 30 seconds into the minute I want to replay the entire minute over. I might also want to bring this client up at position x on another node.", "Do you think you can add meta data in the message itself so that you know whether the consumed data has moved to the next minute? If so, you can turn off auto commit and call commitOffset() directly.", "Interesting idea. I am not sure that will work well, many producers will be logging and I would expect that some messages will arrive out of order. Especially as the minute turns over. In the worst case scenario a producer might have an unexpected shutdown and wake up at some time later. Ideally I would like the producers to not have to write special headers etc. I thought of recording the last message seen assuming that each record is more or less unique but there are edge cases here as well.\n\nI opened this issue up because I am wondering why the two consumers have different functionality. Which one should be used for which cases? It is not entirely clear to a new user like myself. \n\nFrom a mile-high view it seems like if the ConsumerConnection has access to commitOffset() it should be able to call getOffset(). Is there some technical complexity in implementing this?\n", "We discussed in the mailing before about allowing the ZK-based consumer to consume from an arbitrary offset during startup time. Here is the main complexity. Multiple consumers in the same group can consume a topic jointly. When they start up, which consumer sets the offset for which partitions? How do we prevent 2 consumers from setting the offset for the same partition?", "I think here is how that can work -\n\nSay we enable a \"custom.beginning.offsets\" option in the zk consumer. What that means is that this particular group id has chosen to work with custom beginning offsets. \n\nThe requirement is to make sure that only one consumer in the group specifies this option. And if more than one consumer in a group tries to do the same, we throw an exception. This can be implemented via zookeeper locks, where the first consumer id that specifies this option writes the custom offsets in the /consumers/groups/[group_id]/custom_offsets path. Following that any other consumer trying to do the same will get back an exception stating the consumer id that currently has the lock.  \n\nOn the first successful rebalance after startup, the consumer ids can check for this path and reset the consumed offset for the partitions they own to that value. \n\nThis is just a cursory explanation of the idea and many details would have to be worked out. This is very tricky with the current implementation of the consumer rebalancing logic. I feel that this will be much easier to implement if we move to the co-ordinator approach for the consumer rebalancing. I've been meaning to write up a proposal for that. Will write up my ideas and add some ideas for this JIRA to it.", "Closing inactive issue. The old consumer is no longer supported. This jira requirement can be implemented with java consumer API."], "derived": {"summary": "kafka. javaapi.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "ConsumerConnector has no access to \"getOffsetsBefore\"  - kafka. javaapi."}, {"q": "What updates or decisions were made in the discussion?", "a": "Closing inactive issue. The old consumer is no longer supported. This jira requirement can be implemented with java consumer API."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-233", "title": "The producer's load balancing logic can send requests to dead brokers, when using the async producer option", "status": "Closed", "priority": "Major", "reporter": "Neha Narkhede", "assignee": null, "labels": ["newbie"], "created": "2012-01-03T19:08:35.000+0000", "updated": "2014-06-19T05:16:01.000+0000", "description": "The ZK producer, when used with the async producer option does the following \n\n1. Create a pool of async producers, one each for a broker registered under /broker/ids\n2. On each send request, apply the Partitioner, to decide the broker and partition to send the data\n3. Use the Async producer's send API to enqueue that data into the async producer's queue\n4. When the data is dequeued by the ProducerSendThread, use the underlying sync producer to send it to the broker\n\nThe load balancing decision is taken in step 2, before entering the queue. This leaves a window of error, equal to the queue length, when a broker can go down. When this happens, potentially, a queue worth of data can fail to reach a broker, and will be dropped by the EventHandler. \n\nTo correct this, the Producer, with the async option, needs to be refactored to allow only a single queue to hold all requests. And the application of the Partitioner should be moved to the end of the queue, in the EventHandler.", "comments": ["Al though this JIRA is marked with the newbie tag, this is amongst the harder of all the newbie JIRAs. Anyone who is interested in knowing the Producer logic inside out, can give this a try. It will be a very good enhancement to the producer logic", "Hi, I think I am seeing this already implemented in 0.8. Can we close this if that is the case?", "Fixed in 0.8."], "derived": {"summary": "The ZK producer, when used with the async producer option does the following \n\n1. Create a pool of async producers, one each for a broker registered under /broker/ids\n2.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "The producer's load balancing logic can send requests to dead brokers, when using the async producer option - The ZK producer, when used with the async producer option does the following \n\n1. Create a pool of async producers, one each for a broker registered under /broker/ids\n2."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed in 0.8."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-234", "title": "Make backoff time during consumer rebalance configurable", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Jun Rao", "labels": [], "created": "2012-01-05T00:10:06.000+0000", "updated": "2014-10-06T00:42:38.000+0000", "description": "We need to make backoff time during consumer rebalance directly configurable, instead of relying on zkSyncTime.", "comments": ["Patch submitted.", "+1", "Thanks for the review. Committed.", "Hi Jun,\n\nJust wondering why we want to make zkSyncTime as the default value of fetcherBackoffMs at the first place? These two configs do not seem related to me.", "The reasoning is the following. A ZK follower can be up to zkSyncTime behind the ZK leader. Since a ZK client can do reads from any ZK server, it's possible for a rebalance to fail because one of the consumers reads stale data. Waiting for at least zkSyncTime allows the ZK follower to have enough time to sync up with its leader."], "derived": {"summary": "We need to make backoff time during consumer rebalance directly configurable, instead of relying on zkSyncTime.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Make backoff time during consumer rebalance configurable - We need to make backoff time during consumer rebalance directly configurable, instead of relying on zkSyncTime."}, {"q": "What updates or decisions were made in the discussion?", "a": "The reasoning is the following. A ZK follower can be up to zkSyncTime behind the ZK leader. Since a ZK client can do reads from any ZK server, it's possible for a rebalance to fail because one of the consumers reads stale data. Waiting for at least zkSyncTime allows the ZK follower to have enough time to sync up with its leader."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-235", "title": "Add a 'log.file.age' configuration parameter to force rotation of log files after they've reached a certain age", "status": "Closed", "priority": "Major", "reporter": "Mathias Herberts", "assignee": "Swapnil Ghike", "labels": [], "created": "2012-01-05T14:46:29.000+0000", "updated": "2013-06-13T14:51:27.000+0000", "description": "The Kafka client has the ability to start consuming at an offset before or after a given point in time. The granularity of this offset is the log file as the Kafka servers do not keep track of arrival time of various messages.\n\nThis means that the granularity of offsets relative to time depends on arrival rate of messages and thus of log file rotation. A topic with lots of messages will have its log files rotated very often (thus each spans a short time interval) whereas a topic with very few messages might not see its log files rotated for hours.\n\nIn order to circumvent this granularity disparity, having a parameter that would force log file rotation after a certain delay (xxx ms) would allow for pretty much constant time granularity to be available at the cost of more file descriptor being used.", "comments": ["This seems like a useful feature, in particular for kafka-236. What's the age granularity that you are looking for? We probably don't want to create too many files for a topic.", "Didn't we do this?", "Yes, KAFKA-475 added a feature to 0.7 and 0.8 that rolls a new log segment based on the age of the old segment. So in essence, a new log segment is rolled when the old segment exceeds its max permissible size or max permissible age. These maximum size and age limits can be configured in KafkaConfig."], "derived": {"summary": "The Kafka client has the ability to start consuming at an offset before or after a given point in time. The granularity of this offset is the log file as the Kafka servers do not keep track of arrival time of various messages.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Add a 'log.file.age' configuration parameter to force rotation of log files after they've reached a certain age - The Kafka client has the ability to start consuming at an offset before or after a given point in time. The granularity of this offset is the log file as the Kafka servers do not keep track of arrival time of various messages."}, {"q": "What updates or decisions were made in the discussion?", "a": "Yes, KAFKA-475 added a feature to 0.7 and 0.8 that rolls a new log segment based on the age of the old segment. So in essence, a new log segment is rolled when the old segment exceeds its max permissible size or max permissible age. These maximum size and age limits can be configured in KafkaConfig."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-236", "title": "Make 'autooffset.reset' accept a delay in addition to {smallest,largest}", "status": "Resolved", "priority": "Major", "reporter": "Mathias Herberts", "assignee": null, "labels": [], "created": "2012-01-05T14:49:47.000+0000", "updated": "2017-08-17T11:46:45.000+0000", "description": "Add the possibilty to specify a delay in ms which would be used when resetting offset.\n\nThis would allow for example a client to specify it would like its offset to be reset to the first offset before/after the current time - the given offset.", "comments": [" To reset the offset, under the cover, we use getLastestOffsetBefore(), which supports an arbitrary starting timestamp, in addition to smallest and largest. However, getLastestOffsetBefore has a very coarse granularity at this moment. It only returns the offset of the first message in a log segment file based on its last modified time. So, depending on the log segment size, this may or may not be good enough for some applications.", "This can be achieved by using reset consumer group tool or KafkaConsumer.offsetsForTimes api in latest kafka versions."], "derived": {"summary": "Add the possibilty to specify a delay in ms which would be used when resetting offset. This would allow for example a client to specify it would like its offset to be reset to the first offset before/after the current time - the given offset.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Make 'autooffset.reset' accept a delay in addition to {smallest,largest} - Add the possibilty to specify a delay in ms which would be used when resetting offset. This would allow for example a client to specify it would like its offset to be reset to the first offset before/after the current time - the given offset."}, {"q": "What updates or decisions were made in the discussion?", "a": "This can be achieved by using reset consumer group tool or KafkaConsumer.offsetsForTimes api in latest kafka versions."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-237", "title": "create/delete ZK path for a topic in an admin tool", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Jun Rao", "labels": [], "created": "2012-01-05T18:06:11.000+0000", "updated": "2012-01-13T23:53:21.000+0000", "description": "This subtask will implement a create/delete admin tool that create/delete the ZK paths needed for a new topic. The needed ZK paths are described in the design doc in KAFKA-50 and the discussion in KAFKA-47.", "comments": ["Patch attached. The patch contains:\n1. New command line tools to create, list, and delete a topic.\n2. Auto replica assignment.\n3. Unit test. \n\nThere is a minor change from the doc. The ZK paths for a topic is now under:\n/brokers/topics/[topic]/partitions/[partition_id], instead of /brokers/topics/[topic]/[partition_id]. This is mainly to distinguish the new ZK data structures from the old ones.", "Took a look at the patch. Good to see work started on KAFKA-50. I have a couple of comments -\n\n1. It will be good to use a logger in all the tools. This is mainly to be able to cleanly log stack traces. For example, println(\"delection failed because of \" + e) does not print out the stack trace as desired.\n\n2. In the create topic tool, there is a getBrokerList API. This seems like a pretty useful utility API. Would it make sense to expose it in ZkUtils instead of rewriting it there ?\n\n3. In the create topic tool, can the broker list returned by getBrokerList be a Set ? It is expected to be a Set in getManualReplicaAssignment, and also expected to be one.\n\n4. In the create topic tool, replica-assignment-list format is very pretty to read. Can this be described in more detail ? Also, usually entries are described in csv format. Seems like it is easier to understand if the list of entries per partition are comma separated, and the replica ids within a partitions list are \":\" separated. \n\n5. In createReplicaAssignmentPathInZK API, if a topic already exists, it will be good to print its version (timestamp) as part of the exception.\n\n6. Can getTopicPartitionsPartPath API be renamed so that it is easier to understand what it is meant for ? It seems like it returns the replication information for a particular partition id.\n\n7. In AdminUtils.assginReplicasToBrokers(), why is secondReplicaShift required ? Can the next replica index be selected like this - (firstReplicaIndex + j + 1) % num_brokers. Or maybe I'm missing something ? \n\n8. There are a couple of typos in the create topic admin tool\n\n", "Attach patch v2. Comments addressed:\n1. Log4j may not always print to console. So, it's better if we print out to console directly. Added stack trace in the print out.\n2. Moved getBrokerList to ZkUtils.\n3. Sometimes we need a sorted broker list, such as doing replica assignment. So, changed code to convert list to set when needed.\n4. Changed the separator format accordingly.\n5. Actually, version information is mostly for internal use and probably shouldn't be exposed to users. From users' perspective, knowing that a topic exists seems enough. Not sure if returning version info is helpful to the users. Version info is potentailly useful for operators. However, such information can be got from ZK admin tools directly.\n6. renamed\n7. The reason is that if there are multiple partitions (first replica) assigned to the same broker, we want their second replicas to be spread around the rest of brokers. The approach you described will assign the second replica of all those partitions to the same broker.\n8. fixed\n", "Thanks for uploading v2. \n\n5. Today, we expose this sort of information in the zk consumer (ZkUtils.createEphemeralPathExpectConflict) and it has been useful in debugging. Since this API exists as part of the admin package, it does seem like useful information to have.\n7. Got it. Thanks for the explanation.", "Attach v3. \n\n5. Return version id if create topic fails.", "+1. \n\nPartitionMetaData can be a case class, but I will be making that change anyways, as part of KAFKA-238", "thanks for the review; just committed this."], "derived": {"summary": "This subtask will implement a create/delete admin tool that create/delete the ZK paths needed for a new topic. The needed ZK paths are described in the design doc in KAFKA-50 and the discussion in KAFKA-47.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "create/delete ZK path for a topic in an admin tool - This subtask will implement a create/delete admin tool that create/delete the ZK paths needed for a new topic. The needed ZK paths are described in the design doc in KAFKA-50 and the discussion in KAFKA-47."}, {"q": "What updates or decisions were made in the discussion?", "a": "thanks for the review; just committed this."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-238", "title": "add a getTopicMetaData method in broker and expose it to producer ", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Neha Narkhede", "labels": [], "created": "2012-01-05T18:39:14.000+0000", "updated": "2012-01-24T20:55:40.000+0000", "description": "We need a way to propagate the leader and the partition information to the producer so that it can do load balancing and semantic partitioning. One way to do that is to have the producer get the information from ZK directly. This means that the producer needs to maintain a ZK session and has to subscribe to watchers, which can be complicated. An alternative approach is to have the following api on the broker.\n\nTopicMetaData getTopicMetaData(String: topic)\n\nTopicMetaData {\n  Array[PartitionMetaData]: partitionsMetaData\n}\n\nPartitionMetaData {\n  Int: partitionId\n  String: leaderHostname\n  Int: leaderPort\n}\n\nUsing this api, the producer can get the metadata about a topic during initial startup or leadership change of a partition.", "comments": ["We can potentially piggy-back auto topic creation on the getTopicMetaData api. If auto topic creation is enabled and the broker sees no ZK data for a topic, the api will create the topic first.", "As part of adding this getTopicMetaData and changing the producer to use it, we can get rid of the zookeeper client in the producer al together. \n\nHere is how the load balancing would work -\n\n1. On startup, the producer can invoke the getTopicMetaData API and cache the topic and partition information locally.\n2. If the leader of a partition goes down, some send() request from the producer would fail. At this point, the producer will perform actions in step 1 again. And then, retry the send request.\n\nSince mastership change happens pretty rarely, this approach seems reasonable, given that we can get rid of zookeeper client from the producer. \n\nNow, for step 2 to work correctly with the async producer, we need to first resolve KAFKA-233. That will take care of moving the load balancing logic to the end of the queue. So, actions in step 2 can be part of the EventHandler", "One general recommendation I have is that we think about making this as general as possible. The way we are doing APIs is fairly high overhead, and the general difficulty of doing client upgrades means we really want to have fewer, more general apis.\n\nQuestions: \nShouldn't we also give back the host/port info for replicas? Might there be a case where it is better to read from replicas?\nShould this be per-topic or should we just get all metadata?\nAlso, should we consider other metadata? For example the other metadata we have on a per-partition basis is node id, total data size, number of segments, beginning offset and date for each segment. This would make it possible to get rid of the getOffsetsBefore api as that would be a special case of this more general metadata api.\n\nRelated thought: currently we maintain a custom object for each request. This is very good for fetch and produce which need efficiency for the message sets. It is kind of a hassle for this case. An alternative would be to just send a single string field containing the metadata as json, which would make evolution easier for this kind of performance-insensitive api. On the other hand since it is just one api object maybe the cost of a JSON parser jar and non-uniformity with the other apis isn't worth it.", "I can see it's useful for the api to support a list of topics. Returning all topics is probably too much overhead since most producers are interested in a subset of topics.\n\nIn our current replication design, both send and fetch requests are served only by the leader. We can balance the load by having multiple partitions per broker. We may allow followers to serve read in the future. I don't see a clear benefit at this moment though. ", "I see Jay's point about making this API general enough to avoid API upgrades later. It looks like this API can potentially be useful to the producer as well as the consumer, in addition to the admin commands. If we want to support producer and consumer, it means we can make this API return all metadata for all partitions on a topic.\n\nAlso returning host/port for other replicas, and offset/timestamp for log segment might not add significant overhead. Given the benefit of generalization, it doesn't sound like a bad idea.", "This is a draft patch that includes the new getTopicMetadata API on the server. Changes include -\n\n1. Kafka server always requires a zookeeper connection\n\n2. Additional configs on the server are \n\n2.1 auto.create.topics which controls auto creation of topics in the getTopicMetadata API. Default is true\n\n2.2 default.replication.factor which controls the replication factor if the topic is auto created\n\n3. getTopicMetadata request takes in a list of topics. One question is if no list is specified, would returning metadata for all topics make sense ? \n\n3.1 topic metadata request has the following format -\n\nnumber of topics (4 bytes) \nlist of topics (2 bytes + topic.length per topic) detailedMetadata (2 bytes) \ntimestamp (8 bytes) (optional if detailedMetadata = 1 ) \ncount (4 bytes) (optional, if detailedMetadata = 1)\n\n3.2 topic metadata response has the following format -\n\ntopic \npartition-metadata\n\n3.3. partition-metadata has the following format -\n\npartition-id\nleader (optional)\nreplicas \nin-sync replicas  \nlog metadata (optional)\n\n3.4 log metadata has the following format -\n\nnumber of log segments \ntotal size of this log in bytes\nlog segment metadata (optional)\n\n3.5 log segment metadata has the following format -\n\nbeginning offset\nlast modified timestamp\nsize of segment in bytes\n\n4. Test suite cleanup to include zookeeper dependency for the KafkaServerHarness \n\n5. Added unit test TestTopicMetadataTest with mock log manager. This tests the new request format, auto creation of topic and metadata response", "The patch doesn't apply to the 0.8 branch. Could you rebase?\n\ncan't find file to patch at input line 2260\nPerhaps you used the wrong -p or --strip option?\nThe text leading up to this was:\n--------------------------\n|Index: core/src/main/scala/kafka/api/MultiMessageSetSend.scala\n|===================================================================\n|--- core/src/main/scala/kafka/api/MultiMessageSetSend.scala\t(revision 1232541)\n|+++ core/src/main/scala/kafka/api/MultiMessageSetSend.scala\t(working copy)\n", "Some comments:\n\n1. Use the new Logger helper for new log4j logging.\n2. PartitionMetaData: The leader indicator just needs 1 byte, instead of 2.\n3. AdminUtils.getTopicMetaDataFromZK(): Instead of getting broker info from ZK each time, could we collect a set of unique broker ids that are needed and call ZK only once for each unique id to get the broker info?\n3. LogMetaData: LogMetaData is actually a replica level info, not a partition level info. Collecting such data in the same getGetMetaData api is tricky since log meta data is only available at the broker that stores a replica locally. So, this info is not available at every broker, you have to talk to the right broker to get such information. One possibility is to keep this as a separate api, something like getReplicaMetadata(topic, partition, brokerid). Such information is only returned if the api is called on the broker that owns the partition.\n4. SyncProducerTest: no need to extend ZooKeeperTestHarness since it's included in KafkaServerTestHarness now.\n5. We probably need to separate ZK from LogManager. This allows us to use and test LogManager independently. We can probably attach a handler to the LogManager. This can be a separate jira.\n", "Patch applies cleanly on 0.8\n\n1. Done\n2. Changed that and log segment metadata request to 1 byte\n3. Since this is a rarely used API, I can't see the benefit of caching information from zookeeper. Besides, that will add overhead of maintaining cache coherency. Maybe this can be a future enhancement if we see the need at that time.\n4. Good point. This patch doesn't attempt to solve the log metadata problem. It simply lays out the groundwork if we find it useful to return that information through this API or even a separate API. Having said that, adding another API to get the segment metadata might not be a bad idea. I would like to file another JIRA to track the log segment metadata API issue, and not delete the helper code this patch has.\n4. Done\n5. This will be very helpful. I would also like to add a JIRA to cleanup our tests to use EasyMock efficiently.   ", "3. I agree that the API is rarely used. However, each call could request a large number of topics. This is particularly true in the consumer where we make multifetch requests (may consist of hundreds of topics). I wasn't thinking of caching broker info permanently. Rather, we can just cache the broker info within a single getMetaData request. For example, if we have 200 topics (1 partition per topic) and 4 brokers, instead of making 200 ZK read requests, we just need to make 4 requests.\n4. I am ok with tracking this in a separate jira.\n\nSome new comments:\n6. Remove unused imported package in AdminUtils, SyncProducerTest, etc.\n7. TopicMetadataRequest.deserializeTopicMetadata should be sth like deserializeTopicMetadataList\n8. TopicMetaData: keep comments consistent with implementation (e.g., doesLeaderExist is 1 byte now)\n9. ZkUtils: we have a mix of getTopicPartition* and getTopicPart*. We can use either format. We just need to be consistent.\n", "3. Ah, makes sense. I misunderstood that as being able to cache across requests. Within a single request, it does make sense to cache. This patch includes that change.\n4. Filed KAFKA-252 to track this.\n\nIncluded the new comments in this patch.", "+1 on the patch. \n\nThis patch doesn't wire the getMetaData api in the producer/consumer. Is the intention to do that as part of kafka-239 ?", "It was intentional. The zk dependency removal and wiring getMetadata in the producer go hand-in-hand. It will be part of KAFKA-239, but that is blocked by KAFKA-253", "Committed this."], "derived": {"summary": "We need a way to propagate the leader and the partition information to the producer so that it can do load balancing and semantic partitioning. One way to do that is to have the producer get the information from ZK directly.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "add a getTopicMetaData method in broker and expose it to producer  - We need a way to propagate the leader and the partition information to the producer so that it can do load balancing and semantic partitioning. One way to do that is to have the producer get the information from ZK directly."}, {"q": "What updates or decisions were made in the discussion?", "a": "Committed this."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-239", "title": "Wire existing producer and consumer to use the new ZK data structure", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Neha Narkhede", "labels": [], "created": "2012-01-05T18:46:21.000+0000", "updated": "2012-03-02T19:18:20.000+0000", "description": "We can assume the leader of a partition is always the first replica. Data will only be stored in the first replica. So, there is no fault-tolerance support yet. Just make the partition logical.", "comments": ["Quite a bit of changes -\n\n1. The producer depends on zookeeper to fetch the initial list of brokers in the cluster. After that, it uses the getTopicMetadata API to fetch the topic metadata\n2. The partitions are made logical. Al though, it is a bit confusing what the Partition object represents. I think a partition should be associated with a topic, broker and have an id\n3. Removed the ZkLoadbalanceTest and javaapi.PrimitiveApiTest. These were duplicated tests", "This patch needed some cleanup - \n\n1. the zk connection was created more than once in the producer. This can be done once, by moving it to the constructor of the producer pool. \n2. Imports need cleanup as well", "1. Imports are cleaned up\n2. zkClient instance creation is moved to the Producer", "1. ZkUtils.getTopicPartitionsPath: There are a couple reasons why we chose to use /brokers/topics/partitions to store partition data, instead of /brokers/topics: (1) This distinguishes the ZK layout in 0.8 from 0.7. In 0.7, we already store broker ids directly under /brokers/topics. So there could be confusion if we put partition ids at the same level. (2) This also leaves room for future extension to add non-partition level per topic info in ZK.\n\n2. ZkUtils: rename idoesBrokerHostPartition to something like isPartitionOnBroker\n\n3. LogManager: logFlusherScheduler.startUp is called twice.\n\n4. LogManager.getOrCreateLog: The LogManager shouldn't need to know anything about ZK. The check of whether a partition exists on a broker should be done at KafkaApi level. Ideally, we just want to check partition ownership from a local cache, which will be populated by ZK listeners (part of kafka-44). In this patch, we can either not check at all or directly check from ZK (with the intention to have it optimized in kafka-44).\n\n5. KafkaServer: the log cleaner scheduler is scheduled in LogManager, should we start up the schedule there too. If there is good reason to do that, should we protect startUp from being called more than once?\n\n6. KafkaScheduler: what's the benefit of having a startUp method? It seems creating the executor in the constructor is simpler.\n\n7. Partition should be made logical. It only contains topic and partition id. There will be a new entity Replica which is associated with broker id.\n\n8. ProducerPool: remove unused import and fix indentation in close()\n\n9. AsyncProducerTest: There are duplicated code that sets up mock to form expected partition metadata. Can we have a separate method to share the code?\n\n10. Producer: Is zkClient in the constructor just for testing? If so, add a comment to indicate that.\n\n11. TestUtils: why do we need checkSetEqual? Doesn't scala do that by default?\n\n12. ZookeeperConsumerConnectorTest.testLederSelectionForPartition: is it really testing leader election? It seems to be testing partition ownership.\n\n13. ZkUtils.getLeaderForPartition: This is probably fine for this patch. However, we should think whether it is better to get leader info from ZK directly or use the getMetaData api.\n", "1, 3. Done\n4. Better done as part of kafka-44. If this is not done at all in this patch, unit tests break left and right. This patch had to change almost all tests, I'll prefer making more test changes in another patch.\n5. Good suggestion.\n6. These changes were required since the current code doesn't allow you to restart a Kafka server by calling shutdown and start on KafkaServer. It might need some cleanup, but it seems wrong that we can't restart a server that has been shutdown. Ideally, we need to refactor the LogManager, KafkaZookeeper and KafkaServer to remove the interdependencies. \n7. broker id is left there for now. This can change once that new Replica entity is introduced. Changing it in this patch is again introducing way too many changes. I'm thinking all cleanup doesn't have to happen in one huge patch. I will prefer this change is done in another one\n8, 9, 10. Done.\n12. It is meant for checking topic registry to test the leader selection. The partition ownership checks are pulled in from ZkLoadBalanceTest\n\nLets keep in mind, that this is not the only patch that we should do for clean up and refactoring. It is getting harder to manage and rebase too.", "There is something wierd happening with the AutoOffsetResetTest with the latest patch, will take a look and upload another one", "Please address items 2 and 11. For 4, I am fine leaving the ZK check in LogManager for now. Please add a comment that this will be fixed later.\n\nUnit tests seem to hang consistently for me. Seems to hang on testLatestOffsetResetForward(kafka.integration.AutoOffsetResetTest).", ">> 11. TestUtils: why do we need checkSetEqual? Doesn't scala do that by default? \n\nThis is something you had added in the ZookeeperConsumerConnectorTest, not part of this patch. I think Scala does it by default, and I'll clean it up as well.\n\n>> 2. ZkUtils: rename idoesBrokerHostPartition to something like isPartitionOnBroker \n\nI didn't see any grammatical difference between the two, but if you think the latter is easier to understand, I'll incorporate the change\n\n>> Unit tests seem to hang consistently for me. Seems to hang on testLatestOffsetResetForward(kafka.integration.AutoOffsetResetTest). \n\nPlease see my previous update on this JIRA, I've already identified that. Please wait for another patch", "2. Renamed ZkUtils.isPartitionOnBroker\n11. Removed checkSetEquals\n\nThe AutoOffsetResetTest hangs since it was updating the offset for the wrong ZK path. Fixed that.", "+1 on the patch.\n\nFor 2, I was interpreting Host as a noun, instead of a verb. Thus the confusion.", "ah, speaking of different perspectives, which is a good thing :-)"], "derived": {"summary": "We can assume the leader of a partition is always the first replica. Data will only be stored in the first replica.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Wire existing producer and consumer to use the new ZK data structure - We can assume the leader of a partition is always the first replica. Data will only be stored in the first replica."}, {"q": "What updates or decisions were made in the discussion?", "a": "ah, speaking of different perspectives, which is a good thing :-)"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-240", "title": "implement new producer and consumer request format", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": null, "labels": ["fetch", "replication", "wireprotocol"], "created": "2012-01-05T18:57:40.000+0000", "updated": "2013-05-02T02:29:48.000+0000", "description": "We want to change the producer/consumer request/response format according to the discussion in the following wiki:\n\nhttps://cwiki.apache.org/confluence/display/KAFKA/New+Wire+Format+Proposal", "comments": ["so I started looking at parts of the code and the document\n\nthere still seems to be an open question in regards to breaking change or not\n\nI think there is a lot of benefit to not have a breaking change granted the code will get a bit cluttered but nice that we can add the new class (i.e. WireProducerRequest) in the same file as ProducerRequest and the internal guts and works (as suggested in the wiki \"One thought on this is that it is probably not too hard to make most of the above changes as new request types and map the old request types to the new\")\n\nI do not understand the next line though \"However if we are changing the request id and version id scheme then this will likely not be possible.\"\n\nkeeping the old format would be helpful in upgrades and existing clients being able to operate without changes.\n\nassuming that (and would like clarification on \"However if we are changing the request id and version id scheme then this will likely not be possible.\") then we would should agree on the naming for the new extended Request/Response format (e.g. WireXYZRequest WireXYZResponse) or something else besides Wire (I don't really like that but first thing that came to me without much thinking about it) but something to help denote the internal communications from component to component and have all those classes start the same).\n\n\n\n\n", "Joe,\n\nThat's a good question. We had some discussions on this in the mailing list before.\n\nI agree it would be nice if we can make the 0.8 release backward compatible. However, it's a little bit hard because:\n1. We need a process to migrate the ZK data structures and potentially on-disk data organization. This is particular hard since we need to migrate a whole kafka cluster online. In the middle of the migration, some brokers will be using the old structures and some other brokers will be using the new structures. It's not clear what a consumer should behave in this stage.\n2. If something terribly wrong happens during the migration, there is no easy way to rollback the migration.\n\nAn alternative is to make the 0.8 release non-backward compatible. This allows us to incorporate any wire/on-disk changes freely. To upgrade from 0.7 to 0.8, one possibility is to start a new 0.8 cluster. We can provide a tool that continuously mirrors data from the 0.7 cluster to the new 0.8 cluster. Once that's done, we can first upgrade the consumers and point them to the 0.8 cluster, followed by upgrading the producers and pointing them to the 0.8 cluster. This is somewhat more operational work. However, it addresses both of the above issues. ", "I'm going to have to agree with Jun on this.  I really like the idea of reusing the existing mirroring logic and custom tool to provide a data-bridge between a 0.7 and 0.8 cluster.  Rollback becomes a non-issue and it allows us to fully revamp the write-protocol, properly constructing a new one that attempts to foresee any future modifications.  It definitely demands more work and potentially requires duplicated environments and some orchestration, but considering the changes required in the wire format I think it's the appropriate solution.\n\nJoe, just wanted to quickly know if you'd mind splitting this task with me?  I ask only because I had started some work on https://issues.apache.org/jira/browse/KAFKA-49 earlier.  Producer or consumer, either is fine, your pick if you're okay with it.", "I have been looking and going through the producer side so far only.  I was/am scratching my head a bit on how we want to handle the arrays and if we just want to put the topic serialized (the entire thing) and just put that in a position or exactly how we want to store it from the ByteBuffer perspective.\n\nSo in the 7 position would be ByteBuffer.wrap((data: [<topic_data_struct>]).toBytes)\n\nthat should be it I think would be enough we could serialize/deserialize it\n\nIf you want to go ahead and start on the consumer side I am going to give the Producer side a crack further at it tonight/tomorrow hopefully by Wednesday finished or will know what roadblock I have hit.", "So, one possibility I am looking at for the data [<topic_data_struct>] is to use ByteArrayOutputStream and ObjectOutputStream to make the List[WiredTopic] get into the byte buffer.  My concern here though is non JVM clients not being able to support it.\n\nAnother option is making this new format Thrift based (or protobuffer, etc) though I also appreciate and understand that adding yet another dependency yet it allows for language independence for what is now a complex data structure (not the simple 3 fields that exist now).  The entire format could be a Thrift object and the serialized bytes become a single put into the byte buffer\n\nThe last option I have considered is too basically loop through the data structures and increase the ByteBuffer position structure.  So after the ack timeout we could store some hint in the byte buffer to the [] of topic -> [partition,message]  e.g. we have 2 topics being published too with one of those topics having 2 partitions with a message se and the other having 3 paritions/messageet.  So we could create some \"hint\" to know the number of topics and each of the counts of paritions/message for each topic (in this case 2,2,3) and \"put\" the topic1, partitionA, messageA, partitionB,messageB, topic2, partitionC, messageC, partitionD,messageD, partitionE,messageE. The draw back here is some nuance complexity (bordering on esoteric) to take the object model break it out, store it and then pulling the stored value (based on the hint so we know what position is topic and which are partition.  The \"hint\" could be a delimited string maybe (if this is the approach that is adopted) count of topics and for each topic then the count of partition for those topics.  2,2,3 split on , [0] is the count of topics [1] is the count of partition/message for topic1 and [2] is the count of partition/message for topic 2\n\nmight be some other options here?  I am missing something/over complicating? my preference is the thrift approach but I appreciate the \"hint\" approach also and I would be quite alright with that too.... it works with no additive dependency drawback is a tad harder to have client code (\"driver\") adoption \n\nthoughts?  ", "Joe, thanks for working on this jira. Some comments:\n\n1. Using Thrift is one possible option for supporting multiple language bindings. If we want to use thrift, we probably want to use it for both producer and consumer. The concern that I have is that Thrift adds its own layer of RPC and may prevent us from doing things like zero-copy transfer on the consumer side. In any case, supporting multiple language binding is important, but a bigger topic. It is probably beyond the scope of this jira. I prefer that we follow it up in a separate jira.\n\n2. I think you last option is feasible. Basically, we just need a way to encode array structures. This can be done by encoding the array length at the beginning, followed by the encoding of each array element. Note that each array element can have nested arrays and we can encode them in the same way recursively. This is how MultiProduceRequest currently works.", "<< we just need a way to encode array structures\n\nright, here we have arrays within arrays so we need to have the \"hint\" to explain that in the position after ack_timeout\n\nwhat I am proposing is to have a string\n\nX,A,B,C\n\nsplit on \",\"\n\nX is the number of elements in the topics array\n\nA,B,C... (etc) would be for each topic how many partition array elements are there\n\nso this could look\n\n2,2,3\n\nmeaning 2 topic_data_struct elements, the first element of the topic array has 2 partition_data_struct elements in it's array, the second element in the topic array has 3 partition_data_struct elements\n\nthe walking the positions becomes easy and for the gets from the bytebuffer creating the objects\n\nif it was \n\n5,3,3,3,3,4 then 5 topics, the first 4 of them 3 elements and the last 4 elements\n\nI will start in on this approach", "Joe,\n\nI think it could be simpler than that. You can have the following encoding:\n\n# of topics (2 bytes)\ntopic1 encoding (see encoding format below)\n...\ntopicN encoding\n\nTopic Encoding:\ntopic name length (2 bytes)\ntopic name bytes\n# of partitions (2 bytes)\npartition1 encoding (see encoding format below)\n....\npartitionN encoding\n\nPartition encoding:\npartition id: (4 bytes)\nmessageSet encoding", "yup, that is much simpler :)\n\nhere is my first crack on ProducerRequest read and write https://gist.github.com/1636208 for the new format\n\nI will start chipping away refactoring for this change ", "more progress on the producer side https://gist.github.com/1655565 (not enough to upload patch but wanted to put the svn diff in case anyone took a look before I started in on it again tomorrow/monday\n\nhave made a few assumptions and still need to integrate with SyncProducer, ProducerPool, DefaultEventHandler and hook in the response but so far so good.", "Here's the updated FetchRequest: https://gist.github.com/1660067 .  Stil a work in progress will it's integrated into other components.\n\nJoe, a couple of comments:\n1. You don't need to include the \"size\" parameter in ProduceRequest as it's calculated in sizeInBytes() and invoked/written by BoundedByteBufferSend when transmitting on the network connection.\n2. You don't need to include the \"requestTypeId\" since it's passed automatically passed up to the Request superclass (eg. RequestType.ProductRequest) and written out/transmitted by the same BoundedByteBufferSend.\n\nOtherwise, looking good.  I'm expecting to wrap up by Wednesday if I can get some time in after work.", "Joe,\n\nThe changes in github make sense. A couple of suggestions:\n1. We can probably use case class for ProduceRequest.\n2. The return type of produce request handler should be Option[ProduceResponse] since the response is optional.", "Just an FYI that I've added a few more questions related to the FetchRequest side in the wire format wiki.", "Joe, Prashanth,\n\nDo you guys have patches that we can review now? We are getting very close to start using the new request format in 0.8.\n\nThanks,", "I am finishing up the changes for SyncProducer and should have that done later tonight.\n\nOnce those are done I will post them here and then I will jump into fixing the tests for my changes (which I will be working on over the weekend).\n\nPrashanth and I have overlap of our data structure classes (in my changes they are called WiredTopic, WiredPartition)  unless there is some good reason to keep these separate like we see them diverging we will have to fold these in together across our changes", "Cool, I have a draft of long-poll support in KAFKA-48. I am hard coding the values that are in the new fetch request (min_size, max_wait). We should discuss which goes in first since this will change the server side KafkaRequestHandler/KafkaApis code a fair amount. If you are refactoring to remove the old requests I expect that will be the bigger patch and it would be better for me to rebase to you than vice versa. But if you guys are a ways out I can go first too.", "here is the latest on the ProducerRequest  https://gist.github.com/1735455\n\nProducerResponse and getting test cases to work are still pending (will start on those tomorrow night) and then some cleanup TODOs\n\nI made assumption that only the correlation_id will be changing for each request and the other values will be for the entire producer (setting those are TODO in my diff)\n\nverifySendBuffer in SyncProducer seems seems to just be a tracing utility I think turning that into a toString() after a buffer read will give the same result in the new code, i figure there will be a bunch of other tweaks (like that) as I go through the test cases so leaving this for that iteration\n\nAlso I still need to gut out multi* request that is now inherent in the new ProducerRequest as it is", "Hi all,\n\nWasn't able to respond on Friday, but I've just wrapped up the last of the FetchRequest changes.  I've attached a patch you can apply to 0.8.  All tests should pass.  I'll need to coordinate with Joe on how to generate a full patch, or we can leave them separate and have the commiter apply them separately but consecutively.\n\nDetails:\n1. FetchRequest completely modified along with associatd java type.\n2. New FetchResponse, TopicData and PartitionData classes.  The latter *Data classes can be shared with the producer side as much of it is common.\n3. New *Send classes for FetchResponse to encapsulate send logic.  Pretty straightforward.\n3. All the test have been changed to make them multi-fetch by default.  Because they make use of the response and the send, I've not added new tests for them.\n4. I've removed the MultiFetchResponse classes along with associated java types, implicit conversions, request keys and tests.\n\nHave a look.  It's fairly massive so any and all feedback is welcome :)", "here are my changes https://gist.github.com/1748005 I don't want to attach a patch yet as test are not all passing yet (not sure yet if I am going to have time before Tuesday night to fix that) and still have a few TODO I have in the changes.\n\nfeedback welcome ++", "Hey Prashanth, this is great. Question do we need a builder class for FetchRequest versus just using named arguments? It doesn't make a big difference but it would probably be good for us to agree on a pattern and carry it forward for all the various requests. With named arguments presumably it would be something like:\n\nnew FetchRequest(correlationId = 2345454, versionId = 2, ...)", "The reason I made a builder was to separate how data is stored/sent from how the client logically thinks of and creates a request.  You're right though, named arguments would solve some of the problems but because of the nested nature of the object, the builder seems to help with how requests are constructed - it makes the tests cleaner and probably client code as well.  Either way, I'm okay with removing it.\n\nThere is one concern I have with the patch I'd like to solicity feedback on.  The fetch response sorts its messages by topic and partition and leverages this sorted nature when performing binary search when retrieving message sets.  This only works when the response is read from the socket by a client, not upon intial construction.  This asymmetry seems to bug me for some reason.  Anyone hav a more elegant solution?  Should we sort at all?\n\nAlso, the error code from the fetch response needs to move out into the send class.", "Hi - is there a description of the new FetchResponse?  Basically I want to make sure that it includes the topic name to allow for out of order responses which will allow for a future implementation of multiplexed requests/responses that is easier and more efficient than the current method which relies strictly on ordering of the responses.  This is strongly related to the async implementation that Jay is working on.", "I think I've answered my own question - the wiki contains the description and the field correlation_id will allow for this kind of implementation.  Excellent.", "Prashanth, thanks for the patch. Looks good overall.\n1. FetchResponse: Since we already have error code at the PartitionData level, should we get rid of the error code at the FetchResponse level? There is no obvious use case for the latter.\n2. Should FetchResponse.messageSet return ByteBufferMessageSet? That way caller doesn't have to cast.\n3. Could you rebase to the latest 0.8 branch?\n\nJoe, do you think we can take Prashanth's patch first and apply your patch on top later?", "Yes, sounds good.  I will apply his patch and refactor my changes to use his topic and partition classes for the data structure.", "Also, for the builder for FetchRequest. My concern is that it's not clear what the required parameters are.", "1, 2, 3. Done, patch attached.\n\nI also changed the wire format so that the versionId is sent earlier.  This should make brokers and clients handle protocol versioning easier without too much bit fiddling.", "Hey guys, so I started integrate my changes over Prashanth's changes (which are over the 0.8 branch now).\n\nI see now PartitionData has an error code for each partition (and the comments above in regards to that) ... the thing is that my ProducerResponse does not use this and has it's own array of errors (which makes sense).\n\nSo... do we leave out the the error in PartitionData (which is how the wiki is) or do I use PartitionData and just don't implement the error or do I use my own PartitionData class (like I was doing before with WiredPartition ????", "here is v3 patch for ProducerRequest changes and ProducerResponse also inclusive of v2 changes for Fetch\n\nnote a few things\n\n1) I did not integrate the topic & partition data classes from our two changes, I ran into a bump with not being able to get the buffer from the abstract MessageSet and figured we can do this later  wanted to get this up and reviewed see if anything else is left.  Prashanth if you have some ideas here let me know\n\n2) There is still a failed test in AsyncPoolTest\n\n3) I was not sure what to-do with errors and offsets in KafkaApi handleProducerRequest in building the ProducerResponse for what the values should be in that block\n", "Prashanth,\nThanks for the patch. Just committed KAFKA-240-FetchRequest-v2.patch with a minor fix (javaapi.FetchResponse.messageSet returns ByteBufferMessageSet instead of MessageSet, to make it consistent with the scala api). Technically, Response doesn't need initialoffset, since it can be obtained from the fetch request. Could you open another jira to get this fixed?\n\nJoe,\n\nThanks for the patch. Some comments:\n1. ProduceResponse: our original plan was to make it optional, i.e., only send response if ack is not 0. \n2. syncProducer.send should have 1 input parameter, ProduceRequest. Just like SimpleConsumer. Also, the scala version of syncProducer should not use javaapi.ProduceRequest.\n3. ProducerReqeust should use TopicData, instead of WiredData.\n\nAs for your questions.\n1) One possibility is to add a method getSerialized() in MessageSet. Only ByteBufferMessageSet will implement this method. FileBufferMessageSet can throw a non-supported exception.\n3) Offsets should be the offset of the log after the message sets are appended. We may need to change log.append to return a MessageSetAndOffset.\n\n", "Jun, ah, thanks for catching the missing changes in the java api.  Regarding the second point, you're completely right.  That change was made to simplify when reading the response at the client since the ByteBufferMessageSet requires an initial offset - this offset can always be set after the fact.  Ticket is KAFKA-271.", "Prashanth,\n\nAnother thing. We don't enforce that a given topic appears in at most 1 OffsetDetail in a FetchRequest. We will need to do that since FetchResponse code assumes that. ", "Good point.  I can make that change in this ticket rather than creating a new one.\n\nIf so, I'd just get rid of the builder and put that grouping logic into the request itself.  Each client will have to do likewise and the broker can enforce the grouping as well to be safe?", "Yes, we should enforce the same check in the broker (mostly for non-java clients).", "Validation v1 patch attached.  You should be able to apply against 0.8.  Pre-emptive apology to Joe who migh need to upload new patch :(", "Thanks Prashanth, excellent patch. Committed Validation v1 patch to 0.8.", "cool, so I need to rebase, update my changes for the few issues that were called out and submit a new patch.  Not sure if I will have time tomorrow but by Monday will try sooner though, np", "Attached are my changes from the last round of feedback and rebased on the branch from commits.\n\nStill have 1 failing test\n\n[error] Test Failed: testBrokerListAndAsync\n\nand I need to still put the offsets in the response\n\nother items changed and related refactoring done", "Joe,\n\nThanks for the patch. We are almost there. Some comments:\n\n1. ProducerRequest is case class. So the equal() you want is alway defined for you. No need to redefine it. Not sure what scala does for hashCode for case class and whether we should override it or not.\n2. javaapi.SyncProducer: We should get rid of send(topic: String, messages: ByteBufferMessageSet) and only keep send(producerRequest: kafka.javaapi.ProducerRequest).\n3. KafkaApis: We should get rid of the handling of MultiProduce and only support Produce requests. Also, in handleProducerRequest, we should return None if reqeust.required_acks ==0 (not ack_timeout == 0)\n4. DefaultEventHandler.send(): use ProducerRequest without the package name. Also, use the values in ProducerConfig to fill in parameters (like clientid) of ProducerRequest (this is probably what's causing the unit test testBrokerListAndAsync to fail).\n5. FetchResponse: broken lines in PartitionData\n6. javaapi.FetchResponse: could you make data a private member?\n7. The way that SyncProducer sends a ProducerRequest over socket is to first serialize the whole request in a bytebuffer and then sends the bytebuffer through socket. An alternative is to send the request like FetchReponse, using a ProduceRequestSend that reuses TopicDataSend. This avoids code duplication and is more efficient since it sends data in ByteBufferMessagesSet directly to socket and avoids extra copying from messageset to bytebuffer.\n8. javaapi.ProducerRequest: We will need to define a java version of TopicData so that java producers can create request conveniently. The java version of TopicData will use the java version of ByteBufferMessageSet. \n\nFor #7 and #8, it's fine to have them fixed in separate jiras (just link the jiras here).\n", "for #2\n\nthe reason I left send(topic: String, messages: ByteBufferMessageSet)  was because I was unsure what we wanted to-do with the changes that would result in KafkaRecordWriter and DataGenerator both would need to start to implement and pass through data or I could stub things out so it compiles and have separate tickets for each of those implementations which I do not know much about at all (yet)", "If you are not addressing #8 in this jira, we can track #2 and #8 together in a separate jira.", "1) done.  hashCode is implemented by the scala runtime for case which interogates all members http://lampsvn.epfl.ch/trac/scala/browser/scala/branches/2.8.x/src/library/scala/runtime/ScalaRunTime.scala#L151 i removed it for now\n2) KAFKA-288\n3) done. done.\n4) done. done. but still getting the failure of kafka.producer.AsyncProducerTest\n5) errr, I don't see this looks ok to me maybe something in how the patch got applied? I tested it again on another box for what I am uploading no issue.  \n6) done\n7) KAFKA-288\n8) KAFKA-289", "I forgot to remove these 2 files are not needed anymore\nsvn delete .//core/src/main/scala/kafka/api/MultiProducerRequest.scala\n\nsvn delete .//core/src/main/scala/kafka/server/MultiMessageSetSend.scala\n\nonly change from v3", "Joe, \n\nThere are 2 reasons why the unit test fails.\n1. Some default values for ProducerRequest are not set consistently.\n2. The bigger issue is that in scala, Array equal is only testing equal on reference, not the actual value. This is different from how other Seq classes like List behave. To fix that, I have to explicit define equal in ProdueRequest and TopicData.\n\nAttached is a patch of only 4 scala files that I modified. Apply your v4 patch, revert those 4 files and apply my patch. The unit test passes now. \n\nA few other things:\n9. When deserilizing bytes to ProducerRequest, we need to set version id.\n10. In KafkaApi, we should send no ack on ProducerRequest since the producer is not reading the response yet. Add a comment that this will be fixed in kafka-49.\n\nOnce the above 2 issues are addressed, you can just commit without further review. Finally, would you mind committing after kafka-239? That patch changed quite a few unit tests and may take a bit more time to refactor.\n", "ok, sounds good.  I will commit the changes with these last 2 issues after rebasing on kafka-239 once it is committed to the branch, np", "Joe, \n\nkafka-239 has been committed to the 0.8 branch. You are good to go. After rebasing your code, make sure all unit tests still pass.", "Hi Jun, on #9 \n\n  def readFrom(buffer: ByteBuffer): ProducerRequest = {\n    val version_id: Short = buffer.getShort\n\nthat is already there, do you mean something else or different than what I am looking at?", "Yes, however, that information is not passed into ProduceRequest. So the server doesn't know the version of the producer client.", "It is written to the buffer though it is not passed in when constructing the ProducerRequest\n\nval version_id: Short = ProducerRequest.version_id\n\nhappens during the constructor\n\nsince actual changes come in the code I though having this be on the object leve and part of the jar the publisher was calling made the most sense.  It seemed that the client might not need or want to have this knowledge but I can pass it into the constructor, not a problem.\n\nAlso Prashanth pointed out my member vars are not consistent with the rest of the system and camel case I will make that change also to be consistent.", "Joe,\n\nWhen the client creates a ProduceRequest, we don't need the client to specify the version_id explicitly, since it's embedded in the client library. However, when we deserialize received bytes from a client into a ProduceRequest on the broker, we don't want to pick up the version_id that the server is on. Instead, we need to know the version of the client. So, we will need 2 constructors for ProduceRequest, one with version_id and one without. The former will be used on the broker and the latter will be used in the producer client. ", "Thanks Jun, I get the issue now. Right, so after I read the byte buffer I need to return a ProducerRequest contructed with the versionId which right now is just dangling var.  ok, got it.", "9) done\n10) done\n\nchanged member vars to camel case for consistency\n\ntests passing\n\ncommitted to 0.8 branch", "I got a compilation error on 0.8. \n\n\u001b/home/jrao/Intellij_workspace/kafka_0.8/core/src/main/scala/kafka/server/KafkaApis.scala:61: not found: type ProducerResponse\u001b[\n  private def handleProducerRequest(request: ProducerRequest, requestHandlerName: String): Option[ProducerResponse] = {\u001b[\n", "committed missing file and confirmed compile on svn update and all tests passing\n\nresolving I think this ticket is all good now, I do not see anything missing... everything falling out from this ticket is being tracked in other JIRA"], "derived": {"summary": "We want to change the producer/consumer request/response format according to the discussion in the following wiki:\n\nhttps://cwiki. apache.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "implement new producer and consumer request format - We want to change the producer/consumer request/response format according to the discussion in the following wiki:\n\nhttps://cwiki. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "committed missing file and confirmed compile on svn update and all tests passing\n\nresolving I think this ticket is all good now, I do not see anything missing... everything falling out from this ticket is being tracked in other JIRA"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-241", "title": "ConsumerIterator throws a IllegalStateException after a ConsumerTimeout occurs", "status": "Closed", "priority": "Major", "reporter": "Patricio Echague", "assignee": "Jun Rao", "labels": ["newbie"], "created": "2012-01-06T00:55:34.000+0000", "updated": "2014-06-19T05:16:08.000+0000", "description": "Please find the test case attached.\n\nAfter a timeout occurs (property consumer.timeout.ms > 0 ) the consumerIterator throws an IllegalStateException.\n\nThe work around seems to be to recreate the MessageStream an issue a new Iterator.", "comments": ["Patch attached.", "Added a unit test that exposes the bug and a fix.", "+1\n", "Thanks for the review. Just committed this."], "derived": {"summary": "Please find the test case attached. After a timeout occurs (property consumer.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "ConsumerIterator throws a IllegalStateException after a ConsumerTimeout occurs - Please find the test case attached. After a timeout occurs (property consumer."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for the review. Just committed this."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-242", "title": "Subsequent calls of ConsumerConnector.createMessageStreams cause Consumer offset to be incorrect", "status": "Resolved", "priority": "Major", "reporter": "David Arthur", "assignee": null, "labels": [], "created": "2012-01-06T17:21:02.000+0000", "updated": "2017-10-29T09:18:05.000+0000", "description": "When calling ConsumerConnector.createMessageStreams in rapid succession, the Consumer offset is incorrectly advanced causing the consumer to lose messages. This seems to happen when createMessageStreams is called before the rebalancing triggered by the previous call to createMessageStreams has completed. ", "comments": ["Annotated log messages from two different test runs. For each test run, 20k messages were written to \"mytopic\" and \"mytopic1\" each with no consumers running. Once the messages had all been sent, the consumers were started up", "Does this bug still exist in 0.8?", "[~jkreps] I am hitting the same issues \n\nI opened JIRA here https://issues.apache.org/jira/browse/KAFKA-2331\n\nI am calling ConsumerConnector.createMessageStreams fastly and it seems that it is not handling rebalancing of partitions correctly.", "Does this bug still exist in 0.10?", "\ncan upgrade and use new consumer, you can get it from\n https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Detailed+Consumer+Coordinator+Design", "Closing inactive issue. The old consumer is no longer supported."], "derived": {"summary": "When calling ConsumerConnector. createMessageStreams in rapid succession, the Consumer offset is incorrectly advanced causing the consumer to lose messages.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Subsequent calls of ConsumerConnector.createMessageStreams cause Consumer offset to be incorrect - When calling ConsumerConnector. createMessageStreams in rapid succession, the Consumer offset is incorrectly advanced causing the consumer to lose messages."}, {"q": "What updates or decisions were made in the discussion?", "a": "Closing inactive issue. The old consumer is no longer supported."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-243", "title": "Improve consumer connector documentation to include blocking semantics of kafka message streams", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": null, "labels": [], "created": "2012-01-06T19:51:33.000+0000", "updated": "2017-10-29T09:16:32.000+0000", "description": "http://markmail.org/message/6m46awa5bzsxg7cq?q=Questions+on+consumerConnector%2EcreateMessageStreams%28%29+list:org%2Eapache%2Eincubator%2Ekafka-users\n\nIt will be good to improve the ScalaDocs for explaining the semantics of the ConsumerIterator", "comments": ["Closing inactive issue. The old consumer is no longer supported, please upgrade to the Java consumer whenever possible."], "derived": {"summary": "http://markmail. org/message/6m46awa5bzsxg7cq?q=Questions+on+consumerConnector%2EcreateMessageStreams%28%29+list:org%2Eapache%2Eincubator%2Ekafka-users\n\nIt will be good to improve the ScalaDocs for explaining the semantics of the ConsumerIterator.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Improve consumer connector documentation to include blocking semantics of kafka message streams - http://markmail. org/message/6m46awa5bzsxg7cq?q=Questions+on+consumerConnector%2EcreateMessageStreams%28%29+list:org%2Eapache%2Eincubator%2Ekafka-users\n\nIt will be good to improve the ScalaDocs for explaining the semantics of the ConsumerIterator."}, {"q": "What updates or decisions were made in the discussion?", "a": "Closing inactive issue. The old consumer is no longer supported, please upgrade to the Java consumer whenever possible."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-244", "title": "Improve log4j appender to use kafka.producer.Producer, and support zk.connect|broker.list options  ", "status": "Resolved", "priority": "Major", "reporter": "Stefano Santoro", "assignee": "Stefano Santoro", "labels": ["log4j", "newbie"], "created": "2012-01-09T19:08:32.000+0000", "updated": "2012-09-22T11:17:17.000+0000", "description": "Taken from #kafka IRC session with Neha Narkhede:\nThe log4j appender is quite obsolete, there are a few things to change there. Make it use the kafka.producer.Producer instead of SyncProducer. That allows you to use either the broker.list or the zk.connect option\n", "comments": ["see attached patch file", "with this patch the kafka lof4j appender supports formatters. I tested it well against logger properties specifying broker.list, but it hangs when I specify zk.connect instead. The new log4j properties are serializerClass (instead of encoder for serilizer.class), zkConnect (for zk.connect), and brokerList (for broker.list)", "Thanks for getting started on this. I have a couple of requests -\n\n1. Could you please upload a patch to this JIRA and grant it to Apache ? \n2. This patch doesn't apply cleanly to trunk, would you mind updating a patch that would ? \n\nIt will make it much easier for us to review it. \n\nAlso, when you said the zk connect option hangs, did you get a chance to take a look at the debug/trace logs to see where it hangs ? Do you mind uploading those log files here ?", "As you requested I have uploaded the patch as a file which I have generated as follows:\n\nssantoro@stefanoubu:~/dev/kafka$ svn update .\nAt revision 1230275.\n\nssantoro@stefanoubu:~/dev/kafka$ svn diff . > Log4jAppender.patch \nssantoro@stefanoubu:~/dev/kafka$ \n", "Thanks for uploading the patch. A couple of comments -\n\n1. As part of adding the broker.list and zk.connect options, it will be a good idea to get rid of host and port usage in the log4j appender\n2. If both zk.connect and broker.list options are specified, maybe throwing an InvalidConfigException would be good, specifying that only one option should be set.\n3. Also, it will be good to add tests to test the zk.connect option in the log4j appender.", "I opted for configuration flexibility by assigning an option specification priority: zk.connecf, broker.list, and host/port pair (which is then translated into broker.list) If more then one is specified, then the one with the highest priority is picked. What I was hoping for though is that you wold help me understand why specifying broker.list works, but specifying zk.connect hangs. ", "We had the host and port options before we ported all of that to the broker.list option. It seems like a good idea to keep configuration consistent across the code. Regarding the zk.connect option, I'm interested in seeing why it hangs too, though it sounded like you had a test that failed. I'll try writing one myself and see how that goes.", "Added unit test for the ZkConnect kafka log4j appender  parameter. The appender now works with both connection scenarios: broker.list and zk.connect", "Thanks for uploading patch v2. I made one minor change to the patch - \n\nRemoved the host and port parameters. They seem redundant given that we support the broker.list parameter.", "Thanks for your time for taking a look at it, and modifying it. I enjoyed the experience. Let me know if I can help you during on any other tasks during my off time. ", "Stefano,\n\nCommitted this. Thanks for working on this patch ! If you are interested in contributing some more, please take a look at https://issues.apache.org/jira/secure/IssueNavigator.jspa?mode=hide&requestId=12318577. These are list of newbie jiras that are a good start to understanding various Kafka components. \n\nWe will be glad to help out where required.", "Hello,\n\nKafkaLog4jAppender 0.7.1 as commited here has a critical bug caused by a Log4j deadlock, which is not captured in the embedded servers \"integration\" test, let me explain why.\n\nIf Log4j initializes for the first time from the log4j.properties containing kafka appenders, then, when activateOptions() is called during Log4j boot, zookeeper client is being initialized and its send thread and event thread started. The send thread issues a LOG.info. But since Log4j is just starting up, that LOG is not yet created, or bound to the right parent, say logger.org.apache.zookeeper.client, so it is trying a synchronized(rootLogger). But that rootLogger is just under creation or at least locked by the other thread, ramping up Log4j infrastructure. So its a sheer Log4j deadlock.\n\nWhy the tests don't fail its because Log4j is already initialized at that time, and only reconfigured. So until reconfiguration is complete, it keeps using old loggers.\n\nSo if you put KAFKA on rootLogger, or even elsewhere, the KafkaAppender which connects to ZK during activateOptions causes Log4j deadlock if Log4j is not already initialized. We have written a wrapper that postpone activateOptions after Log4j bootstrap, and it is called in a ServletContextListner.start().\n\nhere is my test class reproducing the issue:\n\n\n\nclass DeferredActivationLog4jAppenderIntegrationTest extends Logging {\n\n  val ZKHOST: String = \"localhost\"\n  val zookeeperConnect: String = ZKHOST + \":8083\"\n  var logDirZk: File = null\n  var kafServer: KafkaServer = null\n\n  var zkConsumer: SimpleConsumer = null\n\n  val tLogger = Logger.getLogger(getClass())\n\n  private val brokerZk = 0\n\n  private val ports = choosePorts(1)\n  private val portZk = ports(0)\n\n  private var zkServer: EmbeddedZookeeper = null\n\n  @BeforeClass\n  def before() {\n    before(true)\n  }\n\n  protected def before(retry: Boolean) {\n    System.setProperty(Constants.ZOOKEEPER_RETRY, retry.toString)\n    System.setProperty(Constants.ZOOKEEPER_HOSTS, zookeeperConnect)\n    zkServer = new EmbeddedZookeeper(zookeeperConnect)\n\n    val propsZk = createBrokerConfig(brokerZk, portZk)\n    val logDirZkPath = propsZk.getProperty(\"log.dir\")\n    logDirZk = new File(logDirZkPath)\n    kafServer = createServer(new KafkaConfig(propsZk))\n\n    Thread.sleep(100)\n\n    zkConsumer = new SimpleConsumer(ZKHOST, portZk, 1000000, 64 * 1024)\n  }\n\n  @AfterClass\n  def after() {\n    zkConsumer.close\n\n    kafServer.shutdown\n    Utils.rm(logDirZk)\n\n    Thread.sleep(200)\n    zkServer.shutdown\n    Thread.sleep(200)\n  }\n\n  @Test\n  def testZkConnectLog4jAppends() {\n    setupLog4j\n\n    for (i <- 1 to 5)\n      info(\"test\")\n\n    Thread.sleep(500)\n\n    var offset = 0L\n    val messages = zkConsumer.fetch(new FetchRequest(\"test-topic\", 0, offset, 1024 * 1024))\n\n    var count = 0\n    for (message <- messages) {\n      count = count + 1\n    }\n\n    assertEquals(5, count)\n  }\n\n\n  def setupLog4j {\n    PropertyConfigurator.configure(getLog4jConfigWithZkConnect)\n  }\n\n  private def getLog4jConfigWithZkConnect: Properties = {\n    var props = new Properties()\n    props.put(\"log4j.debug\", \"true\")\n    props.put(\"log4j.appender.STDOUT\", \"org.apache.log4j.ConsoleAppender\")\n    props.put(\"log4j.appender.STDOUT.Target\", \"System.out\")\n    props.put(\"log4j.appender.STDOUT.layout\", \"org.apache.log4j.PatternLayout\")\n    props.put(\"log4j.appender.STDOUT.layout.ConversionPattern\", \"%-5p: %c - %m%n\")\n\n    //    props.put(\"log4j.appender.KAFKA\", \"com.adobe.pass.commons.kafka.producer.DeferredActivationKafkaLog4jAppender\")\n    props.put(\"log4j.appender.KAFKA\", \"kafka.producer.KafkaLog4jAppender\")\n\n    props.put(\"log4j.appender.KAFKA.Topic\", \"test-topic\")\n\n    props.put(\"log4j.appender.KAFKA.ZkConnect\", zookeeperConnect)\n    props.put(\"log4j.appender.KAFKA.layout\", \"org.apache.log4j.PatternLayout\")\n    props.put(\"log4j.appender.KAFKA.layout.ConversionPattern\", \"%-5p: %c - %m%n\")\n\n    props.put(\"log4j.logger.org\", \"TRACE, KAFKA\")\n    props.put(\"log4j.logger.com\", \"TRACE, KAFKA\")\n\n    props.put(\"log4j.rootLogger\", \"INFO\")\n    props\n  }\n", "it looks similar to the initial test but in debug i found that this one configures log4j for the first time when explictly called in test, while the other was already initialized at that point (was a second call to log4j PropertyConfigurer.doConfigure).\nOther info: my tests use TestNG attributes. I mainly reproduced it with rootLogger using Kafka appender, but also when it did not.\n"], "derived": {"summary": "Taken from #kafka IRC session with Neha Narkhede:\nThe log4j appender is quite obsolete, there are a few things to change there. Make it use the kafka.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Improve log4j appender to use kafka.producer.Producer, and support zk.connect|broker.list options   - Taken from #kafka IRC session with Neha Narkhede:\nThe log4j appender is quite obsolete, there are a few things to change there. Make it use the kafka."}, {"q": "What updates or decisions were made in the discussion?", "a": "it looks similar to the initial test but in debug i found that this one configures log4j for the first time when explictly called in test, while the other was already initialized at that point (was a second call to log4j PropertyConfigurer.doConfigure).\nOther info: my tests use TestNG attributes. I mainly reproduced it with rootLogger using Kafka appender, but also when it did not."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-245", "title": "upgrade to zkclient 0.1", "status": "Closed", "priority": "Major", "reporter": "Pierre-Yves Ritschard", "assignee": "Pierre-Yves Ritschard", "labels": ["newbie"], "created": "2012-01-10T17:28:54.000+0000", "updated": "2014-06-19T05:16:19.000+0000", "description": "the zkclient jar bundled with kafka should be synced with what is available on maven central. the artifact which has group com.github.sgroschupf, artifact id zkclient and version 0.1 is from a day after the one bundled with kafka and should thus be sufficient for kafka's needs.\n\nI have tested it locally and find no regressions.", "comments": ["Solve kafka-245", "With the following patch changing the version of the zkclient artifact, it is now possible to fetch it directly from maven central. This kills the zkclient specific code in KafkaProject.config", "Thanks for the patch ! One thing missing in there is deleting the zkclient jar in core/lib, since now it will be pulled from Maven. However, I made that change and will check in this patch.", "Committed this", "Do we know the github revision/date corresponding to the zkclient 0.1 release?", "From Maven, the date is 04-13-2011. ( http://repo1.maven.org/maven2/com/github/sgroschupf/zkclient/maven-metadata.xml )\n\nThe one we had was zkclient-20110412.jar which was a day older. Am I right ?", "the jar contains files from april 13th, there's a \"0.1.0\" git tag but the last commit is from 2010"], "derived": {"summary": "the zkclient jar bundled with kafka should be synced with what is available on maven central. the artifact which has group com.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "upgrade to zkclient 0.1 - the zkclient jar bundled with kafka should be synced with what is available on maven central. the artifact which has group com."}, {"q": "What updates or decisions were made in the discussion?", "a": "the jar contains files from april 13th, there's a \"0.1.0\" git tag but the last commit is from 2010"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-246", "title": "log configuration values used", "status": "Closed", "priority": "Major", "reporter": "Jun Rao", "assignee": null, "labels": ["newbie"], "created": "2012-01-13T15:39:32.000+0000", "updated": "2012-09-04T17:23:55.000+0000", "description": "Currently, it's hard to figure out which configuration value is being used and whether a new configuration is being picked up. Logging all configuration values during startup time can address this issue. We should cover broker, producer and consumer.", "comments": ["Saw label \"newbie\" and gave it a shot.  Attached is a patch for the server, only, in 0.8.  If this is acceptable, I'll do the producer and consumer in the same way.  I'm confused as to which is more appropriate Logger.getLogger or extends Logging.  I see it done both ways in source.  Let me know.\n\nNew to scala and kafka and just trying to learn my way around it before incorporating it into a project.", "Thanks for the patch!\n\nI wonder if instead of adding a logging statement for each value which is a little repetitive we could instead do the following:\nCreate a Props object that wraps Properties and has helpers like getInt,  getString, etc. Something like this: https://github.com/voldemort/voldemort/blob/master/src/java/voldemort/utils/Props.java\nThen add something internal to this helper that records whether a given property is used or not (i.e. a property is used if the get method is called for that key). This class could be used to log out all configuration in a single place and also could log WARN messages for any properties that are not used (since that is likely a typo).\n", "Yes, I agree, it was a little tedious.  This is my first foray into scala, and kafka, so kept it simple.  I might just round out the producer and consumer the easy way, and let someone who knows what they are doing do the fancy version when they get time.  I haven't peeked to see if there is code that iterates the entire config file to find unused values, but, you are right, it would be nice to be warned about a typo.", "I refactored my patch and made the changes to the producer and consumer as well.  This patch supersede's the broker-0.8.patch file.\n\nThere may need to be list support in Logging.scala, but since I didn't have any familiarity with those settings, I skipped it for now.", "This is a duplicate of KAFKA-181."], "derived": {"summary": "Currently, it's hard to figure out which configuration value is being used and whether a new configuration is being picked up. Logging all configuration values during startup time can address this issue.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "log configuration values used - Currently, it's hard to figure out which configuration value is being used and whether a new configuration is being picked up. Logging all configuration values during startup time can address this issue."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is a duplicate of KAFKA-181."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-247", "title": "max.message.size and fetch.size defaults should be consistent", "status": "Closed", "priority": "Minor", "reporter": "Blake Matheny", "assignee": "Pierre-Yves Ritschard", "labels": ["newbie"], "created": "2012-01-13T18:21:36.000+0000", "updated": "2014-06-19T05:16:27.000+0000", "description": "The default max.message.size for a producer is ~976kB. The default fetch.size for a consumer is 300kB. Having the default fetch.size less than the default max.message.size causes new users with messages larger than fetch.size to run into the InvalidMessageSizeException issue.\n\nMaking the default max.message.size less than or equal to the default fetch.size would eliminate that problem for most new setups.", "comments": ["The rationale here is to bump fetch.size a bit beyond the\ndefault produce size.\n\nCoincidentally, it seems that the MaxFetchSize property is used\nnowhere throughout the code.", "Pierre-Yves, thanks for the patch. Took MaxFetchSize out and committed the patch."], "derived": {"summary": "The default max. message.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "max.message.size and fetch.size defaults should be consistent - The default max. message."}, {"q": "What updates or decisions were made in the discussion?", "a": "Pierre-Yves, thanks for the patch. Took MaxFetchSize out and committed the patch."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-248", "title": "In the class kafka.producer.DefaultPartitionner the ligne key.hashCode % numPartitions may return a negative value. this is not a good thing knowing that partittion is (always) > 0. In this case an invalide partition id is thrown in the producer.", "status": "Resolved", "priority": "Major", "reporter": "Bertrand Perrin", "assignee": null, "labels": ["patch"], "created": "2012-01-16T11:41:12.000+0000", "updated": "2012-01-16T15:54:21.000+0000", "description": "In the class kafka.producer.DefaultPartitionner the ligne key.hashCode % numPartitions may return a negative value. this is not a good thing knowing that partittion is (always) > 0. In this case an invalide partition id is thrown in the producer.\n\na patch could be :  key.hashCode.abs  % numPartitions", "comments": ["This is already fixed in 0.7."], "derived": {"summary": "In the class kafka. producer.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "In the class kafka.producer.DefaultPartitionner the ligne key.hashCode % numPartitions may return a negative value. this is not a good thing knowing that partittion is (always) > 0. In this case an invalide partition id is thrown in the producer. - In the class kafka. producer."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is already fixed in 0.7."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-249", "title": "Separate out Kafka mirroring into a stand-alone app", "status": "Resolved", "priority": "Major", "reporter": "Joel Jacob Koshy", "assignee": "Joel Jacob Koshy", "labels": [], "created": "2012-01-20T22:36:02.000+0000", "updated": "2012-04-07T00:05:59.000+0000", "description": "I would like to discuss on this jira, the feasibility/benefits of separating\nout Kafka's mirroring feature from the broker into a stand-alone app, as it\ncurrently has a couple of limitations and issues.\n\nFor example, we recently had to deal with Kafka mirrors that were in fact\nidle due to the fact that mirror threads were not created at start-up due to\na rebalancing exception, but the Kafka broker itself did not shutdown. This\nhas since been fixed, but is indicative of (avoidable) problems in embedding\nnon-broker specific features in the broker.\n\nLogically, it seems to make sense to separate it out to achieve better\ndivision of labor.  Furthermore, enhancements to mirroring may be less\nclunky to implement and use with a stand-alone app.  For example to support\ncustom partitioning on the target cluster, or to mirror from multiple\nclusters we would probably need to be able to pass in multiple embedded\nconsumer/embedded producer configs, which would be less ugly if the\nmirroring process were a stand-alone app.  Also, if we break it out, it\nwould be convenient to use as a \"consumption engine\" for the console\nconsumer which will make it easier to add on features such as wildcards in\ntopic consumption, since it contains a ZooKeeper topic discovery component.\n\nAny suggestions and/or objections to this?\n", "comments": ["Overview of changes:\n\n- New abstract consumer-agent that embeds a topic event watcher to allow\n  topic discovery (similar to what the embedded consumer does). It provides\n  a processMessage hook that concrete implementations can specify.\n- New stand-alone mirror-maker tool that extends from consumer-agent.\n- Console consumer now also extends from consumer-agent, and supports\n  wildcarding in the topic list.\n- New mirror-maker system test that is similar to the embedded consumer\n  system test, but tests with multiple source clusters. When/if we deprecate\n  the embedded consumer we can delete its system test.\n\nSome comments:\n\n- Shutdown logic is somewhat tricky - and some of it was driven by\n  requirements from console consumer. Let me know if you have suggestions on\n  restructuring to make it simpler. The afterStoppingWorkerThread call is\n  before the workersStoppedLatch.countDown so the concrete implementation\n  should not block there forever. Or we could just move it after the\n  countDown, or remove that hook altogether.\n- I thought it would be best to continue to have the embedded consumer and\n  deprecate it later. So I also kept the \"mirror\" prefix in the\n  whitelist/blacklist consumer config options.\n- The mirror-maker tool right now only uses one producer - which is \"ok\"\n  with hacking around the broker.list config property to use multiple\n  producer send threads underneath. However, we probably want multiple\n  producers. So we can keep this under review until after Kafka-253 is\n  merged into 0.7/trunk as that will affect this piece of code.\n- Wildcarding is standard Java regex. However, for convenience a\n  comma-separated list of topics is allowed, in which case ',' is replaced\n  by '|'. In doing so, I have assumed that commas were never allowed as part\n  of topic names.\n- If the whitelist is non-trivial (where triviality is defined by having\n  only alpha-numeric characters and '|') then a topic event watcher is\n  created. Otherwise, no watcher is created. So for example, \"whitetopic.*\"\n  would require a topic watcher, but \"whitetopic01,whitetopic02\" would not.\n- I would have preferred making consumer agent a trait, but that would limit\n  extending it in Java as it contains implementation.\n- I needed to synchronize console consumer's processMessage to implement\n  support for the maxMessages option. One way to avoid this complexity is to\n  add maxMessages option to ConsumerConfig. If it is set, then we count\n  numMessages as an atomic long in ConsumerAgent's loop and add (numMessages\n  < maxMessages) to the Stream.continually condition.\n- There is a consumer shutdown bug (KAFKA-282). It is a simple fix but I\n  think it is better to keep that in a separate jira.\n\nSo how about we keep this pending until the producer refactoring has been\nwell-tested and makes it to trunk? In the interim I can also prepare changes\nto update the Mirroring-howto and add a console consumer how-to as well.\n\n(Also, thanks to Neha and Jun for helping with debugging this.)\n", "Could we support subscribing to wildcard topics directly in consumerConnector? We could add a new api like the following:\n\ncreateMessageStream(topicRegex: String) : KafkaMessageStream\n\nAll topics that match topicRegex will be returned in a single message stream, which can then be iterated.", "+1", "I meant +1 on the suggestion, not my own patch :)\n\nThe approach you propose is more generic and simpler. It should be possible by using just one queue for all the fetchers.\n", "+1 for feature", "Sorry about the delay on this. Here is a patch incorporating the design\nchange. Overview of changes:\n\n- Added topic watcher to ZookeeperConsumerConnector for creating message\n  streams based on filters.\n- The API is slightly different from the previous patch - just one\n  createMessageStreamsByFilter call instead of separate ones for\n  whitelist/blacklist.\n- Since we may now iterate over messages from multiple topics, added a new\n  KafkaMessageAndTopicStream and TopicalConsumerIterator, that iterates over\n  MessageAndTopic objects.\n- For wildcarded consumption, the topic count string may need to change in\n  ZK for new topics. To avoid additional logic to handle this, added\n  WildcardTopicCount for this, distinguished from StaticTopicCount.\n  WildcardTopicCount's encoding is described in the TopicCount class.\n- New mirror maker tool.\n- Updated mirror maker system test.\n- Updated console consumer which now allows one of: topic, whitelist,\n  blacklist\n- Added logIdent field to Logging trait, defaults to \"\".  Not so sure this\n  is a great idea, but the reason I needed this is that the mirror-maker may\n  instantiate multiple ZK connectors and it is very unclear which messages\n  come from which connector.\n  E.g.,\n  [2012-03-26 17:10:06,114] INFO group1_jkoshy-ld-1332807005602-c0176b3f Committing all offsets after clearing the fetcher queues (kafka.consumer.ZookeeperConsumerConnector)\n\nA few other comments:\n\n- Apparently there are issues in recreating messages streams from the same\n  zkconnector, so I have disabled it for all the createMessageStreams*\n  methods. The createMessageStreamsByFilter method will only allow one call\n  to it the way it is implemented, but I don't think that is a serious\n  limitation.\n- I noticed a small caveat in using joptsimple - if I say --whitelist \".*\"\n  it interprets it as \".\". However, --whitelist=\".*\". --whitelist \".+\" all\n  work.\n- I encountered a small shutdown issue - in the system test, I have two\n  connectors. When shutting them down, the first one to shut down triggers a\n  rebalance in the other connector. However, that connector is itself\n  shutting down and sets zkclient to null. So I see null pointer exceptions\n  due to accessing ZK as part of rebalance. We should probably add a\n  isRebalancing atomic bool and not shutdown if that is set, and vice-versa.\n  I can roll that in as part of this patch if it makes sense.\n", "Thanks for the patch. Some comments:\n\n1. For future extension, I am thinking that we should probably unifying KafkaMessageStream and KafkaMessageAndTopicStream to sth like KafkaMessageMetadataStream. The stream gives a iterator of Message and its associated meta data. For now, the meta data can be just topic. In the future, it may include things like partition id and offset.\n\n2. ZookeeperConsumerConnector:\n2.1 updateFetcher: no need to pass in messagStreams\n2.2 ZKRebalancerListener: It seems that kafkaMessageStream can be immutable.\n2.3 createMessageStreamByFilter: topicsStreamsMap is empty when passed to ZKRebalanceListener. This means that the queue is not cleared during rebalance.\n2.4 consumeWildCardTopics: I find it hard to read the code in this method. Is there a real benefit to use implicit conversion here, instead of explicit conversion? It's not clear to me where the conversion is used. The 2-level tuple makes it hard to figure out what the referred fields represent. Is the code relying on groupedTopicThreadIds being sorted by (topic, threadid)? If so, where is that enforced.\n\n3. KafkaServerStartable: Should we remove the embedded consumer now?\n\n4. Utils, UtilsTest: unused import\n", "This is a cool feature. Thanks for the patch ! \n\nA couple of questions -\n\n1. It seems awkward that there is a MessageStream trait and the only API it exposes is clear(). Any reason it doesn't expose the iterator() API ? From a user's perspective, one might think, since it is a stream, it would expose stream specific APIs too. It will be good to add docs to that API to explain exactly what it is meant for. \n\n2. I like the idea of having just one KafkaMessageMetadataStream. \n\n3. There is some critical code that is duplicated in the ZookeeperConsumerConnector. consume() and consumeWildcardTopics() have some code in common. It would be great if this can be refactored to share the logic of registering session expiration listeners, registering watches on consumer group changes and topic partition changes. \n\n4. Could you merge all the logic that wraps the wildcard handling in one API ? Right now, it is distributed between createMessageStreamsByFilter and consumeWildcardTopics. It will be great if there is one API that will pre process the wild cards, create the relevant queues and then call a common consume() that has the logic described in item 5 above. \n\n5. There are several new class variables called wildcard* in ZookeeperConsumerConnector. I'm thinking they can just be variables local to createMessageStreamsByFilter ?\n\n6. There is a MessageAndTopic class, that seems to act as a container to hold message and other metadata, but only exposes one API to get the message. Topic is exposed by making it a public val. Would it make sense to either make it a case class or provide consistent APIs for all fields it holds ?\n\n7. Since now we seem to have more than one iterators for the consumer, would it make sense to rename ConsumerIterator to MessageIterator, and TopicalConsumerIterator to MessageAndMetadataIterator ?\n\n8. rat fails on this patch. There are some files without the Apache header\n\n", "Thanks for the reviews. Further comments inline.\n\nJun's comments:\n\n> 1. For future extension, I am thinking that we should probably unifying\n> KafkaMessageStream and KafkaMessageAndTopicStream to sth like\n> KafkaMessageMetadataStream. The stream gives a iterator of Message and its\n> associated meta data. For now, the meta data can be just topic. In the\n> future, it may include things like partition id and offset. \n\nThat's a good suggestion. I'm not sure if it is better to factor that change\nfor the existing createMessageStreams into 0.8 instead of trunk, because it\nis a fundamental API change that would break existing clients (at compile\ntime).  I can propose this to the mailing list to see if anyone has a\npreference. If no one objects, then we can remove it.\n\n> 2. ZookeeperConsumerConnector: 2.1 updateFetcher: no need to pass in\n> messagStreams \n\nWill do\n\n> 2.2 ZKRebalancerListener: It seems that kafkaMessageStream can be\n> immutable. \n\nIt is mutable because it is updated in consumeWildcardTopics.\n\n> 2.3 createMessageStreamByFilter: topicsStreamsMap is empty when passed to\n> ZKRebalanceListener. This means that the queue is not cleared during\n> rebalance. \n\nRelated to previous comment. The topicsStreamsMap is bootstrapped in\nconsumeWildCardTopics and updated at every topic event if there are new\nallowed topics. So it will be populated before any rebalance occurs.\n\n> 2.4 consumeWildCardTopics: I find it hard to read the code in this method.\n> Is there a real benefit to use implicit conversion here, instead of\n> explicit conversion? It's not clear to me where the conversion is used.\n> The 2-level tuple makes it hard to figure out what the referred fields\n> represent. Is the code relying on groupedTopicThreadIds being sorted by\n> (topic, threadid)? If so, where is that enforced. \n\nThe map flatten method is a bit confusing. I'm using (and hopefully not\nmisusing) this variant:\n\ndef flatten [B] (implicit asTraversable: ((A, B)) â TraversableOnce[B]): Traversable[B]\n\nConverts this map of traversable collections into a map in which all element\ncollections are concatenated.\n\nIt basically allows you to take the KV pairs of a map and generate some\ntraversable collection out of it. Here is how I'm using it: We have a list\nof queues (e.g., List(queue1, queue2)) and a map of\nconsumerThreadIdsPerTopic (e.g.,\n\n  { \"topic1\" -> Set(\"topic1-1\", \"topic1-2\"),\n    \"topic2\" -> Set(\"topic2-1\", \"topic2-2\"),\n    \"topic3\" -> Set(\"topic3-1\", topic3-2\") } ).\n\nFrom the above I need to create pairs of topic/thread -> queue, like this:\n\n  { (\"topic1\", \"topic1-1\") -> queue1,\n    (\"topic1\", \"topic1-2\") -> queue2,\n    (\"topic2\", \"topic2-1\") -> queue1,\n    (\"topic2\", \"topic2-2\") -> queue2,\n    (\"topic3\", \"topic3-1\") -> queue1,\n    (\"topic3\", \"topic3-2\") -> queue2 }\n\nThis is a bit tricky and I had trouble finding a clearer way to write it.\nHowever, I agree that this snippet is hard to read - even I'm having\ndifficulty reading it now, but I think keeping it concise as is and adding\ncomments such as the above example to explain what is going on should help.\n\n> 3. KafkaServerStartable: Should we remove the embedded consumer now? \n\nMy original thought was that it would be good to keep it around for\nfall-back, but I guess it can be removed.\n\n> 4. Utils, UtilsTest: unused import \n\nWill do.\n\n--------------------------------------------------------------------------------\n\nNeha's comments:\n\n> 1. It seems awkward that there is a MessageStream trait and the only API\n> it exposes is clear(). Any reason it doesn't expose the iterator() API ?\n> From a user's perspective, one might think, since it is a stream, it would\n> expose stream specific APIs too. It will be good to add docs to that API\n> to explain exactly what it is meant for.\n\nThe only reason it was added was because I have two message stream types\nnow. Anyway, this will go away if we switch to the common\nKafkaMessageMetadataStream.\n\n> 3. There is some critical code that is duplicated in the\n> ZookeeperConsumerConnector. consume() and consumeWildcardTopics() have\n> some code in common. It would be great if this can be refactored to share\n> the logic of registering session expiration listeners, registering watches\n> on consumer group changes and topic partition changes.\n\nWill do\n\n> 4. Could you merge all the logic that wraps the wildcard handling in one\n> API ? Right now, it is distributed between createMessageStreamsByFilter\n> and consumeWildcardTopics. It will be great if there is one API that will\n> pre process the wild cards, create the relevant queues and then call a\n> common consume() that has the logic described in item 5 above.\n\nSlightly involved, but it is worth doing.\n\n> 5. There are several new class variables called wildcard* in\n> ZookeeperConsumerConnector. I'm thinking they can just be variables local\n> to createMessageStreamsByFilter ?\n\nRelated to above. consumeWildcardTopics actually needs to access these so\nthat's why it's global - in this case global makes sense in that you really\nwouldn't need to (and currently cannot) make multiple calls to\ncreateMessageStreamsByFilter.  However, it would be good to localize them if\npossible to make the code easier to read.\n\n> 6. There is a MessageAndTopic class, that seems to act as a container to\n> hold message and other metadata, but only exposes one API to get the\n> message. Topic is exposed by making it a public val. Would it make sense\n> to either make it a case class or provide consistent APIs for all fields\n> it holds ?\n\nOk, but this will likely go away due to the MessageMetadata discussion.\n\n> 7. Since now we seem to have more than one iterators for the consumer,\n> would it make sense to rename ConsumerIterator to MessageIterator, and\n> TopicalConsumerIterator to MessageAndMetadataIterator ?\n\nMakes sense, but it could break existing users of KafkaMessageStream.  Also,\nif we can get rid of KafkaMessageStream and just go with\nKafkaMessageAndMetadataStream we will have only one iterator type.\n\n> 8. rat fails on this patch. There are some files without the Apache header\n\nGood catch and reminder that reviews should ideally include running rat. I\ndo need to add the header for some files.\n\n", "> 5. There are several new class variables called wildcard* in\n> ZookeeperConsumerConnector. I'm thinking they can just be variables local\n> to createMessageStreamsByFilter ?\n\n>> Related to above. consumeWildcardTopics actually needs to access these so\n>> that's why it's global - in this case global makes sense in that you really\n>> wouldn't need to (and currently cannot) make multiple calls to\n>> createMessageStreamsByFilter. However, it would be good to localize them if\n>> possible to make the code easier to read. \n\nAlso, is there a reason for making ZookeeperConsumerConnector implement the TopicEventHandler trait ? It can simply be a separate class that implements the handleTopicEvent right ? If you do that, you can pass in wildcard related variables in the constructor of this topic handler. I guess only the wildcardTopicWatcher needs to be a class variable, since it is required during shutdown. \n", "This patch addresses the points raised in the review for v2.\n\nI'm terrible at naming methods, so let me know if you have a better name for reinitializeConsumer.\n\nAlso, Jay had brought up some good points wrt the API (https://cwiki.apache.org/confluence/display/KAFKA/Consumer+API+changes). I think it would be good to get this reviewed for any other issues even as that conversation goes on.\n", "v2 looks much better now. Some additional comments:\n\n21. Should isTopicAllowed be part of TopicFilterSpec, especially if we want to extend it in the future? If so, we don't need TopicFilter.\n22. In this patch, I suggest that we only put message and topic in MessageAndMetadata. We can have a separate jira on how to expose offsets to the consumer. There, we need to discuss how a consume can rewind the consumption using the offset returned.\n23. It's probably better to rename KafkaMessageAndMetadataStream to KafkaStream.\n24. ZookeeperConsumerConnector:\n24.1 reinitializeConsumer:  I think it will make the code easier to understand if we explicitly define the type of val consumerThreadIdsPerTopic, topicThreadIds and threadQueueStreamPairs. It would be also very useful to explicitly define the return type of consumerThreadIdsPerTopic.flatten.\n24.2 reinitializeConsumer: This method is called every time a new topic is discovered. It feels strange that we have to register the consumer here. Ideally, each consumer is registered exactly once. Also, it seems that each time this method is called, we only add new entries to loadBalancerListener.kafkaMessageAndMetadataStreams. Shouldn't we clear this map first so that deleted topics can be removed?\n\n25. ByteBufferMessageSet: It's not clear to me if the iterator of ByteBufferMessageSet should return MessageAndMetadata. This is because ByteBufferMessageSet itself doesn't know all the metadata, such as topic and partition. So, it seems the iterator of this class should probably remain MessageAndOffset. MessageAndMetadata is only used for the client api.\n\n26. MirrorMaker: The shutdown hook should close producer.\n", "Thanks for the comments. If it helps quicker review, I can upload an incremental diff (over the previous one). Changes in this patch:\n\n- Removed TopicFilterSpec - I'll update the consumer API proposal wiki with this info.\n- Restored MessageAndOffset  - agreed that it is a bit weird for MessageSet to use MessageAndMetadata, which is why I put \"invalid\" defaults for the metadata fields if unused.\n- Added explicit types for the vals in reinitializeConsumer along with comments showing examples (would have been good if Scala supported named components when specifying maps/tuples, etc.).\n- Added TODO for 0.8 to deal with deleted topics. We probably need to do more in addition to removing the topic from the streams map.\n- Fixed mirrormaker with the producer.close - thanks for catching this.\n", "Incremental patch v3 -> v4.", "v4 patch looks good to me. If there is no objection, I will commit the code tomorrow.", "Actually, just committed kafka-315. Could you rebase?", "Done. I also enabled shallow iteration in the system test.\n", "Incremental patch over v4 in case it helps.", "Joel, thanks for the patch. Just committed to trunk."], "derived": {"summary": "I would like to discuss on this jira, the feasibility/benefits of separating\nout Kafka's mirroring feature from the broker into a stand-alone app, as it\ncurrently has a couple of limitations and issues. For example, we recently had to deal with Kafka mirrors that were in fact\nidle due to the fact that mirror threads were not created at start-up due to\na rebalancing exception, but the Kafka broker itself did not shutdown.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Separate out Kafka mirroring into a stand-alone app - I would like to discuss on this jira, the feasibility/benefits of separating\nout Kafka's mirroring feature from the broker into a stand-alone app, as it\ncurrently has a couple of limitations and issues. For example, we recently had to deal with Kafka mirrors that were in fact\nidle due to the fact that mirror threads were not created at start-up due to\na rebalancing exception, but the Kafka broker itself did not shutdown."}, {"q": "What updates or decisions were made in the discussion?", "a": "Joel, thanks for the patch. Just committed to trunk."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-250", "title": "Provide consumer iterator callback handler", "status": "Resolved", "priority": "Major", "reporter": "Joel Jacob Koshy", "assignee": "Joel Jacob Koshy", "labels": [], "created": "2012-01-20T22:39:44.000+0000", "updated": "2017-10-29T09:17:01.000+0000", "description": "It would be useful to add a \"callback\" option in the consumer, similar to\nthe producer callbacks that we have now. The callback point would be at the\npoint messages are iterated over, and would be helpful for inserting\ninstrumentation code.\n", "comments": ["Closing inactive issue. The old consumer is no longer supported"], "derived": {"summary": "It would be useful to add a \"callback\" option in the consumer, similar to\nthe producer callbacks that we have now. The callback point would be at the\npoint messages are iterated over, and would be helpful for inserting\ninstrumentation code.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Provide consumer iterator callback handler - It would be useful to add a \"callback\" option in the consumer, similar to\nthe producer callbacks that we have now. The callback point would be at the\npoint messages are iterated over, and would be helpful for inserting\ninstrumentation code."}, {"q": "What updates or decisions were made in the discussion?", "a": "Closing inactive issue. The old consumer is no longer supported"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-251", "title": "The ConsumerStats MBean's PartOwnerStats  attribute is a string", "status": "Resolved", "priority": "Major", "reporter": "Pierre-Yves Ritschard", "assignee": null, "labels": [], "created": "2012-01-24T17:09:00.000+0000", "updated": "2017-09-08T15:43:46.000+0000", "description": "The fact that the PartOwnerStats is a string prevents monitoring systems from graphing consumer lag. There should be one mbean per [ topic, partition, groupid ] group.", "comments": ["Thanks for the patch. Some comments:\n\n1. There is compilation error.\n2. Use space instead of TAB.\n3. The new jmx seems to be intended for replacing ZookeeperConsumerConnectorMBean. If so, we should remove ZookeeperConsumerConnectMBean and the implementation in ZookeeperConsumerConnector.\n4. Not every SyncRebalance triggers a successful rebalance (sometimes rebalance is not necessary). So, instead of unregistering/registering the MBean in SyncRebalance, we should do unregisterMBeans followed by registerMBeans at the end of updateFetcher, which is when a rebalance really happens.\n\n", "1 & 2: i must have sent a wrong patch, sorry about that, will reissue\n3: i thought some people may be relying on the actual output, but OK\n4: unregistermbeans needs to be called before topicRegistry is ever touched, or old beans will still exist, I will look at a better place to wrap this\n\nThanks for the quick review, I'll update shortly", "I incorporated your comments safe for no 4 since i don't think it's too bad to sometimes unregister mbeans and reregister them, worse than can happen is a null value look up which graphing systems handle gracefully.", "Sorry, just get the chance to look at this. Couldn't apply the patch since trunk has moved. Could you rebase?", "Change to \"In Progress\" since the patch doesn't apply cleanly.", "[~ijuma] Hi, if it is still relevant and no one is working on it could you assign it to me, please?", "[~eribeiro], I don't know if it's still relevant as it's a pretty told ticket. [~junrao] should know. By the way, you should be able to assign tickets to yourself.", "I cannot. Afaik, it depends on the project, and some (many?) are restricting the assign operation to committers and core members, what I can understand perfectly and comply with.", "[~eribeiro], I am not a committer and I can. :) Ask in the mailing list to be added as a contributor for JIRA and Confluence (you can see many such messages in the archive).", "Since we are developing the new java consumer, not sure if it's worth patching the mbean in the old consumer.", "Closing inactive issue as per above comments."], "derived": {"summary": "The fact that the PartOwnerStats is a string prevents monitoring systems from graphing consumer lag. There should be one mbean per [ topic, partition, groupid ] group.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "The ConsumerStats MBean's PartOwnerStats  attribute is a string - The fact that the PartOwnerStats is a string prevents monitoring systems from graphing consumer lag. There should be one mbean per [ topic, partition, groupid ] group."}, {"q": "What updates or decisions were made in the discussion?", "a": "Closing inactive issue as per above comments."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-252", "title": "Generalize getOffsetsBefore API to a new more general API getLeaderMetadata", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": null, "labels": ["project"], "created": "2012-01-24T19:31:16.000+0000", "updated": "2017-09-07T18:13:07.000+0000", "description": "The relevant discussion is here - https://issues.apache.org/jira/browse/KAFKA-238?focusedCommentId=13191350&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13191350 and on KAFKA-642\n\nWe have an api that gets cluster-wide metadata (getTopicmetdata) which we use for bootstraping knowledge about the cluster. But some things can only be fetched from the leader for the partition.\n\nWith replication, the metadata about log segments can only be returned by a broker that hosts that partition locally. It will be good to expose log segment metadata through a more general replica metadata API that in addition to returning offsets, also returns other metadata like - number of log segments, total size, last modified timestamp, highwater mark, and log end offset.\n\nIt would be good to do a wiki design on this and get consensus on that first since this would be a public api.", "comments": ["Since we are breaking wire protocol and APIs in 0.8, it is important to think about this as part of 0.8", "These API were handled in newer versions."], "derived": {"summary": "The relevant discussion is here - https://issues. apache.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Generalize getOffsetsBefore API to a new more general API getLeaderMetadata - The relevant discussion is here - https://issues. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "These API were handled in newer versions."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-253", "title": "Refactor the async producer to have only one queue instead of one queue per broker in a Kafka cluster", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Jun Rao", "labels": [], "created": "2012-01-24T19:39:34.000+0000", "updated": "2013-03-28T15:40:16.000+0000", "description": "Today, the async producer is associated with a particular broker instance, just like the SyncProducer. The Producer maintains a producer pool of sync/async producers, one per broker. Since the producer pool creates one async producer per broker, we have multiple producer queues for one Producer instance. \n\nWith replication, a topic partition will be logical. This requires refactoring the AsyncProducer to be broker agnostic. As a side effect of this refactoring, we should also ensure that we have only one queue per Producer instance.", "comments": ["Here are some details on a potential refactoring solution -\n\n1. Get rid of the AsyncProducer\n2. In the send(ProducerData) API of the Producer, simply add new ProducerData to an internal queue  \n3. In the DefaultEventHandler,\n\n3.1 it should have access to the Partitioner, custom or default\n3.2 Add a partition API that will tag each ProducerData object with a broker id-partition id (kafka.cluster.Partition)\n3.3 the collate API will group events per (topic, Partition) pair\n3.4 the serialize API will convert the data to a ByteBufferMessageSet => (topic, Partition, BBME) \n3.5 The send() API will use a SyncProducer to send each (topic, Partition, BBME) triple to the Partition.broker_id.\n\nThe above will ensure that there is a single queue in the Producer and that the Producer is broker agnostic. This makes it much easier to resolve KAFKA-239, since that needs to make partitions logical", "Attach a patch.\n\nThe current Producer logic has a number of problems:\n1. There are 2 pluggable handlers EventHandler and CallbackHandler. This makes the logic a bit complicated. Furthermore, we need to maintain a scala and a java version and convert btw the two.\n2. The logics for doing collating, serialization, compression, etc are in the pluggable EventHandler, which means if you plug in your own handler, a lot of the code in the DefaultEventHandler has to be duplicated.\n3. The partition assignment is done at enqueue time. It's better to do this at the dequeue time to minimize the impact of failed brokers.\n4. The event handler is only applicable to async producer. If we want to add auditing in the handler, it is useful for both sync and async producer.\n\nThe new design made the following changes:\na. Get rid of both pluggable handlers.\nb. DefaultEventHandler now does partitioning, serialization, collating, compression, etc and is used by both sync and async producer.\nc. The data flow now is simpler:\n  c1. async: Producer => queue => ProducerSendThread => DefaultEventHandler\n  c2. sync: Producer => DefaultEventHandler\nd. Keep partitioner and encoder pluggable.\n\nSome detailed changes:\ne. BrokerPartition logic is moved to DefaultEventHandler\nf. ProducerPool is simplified to be just a cache for sync producers.\ng. Remove most tests for javaapi.producer and only keep one test in ProducerTest to verify the wrapper code.\nh. Move tests in ProducerTest that don't need the full stack to AsyncProducerTest and remove some tests that don't seem to improve code coverage.\n\nAs for application specific logic, one option is to do that outside of Producer. We can track that as part of kafka-260.", "The patch can go into 0.7 too.", "That is a pretty useful refactoring patch ! I have a few questions -\n\n1. ProducerMethodsTest is empty\n\n2. ProducerData can be a case class instead. You will get toString for free, in addition to equals() which is useful for unit testing. Al though, I'm not sure if there are any obvious downsides of using case classes.\n\n3. Producer: \n\n3.1 How about throwing an InvalidConfigException when both zk.connect and broker.list are specified ? It is probably a misconfiguration that should be corrected. We have gotten bitten by this several times at LinkedIn. The user could easily miss the warning we issue saying that we will just use zk.connect.\n\n3.2 AsyncProducerStats: Do we want to record an event as sent even before it has successfully entered the producer queue ? I'm thinking that one stat counts the number of events successfully entering the producer queue and another stat counts the number of events that got dropped due to a full queue. But maybe, the user is interested in per topic level event counts and those should be available for the sync producer as well. So maybe we can get rid of numEvents from AsyncProducerStats for now ?\n\n4. DefaultEventHandler\nIn the producer send retry logic inside handle() API, what happens if a broker fails after partitionAndCollate() and right before send() ? Simply retrying send to the same broker is not useful, we should probably repartition and retry the send, which will at least attempt to select another live broker.\n\n5. ProducerTest\n\n5.1 testBrokerListAndAsync: It is better to write as many true unit tests as possible. I don't see an advantage in actually instantiating a zookeeper consumer when EasyMock can be used to verify the producer functionality. For example, it seems that here, you want to test the async producer with the broker.list config. Simply verifying the send() API of the SyncProducer should suffice. Unit tests can assume that data doesn't get corrupted over the socket.\n\n5.2 testJavaProducer: Same as above. Use mocks wherever possible.\ntestPartitionedSendToNewBrokerInExistingTopic, testPartitionedSendToNewBrokerInExistingTopic: Any reason for deleting these tests ? They test valid code paths for the ZK producer and we added these to make sure new topics and new brokers in existing topics are handled correctly with the ZK producer.\n\n5.3 Rest of the tests: It will be a good idea to even cleanup this test suite to avoid bringing up a server and 2 SimpleConsumers. We don't know of a good way to mock out the zk part of it, so we can choose to just bring up a local zk instance, and to indicate existence of a new broker, we can just call registerBrokerInZK instead of actually bringing up a broker. \n\nThough, if you want, this can go in a separate jira.\n", "Uploaded patch v2.\n1. removed\n2. made it case class\n3.1 changed.\n3.2 removed #events stat at AsyncProducerStats; added #events per topic at Producer level\n4. improved by doing serialization before partitioning and collating\n5.1 made the test mock and move to AsyncProducerTest\n5.2 made testJavaProducer mock and move to AsyncProducerTest; I don't feel testPartitionedSendToNewBrokerInExistingTopic adds more coverage since there are separate tests to test partitioner and NewBrokerInExistingTopic. Since partitioning and broker discovery are independent, existing test cases should be enough. Ditto for testPartitionedSendToNewTopic.\n5.3 will open another jira to track this.\n", "Attached v3 patch by rebasing to trunk.", "Attach patch v4. Made the following changes:\n1. Renamed AsyncProducerConfigShared to AsyncProducerConfig.\n2. Added producer retry backoff time.\n3. Only retry on the top level of send, i.e., no recursive retries (this ran the risk that send may never finish).", "Attach patch v5. Minor changes in DefaultEventHandler to correctly handle retries recursively.", "ProducerMethodsTest needs to be removed. Other than that, v5 looks good.", "Jun, could you please merge v5 to the 0.8 branch, so that work on KAFKA-239 is unblocked ?", "Thanks for the review. Committed to 0.8 branch. Will close the jira once it's committed to 0.7 branch.", "Was this change merged to the 0.7 branch?", "No. Since most of the development happens on 0.8, we are only fixing blocker issues in 0.7."], "derived": {"summary": "Today, the async producer is associated with a particular broker instance, just like the SyncProducer. The Producer maintains a producer pool of sync/async producers, one per broker.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Refactor the async producer to have only one queue instead of one queue per broker in a Kafka cluster - Today, the async producer is associated with a particular broker instance, just like the SyncProducer. The Producer maintains a producer pool of sync/async producers, one per broker."}, {"q": "What updates or decisions were made in the discussion?", "a": "No. Since most of the development happens on 0.8, we are only fixing blocker issues in 0.7."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-254", "title": "A tool to GET Zookeeper partition-offset and output to files", "status": "Resolved", "priority": "Major", "reporter": "John Fung", "assignee": "John Fung", "labels": [], "created": "2012-01-24T23:21:18.000+0000", "updated": "2012-02-21T00:28:19.000+0000", "description": "A utility that retrieves the offsets of all topic partitions in ZK for a specified group id and save the data to output files. A shell script also comes with this tool to automate the writing of data to files in a specified time interval.\n\nThis utility expects 3 arguments:\n 1. Zk host:port string\n 2. group Id\n 3. Output file pathname", "comments": ["Thanks for the patch. A few comments:\n1. For consistency, let's use joptsimple for command line processing (take a look at some other tools as examples).\n2. We should use ZkClient, instead of the raw ZK client (see ConsumerOffsetChecker)\n3. Could we extend this to support multiple consumer groups? We can use an option --groups to specify all groups. If the option is not specified, we can get all groups registered in ZK. We will need to include group name in the output file.\n4. Use the zk common paths already defined in ZkUtils.\n\nThose comments are applicable to kafka-255 too.", "Hi Jun,\n\nThanks for your review. Attached kafka-254-v2.patch with the changes suggested in the review.", "Thanks John. Committed the patch with the following minor changes:\n1. Allow multiple groups specified from command line.\n2. Rename the tool to ExportZkOffsets.\n3. Remove unused imports."], "derived": {"summary": "A utility that retrieves the offsets of all topic partitions in ZK for a specified group id and save the data to output files. A shell script also comes with this tool to automate the writing of data to files in a specified time interval.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "A tool to GET Zookeeper partition-offset and output to files - A utility that retrieves the offsets of all topic partitions in ZK for a specified group id and save the data to output files. A shell script also comes with this tool to automate the writing of data to files in a specified time interval."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks John. Committed the patch with the following minor changes:\n1. Allow multiple groups specified from command line.\n2. Rename the tool to ExportZkOffsets.\n3. Remove unused imports."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-255", "title": "A tool to UPDATE Zookeeper partition-offset with input from a file", "status": "Resolved", "priority": "Major", "reporter": "John Fung", "assignee": "John Fung", "labels": [], "created": "2012-01-24T23:21:32.000+0000", "updated": "2012-02-21T00:30:03.000+0000", "description": "A utility that updates the offset of broker partitions in ZK.\n\nThis utility expects 2 input files as arguments:\n 1. consumer properties file\n 2. a file contains partition offsets data such as:\n     (This output data file can be obtained by running kafka.tools.GetZkOffsets)\n\n     test01/3-0:285038193\n     test01/1-0:286894308\n", "comments": ["Attached kafka-255-v2.patch which has changes suggested in the review in kafka-254.", "Thanks John. Committed the patch with the following minor changes:\n1. Rename the tool to ImportZkOffsets.\n2. Remove the group option since we can always change the groups to be imported by manipulating the input file.\n3. Remove unused imports."], "derived": {"summary": "A utility that updates the offset of broker partitions in ZK. This utility expects 2 input files as arguments:\n 1.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "A tool to UPDATE Zookeeper partition-offset with input from a file - A utility that updates the offset of broker partitions in ZK. This utility expects 2 input files as arguments:\n 1."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks John. Committed the patch with the following minor changes:\n1. Rename the tool to ImportZkOffsets.\n2. Remove the group option since we can always change the groups to be imported by manipulating the input file.\n3. Remove unused imports."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-256", "title": "Bug in the consumer rebalancing logic leads to the consumer not pulling data from some partitions", "status": "Resolved", "priority": "Critical", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "labels": [], "created": "2012-01-25T19:32:54.000+0000", "updated": "2012-02-03T01:13:36.000+0000", "description": "There is a bug in the consumer rebalancing logic that makes a consumer not pull data from some partitions for a topic. It recovers only after the consumer group is restarted and doesn't hit this bug again.\n\nHere is the observed behavior of the consumer when it hits the bug -\n\n1. Consumer is consuming 2 topics with 1 partition each on 2 brokers\n2. Broker 2 is bounced\n3. Rebalancing operation triggers for topic_2, where the consumer decides to now consume data only from Broker 1 for topic_2\n4. During the rebalancing operation, ZK has not yet deleted the /brokers/topics/topic_1/broker_2, so the consumer still decides to consumer from both brokers for topic_1\n5. While restarting the fetchers, it tries to restart fetcher for broker 2 and throws a RuntimeException. Before this, it has successfully started fetcher for broker 1 and is consuming data from broker_1\n6. This exception trickles all the way upto syncedRebalance API and the oldPartitionsPerTopicMap does not get updated to reflect that for topic_2, the consumer has now seen only broker_1. It still points to topic_2 -> broker_1, broker_2\n7. Next rebalancing attempt gets triggered\n8. By now, broker 2 is restarted and registered in zookeeper\n9. For topic_2, the consumer tries to see if rebalancing needs to be done. Since it doesn't see a change in the cached topic partition map, it decides there is no need to rebalance.\n10. It continues fetching only from broker_1\n", "comments": ["Changes include -\n\n1. The bug was caused due to a stale cache problem\n\n2. This patch fixes the bug by clearing the cache on every single unsuccessful rebalancing attempt. This includes exception during rebalancing OR failure to own one or more partitions \n\n3. A tool to verify if a consumer group successfully completed a rebalancing operation. \n\n4. To facilitate such a tool, partition ownership is split into 3 steps -\n\n4.1 the consumer records its decision to own partitions that it has selected\n4.2 the fetchers are started to start pulling data from the selected partitions\n4.3 the above partition ownership decision is written to zookeeper\n\n5. Cleanup to remove unrequired imports", "This patch applies cleanly to trunk", "A slight change here -\n\nThe fetchers are updated only after the partition ownership is reflected in zookeeper. This will reduce the possibility of duplicate data", "+1 for v3. \n\nI like the idea of having a tool to check if a consumer is correctly balanced.\nA more general comment/question on the kafka.tools package: I thought the tools\npackage is meant for stand-alone tools that people can run on the command-line,\nwhose output can be piped for further processing if desired.  If so, it would\nbe better not to use logging for the tool's output and simply println. \n", "You have a good point about the tools package. This tool is meant for use in running system tests (KAFKA-227) in verifying the correctness of Kafka. This means at the very least the output about whether the rebalancing attempt was successful or not, can be println and maybe the other info useful for debugging can be log4j ? If that makes sense, I'll make that change before committing this patch.\n\nThanks for reviewing this patch and catching the possible duplication issue in v2. ", "Committed this patch to trunk. Will fix KAFKA-262 separately.", "I found a bug in the v3 patch. The reflectPartitionOwnershipDecision API has a bug that doesn't set the value of partitionOwnershipSuccessful correctly. Will fix this as part of KAFKA-262.", "Some comments:\n1. ZookeeperConsumerConnector.reflectPartitionOwnershipDecision, the following code seems incorrect.\n      val success = partitionOwnershipSuccessful.foldLeft(0)((sum, decision) => if(decision) 0 else 1)\n  The function in foldLeft should check both sum and decision. Also, the local variable success should be named to something like hasFailure.\n2. ZookeeperConsumerConnector.syncedRebalance\n  done = false in the catch clause is not necessary. If we hit an exception, done will be left with the initial value, which is false.\n  The else after the following\n         if (done) {\n            return\n          }\n  is not necessary.\n  Also, it seems there is no need to call commitOffset before closeFetchersFprQuues since the latter commits offsets already.\n3. It seems that we don't need to check ownership registry in ZK in processPartition. The same check will be done later in reflectPartitionOwnershipDecision.\n", "1. That is the bug I was referring to in my previous comment. \n\n2. done =false in the catch clause is to prevent a bug, in case the code elsewhere in the rebalance API changes in the future. These bugs are very hard to spot and time consuming to debug. This one-liner seems harmless since it could potentially save a lot of time.\nThough, commitOffsets() can be skipped before closing the fetchers. \n\n3. This is also a good point, and seems like an over optimization. Will get rid of it as part of KAFKA-262."], "derived": {"summary": "There is a bug in the consumer rebalancing logic that makes a consumer not pull data from some partitions for a topic. It recovers only after the consumer group is restarted and doesn't hit this bug again.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Bug in the consumer rebalancing logic leads to the consumer not pulling data from some partitions - There is a bug in the consumer rebalancing logic that makes a consumer not pull data from some partitions for a topic. It recovers only after the consumer group is restarted and doesn't hit this bug again."}, {"q": "What updates or decisions were made in the discussion?", "a": "1. That is the bug I was referring to in my previous comment. \n\n2. done =false in the catch clause is to prevent a bug, in case the code elsewhere in the rebalance API changes in the future. These bugs are very hard to spot and time consuming to debug. This one-liner seems harmless since it could potentially save a lot of time.\nThough, commitOffsets() can be skipped before closing the fetchers. \n\n3. This is also a good point, and seems like an over optimization. Will get rid of it as part of KAFKA-262."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-257", "title": "Hadoop producer should use software load balancer", "status": "Resolved", "priority": "Major", "reporter": "Sam Shah", "assignee": "Sam Shah", "labels": [], "created": "2012-01-26T05:55:38.000+0000", "updated": "2012-02-06T23:12:52.000+0000", "description": "Improvements to Kafka's Hadoop producer:\n\n* Uses new Producer API (as opposed to SyncProducer)\n* Supports software load balancer (Kafka URI for this is kafka+zk://<zk-path>#<topic-name>)\n* Can set compression codec (default is 0 or no compression)", "comments": ["This has been tested and running in production at LinkedIn for one month.", "Thanks for the patch Sam ! Overall, it looks great. I have a few comments/questions -\n\n1. In the example README, it will be good to change \" REGISTER zookeeper-3.3.3.jar;\" to \" REGISTER zookeeper-3.3.4.jar;\". The reason being that there are known critical bugs with 3.3.3 and it will save users who might copy paste the example script as is :-)\n\n2. As a default option, is there any particular reason for using the sync producer ? Using the async producer option provides significant improvement in the network bandwidth utilization. \n\nI have a few more questions/comments about the Hadoop-Kafka bridge, feel free to file another JIRA if you'd rather have it fixed later.\n\n1. It seems like when the broker.list option is selected, only one broker can be specified. This is true if that broker is pointing to a VIP/hardware load balancer, but if not, then the broker.list is a csv of broker_id:broker_host:broker_port. It will be good to support that here.\n\n2. If the kafka.output.producer.type=async, there are a few config options that should be supported. They are listed in AsyncProducerConfigShared\n\n3. Does it make sense to also let the user specify a custom Partitioner as part of the partitioner.class config ? If one is not specified, it defaults to kafka.producer.DefaultPartitioner. \n", "Thanks Neha. Answers:\n\n1. Done. See updated patch.\n2. The KafkaRecordWriter queues up messages up to a configurable amount (default 10MB) so it is amortizing network bandwidth. It doesn't make sense to start up an async background thread, as the task will have to block anyway to push data (it's also not a good idea from a node utilization perspective.)\n\n1. Yup, that's a good point, the old hadoop producer only supported one broker in its URI and I didn't fix it. The updated patch allows multiple brokers separated by commas.\n2. Async shouldn't be used (see point #2 above), which is why I didn't add support for the other options.\n3. I haven't come across a use case yet for a custom partitioner. I can add support later; it should be easy.", "Thanks for the updated patch Sam.\n\n1. The broker.list parameter takes in a csv list of \"broker_id:broker_host:broker_port\". The broker id of each server in this connection URL must be unique. I think the patch allows only one broker id -\n\n+      // e.g. kafka://kafka-server:9000,kafka-server2:9000/foobar\n+\n+      final int brokerId = job.getInt(\"kafka.output.broker_id\", KAFKA_DEFAULT_BROKER_ID);\n+      StringBuilder brokerListBuilder = new StringBuilder();\n+      String delim = \"\";\n+      for (String serverPort : uri.getAuthority().split(\",\")) {\n+        brokerListBuilder.append(delim).append(String.format(\"%d:%s\", brokerId, serverPort));\n+        delim = \",\";\n+      }\n\nThis requirement seems like an overkill to me. I've filed KAFKA-258 to address this improvement in the Producer. Until that is fixed, the Producer would require the connection url to specify a unique broker id.\n\n2. This makes sense. I missed looking at KafkaRecordWriter. ", "Thanks Neha. I've updated the patch so that it just enumerates broker id's (until KAFKA-258 is resolved).", "Thanks for the patch Sam ! Committed it."], "derived": {"summary": "Improvements to Kafka's Hadoop producer:\n\n* Uses new Producer API (as opposed to SyncProducer)\n* Supports software load balancer (Kafka URI for this is kafka+zk://<zk-path>#<topic-name>)\n* Can set compression codec (default is 0 or no compression).", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Hadoop producer should use software load balancer - Improvements to Kafka's Hadoop producer:\n\n* Uses new Producer API (as opposed to SyncProducer)\n* Supports software load balancer (Kafka URI for this is kafka+zk://<zk-path>#<topic-name>)\n* Can set compression codec (default is 0 or no compression)."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for the patch Sam ! Committed it."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-258", "title": "Remove broker.id from the broker.list config in the Producer", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Yang Ye", "labels": [], "created": "2012-01-30T00:12:31.000+0000", "updated": "2012-08-14T16:37:15.000+0000", "description": "The broker.list config option for the Producer requires the format to be a comma-separated list of \"broker_id:broker_host:broker:port\"\n\nFor ease of use, broker host and port should be sufficient to identify a broker. Making it necessary to use a broker id is unintuitive and can be error prone", "comments": ["fixed as part of kafka-369"], "derived": {"summary": "The broker. list config option for the Producer requires the format to be a comma-separated list of \"broker_id:broker_host:broker:port\"\n\nFor ease of use, broker host and port should be sufficient to identify a broker.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Remove broker.id from the broker.list config in the Producer - The broker. list config option for the Producer requires the format to be a comma-separated list of \"broker_id:broker_host:broker:port\"\n\nFor ease of use, broker host and port should be sufficient to identify a broker."}, {"q": "What updates or decisions were made in the discussion?", "a": "fixed as part of kafka-369"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-259", "title": "Give better error message when trying to run shell scripts without having built/downloaded the jars yet", "status": "Resolved", "priority": "Minor", "reporter": "Ross Crawford-d'Heureuse", "assignee": null, "labels": ["newbie"], "created": "2012-01-31T12:40:47.000+0000", "updated": "2013-05-30T03:27:37.000+0000", "description": "Hi there, I've cloned from the kafka github repo and tried to run the start server script:\n\n ./bin/kafka-server-start.sh config/server.properties \n\nWhich results in:\n\nException in thread \"main\" java.lang.NoClassDefFoundError: kafka/Kafka\nCaused by: java.lang.ClassNotFoundException: kafka.Kafka\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:202)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:190)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:306)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:247)\n\nIt seems that Im missing a build step? what have I forgotten to do?\n\nThanks in advance and I look forward to using kafka.\n\nregards\nrcdh", "comments": ["cd kafka\n./sbt update\n./sbt package\n\nthis will make the jars for you to be able to run the server as you are attempting to-do", "This is covered in the README and the releases come with packaged jars. The only thing I think we could do better is error out if there are no jars in dist, let's change this bug to be about doing that.", "as a noob to java/scala I can honestly say that it may be covered in the readme but there is allot of forest in there and not many trees to be seen ;) its just the basics (and probably only form my point of view) :) thanks for your time and effort guys and keep up the great work! ", "Yeah, I didn't mean that in a snotty way, just that if we version control the jars the java people get all sulky and complain that we aren't using maven to download them, but if we do that then the non-maven people are unhappy because nothing works.", "@jay, absolutely no problem mate, sorry if I sounded snooty ;) was not meant as such. As far as I can see maven +- sbt are really good tools. But for a rank noob with java its a case of figuring out the nomenclature and processes that is the java world lol ;) ", "Hi, I have a patch for this, I am going through legal to get it approved. Will upload it here asap.", "I have submitted a patch for this. Basically the patch checks whether the java process returns with an exit code of 1 (abnormal), and if it does checks the output of the java process to see whether there are NoClassDefFoundError or \"Could not find or load main class\" messages and then if it does, displays this message:\n\n\"Please build the project using sbt. Documentation is available at http://kafka.apache.org/\"\n\nPlease let me know if you have any concerns with this approach.", "Thanks for the patch. It doesn't apply to 0.8 though. Could you provide another patch?\n\ngit apply ~/Downloads/KAFKA-259-v1.patch \n/Users/jrao/Downloads/KAFKA-259-v1.patch:21: trailing whitespace.\nif [ $exitval -eq \"1\" ] ; then \n/Users/jrao/Downloads/KAFKA-259-v1.patch:27: trailing whitespace.\n\tif [[ -n \"$match\" ]]; then \nerror: patch failed: bin/kafka-run-class.sh:81\nerror: bin/kafka-run-class.sh: patch does not apply\nerror: patch failed: bin/kafka-run-class.sh:93\nerror: bin/kafka-run-class.sh: patch does not apply\n", "[~junrao] - I executed the simple contributor workflow in this page (https://cwiki.apache.org/confluence/display/KAFKA/Git+Workflow) again. Attached the patch. Can you please try again?", "Thanks for patch v2. +1. Committed to 0.8."], "derived": {"summary": "Hi there, I've cloned from the kafka github repo and tried to run the start server script:. /bin/kafka-server-start.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Give better error message when trying to run shell scripts without having built/downloaded the jars yet - Hi there, I've cloned from the kafka github repo and tried to run the start server script:. /bin/kafka-server-start."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for patch v2. +1. Committed to 0.8."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-260", "title": "Add audit trail to kafka", "status": "Resolved", "priority": "Major", "reporter": "Jay Kreps", "assignee": "Jay Kreps", "labels": [], "created": "2012-01-31T23:58:27.000+0000", "updated": "2015-02-08T00:10:57.000+0000", "description": "LinkedIn has a system that does monitoring on top of our data flow to ensure all data is delivered to all consumers of data. This works by having each logical \"tier\" through which data passes produce messages to a central \"audit-trail\" topic; these messages give a time period and the number of messages that passed through that tier in that time period. Example of tiers for data might be \"producer\", \"broker\", \"hadoop-etl\", etc. This makes it possible to compare the total events for a given time period to ensure that all events that are produced are consumed by all consumers.\n\nThis turns out to be extremely useful. We also have an application that \"balances the books\" and checks that all data is consumed in a timely fashion. This gives graphs for each topic and shows any data loss and the lag at which the data is consumed (if any).\n\nThis would be an optional feature that would allow you to to this kind of reconciliation automatically for all the topics kafka hosts against all the tiers of applications that interact with the data.\n\nSome details, the proposed format of the data is JSON using the following format for messages:\n\n{\n  \"time\":1301727060032,  // the timestamp at which this audit message is sent\n  \"topic\": \"my_topic_name\", // the topic this audit data is for\n  \"tier\":\"producer\", // a user-defined \"tier\" name\n  \"bucket_start\": 1301726400000, // the beginning of the time bucket this data applies to\n  \"bucket_end\": 1301727000000, // the end of the time bucket this data applies to\n  \"host\":\"my_host_name.datacenter.linkedin.com\", // the server that this was sent from\n  \"datacenter\":\"hlx32\", // the datacenter this occurred in\n  \"application\":\"newsfeed_service\", // a user-defined application name\n  \"guid\":\"51656274-a86a-4dff-b824-8e8e20a6348f\", // a unique identifier for this message\n  \"count\":43634\n}\n\nDISCUSSION\n\nTime is complex:\n1. The audit data must be based on a timestamp in the events not the time on machine processing the event. Using this timestamp means that all downstream consumers will report audit data on the right time bucket. This means that there must be a timestamp in the event, which we don't currently require. Arguably we should just add a timestamp to the events, but I think it is sufficient for now just to allow the user to provide a function to extract the time from their events.\n2. For counts to reconcile exactly we can only do analysis at a granularity based on the least common multiple of the bucket size used by all tiers. The simplest is just to configure them all to use the same bucket size. We currently use a bucket size of 10 mins, but anything from 1-60 mins is probably reasonable.\n\nFor analysis purposes one tier is designated as the source tier and we do reconciliation against this count (e.g. if another tier has less, that is treated as lost, if another tier has more that is duplication).\n\nNote that this system makes false positives possible since you can lose an audit message. It also makes false negatives possible since if you lose both normal messages and the associated audit messages it will appear that everything adds up. The later problem is astronomically unlikely to happen exactly, though.\n\nThis would integrate into the client (producer and consumer both) in the following way:\n1. The user provides a way to get timestamps from messages (required)\n2. The user configures the tier name, host name, datacenter name, and application name as part of the consumer and producer config. We can provide reasonable defaults if not supplied (e.g. if it is a Producer then set tier to \"producer\" and get the hostname from the OS).\n\nThe application that processes this data is currently a Java Jetty app and talks to mysql. It feeds off the audit topic in kafka and runs both automatic monitoring checks and graphical displays of data against this. The data layer is not terribly scalable but because the audit data is sent only periodically this is enough to allow us to audit thousands of servers on very modest hardware, and having sql access makes diving into the data to trace problems to particular hosts easier.\n\nLOGISTICS\nI would recommend the following steps:\n1. Add the audit application, the proposal would be to add a new top-level directory equivalent to core or perf called \"audit\" to house this application. At this point it would just be sitting there, not really being used.\n2. Integrate these capabilities into the producer as part of the refactoring we are doing now\n3. Integrate into consumer when possible\n\n", "comments": ["is \"application\" synonymous with the new wired format \"client_id\" is so we should standardize on one or the other. +1 on the feature.", "Good point. No, in the sense that I am basically just describing two independent trains of thought, but you are absolutely right that there is no reason to have an application_id and a client_id. So let's pretend that that was the plan all along :-)", "Attached screenshot of audit app. This shows the aggregate data over a given time range, and graphs any dependencies as well as estimating the lag for loading the data (off the page on the screenshot).", "Draft patch for adding audit trail app to kafka.", "Hi Jay,\n\nI tried to apply this patch in my dev envoirnment, \n\nTo which versiob should apply it?\n\nI tried it on  kafka-0.7.0-incubating.\n\nIf it should work on that version, I couldn't see it on action (It did compile), so maybe I don't get the configuration, can you elaborate on how to configure the producer tier to send AuditData?\n\nThanks.\n\n\n", "I have the audit ui up and running in my dev environment as soon as I get a chance to patch our producers I should be able to submit a couple of tweaks to this patch for 0.7.1 and 0.8. \n\nIf you have the producer code that generates the audit messages that would be pretty useful!", "We don't have a producer and consumer that emit the audit data that is open source, that logic currently resides in a linkedin-specific wrapper class. We would be interested in fully integrating this with the open source kafka.\n\nOne key question is how to get the timestamp used for auditing. Currently we rely on a special field in the message to get the timestamp. To kafka, of course, messages are just opaque byte[], so integrating this is a little challenging. For our usage we just made this a required field for our avro records. Three options for integration:\n1. Support auditing only for messages which contain timestamps. Make the user provide a function for extracting the timestamp from the message if they want to use the audit app.\n2. Add a special field to all messages that contains the timestamp that comes from message creation time. The downside of this is that it requires a change to the message format and this field might not be useful for everyone.\n3. Add a generic key-value style header, and store the timestamp in that. The downside of having a generic header is that you have to store the key too.\n\nI would probably vote for 2. I think timestamp and auditing is useful enough to argue for making it a top-level field.\n\nWe could also implement (1) as a first phase, and then later chose to add the timestamp to making auditing automatic. That might be a better approach to get stuff started easily.", "I like option 2. Auditing is useful to have out-of-the-box.", "Hi,\n\ni like option 2. I wrote a generic consumer that stores the messages of a topic in small batches in Amazon S3. Everything was really simple (and generic) until i added partitioning based on a timestamp. From my experience in most cases messages/events have a creation time. I would imagine, that there are other 'high level' clients, that could make use of a (producer provided) timestamp and could be implemented much simpler, if they would not have to deal with some sort of timestamp extraction api.\n\nlorenz ", "It would be possible to have optional timestamps by using the magic byte at the beginning of the Kafka Messages, no? If the Message contains the old (current) magic byte, then there's no timestamp, if it's the new magic byte, then there is a timestamp (without needing a key) somewhere in the header...", "Hi Jay, \n\nIs the final kafka-audit code available in kafka 0.8 or somewhere else? I don't see a top level directory with name \"audit\" in the kafka source.\n\nI see the audit mentioned in the documentation at http://kafka.apache.org/documentation.html, however I am not able to find more details about how to configure and use it.", "Hi all,\n\nAshok, do you have any news about this kafka-audit topic? Did you resolve your doubts?\n\nI am looking for this kind of feature.\n\nRegards", "\"It also makes false negatives possible since if you lose both normal messages and the associated audit messages it will appear that everything adds up. The later problem is astronomically unlikely to happen exactly, though.\"\n\nThis may be true once messages have reached a broker.  However, if a producer process were to be killed (say by SIGKILL), both it's unack'ed normal messages and the audit data would be lost.  Would it make sense to persist the audit counts to the file system for producers so that they could potentially be recovered?", "Hi guys,\n\nWe've adopted the data model above in Aletheia (https://github.com/outbrain/Aletheia), an open source data delivery framework we've been working on here at Outbrain. \nIn Aletheia we call these audit trails \"Breadcrumbs\", and have them generated by the producer and consumer sides. We're working towards integrating the above mentioned patch in order to provide a client side dashboard.\n\nAletheia is by no means meant to replace Kafka, it is rather an abstraction layer on top of Kafka and other messaging systems, as we point out in the wiki.\nHaving audit capabilities built into Kafka would be really great, meanwhile, you're most welcome to check out Aletheia, perhaps you'll find it useful as it provides the Breadcrumb generation out of the box.\n\n-Stas"], "derived": {"summary": "LinkedIn has a system that does monitoring on top of our data flow to ensure all data is delivered to all consumers of data. This works by having each logical \"tier\" through which data passes produce messages to a central \"audit-trail\" topic; these messages give a time period and the number of messages that passed through that tier in that time period.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add audit trail to kafka - LinkedIn has a system that does monitoring on top of our data flow to ensure all data is delivered to all consumers of data. This works by having each logical \"tier\" through which data passes produce messages to a central \"audit-trail\" topic; these messages give a time period and the number of messages that passed through that tier in that time period."}, {"q": "What updates or decisions were made in the discussion?", "a": "Hi guys,\n\nWe've adopted the data model above in Aletheia (https://github.com/outbrain/Aletheia), an open source data delivery framework we've been working on here at Outbrain. \nIn Aletheia we call these audit trails \"Breadcrumbs\", and have them generated by the producer and consumer sides. We're working towards integrating the above mentioned patch in order to provide a client side dashboard.\n\nAletheia is by no means meant to replace Kafka, it is rather an abstraction layer on top of Kafka and other messaging systems, as we point out in the wiki.\nHaving audit capabilities built into Kafka would be really great, meanwhile, you're most welcome to check out Aletheia, perhaps you'll find it useful as it provides the Breadcrumb generation out of the box.\n\n-Stas"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-261", "title": "Corrupted request shuts down the broker", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Jun Rao", "labels": [], "created": "2012-02-01T23:40:07.000+0000", "updated": "2012-02-02T17:34:20.000+0000", "description": "Currently, a corrupted produce request brings down the broker. Instead, we should just log it and let it go.", "comments": ["patch attached.", "+1", "+1", "Just committed this to trunk."], "derived": {"summary": "Currently, a corrupted produce request brings down the broker. Instead, we should just log it and let it go.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Corrupted request shuts down the broker - Currently, a corrupted produce request brings down the broker. Instead, we should just log it and let it go."}, {"q": "What updates or decisions were made in the discussion?", "a": "Just committed this to trunk."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-262", "title": "Bug in the consumer rebalancing logic causes one consumer to release partitions that it does not own", "status": "Closed", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "labels": [], "created": "2012-02-02T17:36:43.000+0000", "updated": "2014-10-20T14:08:15.000+0000", "description": "The consumer maintains a cache of topics and partitions it owns along with the fetcher queues corresponding to those. But while releasing partition ownership, this cache is not cleared. This leads the consumer to release a partition that it does not own any more. This can also lead the consumer to commit offsets for partitions that it no longer consumes from. \n\nThe rebalance operation goes through following steps -\n\n1. close fetchers\n2. commit offsets\n3. release partition ownership. \n4. rebalance, add topic, partition and fetcher queues to the topic registry, for all topics that the consumer process currently wants to own. \n5. If the consumer runs into conflict for one topic or partition, the rebalancing attempt fails, and it goes to step 1.\n\nSay, there are 2 consumers in a group, c1 and c2. Both are consuming topic1 with partitions 0-0, 0-1 and 1-0. Say c1 owns 0-0 and 0-1 and c2 owns 1-0.\n\n1. Broker 1 goes down. This triggers rebalancing attempt in c1 and c2.\n2. c1's release partition ownership and during step 4 (above), fails to rebalance.\n3. Meanwhile, c2 completes rebalancing successfully, and owns partition 0-1 and starts consuming data.\n4. c1 starts next rebalancing attempt and during step 3 (above), it releases partition 0-1. During step 4, it owns partition 0-0 again, and starts consuming data.\n5. Effectively, rebalancing has completed successfully, but there is no owner for partition 0-1 registered in Zookeeper.\n", "comments": ["This patch removes the cache used by each consumer to decide whether or not it should trigger a rebalance operation. The reason being that it is very tricky to keep the cache updated in each participating consumer, leading to incorrect partition ownership decisions. \n\nThis patch also changes the code to use the topicRegistry correctly. It is used to keep track of the fetcher queues for every topic and partition the consumer owns. Hence, whenever partition ownership is released, the relevant data needs to be deleted from the topic registry.", "Includes some cleanup. Removing oldPartitionsPerTopicMap and oldConsumersPerTopicMap.", "Some comments:\n\n1. In ZookeeperConsumerConnector.reflectPartitionOwnershipDecision, the local variable success is not intuitive. It should be named to something like hasFailure.\n\n2. In ZookeeperConsumerConnector.releasePartitionOwnership. It's not clear to me why this method has to take an input parameter. Wouldn't it be simpler to always release partition ownership according to topicRegistry?\n\n", "1. Will change that before committing the patch\n2. Maybe. This sounds like a doable optimization. Al though, the code is very complex, and each optimization needs to be thought through deeply and comes with extensive testing. I'd like to commit this patch, since it holds off KAFKA-253 and the release and fixes the bug. I can open a JIRA to address that optimization, or can put it as part of KAFKA-265. ", "2. If releasePartitionOwnership always just checks topicRegistry, the code and the logic will be a bit simpler. Could we run the system test and see if there is any issue with the simplification?", "1. Changed releasePartitionOwnership to not take in a map\n2. Changed the name of the variable success to hasPartitionOwnershipFailed.", "In releasePartitionOwnership(), topicAndPartitionsToBeReleased is no longer used and should be removed. Otherwise, the patch looks good."], "derived": {"summary": "The consumer maintains a cache of topics and partitions it owns along with the fetcher queues corresponding to those. But while releasing partition ownership, this cache is not cleared.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Bug in the consumer rebalancing logic causes one consumer to release partitions that it does not own - The consumer maintains a cache of topics and partitions it owns along with the fetcher queues corresponding to those. But while releasing partition ownership, this cache is not cleared."}, {"q": "What updates or decisions were made in the discussion?", "a": "In releasePartitionOwnership(), topicAndPartitionsToBeReleased is no longer used and should be removed. Otherwise, the patch looks good."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-263", "title": "Enhance single host broker failure test to have 2 topics with uneven distribution on the source brokers", "status": "Resolved", "priority": "Major", "reporter": "John Fung", "assignee": "John Fung", "labels": [], "created": "2012-02-03T21:42:18.000+0000", "updated": "2012-02-07T17:39:23.000+0000", "description": "The change is going to have 1 broker in the source cluster to consume messages of only 1 topic as shown below:\n\nsource clusters\n============\nbroker1 - topic1, topic2\nbroker2 - topic1, topic2\nbroker3 - topic1, topic2\nbroker4 - topic1", "comments": ["Submitting patch. Please note that the following 2 files are modified to print topic info together with checksum such that the no. of checksum for each topic could be validated.\n\ncore/src/main/scala/kafka/consumer/ConsoleConsumer.scala\nperf/src/main/scala/kafka/perf/ProducerPerformance.scala", "I took a quick look at the patch and have one suggestion -\n\n-    def init(props: Properties) {}\n+    def init(props: Properties, tp: String) {}\n\nThe above change seems unnecessary. Since the MessageFormatter takes in a Properties object, you can pass in the topic as part of the  --property topic=foo argument for the ConsoleConsumer.\n\nApart of the above, the patch looks good.", "Hi Neha,\n\nThanks for your review. I have made the changes in v2 patch:\n- init is reverted to its original signature\n- topic is passed into ChecksumMessageFormatter from props\n\nJohn", "+1 on v2. Thanks for working on this patch !", "Committed this, after making the following minor changes -\n\nChanged ProducerPerformance to output the checksums at the DEBUG level\nChanged ProducerPerformance to use the new Logging trait."], "derived": {"summary": "The change is going to have 1 broker in the source cluster to consume messages of only 1 topic as shown below:\n\nsource clusters\n============\nbroker1 - topic1, topic2\nbroker2 - topic1, topic2\nbroker3 - topic1, topic2\nbroker4 - topic1.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Enhance single host broker failure test to have 2 topics with uneven distribution on the source brokers - The change is going to have 1 broker in the source cluster to consume messages of only 1 topic as shown below:\n\nsource clusters\n============\nbroker1 - topic1, topic2\nbroker2 - topic1, topic2\nbroker3 - topic1, topic2\nbroker4 - topic1."}, {"q": "What updates or decisions were made in the discussion?", "a": "Committed this, after making the following minor changes -\n\nChanged ProducerPerformance to output the checksums at the DEBUG level\nChanged ProducerPerformance to use the new Logging trait."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-264", "title": "Change the consumer side load balancing and distributed co-ordination to use a consumer co-ordinator", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "labels": [], "created": "2012-02-06T21:53:49.000+0000", "updated": "2016-10-07T16:03:07.000+0000", "description": "A high level design for the zookeeper consumer is here - https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Detailed+Consumer+Coordinator+Design", "comments": ["Interesting stuff here.  Just a question that logically follows from having a coordinator: what happens if the coordinating process goes down or becomes unavailable? ", "The consumer co-ordinator itself is highly available, since it is elected from amongst the available consumer processes in a group. That can be done using the standard recipe for leader election using Zookeeper", "Hmm, forgive me if my questions are trivial :) \n\nWhat I mean is, the election of the leader can be handled by ZK, but what happens if the thread/process/machine with the coordinator goes down?  Does every consumer group listen on the consumer-leader path in ZK and issue a new round of coordinator election, ala how it'll be done for replication leaders?", "If a process goes down, the ephemeral node for /consumers/[group]/leader will get deleted from Zookeeper. Each consumer process listens on that path, and triggers a leader election. However, these are very low level details of the implementation, which I intended to chalk out after the high level design looked good. \n\nYou can read more about Zookeeper based leader election here - http://zookeeper.apache.org/doc/trunk/recipes.html#sc_leaderElection", "Yup, got it, makes sense.  I misinterpreted the wiki and thought election was only done once and at startup, hence my queston.  I'll probably take another look when I get some spare time :-) Thanks, Neha.", "Please refer to this wiki page for the detailed description of the implementation:\n\nhttps://cwiki.apache.org/confluence/display/KAFKA/Kafka+Detailed+Consumer+Coordinator+Design#KafkaDetailedConsumerCoordinatorDesign-11.Implementation", "Is this still going into 0.8?  Is it already in there?", "This is scheduled for 0.9", "I found it browsing \"fix version=0.8\"", "https://cwiki.apache.org/confluence/display/KAFKA/Consumer+co-ordinator link does not work..", "I think this is no longer active."], "derived": {"summary": "A high level design for the zookeeper consumer is here - https://cwiki. apache.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Change the consumer side load balancing and distributed co-ordination to use a consumer co-ordinator - A high level design for the zookeeper consumer is here - https://cwiki. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "I think this is no longer active."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-265", "title": "Add a queue of zookeeper notifications in the zookeeper consumer to reduce the number of rebalancing attempts", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Jun Rao", "labels": [], "created": "2012-02-06T21:59:37.000+0000", "updated": "2012-02-17T02:15:23.000+0000", "description": "The correct fix for KAFKA-262 and other known issues with the current consumer rebalancing approach, is to get rid of the cache in the zookeeper consumer. \nThe side-effect of that fix, though, is the large number of zookeeper notifications that will trigger a full rebalance operation on the consumer. \n\nIdeally, the zookeeper notifications can be batched and only one rebalance operation can be triggered for several such ZK notifications. ", "comments": ["patch attached.", "If the queue is full, the ZKclient listener can hang temporarily. This is not ideal, since ZKClient will not be able to deliver more events until a rebalance operation is completed and the queue is cleared. In practice, this might not be a big issue, but can be easily avoided.\n\nI think there is an alternative solution to this problem, one that will \n\n1. avoid maintaining this queue \n2. reduce memory consumption in the consumer\n3. avoid adding another config option \n\nHow about using just using a boolean variable that will indicate at least one rebalancing operation request ? The watcher thread can use a Condition to wait if the boolean variable is false. The ZK listener can merely set the boolean to true and signal the Condition, so that the watcher thread can proceed with a rebalancing operation.", "That's a good idea. Attach patch v2.", "+1 for v2. Thanks for accomodating the change", "Thanks for the review. Just committed this.", "Found a bug. Condition should use await/notify, instead of wait/notify. Attach a patch.", "Subtle, but critical :-) \n+1", "Found a new problem. The watch executor thread doesn't shutdown properly. Attached another patch.", "+1. Doesn't the test in system_test/broker_failure catch this ?", "Committed the fix for both Condition and shutdown to trunk."], "derived": {"summary": "The correct fix for KAFKA-262 and other known issues with the current consumer rebalancing approach, is to get rid of the cache in the zookeeper consumer. The side-effect of that fix, though, is the large number of zookeeper notifications that will trigger a full rebalance operation on the consumer.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add a queue of zookeeper notifications in the zookeeper consumer to reduce the number of rebalancing attempts - The correct fix for KAFKA-262 and other known issues with the current consumer rebalancing approach, is to get rid of the cache in the zookeeper consumer. The side-effect of that fix, though, is the large number of zookeeper notifications that will trigger a full rebalance operation on the consumer."}, {"q": "What updates or decisions were made in the discussion?", "a": "Committed the fix for both Condition and shutdown to trunk."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-266", "title": "Kafka web console", "status": "Resolved", "priority": "Major", "reporter": "Evan Chan", "assignee": null, "labels": ["project"], "created": "2012-02-07T22:49:18.000+0000", "updated": "2015-02-07T23:57:38.000+0000", "description": "This issue is created to track a community-contributed Kafka Web UI.\n\nHere is an initial list of goals:\n- Be able to easily see which brokers are up\n- Be able to see lists of topics, connected producers, consumer groups, connected consumers\n- Be able to see, for each consumer/partition, its offset, and more importantly, # of bytes unconsumed (== largest offset for partition - current offset)\n- (Wish list) have a graphical view of the offsets\n- (Wish list) be able to clean up consumer state, such as stale claimed partitions\n\nList of challenges/questions:\n- Which framework?  Play! for Scala?\n- Is all the data available from JMX and ZK?  Hopefully, watching the files on the filesystem can be avoided....\n- How to handle large numbers of topics, partitions, consumers, etc. efficiently", "comments": ["I think this would be fantastic to have. We don't have such a thing at LinkedIn, we just have hooks into our monitoring system, but I think this would be better.\n\nWe do have an application that does a data audit. This basically proves that all data that is sent is received by all interested consumers. See KAFKA-260 for a detailed description and screenshot. We had planned to fold this functionality into the open source project. Since both of these are kind of monitoring/health functionality are pretty closely related, maybe it would make sense to combine them.\n\nThe existing app is a java app that implements runs a few jetty servlets to implement a JSON api. The UI just runs off this api and is implemented in hmtl/css/js. The charts are dygraph and it uses jquery for javascript helpers. I am not married to this setup (the original code was written by someone else and just refactored by me), so I would be okay moving it into another mvc framework if there is something much better out there as long as it isn't too complex.", "Jay,\n\nWould it be possible to look at the source of the java app you guys have, or open source it somehow?  It would be great to see how you guys are pulling out the stats, and might save lots of work in doing the web console.\n\nthanks.", "yeah i will post a patch.", "I would like to see a simple monitoring console as something that is embedded within the brokers themselves (ActiveMQ does something similar).  It should have minimal dependencies (jetty, servlets, and Javascript sound appropriate).  The simple HTTP interface should allow for quick checking to see if:\n\n1) Brokers are up\n2) Topic are being written to\n3) Topics are being consumed\n\nWhile most of this data is available via JMX already, HTTP is a preferable alternative.  An external application that provides comprehensive management, audit solution seems to make sense as well.", "JMX, HTTP are both fine but also hooking into Graphite, Ganglia etc make for a more ubuitious friendly system (for the ops folk) I think KAFKA-203 really address this and a web console that is going to be for monitoring only I think should just pull the JSON from the http connector of the coda hale metrics implementation IMHO ", "My take is that, as Joe says, most serious folks already have some system they use for monitoring across their stack (ganglia or whatever). To really work operationally you can't try to replace this or add a new system. For this reason, I kind of prefer the web app to be an optional stand-alone thing since it may not be of use to everyone, though i think that complicates its design (potentially). I think the advantage of the web app is custom display of very kafka-specific things (the audit, cluster status, etc).", "The web console would not primarily be for monitoring -- for us anyways -- and I agree it should not replace Ganglia etc.  (Although at some shops - most notably Google - having web routes for every single service and app is a policy).  I think the web console would be invaluable for debugging and status though.  For looking at a snapshot of the system easily. \n\nAlso, standard JMX just doesnt work in EC2, you don't know what ports need to be opened up. \n\nI think the metric Im most interested in -- number of messages or MB outstanding -- is not available directly from Kafka server itself anyways.  \n\nWhat is the timeframe for KAFKA-203?", "The web console would not primarily be for monitoring -- for us anyways -- and I agree it should not replace Ganglia etc.  (Although at some shops - most notably Google - having web routes for every single service and app is a policy).  I think the web console would be invaluable for debugging and status though.  For looking at a snapshot of the system easily. \n\nAlso, standard JMX just doesnt work in EC2, you don't know what ports need to be opened up. \n\nI think the metric Im most interested in -- number of messages or MB outstanding -- is not available directly from Kafka server itself anyways.  \n\nWhat is the timeframe for KAFKA-203?", "BTW, just some thoughts.  I'm thinking of not so much a web console for individual Kafka brokers, but a web console for an entire Kafka cluster, targetting mostly smaller clusters.   This would grab information from ZK and from JMX (or JMX-over-HTTP) for individual brokers to put together reports like:\n- Status of different Kafka brokers in the cluster, including overall read and write rates, etc.\n- Information on consumers (at least the high level consumers registered in ZK), outstanding messages or how far behind they are, etc.", "Hi\nI started building an admin web app, from which I let the users of the kafka cluster:\n1. Watch how much their consumer group is behind a certain offset,\n2. Update their topic to latest offset \n\nI developed it using play!,  it connects the ZK and the Brokers using the RPC, \n\nWill interest anyone?\n\n\n ", "Absolutely, it seems like a good starting point. Would you mind uploading it as a patch ?", "I am very happy to hear,\n\nI will share you with my code, as soon as I get approval for doing that from my company legal department, I don't think it should be a problem .\n\n\n\n ", "This would be absolutely great to have. Wanted to know, how it has progressed?", "Any problem if it will be in Java? I have to explore better Kafka and I think It would be a nice opportunity....unfortunately I'm not so skilled in Scala", "Andrea, thanks for your interest. The tool can be written in java, if that's more convenient.", "Hey Andrea, Guy, did you make any head way on this? ", "Hi Folks, \nI have a admin web app that I built for this purpose, it connects to Zk and displays information about brokers, topics, and consumers. \n\nIt really useful, especially when just getting started with Kafka, I would like to contribute it to the this project.\n\nIt is a Play App, written in Scala and depends on Playframework, Kafka, and jQuery.  Any concerns about these dependencies?\n\n Also, what are the preferred next steps, should I attach source or put in public github, not sure how you would like to move forward?\n\n", "James,\n\nThanks for your interest. Perhaps you can start with a wiki in Kafka that describes your current design and how it looks? As for the code, we can put it in a separate dir and build a separate jar. So, the additional dependency won't affect other existing jars.", "Hey James,\n\nThat's awesome! Yeah I recommend github and adding it to our Ecosystem page. Maintaining outside tools with the main code base turned out to be a real drag for the people working on the tool. The apache process works better (we think) for a focused code base, github works well for a large diverse ecosystem as each person can maintain their piece without blocking code reviews or the need to release in sync with Kafka. So we aren't trying to suck in everything that integrates with Kafka into the main project.", "Adding another one to the list: https://github.com/claudemamo/kafka-web-console. \n\nThe project is as well built on top of Play and uses AngularJS in addition to jQuery. It's using Twitter's Zookeeper client to make non-blocking queries to Zookeeper for brokers, consumers groups, partitions, etc... I forked the high level consumer and replaced its blocking queues with callbacks so that it would be possible to implement topic feeds without being too much of a drain on resources. I'm going to bump the version no. to 1.0 soon but in the meantime please feel free to take the console for a spin and let me know your thoughts. The current list of implemented features is:\n\n- Topic feeds via Web Sockets\n- Which brokers are up \n- Lists of topics & no. partitions, and connected consumer groups\n- For each consumer/partition, its current offset\n\nFinally the console provides a JSON interface.", "[~claude.mamo] Very cool. I added it to the Ecosystem page. It would be great if you could add a README and a couple screen shots to the project so people would know what they get with it.\n\n[~jmiller] Let me know if you get this up on github and I'll add it to the Ecosystem page too.\n\nIt would also be good to just ping the users list and let people know that this exists. A LOT of people have been asking for something like this so I suspect there will be a lot of interest.", "Definitely [~jkreps], docs are in my TODO list before releasing the first version of the project. I noticed in the JIRA description that a goal for the console is to get a list of connected producers. I checked on ZK as well as on JMX and there is no info on producers, unless I missed it. Is this possible to achieve with the current version of Kafka?", "That is correct. If JMX would be a workable approach, adding JMX for producer connections should be reasonably straight-forward to do..."], "derived": {"summary": "This issue is created to track a community-contributed Kafka Web UI. Here is an initial list of goals:\n- Be able to easily see which brokers are up\n- Be able to see lists of topics, connected producers, consumer groups, connected consumers\n- Be able to see, for each consumer/partition, its offset, and more importantly, # of bytes unconsumed (== largest offset for partition - current offset)\n- (Wish list) have a graphical view of the offsets\n- (Wish list) be able to clean up consumer state, such as stale claimed partitions\n\nList of challenges/questions:\n- Which framework?  Play! for Scala?\n- Is all the data available from JMX and ZK?  Hopefully, watching the files on the filesystem can be avoided.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Kafka web console - This issue is created to track a community-contributed Kafka Web UI. Here is an initial list of goals:\n- Be able to easily see which brokers are up\n- Be able to see lists of topics, connected producers, consumer groups, connected consumers\n- Be able to see, for each consumer/partition, its offset, and more importantly, # of bytes unconsumed (== largest offset for partition - current offset)\n- (Wish list) have a graphical view of the offsets\n- (Wish list) be able to clean up consumer state, such as stale claimed partitions\n\nList of challenges/questions:\n- Which framework?  Play! for Scala?\n- Is all the data available from JMX and ZK?  Hopefully, watching the files on the filesystem can be avoided."}, {"q": "What updates or decisions were made in the discussion?", "a": "That is correct. If JMX would be a workable approach, adding JMX for producer connections should be reasonably straight-forward to do..."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-267", "title": "Enhance ProducerPerformance to generate unique random Long value for payload", "status": "Resolved", "priority": "Major", "reporter": "John Fung", "assignee": "Yang Ye", "labels": [], "created": "2012-02-08T05:41:02.000+0000", "updated": "2012-11-01T08:52:03.000+0000", "description": "This is achieved by:\n1. Adding a new class UniqueRandom to shuffle a range of numbers.\n2. An optional new argument \"start-index\" is added to specify the starting number of the range to be shuffled. If this argument is omitted, it is defaulted to 1. So it is backward compatible with the argument options.\n3. The ending number of the range is the starting number + number of messages - 1.\n\n\nOther ProducerPerformance advancement: \n1. producing to multiple topics\n2. supporting multiple instances of producer performance ( and distinguishes them)\n3. allowing waiting some time after sending a request", "comments": ["\n\nRefactor a log of code\n1. support mutliple topics\n2. support various sized messages\n3. support sleep after sending the mesage\n\n4. refactor a lot of code to make it readable\n\n5. can be used for both producing test data and performance test. For the former usage, one has to give the --initial-message-id argument. ", "Can't find a revision that I can apply the patch cleanly. Could you rebase?", "\nrebase on kafka 42", "Thanks for patch v3. The code is still pretty messy, mostly for historical reasons. For example, the fix/variable length option is mixed with seqIdMode and batch size, etc. I suggest that we do the following: (1) Get rid of the seqId mode and always generate sequential ids in the message header. If the user doesn't specify the starting index, seq Id will start from 0. (2) Alway pad a message with random bytes, whether the message is fix length or variable length. (3) Don't batch messages into sets in the sync mode. Instead, send one message at a time in sync mode. (4) The send gap should probably be added after sending a batch of messages, not after each message. If we do all this, the send thread can just have one simple loop as the following:\n\nwhile(j < messagesPerThread) {\n  for each topic {\n    msgString = // get the message for a seq id of a given size     \n    msg = create Message\n    producer.send(msg)\n    if(config.messageSendGapMs > 0 && batch size is reached)\n      Thread.sleep(config.messageSendGapMs)\n  }\n}\n\nAlso, we need to patch system test since the command line option has changed.\n", "\ncreate a function \"generateProducerData\" for common use. It can generate for both sync and async mode, fixed and variable length, sequential id mode and non-sequential id mode.\n\nThe main structure of the fun() function of ProducerThread is shrinked to :\n\nFor each batch:\n     For each topic:\n           generateProducerData\n           send\n           sleep if required\n     \n\n", "Thanks for patch v4. It's better, but still messy. Some more comments:\n\n40. I  suggest that we combine initialMessageIdOpt and varyMessageSizeOpt into one option, something like message-mode. It will take 3 choices: sequence, fix-length, and variable-length that are mutually exclusive.\n\n41. I suggest that batchSizeOpt be only used in async mode. In sync mode, we always send a single message at a time without batching. This will simplify the way that we generate messages. When generating the data, we just need to generate one message at a time, depending on message-mode.\n\n42. The following 2 methods should be moved out of ProducerThread and put in a util class.\ngenerateProducerData\ngenerateMessageWithSeqId\n\n43. UniqueRandom: Why do we need this class? Can't we just use Random.nextBytes()?\n\n44. The following chunk of code is identical to that in Kafka. Could we create a util function to share the code?\n      val metricsConfig = new KafkaMetricsConfig(verifiableProps)\n      metricsConfig.reporters.foreach(reporterType => {\n        val reporter = Utils.createObject[KafkaMetricsReporter](reporterType)\n        reporter.init(verifiableProps)\n        if (reporter.isInstanceOf[KafkaMetricsReporterMBean])\n          Utils.registerMBean(reporter, reporter.asInstanceOf[KafkaMetricsReporterMBean].getMBeanName)\n      })\n\n45. There are a few long lines like the following. Let's put .format in a separate line.\n            println((\"%s, %d, %d, %d, %d, %.2f, %.4f, %d, %.4f\").format(formattedReportTime, config.compressionCodec.codec,\n                                                                        threadId, config.messageSize, config.batchSize, (bytesSent*1.0)/(1024 * 1024), mbPerSec, nSends, numMessagesPerSec))\n", "\nChange since v4:\n\n1. removing the \"UniqueRandom\" class and its related usages\n\n2. further simplify the code of function generateProducerData()", "Thanks for patch v5. Looks good. Some minor comments:\n\n50. In the following line in generateProducerData(), we should allocate a byte array and pass it to the constructor of Message. ByteBuffer.array() gives you the backing array and it may or may not be the size that you allocated.\n        new Message(ByteBuffer.allocate(if(config.isFixSize) config.messageSize else 1 + rand.nextInt(config.messageSize)).array())\n\n51. Could you break all long lines into multiple lines?\n\n52. Could you expose producer.num.retries and producer.retry.backoff.ms to the command line and default them to the same value as in ProducerConfig?\n", "Thanks for patch v5, we are pretty close to checking this in. A few more review comments -\n\n1. Fix the description of the topics argument, it should just say \"produce to\". Let's remove the consume from. Also, if it is a csv list, let's add that to the description as well\n2. Fix the description of batch-size. Right now, it just says \"size\". Same for threads, it just says \"count\"\n3. Let's describe the supported compression codecs in the description for compression-codec.\n4. Fix typo in the description for metrics-dir\n5. I think it is valuable to default to async. That is the most common production usage, I doubt we will frequently test one-message-at-a-time\n6. Should message-send-gap be renamed to message-send-gap-ms ?\n7. Now that we have metrics csv reporter, will we use ProducerPerformance to calculate the producer throughput in MB/s, messages/sec at all ? I guess we can just enable csv reporting and rely on our metrics to measure throughput/latency correctly. In that case, we don't need to compute the various metrics and also can get rid of the println and showDetailedStats parameter. With this change, we can view ProducerPerformance like a workload generator only. Not sure if I'm missing something here though.\n8. If the changes specified in #7 are implemented, let's change the system test script to dump the producer metrics for ProducerPerformance through the new metrics-dir option.\n9. generateProducerData is a bit unreadable. Recommend you separately compute the messageId and messageSize and pass it into generateMessageWithSeqId\n", "\nchange since v6:\n\n1. the producer performance furthur simplified, long line decoupled into two lines\n\n2. \"showdetai\" option is removed from producerPerformance\n\n3. producer performance use async for default\n\n4. #messages sent in replication_test case 1 and test 0001 are set to 100,000, max log file size is changed to 1024000000, so that few log segments are created, the log checksum checking could be speed up.", "\nMove the logging of successfully sending messages from Producer Performance to syncProducer", "Thanks for patch v7. Not sure if printing messages in syncProducer is a good idea. In general, messages sent in syncProducer are not necessarily strings and may not be printable. ProducerPerformance, on the other hand, always sends string messages, which are printable. So, I suggest that we keep the message printing in ProducerPerformance. ", "Keeping the printing in ProducerPerformance would work, but we need to have a consistent way of counting the messages that were successfully sent by the async producer, during system testing. Right now, ProducerPerformance prints messages assuming they will get sent but only SyncProducer really knows if the messages were sent successfully. Thoughts ?", "What we can do is that, in system tests, tune #retries and backoff time in ProducerPerformance according to the failure scenario so that we expect no data loss on the producer side. Both knobs are exposed in ProducerPerformance now.", "V6 should be the right one, please review this one", "+1 on patch v6. Committed to 0.8. ", "\nThis is a follow up patch in order to simulate real data load. Before we generate sequential messages padded with a series of character \"x\"'s, which leads to very good compression ratio which is not typical in real case. So we want to generate random payload.\n\nTo give a comparison: sending 400 messages with batch size 200, before the data size is 48800 bytes, after the patch the size is 65600 bytes.\n\n\n"], "derived": {"summary": "This is achieved by:\n1. Adding a new class UniqueRandom to shuffle a range of numbers.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Enhance ProducerPerformance to generate unique random Long value for payload - This is achieved by:\n1. Adding a new class UniqueRandom to shuffle a range of numbers."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is a follow up patch in order to simulate real data load. Before we generate sequential messages padded with a series of character \"x\"'s, which leads to very good compression ratio which is not typical in real case. So we want to generate random payload.\n\nTo give a comparison: sending 400 messages with batch size 200, before the data size is 48800 bytes, after the patch the size is 65600 bytes."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-268", "title": "Add  another reconnection condition to the syncProducer:  the time elapsed since last connection", "status": "Resolved", "priority": "Minor", "reporter": "Yang Ye", "assignee": null, "labels": [], "created": "2012-02-09T19:03:13.000+0000", "updated": "2012-02-20T22:25:23.000+0000", "description": "Add  another reconnection condition to the syncProducer:  the time elapsed since last connection. If it's larger than the pre-specified threshold, close the connection and reopen it. ", "comments": ["Some comments:\n1. SyncProducerConfigShared: change reconnectTimeInterval to reconnectTimeIntervalMs and reconnect.timeInterval to reconnect.time.interval.ms, to be consistent with existing naming convention.\n2. There is no real change to ByteBufferMessageSet and Utils. Please revert them.\n3. The default value of reconnectTimeInterval is too small. Let's set it to 10 minutes. Also, we probably need a way to turn off time-based reconnect. How about using a negative value to indicate that? Let's also add a comment in the code to document that.", "got them, nice suggestions", "As suggested by the first comment", "Thanks for the patch, Yang. Just committed this.", "A suggestion: It's possible that we have a number of producer clients all started at around the same time. Then they will all reconnect at almost exactly the same time. This is probably not good for load balancing. What we want is to spread the reconnects. One way to do that is to initialize lastConnectionTime to a random timestamp btw (now - reconnectTimeInterval) and now.", "Sure, what about doing it like this,   I added on field in the SyncProducer object,  var randomGenerator = new Random() \n\nthen change the initilaization of lastConnectionTime to:\nprivate var lastConnectionTime = System.currentTimeMillis - config.reconnectInterval + SyncProducer.randomGenerator.nextDouble() * config.reconnectInterval\n\n\nOtherwise if we write \nprivate var lastConnectionTime = System.currentTimeMillis - config.reconnectInterval + new Random().nextDouble() * config.reconnectInterval\n\nor \nprivate var lastConnectionTime = System.currentTimeMillis - config.reconnectInterval + new Random(System.currentTimeMillis).nextDouble() * config.reconnectInterval\n\n\nwe may worry about the determinism of pseudo random generator\n", "How about the following?\nprivate var lastConnectionTime = System.currentTimeMillis - SyncProducer.randomGenerator.nextDouble() * config.reconnectInterval", "This is clearer, thanks. This part is also included in the shallow_iterator patch. ", "Committed the improvement to trunk."], "derived": {"summary": "Add  another reconnection condition to the syncProducer:  the time elapsed since last connection. If it's larger than the pre-specified threshold, close the connection and reopen it.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add  another reconnection condition to the syncProducer:  the time elapsed since last connection - Add  another reconnection condition to the syncProducer:  the time elapsed since last connection. If it's larger than the pre-specified threshold, close the connection and reopen it."}, {"q": "What updates or decisions were made in the discussion?", "a": "Committed the improvement to trunk."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-269", "title": "./system_test/producer_perf/bin/run-test.sh  without --async flag does not run", "status": "Resolved", "priority": "Major", "reporter": "Praveen Ramachandra", "assignee": null, "labels": ["newbie", "performance"], "created": "2012-02-11T03:28:42.000+0000", "updated": "2016-12-21T02:35:50.000+0000", "description": "When I run the tests without --async option, The tests doesn't produce even a single message. \n\nFollowing defaults where changed in the server.properties\nnum.threads=Tried with 8, 10, 100\nnum.partitions=10\n\n\n", "comments": ["GitHub user rekhajoshm opened a pull request:\n\n    https://github.com/apache/kafka/pull/49\n\n    KAFKA-269: run-test.sh async test\n\n    KAFKA-269: run-test.sh async test\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/rekhajoshm/kafka KAFKA-269\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/kafka/pull/49.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #49\n    \n----\ncommit 99302738459c1be9166ca9808971643bc220f675\nAuthor: Joshi <rekhajoshm@gmail.com>\nDate:   2015-02-23T01:38:29Z\n\n    KAFKA-269: run-test.sh async test\n\n----\n", "[~praveen27] \nBy default if no  --sync option is provided the producer run is in async mode.\n\nRan the run-test.sh without --async option and works on 0.8.2.or maybe you can check your zk/topic/reporting-interval?\nstart producing 2000000 messages ...\nstart.time, end.time, compression, message.size, batch.size, total.data.sent.in.MB, MB.sec, total.data.sent.in.nMsg, nMsg.sec\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/Users/rjoshi2/Documents/code/kafka-fork/kafka/core/build/dependant-libs-2.10.4/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/Users/rjoshi2/Documents/code/kafka-fork/kafka/core/build/dependant-libs-2.10.4/slf4j-log4j12-1.7.6.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n2015-02-22 17:35:18:471, 2015-02-22 17:35:24:331, 0, 200, 200, 381.47, 65.0972, 2000000, 341296.9283\nwait for data to be persisted\n\nThe patch as, AFAIU, 'async' is not a recognized option.Only - -sync is.\nThanks.", "This is for the old system tests, which have been removed.", "Github user rekhajoshm closed the pull request at:\n\n    https://github.com/apache/kafka/pull/49\n"], "derived": {"summary": "When I run the tests without --async option, The tests doesn't produce even a single message. Following defaults where changed in the server.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "./system_test/producer_perf/bin/run-test.sh  without --async flag does not run - When I run the tests without --async option, The tests doesn't produce even a single message. Following defaults where changed in the server."}, {"q": "What updates or decisions were made in the discussion?", "a": "Github user rekhajoshm closed the pull request at:\n\n    https://github.com/apache/kafka/pull/49"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-270", "title": " sync producer / consumer test producing lot of kafka server exceptions & not getting the throughput mentioned here http://incubator.apache.org/kafka/performance.html", "status": "Resolved", "priority": "Major", "reporter": "Praveen Ramachandra", "assignee": null, "labels": ["clients", "core", "newdev", "performance"], "created": "2012-02-11T03:40:54.000+0000", "updated": "2017-08-30T18:22:52.000+0000", "description": "I am getting ridiculously low producer and consumer throughput.\n\nI am using default config values for producer, consumer and broker\nwhich are very good starting points, as they should yield sufficient\nthroughput.\n\nAppreciate if you point what settings/changes-in-code needs to be done\nto get higher throughput.\n\nI changed num.partitions in the server.config to 10.\n\nPlease look below for exception and error messages from the server\nBTW: I am running server, zookeeper, producer, consumer on the same host.\n\n\n====Consumer Code=====\n       long startTime = System.currentTimeMillis();\n       long endTime = startTime + runDuration*1000l;\n\n       Properties props = new Properties();\n       props.put(\"zk.connect\", \"localhost:2181\");\n       props.put(\"groupid\", subscriptionName); // to support multiple\nsubscribers\n       props.put(\"zk.sessiontimeout.ms\", \"400\");\n       props.put(\"zk.synctime.ms\", \"200\");\n       props.put(\"autocommit.interval.ms\", \"1000\");\n\n       consConfig =  new ConsumerConfig(props);\n       consumer =\nkafka.consumer.Consumer.createJavaConsumerConnector(consConfig);\n\n       Map<String, Integer> topicCountMap = new HashMap<String, Integer>();\n       topicCountMap.put(topicName, new Integer(1)); // has the topic\nto which to subscribe to\n       Map<String, List<KafkaMessageStream<Message>>> consumerMap =\nconsumer.createMessageStreams(topicCountMap);\n       KafkaMessageStream<Message> stream =  consumerMap.get(topicName).get(0);\n       ConsumerIterator<Message> it = stream.iterator();\n\n       while(System.currentTimeMillis() <= endTime )\n       {\n           it.next(); // discard data\n           consumeMsgCount.incrementAndGet();\n       }\n\n====End consumer CODE============================\n\n\n=====Producer CODE========================\n       props.put(\"serializer.class\", \"kafka.serializer.StringEncoder\");\n       props.put(\"zk.connect\", \"localhost:2181\");\n           // Use random partitioner. Don't need the key type. Just\nset it to Integer.\n           // The message is of type String.\n           producer = new kafka.javaapi.producer.Producer<Integer,\nString>(new ProducerConfig(props));\n\n\n       long endTime = startTime + runDuration*1000l; // run duration\nis in seconds\n       while(System.currentTimeMillis() <= endTime )\n       {\n           String msg =\norg.apache.commons.lang.RandomStringUtils.random(msgSizes.get(0));\n           producer.send(new ProducerData<Integer, String>(topicName, msg));\n           pc.incrementAndGet();\n\n       }\n       java.util.Date date = new java.util.Date(System.currentTimeMillis());\n       System.out.println(date+\" :: stopped producer for topic\"+topicName);\n\n=====END Producer CODE========================\n\nI see a bunch of exceptions like this\n\n[2012-02-11 02:44:11,945] ERROR Closing socket for /188.125.88.145 because of error (kafka.network.Processor)\njava.io.IOException: Connection reset by peer\n\tat sun.nio.ch.FileChannelImpl.transferTo0(Native Method)\n\tat sun.nio.ch.FileChannelImpl.transferToDirectly(FileChannelImpl.java:405)\n\tat sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:506)\n\tat kafka.message.FileMessageSet.writeTo(FileMessageSet.scala:107)\n\tat kafka.server.MessageSetSend.writeTo(MessageSetSend.scala:51)\n\tat kafka.network.MultiSend.writeTo(Transmission.scala:95)\n\tat kafka.network.Processor.write(SocketServer.scala:332)\n\tat kafka.network.Processor.run(SocketServer.scala:209)\n\tat java.lang.Thread.run(Thread.java:662)\n\njava.io.IOException: Connection reset by peer\n\tat sun.nio.ch.FileDispatcher.read0(Native Method)\n\tat sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:21)\n\tat sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:198)\n\tat sun.nio.ch.IOUtil.read(IOUtil.java:171)\n\tat sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:243)\n\tat kafka.utils.Utils$.read(Utils.scala:485)\n\tat kafka.network.BoundedByteBufferReceive.readFrom(BoundedByteBufferReceive.scala:54)\n\tat kafka.network.Processor.read(SocketServer.scala:304)\n\tat kafka.network.Processor.run(SocketServer.scala:207)\n\tat java.lang.Thread.run(Thread.java:662)\n\nAnd Many INFO messages e.g.,\nINFO: Expiring session 0x1356a43167e0009, timeout of 6000ms exceeded (org.apache.zookeeper.server.ZooKeeperServer)\nINFO: Closed socket connection for client /127.0.0.1:59884 which had sessionid 0x1356a43167e0022 (org.apache.zookeeper.server.NIOServerCnxn)\n\n\n", "comments": ["A few things to try:\n\n1. It seems that there is ZK session expiration in the client. This should be rare. If it's frequent, it's very likely caused by client GC. Please check your GC log.\n2. Enable debug level logging in FileMessageSet in the broker. You will see the flush time for each log write. See if the flush time is reasonable (typically low 10s of ms) since it controls how many IOs a broker can do per second. ", "can also try closing the producer.\nI would get the below error in broker console without closing producer. If I close the producer, the error will never appear.\n[2013-11-05 14:07:34,097] ERROR Closing socket for /192.168.30.114 because of error (kafka.network.Processor)\njava.io.IOException: Connection reset by peer\n        at sun.nio.ch.FileDispatcher.read0(Native Method)\n        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:21)\n        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:198)\n        at sun.nio.ch.IOUtil.read(IOUtil.java:171)\n        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:243)\n        at kafka.utils.Utils$.read(Utils.scala:538)\n        at kafka.network.BoundedByteBufferReceive.readFrom(BoundedByteBufferReceive.scala:54)\n        at kafka.network.Processor.read(SocketServer.scala:311)\n        at kafka.network.Processor.run(SocketServer.scala:214)\n        at java.lang.Thread.run(Thread.java:662)", "Any updates on this issue.", "Closing due to inactivity. Pl reopen if you think the issue still exists\n"], "derived": {"summary": "I am getting ridiculously low producer and consumer throughput. I am using default config values for producer, consumer and broker\nwhich are very good starting points, as they should yield sufficient\nthroughput.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "sync producer / consumer test producing lot of kafka server exceptions & not getting the throughput mentioned here http://incubator.apache.org/kafka/performance.html - I am getting ridiculously low producer and consumer throughput. I am using default config values for producer, consumer and broker\nwhich are very good starting points, as they should yield sufficient\nthroughput."}, {"q": "What updates or decisions were made in the discussion?", "a": "Closing due to inactivity. Pl reopen if you think the issue still exists"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-271", "title": "Modify new FetchResponse object to remove the initial offset field", "status": "Resolved", "priority": "Major", "reporter": "Prashanth Menon", "assignee": "Jay Kreps", "labels": ["replication"], "created": "2012-02-13T15:11:13.000+0000", "updated": "2012-12-06T16:23:34.000+0000", "description": "Currently in the 0.8 branch, the FetchResponse contains the initial offset field for a set of messages.  This field isn't technically required since the client should have access to the request, and therefore the initialOffset which it can set manually.  This should simplify the wire protocol.", "comments": ["Addressed this as part of KAFKA-642."], "derived": {"summary": "Currently in the 0. 8 branch, the FetchResponse contains the initial offset field for a set of messages.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Modify new FetchResponse object to remove the initial offset field - Currently in the 0. 8 branch, the FetchResponse contains the initial offset field for a set of messages."}, {"q": "What updates or decisions were made in the discussion?", "a": "Addressed this as part of KAFKA-642."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-272", "title": "add JMX on broker to track bytes/messages per topic", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Jun Rao", "labels": [], "created": "2012-02-14T17:19:05.000+0000", "updated": "2012-02-15T22:04:21.000+0000", "description": null, "comments": ["Patch attached.", "Does the per topic bean only count messages produced ?  Would it make sense to have a bytes-in/messages-in and a bytes-out/messages-out bean separately ?\nOr maybe I'm missing why only having bytes-in/messages-in is useful ?", "Upload patch v2. Add bytesOut. MessagesOut is a bit hard since currently we don't iterate messageset to send fetch response.", "+1", "Thanks for the review. Committed to trunk."], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "add JMX on broker to track bytes/messages per topic"}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for the review. Committed to trunk."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-273", "title": "Occassional GZIP errors on the server while writing compressed data to disk", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "labels": [], "created": "2012-02-16T22:22:41.000+0000", "updated": "2017-08-17T12:00:09.000+0000", "description": "Occasionally, we see the following errors on the Kafka server -\n\n2012/02/08 14:58:21.832 ERROR [KafkaRequestHandlers] [kafka-processor-6] [kafka] Error processing MultiProducerRequest on NusImpressionSetEvent:0\njava.io.EOFException: Unexpected end of ZLIB input stream\n        at java.util.zip.InflaterInputStream.fill(InflaterInputStream.java:223)\n        at java.util.zip.InflaterInputStream.read(InflaterInputStream.java:141)\n        at java.util.zip.GZIPInputStream.read(GZIPInputStream.java:92)\n        at java.io.FilterInputStream.read(FilterInputStream.java:90)\n        at kafka.message.GZIPCompression.read(CompressionUtils.scala:52)\n        at kafka.message.CompressionUtils$$anonfun$decompress$1.apply$mcI$sp(CompressionUtils.scala:143)\n        at kafka.message.CompressionUtils$$anonfun$decompress$1.apply(CompressionUtils.scala:143)\n        at kafka.message.CompressionUtils$$anonfun$decompress$1.apply(CompressionUtils.scala:143)\n        at scala.collection.immutable.Stream$$anonfun$continually$1.apply(Stream.scala:598)\n        at scala.collection.immutable.Stream$$anonfun$continually$1.apply(Stream.scala:598)\n        at scala.collection.immutable.Stream$Cons.tail(Stream.scala:555)\n        at scala.collection.immutable.Stream$Cons.tail(Stream.scala:549)\n        at scala.collection.immutable.Stream$$anonfun$takeWhile$1.apply(Stream.scala:394)\n        at scala.collection.immutable.Stream$$anonfun$takeWhile$1.apply(Stream.scala:394)\n        at scala.collection.immutable.Stream$Cons.tail(Stream.scala:555)\n        at scala.collection.immutable.Stream$Cons.tail(Stream.scala:549)\n        at scala.collection.immutable.Stream.foreach(Stream.scala:255)\n        at kafka.message.CompressionUtils$.decompress(CompressionUtils.scala:143)\n        at kafka.message.ByteBufferMessageSet$$anon$1.makeNextOuter(ByteBufferMessageSet.scala:119)\n        at kafka.message.ByteBufferMessageSet$$anon$1.makeNext(ByteBufferMessageSet.scala:132)\n        at kafka.message.ByteBufferMessageSet$$anon$1.makeNext(ByteBufferMessageSet.scala:81)\n        at kafka.utils.IteratorTemplate.maybeComputeNext(IteratorTemplate.scala:59)\n        at kafka.utils.IteratorTemplate.hasNext(IteratorTemplate.scala:51)\n        at scala.collection.Iterator$class.foreach(Iterator.scala:631)\n        at kafka.utils.IteratorTemplate.foreach(IteratorTemplate.scala:30)\n        at scala.collection.IterableLike$class.foreach(IterableLike.scala:79)\n        at kafka.message.MessageSet.foreach(MessageSet.scala:87)\n        at kafka.log.Log.append(Log.scala:204)\n        at kafka.server.KafkaRequestHandlers.kafka$server$KafkaRequestHandlers$$handleProducerRequest(KafkaRequestHandlers.scala:70)\n        at kafka.server.KafkaRequestHandlers$$anonfun$handleMultiProducerRequest$1.apply(KafkaRequestHandlers.scala:63)\n        at kafka.server.KafkaRequestHandlers$$anonfun$handleMultiProducerRequest$1.apply(KafkaRequestHandlers.scala:63)\n        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)\n        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)\n        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)\n        at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:34)\n        at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)\n        at scala.collection.mutable.ArrayOps.map(ArrayOps.scala:34)\n        at kafka.server.KafkaRequestHandlers.handleMultiProducerRequest(KafkaRequestHandlers.scala:63)\n        at kafka.server.KafkaRequestHandlers$$anonfun$handlerFor$4.apply(KafkaRequestHandlers.scala:42)\n        at kafka.server.KafkaRequestHandlers$$anonfun$handlerFor$4.apply(KafkaRequestHandlers.scala:42)\n        at kafka.network.Processor.handle(SocketServer.scala:297)\n        at kafka.network.Processor.read(SocketServer.scala:320)\n        at kafka.network.Processor.run(SocketServer.scala:215)\n        at java.lang.Thread.run(Thread.java:619)\n", "comments": ["We've seen this error very occasionally, but in our deployments the Deflator uses jdk-1.6.0.21 and zlib-1.2.3 on Linux and jdk-1.6.0_16 and zlib-1.2.3 on Solaris. And the Inflator uses jdk-1.6.0.21 and zlib-1.2.3 on Linux.\n\nAccording to this Java bug - http://bugs.sun.com/bugdatabase/view_bug.do;jsessionid=e8f7802ea035813254fc6aba9bf0?bug_id=6519463, the bug is fixed by using a combination of zlib1.23 and jdk7-b72\n\nHere is the code snippet from InflatorInputStream.java - \n  157                   if (inf.needsInput()) {\n  158                       fill();\n  159                   }\n\nThis bug occurs when the native Inflator on the platform indicates there are more bytes to decompress, when there aren't any. So, the InflatorInputStream.read() calls fill() based on that, where it throws EOFException(). \n\nThe workaround seems to be catching the EOFException in CompressionUtils.decompress and do nothing.\n", "We have also seem the following gzip issue. Is that the same issue since it's not triggered by EOF?\n\n ERROR [CompressionUtils$] [kafka-processor-0] [kafka] Error while reading from the GZIP input stream\njava.io.IOException: Corrupt GZIP trailer\n        at java.util.zip.GZIPInputStream.readTrailer(GZIPInputStream.java:182)\n        at java.util.zip.GZIPInputStream.read(GZIPInputStream.java:94)\n        at java.io.FilterInputStream.read(FilterInputStream.java:90)\n        at kafka.message.GZIPCompression.read(CompressionUtils.scala:52)\n        at kafka.message.CompressionUtils$$anonfun$decompress$1.apply$mcI$sp(CompressionUtils.scala:143)\n        at kafka.message.CompressionUtils$$anonfun$decompress$1.apply(CompressionUtils.scala:143)\n        at kafka.message.CompressionUtils$$anonfun$decompress$1.apply(CompressionUtils.scala:143)\n        at scala.collection.immutable.Stream$$anonfun$continually$1.apply(Stream.scala:598)\n        at scala.collection.immutable.Stream$$anonfun$continually$1.apply(Stream.scala:598)\n        at scala.collection.immutable.Stream$Cons.tail(Stream.scala:555)\n        at scala.collection.immutable.Stream$Cons.tail(Stream.scala:549)\n        at scala.collection.immutable.Stream$$anonfun$takeWhile$1.apply(Stream.scala:394)\n        at scala.collection.immutable.Stream$$anonfun$takeWhile$1.apply(Stream.scala:394)\n        at scala.collection.immutable.Stream$Cons.tail(Stream.scala:555)\n        at scala.collection.immutable.Stream$Cons.tail(Stream.scala:549)\n        at scala.collection.immutable.Stream.foreach(Stream.scala:255)\n        at kafka.message.CompressionUtils$.decompress(CompressionUtils.scala:143)\n        at kafka.message.ByteBufferMessageSet$$anon$1.makeNextOuter(ByteBufferMessageSet.scala:119)\n        at kafka.message.ByteBufferMessageSet$$anon$1.makeNext(ByteBufferMessageSet.scala:132)\n        at kafka.message.ByteBufferMessageSet$$anon$1.makeNext(ByteBufferMessageSet.scala:81)\n        at kafka.utils.IteratorTemplate.maybeComputeNext(IteratorTemplate.scala:59)\n        at kafka.utils.IteratorTemplate.hasNext(IteratorTemplate.scala:51)\n        at scala.collection.Iterator$class.foreach(Iterator.scala:631)\n        at kafka.utils.IteratorTemplate.foreach(IteratorTemplate.scala:30)\n        at scala.collection.IterableLike$class.foreach(IterableLike.scala:79)\n        at kafka.message.MessageSet.foreach(MessageSet.scala:87)\n        at kafka.log.Log.append(Log.scala:204)\n        at kafka.server.KafkaRequestHandlers.kafka$server$KafkaRequestHandlers$$handleProducerRequest(KafkaRequestHandlers.scala:70)\n        at kafka.server.KafkaRequestHandlers$$anonfun$handleMultiProducerRequest$1.apply(KafkaRequestHandlers.scala:63)\n        at kafka.server.KafkaRequestHandlers$$anonfun$handleMultiProducerRequest$1.apply(KafkaRequestHandlers.scala:63)\n        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)\n        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)\n        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)\n        at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:34)\n        at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)\n        at scala.collection.mutable.ArrayOps.map(ArrayOps.scala:34)\n        at kafka.server.KafkaRequestHandlers.handleMultiProducerRequest(KafkaRequestHandlers.scala:63)\n        at kafka.server.KafkaRequestHandlers$$anonfun$handlerFor$4.apply(KafkaRequestHandlers.scala:42)\n        at kafka.server.KafkaRequestHandlers$$anonfun$handlerFor$4.apply(KafkaRequestHandlers.scala:42)\n        at kafka.network.Processor.handle(SocketServer.scala:297)\n        at kafka.network.Processor.read(SocketServer.scala:320)\n        at kafka.network.Processor.run(SocketServer.scala:215)\n        at java.lang.Thread.run(Thread.java:619)\n", "Changed CompressionUtils.decompress to handle EOFException and return -1 from the read API", "Jun, I am not sure its the same issue. See this - http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4262583\n\nThe Java bugs mention that it happens due to very large compressed or uncompressed data (larger than 2GB). Not sure how Kafka server can get into that situation, since the request size is 100 MB. ", "The patch for EOF looks fine. We probably need to do some system test to make sure this doesn't introduce new problems, especially when the compressed size is relatively large. Once that test is done. We can commit the patch.", "Have run the system test with message size = 100K, batch size = 200 and compression turned on. It passed.", "Is this still happening?", " We have seen this issues recently. Pl reopen if you think the issue still exists \n"], "derived": {"summary": "Occasionally, we see the following errors on the Kafka server -\n\n2012/02/08 14:58:21. 832 ERROR [KafkaRequestHandlers] [kafka-processor-6] [kafka] Error processing MultiProducerRequest on NusImpressionSetEvent:0\njava.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Occassional GZIP errors on the server while writing compressed data to disk - Occasionally, we see the following errors on the Kafka server -\n\n2012/02/08 14:58:21. 832 ERROR [KafkaRequestHandlers] [kafka-processor-6] [kafka] Error processing MultiProducerRequest on NusImpressionSetEvent:0\njava."}, {"q": "What updates or decisions were made in the discussion?", "a": "We have seen this issues recently. Pl reopen if you think the issue still exists"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-274", "title": "Handle corrupted messages cleanly", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "labels": [], "created": "2012-02-16T23:12:10.000+0000", "updated": "2012-05-15T00:58:37.000+0000", "description": "This is related to KAFKA-273 and is filed to improve the code to have more defensive checks against corrupted data. The data could get corrupted either due to a Kafka bug or due to bit flipping on the network. The message iterator should check if the message is valid before trying to decompress it. On the producer side, defensive checks can be turned on to verify the messages before writing those to the socket.", "comments": ["Guarded against message set sizes > Int.MaxValue", "+1 for the patch."], "derived": {"summary": "This is related to KAFKA-273 and is filed to improve the code to have more defensive checks against corrupted data. The data could get corrupted either due to a Kafka bug or due to bit flipping on the network.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Handle corrupted messages cleanly - This is related to KAFKA-273 and is filed to improve the code to have more defensive checks against corrupted data. The data could get corrupted either due to a Kafka bug or due to bit flipping on the network."}, {"q": "What updates or decisions were made in the discussion?", "a": "+1 for the patch."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-275", "title": "max.message.size is not enforced for compressed messages", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": null, "labels": [], "created": "2012-02-17T01:56:39.000+0000", "updated": "2017-08-17T12:02:41.000+0000", "description": "The max.message.size check is not performed for compressed messages, but only for each message that forms a compressed message. Due to this, even if the max.message.size is set to 1MB, the producer can technically send n 1MB messages as one compressed message. This can cause memory issues on the server as well as deserialization issues on the consumer. The consumer's fetch size has to be > max.message.size in order to be able to read data. If one message is larger than the fetch.size, the consumer will throw an exception and cannot proceed until the fetch.size is increased. \n\nDue to this bug, even if the fetch.size > max.message.size, the consumer can still get stuck on a message that is larger than max.message.size.", "comments": ["In corner cases, this can cause KAFKA-273", "This issue is fixed in latest versions.  Please reopen if the issue still exists. \n"], "derived": {"summary": "The max. message.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "max.message.size is not enforced for compressed messages - The max. message."}, {"q": "What updates or decisions were made in the discussion?", "a": "This issue is fixed in latest versions.  Please reopen if the issue still exists."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-276", "title": "Enforce max.message.size on the total message size, not just on payload size", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": null, "labels": [], "created": "2012-02-17T02:01:28.000+0000", "updated": "2017-08-17T11:52:52.000+0000", "description": "Today, the max.message.size config is enforced only on the payload size of the message. But the actual message size is header size + payload size.", "comments": ["This was fixed in newer Kafka versions."], "derived": {"summary": "Today, the max. message.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Enforce max.message.size on the total message size, not just on payload size - Today, the max. message."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was fixed in newer Kafka versions."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-277", "title": "Add a shallow iterator to the ByteBufferMessageSet, which is only used in SynchProducer.verifyMessageSize() function", "status": "Resolved", "priority": "Major", "reporter": "Yang Ye", "assignee": "Yang Ye", "labels": [], "created": "2012-02-17T22:41:54.000+0000", "updated": "2012-02-23T22:59:50.000+0000", "description": "Shallow iterator just traverse the first level messages of a ByteBufferMessageSet, compressed messages won't be decompressed and treated individually ", "comments": ["The lastConnectionTime adjustment is also in this patch.\n\nAlso the following file is also affected. Curious, in the repository copy, the variable \"event\" has no context (an variable not defined), only when I change it to events the make process can succeed. \n\n--- core/src/main/scala/kafka/producer/async/DefaultEventHandler.scala  (revision 1245727)\n+++ core/src/main/scala/kafka/producer/async/DefaultEventHandler.scala  (working copy)\n@@ -38,8 +38,8 @@\n       processedEvents = cbkHandler.beforeSendingData(events)\n\n     if(logger.isTraceEnabled)\n-      processedEvents.foreach(event => trace(\"Handling event for Topic: %s, Partition: %d\"\n-        .format(event.getTopic, event.getPartition)))\n+      processedEvents.foreach(events => trace(\"Handling event for Topic: %s, Partition: %d\"\n+        .format(events.getTopic, events.getPartition)))\n\n     send(serialize(collate(processedEvents), serializer), syncProducer)\n   }\n", "Some comments:\n1. The variable event just binds to every item in the sequence by the foreach method. There is no need to rename it since each item in processedEvents is supposed to be a single event.\n2. In ByteBufferMessageSet, instead of duplicating code in shallowIterator, could we rename deepIterator to internalIterator and add a flag to control whether we want to do shallow iteration or deep iteration? In general, we don't want to expose the shallow iterator externally. So, it's better if we just add a verifyMessageSize method in ByteBufferMessageSet that uses shallow iterator.\n ", "Also, please add a unit test for this. Use a max_message size larger than each individual uncompressed message, but smaller than the compressed message.", "in ByteBufferMessageSet, internal_iterator() is built with one flag to control the deep or shallow behavior. verifyMessageSize() function is moved as an member function. \n\nUnit test is built as a separate function in SynchProducerTest.scala ", "Thanks for the patch. It looks good. Just committed to trunk."], "derived": {"summary": "Shallow iterator just traverse the first level messages of a ByteBufferMessageSet, compressed messages won't be decompressed and treated individually.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add a shallow iterator to the ByteBufferMessageSet, which is only used in SynchProducer.verifyMessageSize() function - Shallow iterator just traverse the first level messages of a ByteBufferMessageSet, compressed messages won't be decompressed and treated individually."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for the patch. It looks good. Just committed to trunk."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-278", "title": "Issues partitioning a new topic", "status": "Resolved", "priority": "Minor", "reporter": "Matt", "assignee": null, "labels": [], "created": "2012-02-17T22:57:47.000+0000", "updated": "2013-07-11T22:43:33.000+0000", "description": "There are two cases where correct partitioning fails for a new topic.\n\nCase 1: Topic exists on current Kafka cluster. A new broker is added to the cluster. The new broker will never host partitions for the existing topic.\n\nTo reproduce:\n1) Create a cluster of brokers along with a ZooKeeper ensemble.\n2) Send messages for a topic to the cluster.\n3) Add a new broker to the cluster.\n4) New broker will never see the existing topic.\n\nCase 2: Topic does not exist on current Kafka cluster. Producer sends messages to a new topic that did not previously exist in the cluster. If, during the producer session, one or more partitions are not created on a broker, the broker will never host those partitions.\n\nTo reproduce:\n1) Create a cluster of brokers along with a ZooKeeper ensemble.\n2) Send messages to a new topic.\n3) Shut down the producer before the topic is created on at least one broker.\n4) The broker that did not allocate the topic will never host the topic.\n\nMy guess(!) here is that when a new producer is created, it gets a list of topics and partitions based on the current state of the brokers in the cluster. Since some brokers are missing the topic, the producer will never send messages to that broker and partitions will never be created.\n\n\nWork around:\nManually create the topic/partition directories in the kafka logs directory and reboot kafka. It will register the topic/partitions in ZooKeeper.", "comments": ["I changed the description and title to reflect that there are 2 ways that this condition can arise.", "Refined the steps to reproduce.", "Added a work around.", "Here's a patch we applied to our fork to deal with this issue.\n\nWhat it does is bootstrap new brokers in the same way existing brokers are bootstrapped for new topics.\n\nIt includes a test.", "Hi, I didn't realize it was expected for multiple brokers to split partitions of a single topic in 0.7? Can anyone point me to where that behavior is documented?", "Basically, in 0.7, a partition is local to a broker. So, if you send data to a broker, a partition for that topic will be automatically created in that broker.", "This should be fixed in 0.8 where we handle partitioning properly."], "derived": {"summary": "There are two cases where correct partitioning fails for a new topic. Case 1: Topic exists on current Kafka cluster.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Issues partitioning a new topic - There are two cases where correct partitioning fails for a new topic. Case 1: Topic exists on current Kafka cluster."}, {"q": "What updates or decisions were made in the discussion?", "a": "This should be fixed in 0.8 where we handle partitioning properly."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-279", "title": "kafka-console-producer does not take in customized values of --batch-size or --timeout", "status": "Resolved", "priority": "Minor", "reporter": "milind parikh", "assignee": "Jun Rao", "labels": [], "created": "2012-02-19T19:04:09.000+0000", "updated": "2012-03-21T00:25:38.000+0000", "description": "1. While the default console-producer, console-consumer paradigm works great, when I try modiying the batch size\n\nbin/kafka-console-producer.sh --batch-size 300   --zookeeper localhost:2181 --topic test1\n\nit gives me a\n\nException in thread \"main\" java.lang.NumberFormatException: null\n    at java.lang.Integer.parseInt(Integer.java:443)\n    at java.lang.Integer.parseInt(Integer.java:514)\n    at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:207)\n    at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)\n    at kafka.utils.Utils$.getIntInRange(Utils.scala:189)\n    at kafka.utils.Utils$.getInt(Utils.scala:174)\n    at kafka.producer.async.AsyncProducerConfigShared$class.$init$(AsyncProducerConfig.scala:45)\n    at kafka.producer.ProducerConfig.<init>(ProducerConfig.scala:25)\n    at kafka.producer.ConsoleProducer$.main(ConsoleProducer.scala:108)\n    at kafka.producer.ConsoleProducer.main(ConsoleProducer.scala)\n\nI have looked at the code and can't figure out what's wrong\n\n2. When I do bin/kafka-console-producer.sh --timeout 30000   --zookeeper localhost:2181 --topic test1\n\nI would think that console-producer would wait for 30s if the batch size (default 200) is not full. It doesn't. It takes the same time without the timeout parameter (default 1000) and dumps whatever the batch size.\n\n\nResolution from Jun\n\n1. The code does the following to set batch size\n     props.put(\"batch.size\", batchSize)\nInstead, it should do\n     props.put(\"batch.size\", batchSize.toString)\n\n2. It sets the wrong property name for timeout. Instead of doing\n   props.put(\"queue.enqueueTimeout.ms\", sendTimeout.toString)\nit should do\n   props.put(\"queue.time\", sendTimeout.toString)\n", "comments": ["Milind, \n\nAttached is a patch to trunk. Could you try it out and see if it fixes your problem?", "I tried the patch and issuing the same command get the following stack trace part-way through an import of a 100000 line file (--batchsize 300):\n[2012-02-27 16:44:37,911] ERROR Event queue is full of unsent messages, could not send event: 7300043|103|60|1329080400|en|1987973|269118099490000000000000103153898086 (kafka.producer.async.AsyncProducer)\nException in thread \"main\" kafka.producer.async.QueueFullException: Event queue is full of unsent messages, could not send event: 7300043|103|60|1329080400|en|1987973|269118099490000000000000103153898086\n        at kafka.producer.async.AsyncProducer.send(AsyncProducer.scala:121)\n        at kafka.producer.ProducerPool$$anonfun$send$1$$anonfun$apply$mcVI$sp$1$$anonfun$apply$2.apply(ProducerPool.scala:131)\n        at kafka.producer.ProducerPool$$anonfun$send$1$$anonfun$apply$mcVI$sp$1$$anonfun$apply$2.apply(ProducerPool.scala:131)\n        at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)\n        at scala.collection.immutable.List.foreach(List.scala:45)\n        at kafka.producer.ProducerPool$$anonfun$send$1$$anonfun$apply$mcVI$sp$1.apply(ProducerPool.scala:131)\n        at kafka.producer.ProducerPool$$anonfun$send$1$$anonfun$apply$mcVI$sp$1.apply(ProducerPool.scala:130)\n        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)\n        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)\n        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:130)\n        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:102)\n        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:102)\n        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)\n        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)\n        at kafka.producer.ProducerPool.send(ProducerPool.scala:102)\n        at kafka.producer.Producer.zkSend(Producer.scala:143)\n        at kafka.producer.Producer.send(Producer.scala:105)\n        at kafka.producer.ConsoleProducer$.main(ConsoleProducer.scala:120)\n        at kafka.producer.ConsoleProducer.main(ConsoleProducer.scala)\n", "That typically means that you are sending data at a rate faster than the broker can persist. Try increasing flush.interval to sth like 10000 on the broker to increase server throughput.", "Milind, does the attached patch fix your problem ?", "I tested this and verified that this fixes the error for --batch-size.  ", "Edward,\n\nThanks for the review. Just committed the patch to trunk."], "derived": {"summary": "1. While the default console-producer, console-consumer paradigm works great, when I try modiying the batch size\n\nbin/kafka-console-producer.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "kafka-console-producer does not take in customized values of --batch-size or --timeout - 1. While the default console-producer, console-consumer paradigm works great, when I try modiying the batch size\n\nbin/kafka-console-producer."}, {"q": "What updates or decisions were made in the discussion?", "a": "Edward,\n\nThanks for the review. Just committed the patch to trunk."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-280", "title": "change on-disk log layout to {log.dir}/topicname/partitionid", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": null, "labels": ["replication"], "created": "2012-02-22T17:54:33.000+0000", "updated": "2015-02-07T23:54:14.000+0000", "description": "Currently, the on-disk layout is {log.dir}/topicname-partitionid. The problem is that there is no appropriate place to store topicname level information such as topic version. An alternative layout is {log.dir}/topicname/partitionid. Then, we can store topic level meta data under {log.dir}/topicname. ", "comments": ["FWIW, I think this new proposal is esthetically nicer too."], "derived": {"summary": "Currently, the on-disk layout is {log. dir}/topicname-partitionid.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "change on-disk log layout to {log.dir}/topicname/partitionid - Currently, the on-disk layout is {log. dir}/topicname-partitionid."}, {"q": "What updates or decisions were made in the discussion?", "a": "FWIW, I think this new proposal is esthetically nicer too."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-281", "title": "support multiple root log directories", "status": "Closed", "priority": "Major", "reporter": "Jun Rao", "assignee": null, "labels": [], "created": "2012-02-22T17:58:18.000+0000", "updated": "2013-01-08T23:28:53.000+0000", "description": "Currently, the log layout is {log.dir}/topicname-partitionid and one can only specify 1 {log.dir}. This limits the # of topics we can have per broker. We can potentially support multiple directories for {log.dir} and just assign topics using hashing or round-robin.", "comments": ["I would recommend not using round-robin as that would lead to having to have some meta-data that keeps track of what directory goes where.  Hashing is easy, but the downside is that it's not trivially discoverable if a person is using a command line shell to browse the directory structure.", "Is this to work around the max subdirectory limits some filesystems have (e.g. I think ext4 has a limit of 64k subdirectories per directory)?\n\nThe other advantage of this is that you can actually get rid of RAID and just run with JBOD using a separate mount point for each drive and having a data directory per drive (a la Hadoop). We wouldn't do this now, but if we had replication this would be a big win. The overhead of RAID is usually like a 20-30% perf hit, plus the additional disk space it takes up. In this setup you would be depending on replication for disk failures. The trade-off is that a single drive failure would kill a machine. In practice due to raid resync perf hit we seem to have this problem already.", "Yes.  There's not just a hard limit - there is a practical limit.  We've found that EXT3 that limit is around 20k.  The limit has to do with some of the low level posix apis and how they are implemented, I saw a post some time ago about how to make this better, but for the time being it's generally inefficient in most filesystems to have large numbers of files/directories in a single directory.\n\nAlso, as you point out, it makes it next to impossible to easily add additional storage since there is only basically one mount point.", "I guess this was done on 0.8 as part of KAFKA-188", "fixed in KAFKA-188"], "derived": {"summary": "Currently, the log layout is {log. dir}/topicname-partitionid and one can only specify 1 {log.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "support multiple root log directories - Currently, the log layout is {log. dir}/topicname-partitionid and one can only specify 1 {log."}, {"q": "What updates or decisions were made in the discussion?", "a": "fixed in KAFKA-188"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-282", "title": "Currently iterated chunk is not cleared during consumer shutdown", "status": "Resolved", "priority": "Major", "reporter": "Joel Jacob Koshy", "assignee": "Joel Jacob Koshy", "labels": [], "created": "2012-02-22T23:30:55.000+0000", "updated": "2015-06-25T07:05:05.000+0000", "description": "During consumer connector shutdown, fetch queues are cleared, but the currently iterated chunk is not cleared.", "comments": ["I still have this issue with the API version 0.8.1.1. During the connector shutdown and after offsets have been committed, the ConsumerIterator give me more messages.\n\nWhat was the resolution to this issue ?"], "derived": {"summary": "During consumer connector shutdown, fetch queues are cleared, but the currently iterated chunk is not cleared.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Currently iterated chunk is not cleared during consumer shutdown - During consumer connector shutdown, fetch queues are cleared, but the currently iterated chunk is not cleared."}, {"q": "What updates or decisions were made in the discussion?", "a": "I still have this issue with the API version 0.8.1.1. During the connector shutdown and after offsets have been committed, the ConsumerIterator give me more messages.\n\nWhat was the resolution to this issue ?"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-283", "title": "add jmx beans in broker to track # of failed requests", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Jun Rao", "labels": [], "created": "2012-02-24T02:26:57.000+0000", "updated": "2012-02-25T23:08:22.000+0000", "description": null, "comments": ["patch attached.", "+1. This is very useful", "Thanks for the review. Just committed this."], "derived": {"summary": "", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "add jmx beans in broker to track # of failed requests"}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for the review. Just committed this."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-284", "title": "C++ client does not compile", "status": "Resolved", "priority": "Trivial", "reporter": "Niek Sanders", "assignee": null, "labels": [], "created": "2012-02-24T06:14:27.000+0000", "updated": "2012-03-01T22:24:05.000+0000", "description": "clients/cpp/src/encoder.hpp has an unclosed comment on line 19.\n\n\n/*\n * encoder.hpp\n *\n\nNeeds to be\n\n/*\n * encoder.hpp\n */\n", "comments": ["Niek do you want to upload a patch for this?", "I noticed another thing also\n\nautoconf.sh\n\ndoes not have execute on it", "Index: clients/cpp/src/encoder.hpp\n===================================================================\n--- clients/cpp/src/encoder.hpp\t(revision 1295383)\n+++ clients/cpp/src/encoder.hpp\t(working copy)\n@@ -16,7 +16,7 @@\n */\n /*\n  * encoder.hpp\n- *\n+ */\n \n #ifndef KAFKA_ENCODER_HPP_\n #define KAFKA_ENCODER_HPP_", "+1\n\nuploaded the patch as file for convenience", "committed"], "derived": {"summary": "clients/cpp/src/encoder. hpp has an unclosed comment on line 19.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "C++ client does not compile - clients/cpp/src/encoder. hpp has an unclosed comment on line 19."}, {"q": "What updates or decisions were made in the discussion?", "a": "+1\n\nuploaded the patch as file for convenience"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-285", "title": "Increase maximum value of log.retention.size", "status": "Closed", "priority": "Major", "reporter": "Elben Shira", "assignee": null, "labels": ["patch"], "created": "2012-02-24T16:18:12.000+0000", "updated": "2012-09-04T17:25:52.000+0000", "description": "The log.retention.size property was retrieved as an Int, which means a maximum of Int.MaxValue (2 GB). This patch gets the property as a long.", "comments": ["This is a clean patch on branches/0.7. The other one (KAFKA-285.patch) applies cleanly on trunk.", "Applies cleanly to trunk.", "Thanks for the patch, Elben. Just committed the patch to trunk."], "derived": {"summary": "The log. retention.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Increase maximum value of log.retention.size - The log. retention."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for the patch, Elben. Just committed the patch to trunk."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-286", "title": "consumer sometimes don't release partition ownership properly in ZK during rebalance", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Jun Rao", "labels": [], "created": "2012-02-25T22:59:25.000+0000", "updated": "2012-02-27T19:52:35.000+0000", "description": null, "comments": ["Patch attached.", "Wondering if the patch can get the consumer into the following state -\n\nSay, there are 2 consumers in a group, c1 and c2. Both are consuming topic1 with partitions 0-0, 0-1 and 1-0. Say c1 owns 0-0 and 0-1 and c2 owns 1-0. \n\n1. Broker 1 goes down. This triggers rebalancing attempt in c1 and c2. \n2. c1 releases partition ownership, but fails to rebalance. \n3. Meanwhile, c2 completes rebalancing successfully, and owns partition 0-1 and starts consuming data. \n4. c1 starts next rebalancing attempt and it releases partition 0-1 (since 0-1 is still part of topicRegistry). It owns partition 0-0 again, and starts consuming data. \n5. Effectively, rebalancing has completed successfully, but there is no owner for partition 0-1 registered in Zookeeper. \n\nI think using the topicRegistry cache is dangerous, since it has to be in sync with the ownership information in zookeeper. How about reading the ownership information from ZK along with the other data and only release that ?\n", "That's a good point. What can happen is that we may delete ZK paths that c1 didn't successfully own in step 2, if rebalance fails. Added patch v2 that deletes all temporarily owned ZK paths in reflectPartitionOwnershipDecision, if we can't own everything. I think this fix addresses this issue.", "That's a good change and will handle the majority of failure cases. There is another failure case, I think still needs to be fixed in the rebalancing -\n\nSay, for the above mentioned scenario, c1 fails to rebalance due to some error/exception that exercises this code path -\n\n          try {\n            done = rebalance(cluster)\n          }\n          catch {\n            case e =>\n              /** occasionally, we may hit a ZK exception because the ZK state is changing while we are iterating.\n               * For example, a ZK node can disappear between the time we get all children and the time we try to get\n               * the value of a child. Just let this go since another rebalance will be triggered.\n               **/\n              info(\"exception during rebalance \", e)\n          }\n\nAfter this, c1 only closes its fetcher queues and backs off (0-0 and 0-1 are already released), while c2 owns 0-1.\nThen during step 4 above, c1 releases things from its topic registry again which contains 0-0 and 0-1. So it releases 0-1, which it does not own anymore\n\n", "Will this happen? It doesn't seem possible to me. In step 2, when we release 0-0 and 0-1 during rebalance, we clear topicRegistry. Since this rebalance fails, topicRegistry will not be populated. So, in step 4, there is nothing to release for c1.", "Yes, missed the fact that after releasing the partitions, it also gets deleted from the passed in topic registry. Looks good now, we will have to be careful about keeping this topic registry cache in sync at all times though. I like the idea of refreshing the cache from ZK during each rebalance attempt, but we can look into that later. \n\n+1 on v2.", "Thanks for the review. Just committed this to trunk."], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "consumer sometimes don't release partition ownership properly in ZK during rebalance"}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for the review. Just committed this to trunk."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-287", "title": "Running a consumer client using scala 2.8 fails", "status": "Resolved", "priority": "Major", "reporter": "Elben Shira", "assignee": null, "labels": [], "created": "2012-02-27T21:24:03.000+0000", "updated": "2015-02-07T23:52:30.000+0000", "description": "Built the kafka library using the instructions found in the README. My client uses scala 2.9.1, sbt 0.11. My consumer client has this snippet of code: https://gist.github.com/a35006cc25e39ba386e2\n\nThe client compiles, but running it produces this stacktrace: https://gist.github.com/efeb85f50402b477d6e0\n\nI think this may be because of a bug found in scala 2.9.0 (though I'm not sure if it was present in scala 2.8.0): https://issues.scala-lang.org/browse/SI-4575\n\nTo get around this, I built the kafka library using scala 2.9.1 (by changing build.properties).", "comments": ["I run Kafka in production using Scala 2.9.1 changing build.properties and creating a new build\n\nI run sbt 11.2 for all of my code but use the existing sbt in the Kafka project for building kafka.\n\nKAFKA-134 tries to address an upgrade to 0.10.1 which I tried once but ran into an issue.  we should look at KAFKA-134 and making it be 11.2 (should not be much more than what was already in 0.10.1 changes)\n\ndo you want to take a look into KAFKA-134 ? I can review when you are done..   I think this ticket though is covered in KAFKA-134 changes.\n\nthe workaround is exactly what you did, change build.properties to 2.9.1 and rebuild.  works great just some warnings otherwise have no issue in production"], "derived": {"summary": "Built the kafka library using the instructions found in the README. My client uses scala 2.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Running a consumer client using scala 2.8 fails - Built the kafka library using the instructions found in the README. My client uses scala 2."}, {"q": "What updates or decisions were made in the discussion?", "a": "I run Kafka in production using Scala 2.9.1 changing build.properties and creating a new build\n\nI run sbt 11.2 for all of my code but use the existing sbt in the Kafka project for building kafka.\n\nKAFKA-134 tries to address an upgrade to 0.10.1 which I tried once but ran into an issue.  we should look at KAFKA-134 and making it be 11.2 (should not be much more than what was already in 0.10.1 changes)\n\ndo you want to take a look into KAFKA-134 ? I can review when you are done..   I think this ticket though is covered in KAFKA-134 changes.\n\nthe workaround is exactly what you did, change build.properties to 2.9.1 and rebuild.  works great just some warnings otherwise have no issue in production"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-288", "title": "java impacted changes from new producer and consumer request format", "status": "Resolved", "priority": "Major", "reporter": "Joe Stein", "assignee": null, "labels": ["replication", "wireprotocol"], "created": "2012-02-27T23:53:05.000+0000", "updated": "2016-10-07T16:51:57.000+0000", "description": "1) javaapi.SyncProducer: We should get rid of send(topic: String, messages: ByteBufferMessageSet) and only keep send(producerRequest: kafka.javaapi.ProducerRequest). \n\nThis affects KafkaRecordWriter and DataGenerator\n\n2)  javaapi.ProducerRequest: We will need to define a java version of TopicData so that java producers can create request conveniently. The java version of TopicData will use the java version of ByteBufferMessageSet. ", "comments": [], "derived": {"summary": "1) javaapi. SyncProducer: We should get rid of send(topic: String, messages: ByteBufferMessageSet) and only keep send(producerRequest: kafka.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "java impacted changes from new producer and consumer request format - 1) javaapi. SyncProducer: We should get rid of send(topic: String, messages: ByteBufferMessageSet) and only keep send(producerRequest: kafka."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-289", "title": "reuse topicdata when sending producerrequest", "status": "Resolved", "priority": "Major", "reporter": "Joe Stein", "assignee": null, "labels": ["optimization", "replication", "wireprotocol"], "created": "2012-02-27T23:55:00.000+0000", "updated": "2016-10-07T16:45:46.000+0000", "description": "The way that SyncProducer sends a ProducerRequest over socket is to first serialize the whole request in a bytebuffer and then sends the bytebuffer through socket. An alternative is to send the request like FetchReponse, using a ProduceRequestSend that reuses TopicDataSend. This avoids code duplication and is more efficient since it sends data in ByteBufferMessagesSet directly to socket and avoids extra copying from messageset to bytebuffer. ", "comments": [], "derived": {"summary": "The way that SyncProducer sends a ProducerRequest over socket is to first serialize the whole request in a bytebuffer and then sends the bytebuffer through socket. An alternative is to send the request like FetchReponse, using a ProduceRequestSend that reuses TopicDataSend.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "reuse topicdata when sending producerrequest - The way that SyncProducer sends a ProducerRequest over socket is to first serialize the whole request in a bytebuffer and then sends the bytebuffer through socket. An alternative is to send the request like FetchReponse, using a ProduceRequestSend that reuses TopicDataSend."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-290", "title": "use propertyExists to test if both broker.list and zk.connect are present", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Jun Rao", "labels": [], "created": "2012-02-29T01:03:22.000+0000", "updated": "2012-03-01T19:38:55.000+0000", "description": null, "comments": ["patch attached.", "There is another place where it will be worthwhile to use propertyExists instead of a non-null check -\n\n  if(brokerList != null && Utils.getString(props, \"partitioner.class\", null) != null)\n    throw new InvalidConfigException(\"partitioner.class cannot be used when broker.list is set\")\n", "Attach patch v2.", "But, I think the check for both properties absent, should also be there, right ?", "Attach patch v3 that requires either broker.lost or zk.connect to be present. Have to patch some unit tests that don't specify either broker.list or zk.connect in ProducerConfig.", "+1 on v3. ", "Fix exists in trunk"], "derived": {"summary": "", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "use propertyExists to test if both broker.list and zk.connect are present"}, {"q": "What updates or decisions were made in the discussion?", "a": "Fix exists in trunk"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-291", "title": "Add builder to create configs for consumer and broker", "status": "Resolved", "priority": "Major", "reporter": "John Wang", "assignee": null, "labels": [], "created": "2012-03-02T19:42:00.000+0000", "updated": "2015-02-07T23:52:21.000+0000", "description": "Creating Consumer and Producer can be cumbersome because you have to remember the exact string for the property to be set. And since these are just strings, IDEs cannot really help.\n\nThis patch contains builders that help with this.", "comments": ["examples (in java):\n\nProducer:\n\ninstead of:\n\n  Properties props = new Properties();\n  props.put(\"zk.connect\", \"localhost:2181\");\n  props.put(\"serializer.class\", \"kafka.serializer.DefaultEncoder\");\n\n ProducerConfig producerConfig = new ProducerConfig(props);\nwe can do:\n   \n  ProducerConfig producerConfig = new ProducerConfigBuilder().setZkConnect(\"localhost:2181\").setSerializerClass(\"kafka.serializer.DefaultEncoder\").build();\n\n\nConsumer:\n\ninstead of:\n\nProperties props = new Properties();\nprops.put(\"zk.connect\", _zookeeperUrl);\nprops.put(\"consumer.timeout.ms\", _kafkaSoTimeout);\nprops.put(\"groupid\", _consumerGroupId);\n\nConsumerConfig consumerConfig = new ConsumerConfig(props);\n\nwe can do:\n\nConsumerConfig consumerConfig = new ConsumerConfigBuilder().setGroupId(_consumerGroupId).setZkConnect(_zookeeperUrl).setGroupId(_consumerGroupId).build();", "Not sure if the ConfigBuilder is going to reduce complexity. The strings like \"localhost:2181\" will generally not be hardcoded in java code and will be passed from some config map, this passing of config values will have the same caveats. "], "derived": {"summary": "Creating Consumer and Producer can be cumbersome because you have to remember the exact string for the property to be set. And since these are just strings, IDEs cannot really help.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Add builder to create configs for consumer and broker - Creating Consumer and Producer can be cumbersome because you have to remember the exact string for the property to be set. And since these are just strings, IDEs cannot really help."}, {"q": "What updates or decisions were made in the discussion?", "a": "Not sure if the ConfigBuilder is going to reduce complexity. The strings like \"localhost:2181\" will generally not be hardcoded in java code and will be passed from some config map, this passing of config values will have the same caveats."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-292", "title": "broker deletes all file segments when cleaning up an empty log segment", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Jun Rao", "labels": [], "created": "2012-03-03T01:46:32.000+0000", "updated": "2012-03-18T18:35:44.000+0000", "description": "Suppose that a log has only one log segment left and it's empty. If that segment expires and is deleted, we roll out a new segment of the same name. However, the deletion happens after the log is rolled. This will make the log directory empty, which should never happen.", "comments": ["patch attached.", "It will be very useful to have a unit test for this. Will make it easier to catch this in the future.", "Attach patch v2. There is already a unit test for this. Just need to add the necessary checking.", "+1 on v2. Good catch !", "Committed to trunk."], "derived": {"summary": "Suppose that a log has only one log segment left and it's empty. If that segment expires and is deleted, we roll out a new segment of the same name.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "broker deletes all file segments when cleaning up an empty log segment - Suppose that a log has only one log segment left and it's empty. If that segment expires and is deleted, we roll out a new segment of the same name."}, {"q": "What updates or decisions were made in the discussion?", "a": "Committed to trunk."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-293", "title": "Allow to configure all broker ids at once similar to how zookeeper handles server ids", "status": "Resolved", "priority": "Major", "reporter": "Thomas Dudziak", "assignee": null, "labels": [], "created": "2012-03-05T21:58:38.000+0000", "updated": "2015-02-07T23:52:03.000+0000", "description": "Zookeeper allows to specify all server ids in the same configuration (https://zookeeper.apache.org/doc/r3.3.3/zookeeperAdmin.html#sc_configuration) which has the benefit that the configuration file is the same for all zookeeper instances. A similar approach for Kafka would be quite useful, e.g.\n\nbrokerid.1=<host 1>\nbrokerid.2=<host 2>\n\netc.\nIt'd still require per-instance configuration (myid file in the zookeeper case) but that can be created separately (e.g. by the deployment tool used).", "comments": ["The current approach is, admittedly, hacky; but the zookeeper approach is equally hacky if you ask me. I don't think it is good to put any host information in your server config.\n\nIf we want to fix this issue I think the right way to go would be to create a zk sequence of ids. Servers would store the broker id in their data directory (maybe we could add a ${data.dir}/meta.properties file to hold this kind of stuff). If a server starts and has no meta.properties file then it would increment the zk id counter and take a new id and store it persistently with its logs. On startup the broker would take its id from this file.\n\nThe reason this approach is better is because the node.id is not really configuration. That is if you swap the node.ids for two brokers you effectively corrupt the logs in the eyes of consumers. So the node.id is pinned to a particular set of logs. This would make any manual borking of your node id impossible and takes the node id out of the things the user has to configure.\n\nThoughts?", "I like the meta.properties approach, better than Zookeeper's approach to its configuration. It is certainly more involved, but has some advantages, like Jay mentioned. Just one thing to be careful about if we use the ephemeral sequential node feature in ZK -\n\nA kafka broker asks zookeeper to create an ephemeral sequential node, and the create() succeeds on the server but the server crashes before returning the name of the node to kafka. When the kafka broker reconnects, its session is still valid and, thus, the node is not removed. The implication is that it is difficult for the Kafka broker to know if its node was created or not. \n\nOne way of handling recoverable errors while using sequential ephemeral nodes is that we can simply assume that the node wasn't created and create another one. This means that the ids will not be contiguous. ", "I agree that the zookeeper approach is hacky, but at least in the zookeeper case they don't have an (obvious) choice because zookeeper is the source of truth (though admittedly they could use DNS or other means to discover the other servers on startup).\nFor Kafka, I like your idea because it basically means I don't have to care about the id except if I want to move the data to a different physical server (and even then I only need to know that I have to move the meta.properties file as well).\nThe only problem is that it relies on the zookeeper sequence maintaining integrity (i.e. it can be restarted as that can lead to duplicate ids). Maybe a better way might be if the kafka nodes generate uuids and use them as node names in zookeeper. If the path already exist, then they could simply create a new uuid and try again."], "derived": {"summary": "Zookeeper allows to specify all server ids in the same configuration (https://zookeeper. apache.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Allow to configure all broker ids at once similar to how zookeeper handles server ids - Zookeeper allows to specify all server ids in the same configuration (https://zookeeper. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "I agree that the zookeeper approach is hacky, but at least in the zookeeper case they don't have an (obvious) choice because zookeeper is the source of truth (though admittedly they could use DNS or other means to discover the other servers on startup).\nFor Kafka, I like your idea because it basically means I don't have to care about the id except if I want to move the data to a different physical server (and even then I only need to know that I have to move the meta.properties file as well).\nThe only problem is that it relies on the zookeeper sequence maintaining integrity (i.e. it can be restarted as that can lead to duplicate ids). Maybe a better way might be if the kafka nodes generate uuids and use them as node names in zookeeper. If the path already exist, then they could simply create a new uuid and try again."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-294", "title": "\"Path length must be > 0\" error during startup", "status": "Resolved", "priority": "Major", "reporter": "Thomas Dudziak", "assignee": null, "labels": [], "created": "2012-03-06T03:47:58.000+0000", "updated": "2024-03-14T12:04:46.000+0000", "description": "When starting Kafka 0.7.0 using zkclient-0.1.jar, I get this error:\n\nINFO 2012-03-06 02:39:04,072  main kafka.server.KafkaZooKeeper Registering broker /brokers/ids/1\nFATAL 2012-03-06 02:39:04,111  main kafka.server.KafkaServer Fatal error during startup.\njava.lang.IllegalArgumentException: Path length must be > 0\n        at org.apache.zookeeper.common.PathUtils.validatePath(PathUtils.java:48)\n        at org.apache.zookeeper.common.PathUtils.validatePath(PathUtils.java:35)\n        at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:620)\n        at org.I0Itec.zkclient.ZkConnection.create(ZkConnection.java:87)\n        at org.I0Itec.zkclient.ZkClient$1.call(ZkClient.java:308)\n        at org.I0Itec.zkclient.ZkClient$1.call(ZkClient.java:304)\n        at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:675)\n        at org.I0Itec.zkclient.ZkClient.create(ZkClient.java:304)\n        at org.I0Itec.zkclient.ZkClient.createPersistent(ZkClient.java:213)\n        at org.I0Itec.zkclient.ZkClient.createPersistent(ZkClient.java:223)\n        at org.I0Itec.zkclient.ZkClient.createPersistent(ZkClient.java:223)\n        at kafka.utils.ZkUtils$.createParentPath(ZkUtils.scala:48)\n        at kafka.utils.ZkUtils$.createEphemeralPath(ZkUtils.scala:60)\n        at kafka.utils.ZkUtils$.createEphemeralPathExpectConflict(ZkUtils.scala:72)\n        at kafka.server.KafkaZooKeeper.registerBrokerInZk(KafkaZooKeeper.scala:57)\n        at kafka.log.LogManager.startup(LogManager.scala:124)\n        at kafka.server.KafkaServer.startup(KafkaServer.scala:80)\n        at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:47)\n        at kafka.Kafka$.main(Kafka.scala:60)\n        at kafka.Kafka.main(Kafka.scala)\n\nThe problem seems to be this code in ZkClient's createPersistent method:\n\nString parentDir = path.substring(0, path.lastIndexOf('/'));\ncreatePersistent(parentDir, createParents);\ncreatePersistent(path, createParents);\n\nwhich doesn't check for whether parentDir is an empty string, which it will become for /brokers/ids/1 after two recursions.\n", "comments": ["Are you using namespace in ZK connection string? If so, the typical problem is that the namespace is not present. You have to manually create the namespace in ZK.", "Ah I see, this is very non-descriptive error then. Maybe you could add this to the documentation/FAQ (or make the error more descriptive) ?", "I think we can fix this so it gives a more intuitive error message that explains the problem. No one will be able to figure this out otherwise.", "This issue happens also in 0.8.0\nIt would be even better, if the chroot is not present in zk, that it be automatically created, thus avoiding this issue altogether.", "We can auto-create it, but then we can't prevent config mistakes. We can probably start by just providing a more meaningful error. One way is to just catch IllegalArgumentException with that message and covert it to a more meaningful exception (and message). Jason, you want to give this a shot?", "With KAFKA-404 committed, this is resolved too.", "Github user fsaintjacques closed the pull request at:\n\n    https://github.com/apache/kafka/pull/2\n"], "derived": {"summary": "When starting Kafka 0. 7.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "\"Path length must be > 0\" error during startup - When starting Kafka 0. 7."}, {"q": "What updates or decisions were made in the discussion?", "a": "Github user fsaintjacques closed the pull request at:\n\n    https://github.com/apache/kafka/pull/2"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-295", "title": "Bug in async producer DefaultEventHandler retry logic", "status": "Resolved", "priority": "Major", "reporter": "Prashanth Menon", "assignee": "Prashanth Menon", "labels": [], "created": "2012-03-06T18:00:58.000+0000", "updated": "2012-04-02T01:50:45.000+0000", "description": "In the DefaultEventHandler's retry loop, the logic should not return after a successful retry.  Rather, it should set a boolean flag indicating that the retry was successful and exit or break the while loop.  In the end it should throw an exception only the flag is false.  Otherwise, it should continue the outer for loop and send remaining data to remaning brokers.", "comments": ["This is a small but important bug.  I can drop a patch since I've been touching that code anyways.  It will probably come in after KAFKA-49.", "Incorporated as part of KAFKA-300 and KAFKA-305."], "derived": {"summary": "In the DefaultEventHandler's retry loop, the logic should not return after a successful retry. Rather, it should set a boolean flag indicating that the retry was successful and exit or break the while loop.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Bug in async producer DefaultEventHandler retry logic - In the DefaultEventHandler's retry loop, the logic should not return after a successful retry. Rather, it should set a boolean flag indicating that the retry was successful and exit or break the while loop."}, {"q": "What updates or decisions were made in the discussion?", "a": "Incorporated as part of KAFKA-300 and KAFKA-305."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-296", "title": "Update Go Client to new version of Go", "status": "Resolved", "priority": "Major", "reporter": "AaronR", "assignee": null, "labels": [], "created": "2012-03-08T21:35:05.000+0000", "updated": "2013-04-03T16:25:10.000+0000", "description": "go  (http://golang.org) is close to releasing a new release of go (1.0) which requires updates to the client, in the meantime most of the go community has moved to this version.  \n\nThis change contains:\n\n* language changes to existing client (os.Error, time. Signals, etc)\n* removes the Makefile's (no longer used by go)\n* It also runs \"go fmt\"  (formats it in standard go) which are most of the lines changes, from spaces to tabs.\n* updates the import path to allow for \"go get\" installs (don't need to get source and build)   \n\nNot sure which versions this should apply to, but i think it should go to 0.7 and newer.", "comments": ["Patches attached in separate file.", "good updates, however the formatting wouldn't be so big if you stuck with the current switches..\n \n(from the old Makefile and still work..)\ngofmt -w -tabwidth=2 -tabindent=false \n\ni'd rather use spaces and not tabs..", "Ill submit a new patch that removes the formatting changes. ", "updated simpler patch without fomatting changes.  \n\n", "I can't seem to apply the patch. Could you rebase?\npatching file clients/go/tools/publisher/publisher.go\nHunk #1 FAILED at 22.\nHunk #2 succeeded at 87 with fuzz 2 (offset 38 lines).\n1 out of 2 hunks FAILED -- saving rejects to file clients/go/tools/publisher/publisher.go.rej\npatching file clients/go/tools/publisher/publisher.go\nHunk #1 FAILED at 23.\nHunk #2 FAILED at 41.\n2 out of 2 hunks FAILED -- saving rejects to file clients/go/tools/publisher/publisher.go.rej\n", "Sorry about that, i tried to make a svn patch unsuccessfully, but this is a .git patch, i saw a few other ones so hopefully that is ok.  \n\n", "Thanks for the patch. Just committed to trunk.", "Actually, I have to revert the commit since the patch didn't apply cleanly.\n\n!       clients/go/tools/offsets/Makefile\nM       clients/go/tools/offsets/offsets.go\nM       clients/go/tools/consumer/consumer.go\n!       clients/go/tools/consumer/Makefile\nM       clients/go/tools/publisher/publisher.go\n!       clients/go/tools/publisher/Makefile\n!       clients/go/kafka_test.go\n", "Also, could you add some basic unit tests for the go client?", "this patch applies cleanly to svn & moves some of the files to fit better with latest go style.  \nI temporarily removed the remote import so that it would compile the standalone apps (appears to be a bit of chicken & egg problem).", "that version looks and works for me.  Not sure why you were getting errors on the standalone apps using the remote import, as that was working for me.  The remote install can be a future change.   If we can get this closed then I can resubmit #297, #298, and a #314 ( MultiProduce ) .  \n\nAlso, we probably should update the docs to include notes about having to modify GOPATH etc since it is not go get installable.  I can add that in #297.  ", "thanks, yeah i did add comments to the README about setting GOPATH.\n\nJun, can you verify & commit please.", "I thought we were moving clients out of the main project?", "this is a very old ticket that was never completed."], "derived": {"summary": "go  (http://golang. org) is close to releasing a new release of go (1.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Update Go Client to new version of Go - go  (http://golang. org) is close to releasing a new release of go (1."}, {"q": "What updates or decisions were made in the discussion?", "a": "this is a very old ticket that was never completed."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-297", "title": "Go Client Publisher Improvments", "status": "Closed", "priority": "Major", "reporter": "AaronR", "assignee": null, "labels": [], "created": "2012-03-08T21:43:26.000+0000", "updated": "2013-04-04T20:45:37.000+0000", "description": "The current go client creates a connection to the broker, sends single message, disconnects.  It can also only send single messages, not message sets.\n\nImprove to:\n\n* have single connection that stays open\n* send message sets\n", "comments": ["updated patch reflecting changes to 296-v2.patch\n\nI think an additional change (that can only be made on commit as far as i know) that would be nice is rename go/src/publisher.go > producer.go  (and /tools/publisher etc).  to match other project terminology.  ", "updating the patch to a better git one, as I don't think the svn i made will apply cleanly.  ", "Old bug. Client libraries, such as this Go library, are no longer being maintained in the main project.\n\nPlease close. Won't Fix."], "derived": {"summary": "The current go client creates a connection to the broker, sends single message, disconnects. It can also only send single messages, not message sets.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Go Client Publisher Improvments - The current go client creates a connection to the broker, sends single message, disconnects. It can also only send single messages, not message sets."}, {"q": "What updates or decisions were made in the discussion?", "a": "Old bug. Client libraries, such as this Go library, are no longer being maintained in the main project.\n\nPlease close. Won't Fix."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-298", "title": "Go Client support max message size", "status": "Closed", "priority": "Major", "reporter": "AaronR", "assignee": null, "labels": [], "created": "2012-03-08T22:26:27.000+0000", "updated": "2013-04-04T20:44:35.000+0000", "description": "The go client has a bug when max message size is reached on broker-consumer request, such that resulting bytes sent to consumer contain a truncated/partial message as last message.   Go client was not recognizing and handling this partial message.\n\n", "comments": ["depends on previous 2 go patches.   296, 297", "whoops, ignore wrong previous file.\n\nthis patch depends on 296, 297", "updating based on changes to 297, 296", "updating patch to git patch, not sure the previous v2 svn one was created correctly.", "Old bug. Client libraries, such as this Go library, are no longer being maintained in the main project.\n\nPlease close. Won't Fix."], "derived": {"summary": "The go client has a bug when max message size is reached on broker-consumer request, such that resulting bytes sent to consumer contain a truncated/partial message as last message. Go client was not recognizing and handling this partial message.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Go Client support max message size - The go client has a bug when max message size is reached on broker-consumer request, such that resulting bytes sent to consumer contain a truncated/partial message as last message. Go client was not recognizing and handling this partial message."}, {"q": "What updates or decisions were made in the discussion?", "a": "Old bug. Client libraries, such as this Go library, are no longer being maintained in the main project.\n\nPlease close. Won't Fix."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-299", "title": "Change broker request and response to use Seqs rather than Array", "status": "Resolved", "priority": "Major", "reporter": "Prashanth Menon", "assignee": "Prashanth Menon", "labels": ["0.8", "replication", "wireprotocol"], "created": "2012-03-09T02:56:19.000+0000", "updated": "2015-02-07T23:51:34.000+0000", "description": "The new Produce and Fetch request and response classes use primitive Arrays, but becaue they are case classes and Java's array hashCode/equals functionality is broken, the case class equality contract is broken as well.  We should change the models to use Seqs to resolve the issue along with gaining all the functional benefits that goes along with it.  This change will require appropriate Java versions to convert between Array's and Seqs for Java clients.", "comments": ["Assigning to you Prashanth, since you mentioned you want to resolve this"], "derived": {"summary": "The new Produce and Fetch request and response classes use primitive Arrays, but becaue they are case classes and Java's array hashCode/equals functionality is broken, the case class equality contract is broken as well. We should change the models to use Seqs to resolve the issue along with gaining all the functional benefits that goes along with it.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Change broker request and response to use Seqs rather than Array - The new Produce and Fetch request and response classes use primitive Arrays, but becaue they are case classes and Java's array hashCode/equals functionality is broken, the case class equality contract is broken as well. We should change the models to use Seqs to resolve the issue along with gaining all the functional benefits that goes along with it."}, {"q": "What updates or decisions were made in the discussion?", "a": "Assigning to you Prashanth, since you mentioned you want to resolve this"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-300", "title": "Implement leader election", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "labels": [], "created": "2012-03-13T00:14:20.000+0000", "updated": "2013-05-02T02:29:50.000+0000", "description": "According to the Kafka replication design (https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Replication), this JIRA will involve implementing the leaderElection() procedure. ", "comments": ["This patch includes the following changes -\n\n1. Implement leader election procedure using only assigned replicas list. Basically, any replica in AR list can become a leader with preference to the preferred replica (1st replica in AR list)\n\n2. Introduced the Partition and Replica data structures with CUR, ISR, RAR unused. Also, commitQ will be added as part of KAFKA-46\n\n3. Each broker registers listeners on /brokers/topics and /brokers/topics/[topic/partitions. Also for each partition assigned to the broker, it registers leader change listener on /brokers/topics/[topic]/partitions/[partition]/leader\n\n4. Currently the interaction between KafkaZookeeper and LogManager is unwieldly, but it will be fixed once KAFKA-307 is resolved.\n\n5. Added LeaderElectionTest and helper code to wait until leader is elected for a particular topic and partition ", "Thanks for the patch. Some comments:\n1. KafkaZookeeper:\n1.1 LeaderChangeListener.handleDataDeleted: no need to first check that leader exists. Since leader could change immediately after the check is done and subsequent logic still needs to handle this case.\n1.2 TopicChangeListener: curChilds returned from the call back gives all children, not just new children.\n1.3 SessionExpireListener: We could missed some new topics created when a session expires. Just calling subscribeToTopicAndPartitionsChanges may not be enough since we may need to call handleNewTopics for those missed new topics. \n1.4 leaderElection probably needs to return a boolean to indicate if the election succeeded or not.\n\n2. LogManager:\n2.1 It seems to me that replicas is not directly tied to LogManager and probably should be kept somewhere else, like KafkaServer. \n2.2 It aslo seems that we need to keep a map of (topic, partition_id, broker_id) => Replica so that we can address each replica more efficiently. This can be either a 1 level or a 2 level map.\n\n3. DefaultEventHandler.handle: We probably should break out of the loop when outstandingProduceRequests is empty.\n\n4. ZkUtils.waitUntilLeaderIsElected should be moved to test (sth like ZKTestUtil).\n\n5. KafkaServer,BrokerPartitionInfo,DefaulEventHandler,LazyProducerTest,LeaderElectionTest: removed unused imports\n\n\n\n", "1.1 Removed the check. We probably don't need that optimization\n1.2 If you read the patch closely, you'll see that it treats it like all children. Maybe the name newTopics for that variable is confusing. So changed that.\n1.3 That's right. Changed that.\n1.4 We'll see when we need this.\n\n2.1 That's right. This is related to the refactor and will be included in that patch.\n2.2 I'll see if that is useful during the refactor patch.\n\n3. Changed that.\n\n4. Yeah, had intended that, but forgot. \n", "+1 on patch v2.", "Thanks for the review. Just committed this. "], "derived": {"summary": "According to the Kafka replication design (https://cwiki. apache.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Implement leader election - According to the Kafka replication design (https://cwiki. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for the review. Just committed this."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-301", "title": "Implement the broker startup procedure", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "labels": ["replication"], "created": "2012-03-13T00:17:18.000+0000", "updated": "2013-05-02T02:29:50.000+0000", "description": "This JIRA will involve implementing the list of actions to be taken on broker startup, as listed by the brokerStartup() and startReplica() algorithm in the Kafka replication design doc. Since the stateChangeListener is part of KAFKA-44, this JIRA can leave it as a stub.", "comments": ["Since broker startup needs to use leader election ", "This patch adds the state change listeners, al though they will be exercised only when KAFKA-302 is in.\n\n1. Introduced ZKQueue to communicate state change requests from the leader to the followers\n\n2. State change requests are persistent, since some state changes need to be communicated even when the follower is not up (e.g delete topic)\n\n3. Introduced leader epochs. The follower can skip certain state change requests if the request epoch is less than the current epoch for that partition\n\n4. Added tests for the ZKQueue, and left TODOs for testing StateChangeRequestHandler. These tests can be added as part of KAFKA-302", "Uh oh, it looks like this patch implements functionality that was part of KAFKA-44?  Do we still need 44 then?", "Yeah, after finishing up on KAFKA-300, I took up this one and while working on it, the line between KAFKA-44 and this one started blurring. Though the real piece of work would be KAFKA-302. Sorry, didn't mean to step on someone else's toes. Just want to clear the path to let my work on KAFKA-46 progress. Having said that, do you want to pick up KAFKA-302 or even pick up this one ? I actually don't mind anything as long as we have KAFKA-45 resolved by next week.", "Haha, no problem Neha.  I'll let you wrap up KAFKA-301 and I can get started on KAFKA-302.  If all goes well, I can get a patch in by Wednesday and after reviews and adjustments, we should be able to commit by Friday.\n\nWe should also probably go ahead and remove KAFKA-44?", "Sounds good. Probably letting the JIRA and related discussions/patches be, might be a good idea. This JIRA is marked to incorporate KAFKA-44.", "A suggesstion I'd like to throw out there:  Can we move the core replica logic (leaderElection, becomeLeader and becomeFollower) to where replicas are actually managed, mainly ReplicaManager?  I think this will keep things cleaner and concise.  If we do this, we should extract kafka-specific ZK logic out of ZkUtils into KafkaZooKeeper such as functions to read the ISR, AR and leader replica or register leader change listeners.  With this, ReplicaManager becomes the central place to manage replicas; it uses KafkaZooKeeper to interact with ZK in a kafka-specific way and uses LogManager to create local replicas.  KafkaApis, StateRequestHandler and all the topic/leader/partition reassignment listeners use ReplicaManager to manage replicas in a general fashion.\n\nThoughts?", "The refactoring in KAFKA-307 attempted to reduce such interdependencies amongst the various server components. Your suggestion of keeping all logic related to managing replicas in ReplicaManager makes a lot of sense. However, it seems like a good idea to keep all ZK specific logic bundled inside KafkaZookeeper. Ideally, LogManager, ReplicaManager and KafkaZookeeper should really not know about each other. Only KafkaServer \"manages\" these sub-components. But, in reality, it could be slightly difficult to keep these sub components completely independent of each other.", "Canceling patch since the epoch generation can be done without sequential nodes in ZK", "1. Introduced ZKQueue to communicate state change requests from the leader to the followers. However, the logic for shrinking a full queue is left for another patch, when we actually have create/delete topic or add/remove partition that enqueues requests into the queue. \n\n2. State change requests are persistent, since some state changes need to be communicated even when the follower is not up (e.g delete topic) \n\n3. Introduced leader epochs. The follower can skip certain state change requests if the request epoch is less than the current epoch for that partition. \n\n4. Added tests for the ZKQueue, and left TODOs for testing StateChangeRequestHandler. These tests can be added as part of create/delete topic support\n", "Can't seem to apply patch v1 to 0.8 branch.\n\npatching file core/src/test/scala/unit/kafka/producer/AsyncProducerTest.scala\npatching file core/src/test/scala/unit/kafka/server/LeaderElectionTest.scala\npatching file core/src/test/scala/unit/kafka/server/StateChangeTest.scala\npatching file core/src/main/scala/kafka/producer/Producer.scala\npatching file core/src/main/scala/kafka/producer/async/QueueFullException.scala\npatching file core/src/main/scala/kafka/admin/AdminUtils.scala\npatching file core/src/main/scala/kafka/common/NoEpochForPartitionException.scala\ncan't find file to patch at input line 374\nPerhaps you used the wrong -p or --strip option?\nThe text leading up to this was:\n--------------------------\n|Index: core/src/main/scala/kafka/common/QueueFullException.scala\n|===================================================================\n|--- core/src/main/scala/kafka/common/QueueFullException.scala\t(revision 1304473)\n|+++ core/src/main/scala/kafka/common/QueueFullException.scala\t(working copy)\n--------------------------\nFile to patch: \n", "I moved QueueFullException from kafka.producer.async to kafka.common. I think that somehow didn't get reflected in the v1 patch. \n\nUploading a patch that applies cleanly on 0.8", "I think one thing that can be improved is the structure of our ZK metadata. This is not limited just to this patch. I think for znodes that store more than one value could benefit using a simple json format instead of combinations of \",\" and \":\". This is true for the state change nodes as well as the ISR path.\n\nAl though, since this is just a format change and we can still review v2 since the main functionality added is the state change listener, epoch and the ZKQueue.", "Thanks for the patch. Some comments:\n1. some new files without Apache header.\n\n2. ZKQueue: Reading items one at a time out of the queue seems expensive since we have to read all children, sort them and get the head. The drainAll and readAll api are much more efficient. Do we really need apis like poll and peek that deal with one item at a time?\n\n3. KafkaZooKeeper:\n3.1 Can StateChangeRequestHandler be created inside StateChangeListener?\n3.2 StateChangeListener.ensureEpochValidity(). Not all state changes are initiated by a leader. For example, create topic ddl directly publishes state change requests,\n\n4. LeaderElectionTest: unused imports\n\n5. No need to change VerifyConsumerRebalance.\n\n6. Yes, it would be better if we use json representing the value of each state change.\n", "Here are some superficial comments:\n1. This adds more kafka-specific logic into ZkUtils as static methods. I think Prasanth commented on this. Maybe given the current state, it is better to ignore this for now, but I feel this is a bad approach as ZkUtils is fundamentally unmockable, right? A better way would be to make fully object-oriented classes that represent the domain logic we have around partitions, clusters, epochs, etc. This is harder to do because you have to think through this model but ultimately easier to work with and test, I feel. It definitely makes sense to have utility methods like readData() but application specific methods like incrementEpochForPartition() are less good.\n2. \"ZKQueue\" should be \"ZkQueue\" for consistency I think. \n3. StateChangeRequest is not really a request in the sense of Request.scala, right? We are already unfortunately overloading Request twice, once for API objects and once for the RequestQueue items. Could we call these Commands?\n4. I like the ZKQueue a lot, I think that approach to wrapping up zk logic is very good. One thing though is that the usage of this queue is a bit difficult because as I understand the poll/drain methods are non-blocking, so one has to write a zk watcher to trigger the action. I wonder if a better way wouldn't be to wrap up the zk watcher logic as well so that the queue acted more like a juc BlockingQueue. In this case the usage would be more like while(true) { val cmd = q.take(); handle(cmd);}. In other words, I should be able to start a kafka broker I have instantiated with a fake ZkQueue and pump in cmds to test it out without ZK. I am not sure if this is really possible to do or if it would actually be better, just throwing it out there...", "Jun's comments -\n1, 4, 5, 6. Fixed it\n\n2. Changed the ZkQueue to be sort of a blocking queue as per Jay's suggestion. It is not a complete blocking queue since the put operation does not block. The leader needs to know that the queue is full immediately to be able to initiate some kind of queue shrink operation. The dequeue operation is blocking.\n\n3.1 Removed the StateChangeListener since now, the ZkQueue wraps its own listener\n3.2 Good point. Fixed it.\n\nJay's comments -\n1. You have a valid point here. I agree that we should be able to refactor the code to wrap up custom ZK logic in Kafka classes. Right now, the leader election stuff it stuck in there and hence the epoch increment API too. But since we might end up changing the leader election algorithm itself, I would suggest waiting a bit before attempting this refactoring.\n\n2. Changed ZkQueue and StateChangeCommand for consistency.\n\n3. I like this suggestion, gave it a shot. Let me know what you think.\n", "Anybody up for reviewing patch v3 ?", "v3 looks good to me. A couple of minor comments:\n31. ZkQueue: Is latestQueueItemPriority used to prevent sequential nodes being added out of order in ZK? If so, should we log an error if this does happen?\n32. StateChangeCommandHandler.run(): local vals topic, partition, and epoch are not used.\n\n", "Thanks for the review Jun !\n\n3.1. This check protects against a subtle issue. The take() API removes a node only from the internal queue, not from ZK. An item is removed from the ZkQueue only through the remove() API. This is to handle \"act-and-then-delete\" behavior for a state change. Now if another item is added to a ZkQueue after the previous item was taken, but before it was removed from ZK, there is a risk of adding that previous item back into the internal queue. latestQueueItemPriority tracks the last item that was taken from the queue, so only items with priority larger than that can be added to the queue.\n\n3.2 Will remove that on the checkin.", "Thanks for reviewing the patches ! Just committed this."], "derived": {"summary": "This JIRA will involve implementing the list of actions to be taken on broker startup, as listed by the brokerStartup() and startReplica() algorithm in the Kafka replication design doc. Since the stateChangeListener is part of KAFKA-44, this JIRA can leave it as a stub.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Implement the broker startup procedure - This JIRA will involve implementing the list of actions to be taken on broker startup, as listed by the brokerStartup() and startReplica() algorithm in the Kafka replication design doc. Since the stateChangeListener is part of KAFKA-44, this JIRA can leave it as a stub."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for reviewing the patches ! Just committed this."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-302", "title": "Implement the become leader and become follower state change operations", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "labels": ["replication"], "created": "2012-03-13T00:20:39.000+0000", "updated": "2012-06-10T20:41:32.000+0000", "description": "This JIRA will involve implementing the becomeLeader() and becomeFollower() APIs, as described in the Kafka replication design proposal. This can still be done independently of KAFKA-44", "comments": ["The work here would require KAFKA-301 to be completed. ", "This JIRA implements the becomeLeader() and becomeFollower() APIs as listed here - https://cwiki.apache.org/confluence/display/KAFKA/Kafka+replication+detailed+design+V2#KafkareplicationdetaileddesignV2-Becomeleader.\nSince commit thread and replica fetcher thread is part of KAFKA-46, please leave empty stubs for those, in the patch for this JIRA.\n", "Hi folks.  Now that KAFKA-301 has been committed, I'll begin taking a look at this.  As usually, time is tight, but I'm hoping to get something in this weekend.", "Prashanth, \n\nTo get the work for KAFKA-46 going, I took a stab at the becomeLeader/becomeFollower() logic. I would appreciate if you could review it. If you already have a patch, we can reverse the order, and review your patch, get it checked in and then do the rest of the review for KAFKA-46. We can try not to waste any of the work you might've already put in, for this JIRA.\n\n", "So, I have to apologize on my tardiness; I had completed most of the work, but didn't get a chance to clean it up before submitting for review.  What I'll do is tidy it up, and try to cherry-pick and psuedo-merge it with your implementation.  Expect something this weekend.", "Oh boy, taking a look at the KAFKA-46 patch, it's pretty massive.  At this point, it might be better/easier to review the section of KAFKA-46 that covers this ticket.  I'll comment there.", "If you don't have a patch ready for submission, let me attempt to pull out changes related to this JIRA from KAFKA-46. It will hopefully make it easier to review it in pieces.", "Fixed as part of KAFKA-46"], "derived": {"summary": "This JIRA will involve implementing the becomeLeader() and becomeFollower() APIs, as described in the Kafka replication design proposal. This can still be done independently of KAFKA-44.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Implement the become leader and become follower state change operations - This JIRA will involve implementing the becomeLeader() and becomeFollower() APIs, as described in the Kafka replication design proposal. This can still be done independently of KAFKA-44."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed as part of KAFKA-46"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-303", "title": "Dead code in the Log4j appender", "status": "Resolved", "priority": "Trivial", "reporter": "Jose Quinteiro", "assignee": null, "labels": ["log4j"], "created": "2012-03-14T17:55:39.000+0000", "updated": "2012-03-21T00:38:34.000+0000", "description": "The log4j appender has the following class definition in it:\n\nclass DefaultStringEncoder extends Encoder[LoggingEvent] {\n override def toMessage(event: LoggingEvent):Message = new Message(event.getMessage.asInstanceOf[String].getBytes)\n}\n\nThis left over and no longer used. The current version uses kafka.serializer.StringEncoder as the default encoder.", "comments": ["Index: core/src/main/scala/kafka/producer/KafkaLog4jAppender.scala\n===================================================================\n--- core/src/main/scala/kafka/producer/KafkaLog4jAppender.scala\t(revision 1300634)\n+++ core/src/main/scala/kafka/producer/KafkaLog4jAppender.scala\t(working copy)\n@@ -93,8 +93,4 @@\n   }\n \n   override def requiresLayout: Boolean = false\n-}\n-\n-class DefaultStringEncoder extends Encoder[LoggingEvent] {\n-  override def toMessage(event: LoggingEvent):Message = new Message(event.getMessage.asInstanceOf[String].getBytes)\n-}\n+}\n", "Jose,\n\nThanks for the patch. Just committed to trunk."], "derived": {"summary": "The log4j appender has the following class definition in it:\n\nclass DefaultStringEncoder extends Encoder[LoggingEvent] {\n override def toMessage(event: LoggingEvent):Message = new Message(event. getMessage.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Dead code in the Log4j appender - The log4j appender has the following class definition in it:\n\nclass DefaultStringEncoder extends Encoder[LoggingEvent] {\n override def toMessage(event: LoggingEvent):Message = new Message(event. getMessage."}, {"q": "What updates or decisions were made in the discussion?", "a": "Jose,\n\nThanks for the patch. Just committed to trunk."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-304", "title": "Simple Consumer message set behavior broken on 0.8 branch", "status": "Resolved", "priority": "Critical", "reporter": "Neha Narkhede", "assignee": "Prashanth Menon", "labels": [], "created": "2012-03-14T18:38:06.000+0000", "updated": "2012-03-15T16:10:25.000+0000", "description": "The testZKSendWithDeadBroker() test commented out in ProducerTest actually exposes a bug with path KAFKA-240. It seems that even when the broker has some messages for a topic partition, the simple consumer doesn't return those.\n\nI've simplified this test and attached a diff to this bug. Apply it on 0.8 branch to reproduce this. ", "comments": ["The test creates a topic new-topic with partitions on one broker. The producer sends 2 messages to one partition on that broker. The file message set on that broker returns 2 messages, but simple consumer pulling from the same broker returns only 1 message.", "Hi Neha,\n\nI took a glancing look at the test and I think there might be a mistake.  The line that reads: \"assertEquals(message, messageSet.next.message)\" should actually be \"assertEquals(message, messageAndOffset.message)\".  The former progresses the iterator a second time after the preceeding line so two messages are consumed within one iteration of the loop, wherare the latter looks at one message within the current iteration alone.  I'll take a closer look when I get a chance.\n\n\n", "Modified the test with the change and it seems to be passing.  Can you double-check, Neha?  The change is on line 215.", "Prashanth, that looks like a bug in the test itself."], "derived": {"summary": "The testZKSendWithDeadBroker() test commented out in ProducerTest actually exposes a bug with path KAFKA-240. It seems that even when the broker has some messages for a topic partition, the simple consumer doesn't return those.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Simple Consumer message set behavior broken on 0.8 branch - The testZKSendWithDeadBroker() test commented out in ProducerTest actually exposes a bug with path KAFKA-240. It seems that even when the broker has some messages for a topic partition, the simple consumer doesn't return those."}, {"q": "What updates or decisions were made in the discussion?", "a": "Prashanth, that looks like a bug in the test itself."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-305", "title": "SyncProducer does not correctly timeout", "status": "Resolved", "priority": "Critical", "reporter": "Prashanth Menon", "assignee": "Prashanth Menon", "labels": [], "created": "2012-03-15T13:11:14.000+0000", "updated": "2012-04-02T01:48:37.000+0000", "description": "So it turns out that using the channel in SyncProducer like we are to perform blocking reads will not trigger socket timeouts (though we set it) and will block forever which is bad.  This bug identifies the issue: http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4614802 and this article presents a potential work-around: http://stackoverflow.com/questions/2866557/timeout-for-socketchannel for workaround. The work-around is a simple solution that involves creating a separate ReadableByteChannel instance for timeout-enabled reads.", "comments": ["Produce ACK should not be blocking. It probably makes sense to fix this before closing KAFKA-49.", "I, unfortunately, didn't get a chance to work on this over the weekend.  From my point of view, creating a new ReadableByteChannel that wraps the socket channel InputStream seems like the simplest solution.  Then the SyncProudcer will have a writeChannel (the SocketChannel) and a readChannel (the wrapped version).  All writes and reads go through the respective channels with the additional of timeout functionality.  \n\nAnother step we can take it is to move all that logic into some class BlockingChannel which can be reused on the consumer side in SimpleConsumer.  Such a class would have, perhaps, four methods: connect, disconnect, send and receive.  Connect and disconnect would be synchronized, send would take a Request object and receive would return a Tuple2[Receive, Int] like usual.  Send and receive will need to be synchronized externally, meaning the class can be effectively treated like a regular Channel otherwise ...\n\nThoughts?", "Hi all, I've attached a patch.  Some notes:\n\n- New class called BlockingChannel that has timeouts enabled.\n- SyncProducer uses BlockingChannel instead of creating its own SocketChannel\n- Re-introducsed testZKSendWithDeadBroker which passes now.\n\nI'd like to get feedback on this.  It's simple and may be reused on the consumer side.  When I think about it, it would be nice to combine SimpleConsumer and SyncProducer into one generic \"SimpleClient\" since the functionality is effectively the same.\n\nI'd also like to benchmark this against a pure NIO implementation where we can use selectors to enabled timeout functionality.  It'll be more complex and will require minor adjustment to BoundedByteBuffer and BoundedByteBufferSend but it may be worth it.", "Prashanth,\n\nThanks for the patch. This is very useful. Some comments:\n\n1. I think it makes sense for SimpleConsumer to use BlockingChannel as well. Could you change that in this patch too?\n2. ProducerTest.testZKSendWithDeadBroker: This test doesn't really test the timeout on getting a response. We probably need to create a mock kafkaserver (that don't send a response) to test this out.\n3. BlockingChannel: \n3.1 We probably should rename timeoutMs to readTimeoutMs since only reads are subject to the timeout.\n3.2 We should pass in a socketSendBufferSize and a socketReceiveBufferSize.\n3.3 Should host and port be part of the constructor? It seems to me it's cleaner if each instance of BlockingChannel is tied to 1 host and 1 port.\n\nI'd also be interested in your findings on the comparison with NIO with selectors.\n", "Thanks for review, Jun.\n\n1. Will do.\n2. So that test actually exposed the issue to begin with - the initial send would fail and then hang forever when attempting to refresh the topic metadata.  Regardless, I'll create a separate more direct test for timeouts.  On my local machine, this test seems to be unpredictable around 30% of the time.  In these cases, it seems like the ephemeral broker nodes aren't removed from ZK and bringing back up a broker after shutdown throws a \"Broker already exists\" exception.  Is anyone else experiencing it or just me?  Increasing the wait time after shutdown helps but not 100%.\n3. 1,2,3 Sounds fair.\n\nI should be able to get a patch in for this by Friday.  Then continue on KAFKA-49 over the weekend and get it in on Saturday or Sunday should the review go okay.  Apologies for the delays :(", "Prashant,\n\n2. If you want to make sure that a broker is shut down, you need to call kafkaServer.awaitShutdown after calling kafkaServer.shutdown. Overall, I don't quite understand how the new test works. It only brought down 1 broker and yet the comment says all brokers are down. If it is indeed that all brokers are down, any RPC call to the broker should get a broken pipe or socket closed exception immediately, not a sockettimeout exception. So, to really test that the timeout works, we need to keep the broker alive and somehow delay the response from the server. This can probably be done with a mock request handler.", "Prashanth,\n\nThanks for the patch. A couple of suggestions -\n\n1. Since you are adding a new abstraction, BlockingChannel, would it make sense to change SimpleConsumer to use it ? Its your call if you'd rather fix it in another JIRA.\n2. In BlockingChannel, since you are synchronizing on a lock, any reason the connected boolean be a volatile ? Also, you can avoid resetting the read and write channels to null values in disconnect.\n3. Lets add some more tests for this, since it is unclear if the workaround of wrapping input stream in a channel actually works or not. I like Jun's suggestion of mocking out the request handler to achieve this. Tests would include SyncProducer as well as async producer (DefaultEventHandler)", "I've uploaded a new patch with the suggestions, but it's not ready for commit, just another review.  A few notes:\n\n1. BlockingChannel modified to meet suggestions.\n2. SimpleConsumer uses BlockingChannel.\n3. To test the BlockingChannel (in SyncProducer and async producer), I bring up a regular server but shutdown the requesthandler.  So the socket remains open, accepts requests and queues them in the request channel, but there are no handlers processing requests.\n4. The original testZKSendWithDeadBroker wasn't commented entirely correctly.  I've modified to actually test what the name suggests.\n5. Though I wait for the broker to do down, testZKSendWithDeadBroker still unpredictably throws the \"Broker already registered\" exception.  Are you experiencing this locally?\n\nI think there might be an issue with the BrokerPartitionInfo and ProduerPool classes.  ProducerPool never removes producers even if one is connected to a downed broker, so calls to getAnyProducer (used by BrokerPartitioninfo.updateInfo to update cached topic metadata information) could return the same \"bad\" producer on consecutive calls when attempting to refresh the cache.  This could potentially cause an entire send to fail though there may exist a broker that is able to service the topic metadata request.  We need to somehow, remove \"bad\" producers, or refresh the ProducerPool when brokers go down, or have BrokerPartitionInfo retry its updateInfo call a certain number of times.  Thoughts?", "Prashanth,\n\nv2 patch looks good. \n\nAs for 5, I do see transient failures of testZKSendWithDeadBroker. This is a bit weird. During broker shutdown, we close the ZK client, which should cause all ephemeral nodes to be deleted in ZK. Could you verify if this is indeed the behavior of ZK?\n\nAs for BrokerPartitionInfo and ProducerPool, we should clean up dead brokers. Could you open a separate jira to track that?", "v2 looks good. \n\nRegarding the test failure, I debugged it and see a probable bug with either Zookeeper or ZkClient. See below - \n\n[info] Test Starting: testZKSendWithDeadBroker(kafka.producer.ProducerTest)\nShutting down broker 0\n[2012-03-23 11:50:36,870] DEBUG Deleting ephemeral node /brokers/ids/0 for session 0x13640e55f240013 (org.apache.zookeeper.server.DataTree:831)\n[2012-03-23 11:50:36,873] DEBUG Deleting ephemeral node /brokers/topics/new-topic/partitions/3/leader for session 0x13640e55f240013 (org.apache.zookeeper.server.DataTree:831)\n[2012-03-23 11:50:36,873] DEBUG Deleting ephemeral node /brokers/topics/new-topic/partitions/1/leader for session 0x13640e55f240013 (org.apache.zookeeper.server.DataTree:831)\n[2012-03-23 11:50:36,873] DEBUG Deleting ephemeral node /brokers/topics/new-topic/partitions/2/leader for session 0x13640e55f240013 (org.apache.zookeeper.server.DataTree:831)\n[2012-03-23 11:50:36,873] DEBUG Deleting ephemeral node /brokers/topics/new-topic/partitions/0/leader for session 0x13640e55f240013 (org.apache.zookeeper.server.DataTree:831)\nShut down broker 0\nRestarting broker 0\n[2012-03-23 11:50:45,194] DEBUG Deleting ephemeral node /brokers/ids/1 for session 0x13640e55f24001b (org.apache.zookeeper.server.DataTree:831)\n[error] Test Failed: testZKSendWithDeadBroker(kafka.producer.ProducerTest)\njava.lang.RuntimeException: A broker is already registered on the path /brokers/ids/0. This probably indicates that you either have configured a brokerid that is already in use, or else you have shutdown this broker and restarted it faster than the zookeeper timeout so it appears to be re-registering.\n\tat kafka.utils.ZkUtils$.registerBrokerInZk(ZkUtils.scala:109)\n\tat kafka.server.KafkaZooKeeper.kafka$server$KafkaZooKeeper$$registerBrokerInZk(KafkaZooKeeper.scala:60)\n\tat kafka.server.KafkaZooKeeper.startup(KafkaZooKeeper.scala:52)\n\tat kafka.server.KafkaServer.startup(KafkaServer.scala:84)\n\tat kafka.producer.ProducerTest.testZKSendWithDeadBroker(ProducerTest.scala:173)\n\nNotice that after shutting down broker 0, the ephemeral node was deleted from its in memory data tree. That happens part of the close session workflow. Still, when we try to create the ephemeral node again, it complains that it already exists. \n\nI'll come back to this zookeeper bug later. I'd say lets checkin this test since it helps reproduce this zk bug. \n\nI think your patch looks good. ", "Thanks for the input everyone.  Regarding the ZK failure, that is effectively the trace I'm seeing on my end as well - the log makes it clear that the ephemeral nodes get deleted but the test still fails when creating them afterwards.  \n\nI would like to delay commiting this patch, atleast for the weekend, as I'd like to perform a little benchmark against a pure NIO implementation.  The benefits there would be having timeouts for both read and write operations and a potential performance boost.", "If this is indeed a ZK issue, we can probably check/wait that the ephemeral node is gone before restarting the broker.", "I've attached another non-blocking implementation that uses selectors, but I'm not seeing any significant performance boost on my machine.  I tested it on the producer side using the ProducerPerformance class by varying the number of messages, the message sizes and the number of threads.  Each test scenario was run four times and the average result was used.  Find the results here: https://gist.github.com/2202142.  \n\nFor what it's worth, I think we should go ahead with the simple solution attached in the v2 path - if everyone is okay with it, please commit.  Regarding the test error, it could potentially be a valid ZK or ZKClient bug.  I can investigate a little by digging into ZKClient and asking around the mailing list and channels.  Keeping the test in breaks the test unpredictably.  Thought I'm not entirely okay with it keeping the bug in, waiting for the node to go down doesn't seem to be the right solution either.  ", "Prashanth,\n\nThanks for the patch. I agree that v2 is less risky than the selector approach. So, we can revisit the selector approach later. Thanks for the patch though and it will probably be useful in the future. Committed v2 patch to 0.8 branch with the following minor changes in DefaultEventHandler:\n* log all unsent messages\n* maintain outstandingRequests properly on both successful and unsuccessful sends.\n\nCould you file 2 jiras, one for taking out dead brokers in ProducerPool and another for transient failures due to ZK ephemeral node not deleted in time?", "I found the zookeeper related problem, filed KAFKA-320 and also included a patch.", "Awesome!", "KAFKA-300 and KAFKA-305 ticket together resolve KAFKA-295."], "derived": {"summary": "So it turns out that using the channel in SyncProducer like we are to perform blocking reads will not trigger socket timeouts (though we set it) and will block forever which is bad. This bug identifies the issue: http://bugs.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "SyncProducer does not correctly timeout - So it turns out that using the channel in SyncProducer like we are to perform blocking reads will not trigger socket timeouts (though we set it) and will block forever which is bad. This bug identifies the issue: http://bugs."}, {"q": "What updates or decisions were made in the discussion?", "a": "KAFKA-300 and KAFKA-305 ticket together resolve KAFKA-295."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-306", "title": "broker failure system test broken on replication branch", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "John Fung", "labels": ["replication"], "created": "2012-03-15T16:33:36.000+0000", "updated": "2012-07-10T18:06:45.000+0000", "description": "The system test in system_test/broker_failure is broken on the replication branch. This test is a pretty useful failure injection test that exercises the consumer rebalancing feature, various replication features like leader election. It will be good to have this test fixed as well as run on every checkin to the replication branch", "comments": ["KAFKA-45 marks the start of server side replication related code changes. I think this test is a pretty good sanity check, if not a complete system testing suite. I would prefer having this fixed before accepting more patches on 0.8 branch. ", "Broker Failure Test is broken in Kafka 0.8 branch. This patch is fixing the issues and contains the following changes:\n1. All server_*.properties are updated such that the first brokerid is starting from '0'\n2. All mirror_producer*.properties are updated to use zk.connect (and not broker.list)\n3. After the source brokers cluster is started, call kafka.admin.CreateTopicCommand to create topic.\n\nCurrently this patch is working with branch 0.8 (rev. 1342841 patched with KAFKA-46) with the following workarounds:\n\n1. Before starting the target brokers cluster, start and stop one target broker to eliminate the following error:\n       org.I0Itec.zkclient.exception.ZkNoNodeException: \n       org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/ids\n\n2. The argument \"--consumer-timeout-ms\" doesn't seem to work properly. The consumer processes will be terminated manually\n\n3. Consumer Lag info is not available from Zookeeper. Therefore, extra sleep time is added to the test to wait for the complete consumption of messages\n\nThe above issues are being investigated.", "Sorry, just got to review this. Trunk has moved, could you rebase? \n\nFor 1, do you know the cause of this? Is this a bug? If so, please create a jira.\n\nFor 2,3, we just merged some changes from trunk to 0.8. Could you retry and see if this works now?\n", "Uploaded kafka-306-v2.patch for branch 0.8 with the following changes:\n\n1. Removed the worked around code and comments for NoNodeException (which is not reproducible with the latest 0.8 code).\n2. The script can take a command line argument to bounce any combination of source broker, target broker and mirror maker in a round-robin fashion.\n3. Use \"info\", \"kill_child_processes\" methods from a common script \"system_test/common/util.sh\".\n4. Updated README.", "Thanks for patch v2. Some comments:\n\n21. run_test.sh\n21.1 Does the following check in start_test() need to be repeated for source, mirror and target, or can we just use 1 check for all 3 cases?\n            if [[ $num_iterations -ge $iter && $svr_idx -gt 0 ]]; then\n                echo\n                info \"==========================================\"\n                info \"Iteration $iter of ${num_iterations}\"\n                info \"==========================================\"\n21.2 Do we need to sleep for 30s at the end start_test()? Isn't calling wait_for_zero_consumer_lags enough? Also, the comment says sleep for 10s.\n21.3 In the header, we should add that mirror make can be terminated too.\n21.4 If the test fails, could we generate a list of missing messages in a file? Ideally, messages can just be strings with sequential numbers in them.\n\n22. The following test seems to fail sometimes.\nbin/run-test.sh 2 23\n\n23. README: We should add that one needs to do ./sbt package at the root level first.\n\n\n", "Uploaded kafka-306-v3.patch with the following changes:\n\n1. Set the server_source*.properties - log file size to approx 10MB:\nlog.file.size=10000000\n\n2. Set the server_target*.properties - log file size to approx 10MB:\nlog.file.size=10000000", "Hi Jun,\n\nThanks for reviewing kafka-306-v2.patch.\n\nkafka-306-v4.patch is uploaded with the following changes suggested by you:\n\n21.1 The following check is required for each of the source, target and mirror maker. It is because the following 2 lines are needed for:\n    Line 1: find out if the $bounce_source_id is a char in the string $svr_to_bounce\n    Line 2: check to see if $num_iterations is already reached and if $svr_idx > 0 (meaning this server needs to be bounced)\n\n    Line 1:        svr_idx=`expr index $svr_to_bounce $bounce_source_id`\n    Line 2:        if [[ $num_iterations -ge $iter && $svr_idx -gt 0 ]]; then\n\n21.2 ConsumerOffsetChecker needs to be enhanced for 0.8 and it depends on KAFKA-313. \"sleep\" is temporarily used for kafka to catch up with the offset lags.\n\n21.3 The header is now updated to \"#### Starting Kafka Broker / Mirror Maker Failure Test ####\"\n\n21.4 There is a file \"checksum.log\" generated at the end of the test which will give the checksums found in producer, source consumer, target consumer logs\n\n22. You may see inconsistent failure in this test due to the issue specified in KAFKA-370\n\n23. README is updated with the steps for ./sbt package\n\nThanks,\nJohn\n", "Thanks for patch v4. A few more comments:\n\n21.2 KAFA-313 adds 2 more options, which option does this jira depends on?\n\n21.3 I meant that we should add mirror maker in the following line in the header:\n# 5. One of the Kafka SOURCE or TARGET brokers in the cluster will\n#    be randomly terminated and waiting for the consumer to catch up.\n\n21.4 Instead of using checksum, can we use the message string itself? This makes it a bit easier to figure out the missing messages, if any.\n\n22. Just attached a patch to kafka-370. Could you give it a try?\n", "** In replying to Jun's question about KAFKA-313: in this script, the function \"wait_for_zero_consumer_lag\" is calling ConsumerOffsetChecker to get the Consumer lag value. However, the topic-partition info is changed in 0.8 and it's not returned correctly in ConsumerOffsetChecker. Please refer to this comment: https://issues.apache.org/jira/browse/KAFKA-313?focusedCommentId=13397990&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13397990\n\n** Uploaded kafka-306-v5.patch. Changes made in kafka-306-v5.patch:\n\n1. In \"initialize\" function, added code to find the location of the zk & kafka log4j log files.\n\n2. In \"cleanup\" function, added code to remove the zk & kafka log4j log files\n\n3. The header of the script is now removed and the description are in README\n\n4. Use getopt to process command line arguments\n\n5. Consolidated the following functions:\n\n    * start_console_consumer_for_source_producer\n    * start_console_consumer_for_mirror_producer\n    * wait_for_zero_source_console_consumer_lags\n    * wait_for_zero_mirror_console_consumer_lags\n\n6. The file to notify producer to stop:\nThe producer is sent to the background to run in a while-loop. If a file is used to notify the producer process in the background, the producer will exit properly inside the while loop.\n\n7. The following check is required for each of the source, target and mirror maker. It is because the following 2 lines are needed for:\n\n    * Line 1: find out if the $bounce_source_id is a char in the string $svr_to_bounce\n    * Line 2: check to see if $num_iterations is already reached and if $svr_idx > 0 (meaning this server needs to be bounced)\n\n    * Line 1: svr_idx=`expr index $svr_to_bounce $bounce_source_id`\n    * Line 2: if [[ $num_iterations -ge $iter && $svr_idx -gt 0 ]]; then", "Uploaded kafka-306-v6.patch and made further changes in ProducerPerformance and ConsoleConsumer to support producing sequential message IDs such that it would be easier to troubleshoot data loss.\n\nProducerPerformance.scala\n\n    * Added command line option \"--seq-id-starting-from\". This option enable \"seqIdMode\" with the following changes:\n\n    * Every message will be tagged with a sequential message ID such that all IDs are unique\n    * Every message will be sent by its own producer thread sequentially\n    * Each producer thread will use a unique range of numbers to give sequential message IDs\n    * All message IDs are leftpadded with 0s for easier troubleshooting\n    * Extra characters are added to the message to make up the required message size\n\nConsoleConsumer.scala\n\n    * Added DecodedMessageFormatter class to display message contents\n\nrun-test.sh\n\n    * Modified to use the enhanced ProducerPerformance and ConsoleConsumer\n    * Validate \"MessageID\" instead of \"checksum\" for data matching between source and target consumers\n\n", "John, thanks for the patch. The test script itself looks good - as we\ndiscussed on the other jira we can do further cleanup separately. Here are\nsome comments on the new changes:\n\nProducerPerformance:\n- seqIdStartFromopt -> startId or initialId would be more\n  convenient/intuitive.\n- May be better not to describe the message format in detail in the help\n  message. I think the template: \"Message:000..1:xxx...\" is good enough.\n- On line 136, 137 I think you mean if (options.has) and not\n  if(!options.has) - something odd there. Can you double-check?\n- Try to avoid using vars if possible. vals are generally clearer and safer\n  - for example,\n  val isFixSize = options.has(seqIdStartFromOpt) || !options.has(varyMessageSizeOpt)\n  val numThreads = if (options.has(seqIdStartFromOpt)) 1 else options.valueOf(numThreadsOpt).intValue()\n  etc.\n- For user-specified options that you override can you log a warning?\n- Instead of the complicated padding logic I think you can get it for free\n  with Java format strings - i.e., specify the width/justification of each\n  column in the format string. That would be much easier I think.\n- numThreads override to 1 -> did it work to prefix the id with thread-id\n  and allow > 1 thread?\n\nServer property files:\n- send/receive.buffer.size don't seem to be valid config options - may be\n  deprecated by the socket buffer size settings, but not sure.\n\nUtil functions:\n\n- Small suggestion: would be better to echo the result than return. So you\n  can have: idx=$(get_random_range ...) which is clearer than\n  get_random_range; idx=$? . Also, non-zero bash returns typically indicate\n  an error.\n", "Hi Joel,\n\nThanks for reviewing. I just uploaded kafka-306-v7.patch with the changes you suggested:\n\nProducerPerformance\n===================\n* seqIdStartFromopt -> startId or initialId would be more convenient/intuitive.\n- Changed\n\n* May be better not to describe the message format in detail in the help message. I think the template: \"Message:000..1:xxx...\" is good enough.\n- Changed\n\n* On line 136, 137 I think you mean if (options.has) and not if(!options.has) - something odd there. Can you double-check?\n- In \"seqIdMode\", if \"numThreadsOpt\" is not specified, numThreads default to 1. Otherwise, it will take the user specified value\n\n* Try to avoid using vars if possible. vals are generally clearer and safer, for example,\n  val isFixSize = options.has(seqIdStartFromOpt) || !options.has(varyMessageSizeOpt)\n  val numThreads = if (options.has(seqIdStartFromOpt)) 1 else options.valueOf(numThreadsOpt).intValue()\n- This is because the values may be overridden later by user specified values. Therefore, some of the val is changed to var\n\n* For user-specified options that you override can you log a warning?\n- Changed\n\n* Instead of the complicated padding logic I think you can get it for free with Java format strings - i.e., specify the width/justification of each column in the format string. That would be much easier I think.\n- Changed\n\n* numThreads override to 1 -> did it work to prefix the id with thread-id and allow > 1 thread?\n- numThreads will be overridden if \"--threads\" is specified in command line arg\n\n\nServer property files\n=====================\n* send/receive.buffer.size don't seem to be valid config options - may be deprecated by the socket buffer size settings, but not sure.\n- Changed\n\nUtil functions\n==============\n* Small suggestion: would be better to echo the result than return.\nSo you can have: idx=$(get_random_range ...) which is clearer than get_random_range; idx=$? .\nAlso, non-zero bash returns typically indicate an error.\n- Changed\n", "Uploaded kafka-306-v8.patch.\n\nThe changes made in the previous patch (kafka-306-v7.patch) will break single_host_multi_brokers/bin/run-test.sh due to the fact that ProducerPerformance will no longer print the message checksum.\n\nThe changes made in this patch supports single_host_multi_brokers/bin/run-test.sh to make use of the sequential message ID for test results validation.", "Thanks for making the changes - looks better.\n\n> - This is because the values may be overridden later by user specified\n> values. Therefore, some of the val is changed to var \n\nI meant even with overrides I don't think you need these vars and they can \nbe handled better with vals. However, it's a minor issue and looking at\nProducerPerformance it seems it needs an overhaul - the main loop is pretty hard to \nread. We should probably do that in a separate jira as it isn't directly\nrelated to this one.\n\nBTW, it seems bytesSent is not updated in seqIdMode.\n", "Patch v8 looks good overall. Some minor comments on ProducerPerformance:\n\n81. Could we default numThreadsOpt to 1? Then we can get rid of the following override.\n\n      if (!options.has(numThreadsOpt)) { \n        numThreads = 1 \n        warn(\"seqIdMode - numThreads is overridden to: \" + numThreads)\n      }\n\n82. Could we replace the following code\n            if (config.seqIdMode) {\n              producer.send(new ProducerData[Message,Message](config.topic, null, message))\n            }\n            else if(!config.isFixSize) {\n     with\n            if(!config.isFixSize || !config.seqIdMode) {", "Thanks Jun for reviewing. Your suggestion are made in kafka-306-v9.patch.\n\nThe changes are:\n91. numThreadsOpt is defaulted to 1 and the 'if' block is removed\n\n92. The following block is actually not necessary and it's now removed:\n          if (config.seqIdMode) { \n              producer.send(new ProducerData[Message,Message](config.topic, null, message)) \n            } ", "John, thanks for patch v9. Removed the commented out code in ProducerPerformance and committed to 0.8."], "derived": {"summary": "The system test in system_test/broker_failure is broken on the replication branch. This test is a pretty useful failure injection test that exercises the consumer rebalancing feature, various replication features like leader election.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "broker failure system test broken on replication branch - The system test in system_test/broker_failure is broken on the replication branch. This test is a pretty useful failure injection test that exercises the consumer rebalancing feature, various replication features like leader election."}, {"q": "What updates or decisions were made in the discussion?", "a": "John, thanks for patch v9. Removed the commented out code in ProducerPerformance and committed to 0.8."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-307", "title": "Refactor server code to remove interdependencies between LogManager and KafkaZooKeeper", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "labels": [], "created": "2012-03-16T01:14:31.000+0000", "updated": "2012-03-23T20:20:08.000+0000", "description": "Currently, LogManager wraps KafkaZooKeeper which is meant for all zookeeper interaction of a Kafka server. With replication, KafkaZookeeper will handle leader election, various state change listeners and then start replicas. Due to interdependency between LogManager and KafkaZookeeper, starting replicas is not possible until LogManager starts up completely. Due to this, we have to separate the broker startup procedures required for replication to get around this problem.\n\nIt will be good to refactor and clean up the server code, before diving deeper into replication.", "comments": ["Attaching a patch for this refactoring. Broadly, it includes the following changes -\n\n1. LogManager and KafkaZooKeeper are decoupled and don't depend on each other\n\n2. KafkaServer maintains replicas, instead of LogManager\n", "Overall looks good. Some suggestions:\n\n1. KafkaServer:\nReplicas are important data structures on each broker. Could we create a separate class Replicas that manage all replicas needed on a broker? Also, I see 2 different apis for adding a replica data structure. For replicas physically assigned to a broker, they always need a local Log. So, when adding those replicas, we need an api that creates a new log, if it's not there already (e.g. for newly created topics). The rest of replica data structures needed on a broker are not physically assigned to this broker and they are used to track the progress of the replicas in the followers. When adding those replicas, we need another api that doesn't force the creation of a local log.\n\n2. log4j.properties: Do we really want to turn off logging for all kafka during unit tests? How about keeping it at ERROR level. We can probably turn off logging for ZK.", "Thanks for the review !\n\n1. I like your suggestion. I wasn't quite satisfied with the way replicas were being managed. Incorporated the suggested changes. Looks much better now.\n\n2. Didn't mean to check it in. But yes, I think zookeeper should be on ERROR and so should Kafka.", "Thanks for patch v2. Looks cleaner. A couple of other comments:\n\n3. Is it better for KafkaZooKeeper to call ReplicaManager.addLocalReplica directly, instead of going through KafkaServer? For all replicas added in KafkaZookeeper, we know they are all local and should have a log. We could call LogManager.getOrCreateLog to get the log and pass it to ReplicaManager.\n\n4. There are unused imports in KafkaServer.", "3. Yeah, I thought about this, but then that requires LogManager to be referenced inside KafkaZooKeeper again. So, thats why it goes through KafkaServer. The point was to have only KafkaServer have access to LogManager, ReplicaManager and KafkaZookeeper, but not have those 3 components have interdependencies. What do you think ?\n\n4. Will clean it up before committing the patch.", "3. Fixed addReplica logic to use getOrCreateLog instead of getLog\n4. Cleaned up the imports", "+1 on patch v3.", "Thanks for the review ! Committed this."], "derived": {"summary": "Currently, LogManager wraps KafkaZooKeeper which is meant for all zookeeper interaction of a Kafka server. With replication, KafkaZookeeper will handle leader election, various state change listeners and then start replicas.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Refactor server code to remove interdependencies between LogManager and KafkaZooKeeper - Currently, LogManager wraps KafkaZooKeeper which is meant for all zookeeper interaction of a Kafka server. With replication, KafkaZookeeper will handle leader election, various state change listeners and then start replicas."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for the review ! Committed this."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-308", "title": "Corrupted message stored in log segment on disk", "status": "Resolved", "priority": "Blocker", "reporter": "Neha Narkhede", "assignee": null, "labels": [], "created": "2012-03-18T00:58:09.000+0000", "updated": "2014-03-20T21:48:53.000+0000", "description": "One of our consumers got stuck on a particular topic partition and threw the following exception -\n\n\n2012/03/16 05:20:51.285 ERROR [FetcherRunnable] [FetchRunnable-0] [kafka] error in FetcherRunnable for service-call:33-0: fetched offset = 387722824645: consumed offset = 387722824645\nkafka.common.InvalidMessageSizeException: invalid message size: 393216 only received bytes: 143266 at 387722824645( possible causes (1) a single message larger than the fetch size; (2) log corruption )\n        at kafka.message.ByteBufferMessageSet$$anon$1.makeNextOuter(ByteBufferMessageSet.scala:114)\n        at kafka.message.ByteBufferMessageSet$$anon$1.makeNext(ByteBufferMessageSet.scala:161)\n        at kafka.message.ByteBufferMessageSet$$anon$1.makeNext(ByteBufferMessageSet.scala:94)\n        at kafka.utils.IteratorTemplate.maybeComputeNext(IteratorTemplate.scala:59)\n        at kafka.utils.IteratorTemplate.hasNext(IteratorTemplate.scala:51)\n        at kafka.message.ByteBufferMessageSet.shallowValidBytes(ByteBufferMessageSet.scala:65)\n        at kafka.message.ByteBufferMessageSet.validBytes(ByteBufferMessageSet.scala:60)\n        at kafka.consumer.PartitionTopicInfo.enqueue(PartitionTopicInfo.scala:57)\n        at kafka.consumer.FetcherRunnable$$anonfun$run$5.apply(FetcherRunnable.scala:79)\n        at kafka.consumer.FetcherRunnable$$anonfun$run$5.apply(FetcherRunnable.scala:65)\n        at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)\n        at scala.collection.immutable.List.foreach(List.scala:45)\n        at kafka.consumer.FetcherRunnable.run(FetcherRunnable.scala:65)\n\nWe ran the DumpLogSegments tool on the log segment for that partition and it shows the log segment is corrupted - \n\n\n[2012-03-17 17:44:45,269] INFO offset: 387722824645 isvalid: false payloadsize: 393211 magic: 0 compresscodec: NoCompressionCodec (kafka.tools.DumpLogSegments$)\n[2012-03-17 17:44:45,269] INFO \n\nReading file message set from location 394088 (kafka.message.FileMessageSet)\n[2012-03-17 17:44:45,269] INFO Creating message byte buffer of size 1634499840 (kafka.message.FileMessageSet)\nException in thread \"main\" java.lang.OutOfMemoryError: Java heap space\n\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:39)\n\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:312)\n\tat kafka.message.FileMessageSet$$anon$1.makeNext(FileMessageSet.scala:126)\n\tat kafka.message.FileMessageSet$$anon$1.makeNext(FileMessageSet.scala:108)\n\tat kafka.utils.IteratorTemplate.maybeComputeNext(IteratorTemplate.scala:59)\n\tat kafka.utils.IteratorTemplate.hasNext(IteratorTemplate.scala:51)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:631)\n\tat kafka.utils.IteratorTemplate.foreach(IteratorTemplate.scala:30)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:79)\n\tat kafka.message.MessageSet.foreach(MessageSet.scala:87)\n\tat kafka.tools.DumpLogSegments$.main(DumpLogSegments.scala:92)\n\tat kafka.tools.DumpLogSegments.main(DumpLogSegments.scala)\n\nUpon inspecting the log segment using hexdump, it shows that the corrupted message had a suspicious size (larger than the rest of the messages for that topic), followed by a magic byte value of 0 and attributes value of 3\n\nnnarkhed-ld:kafka-trunk nnarkhed$ hexdump /tmp/387722823777.kafka -s 868 -n 6 -x\n0000364    0600    0000    0300                                        \n\nThe first 4 bytes are the size of the mesage (393216) and the last 2 bytes are the magic byte followed by attributes byte. \n\n\n", "comments": ["The corrupted log segment is uploaded here - http://people.apache.org/~nehanarkhede/kafka-misc/kafka-308/corrupted-log.tar.gz", "[~nehanarkhede] If no updates perhaps we should close this--perhaps it was just random disk corruption or something...", "Yes, agree."], "derived": {"summary": "One of our consumers got stuck on a particular topic partition and threw the following exception -\n\n\n2012/03/16 05:20:51. 285 ERROR [FetcherRunnable] [FetchRunnable-0] [kafka] error in FetcherRunnable for service-call:33-0: fetched offset = 387722824645: consumed offset = 387722824645\nkafka.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Corrupted message stored in log segment on disk - One of our consumers got stuck on a particular topic partition and threw the following exception -\n\n\n2012/03/16 05:20:51. 285 ERROR [FetcherRunnable] [FetchRunnable-0] [kafka] error in FetcherRunnable for service-call:33-0: fetched offset = 387722824645: consumed offset = 387722824645\nkafka."}, {"q": "What updates or decisions were made in the discussion?", "a": "Yes, agree."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-309", "title": "Bug in FileMessageSet's append API can corrupt on disk log", "status": "Resolved", "priority": "Critical", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "labels": [], "created": "2012-03-18T01:14:38.000+0000", "updated": "2012-03-22T16:59:28.000+0000", "description": "In FileMessageSet's append API, we write a ByteBufferMessageSet to a log in the following manner -\n\n    while(written < messages.sizeInBytes)\n      written += messages.writeTo(channel, 0, messages.sizeInBytes)\n\nIn ByteBufferMessageSet, the writeTo API uses buffer.duplicate() to append to a channel -\n\n  def writeTo(channel: GatheringByteChannel, offset: Long, size: Long): Long =\n    channel.write(buffer.duplicate)\n\nIf the channel doesn't write the ByteBuffer in one call, then we call it again until sizeInBytes bytes are written. But the next call will use buffer.duplicate() to write to the FileChannel, which will write the entire ByteBufferMessageSet again to the file. \n\nEffectively, we have a corrupted set of messages on disk. \n\nThinking about it, FileChannel is a blocking channel, so ideally, the entire ByteBuffer should be written to the FileChannel in one call. I wrote a test (attached here) and saw that it does. But I'm not aware if there are some corner cases when it doesn't do so. In those cases, Kafka will end up corrupting on disk log segment.\n", "comments": ["The test includes a FileChannelTest that writes byte buffer of varying lengths to a file channel in a single call and checks if the buffer was completely written.", "This can potentially cause the log corruption described in KAFKA-308", "This patch changes the writeTo API of the ByteBufferMessageSet to use the message set's buffer to write to the FileChannel. The writeTo API does *not* change the underlying buffer's position marker. \n\nThe right fix might be to not call ByteBufferMessageSet's writeTo in a loop in FileMessageSet's append API, since the guarantee of a blocking channel would not allow it to return without writing the entire message set or throwing an error. But that fix is arguably higher risk, so punting it for now, until we fully understand the guarantees of FileChannel", "+1 on the patch."], "derived": {"summary": "In FileMessageSet's append API, we write a ByteBufferMessageSet to a log in the following manner -\n\n    while(written < messages. sizeInBytes)\n      written += messages.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Bug in FileMessageSet's append API can corrupt on disk log - In FileMessageSet's append API, we write a ByteBufferMessageSet to a log in the following manner -\n\n    while(written < messages. sizeInBytes)\n      written += messages."}, {"q": "What updates or decisions were made in the discussion?", "a": "+1 on the patch."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-310", "title": "Incomplete message set validation checks in kafka.log.Log's append API can corrupt on disk log", "status": "Resolved", "priority": "Critical", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "labels": [], "created": "2012-03-19T21:33:21.000+0000", "updated": "2012-03-22T16:04:26.000+0000", "description": "The behavior of the ByteBufferMessageSet's iterator is to ignore and return false if some trailing bytes are found that cannot be de serialized into a Kafka message. The append API in Log, iterates through a ByteBufferMessageSet and validates the checksum of each message. Though, while appending data to the log, it just uses the underlying ByteBuffer that forms the ByteBufferMessageSet. Now, due to some bug, if the ByteBuffer has some trailing data, that will get appended to the on-disk log too. This can cause corruption of the log.", "comments": ["This can potentially cause the log corruption described in KAFKA-308", "In this patch, Log's append API truncates the ByteBufferMessageSet to validBytes before appending its backing byte buffer to the FileChannel.", "ByteBufferMessageSet.validBytes currently makes a deep iteration of all messages, which means that we need to decompress messages. To avoid this overhead, we should change ByteBufferMessageSet.validBytes to use a shallow iterator.", "That's true. This was probably overlooked KAFKA-277. Fixed it and uploaded v2.", "+1 on v2.", "Thanks for the review. Committed this to trunk"], "derived": {"summary": "The behavior of the ByteBufferMessageSet's iterator is to ignore and return false if some trailing bytes are found that cannot be de serialized into a Kafka message. The append API in Log, iterates through a ByteBufferMessageSet and validates the checksum of each message.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Incomplete message set validation checks in kafka.log.Log's append API can corrupt on disk log - The behavior of the ByteBufferMessageSet's iterator is to ignore and return false if some trailing bytes are found that cannot be de serialized into a Kafka message. The append API in Log, iterates through a ByteBufferMessageSet and validates the checksum of each message."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for the review. Committed this to trunk"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-311", "title": "Allow KafkaLog4jAppender to take in a configurable producer.type", "status": "Resolved", "priority": "Minor", "reporter": "Ed Buck", "assignee": "Jay Kreps", "labels": [], "created": "2012-03-20T19:01:56.000+0000", "updated": "2012-08-14T20:44:56.000+0000", "description": "The kafka log4j in trunk uses only SyncProducer, which means it will get lower throughput than AsyncProducer.\n\nChange KafkaLog4jAppender to take in the producer.type config and be able to pick either the sync or the async producers. ", "comments": ["Now accepts passing in a producer type, defaults to sync", "Thanks for the patch ! Would it make sense to default to async since the throughput requirement for log4j messages is typically on the higher side ?", "Covered by KAFKA-323."], "derived": {"summary": "The kafka log4j in trunk uses only SyncProducer, which means it will get lower throughput than AsyncProducer. Change KafkaLog4jAppender to take in the producer.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Allow KafkaLog4jAppender to take in a configurable producer.type - The kafka log4j in trunk uses only SyncProducer, which means it will get lower throughput than AsyncProducer. Change KafkaLog4jAppender to take in the producer."}, {"q": "What updates or decisions were made in the discussion?", "a": "Covered by KAFKA-323."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-312", "title": "Add 'reset' operation for AsyncProducerDroppedEvents", "status": "Resolved", "priority": "Minor", "reporter": "Dave DeMaagd", "assignee": null, "labels": ["patch"], "created": "2012-03-21T16:50:24.000+0000", "updated": "2015-04-04T22:03:25.000+0000", "description": "Currently no functionality exists to reset AsyncProducerDroppedEvents data, which can create problems with monitoring/alerting based off of this value (depending on what system you are using and how it has been implemented).  Diff attached. \n\nImpacted files:\n\ncore/src/main/scala/kafka/producer/async/AsyncProducerStats.scala\ncore/src/main/scala/kafka/producer/async/AsyncProducerStatsMBean.scala", "comments": ["+1. This is useful"], "derived": {"summary": "Currently no functionality exists to reset AsyncProducerDroppedEvents data, which can create problems with monitoring/alerting based off of this value (depending on what system you are using and how it has been implemented). Diff attached.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add 'reset' operation for AsyncProducerDroppedEvents - Currently no functionality exists to reset AsyncProducerDroppedEvents data, which can create problems with monitoring/alerting based off of this value (depending on what system you are using and how it has been implemented). Diff attached."}, {"q": "What updates or decisions were made in the discussion?", "a": "+1. This is useful"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-313", "title": "Add JSON/CSV output and looping options to ConsumerGroupCommand", "status": "Patch Available", "priority": "Minor", "reporter": "Dave DeMaagd", "assignee": "Ashish Singh", "labels": ["newbie", "patch"], "created": "2012-03-23T00:24:06.000+0000", "updated": "2020-05-27T20:57:08.000+0000", "description": "Adds:\n* '--loop N' - causes the program to loop forever, sleeping for up to N seconds between loops (loop time minus collection time, unless that's less than 0, at which point it will just run again immediately)\n* '--asjson' - display as a JSON string instead of the more human readable output format.\n\nNeither of the above  depend on each other (you can loop in the human readable output, or do a single shot execution with JSON output).  Existing behavior/output maintained if neither of the above are used.  Diff Attached.\n\nImpacted files:\n\ncore/src/main/scala/kafka/admin/ConsumerGroupCommand.scala", "comments": ["This tool also needs to be updated for 0.8 for the broker failure and other system tests.", "Is JSON better than CSV for this kind of thing? CSV is sometimes more shell friendly and that is what we are using for the perf tests...\n\nIf we like JSON then I am +1", "I worked with Dave on another version of this patch but will need to revisit. We can add CSV as well.", "[~jjkoshy] Do we still need this feature today?", "[~jjkoshy] I am planning to take a stab at this. If it is OK, then kindly assign this JIRA to me.", "I'm not sure if there is a strong need for this. In Linux you can use watch to repeat the command:\n\nwatch -n 10 ./bin/kafka-consumer-offset-checker.sh --zookeeper <zk> --topic <topic> --group <groupid> 2> /dev/null\n\nHaving an in-built loop does save the expense of spinning up a whole VM so it does not hurt to have it I guess.", "RB: https://reviews.apache.org/r/28096/\nAdds --loop N and --output.format options. Supports csv and json output formats.", "[~jjkoshy] while working on this I realized that user needs to know consumer groups available at a given time. I could not find an easy way to do this. A tool to get this information will be handy, created KAFKA-1773.", "[~jjkoshy] can you take a look at the patch?", "[~jjkoshy], just a reminder :), still waiting for your review.", "[~jjkoshy], still waiting for your review.", "[~singhashish] We are moving to a centralized ConsumerCommand in KAFKA-1476 that will replace all other consumer admin tools. Can you review to see if the tool has the options you are looking for? Going forward, I think we may want to improve that and phase out the current tools.", "ConsumerCommand in KAFKA-1476 does not allow looping and does not support JSON output.\n\nIf KAFKA-1476 will be committed soon, it makes sense to re-implement this in the new ConsumerCommand and deprecate the existing ConsumerOffsetChecker.", "bq. If KAFKA-1476 will be committed soon, it makes sense to re-implement this in the new ConsumerCommand and deprecate the existing ConsumerOffsetChecker.\n\n+1. I'm helping us commit KAFKA-1476", "Sounds good to me.", "Updated reviewboard https://reviews.apache.org/r/28096/\n against branch trunk", "[~gwenshap], [~nehanarkhede] I have updated the patch and the JIRA. Could you guys take a look.", "[~nehanarkhede] and [~gwenshap], pinging you guys again :)", "[~nehanarkhede] and [~gwenshap], pinging again for review.", "[~singhashish] I'm sorry for the delay. Not sure how this slipped through the list that [~junrao] is maintaining for patches under review. We really need our review-nag script :)\nThis just crossed my mind. Since the last time you uploaded this patch, we started the KIP Review process for all user-facing changes. My guess is that changes to user-facing tools are included in that. Would you mind starting a quick KIP discussion on this, it should be a quick vote. I can help merge your patch right after (we might need a rebase)", "[~nehanarkhede] thanks for the review! Makes sense to put out a KIP. Created [KIP-23|https://cwiki.apache.org/confluence/display/KAFKA/KIP-23].", "Updated reviewboard https://reviews.apache.org/r/28096/\n against branch trunk", "[~nehanarkhede] pinging for review.", "[~nehanarkhede] - mind if I review?", "[~gwenshap] need help with getting this KIP to done state :). It has been quite some time.", "Moving to \"in progress\" until new patch is submitted fixing issues highlighted in RB.", "Updated reviewboard https://reviews.apache.org/r/28096/\n against branch trunk", "Updated reviewboard https://reviews.apache.org/r/28096/\n against branch trunk", "Updated reviewboard https://reviews.apache.org/r/28096/\n against branch trunk", "Updated reviewboard https://reviews.apache.org/r/28096/\n against branch trunk", "Hi guys,\n\nsince this story is incomplete yet - may I suggest my solution for implementation of CSV and JSON outputs?\n\nI developed it for our current Kafka 0.8.2 but I'll be happy to port it to 0.9 branch. Here is a gist: https://gist.github.com/melan/929d6db871fe5edef941, I know that ConsumerOffsetChecker is deprecated and I'm ready to port it to ConsumerGroupCommand.\n\nIdea is simple: I added a Printer trait and implement it as \"default\" printer, CSV or JSON printers. We also implemented it as a stateful printer: when the tool is called directly in-proc instead of shell call it's better to get results as typed variables instead of string parsing.\n\nPlease let me know your opinion about this change and if it make sense - I'll push those changes within the next few days.\n\nThanks,\nDmitry", "Is this old ticket still valid? Can/should we close it?"], "derived": {"summary": "Adds:\n* '--loop N' - causes the program to loop forever, sleeping for up to N seconds between loops (loop time minus collection time, unless that's less than 0, at which point it will just run again immediately)\n* '--asjson' - display as a JSON string instead of the more human readable output format. Neither of the above  depend on each other (you can loop in the human readable output, or do a single shot execution with JSON output).", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add JSON/CSV output and looping options to ConsumerGroupCommand - Adds:\n* '--loop N' - causes the program to loop forever, sleeping for up to N seconds between loops (loop time minus collection time, unless that's less than 0, at which point it will just run again immediately)\n* '--asjson' - display as a JSON string instead of the more human readable output format. Neither of the above  depend on each other (you can loop in the human readable output, or do a single shot execution with JSON output)."}, {"q": "What updates or decisions were made in the discussion?", "a": "Is this old ticket still valid? Can/should we close it?"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-314", "title": "Go Client Multi-produce", "status": "Closed", "priority": "Minor", "reporter": "AaronR", "assignee": null, "labels": [], "created": "2012-03-23T21:50:19.000+0000", "updated": "2013-04-04T20:45:59.000+0000", "description": "Add support for Multi-Produce, and Multi-Fetch to the go client", "comments": ["For Review, an early version not finalized yet.  ", "i'm the process of adding support for the new wire protocol, I don't know that we really want to add support for these as they are going away..\n\n", "True, the new wire protocol changes some of this.  However, the bulk of the changes here are to update the GoClient to suport multi-topic, multi-partition in its internals.   The change from the current [(topic,partition, messages),(topic,partition,messages)] over to the new [(topic,(partition,messages),(etc)] only changes a few lines of code.  I considered this step 1 to get to step 2 being the new wire format.  Not sure if you are interested in collaborating on this, sounds like it conflicts with work you are doing, so i should drop it?  ", "I agree, I was thinking about taking this and reworking it for the new protocol.  I need to look at the patch a bit more but maybe we should just fold this patch into that one, then we just drop the old format.  What do u think?", "That sounds good.    But, I also have the 297, 298 issues out there (298 is a pretty big issue for me although you only see it under certain uses cases, mine being one).   What if we drop all of them together, get it working on github https://github.com/araddon/kafka and then submit all at once?  I don't think 296 has been included yet either.  That way we can see each others changes more easily?  ", "Old bug. Client libraries, such as this Go library, are no longer being maintained in the main project.\n\nPlease close. Won't Fix."], "derived": {"summary": "Add support for Multi-Produce, and Multi-Fetch to the go client.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Go Client Multi-produce - Add support for Multi-Produce, and Multi-Fetch to the go client."}, {"q": "What updates or decisions were made in the discussion?", "a": "Old bug. Client libraries, such as this Go library, are no longer being maintained in the main project.\n\nPlease close. Won't Fix."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-315", "title": "enable shallow iterator in ByteBufferMessageSet to allow mirroing data without decompression", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Jun Rao", "labels": [], "created": "2012-03-23T23:10:12.000+0000", "updated": "2012-04-06T21:37:06.000+0000", "description": "Currently, the iterator of ByteBufferMessageSet does deep iteration, ie, if messages are compressed, they will be decompressed first during the iteration. This adds CPU overhead. For mirroring data between 2 Kafka clusters, we can use a shallow iterator to avoid the decompression overhead.", "comments": ["patch attached.", "Patch looks good - would be great if you can provide some idea on the performance overhead of decompression vs. compression from the analysis that you did.\n\nAlso, we can add this config under \"Important configuration parameters for a mirror\" in the mirroring wiki.\n\nOne question:  ConsumerIterator still decodes the event - which would generally only make sense if you are using the DefaultDecoder right? So maybe we can just add a condition there to just return item.message if enableShallowIterator is true and also require that decoder is an instance of DefaultDecoder?\n", "Thanks for the review. Committed to trunk. \n\nFor performance, with a socket buffer and a fetch size of 2MB, I was able to improve the cross DC mirroring throughput btw 2 brokers from about 9MB/sec to 30MB/sec, using this patch.\n\nAs for the decoder, shallowIterator only works with the DefaultDecoder. If a wrong decoder is used, the user will get an exception and realize the decoder problem. So, this is probably not a big concern."], "derived": {"summary": "Currently, the iterator of ByteBufferMessageSet does deep iteration, ie, if messages are compressed, they will be decompressed first during the iteration. This adds CPU overhead.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "enable shallow iterator in ByteBufferMessageSet to allow mirroing data without decompression - Currently, the iterator of ByteBufferMessageSet does deep iteration, ie, if messages are compressed, they will be decompressed first during the iteration. This adds CPU overhead."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for the review. Committed to trunk. \n\nFor performance, with a socket buffer and a fetch size of 2MB, I was able to improve the cross DC mirroring throughput btw 2 brokers from about 9MB/sec to 30MB/sec, using this patch.\n\nAs for the decoder, shallowIterator only works with the DefaultDecoder. If a wrong decoder is used, the user will get an exception and realize the decoder problem. So, this is probably not a big concern."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-316", "title": "disallow recursively compressed message", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": null, "labels": [], "created": "2012-03-23T23:20:47.000+0000", "updated": "2017-09-11T09:05:53.000+0000", "description": "Currently, it is possible to create a compressed Message that contains a set of Messages, each of which is further compressed. Support recursively compressed messages has little benefit and can complicates the on disk storage format. We should probably disallow this.", "comments": ["Add a \"depth\" variable to keep track of recursion level. If depth > 0, throw an InvalidMessageException", "I don't see any reason to prevent nested messages. Do we need to fix this?", "In addition to the reasons citing in the description, it makes client message decoding more complex. I'm not sure what reason there would be to keep it.", "This was fixed in newer Kafka versions using client-side MemoryRecords/related code. Related JIRA : KAFKA-2066,  KAFKA-1739 "], "derived": {"summary": "Currently, it is possible to create a compressed Message that contains a set of Messages, each of which is further compressed. Support recursively compressed messages has little benefit and can complicates the on disk storage format.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "disallow recursively compressed message - Currently, it is possible to create a compressed Message that contains a set of Messages, each of which is further compressed. Support recursively compressed messages has little benefit and can complicates the on disk storage format."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was fixed in newer Kafka versions using client-side MemoryRecords/related code. Related JIRA : KAFKA-2066,  KAFKA-1739"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-317", "title": "Add support for new wire protocol to Go client", "status": "Closed", "priority": "Major", "reporter": "Jeffrey Damick", "assignee": null, "labels": ["go-client"], "created": "2012-03-24T13:36:49.000+0000", "updated": "2013-04-04T20:44:09.000+0000", "description": "Add support for the new wire protocol to the Go client (https://cwiki.apache.org/confluence/display/KAFKA/New+Wire+Format+Proposal)\n\n", "comments": ["i'm in the process of completing this and will add a patch in the next few days.", "Old bug. Client libraries, such as this Go library, are no longer being maintained in the main project.\n\nPlease close. Won't Fix."], "derived": {"summary": "Add support for the new wire protocol to the Go client (https://cwiki. apache.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add support for new wire protocol to Go client - Add support for the new wire protocol to the Go client (https://cwiki. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "Old bug. Client libraries, such as this Go library, are no longer being maintained in the main project.\n\nPlease close. Won't Fix."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-318", "title": "update zookeeper dependency to 3.3.5", "status": "Resolved", "priority": "Major", "reporter": "Pierre-Yves Ritschard", "assignee": null, "labels": [], "created": "2012-03-26T10:04:12.000+0000", "updated": "2015-04-08T01:24:06.000+0000", "description": "zookeeper 3.3.5 fixes a bunch of nasty bugs, update from 3.3.4", "comments": ["this simple patch bumps the requirement", "Pierre-Yves,\n\nThanks for the patch. Could you summarize the key benefits of 3.3.5 over 3.3.4?", "Hi, as per http://www.cloudera.com/blog/2012/03/apache-zookeeper-3-3-5-has-been-released/\n\nI think two items are important on the client library side:\n\n* ZOOKEEPER-1412 Java client watches inconsistently triggered on reconnect\n* ZOOKEEPER-1309 Creating a new ZooKeeper client can leak file handles\n\nI think this last fix only concerns the server part, but is worth mentionning:\n\n* ZOOKEEPER-1367 Data inconsistencies and unexpired ephemeral nodes after cluster restart", "Perhaps the dependency should be updated before the 0.8 release. Zookeeper 3.3.6 is available and claims to be backward compatible.", "I recommend we just take this on trunk. It will get in 0.8.1 soon enough.", "We are on 3.4.6 now..."], "derived": {"summary": "zookeeper 3. 3.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "update zookeeper dependency to 3.3.5 - zookeeper 3. 3."}, {"q": "What updates or decisions were made in the discussion?", "a": "We are on 3.4.6 now..."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-319", "title": "compression support added to php client does not pass unit tests", "status": "Closed", "priority": "Trivial", "reporter": "jon", "assignee": null, "labels": [], "created": "2012-03-26T14:25:42.000+0000", "updated": "2013-04-04T20:41:38.000+0000", "description": "The fix from #KAFKA-159 breaks unit tests. The client has changed to expect a compression algo.", "comments": ["diff --git a/clients/php/src/lib/Kafka/Encoder.php b/clients/php/src/lib/Kafka/Encoder.php\nindex 3c05cfd..edc8258 100644\n--- a/clients/php/src/lib/Kafka/Encoder.php\n+++ b/clients/php/src/lib/Kafka/Encoder.php\n@@ -40,7 +40,7 @@ class Kafka_Encoder\n \t *\n \t * @return string\n \t */\n-\tstatic public function encode_message($msg, $compression) {\n+\tstatic public function encode_message($msg, $compression=0) {\n \t\t// <MAGIC_BYTE: 1 byte> <COMPRESSION: 1 byte> <CRC32: 4 bytes bigendian> <PAYLOAD: N bytes>\n \t\treturn pack('CCN', self::CURRENT_MAGIC_VALUE, $compression, crc32($msg)) \n \t\t\t . $msg;\n@@ -56,7 +56,7 @@ class Kafka_Encoder\n \t *\n \t * @return string\n \t */\n-\tstatic public function encode_produce_request($topic, $partition, array $messages, $compression) {\n+\tstatic public function encode_produce_request($topic, $partition, array $messages, $compression=0) {\n \t\t// encode messages as <LEN: int><MESSAGE_BYTES>\n \t\t$message_set = '';\n \t\tforeach ($messages as $message) {\ndiff --git a/clients/php/src/tests/Kafka/BoundedByteBuffer/ReceiveTest.php b/clients/php/src/tests/Kafka/BoundedByteBuffer/ReceiveTest.php\nindex a751d7e..480bd4d 100644\n--- a/clients/php/src/tests/Kafka/BoundedByteBuffer/ReceiveTest.php\n+++ b/clients/php/src/tests/Kafka/BoundedByteBuffer/ReceiveTest.php\n@@ -1,3 +1,4 @@\n+<?php\n /**\n  * Licensed to the Apache Software Foundation (ASF) under one or more\n  * contributor license agreements.  See the NOTICE file distributed with\n@@ -15,7 +16,7 @@\n  * limitations under the License.\n  */\n \n-<?php\n+\n if (!defined('PRODUCE_REQUEST_ID')) {\n \tdefine('PRODUCE_REQUEST_ID', 0);\n }\ndiff --git a/clients/php/src/tests/Kafka/BoundedByteBuffer/SendTest.php b/clients/php/src/tests/Kafka/BoundedByteBuffer/SendTest.php\nindex 72c8f30..509a4c6 100644\n--- a/clients/php/src/tests/Kafka/BoundedByteBuffer/SendTest.php\n+++ b/clients/php/src/tests/Kafka/BoundedByteBuffer/SendTest.php\n@@ -1,3 +1,4 @@\n+<?php\n /**\n  * Licensed to the Apache Software Foundation (ASF) under one or more\n  * contributor license agreements.  See the NOTICE file distributed with\n@@ -15,7 +16,6 @@\n  * limitations under the License.\n  */\n \n-<?php\n \n /**\n  * Description of Kafka_BoundedByteBuffer_SendTest\ndiff --git a/clients/php/src/tests/Kafka/EncoderTest.php b/clients/php/src/tests/Kafka/EncoderTest.php\nindex 628b05f..471d31c 100644\n--- a/clients/php/src/tests/Kafka/EncoderTest.php\n+++ b/clients/php/src/tests/Kafka/EncoderTest.php\n@@ -1,3 +1,4 @@\n+<?php\n /**\n  * Licensed to the Apache Software Foundation (ASF) under one or more\n  * contributor license agreements.  See the NOTICE file distributed with\n@@ -14,8 +15,6 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-\n-<?php\n if (!defined('PRODUCE_REQUEST_ID')) {\n \tdefine('PRODUCE_REQUEST_ID', 0);\n }\n@@ -30,7 +29,7 @@ class Kafka_EncoderTest extends PHPUnit_Framework_TestCase\n \tpublic function testEncodedMessageLength() {\n \t\t$test = 'a sample string';\n \t\t$encoded = Kafka_Encoder::encode_message($test);\n-\t\t$this->assertEquals(5 + strlen($test), strlen($encoded));\n+\t\t$this->assertEquals(6 + strlen($test), strlen($encoded));\n \t}\n \t\n \tpublic function testByteArrayContainsString() {\n@@ -54,7 +53,7 @@ class Kafka_EncoderTest extends PHPUnit_Framework_TestCase\n \t\t}\n \t\t$size = 4 + 2 + 2 + strlen($topic) + 4 + 4;\n \t\tforeach ($messages as $msg) {\n-\t\t\t$size += 9 + strlen($msg);\n+\t\t\t$size += 10 + strlen($msg);\n \t\t}\n \t\t$this->assertEquals($size, strlen($encoded));\n \t}\ndiff --git a/clients/php/src/tests/Kafka/FetchRequestTest.php b/clients/php/src/tests/Kafka/FetchRequestTest.php\nindex ce3f274..408309d 100644\n--- a/clients/php/src/tests/Kafka/FetchRequestTest.php\n+++ b/clients/php/src/tests/Kafka/FetchRequestTest.php\n@@ -1,3 +1,4 @@\n+<?php\n /**\n  * Licensed to the Apache Software Foundation (ASF) under one or more\n  * contributor license agreements.  See the NOTICE file distributed with\n@@ -15,8 +16,6 @@\n  * limitations under the License.\n  */\n \n-<?php\n-\n /**\n  * Description of FetchRequestTest\n  *\n@@ -85,4 +84,4 @@ class Kafka_FetchRequestTest extends PHPUnit_Framework_TestCase\n \t\t$this->assertContains('offset:'  . $this->offset,    (string)$this->req);\n \t\t$this->assertContains('maxSize:' . $this->maxSize,   (string)$this->req);\n \t}\n-}\n\\ No newline at end of file\n+}\ndiff --git a/clients/php/src/tests/Kafka/MessageTest.php b/clients/php/src/tests/Kafka/MessageTest.php\nindex 38c3cc6..eed94ef 100644\n--- a/clients/php/src/tests/Kafka/MessageTest.php\n+++ b/clients/php/src/tests/Kafka/MessageTest.php\n@@ -1,3 +1,4 @@\n+<?php\n /**\n  * Licensed to the Apache Software Foundation (ASF) under one or more\n  * contributor license agreements.  See the NOTICE file distributed with\n@@ -15,7 +16,6 @@\n  * limitations under the License.\n  */\n \n-<?php\n \n /**\n  * @author Lorenzo Alberton <l.alberton@quipo.it>\ndiff --git a/clients/php/src/tests/Kafka/ProducerTest.php b/clients/php/src/tests/Kafka/ProducerTest.php\nindex a6705fa..c25a96e 100644\n--- a/clients/php/src/tests/Kafka/ProducerTest.php\n+++ b/clients/php/src/tests/Kafka/ProducerTest.php\n@@ -1,3 +1,4 @@\n+<?php\n /**\n  * Licensed to the Apache Software Foundation (ASF) under one or more\n  * contributor license agreements.  See the NOTICE file distributed with\n@@ -15,7 +16,6 @@\n  * limitations under the License.\n  */\n \n-<?php\n \n /**\n  * Override connect() method of base class\ndiff --git a/clients/php/src/tests/bootstrap.php b/clients/php/src/tests/bootstrap.php\nindex cbeb8cc..1681cc1 100644\n--- a/clients/php/src/tests/bootstrap.php\n+++ b/clients/php/src/tests/bootstrap.php\n@@ -1,3 +1,4 @@\n+<?php\n /**\n  * Licensed to the Apache Software Foundation (ASF) under one or more\n  * contributor license agreements.  See the NOTICE file distributed with\n@@ -15,7 +16,6 @@\n  * limitations under the License.\n  */\n \n-<?php\n \n function test_autoload($className)\n {\n@@ -50,4 +50,4 @@ set_include_path(\n );\n \n date_default_timezone_set('Europe/London');\n- \n\\ No newline at end of file\n+ \n-- \n1.7.4.1\n", "Patch superseded by KAFKA-419", "This bug has been superceded and should be closed."], "derived": {"summary": "The fix from #KAFKA-159 breaks unit tests. The client has changed to expect a compression algo.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "compression support added to php client does not pass unit tests - The fix from #KAFKA-159 breaks unit tests. The client has changed to expect a compression algo."}, {"q": "What updates or decisions were made in the discussion?", "a": "This bug has been superceded and should be closed."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-320", "title": "testZKSendWithDeadBroker fails intermittently due to ZKNodeExistsException", "status": "Resolved", "priority": "Critical", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "labels": [], "created": "2012-03-26T21:27:57.000+0000", "updated": "2012-04-08T02:00:49.000+0000", "description": "The testZKSendWithDeadBroker inside ProducerTest fails intermittently with the following exception -\n\n[error] Test Failed: testZKSendWithDeadBroker(kafka.producer.ProducerTest)\njava.lang.RuntimeException: A broker is already registered on the path /brokers/ids/0. This probably indicates that you either have configured a brokerid that is already in use, or else you have shutdown this broker and restarted it faster than the zookeeper timeout so it appears to be re-registering.\n        at kafka.utils.ZkUtils$.registerBrokerInZk(ZkUtils.scala:109)\n        at kafka.server.KafkaZooKeeper.kafka$server$KafkaZooKeeper$$registerBrokerInZk(KafkaZooKeeper.scala:60)\n        at kafka.server.KafkaZooKeeper.startup(KafkaZooKeeper.scala:52)\n        at kafka.server.KafkaServer.startup(KafkaServer.scala:84)\n        at kafka.producer.ProducerTest.testZKSendWithDeadBroker(ProducerTest.scala:174)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at junit.framework.TestCase.runTest(TestCase.java:164)\n        at junit.framework.TestCase.runBare(TestCase.java:130)\n        at junit.framework.TestResult$1.protect(TestResult.java:110)\n        at junit.framework.TestResult.runProtected(TestResult.java:128)\n        at junit.framework.TestResult.run(TestResult.java:113)\n        at junit.framework.TestCase.run(TestCase.java:120)\n        at junit.framework.TestSuite.runTest(TestSuite.java:228)\n        at junit.framework.TestSuite.run(TestSuite.java:223)\n        at junit.framework.TestSuite.runTest(TestSuite.java:228)\n        at junit.framework.TestSuite.run(TestSuite.java:223)\n        at org.scalatest.junit.JUnit3Suite.run(JUnit3Suite.scala:309)\n        at org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:40)\n        at sbt.TestRunner.run(TestFramework.scala:53)\n        at sbt.TestRunner.runTest$1(TestFramework.scala:67)\n        at sbt.TestRunner.run(TestFramework.scala:76)\n        at sbt.TestFramework$$anonfun$10$$anonfun$apply$11.runTest$2(TestFramework.scala:194)\n        at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)\n        at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)\n        at sbt.NamedTestTask.run(TestFramework.scala:92)\n        at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)\n        at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)\n        at sbt.TaskManager$Task.invoke(TaskManager.scala:62)\n        at sbt.impl.RunTask.doRun$1(RunTask.scala:77)\n        at sbt.impl.RunTask.runTask(RunTask.scala:85)\n        at sbt.impl.RunTask.sbt$impl$RunTask$$runIfNotRoot(RunTask.scala:60)\n        at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)\n        at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)\n        at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)\n        at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)\n        at sbt.Control$.trapUnit(Control.scala:19)\n        at sbt.Distributor$Run$Worker.run(ParallelRunner.scala:131)\n\nThe test basically restarts a server and fails with this exception during the restart\n\nThis is unexpected, since server1, after shutting down, should trigger the deletion of its registration of the broker id from ZK. But, here is the Kafka bug causing this problem -\n\nIn the test during server1.shutdown(), we do close the zkClient associated with the broker and it successfully deletes the broker's registration info from Zookeeper. After this, server1 can be succesfully started. Then the test completes and in the teardown(), we call server1.shutdown(). During this, the server doesn't really shutdown, since it is protected with the isShuttingDown variable, which was never set to false in the startup() API. Now, this leads to an open zkclient connection for the current test run. \n\nIf you try to re-run ProducerTest without exiting sbt, it will first bring up the zookeeper server. Then, since the kafka server during the previous run is still running, it can succesfully renew its session with zookeeper, and retain the /brokers/ids/0 ephemeral node. If it does this before server1.startup() is called in the test, the test will fail.\n\nThe fix is to set the shutdown related variables correctly in the startup API of KafkaServer. Also, during debugging this, I found that we don't close zkclient in the Producer as well. Due to this, unit tests throw a whole bunch of WARN that look like - \n\n[2012-03-26 14:14:27,703] INFO Opening socket connection to server nnarkhed-ld /127.0.0.1:2182 (org.apache.zookeeper.ClientCnxn:1061)\n[2012-03-26 14:14:27,703] WARN Session 0x13650dbf8dd0005 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn:1188)\njava.net.ConnectException: Connection refused\n        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)\n        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1146)\n", "comments": ["This patch includes the following -\n\n1. Fixes the kafka server restart bug by resetting the shutdown state variables in the startup() API of the KafkaServer. \n\n2. Shuts down the zkclient in the Producer\n\n3. ZkClient and Zookeeper can be set to WARN since we fixed the real issue, causing several warnings during the unit tests.\n", "Can someone review this ?", "That's a good finding. We should probably patch it in both trunk and 0.8. Just one comment.\n\nWe should only allow a KafkaServer to startup if it has been shutdown.", "Great catch!  +1, looks good and works on my machine.", "OK, I made some more changes -\n\n1. Cleaned up zkClient instance creations in unit tests. Now it is wrapped up inside ZookeeperTestHarness, so we ensure that it gets cleanup at an appropriate time. \n\n2. Changed Kafka server startup and shutdown behavior. Possibly made it more complex. Basically, \n\n2.1 A Kafka server can startup if it is not already starting up, if it is not currently being shutdown, or if it hasn't been already started\n\n2.2 A Kafka server can shutdown if it is not already shutting down, if it is not currently starting up, or if it hasn't been already shutdown. ", "The ZookeeperTestHarness change looks nice. a couple more comments:\n\n4. KafkaServer: It does look  a bit more complex now and some of the testing is not done atomically. How about the following? \n4.1 add an AtomticBoolean isServerStartable and initialize to true;\n4.2 in startup(), if we can atomically set isServerStartable from true to false, proceed with startup; otherwise throw an exception.\n4.3 in shutdown(), if isServerStartable is false, proceed with shutdown, at the very end, set isServerStartable to true. \nStartup() and shutdown() are expected to be called from the same thread. So we can expect a shutdown won't be called until a startup completes.\n\n5. SyncProducerTest: unused imports\n", "Thanks for the review!\n\n4. Regarding this, what do people think about the conditions under which a Kafka server should be allowed to startup and shutdown (listed under 2.1 and 2.2 above) ?\n5. Will fix this before checkin.\n6. Also, looks like improving the kafka server startup and shutdown is orthogonal to this bug fix. Can this be fixed (cleanly) through another JIRA ? I'd like to include just the fix for this issue as part of the checkin. ", "Yes, we can use another jira to see how we can improve kafka server startup and shutdown. For this jira, we can just make minimal changes in kafka server.", "Filed KAFKA-328 for improving startup and shutdown API of Kafka server.\n\nKept everything in v2 minus the complexity of changes in Kafka server.", "Overall, patch v3 looks good. I made a minor tweak of KafkaServer on top of v3. How does that look? You will need to:\n1. apply patch v3\n2. svn revert core/src/main/scala/kafka/server/KafkaServer.scala\n3. apply patch kafka-320-v3-delta.patch", "After applying the v3-delta patch, I see that some zkclient connections are not closed cleanly - \n[2012-04-06 17:34:12,805] WARN Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running (org.apache.zookeeper\n.server.NIOServerCnxn:639)\n[2012-04-06 17:34:12,809] WARN Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running (org.apache.zookeeper\n.server.NIOServerCnxn:639)\n[2012-04-06 17:34:13,201] WARN EndOfStreamException: Unable to read additional data from client sessionid 0x1368a38c37a0005, likely client has clos\ned socket (org.apache.zookeeper.server.NIOServerCnxn:634)\n[2012-04-06 17:34:13,264] WARN EndOfStreamException: Unable to read additional data from client sessionid 0x1368a38a86a0080, likely client has clos\ned socket (org.apache.zookeeper.server.NIOServerCnxn:634)\n\nAlso, after you set the isShutdown variable and before you check canStart, there can be interleaving between startup and shutdown, that can lead to open zookeeper client connection.", "Ok, we can take v3 for now and track the broker startup/shutdown in kafka-328.", "Checked into trunk and 0.8 branch"], "derived": {"summary": "The testZKSendWithDeadBroker inside ProducerTest fails intermittently with the following exception -\n\n[error] Test Failed: testZKSendWithDeadBroker(kafka. producer.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "testZKSendWithDeadBroker fails intermittently due to ZKNodeExistsException - The testZKSendWithDeadBroker inside ProducerTest fails intermittently with the following exception -\n\n[error] Test Failed: testZKSendWithDeadBroker(kafka. producer."}, {"q": "What updates or decisions were made in the discussion?", "a": "Checked into trunk and 0.8 branch"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-321", "title": "Remove dead brokers from ProducerPool", "status": "Resolved", "priority": "Major", "reporter": "Prashanth Menon", "assignee": null, "labels": ["replication"], "created": "2012-03-27T00:33:00.000+0000", "updated": "2015-02-07T23:50:42.000+0000", "description": "Currently, the ProducerPool does not remove producers that are tied to dead brokers.  Keeping such producers around can adversely effect normal producer operation by handing them out and failling - one such scenario is when updating cached topic metadata.  It's best if we remove any producers that are tied to dead brokers to avoid such situations.", "comments": [], "derived": {"summary": "Currently, the ProducerPool does not remove producers that are tied to dead brokers. Keeping such producers around can adversely effect normal producer operation by handing them out and failling - one such scenario is when updating cached topic metadata.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Remove dead brokers from ProducerPool - Currently, the ProducerPool does not remove producers that are tied to dead brokers. Keeping such producers around can adversely effect normal producer operation by handing them out and failling - one such scenario is when updating cached topic metadata."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-322", "title": "Remove one-off Send objects", "status": "Resolved", "priority": "Minor", "reporter": "Jay Kreps", "assignee": "Jay Kreps", "labels": ["replication"], "created": "2012-03-29T07:14:30.000+0000", "updated": "2016-05-17T14:03:02.000+0000", "description": "We seem to be accumulating a bunch of unnecessary classes that implement Send. I am not sure why people are doing this. Example:\nProducerResponseSend.scala\n\nIt is not at all clear why we would add a custom send object for each request/response type. They all do the same thing. The only reason for having the concept of a Send object was to allow two implementations: ByteBufferSend and MessageSetSend, the later let's us abstract over the difference between a normal write and a sendfile() call.\n\nI think we can refactory ByteBufferSend to take one or more ByteBuffers instead of just one and delete all of these one-offs.", "comments": ["This was fixed a long while back, cleaning up."], "derived": {"summary": "We seem to be accumulating a bunch of unnecessary classes that implement Send. I am not sure why people are doing this.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Remove one-off Send objects - We seem to be accumulating a bunch of unnecessary classes that implement Send. I am not sure why people are doing this."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was fixed a long while back, cleaning up."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-323", "title": "Add the ability to use the async producer in the Log4j appender", "status": "Resolved", "priority": "Major", "reporter": "Jose Quinteiro", "assignee": "Jay Kreps", "labels": ["appender", "log4j"], "created": "2012-03-29T19:51:52.000+0000", "updated": "2012-08-14T20:36:30.000+0000", "description": "I needed the log4j appender to use the async producer, so I added a couple of configuration methods to the log4j appender. I only added methods for the configuration fields that I needed. There are several in in the various ProducerConfigs that still cannot be set in the appender.\n\nSample use:\n\n\n\t\t\tKafkaLog4jAppender kafkaAppender = new KafkaLog4jAppender();\n\t\t\tkafkaAppender.setZkConnect( \"localhost:2181/kafka\" );\n\t\t\tkafkaAppender.setTopic( \"webapp\" );\n\t\t\tkafkaAppender.setProducerType( \"async\" );\n\t\t\tkafkaAppender.setEnqueueTimeout( Integer.toString( Integer.MIN_VALUE ) );\n\t\t\tkafkaAppender.activateOptions();\n", "comments": ["Index: core/src/main/scala/kafka/producer/KafkaLog4jAppender.scala\n===================================================================\n--- core/src/main/scala/kafka/producer/KafkaLog4jAppender.scala (revision 1307067)\n+++ core/src/main/scala/kafka/producer/KafkaLog4jAppender.scala (working copy)\n@@ -22,9 +22,7 @@\n import org.apache.log4j.AppenderSkeleton\n import org.apache.log4j.helpers.LogLog\n import kafka.utils.Logging\n-import kafka.serializer.Encoder\n import java.util.{Properties, Date}\n-import kafka.message.Message\n import scala.collection._\n\n class KafkaLog4jAppender extends AppenderSkeleton with Logging {\n@@ -34,7 +32,11 @@\n   var serializerClass:String = null\n   var zkConnect:String = null\n   var brokerList:String = null\n-\n+  var producerType:String = null\n+  var compressionCodec:String = null\n+  var enqueueTimeout:String = null\n+  var queueSize:String = null\n+\n   private var producer: Producer[String, String] = null\n\n   def getTopic:String = topic\n@@ -49,6 +51,18 @@\n   def getSerializerClass:String = serializerClass\n   def setSerializerClass(serializerClass:String) { this.serializerClass = serializerClass }\n\n+  def getProducerType:String = producerType\n+  def setProducerType(producerType:String) { this.producerType = producerType }\n+\n+  def getCompressionCodec:String = compressionCodec\n+  def setCompressionCodec(compressionCodec:String) { this.compressionCodec = compressionCodec }\n+\n+  def getEnqueueTimeout:String = enqueueTimeout\n+  def setEnqueueTimeout(enqueueTimeout:String) { this.enqueueTimeout = enqueueTimeout }\n+\n+  def getQueueSize:String = queueSize\n+  def setQueueSize(queueSize:String) { this.queueSize = queueSize }\n+\n   override def activateOptions() {\n     val connectDiagnostic : mutable.ListBuffer[String] = mutable.ListBuffer();\n     // check for config parameter validity\n@@ -68,6 +82,11 @@\n       LogLog.warn(\"Using default encoder - kafka.serializer.StringEncoder\")\n     }\n     props.put(\"serializer.class\", serializerClass)\n+    //These have default values in ProducerConfig and AsyncProducerConfig. We don't care if they're not specified\n+    if(producerType != null) props.put(\"producer.type\", producerType)\n+    if(compressionCodec != null) props.put(\"compression.codec\", compressionCodec)\n+    if(enqueueTimeout != null) props.put(\"queue.enqueueTimeout.ms\", enqueueTimeout)\n+    if(queueSize != null) props.put(\"queue.size\", queueSize)\n     val config : ProducerConfig = new ProducerConfig(props)\n     producer = new Producer[String, String](config)\n     LogLog.debug(\"Kafka producer connected to \" + (if(config.zkConnect == null) config.brokerList else config.zkConnect))", "Thanks for the patch ! Would you mind attaching it as a file, and granting it to Apache ?"], "derived": {"summary": "I needed the log4j appender to use the async producer, so I added a couple of configuration methods to the log4j appender. I only added methods for the configuration fields that I needed.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Add the ability to use the async producer in the Log4j appender - I needed the log4j appender to use the async producer, so I added a couple of configuration methods to the log4j appender. I only added methods for the configuration fields that I needed."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for the patch ! Would you mind attaching it as a file, and granting it to Apache ?"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-324", "title": "enforce broker.id to be a non-negative integer", "status": "Closed", "priority": "Major", "reporter": "Jun Rao", "assignee": "Swapnil Ghike", "labels": ["newbie", "noob"], "created": "2012-04-03T18:26:53.000+0000", "updated": "2012-08-17T21:10:10.000+0000", "description": "In DefaultEventHandler, it seems that we rely on the fact that broker.id is a non-negative integer. However, we don't enforce that in broker startup. ", "comments": ["For negative broker id, it should say the following.\n\njava.lang.IllegalArgumentException: requirement failed: The broker Id must be non-negative.\n\tat scala.Predef$.require(Predef.scala:145)\n\tat kafka.server.KafkaConfig.<init>(KafkaConfig.scala:36)\n\tat kafka.Kafka$.main(Kafka.scala:38)\n\tat kafka.Kafka.main(Kafka.scala)\n", "+1. Thanks for the patch!", "Instead of using require, it's probably better to use Utils.getIntInRange with a range of 0 and max_int. This way, the error message is more meaningful.", "Actually, the error message seems fine. It's just that using Utils.getIntInRange is more consistent with the rest of the code in KafkaConfig.", "Jun, I don't think getIntInRange provides any more functionality over require. The only difference is that it throws KafkaException, but if a config value is incorrect, I would imagine it is preferable to get IllegalArgumentException. And require allows you to plug in whatever description you need to make the error message meaningful. It is true that currently our code is using getIntInRange, but I actually like require better. \n\nSwapnil, as part of maybe another JIRA, do you mind looking into if it is a good idea to move to using require for other config options as well ?", "That would be fine if we change all KafkaException to require in the get* helper function.", "I think require is convenient to use - maybe we should have a Utils.require(bool, errormsg) that throws a KafkaException if the condition is unmet.", "Using require in Utils looks good to me. Should I make the change in get* functions in Utils?", "Actually all the get* helper functions in Utils throw IllegalArgumentException in 0.7 trunk. Can change the KafkaExceptions to IllegalArgumentException in 0.8. Adding a Utils.require(bool, errmsg) to maintain similar looks of KanfaConfig.", "How about we add a new getIntInRange() that doesn't take default (i.e., a required field) and use that for setting broker.id in KafkaConfig? If we think require is more intuitive, we can change all get* helper functions to use require. This way, all properties are set in the same style in KafkaConfig.", "Done. \n\nAs a side effect, one test case was modified to catch IllegalArgumentException instead of KafkaException.", "Thanks for patch v3. Looks good overall. One comment:\n\n1. For all get* in utils that take ByteBuffer, we should throw KafkaException, instead of IllegalArgumentException since they are not argument errors.", "Yes, should have seen that. :\\ Is there any concise require-like expression to throw a custom exception? I checked but didn't see any.", "If there is no such function in scala, we can write our own requireDummy(bool, errmsg, errcode-for-exception). The default way is to go back to if else and throw statements.", "Yes, we can just do the if/else statements for those. They are not really requirements, but are errors.", "Thanks. Attached the changes.", "Thanks for the patch. Committed to 0.8."], "derived": {"summary": "In DefaultEventHandler, it seems that we rely on the fact that broker. id is a non-negative integer.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "enforce broker.id to be a non-negative integer - In DefaultEventHandler, it seems that we rely on the fact that broker. id is a non-negative integer."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for the patch. Committed to 0.8."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-325", "title": "revisit broker config in 0.8", "status": "Closed", "priority": "Blocker", "reporter": "Jun Rao", "assignee": "Swapnil Ghike", "labels": ["optimization"], "created": "2012-04-03T18:27:43.000+0000", "updated": "2012-09-14T01:30:11.000+0000", "description": "With the create topic ddl, some of the broker configs like topic.partition.count.map probably don't make sense anymore. ", "comments": ["This patch will conflict with the last patch in KAFKA-495.\n\n- Removed topic.partition.count.map from KafkaConfig.\n- Removed the check on partition id in LogManager.createLog.\n- Removed one unit test from LogManagerTest.\n\n", "Rebased.", "Thanks for the patch. Just one comment.\n\nKafkaConfig: Could you remove stateChangeQSize and monitoringPeriodSecs? Neither is actually used.", "monitoringPeriodSecs seems to be used in SocketServer in starting a new processor and registerMBean.", "Rebased and made the change.", "Thanks for patch v3. Committed to 0.8."], "derived": {"summary": "With the create topic ddl, some of the broker configs like topic. partition.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "revisit broker config in 0.8 - With the create topic ddl, some of the broker configs like topic. partition."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for patch v3. Committed to 0.8."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-326", "title": "CallbackHandler.afterDequeuingExistingData is not called during event queue timeout", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Jun Rao", "labels": [], "created": "2012-04-06T15:23:32.000+0000", "updated": "2012-04-06T17:01:26.000+0000", "description": "CallbackHandler.afterDequeuingExistingData is only called when new events are coming and dequeued. It should be called when no new events are coming, but a queue timeout is reached.", "comments": ["Attached a patch.", "+1. Looks good", "Thanks for the review.committed to trunk."], "derived": {"summary": "CallbackHandler. afterDequeuingExistingData is only called when new events are coming and dequeued.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "CallbackHandler.afterDequeuingExistingData is not called during event queue timeout - CallbackHandler. afterDequeuingExistingData is only called when new events are coming and dequeued."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for the review.committed to trunk."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-327", "title": "Monitoring and tooling for Kafka replication", "status": "Resolved", "priority": "Critical", "reporter": "Neha Narkhede", "assignee": null, "labels": ["replication", "tools"], "created": "2012-04-06T17:51:56.000+0000", "updated": "2014-03-20T21:57:22.000+0000", "description": "This is an umbrella JIRA to hold all monitoring and tooling related items required for Kafka replication. ", "comments": ["Seems done."], "derived": {"summary": "This is an umbrella JIRA to hold all monitoring and tooling related items required for Kafka replication.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Monitoring and tooling for Kafka replication - This is an umbrella JIRA to hold all monitoring and tooling related items required for Kafka replication."}, {"q": "What updates or decisions were made in the discussion?", "a": "Seems done."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-328", "title": "Write unit test for kafka server startup and shutdown API ", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "BalajiSeshadri", "labels": ["newbie"], "created": "2012-04-06T19:53:57.000+0000", "updated": "2014-12-02T23:18:37.000+0000", "description": "Background discussion in KAFKA-320\n\nPeople often try to embed KafkaServer in an application that ends up calling startup() and shutdown() repeatedly and sometimes in odd ways. To ensure this works correctly we have to be very careful about cleaning up resources. This is a good practice for making unit tests reliable anyway.\n\nA good first step would be to add some unit tests on startup and shutdown to cover various cases:\n1. A Kafka server can startup if it is not already starting up, if it is not currently being shutdown, or if it hasn't been already started\n2. A Kafka server can shutdown if it is not already shutting down, if it is not currently starting up, or if it hasn't been already shutdown. ", "comments": ["Please assign to me i will work on the fix.", "Hey [~balajisn], it will be good to see if we already have a unit test for this. If not, write one and see if this is still an issue or not. ", "Sure will do. Can you guys please verify my patch for KAFKA-1476.\n\nBalaji\n\n", "Is this the kind of test you are expecting.\n\n @Test\n  def testServerStartupConsecutively(){\n    var server = new KafkaServer(config)\n    server.startup()\n    Thread.sleep(100)\n    try{\n    \tserver.startup()\n    }\n    catch{\n      case ex => {\n        assertTrue(ex.getMessage().contains(\"This scheduler has already been started!\"))\n      }\n    }\n    \n    server.shutdown()\n  }", "[~nehanarkhede] can you please advise on my comments on this unit test ?.", "[~nehanarkhede] i didnot see any such unit tests i have mentioned in my comment in current codebase.", "[~nehanarkhede] can u please suggest on my previous comment.", "[~balaji.seshadri@dish.com] Thanks for looking into this. The tests mentioned in the description of the JIRA can be added to the ServerShutdownTest test suite. However, I would avoid adding any sleeps as it defies the point of the test. I'm guessing the fix for the right behavior already exists, but your tests would verify it. I will also take a look at KAFKA-1476 today.", "Ping [~balaji.seshadri@dish.com]", "[~nehanarkhede] please find patch attached.", "Thanks for the patch, [~balaji.seshadri@dish.com]. The patch has a lot of unrelated indentation and whitespace changes that distracts from the actual changes and makes the review process harder. Could you please fix your env so all your patches don't suffer from the whitespace/indentation issues?", "[~nehanarkhede] Can you please send me code style you are using ?.\n\nAre you are using IntelliJ or Eclipse,please give me the formmater xml,i will import it to my environment.", "[~balaji.seshadri@dish.com] I use Intellij 13 community edition and whatever formatting that comes in that by default. ", "[~nehanarkhede] Please find patch for Startup Test.", "Thanks for the patch. Would you mind using our patch review tool going forward? It will make it easier to review.\n1. Better to use intercept[IllegalStateException] in the test. \n2. We should add all relevant test cases mentioned in the description like repeated shutdown", "[~nehanarkhede] Please review", "Few review comments-\n1. Can you please use intercept[IllegalStateException] in testServerStartupConsecutively? \n2. Can you move server.shutdown() to the finally block in testServerStartupConsecutively?\n3. Can you add a similar try catch finally block in testConsecutiveShutdown as well?", "Created reviewboard https://reviews.apache.org/r/27818/diff/\n against branch origin/trunk", "Updated reviewboard https://reviews.apache.org/r/27818/diff/\n against branch origin/trunk", "[~nehanarkhede] Please find changes in review board.\n\nhttps://reviews.apache.org/r/27818/", "[~balaji.seshadri@dish.com] Reviewed. Left a suggestion on the rb", "Updated reviewboard https://reviews.apache.org/r/27818/diff/\n against branch origin/trunk", "[~nehanarkhede] Please find updated review board.", "Thanks for your patch. Pushed to trunk."], "derived": {"summary": "Background discussion in KAFKA-320\n\nPeople often try to embed KafkaServer in an application that ends up calling startup() and shutdown() repeatedly and sometimes in odd ways. To ensure this works correctly we have to be very careful about cleaning up resources.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Write unit test for kafka server startup and shutdown API  - Background discussion in KAFKA-320\n\nPeople often try to embed KafkaServer in an application that ends up calling startup() and shutdown() repeatedly and sometimes in odd ways. To ensure this works correctly we have to be very careful about cleaning up resources."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for your patch. Pushed to trunk."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-329", "title": "Remove the watches/broker for new topics and partitions and change create topic admin API to send start replica state change to all brokers", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Prashanth Menon", "labels": ["replication"], "created": "2012-04-09T16:50:23.000+0000", "updated": "2012-06-18T02:56:41.000+0000", "description": "Currently in 0.8, all brokers register a watch on /brokers/topics and /brokers/topics/[topic] for all topics in a Kafka cluster. The watches are required to discover new topics. \nThere is another way this can be achieved, as proposed here - https://cwiki.apache.org/confluence/display/KAFKA/Kafka+replication+detailed+design+V2#KafkareplicationdetaileddesignV2-Createtopic\n\nBasically, the create-topic admin command sends start-replica state change request to all brokers in the assigned replicas list.", "comments": ["The ticket is to revisit topic creation based on the V3 design (https://cwiki.apache.org/confluence/display/KAFKA/kafka+Detailed+Replication+Design+V3). We probably should first change the ZK layout by using 1 ZK node per topic, instead of 1 ZK node per partition. This will make it easier to register watches for new topics. Then, when the controller framework is ready, we can move the logic of handling new topics to the controller.", "So, changing the CreateTopicCommand and AdminUtils to place the topic+replica assignment into the new ZK path is simple, but I'm not sure changing the ZK watch structure can be done in this ticket without either breaking several tests that rely on existing ZK watch behaviour or implementing additional watchers that will be integrated/used when the controller is implemented.  It may be a better idea to make this a subtask of the controller implementation ticket and roll it in with the larger controller patch?  Or perhaps break up the controller ticket by the sections outlined in the V3 wiki along with a common framework? \n\nLet me know if I'm missing something or over complicating things, the V3 design is a little knew to me so I may have missed some discussions.", "We know that we want to change the ZK layout from the current \"1 path per partition\" design to the \"1 path per topic\" one. This can be done either before we move to the controller implementation or after. Doing this as part of the controller implementation may make it too big.\n\nI was thinking that n this patch, we just move the ZK layout to 1 path per topic without the controller logic in place. We will need to (1) patch the create/delete/list topic ddl; (2) change the topic listener in both KafkaZookeeper and ZookeeperConsumerConnector to read from the new path. This makes it easier when we move to the controller implementation since the correct ZK layout is already in place. If we change the ZK layout later, we will need to first implement the controller logic based on the current ZK layout and change it again later.", "Prashanth,\n\nDo you plan to work on this jira anytime soon? Thanks,", "Hi Jun,\n\nYes, I'm looking to wrap it up this weekend.", "Hi all,\n\nI've attached a very early draft of the work required here.  It changes the CreateTopic ddl, currently writing to both old and new locations to keep tests running while the patch is completed.  The largest change comes in KafkaZooKeeper to the topic watcher.  Thinking about it, most of the code in there regarding leader election will be gone when the controller goes in, but I've left the important pieces in there to keep tests working.  I'll continue working to get the consumer ZooKeeperConsumerConnector side working with the new path.\n\nI'd like to get some preliminary feedback on the patch.  Hopefully, I'm going in the right direction based off my understanding on the V3 design.  Comments welcome :)\n\nSide note: As we move into storing info in ZK as json, should we investigate using a friendlier library?  The native Scala one is quite nasty.", "Prashanth,\n\nThanks for patch. Some comments:\n\n1. AdminUtils.createTopicPartitionAssignmentPathInZK(): In V3, we don't need the topic version anymore. So, we don't need to store it in the ZK. \n\n2. KafkaZookeeper: For this patch, we can just keep the main logic the same as in V2 (i.e., assuming no controller). The only changes will be based on that (1) partition assignment is stored in 1 ZK path per topic; (2) partitions and partition assignment never change after created (such logic will be added later in the controller). So, we can keep subscribeToTopicAndPartitionsChanges(), but obtain partition assignment from a different ZK path. Ditto for handleNewTopics. We can punt on the handling of delete topics since there is a separate jira for that. We can get rid of handleNewPartitions(). Most of the code in this class can probably be reused when we move the logic to the controller.\n", "Thanks for the review, Jun.  New draft is attached incorporating both your suggestions, though it's still in draft and not final.  My only concern, and I'm not sure if it's my machine or not, but the ZooKeeperConsumerConnectorTest takes an unusually long time to run and throws exceptions intermittently.  \n\nI'll take a final walkthrough tomorrow, along with any additional suggestions and submit another final patch for review.\n", "Thanks for draft v2. Some quick comments:\n\n21. KafkaZookeeper: Do we still need onTopicChange and initLeader if we keep handleNewTopics?\n\n22. AdminUtils: We should get rid of the old way of storing topic data in ZK and just keep the new way.\n\nAs for json library, any json library works well with Scala?", "Hi Jun,\n\nNew patch with your changes incorporated.  I think it's functionally complete, but I may have missed some things.  \n\nAs for a json library, I'm not sure what plays well specifically with Scala, though I suspect most basic Java json libs should be fine.  I personally have experience with Jackson which has a nice Scala wrapper lib called Jerkson (https://github.com/codahale/jerkson).  There is also Lift-JSON, but I'm not familiar with though I've heard good things from people.  Perhaps integrating a JSON lib can be part of a separate JIRA?", "I recommend we just use the json library scala includes. It is supposed to be slow and not very general, but for our limited purposes that is probably fine.", "Prashanth,\n\nThanks for patch v1. Some comments:\n\n31. AdminUtils: remove used imports\n\n32. CreateTopicCommand.createTopic(): add space after if when assigning to partitionReplicaAssignment\n\n33. TopicChangeListener.handleChildChange(): add a TODO comment for handling topic deletion.\n\n34. ZookeeperConsumerConnectorTest.testCompressionSetConsumption() seems to always fail on sending messages now. Not clear to me why though.\n\n", "Prahsanth,\n\nI was trying to figure out why ZookeeperConsumerConnectorTest.testCompressionSetConsumption() failed. For that, I added a bit more logging in DefaultEventHandler. However, after the logging was added, the test seems to pass. I suspect this is probably due to a time-dependant transient failure. I checked in the extra logging as a trivial change. So, if you can fix the remaining comments and there is no more unit test failure, you can just commit the patch without further review.", "Hi Jun,\n\n31, 32, 33, 34  - All Done.  I've committed the changes to 0.8.\n\nAs for the transient failure, I'm not able to reproduce them, but I suspect it may have something to do with the timing of topic creation and leader election.  The tests in ZooKeeperConsumerConnectorTest use auto-creation of topics, and so, the DefaultEventHandler may attempt send messages three times (by default) and fail before the leader is able to bootstrap itself.  If this is indeed the case, we can either increase the producer retry count or, more deterministically, create and wait for the leader using the normal CreateTopicCommand.createTopic and TestUtils.waitUntilLeaderIsElected methods (which I'd prefer).      ", "Committed to 0.8."], "derived": {"summary": "Currently in 0. 8, all brokers register a watch on /brokers/topics and /brokers/topics/[topic] for all topics in a Kafka cluster.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Remove the watches/broker for new topics and partitions and change create topic admin API to send start replica state change to all brokers - Currently in 0. 8, all brokers register a watch on /brokers/topics and /brokers/topics/[topic] for all topics in a Kafka cluster."}, {"q": "What updates or decisions were made in the discussion?", "a": "Committed to 0.8."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-330", "title": "Add delete topic support ", "status": "Closed", "priority": "Blocker", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "labels": ["features", "project"], "created": "2012-04-09T16:54:33.000+0000", "updated": "2015-03-01T21:47:03.000+0000", "description": "One proposal of this API is here - https://cwiki.apache.org/confluence/display/KAFKA/Kafka+replication+detailed+design+V2#KafkareplicationdetaileddesignV2-Deletetopic", "comments": ["During controller failover, we need to remove unneeded leaderAndISRPath that the previous controller didn't get a chance to remove.", "The delete topic logic can follow the same logic in partition reassignment.\n1. Create a ZK path to indicate that we want to delete a topic.\n2. The controller registers a listener to the deleteTopic path and when the watcher is triggered:\n2.1 Send stopReplica requests to each relevant broker.\n2.2 Each broker then delete the local log directory.\n2.3 Once the stopReplica request completes, the controller deletes the deleteTopic path and the delete topic command completes.", "I've been doing a lot of manual resetting of data in Kafka and one thing I noticed is that clients don't always behave so well when I do that. So when you implement this you should probably also make sure that the current kafka clients behave well when a topic is removed, i.e. error or reset as appropriate.", "I'll take this on, hoping to get a patch in this weekend.", "Prashanth, any progress on this jira?", "Apologies, I began work on this jira before going on break.  Now that I'm back, I should be able to wrap it up.", "Hey Prashanth, how's this JIRA coming along ?", "Any news on this?", "Prashanth, mind if I take a look at this ? I have some time this week.", "Apologies for not getting to this.  Neha, go ahead and run with it.", "Here is a broad description of how delete topic can work in Kafka -\n\n1. The delete topic tool writes to a /delete_topics/[topic] path\n2. The controller's delete topic listener fires and does the following -\n2.1 List the partitions for the topic to be deleted\n2.2 For each partition, do the following -\n2.2.1 Move the partition to OfflinePartition state. Take the leader offline. From this point on, all produce/consume requests for this partition will start failing \n2.2.2 For every replica for a partition, first move it to OfflineReplica state (it is removed from isr) then to NonExistentReplica (send stop-replica request with delete flag on to each replica)\n2.3 Delete the /brokers/topics/[topic] path from zookeeper\n2.4 Delete the /delete_topics/[topic] path to signify completion of the delete operation\n", "We should check if KAFKA-784 is still an issue after adding delete topic support to Kafka 0.8", "I'd also like to see an auto-delete feature, where by a topic can be automatically be deleted, after it has been garbage collected, and has no more messages.  This could be set to happen automatically, after an expiration time.  This may require exposing an api on each broker so a broker can ask if any other brokers have messages pending for a topic, before deciding the topic should be removed.", "Delete topic admin path schema updated at https://cwiki.apache.org/confluence/display/KAFKA/Kafka+data+structures+in+Zookeeper", "It makes sense to include the delete topic feature in the 0.8 beta release since most people might create test topics that would require cleanup", "Patch v1 attached. \n\nHow topics are deleted: \n1. The DeleteTopicsCommand writes to /admin/delete_topics in zk and exits.\n2. The DeleteTopicsCommand complains if a topic that is being deleted is absent in zookeeper. It won't run even if at least one of the topics specified is actually present in the zookeeper. \n3. A DeleteTopicsListener is triggered in controller. It moves the replicas and partitions to Offline->NonExistent states, deletes the partitions from controller's memory, sends StopReplicaRequests with deletePartition=true.\n4. Brokers on receiving the StopReplicaRequest remove the partition from their own memory and delete the logs.\n5. If all the partitions were successfully deleted, the topic path is deleted from zookeeper.\n6. Controller always deletes the admin/delete_topics path at the end. It checks in removeFromTopicsBeingDeleted() whether each topic has been deleted from zookeeper, at which point it declares victory or logs a warning of shame.\n\n\nHow to validate that the topics have been deleted:\n1. Rerun the DeleteTopicsCommand, it should complain that the topics are absent in zookeeper.\n\n\nSpecial comments:\nA. TopicChangeListener:\n1. I think that we should not handle deleted topics here. We should rather modify the controller's memory in NonExistentPartition state change. This is because the controller will release its lock between DeleteTopics listener and TopicChangeListener, we should want the controller's memory to be up-to-date when the lock is released with the completion of DeleteTopics listener.\n2. Probably there is no need to add the new topics' partititon-replica assignment to controllerContext.partitionReplicaAssignment, because onNewTopicCreation() will do that. I put a TODO there. Please correct if I am wrong.\n\n\nHandling failures:\n\nA. What happens when controller fails:\n1. Before OfflineReplica state change: New controller context will be initialized and initializeAndMaybeTriggerTopicDeletion() will delete the topics.\n2. After OfflineReplica state change and before OfflinePartition state change: Initialization of controller context will re-insert replicas into ISR, and initializeAndMaybeTriggerTopicDeletion() will delete the topics.\n3. After OfflinePartition state change and before NonExistentReplica state change: Ditto as 2.\n4. After NonExistentReplica state change and before NonExistentPartition state change: The replicas that were deleted will be restarted on individual brokers, then the topics will be deleted.\n5. After NonExistentPartition state change and before deleting topics from zk: Ditto as 3. (The NonExistentPartition state change in partition state machine currently does not delete the partitions from zk, it assumes that the controller will delete them, which is similar to what we do for some other state changes as of now).\nI think the deletion should proceed smoothly even if the controller fails over in the middle of 1,2,3,4 or 5 above.\n\nB. What happens if a topic is deleted when a broker that has a replica of that topic's partition is down? =>\ni. When the broker comes back up and the topic has been deleted from zk, the controller can only tell the broker which topics are currently alive. The broker should delete the dead logs when it receives the first leaderAndIsr request. This can be done just before starting the hw checkpointing thread. \nii. This will also be useful in replica reassignment for a partition. When the replica reassignment algorithms sends a StopReplica request with delete=true, the receiving broker could be down. After the broker is back up, it will realize that it needs to delete the logs for certain partitions that are no longer assigned to it.\n\n\nPossible corner cases:\n1. What happens to hw checkpointing for deleted partitions? => checkpointHighWatermarks() reads the current allPartitions() on a broker and writes the hw. So the hw for deleted partitions will disappear.\n\n2. What happens to Produce/Fetch requests in purgatory? => \ni. After the topics have been deleted, produce requests in purgatory will expire because there will no fetchers, fetch requests will expire because producer requests would fail in appendToLocalLog() and no more data will be appended.\nii. Expiration of producer requests is harmless. \niii. Expiration of fetch requests will try to send whatever data is remaining, but it will not be able to send any data because the replica would be dead. We could think of forcing the delayed fetch requests to expire before the replica is deleted and remove the expired requests from the delayed queue, but that would probably require synchronizing on the delayed queue. Thoughts?\n\n\nOther unrelated changes: \nA. ReplicaStateMachine\n1. Moved NonExistentReplica to the bottom of cases to maintain the same order as PartitionStateMachine.\n2. Deleted a redundant replicaState.put(replica,OnlineReplica) statement.\n3. Even if a replica is not in the ISR, it should always be moved to OfflineReplica state.\n\nB. Utils.scala:\n1. Bug fix in seqToJson().  \n\nTesting done:\n1. Bring up one broker, create topics, delete topics, verify zk, verify that logs are gone. \n2. Bring up two brokers, create topics, delete topics, verify zk, verify that logs are gone from both brokers.\n3. Repeat the above 1 and 2 with more than one partition per topic.\n4. Write to admin/delete_paths, bring up the controller, watch the topic and logs get deleted.\n5. Bring up two brokers, create two topics with replication factor of two, verify that the logs get created. Now, shut down broker 1 and delete a topic. Verify that the topic disappears from zk and logs of broker 0. Bring up broker 1, verify that the topic disappears from the logs of broker 1 because controller (broker 0) will send leaderAndIsr request for the remaining topic.\n6. Validate error inputs.\n7. Validate that the tool prints error when a non-existent topic is being deleted.\n\nIs it ok if I write unit tests after this patch is checked in, in case there are modifications?", "Actually scratch 5 in \"How topics are deleted\". Topics are always deleted from zk.", "Thanks for the patch! Some suggestions -\n\n1. In controller, it is important to not let a long delete topics operation block critical state changes like elect leader. To make this possible, relinquish the lock between the deletes for individual topics\n2. If you do relinquish the lock like I suggested above, you need to now take care of avoid leader elections for partitions being deleted\n3. Since now you will handle topic deletion for individual topics, it might be worth changing the zookeeper structure for delete topics so status on individual topic deletes gets reported accordingly. One way to do this is to introduce a path to indicate that the admin tool has initiated delete operation for some topics (/admin/delete_topics_updated), and create child nodes under /admin/delete_topics, one per topic. As you complete individual topic deletion, you delete the /admin/delete_topics/<topic> path. Admin tool creates the /admin/delete_topics/<topic> path and updates /admin/delete_topics_updated. Controller only registers a data change watcher on /admin/delete_topics_updated. When this watcher fires, it reads the children of /admin/delete_topics and starts topic deletion. \n4. On startup/failover, the controller registers a data change watch on /admin/delete_topics_updated, and then reads the list of topics under /admin/delete_topics.\n5. Admin tool never errors out since it just adds to the list of deleted topics\n\nOn the broker side, there are a few things to be done correctly -\n\n1. KafkaApis\nAfter receiving stop replica request, request handler should reject produce/fetch requests for partitions to be deleted by returning PartitionBeingDeleted error code. Once the delete is complete, the partition can be removed from this list. In that case, it will return UnknownTopicOrPartition error code\n\n2. ReplicaManager\n2.1 Remove unused variable leaderBrokerId from makeFollower()\n2.2 Fix the comment inside recordFollowerPosition to say \"partition hasn't been created or has been deleted\"\n2.3 Let the partition do the delete() operation. This will ensure that the leaderAndIsrUpdateLock is acquired for the duration of the delete. This will avoid interleaving leader/isr requests with stop replica requests and simplify the reasoning of log truncate/highwatermark update operations\n\n3. Partition - Introduce a new delete() API that works like this -\n1. Acquire leaderIsrUpdateLock so that create log does not interfere with delete log. Also remove/add fetcher does not interfere with delete log.\n2. Removes fetcher for the partition\n3. Invoke delete() on the log. Be careful how current read/write requests will be affected.\n\n4. LogManager\n1. When deleteLogs() is invoked, remove logs from allLogs. This will prevent flush being invoked on the log to be deleted.\n2. Invoke log.delete() on every individual log.\n3. log.markDeletedWhile(_ => true) will leave an extra rolled over segment in the in memory segment list\n\n5. Log\n1. Log delete should acquire \"lock\" to prevent interleaving with append/truncate/roll/flush etc\nFollowing steps need to be taken during log.delete()\n2. Invoke log.close()\n3. Invoke segmentList.delete(), where SegmentList.delete() only does contents.set(new Array[T](0))\n4. Invoke segment.delete()\n5. Update a flag deleted = true\n\nFew questions to be thought about -\n\n- Are any changes required to roll(). If deleted flag is true, then skip roll().\n- Are any changes required to markDeletedWhile(). Same as roll. If deleted flag is true, skip\n- Are any changes required to flush() ? This can be invoked either during roll or by append. It cannot be invoked by the flush thread since that is disabled for logs to be deleted. This needs to be handled by using lastOption. \n- See what to do with truncateTo(). This is used during make follower in Partition. This won't interfere with delete since Partition's delete acquires the leaderIsrUpdateLock. Another place that uses truncateTo() is the handleOffsetOutOfRange on the follower. This won't interleave since the replica fetcher was already removed before attempting to delete the log\n- See what to do with truncateAndStartWithNewOffset(). This won't interleave with delete log since the replica fetcher was already removed before attempting to delete the log\n- What if the broker is writing from the log when stop replica is deleting it ? Since log.delete() acquires the \"lock\", either append starts before or after the delete. If it starts after, then the changed mentioned in #7 and #9 should be made. \n- What if the broker is about to write to the log that is under deletion ? Same as above\n- What if the broker is reading from the log that is being deleted ? It will get a ClosedChannelException, I think. This needs to be conformed. The test can run a consumer that is consuming data from beginning of a log and you can invoke delete topic. \n- What if the broker about to read from the log that is being deleted ? It will try reading from a file channel that is closed. This will run into ClosedChannelException. Should we catch ClosedChannelException and log an appropriate error and send PartitionDeleted error code when that happens ?\n- What happens to the partition entry from the high watermark file when it is being deleted ? When partition is removed from allPartitions, the next high watermark checkpoint removes the partition's entry from the high watermark file.\n- What happens to requests in the purgatory when partition has been deleted ? When a partition has been removed from allPartitions, then the requests in the purgatory will send UnknownTopicOrPartitionCode back to the client.\n\n6. Log.read()\nval first = view.head.start\nThis needs to change to headOption. Return empty message set when this returns None\n\n7. Log.flush()\nsegments.view.last.flush()\nNeed to change the above to segments.view.lastOption. If that returns None, then return without flushing. \n\n8. SegmentList.delete()\ncontents.set(new Array[T](0))\n\n9. Log.append()\nFix this to use lastOption - val segment = maybeRoll(segments.view.last)\nIf None, then return (-2,-2) to signify that the log was deleted\n\n\n\n\n", "Replying to a few comments, will follow up with changes according to others:\n\nOn the controller side: \n1. I think that the delete topics command will not take too long to complete, in any case it won't take any longer than Preferred Replica Election command. Both commands write to /admin zk path and trigger listeners that may send send some requests and update some zk paths. I believed that the reason for relinquishing the lock in ReassignPartitions listeners after every partition reassignment was that the controller waits for the new replicas to join the ISR, which could take long.\n2. Hence I think that we should not relinquish the lock between deletion of two topics.\n3. So maybe we don't need to use two separate zk paths? If we rerun the DeleteTopicsCommand, it should complain that the topics are absent in zookeeper if the topics were successfully deleted.\n\nOn the broker side:\n4. LogManager: \n1. deleteLogs() indeed removes the logs from allLogs.\n2. delete() is invoked on every individual log.\n3. Yes, following up on this.\n\n5. Log: \n1. The lock is acquired by all these functions, but I will double check if it needs to be acquired at the top level for our purpose.\n3. Well, log.delete() takes care of deleting the individual segments. \n\nWill make modifications to Log*, hopefully they will address all your comments.\n", "Let's do some zookeeper math here to see how long it takes to delete one topic, 8 partitions from a 6 node kafka cluster -\n\n# of zk ops                  operation during delete topic\n1                                    val partitionAssignment = ZkUtils.getPartitionAssignmentForTopics(zkClient, topics.toSeq)        \n7                                    val brokers = ZkUtils.getAllBrokersInCluster(zkClient)\t\t\t\t\t\t\t\t\t\t\n1                                    ZkUtils.getAllReplicasOnBroker(zkClient, topics.toSeq, brokers.map(_.id))\t        (This is a redundant read from zookeeper, so reuse the info read in step 1)\n2                                    removeReplicaFromIsr -> getLeaderIsrAndEpochForPartition, conditionalUpdatePersistentPath\n9                                    removeFromTopicsBeingDeleted -> readDataMaybeNull (1), deletePath (8)\n\n20 zookeeper ops. With 10ms per op, (which is a what a zookeeper cluster that kafka consumers and brokers share does in best case), that is 200ms per topic\nWith 50 such topics, it is 10 seconds. That is the amount of time you are starving other partitions from being available! \nWhat you can do, for simplicity purposes, is keep the existing long lock on the controller side for this patch. We can improve it later or in 0.8.1\n\nAlso, the log side of your patch does not acquire the lock. You used the delete APIs that were used by unit tests so far. So they don't deal with the issues I've mentioned above in my comments.\nRegarding LogManager - Let's look at the modified version of your patch and see if that solves the problems I've outlined above wrt to interleaving other operations with delete log.\n", "Thanks for the excellent explanation. Some of these zk operations will not be repeated for every topic, for example, ZkUtils.getAllBrokersInCluster(zkClient) or removeFromTopicsBeingDeleted. But anyways, it seems that the cost of ZK operations is even worse because removeReplicaFromIsr() makes 2 Zk operations for each replica, which would be responsible for 2*50*8*3(repl-factor) = 2400 zk operations. \n\nI agree with you, let's optimize this after log deletion works correctly. \n\nSimilarly, preferred replica election will suffer from a very high number of zk operations since the callbacks will elect leader for every partition. So, we could relinquish the lock in preferred replica election too.", "Thanks for the patch. Even though the patch is not big, it touches quite a few critical components such as controller, replica manager, and log. It will take some time to stabilize this. We probably should consider pushing this out of 0.8 so that we don't delay the 0,8 release too much. One quick comment:\n\n1. KafkaControler.onTopicsDeletion(): Why do we need to read things like partitionAssignment and brokers from ZK? Could we just use the cached data in controller context?", "Yes, I agree with you Jun. Attaching a temporary patch v2 for the records, which needs testing. Patch v2 reads the cached data from the controller context. We don't need to review this patch since Log has significantly changed in trunk, so I will need to rework that part.", "move to 0.8.1 to reduce the remaining work in 0.8.0.", "Created reviewboard https://reviews.apache.org/r/17460/\n against branch trunk", "Delete topic is a pretty tricky feature and there are multiple ways to solve it. I will list the various approaches with the tradeoffs here. Few things to think about that make delete topic tricky -\n\n1. How do you handle resuming delete topics during controller failover?\n2. How do you handle re-creating topics if brokers that host a subset of the replicas are down?\n3. If a broker fails during delete topic, how does it know which version of the topic it has logs for, when it restarts? This is relevant if we allow re-creating topics while a broker is down\n\nWill address these one by one. \n\n#1 is pretty straightforward to handle and can be achieved in a way similar to partition reassignment (through an admin path in zookeeper indicating a topic deletion that has not finished)\n\n#2 is an important policy decision that can affect the complexity of the design for this feature. If you allow topics to be deleted while brokers are down, the broker needs a way to know that it's version of the topic is too old. This is mainly an issue since a topic can be re-created and written to, while a broker is down. We need to ensure that a broker does not join the quorum with an older version of the log. There are 2 ways to solve this problem that I could think off -\n   1. Do not allow topic deletion to succeed if a broker hosting a replica is down. Here, the controller keeps track of the state of each replica during topic deletion    (TopicDeletionStarted, TopicDeletionSuccessful, TopicDeletionFailed) and only marks the topic as deleted if all replicas for all partitions of that topic are successfully deleted. \n   2. Allow a topic to be deleted while a broker is down and keep track of the \"generation\" of the topic in a fault tolerant, highly available and consistent log. This log can either be zookeeper or a Kafka topic. The main issue here is how many generations would we have to keep track off for a topic. In other words, can this \"generation\" information ever be garbage collected. There isn't a good bound on this since it is unclear when the failed broker will come back online and when a topic will be re-created. That would mean keeping this generation information for potentially a very long time and incurring overhead during recovery or bootstrap of generation information during controller or broker fail overs. This is especially a problem for use cases or tests that keep creating and deleting a lot of short lived topics. Essentially, this solution is not scalable unless we figure out an intuitive way to garbage collect this topic metadata. It would require us to introduce a config for controlling when a topic's generation metadata can be garbage collected. Note that this config is different from the topic TTL feature which controls when a topic, that is currently not in use, can be deleted. Overall, this alternative is unnecessarily complex for the benefit of deleting topics while a broker is down.\n\n#3 is related to the policy decision made about #2. If a topic is not marked deleted successfully while a broker is down, the controller will automatically resume topic deletion when a broker restarts. \n\nThis patch follows the previous approach of not calling a topic deletion successful until all replicas have confirmed the deletion of local state for that topic. This requires the following changes -\n1. TopicCommand issues topic deletion by creating a new admin path /admin/delete_topics/<topic>\n\n2. The controller listens for child changes on /admin/delete_topic and starts topic deletion for the respective topics\n\n3. The controller has a background thread that handles topic deletion. The purpose of having this background thread is to accommodate the TTL feature, when we have it. This thread is signaled whenever deletion for a topic needs to be started or resumed. Currently, a topic's deletion can be started only by the onPartitionDeletion callback on the controller. In the future, it can be triggered based on the configured TTL for the topic. A topic's deletion will be halted in the following scenarios -\n* broker hosting one of the replicas for that topic goes down\n* partition reassignment for partitions of that topic is in progress\n* preferred replica election for partitions of that topic is in progress (though this is not strictly required since it holds the controller lock for the entire duration from start to end)\n\n4. Topic deletion is resumed when -\n* broker hosting one of the replicas for that topic is started\n* preferred replica election for partitions of that topic completes\n* partition reassignment for partitions of that topic completes \n\n5. Every replica for a topic being deleted is in either of the 3 states - \n* TopicDeletionStarted (Replica enters TopicDeletionStarted phase when the onPartitionDeletion callback is invoked. This happens when the child change watch for /admin/delete_topics fires on the controller. As part of this state change, the controller sends StopReplicaRequests to all replicas. It registers a callback for the StopReplicaResponse when deletePartition=true thereby invoking a callback when a response for delete replica is received from every replica)\n* TopicDeletionSuccessful (deleteTopicStopReplicaCallback() moves replicas from TopicDeletionStarted->TopicDeletionSuccessful depending on the error codes in StopReplicaResponse)\n* TopicDeletionFailed. (deleteTopicStopReplicaCallback() moves replicas from TopicDeletionStarted->TopicDeletionSuccessful depending on the error codes in StopReplicaResponse. In general, if a broker dies and if it hosted replicas for topics being deleted, the controller marks the respective replicas in TopicDeletionFailed state in the onBrokerFailure callback. The reason is that if a broker fails before the request is sent and after the replica is in TopicDeletionStarted state, it is possible that the replica will mistakenly remain in TopicDeletionStarted state and topic deletion will not be retried when the broker comes back up.)\n\n6. The delete topic thread marks a topic successfully deleted only if all replicas are in TopicDeletionSuccessful state and it starts the topic deletion teardown mode where it deletes all topic state from the controllerContext as well as from zookeeper. This is the only time the /brokers/topics/<topic> path gets deleted. \nOn the other hand, if no replica is in TopicDeletionStarted state and at least one replica is in TopicDeletionFailed state, then it marks the topic for deletion retry. \n\n7. I've introduced callbacks for controller-broker communication. Ideally, every callback should be of the following format (RequestOrResponse) => Unit. BUT since StopReplicaResponse doesn't carry the replica id, this is handled in a somewhat hacky manner in the patch. The purpose is to fix the approach of upgrading controller-broker protocols in a reasonable way before having delete topic upgrade StopReplica request in a one-off way. Will file a JIRA for that.\n\nSeveral integration tests added for delete topic -\n\n1. Topic deletion when all replica brokers are alive\n2. Halt and resume topic deletion after a follower replica is restarted\n3. Halt and resume topic deletion after a controller failover\n4. Request handling during topic deletion\n5. Topic deletion and partition reassignment in parallel\n6. Topic deletion and preferred replica election in parallel\n7. Topic deletion and per topic config changes in parallel", "Updated reviewboard https://reviews.apache.org/r/17460/\n against branch trunk", "Updated reviewboard https://reviews.apache.org/r/17460/\n against branch trunk", "Updated reviewboard https://reviews.apache.org/r/17460/\n against branch trunk", "Updated reviewboard https://reviews.apache.org/r/17460/\n against branch trunk", "Updated reviewboard https://reviews.apache.org/r/17460/ against branch trunk.\n\nAsking for a more detailed review as the patch is somewhat tested and refactored to make the topic deletion logic easier to maintain and understand. ", "Updated reviewboard https://reviews.apache.org/r/17460/\n against branch trunk", "Updated reviewboard  against branch trunk", "Updated reviewboard https://reviews.apache.org/r/17460/\n against branch trunk", "Updated reviewboard https://reviews.apache.org/r/17460/\n against branch trunk", "Updated reviewboard https://reviews.apache.org/r/17460/\n against branch trunk", "Thanks for the reviews. This is a big patch, please do submit your review even after checkin, I will fix the issues in follow up JIRAs.", "Can we have https://issues.apache.org/jira/secure/attachment/12625445/KAFKA-930_2014-01-27_13%3A28%3A51.patch\nthis merged now that delete support is in?", "Sriram,\n\nYou can check in that patch now. You probably would have to add an additional check to see whether a partition whose leader is to be moved to the preferred replica is in a topic to be deleted, while holding the controller lock. If so, skip leader balancing.", "[~sriramsub] It will not be enough to just drop the partitions that belong to topics being deleted from the preferred replica list. In addition that, I think we may also have to leave them out while computing what the preferred replica imbalance factor is.", "Updated reviewboard https://reviews.apache.org/r/17460/\n against branch trunk", "Could the default config please set {code}delete.topic.enable=true{code} by default, so that kafka behaves more intuitively out of the box?", "+1 on enabling by default.", "created KAFKA-1993 to track this suggestion."], "derived": {"summary": "One proposal of this API is here - https://cwiki. apache.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add delete topic support  - One proposal of this API is here - https://cwiki. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "created KAFKA-1993 to track this suggestion."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-331", "title": "recurrent produce errors", "status": "Resolved", "priority": "Major", "reporter": "Pierre-Yves Ritschard", "assignee": null, "labels": [], "created": "2012-04-12T08:33:02.000+0000", "updated": "2015-02-07T23:50:25.000+0000", "description": "I am using trunk and regularily see such errors popping up:\n\n\n32477890 [kafka-processor-7] ERROR kafka.server.KafkaRequestHandlers  - Error processing ProduceRequest on pref^@^@^@:0\njava.io.FileNotFoundException: /mnt/kafka/logs/pref^@^@^@-0/00000000000000000000.kafka (Is a directory)\n        at java.io.RandomAccessFile.open(Native Method)\n        at java.io.RandomAccessFile.<init>(RandomAccessFile.java:233)\n        at kafka.utils.Utils$.openChannel(Utils.scala:324)\n        at kafka.message.FileMessageSet.<init>(FileMessageSet.scala:75)\n        at kafka.log.Log.loadSegments(Log.scala:144)\n        at kafka.log.Log.<init>(Log.scala:116)\n        at kafka.log.LogManager.createLog(LogManager.scala:149)\n        at kafka.log.LogManager.getOrCreateLog(LogManager.scala:204)\n        at kafka.server.KafkaRequestHandlers.kafka$server$KafkaRequestHandlers$$handleProducerRequest(KafkaRequestHandlers.scala:69)\n        at kafka.server.KafkaRequestHandlers.handleProducerRequest(KafkaRequestHandlers.scala:53)\n        at kafka.server.KafkaRequestHandlers$$anonfun$handlerFor$1.apply(KafkaRequestHandlers.scala:38)\n        at kafka.server.KafkaRequestHandlers$$anonfun$handlerFor$1.apply(KafkaRequestHandlers.scala:38)\n        at kafka.network.Processor.handle(SocketServer.scala:296)\n        at kafka.network.Processor.read(SocketServer.scala:319)\n        at kafka.network.Processor.run(SocketServer.scala:214)\n        at java.lang.Thread.run(Thread.java:679)\n32477890 [kafka-processor-7] ERROR kafka.network.Processor  - Closing socket for /xx.xx.xx.xx because of error\njava.io.FileNotFoundException: /mnt/kafka/logs/pref^@^@^@-0/00000000000000000000.kafka (Is a directory)\n        at java.io.RandomAccessFile.open(Native Method)\n        at java.io.RandomAccessFile.<init>(RandomAccessFile.java:233)\n        at kafka.utils.Utils$.openChannel(Utils.scala:324)\n        at kafka.message.FileMessageSet.<init>(FileMessageSet.scala:75)\n        at kafka.log.Log.loadSegments(Log.scala:144)\n        at kafka.log.Log.<init>(Log.scala:116)\n        at kafka.log.LogManager.createLog(LogManager.scala:149)\n        at kafka.log.LogManager.getOrCreateLog(LogManager.scala:204)\n        at kafka.server.KafkaRequestHandlers.kafka$server$KafkaRequestHandlers$$handleProducerRequest(KafkaRequestHandlers.scala:69)\n        at kafka.server.KafkaRequestHandlers.handleProducerRequest(KafkaRequestHandlers.scala:53)\n        at kafka.server.KafkaRequestHandlers$$anonfun$handlerFor$1.apply(KafkaRequestHandlers.scala:38)\n        at kafka.server.KafkaRequestHandlers$$anonfun$handlerFor$1.apply(KafkaRequestHandlers.scala:38)\n        at kafka.network.Processor.handle(SocketServer.scala:296)\n        at kafka.network.Processor.read(SocketServer.scala:319)\n        at kafka.network.Processor.run(SocketServer.scala:214)\n        at java.lang.Thread.run(Thread.java:679)\n\nThis results in a \"pref\" directory created inside the log dir. The original topic should be prefix, somehow a NUL gets inserted there.\nThe producing was done with a kafka.javaapi.producer.Producer instance, on which send was called with a kafka.javaapi.producer.ProducerData instance.\n\nThere are no log entries created inside that dir and no impact on the overall operation of the broker operations and consumers.\n\nIs the producer thread-safe ?\n\n\n\n\n", "comments": ["The producer is thread safe. Are you using the sync or the async mode in the producer? Do you have a simple test that can reproduce this?"], "derived": {"summary": "I am using trunk and regularily see such errors popping up:\n\n\n32477890 [kafka-processor-7] ERROR kafka. server.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "recurrent produce errors - I am using trunk and regularily see such errors popping up:\n\n\n32477890 [kafka-processor-7] ERROR kafka. server."}, {"q": "What updates or decisions were made in the discussion?", "a": "The producer is thread safe. Are you using the sync or the async mode in the producer? Do you have a simple test that can reproduce this?"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-332", "title": "Mirroring should use multiple producers; add producer retries to DefaultEventHandler", "status": "Resolved", "priority": "Minor", "reporter": "Joel Jacob Koshy", "assignee": "Joel Jacob Koshy", "labels": [], "created": "2012-04-13T21:49:29.000+0000", "updated": "2012-04-25T02:25:07.000+0000", "description": "I'm clubbing these two together as these are both important for mirroring.\n\n(1) Multiple producers:\n\nShallow iteration (KAFKA-315) helps improve mirroring throughput when\nmessages are compressed. With shallow iteration, the mirror-maker's consumer\ndoes not do deep iteration over compressed messages. However, when its\nembedded producer sends these messages to the target cluster's brokers, the\nreceiving broker does deep iteration to validate the messages before\nappending to the log.\n\nIn the current (pre- KAFKA-48) request handling mechanism, one producer\neffectively translates to one server-side thread for handling produce\nrequests, so there is still a bottleneck due to decompression (due to\nmessage validation) on the target broker.\n\nOne way to work around this is to use broker.list with multiple brokers\nspecified per broker. E.g.,\nbroker.list=0:localhost:9191,1:localhost:9191,2:localhost:9191,... which\neffectively emulates multiple server-side threads. It would be better to\njust add a num.producers option to the mirror-maker and instantiate that\nmany producers.\n\n(2) Retries:\n\nIf the mirror-maker uses broker.list and one of the brokers is bounced for\nany reason, messages can get lost. Message loss can be reduced/avoided if\nthe brokers are behind a VIP and if retries are supported. This option will\nnot work for the zk-based producer because the decision of which broker to\nsend to has already been made, so retries would go to the same (potentially\nstill down) broker. (With KAFKA-253 it would work for zk-based producers as\nwell, but that is only in 0.8).\n", "comments": ["Some comments:\n1. DefaultEventHandler: \n1.1. It would be useful to see the retry # in trace log\n1.2 We should capture all Throwable.\n\n2. ProducerConfig: explain a bit more why num.retries is not appropriate for zk-based producer. Basically, during resend, we don't re-select brokers.\n\n3. MirrorMaker: The usage of circularIterator is pretty fancy. Would it be simpler to just put all producers in an array and loop through it circularly? ", "Thanks for the review.\n\n- Captured throwable\n- Trace logging retry attempt\n- Additional doc on num.retries\n\nCircular iterator: I thought it would be a convenient pattern for round-robin selection and in fact makes the code simpler/clearer. If it is hard to read, we can just go with explicit modulo-based selection based on a counter - although IMO that is less clean.\n", "Patch v2 looks good. One more comment:\n4. ProducerSendThread.tryToHandle() should catch Throwable too.\n", "tryToHandle catches all now.", "Thanks for the patch. Committed to trunk."], "derived": {"summary": "I'm clubbing these two together as these are both important for mirroring. (1) Multiple producers:\n\nShallow iteration (KAFKA-315) helps improve mirroring throughput when\nmessages are compressed.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Mirroring should use multiple producers; add producer retries to DefaultEventHandler - I'm clubbing these two together as these are both important for mirroring. (1) Multiple producers:\n\nShallow iteration (KAFKA-315) helps improve mirroring throughput when\nmessages are compressed."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for the patch. Committed to trunk."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-333", "title": "mirroring - enable non-random partitioning in embedded producer", "status": "Resolved", "priority": "Major", "reporter": "xiaoyu wang", "assignee": null, "labels": [], "created": "2012-04-18T20:18:26.000+0000", "updated": "2015-02-07T23:50:08.000+0000", "description": "Currently the producer for mirroring uses the random partitioner. It will be very useful if we can specify partitioner there. One use case is when we aggregate ad server logs by mirroring the local kafka cluster on each ad server, we want to partition ad logs based on where they come from - the ad server id.  ", "comments": [], "derived": {"summary": "Currently the producer for mirroring uses the random partitioner. It will be very useful if we can specify partitioner there.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "mirroring - enable non-random partitioning in embedded producer - Currently the producer for mirroring uses the random partitioner. It will be very useful if we can specify partitioner there."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-334", "title": "Some tests fail when building on a Windows box", "status": "Resolved", "priority": "Minor", "reporter": "Roman Garcia", "assignee": null, "labels": ["build-failure", "test-fail"], "created": "2012-04-24T05:30:00.000+0000", "updated": "2016-09-13T01:08:13.000+0000", "description": "Trying to create a ZIP distro from sources failed.\nOn Win7. On cygwin, command shell and git bash.\nTried with incubator-src download from ASF download page, as well as fresh checkout from latest trunk (r1329547).\nOnce I tried the same on a Linux box, everything was working ok.\n\nsvn co http://svn.apache.org/repos/asf/incubator/kafka/trunk kafka-0.7.0\n./sbt update (OK)\n./sbt package (OK)\n./sbt release-zip (FAIL)\nTests failing:\n[error] Error running kafka.integration.LazyInitProducerTest: Test FAILED\n[error] Error running kafka.zk.ZKLoadBalanceTest: Test FAILED\n[error] Error running kafka.javaapi.producer.ProducerTest: Test FAILED\n[error] Error running kafka.producer.ProducerTest: Test FAILED\n[error] Error running test: One or more subtasks failed\n[error] Error running doc: Scaladoc generation failed\nStacks:\n[error] Test Failed: testZKSendWithDeadBroker\njunit.framework.AssertionFailedError: Message set should have another message\n        at junit.framework.Assert.fail(Assert.java:47)\n        at junit.framework.Assert.assertTrue(Assert.java:20)\n        at kafka.javaapi.producer.ProducerTest.testZKSendWithDeadBroker(ProducerTest.scala:448)\n[error] Test Failed: testZKSendToNewTopic\njunit.framework.AssertionFailedError: Message set should have 1 message\n        at junit.framework.Assert.fail(Assert.java:47)\n        at junit.framework.Assert.assertTrue(Assert.java:20)\n        at kafka.javaapi.producer.ProducerTest.testZKSendToNewTopic(ProducerTest.scala:416)\n[error] Test Failed: testLoadBalance(kafka.zk.ZKLoadBalanceTest)\njunit.framework.AssertionFailedError: expected:<5> but was:<0>\n        at junit.framework.Assert.fail(Assert.java:47)\n        at junit.framework.Assert.failNotEquals(Assert.java:277)\n        at junit.framework.Assert.assertEquals(Assert.java:64)\n        at junit.framework.Assert.assertEquals(Assert.java:195)\n        at junit.framework.Assert.assertEquals(Assert.java:201)\n        at kafka.zk.ZKLoadBalanceTest.checkSetEqual(ZKLoadBalanceTest.scala:121)\n        at kafka.zk.ZKLoadBalanceTest.testLoadBalance(ZKLoadBalanceTest.scala:89)\n[error] Test Failed: testPartitionedSendToNewTopic\njava.lang.AssertionError:\n  Unexpected method call send(\"test-topic1\", 0, ByteBufferMessageSet(MessageAndOffset(message(magic = 1, attributes = 0, crc = 2326977762, payload = java.nio.HeapByteBuffer[pos=0 lim=5 cap=5]),15), )):\n    close(): expected: 1, actual: 0\n        at org.easymock.internal.MockInvocationHandler.invoke(MockInvocationHandler.java:45)\n        at org.easymock.internal.ObjectMethodsFilter.invoke(ObjectMethodsFilter.java:73)\n        at org.easymock.internal.ClassProxyFactory$MockMethodInterceptor.intercept(ClassProxyFactory.java:92)\n        at kafka.producer.SyncProducer$$EnhancerByCGLIB$$4385e618.send(<generated>)\n        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)\n        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)\n        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)\n        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)\n        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)\n        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)\n        at kafka.producer.Producer.zkSend(Producer.scala:137)\n        at kafka.producer.Producer.send(Producer.scala:99)\n        at kafka.producer.ProducerTest.testPartitionedSendToNewTopic(ProducerTest.scala:576)\n[error] Test Failed: testZKSendToNewTopic\njunit.framework.AssertionFailedError: Message set should have 1 message\n        at junit.framework.Assert.fail(Assert.java:47)\n        at junit.framework.Assert.assertTrue(Assert.java:20)\n        at kafka.producer.ProducerTest.testZKSendToNewTopic(ProducerTest.scala:429)\n", "comments": ["Changed to minor, as doesn't look important once binaries get distributed formally.", "[~romangarciam] Do these still fail on latest trunk?", "Now the flow I described works, but doesn't look like it's running any test. \nI did:\nsbt.bat update (OK)\nsbt.bat package  (OK)\nsbt.bat release-zip (OK, only \"success\" is printed...no tests ran)\nsbt.bat test (several tests fail)\n\n[error] Failed: : Total 214, Failed 22, Errors 0, Passed 192, Skipped 0\n[error] Failed tests:\n[error]         kafka.log.LogManagerTest\n[error]         kafka.log.LogTest\n[error]         kafka.log.LogSegmentTest\n[error]         kafka.log.LogCleanerIntegrationTest\n[error]         kafka.integration.LazyInitProducerTest\n[error]         kafka.admin.AdminTest\n[error]         kafka.log.CleanerTest\n\nSome of the tests stack heads:\n----------------------\n[error] Test Failed: testCleanupExpiredSegments(kafka.log.LogManagerTest)\njunit.framework.AssertionFailedError: Now there should only be only one segment in the index. expected:<1> but was:<6>\n----------------------\n[info] Test Starting: testProduceAndMultiFetch(kafka.integration.LazyInitProducerTest)\n[error] Test Failed: testProduceAndMultiFetch(kafka.integration.LazyInitProducerTest)\njava.lang.AssertionError: expected:<List(a_test1, b_test1)> but was:<List()>\n----------------------\n[info] Test Starting: testOpenDeletesObsoleteFiles\n[error] Test Failed: testOpenDeletesObsoleteFiles\nkafka.common.KafkaStorageException: Failed to change the log file suffix from  to .deleted for log segment 0\n----------------------\n[info] Test Starting: testChangeFileSuffixes(kafka.log.LogSegmentTest)\n[error] Test Failed: testChangeFileSuffixes(kafka.log.LogSegmentTest)\nkafka.common.KafkaStorageException: Failed to change the log file suffix from  to .deleted for log segment 40\n----------------------\n[info] Test Starting: testPartitionReassignmentWithLeaderInNewReplicas(kafka.admin.AdminTest)\n[error] Test Failed: testPartitionReassignmentWithLeaderInNewReplicas(kafka.admin.AdminTest)\njunit.framework.AssertionFailedError: null\n        at junit.framework.Assert.fail(Assert.java:47)\n        at junit.framework.Assert.assertTrue(Assert.java:20)\n        at junit.framework.Assert.assertTrue(Assert.java:27)\n        at kafka.admin.AdminTest.testPartitionReassignmentWithLeaderInNewReplicas(AdminTest.scala:167)\n", "Moving the fix to post 0.8.2 for now.", "Resolving this since it's been open forever and it seems there are no plans to fix it, especially since we have changed build systems."], "derived": {"summary": "Trying to create a ZIP distro from sources failed. On Win7.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Some tests fail when building on a Windows box - Trying to create a ZIP distro from sources failed. On Win7."}, {"q": "What updates or decisions were made in the discussion?", "a": "Resolving this since it's been open forever and it seems there are no plans to fix it, especially since we have changed build systems."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-335", "title": "Implement an embedded controller", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Yang Ye", "labels": [], "created": "2012-05-07T17:09:12.000+0000", "updated": "2012-06-14T15:19:03.000+0000", "description": "This ticket will implement a controller as described in https://cwiki.apache.org/confluence/display/KAFKA/kafka+Detailed+Replication+Design+V3\n\nThis includes creating the controller path, setting up necessary watchers (e.g, Broker path and TopicPath), and failover the controller.", "comments": ["also refine the buffer size of BlockingChannel", "Thanks for the patch. Some comments:\n\n1. KafkaController:\n1.1 Typo satup\n1.2 Sometimes, multiple new lines are added between methods. Can we make this consistent?\n1.3 It's probably better to create a separate class ControllerChannelManager that manages all queues and RequestSendThreads. ControllerChannelManager will support the following API:\n  def sendRequest(brokerId : Int, request : RequestOrResponse, callback: (RequestOrResponse) => Unit = null)\n  def addBroker(brokerId: Int, broker: Broker)\n  def removeBroker(brokerId: Int)\n1.4 BrokerChangeListener.handleChildChange(): can we rename curChilds to javaCurChildren and scalaCurChildren to curChildren since the code is written in scala?\n1.5 We don't need to fill in the content of the listener in this jira. Just add a TODO comment.\n\n2. RequestSendThread:\n2.1 run(): We can't shutdown this thread if no request is added to the queue since queue.take() is blocking. We need to do something like ReplicaFetcherThread by interrupting the thread to make the thread shutdownable. \n\n3. KafkaZookeeper: It seems to me that it's better to move the controller election logic to KafkaController. On startup, KafkaController checks if it can become the controller. If yes, it registers the listeners on brokers and topics. Otherwise, it registers a listener on the controller path for controller failover. This way, all logic related to controller is self contained in KafkaController.\n\n4. BlockingChannel:\n4.1 useDefaultBufferSize => UseDefaultBufferSize\n4.2 remove new lines added after the constructor\n4.3 Today, we have logic in SyncProducer and SimpleConsumer to reconnect if there is any socket IO exception while sending/receiving data. We probably need something like that in RequestSendThread too. Instead of duplicating this logic. It's probably better to move this logic to BlockingChannel. I was thinking that instead of have a send and a receive API, BlockingChannel can just support 1 sendAndReceive api and implements the reconnect logic for socket exception.\n\n5. ZkUtils: getController and getTopicPartitionLeaderAndISR are not used anywhere.\n", "1. KafkaController:\n1.1 Typo satup\n-----> fixed\n\n1.2 Sometimes, multiple new lines are added between methods. Can we make this consistent?\n-----> fixed\n\n1.3 It's probably better to create a separate class ControllerChannelManager that manages all queues and RequestSendThreads. ControllerChannelManager will support the following API:\n  def sendRequest(brokerId : Int, request : RequestOrResponse, callback: (RequestOrResponse) => Unit = null)\n  def addBroker(brokerId: Int, broker: Broker)\n  def removeBroker(brokerId: Int)\n\n-----> done\n\n1.4 BrokerChangeListener.handleChildChange(): can we rename curChilds to javaCurChildren and scalaCurChildren to curChildren since the code is written in scala?\n\n-----> done\n\n1.5 We don't need to fill in the content of the listener in this jira. Just add a TODO comment.\n\n-----> the brokerListener is needed \n\n2. RequestSendThread:\n2.1 run(): We can't shutdown this thread if no request is added to the queue since queue.take() is blocking. We need to do something like ReplicaFetcherThread by interrupting the thread to make the thread shutdownable.\n\n-----> done\n\n3. KafkaZookeeper: It seems to me that it's better to move the controller election logic to KafkaController. On startup, KafkaController checks if it can become the controller. If yes, it registers the listeners on brokers and topics. Otherwise, it registers a listener on the controller path for controller failover. This way, all logic related to controller is self contained in KafkaController.\n\nSeems that's not applicable, talk in person\n\n4. BlockingChannel:\n4.1 useDefaultBufferSize => UseDefaultBufferSize\n-----> fixed\n\n4.2 remove new lines added after the constructor\n-----> fixed\n\n4.3 Today, we have logic in SyncProducer and SimpleConsumer to reconnect if there is any socket IO exception while sending/receiving data. We probably need something like that in RequestSendThread too. Instead of duplicating this logic. It's probably better to move this logic to BlockingChannel. I was thinking that instead of have a send and a receive API, BlockingChannel can just support 1 sendAndReceive api and implements the reconnect logic for socket exception.\n\n-----> Let me do it separately, is it ok?\n\n5. ZkUtils: getController and getTopicPartitionLeaderAndISR are not used anywhere. \n\n-----> removed in the patch", "Thanks for patch v2. Some more comments:\n\n21. ControllerChannelManager:\n21.1 remove unused imports\n21.2 allBrokers in constructor doesn't need to be val since it's only used in initialization\n21.3 Do ControllerChannelManager, RequestSendThread need to be nested under KafkaController? They don't seems to use any instance variable of KafkaController.\n21.4 RequestSendThread: There is still a problem with shutdown. If when shutdown() is called, the thread is waiting on queue.take(), then Stream.continually will break and throw an Interrupted exception and shutdown is handled properly. However, when shutdown() is called, the thread may be performing a non-blocking operation. In this case, the thread needs to check if it's interrupted, which it's not doing. The simplest thing to do, is to change Stream to a while loop that checks if an isRunning variable is true. Shutdown() will set the isRunning flag to false and send the interruption.\n\n3. Could you explain a bit more why the controller election logic can't be moved to KafkaController?\n", "For 4.3, could you create a separate jira to track this?", "21. ControllerChannelManager:\n21.1 remove unused imports\n\nDone\n\n21.2 allBrokers in constructor doesn't need to be val since it's only used in initialization\n\nNew it's var since it's modified during the running\n\n21.3 Do ControllerChannelManager, RequestSendThread need to be nested under KafkaController? They don't seems to use any instance variable of KafkaController.\n\nDone\n\n21.4 RequestSendThread: There is still a problem with shutdown. If when shutdown() is called, the thread is waiting on queue.take(), then Stream.continually will break and throw an Interrupted exception and shutdown is handled properly. However, when shutdown() is called, the thread may be performing a non-blocking operation. In this case, the thread needs to check if it's interrupted, which it's not doing. The simplest thing to do, is to change Stream to a while loop that checks if an isRunning variable is true. Shutdown() will set the isRunning flag to false and send the interruption.\n\n\nDone\n", "Thanks for patch v3. A few more comments:\n\n31. KafkaController:\n31.1 In general, the watcher for a ZK path needs to be subscribed before reading the path. Otherwise, some events on the path could be missed. So in starup(): registerControllerExistListener() needs to be done before tryToBecomeController. In tryToBecomeController(),  registerBrokerChangeListener() and registerTopicChangeListener() need to be called before reading brokers and topics from ZK.\n31.2 do we need 3 locks? We can probably just use 1 lock to synchronize all access to allBrokers and allTopics.\n31.3 tryToBecomeController(): need to add the initial set of brokers to ControllerChannelManager\n31.4 need a sessionExpiration listener so that it can clean state after the controller lost its registration (e.g., shut down ControllerChannelManager and  call tryToBecomeController).\n31.5 In tryToBecomeController, set controllerChannelManager to null if it can't become a controller.\n\n32. BrokerChangeListener.handleChildChange(): should remove deleted brokers from allBrokers.\n\n33. ControllerExistListener.handleDataChange(): If no logic is needed here, add a comment to make it clear.\n\n34. ZkUitls: remove extra new line after getTopicPartitionLeaderAndISR\n\n35. ControllerBasicTest: remove extra new lines btw methods\n\n36. RequestSendThread: Should we name the thread \"requestSendThread-brokerid\"?\n", "31. KafkaController:\n31.1 In general, the watcher for a ZK path needs to be subscribed before reading the path. Otherwise, some events on the path could be missed. So in starup(): registerControllerExistListener() needs to be done before tryToBecomeController. In tryToBecomeController(),  registerBrokerChangeListener() and registerTopicChangeListener() need to be called before reading brokers and topics from ZK.\n\nFixed this\n\n31.2 do we need 3 locks? We can probably just use 1 lock to synchronize all access to allBrokers and allTopics.\n\nIt's not necessary, so I replaced them with just one lock\n\n\n31.3 tryToBecomeController(): need to add the initial set of brokers to ControllerChannelManager\n\nI think we already added the initial set of brokers to the manager\n\n\n31.4 need a sessionExpiration listener so that it can clean state after the controller lost its registration (e.g., shut down ControllerChannelManager and  call tryToBecomeController).\n\nFixed this\n\n31.5 In tryToBecomeController, set controllerChannelManager to null if it can't become a controller.\nI don't think it's necessary, because the manager is always initialized as null, and only changed to not null if it becomes the controller\n\n\n32. BrokerChangeListener.handleChildChange(): should remove deleted brokers from allBrokers.\nFixed this\n\n\n33. ControllerExistListener.handleDataChange(): If no logic is needed here, add a comment to make it clear.\n\nFixed this\n\n34. ZkUitls: remove extra new line after getTopicPartitionLeaderAndISR\n\n\n\n35. ControllerBasicTest: remove extra new lines btw methods\nFixed it\n\n\n36. RequestSendThread: Should we name the thread \"requestSendThread-brokerid\"?\nFixed it", "setting the controllerConnectionManager to null at session expiration ", "A few more comments for v5 patch.\n\n41. KafkaController: There are a couple of corner cases.\n41.1 registerSessionExpirationListener() need to be called even if the controller is not active.\n41.2 if a broker can't become the controller, it needs to call registerControllerExistListener() to register the existence watcher.\n\n42. ControllerChannelManager: startup() should use broker, not allBrokers. Also, allBrokers shouldn't be val.", "41.1 registerSessionExpirationListener() need to be called even if the controller is not active.\n\nfixed it\n\n\n41.2 if a broker can't become the controller, it needs to call registerControllerExistListener() to register the existence watcher.\n\nfixed it\n\n\n42. ControllerChannelManager: startup() should use broker, not allBrokers. Also, allBrokers shouldn't be val. \n\nfixed it", "Thanks for the patch. Just committed to 0.8 branch."], "derived": {"summary": "This ticket will implement a controller as described in https://cwiki. apache.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Implement an embedded controller - This ticket will implement a controller as described in https://cwiki. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for the patch. Just committed to 0.8 branch."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-336", "title": "add an admin RPC to communicate state changes between the controller and the broker", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Yang Ye", "labels": [], "created": "2012-05-07T17:15:07.000+0000", "updated": "2012-06-06T01:31:11.000+0000", "description": "Based on the discussion in https://cwiki.apache.org/confluence/display/KAFKA/kafka+Detailed+Replication+Design+V3 , it's more efficient to communicate state change commands between the controller and the broker using a direct RPC than via ZK. This ticket will be implementing an admin RPC client for the controller to send state change commands.", "comments": ["Controller to Broker RPC added with unit test", "Thanks for the patch. Some comments:\n\n1. LeaderAndISRRequest\n1.1 isInit just needs 1 byte.\n1.2 remove the commented out code\n\n2. PartitionLeaderAndISRRequest\n2.1 zkVersion should be long.\n2.2 how about we name the class LeaderAndISR?\n2.3 remove unused imports\n\n3. ControllerToBorkerRequestTest: remove unused imports\n\n4. We will need a client wrapper like SimpleConsumer so that the controller can use to send the commands through RPC.\n\n5. On the server side, we need to add a new handler in KafkaApis that can handle the new requests. We also need to create new response classes for those commands. For this jira, the server can just send a dummy response.\n", "Awesome. Couple of thoughts:\n\n1. It would be good to seperate the tests into a serialization test that just serializes the request and deserializes it and a seperate test that actually sends to the server. We have found having the simple in-process test helpful since it is easier to debug.\n\n2. Ideally we should also add a compatability test that saves out some binary data to ensure we don't break compatability in the future by accident. Neha has details on this. We could wait and do this later when we have everything working in case we need to make changes.\n\n3. Broker is spelled as Borker\n\n4. Can you change HashMap[Tuple2[String, Int] in the method signature to Map[(String,Int)], means the same but is a little better style.\n\nI recommend we not create another SimpleConsumer wrapper, but instead make a base KafkaConnection that takes any API object, and use that to implement all the clients. These internal things then won't need a special wrapper they can just directly use the KafkaConnection. This also means that in the future if the consumer needs to send a produce request (either for offset storage or audit trail) it doesn't need a separate tcp connection.\n\nOne meta comment: these precisely versioned, compact, typed request objects are okay for public service apis because we should make them so rarely and the really important thing is compatability and efficiency. For these internal broker-to-broker apis should we instead just create a generic BrokerCommandRequest/Response which just holds a JSON string? This might be easier to evolve. I think scala has some (fairly junky) json parser included. This might be easier to evolve. Or maybe having this inconsistency is not worth it. It really depends on how many of these we think we will have.", "Never mind the KafkaConnection comment we already have a BlockingChannel which is essentially the same thing.", "Thanks for the new patch. Some comments:\n\nFrom previous review:\n#2.2. from my comment and #4 from Jay's comment are still not addressed.\n\nNew review comments:\n\n11. It seems that we haven't really documented the exact format of the new requests and the responses. I documented them in https://cwiki.apache.org/confluence/display/KAFKA/kafka+Detailed+Replication+Design+V3 . Does it make sense? If so, this would require some changes in this patch.\n\n12. KafkaZookeeper, KafkaApis: For this patch, we can just add a TODO comment in the handler in KafkaApis) and  simply send a dummy response (with eror_code = 0). We can remove the code in KafkaZookeeper since the handler probably shouldn't in KafkaZookeeper when it's actually put in.\n\n13. Are the changes in Partition and Replica still necessary if #12 is resolved?\n\nFor Jay's comment, for now, it seems that we just need to add a couple of new types of request. So, we can probably start with customized request objects.\n\n", "Updated the new request/response format proposal in https://cwiki.apache.org/confluence/display/KAFKA/kafka+Detailed+Replication+Design+V3, adding version_id, timeout, etc.", "Another comment:\n\n14. To facilitate a controller to send commands to individual brokers, it would useful to have some kind of ControllerCommandManager that has the following API:\nControllerCommandManager {\n\n    sendCommand(commandRequest: Request, brokerId: Int)\n}\n\nUnder the cover, the ControllerCommandManager can potentially maintain a queue per broker and have a separate send thread that gets command from the queue, sends the request using a BlockingChannel to the right broker and deserializes and checks the response.", "Thanks for patch V3. Some more comments.\n\n21. LeaderAndISRRequest:\n21.1 There is no need to put requestTypeId in the constructor. We know the constant of the request Id.\n21.2 There is no need to put requestId in write(). This will be done in BoundedByteBufferSend. See how a FetchRequest is serialized and sent to socket.\n21.3 The above comments apply to StopReplicaRequest too.\n21.4 In constructor, leaderAndISRInfos should be just a Map, not a mutable.HashMap.\n21.5 To be consistent, let's use leaderEpoc, instead of leaderGenId (see the updated protocol in the wiki).\n\n22. LeaderAndISRResponse:\n22.1 The patch doesn't follow the protocol design in the wiki. Is there a reason?\n\n23. Currently, for each type of Response, we have a customized ResponseSend object. Those ResponseSend objects all share the same pattern: writing a header and then the content. It would be good if we can consolidate those ResponseSend objects. The only exception is the response for Fetch request, which uses the sendfile API and has to be customized.\n\n24. KafkaApi:\nThe response shouldn't return dummyTopic. Instead, it should just return NoError for each (topic,partition) in the request.\n\n25. Unit test:\nLet's add a unit test that tests the end-to-end RPC protocol for the new requests, similar to BackwardsCompatibilityTest.testProtocolVersion0().\n\nIn the new patch, it would be great if you can respond to each of the review comments (whether you agree and have fixed it, or you have a different idea, etc).\n", "Fixed as part of kafka-349"], "derived": {"summary": "Based on the discussion in https://cwiki. apache.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "add an admin RPC to communicate state changes between the controller and the broker - Based on the discussion in https://cwiki. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed as part of kafka-349"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-337", "title": "upgrade ZKClient to allow conditional updates in ZK", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Jun Rao", "labels": [], "created": "2012-05-07T17:18:15.000+0000", "updated": "2012-06-20T01:32:50.000+0000", "description": "Based on https://cwiki.apache.org/confluence/display/KAFKA/kafka+Detailed+Replication+Design+V3 , there are a few cases that we need to update a ZK path conditionally (based on ZK version). We will need to upgrade to the latest version of ZKClient that exposes the version of a ZK path during writes.", "comments": ["Since zkclient hasn't published a new version in maven, directly including a new version of zkclient jar as of May 22, 2012, which exposes ZK version during writes.", "+1\n\nFor reference, here is the zkclient change for the updated writeData API.\n\nhttps://github.com/sgroschupf/zkclient/commit/e72d8fd682907004d0b1488e47d8fe55d405b751\n", "committed to 0.8"], "derived": {"summary": "Based on https://cwiki. apache.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "upgrade ZKClient to allow conditional updates in ZK - Based on https://cwiki. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "committed to 0.8"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-338", "title": "controller failover", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Yang Ye", "labels": [], "created": "2012-05-07T17:24:31.000+0000", "updated": "2012-08-01T16:16:15.000+0000", "description": "We want to implement that controller failover logic according to https://cwiki.apache.org/confluence/display/KAFKA/kafka+Detailed+Replication+Design+V3\n", "comments": ["Prashanth,\n\nThanks for signing up for this jira. One thing that I realized is that it may be a bit hard to do this jira and kafka-343 (moving leader election logic to controller) independently. This is because (1) a failed broker could embed a controller and therefore it's bit hard to separate out the testing of broker failure and controller failure; (2) some of the logic in controller failover can be shared with the leader election logic in controller. Since Yang already started working on kafka-343, it's probably better if he takes on this jira as well. Do you think that you could pick up one of the following jiras instead? Sorry about that.\n\nKAFKA-330: Delete topic support\nKAFKA-369: remove ZK dependency on producer\nKAFKA-288: rework java producer API\nKAFKA-289: reuse TopicData when sending produce request\nKAFKA-355: Move the highwatermark maintenance logic out of Log.\nKAFKA-356: create a generic thread class", "Committed as part of kafka-343"], "derived": {"summary": "We want to implement that controller failover logic according to https://cwiki. apache.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "controller failover - We want to implement that controller failover logic according to https://cwiki. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "Committed as part of kafka-343"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-339", "title": "using MultiFetch in the follower", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Jun Rao", "labels": [], "created": "2012-05-07T17:27:40.000+0000", "updated": "2012-06-23T07:18:29.000+0000", "description": "A broker could be following multiple topic/partitions from the broker. Instead of using 1 fetcher thread per topic/partition, it would be more efficient to use 1 fetcher thread that issues multi-fetch requests.", "comments": ["Uploaded patch v1. \n\nCreated an AbstractFetcher and AbstractFetcherManager, which contain the common code path for fetchers used in followers and real consumer clients. Added ReplicaFetcher and ReplicaFetcherManager (use multi-fetch) to replace ReplicaFetcherThread.\n\nWill reimplement the fetcher in consumer client based on AbstractFetcher and AbstractFetcherManager in a separate jira, kafka-362.", "Thanks for the patch. Some comments:\n\nAbstractFetcher:\n- currentOffset should never be empty, so we can get rid of the if.\n- hasPartition, partitionCount need to synchronize fetchMap\n- \"shutting down\" should probably be removed from the string on line 106\n- newOffset can be computed from the messageSet - so the processPartitionData implementation does not need to return the log end offset (and likewise when we use this in the high-level consumer). It's probably safer to prevent processPartitionData from overriding the new offset, and I don't see any benefit in allowing it to do so.\n\nAbstractFetcherManager:\n- addFetcher:\n  - can rename to maybeAddFetcher\n  - also, maybe we should move the info log on line 38 to the None case and print the fetcher id; and add another log for the other case saying there's already a fetcher.\n- What is the purpose of having a fetcher ID vs simply topic-partition?\n- Should synchronize fetcherRunnableMap in shutdown with mapLock\n\nReplicaManager:\n- Maybe some corner case that I'm missing, but makeFollower already passes in the new leaderBrokerId so why do we need to re-read from ZooKeeper (line 173)?\n\nReplicaFetchTest:\n- Ideally producer.close() should be before the waitUntilTrue\n- The condition function uses & instead of &&.\n- Also, instead of the hard-coded 60L I think it would be clearer and sufficient to do something like:\n  val expectedOffset = brokers.head.getLogManager...logEndOffset\n  assertEquals(brokers.size, brokers.count( broker => broker.getLogManager...logEndOffset == expectedOffset ))\n", "Thanks for the review. Attaching patch v2.\n\nAbstractFetcher:\n- currentOffset actually can be none. A fetcher can be removed after the multi-fetch request is made.\n\nAbstractFetcherManager:\n-- addFetcher actually always adds a fetcher, but not always creates a new fetcher thread. I see the naming is a bit confusing. Renamed AbstractFetcher to AbstractFetcherThread.\n-- Fetchermanager is maintaining 1 or more fetcherThreads per source broker. Be default, there is 1 fetcherThread per broker. However, for higher degree of parallelism, more fetcherThreads can be configured. A fetcher corresponds to the fetching from 1 partition of a topic. Multiple fetchers can be added to a fetcherThread.\n\nReplicaManager:\n-- We need to get the host/port from ZK for a given broker id. Such information should be cached. Will create a separate jira to address this issue.\n\nThe rest of of comments have been fixed.", "+1 on v2\n\nMinor comments:\n- AbstractFetcherManager: may be useful to log the fetcher-id for the topic/partition when adding the fetcher\n- ReplicaFetcherManager: miss-match -> mismatch\n- ReplicaManager: makeFollower: I still don't think we need to look up zk for the leader as the value passed in should be current.\n", "+1 on v2, assuming Joel's comments are addressed.", "Thanks for the review. Committed patch v2 with a minor improvement by testing multiple topics in ReplicaFetcherTest. \n\nAs for unnecessary ZK reads in ReplicaManager, this should be fixed as part of kafka-343 when moving the leader election logic to the controller."], "derived": {"summary": "A broker could be following multiple topic/partitions from the broker. Instead of using 1 fetcher thread per topic/partition, it would be more efficient to use 1 fetcher thread that issues multi-fetch requests.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "using MultiFetch in the follower - A broker could be following multiple topic/partitions from the broker. Instead of using 1 fetcher thread per topic/partition, it would be more efficient to use 1 fetcher thread that issues multi-fetch requests."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for the review. Committed patch v2 with a minor improvement by testing multiple topics in ReplicaFetcherTest. \n\nAs for unnecessary ZK reads in ReplicaManager, this should be fixed as part of kafka-343 when moving the leader election logic to the controller."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-340", "title": "Implement clean shutdown in 0.8", "status": "Closed", "priority": "Blocker", "reporter": "Jun Rao", "assignee": "Joel Jacob Koshy", "labels": ["bugs"], "created": "2012-05-15T18:19:27.000+0000", "updated": "2012-10-26T16:22:26.000+0000", "description": "If we are shutting down a broker when the ISR of a partition includes only that broker, we could lose some messages that have been previously committed. For clean shutdown, we need to guarantee that there is at least 1 other broker in ISR after the broker is shut down.", "comments": ["For this to happen, each broker needs to know the # of replicas in a partition. We can change the leaderAndISRRequest to pass this information from the controller to the broker.", "This may not be too difficult to implement, but tricky to use so I would\nlike to put down a more detailed description as that may help come up with\nmore subtleties and corner cases to account for. The actual invocation of\nshutdown (describe at end) needs to be thought through.\n\nThe existing shutdown hook provides a conventional shutdown approach and is\nconvenient from an operations perspective in that it can be easily scripted\n(just kill -s SIGTERM pid).  The problem with the existing shutdown logic which\nthis jira is meant to address is that it simply brings the broker down, which\ntakes partitions offline abruptly. This can lead to message loss during rolling\nbounces - e.g., consider a rolling bounce of brokers B1, B2. If a producer is\nconfigured with num-request-acks -1, and T1-P1 is on (B1, B2) and B2 is leader.\nIf B1 is bounced, it may fall out of the ISR for T1-P1. B2 continues to receive\nmessages to T1-P1 and commits them immediately. After B1 comes back up, if B2\nis bounced, then B1 would become the new leader, but its log would be truncated\nto the last HW which is smaller than the committed (and ack'd) offsets on B2.\n\nThe original proposal (to wait until there is at least one other broker in ISR\nbefore shutting down) is also not without issues. It could lead to long waits\nduring clean shutdown. Furthermore, it involves moving all the leaders on the\nbroker that is being shutdown to other brokers. Partitions that are moved later\nwill experience longer outages than the partitions earlier in the list. In\naddition to avoiding message loss, another goal in clean shutdown should be to\nkeep partitions as available as possible. Jun suggested another possible\napproach: before shutting down a broker, pre-emptively relinquish and move the\nleadership of partitions for which it is currently leader to another broker in\nISR. One benefit of this approach is that leader transition can be done\nserially, one partition at a time. So each partition will only be unavailable\nfor the period it takes to establish a new leader.\n\nThe actual process to shut down broker B may look something like this. (This\nassumes the command is issued to the current controller but can be adapted for\nthe other options outlined at the end.)\n\n- For each partition T-P that B is leader for:\n  - Move leadership to another broker that is in ISR. (So if |ISR| == 1\n    - meaning only B - and replication factor >= 2 then we have to wait.)\n  - Issue the leaderAndISR request to the updated list of brokers in\n    leaderAndISR.\n\n- For all partitions for which Bx is a follower:\n  - Issue a \"stopFetcher\" for T-P. (Explicitly stopping the fetchers helps\n    reduce the possibility of timeouts on producer requests, since not doing so\n    would cause B to remain in ISR longer.)\n  - Issue a leaderAndISR request to the updated list of brokers in\n    leaderAndISR.\n\nHere are a few options, for the actual clean-shutdown invocation. These are not\nmutually exclusive, so we can pick one for now and implement the others later\nif they make sense.\n\n- Option 1: JMX-operation on the Kafka-controller to clean shut-down a\n  specified broker. (If the controller fails during its execution, then the\n  operation will simply fail.) This could also be a JMX-operation on individual\n  brokers, but that would require individual brokers to forward the\n  \"relinquishLeadership\" request to the controller.\n- Option 2: The broker's shutdown handler sends a \"relinquishLeadership\"\n  request to the controller and awaits a response from the controller. See note\n  below on the effect of a failure in this step.\n- Option 3: Expose a new command port to issue a \"cleanShutdown brokerid\"\n  command to the controller. As above, this could be on individual brokers as\n  well. The command port would also be useful for other commands such as \"print\n  broker config\", \"dump stats\", etc.\n\nThe main disadvantage with Option 2 is that if the leader transition fails then\nyou cannot really retry - well, you could wait/retry until the transition is\nsuccessful, but then you would need to introduce a timeout. Introducing a\ntimeout has its own issues - e.g., what is a reasonable timeout; also you would\nbe forced to wait for the timeout during a full cluster shutdown. With Options\n1/3 the operation would just fail. (And if you *know* that you are doing a full\ncluster shutdown you can go ahead and use the SIGTERM-based shutdown.)\n\nTypical scenarios:\n- Rolling bounce. After a broker is bounced and comes back up it can take some\n  time for it to return to ISR (if it fell out). In this case, the\n  relinquishLeadership operation of the next broker in the bounce sequence may\n  fail but would succeed after the preceding broker comes back up and returns\n  to ISR.\n- Full cluster shutdown. In this case, relinquishLeadership would likely fail\n  for brokers that come later in the shutdown sequence. However, the user would\n  know this is a full cluster shutdown and just use the usual SIGTERM-based\n  shutdown.\n- Replication factor of 1: similar to full cluster shutdown.\n", "Short summary of this implementation of clean shutdown:\n\n- Shutdown is triggered through a JMX operation on the controller.\n- Steps during shutdown:\n  - Record the broker as shutting down in controller context.\n    - This set will contain all shutting brokers until they are actually\n      taken down. The liveBroker set will mask these (through the custom\n      getter/setter).\n  - Send a \"wildcard\" StopReplica request to the broker to stop its replica\n    fetchers. This will cause it to fall out of ISR sooner (as explained in\n    the previous comment.)\n  - Identify partitions led by the broker with replication factor > 1\n  - Transition leadership to another broker in ISR\n- Return the number of remaining partitions that are led by the broker.\n\nIn practice, the way you would do clean shutdown is:\n- Use the admin tool: ./bin/kafka-run-class.sh kafka.admin.ShutdownBroker\n  --broker <bid> --zookeeper <zkconnect>\n- If the shutdown status that it prints out is \"complete\" then it means\n  broker <bid> has stopped its replica fetchers, and does not lead any\n  partitions. In this case, send a SIGTERM to the Kafka process to actually\n  take down the broker.\n- If the shutdown status that is prints is \"incomplete\" then you may want to\n  wait a bit before retrying - which would typically make sense in a rolling\n  bounce.\n- If you are bringing down the entire cluster, you will eventually hit the\n  \"incomplete\" status - since there will be insufficient brokers to move the\n  partition leadership to. In this case the operator presumably knows the\n  situation and will proceed to do an \"unclean\" shutdown on the remaining\n  brokers.\n- If the jmx operation itself fails (say due to a controller failover),\n  simply retry.\n\nOther comments:\n\n- I initially thought to use boolean for handleStateChange, but needed to\n  query for the actual moved partition counts so did away with that.\n- Also, considered using a zkpath (instead of jmx), but did not do this\n  because we would effectively lock the zkclient event thread until all\n  partition leadership moves are attempted. In this implementation the\n  controller context's lock is relinquished after moving each partition.\n  Another benefit of jmx over the zkpath is that it is convenient to return\n  the shutdown status so there's no need for a follow-up status check.\n- For stopping the replica fetchers, I simply used a \"wildcard\" StopReplica\n  request - i.e., without any partitions listed. The broker will not get any\n  more leaderAndIsr requests (since it is no longer exposed under\n  liveBrokers) so the fetchers will not restart.\n- I added a slightly dumb unit test (in addition to local stand-alone\n  testing), but we will need a more rigorous system test for this.\n- Please let me know if you can think of corner cases to test for.\n", "Thanks for the patch, it looks very well thought out. A few questions/comments -\n\n1. KafkaController\n1.1 It seems like we might need to move the filter to the getter instead of the setter for liveBrokerIds. This is because if the shutting down broker list changes after setting the value for live brokers and reading it, the list of live brokers might either miss out alive brokers or include dead brokers.\n1.2 Since both alive and shutting down broker lists are set, you can use the set difference notation instead of individually reading and filtering out unwanted brokers. Not sure how sets are implemented under the covers in Scala, but my guess is that the set difference method is more efficient.\n1.3 Can we rename replicatedPartitionsBrokerLeads to replicatePartitionsThisBrokerLeads ?\n1.4 The leader movement sets the isr correctly for the partition by removing the current leader as well as any broker that is shutting down from the isr. Then, the replica is marked as offline which tries to remove the broker from the isr again. Let's ignore the fact that the OfflineReplica state change reads the isr from zookeeper since we are going to address that in a separate JIRA already and I don't think we should optimize too early here. What we can do is add a check to OfflineReplica state change that doesn't do the zookeeper write and rpc request if the replica is already not in the isr.\n1.5 In shutdownBroker,\n      controllerContext.shuttingDownBrokerIds.add(id)\n      controllerContext.liveBrokers = controllerContext.liveBrokers.filter(_.id != id)\nIt seems like the 2nd statement is not required since the shutting down brokers should be removed from the live brokers list anyways. This is another reason why moving it to the getter might be useful and safer.\n1.6 If the shutdownBroker API throws a runtime exception, what value is returned to the admin command ?\n1.7 In shutdownBroker, the controllerLock is acquired and released atleast four times. Probably you wanted to release the lock between the move for each partition in order to not block on an unimportant operation like broker shutdown. It is usually wise to release a lock for blocking operations to avoid deadlock/starvation. So, I didn't get why the lock is released for sending the stop replica request. It seems like the lock can be acquired up until the list of partitions to be moved is computed ? This acquire/release lock multiple times approach is always tricky, but right now after reading the code once, I'm not sure if this particular API would run into any race condition or not. So far, my intuition is that the only bad thing that can happen is we miss out on moving some leaders from the broker, which is not a critical operation anyway.\n\n2. StopReplicaRequest\n\n2.1 I think it is a little awkward to overload the stop replica request with no partitions to mean that it is not supposed to shutdown the replica fetcher thread. In the future, if we find a use case where we need to do this only for some partitions, we might need to change the stop replica request format. What do people think about adding a flag to the stop replica request that tells the broker if it should just shutdown the replica fetcher thread or delete the replica as well ?\n", "Thanks for the patch. Looks good overall. Some comments:\n\n10. KafkaController.shutdownBroker:\n10.1 Just stopping the follower in the broker to be shut down doesn't make it faster to fall out of leader's isr. This is because the leader will still need to wait for the timeout before dropping the broker out of the isr. The controller will need to shrink isr and send a leaderAndIsr request to each of the leaders. If we do this, there is probably no need for the wildcard stopReplica request.\n10.2 It's better to use partitionsToMove in the following statement. \n    debug(\"Partitions with replication factor > 1 for which broker %d is leader: %s\"\n          .format(id, replicatedPartitionsBrokerLeads.mkString(\",\")))\n\n11. IsrPartitionLeaderSelector:\n11.1 The name seems very general. Could we rename it to something like controlledShutdownLeaderElector?\n11.2 In the leader election logic, there is no need to make sure that the new leader is not the current leader. The customized controllerContext.liveBrokerIds should have filtered out the current leader (which is shut down by the jmx operation).\n\n12. StopReplicaRequest: Agree with Neha here. We need to add a flag to distinguish between the case that we just want to stop the replica and the case that we want to stop the replica and delete its data. The latter will be used in reassigning partitions (and delete topics in the future).\n\n", "Responses inlined. The other comments are addressed in the patch.\n\nNeha's comments:\n\n> 1. KafkaController\n> 1.1 It seems like we might need to move the filter to the getter instead of the setter for liveBrokerIds. This is\n>     because if the shutting down broker list changes after setting the value for live brokers and reading it, the list\n>     of live brokers might either miss out alive brokers or include dead brokers.\n\nGood catch - fixed this. I left the filter on the setter as well, although it isn't strictly needed. Also added a\nliveOrShuttingDownBrokerIds method as leaderAndIsr requests would otherwise only be sent to live brokers.\n\n> 1.2 Since both alive and shutting down broker lists are set, you can use the set difference notation instead of\n>     individually reading and filtering out unwanted brokers. Not sure how sets are implemented under the covers in\n>     Scala, but my guess is that the set difference method is more efficient.\n\nI would have, but the sets are of different underlying types (Broker vs. broker-id (int))\n\n> 1.3 Can we rename replicatedPartitionsBrokerLeads to replicatePartitionsThisBrokerLeads ?\n\nThe main reason I didn't name it with \"this\" is that it could mean either the shutting-down broker or the controller\nwhere this code executes. Ideally it should be called say, replicatedPartitionsShuttingDownBrokerLeads which seemed\ntoo verbose. Would partitionsToMove be better?\n\n> 1.4 The leader movement sets the isr correctly for the partition by removing the current leader as well as any broker\n>     that is shutting down from the isr. Then, the replica is marked as offline which tries to remove the broker from\n>     the isr again. Let's ignore the fact that the OfflineReplica state change reads the isr from zookeeper since we are\n>     going to address that in a separate JIRA already and I don't think we should optimize too early here. What we can\n>     do is add a check to OfflineReplica state change that doesn't do the zookeeper write and rpc request if the\n>     replica is already not in the isr.\n\nThat sounds good - made that change (in a helper function since I needed similar logic in the shutdown procedure).\n\n> 1.6 If the shutdownBroker API throws a runtime exception, what value is returned to the admin command ?\n\nThrows an exception - i.e. no return.\n\n> 1.7 In shutdownBroker, the controllerLock is acquired and released atleast four times. Probably you wanted to release\n>     the lock between the move for each partition in order to not block on an unimportant operation like broker\n>     shutdown. It is usually wise to release a lock for blocking operations to avoid deadlock/starvation. So, I didn't\n>     get why the lock is released for sending the stop replica request. It seems like the lock can be acquired up until\n>     the list of partitions to be moved is computed ? This acquire/release lock multiple times approach is always\n>     tricky, but right now after reading the code once, I'm not sure if this particular API would run into any race\n>     condition or not. So far, my intuition is that the only bad thing that can happen is we miss out on moving some\n>     leaders from the broker, which is not a critical operation anyway.\n\nI could refactor this a bit, but not sure it saves much. It is definitely subtle - i.e., potential interference with\nother operations (e.g., partition reassignment) when the lock is released. I did a pass of all usages of\nliveBrokerIds, liveBrokers and I think it's fine. If a partition is reassigned before the leadership moves that is\nalso fine as there is a check (if (currLeader == id)) before attempting moving leadership.\n\n> 2. StopReplicaRequest\n> 2.1 I think it is a little awkward to overload the stop replica request with no partitions to mean that it is not\n>     supposed to shutdown the replica fetcher thread. In the future, if we find a use case where we need to do this\n>     only for some partitions, we might need to change the stop replica request format. What do people think about\n>     adding a flag to the stop replica request that tells the broker if it should just shutdown the replica fetcher\n>     thread or delete the replica as well ?\n\nNot very sure if this is similar to Jun's comment. Instead of explicitly specifying all the partitions, not\nspecifying anything in the partition set meant \"all partitions\" (although I removed this in v2). I agree that it is\nhelpful to introduce a flag. Also see response to Jun's comment. Also, fixed a bug in addStopReplicaRequestForBrokers\nin the controller channel manager - basically the immutable set value wasn't getting updated.\n\nJun's comments\n\n> 10. KafkaController.shutdownBroker:\n> 10.1 Just stopping the follower in the broker to be shut down doesn't make it faster to fall out of leader's isr.\n>      This is because the leader will still need to wait for the timeout before dropping the broker out of the isr.\n>      The controller will need to shrink isr and send a leaderAndIsr request to each of the leaders.\n>      If we do this, there is probably no need for the wildcard stopReplica request.\n\nI think it helps to some degree - i.e., otherwise the replica on the shutting down broker will remain in ISR for the\npartitions it is following. Pre-emptively stopping the fetchers will encourage it to fall out (yes after the timeout)\nbut the shutdown operation itself could take some time.\n\nI like your suggestion of actively shrinking the ISR - made that change. I think we still need to do a\nstopReplicaFetcher in order to stop the fetchers (or it will re-enter ISR soon enough). Also, we need to send it a\nleaderAndIsr request for the partitions being moved (to tell it that it is no longer the leader). That would cause it\nto start up replica fetchers to the new leaders. So what I ended up doing is: after all the partition movements are\ncomplete, send a stop replica request, and update leaderAndIsr for all partitions present on the shutting-down broker\nthat removes it from isr.\n\nSomehow, I still think it is better to use StopReplica to force a \"full shutdown\" of the replica fetcher on the\nshutting down broker. Right now it is possible for a fetch request to be sent from the broker to the (new) leader\nat the same time the leaderAndIsr is shrunk to exclude the broker. The leader could then re-expand the ISR.\n\n> 11.2 In the leader election logic, there is no need to make sure that the new leader is not the current leader. The\n>      customized controllerContext.liveBrokerIds should have filtered out the current leader (which is shut down by the\n>      jmx operation).\n\nActually, I needed to change this to include shutting down brokers (since the leaderAndIsr request also needs to\nbe sent to the shutting down broker) so I still need to do check.\n\n> 12. StopReplicaRequest: Agree with Neha here. We need to add a flag to distinguish between the case that we just want\n>     to stop the replica and the case that we want to stop the replica and delete its data. The latter will be used\n>     in reassigning partitions (and delete topics in the future).\n\nDone. I went with a \"global\" delete partitions flag (as opposed to per-partition) in the request format. We can\ndiscuss whether we should make this a per-partition flag.\n", "Thanks for patch v2, looks good overall. Couple minor questions -\n\n3. KafkaController\n\n3.1 In maybeRemoveReplicaFromIsr, let's add a warn statement that says why the replica was not removed from the isr. There are 3 conditions when that happens, lets state which condition was satisfied. This will be very useful for debugging.\n3.2 If removing the replica from isr is unnecessary, why do we still have to send the leader and isr request to the leader ?\n3.3 In the OfflineReplica state change, since the leader doesn't change, we probably don't need to update the leader and isr cache too. Realize that this is not introduced in this patch, but will be nice to fix it anyway.\n3.4 What is the point of sending leader and isr request at the end of shutdownBroker, since the OfflineReplica state change would've taken care of that anyway. It seems like you just need to send the stop replica request with the delete partitions flag turned off, no ?\n\n4. PartitionLeaderSelector\n\nMinor logging correction - So far we have been using [%s,%d] convention for logging topic-partition. Let's change the following logging statement to do the same -\ndebug(\"Partition %s-%d : current leader = %d, new leader = %d\"\n\n5. StopReplicaRequest\n\nProbably makes sense to change warn to error or even throw exception to alert on an invalid byte in the stop replica request. This is pretty serious and probably points to corruption somewhere in the code\n", "Thanks for patch v2. Some more comments and clarification:\n\n20. ControllerContext.liveBrokers: We already filter out shutdown brokers in the getter. Do we still need to do the filtering at the setter?\n\n21. KafkaController.shutdownBroker():\n21.1 We probably should synchronize this method to allow only 1 outstanding shutdown operation at a time. Various maps in ControllerContext are updated in this method and they are not concurrent.\n21.2 My understanding is that this method returns the number of partitions whose leader is still on this broker. If so, shouldn't partitionsRemaining be updated after leaders are moved? Also, could you add a comment to this method to describe what it does and what the return value is?\n21.3 I think we need to hold the controller lock when calling maybeRemoveReplicaFromIsr.\n21.4 We should send stopReplica requests to all partitions on this broker, not just partitions who are followers, right?\n\n", "Thanks for the reviews. I did a lot of stand-alone testing, which I will continue while you review this. There are\nprobably several corner cases and bounce sequences that are yet to be tested. For e.g., an (albeit non-critical) caveat\nin not using zk to record the shutting down brokers is that on controller failover, the new controller will be unaware\nof the fact. E.g., say there are two brokers (0, 1), one topic with replication factor 2. 0 is the leader and\ncontroller. Shutdown command on broker 1. Shutdown command on broker 0 (which will give an incomplete status). At this\npoint only 0 is in leaderAndIsr. If you send a SIGTERM to broker 0, then broker 1 will become the leader again.\n\n> 3.2 If removing the replica from isr is unnecessary, why do we still have to send the leader and isr request to the\n>     leader ?\n\nFixed this. Also in v2, I had an additional requirement on removing a replica from ISR: if it is the only replica in\nISR, then it cannot be removed. i.e., you would then end up with a leaderAndIsr with a leader, but an empty ISR which\nI felt does not really make sense. Anyway, I did not really need to do this in v3 - when shutting down a broker, I only\nremove it from ISR if the partition leadership is not on the broker that is shutting down.\n\nWorking through this comment led me to wonder - when the last replica of a partition goes offline, the leaderAndIsr\npath isn't updated - i.e., the zk path still says the leader is \"x\" although \"x\" is offline. It is non-critical though\nsince clients won't be able to communicate with it, but maybe leader election should enter an invalid broker id in such\ncases.\n\n> 3.4 What is the point of sending leader and isr request at the end of shutdownBroker, since the OfflineReplica state\n> change would've taken care of that anyway. It seems like you just need to send the stop replica request with the delete\n> partitions flag turned off, no ?\n\nI still need (as an optimization) to send the leader and isr request to the leaders of all partitions that are present\non the shutting down broker so it can remove the shutting down broker from its inSyncReplicas cache\n(in Partition.scala) so it no longer waits for acks from the shutting down broker if a producer request's num-acks is\nset to -1. Otherwise, we have to wait for the leader to \"organically\" shrink the ISR.\n\nThis also applies to partitions which are moved (i.e., partitions for which the shutting down broker was the leader):\nthe ControlledShutdownLeaderSelector needs to send the updated leaderAndIsr request to the shutting down broker as well\n(to tell it that it is no longer the leader) at which point it will start up a replica fetcher and re-enter the ISR.\nSo in fact, there is actually not much point in removing the \"current leader\" from the ISR in the\nControlledShutdownLeaderSelector.selectLeader.\n\nJun's comments\n\nThanks for patch v2. Some more comments and clarification:\n\n> 20. ControllerContext.liveBrokers: We already filter out shutdown brokers in the getter. Do we still need to do the\n> filtering at the setter?\n\nIt is not required but doesn't hurt. So I had left it there so the filter on the getter has less elements to deal with.\nAlthough I don't think it matters at all to remove it from the setter - it's all in memory and we're dealing with very\nsmall sets. Anyway, removed it from the setter in v3.\n\n> 21. KafkaController.shutdownBroker():\n> 21.1 We probably should synchronize this method to allow only 1 outstanding shutdown operation at a time. Various maps\n> in ControllerContext are updated in this method and they are not concurrent.\n\nI think it should be fine, since those map modifications are guarded by the controller context lock. That said, adding\na shutdown lock should make it safer so I added a brokerShutdownLock. I don't think anything in there can block\nindefinitely (i.e., if there is, then we would need a way to cancel the operation).\n\n> 21.2 My understanding is that this method returns the number of partitions whose leader is still on this broker. If so,\n> shouldn't partitionsRemaining be updated after leaders are moved? Also, could you add a comment to this method to\n> describe what it does and what the return value is?\n\nYes - it is updated. i.e., invoking the method will give the most current value. That is why I needed to call it again\nat the end.\n\n21.3 I think we need to hold the controller lock when calling maybeRemoveReplicaFromIsr.\n\nThe controller context is not being accessed here - or am I missing something?\n\n21.4 We should send stopReplica requests to all partitions on this broker, not just partitions who are followers, right?\n\nCorrect - sorry, I don't fully recall the reason I had that filter in there. I think it might have been to avoid the\nissue described in 3.2 of Neha's comments - it was possible to end up with leaderAndIsr that had leader=0, isr = 1,\nwhich does not make sense. In any event, it is fixed now.\n\nOther comments are addressed.\n", "21.2 My confusion is that I thought replicatedPartitionsBrokerLeads is a val instead of a method. Could you change replicatedPartitionsBrokerLeads.toSet to replicatedPartitionsBrokerLeads().toSet to indicate that there is side effect? Also, could you add a comment to shutdownBroker() to describe what it does and what the return value is? \n\n21.3 Looked again. Yes, you are right. We don't need to lock there.\n\n21.4 The code still doesn't send stopReplicaRequests for partitions whose leader is on the broker to be shut down. This is not truly needed since the leader won't issue any fetch request. However, it would be better to stop those replicas too.", "21.2: sure will make those changes.\n\n21.4: We don't send stopReplicaRequests for partitions whose leader is *still* on the broker to be shut down - we could, but it probably doesn't matter and the existing behavior is a side-effect of the if condition that needs to be there. Say, there are two partitions (\"a\", \"b\") led by the broker, and \"c\" is some other partition that the broker is following. Suppose leadership of \"a\" is successfully moved to another broker, but leadership of \"b\" is not. In this case, the broker will get StopReplica requests for \"a\" and \"c\" but not \"b\" because of the \"if (controllerContext.allLeaders(topicAndPartition) != id) {\" to prevent removing it from the ISR which does not make sense because:\n(i) The partition/replica is still online (since leadership was not moved).\n(ii) In my comment for v2. I mentioned the possibility of a shutting down broker remaining in ISR. So suppose broker 0 was shutdown, the leader is now 1, and ISR is still 0,1. Now shutdown broker 1, then leader election will fail (because 0 is shutting down) - in which case I should not remove 1 from the ISR. (Without that guard we would end up with leader: 1, isr: 0 which does not make sense.\n", "21.4 I looked again. Yes, your code is correct.\n\nOne more minor comment.\n22. ControllerBrokerRequestBatch.sendRequestsToBrokers(): In m.foreach, instead of using r, could we use case(brokerId, partitionsToBeStopped)? Then we can refer to those names directly, instead of r._1 and r._2.\n\nOther than that, the patch looks good.", "Thanks for the review - committed to 0.8 with one more tweak: the bean should be registered only onControllerFailure and unregistered on session expiration.\n\nAlso, I'll file a separate follow-up jira as I think it would be better to use a call-back to ensure stop-replicas complete before shrinking ISR.\n", "Patch is committed."], "derived": {"summary": "If we are shutting down a broker when the ISR of a partition includes only that broker, we could lose some messages that have been previously committed. For clean shutdown, we need to guarantee that there is at least 1 other broker in ISR after the broker is shut down.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Implement clean shutdown in 0.8 - If we are shutting down a broker when the ISR of a partition includes only that broker, we could lose some messages that have been previously committed. For clean shutdown, we need to guarantee that there is at least 1 other broker in ISR after the broker is shut down."}, {"q": "What updates or decisions were made in the discussion?", "a": "Patch is committed."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-341", "title": "Create a new single host system test to validate all replicas on 0.8 branch", "status": "Resolved", "priority": "Major", "reporter": "John Fung", "assignee": "John Fung", "labels": [], "created": "2012-05-15T20:42:09.000+0000", "updated": "2012-06-14T16:24:05.000+0000", "description": null, "comments": ["This patch has been tested based on the following pre-requisites:\n1. Kafka 0.8 branch rev. 1342841\n2. KAFKA-46 patch applied\n\nThis test will do the following:\n1. 3 brokers are started up and have replicas enabled by default (no. of replicas should be less than or equal to no. of brokers)\n2. The logs will be parsed to find out which broker is the leader\n3. The leader will be stopped (currently commented out until KAFKA-350 is fixed)\n4. Producer will send out some messages\n5. Start the broker which is stopped in step 3 (currently commented out until KAFKA-350 is fixed)\n6. Repeat Step 2 for all 3 brokers.\n7. At the end of the test, it will compare the logs and validate the data file checksum", "Thanks for the patch. Some comments:\n\n1. Can we reuse kafka-run-class.sh in root/bin instead of making a copy in system_test/single_host_multi_brokers/bin?\n\n2. This test can just focus of the replication of a single cluster. We probably don't need to bundle the mirroring test here. So, no need to set up a target cluster.\n\n3. Can we print out either PASS or FAIL at the end of the test based on the validation that the test does?\n\n4. Could you write a README for the test so that people know how to run it and how to interpret the output?\n", "I remembered I also had some review comments lying around. \nJohn - I think it's great that we now have this system test to help\nwith 0.8 development. Brief comments - may have overlap with\nJun's:\n- kafka-run-class should be removed.\n- Add readme.\n- run-test.sh:\n  - typo on line 56 in var name, but doesn't hurt\n  - typo on line 65 in var name, but doesn't hurt\n  - line 94-97: cut is brittle for this purpose. Better to use sed/awk.\n  - There are a lot of util functions such as info, kill_child_processes\n    that we use in all our scripts. It would be good to factor these out\n    into a common script and source it from the system tests.\n- No need any references to any target cluster. E.g., zk_target,\n  kafka_target, etc. Likewise, should rename *_source_cluster to simply\n  *_cluster.\n", "Hi Jun, Joel,\n\nThanks for reviewing this patch. A v2 patch is attached to this JIRA. The following changes are made according to your suggestions.\n\n\n** Changes not made from Review Suggestions\n\n1. An extra copy of kafka-run-class.sh is added to the bin directory of this specific test folder because a customized copy of log4j.properties is required to enable some debugging code in ConsoleConsumer and ProducerPerformance for checksum validation\n\n\n** Changes made from Review Suggestions\n\n1. Move common functions to a separate script at system_test\\common\\util.sh\n\n2. Fixed typos (line 56 & 65 in previous version)\n\n3. Rename *_source_cluster to *_cluster\n\n4. README added\n\n5. PASS or FAIL validation is added at the end of the test\n\n6. Target cluster related code removed\n\n7. All the \"cut\" are replaced with \"awk\"\n\n\n** Other changes made\n\n1. The script will check the no. of brokers required and generate the corresponding no. of server.properties files\n", "Thanks for patch v2. I kept getting the following exception. Do you know why?\n\n2012-06-12 09:24:23 creating topic [mytest] on [localhost:2181]\ncreation failed because of org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/ids\norg.I0Itec.zkclient.exception.ZkNoNodeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/ids\n\tat org.I0Itec.zkclient.exception.ZkException.create(ZkException.java:47)\n\tat org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:685)\n\tat org.I0Itec.zkclient.ZkClient.getChildren(ZkClient.java:413)\n\tat org.I0Itec.zkclient.ZkClient.getChildren(ZkClient.java:409)\n\tat kafka.utils.ZkUtils$.getChildren(ZkUtils.scala:354)\n\tat kafka.utils.ZkUtils$.getSortedBrokerList(ZkUtils.scala:69)\n\tat kafka.admin.CreateTopicCommand$.createTopic(CreateTopicCommand.scala:87)\n\tat kafka.admin.CreateTopicCommand$.main(CreateTopicCommand.scala:72)\n\tat kafka.admin.CreateTopicCommand.main(CreateTopicCommand.scala)\nCaused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/ids\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:102)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:42)\n\tat org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1249)\n\tat org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1277)\n\tat org.I0Itec.zkclient.ZkConnection.getChildren(ZkConnection.java:99)\n\tat org.I0Itec.zkclient.ZkClient$2.call(ZkClient.java:416)\n\tat org.I0Itec.zkclient.ZkClient$2.call(ZkClient.java:413)\n\tat org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:675)\n\t... 7 more\n\nAlso, the test seems to be a bit long since there are a few places where we sleep for 10 or 30 seconds. Can those sleep be reduced?", "Uploaded kafka-341-v3.patch in which more comments are added to common/util.sh", "Uploaded kafka-341-v4.patch to fix the followings:\n1. reduced sleep time\n2. fixed the way to count checksum in producer log", "Hi Jun,\n\nI just uploaded a patch \"kafka-341-v4.patch\" which reduces sleep time. I also tried this patch with the latest 0.8 branch and I couldn't reproduce the exception you saw in ZK. Could you please try again with this latest patch?\n\nThanks,\nJohn", "For sleep time, could we make it more adaptive? Basically, we can periodically check whether certain condition (so that we can do the verification) is met until some max time is reached. That way, we don't have to sleep for the max time in the normal case.\n\nOtherwise, the patch looks fine. The ZK exception was due to some old brokers running in the background on my machine.", "Uploaded kafka-341-v5.patch based on Jun's suggestion to reduce the sleep time.\n\nThe change in this patch is in the validation stage, the script will compare the replicas' data sizes after a sleep of 5 sec repeatedly up to 30 sec max.", "John, thanks for the patch. Just committed to 0.8."], "derived": {"summary": "", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Create a new single host system test to validate all replicas on 0.8 branch"}, {"q": "What updates or decisions were made in the discussion?", "a": "John, thanks for the patch. Just committed to 0.8."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-342", "title": "revisit the broker startup procedure according to V3 design", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Yang Ye", "labels": [], "created": "2012-05-15T22:58:09.000+0000", "updated": "2012-08-01T16:16:47.000+0000", "description": "We need to change the implementation based on https://cwiki.apache.org/confluence/display/KAFKA/kafka+Detailed+Replication+Design+V3", "comments": ["Hi all, everybody okay if I take this on?", "Prashanth,\n\nThanks for volunteering. It turns out that the broker startup logic didn't change btw V2 and V3 design. So, this jira is no longer needed. Would you be interested in working on one of the following jiras instead?\n\nKAFKA-338: controller failover\nKAFKA-369: remove ZK dependency from producer client\n", "Sure, I can look at KAFKA-338.  Thanks, Jun.", "Committed as part of kafka-343"], "derived": {"summary": "We need to change the implementation based on https://cwiki. apache.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "revisit the broker startup procedure according to V3 design - We need to change the implementation based on https://cwiki. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "Committed as part of kafka-343"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-343", "title": "revisit the become leader and become follower state change operations using V3 design", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Yang Ye", "labels": [], "created": "2012-05-15T23:00:57.000+0000", "updated": "2012-08-06T18:19:18.000+0000", "description": "We need to reimplement become leader/follower using the controller model described in https://cwiki.apache.org/confluence/display/KAFKA/kafka+Detailed+Replication+Design+V3", "comments": ["following description is incomplete but I will fill them with more comprehensive descriptions. Also the patch may still be prone to some bug. I shall work on fixing in.\n\nnotes:\n1. ZkUtils.scala\n1.1 change the return type of readData() and readDataMaybeNull() from \"String\" to (Stat, String)\n1.2 add a \"conditionalUpdatePersistentPath()\" function\n1.3 Change the return type from \"String\" to \"Int\" for partitions and replicas in a few utility functions\n1.4 Add some utility functions like \"getReplicasForPartition\", \"getReplicaAssignmentForTopics\", \"getPartitionLeaderAndISRFroTopics\", \n\"getPartitionAssignmentForTopics\", etc. \n1.5 change the return type of \"updatePersistentPath\" from \"null\" to \"Int' which corresponds to the new zkPath version\n\n2. KafkaServer.scala\n2.1 Order of startup: \nsocketServer first, kafkaZookeeper second, replicaManager 3rd, KafkaController 4th, KafkaApis 5th\n2.2 Change the type of \"stopReplica\", \"makeLeader\", \"makeFollower\" to \"Short\" which corresponds to error code\n\n3. ReplicaManager.scala\n3.1 add a StopReplica() function\n3.2 Expose the \"allReplicas\" filed to outside (was private before) for usage in \"KafkaApis\"\n\n4. kafkaController.scala\n4.1 Basically implement the failover logic according to the design\n\n5 kafkaZookeeper.scala:\n5.1 Migrate the leader election logic\n5.2 add a \"initLocalReplica\" function for startup.\n\n6. KafkaApis.scala:\nImplement the \"handleLeaderAndISRRequest\" and \"handleStopReplicaRequest\" function\n\n7. LeaderAndISRRequest.scala:\nChange the zkPathVersion from \"Long\" type to \"Int\" ", "Thanks for the patch. Overall, the patch looks promising. Some comments:\n\n1. KafkaController:\n1.1 Is the following global import needed since it's imported in a nested class already?\n   import collection.JavaConversions._\n1.2 ControllerChannelManager: brokers in allBrokers are added twice in the constructor.\n1.3 BrokerChangeListener: \n  If you want to do sth for each item in a collection but don't need an output collection, use collection.foreach instead of collection.map. For example, the following usage of map should be changed to foreach.\n        addedBrokersSeq.map(controllerChannelManager.addBroker(_))\n        deletedBrokerIds.map(controllerChannelManager.removeBroker(_))\n1.4 onBrokerChange():\n1.4.1 We should use cached eplica assignment allTopicPartitionAssignment in controller, instead of re-reading from ZK on every broker failure.\n1.4.2 Instead of sending a leaderAndISRRequest per topicPartition, we should batch them and send only 1 leaderAndISRRequest per broker for each onBrokerChange() call.\n1.4.3 If no assigned replica is in liveBrokerIds, we should log an error for the affected topic/partition.\n1.4.4 Instead of using curZkPathVersion + 1 as the new version of a ZK path, we should use the new ZK version returned in conditionalUpdate. There is no guarantee that the new version is always curZkPathVersion + 1.\n1.4.5 We should add an info level log for the final leaderAndISRRequest that we send to each broker.\n1.4.6 If we select a new leader that's not in ISR, we should log a warning and indicate that it can cause potential data loss.\n1.4.7 ZkUtils.getLeaderAndISRForPartition may return none. We need to handle it.\n1.5 TopicChangeListener.handleChildChange(): We only need to update allTopicPartitionAssignment for the new and deleted topics. \n1.6 handleNewTopics(): We should use allTopicPartitionAssignment, instead of reading from ZK.\n1.7 allTopicPartitionAssignment: the replicas should be a list, instead of a set. This is because the ordering of the replicas is important and the 1st replica is the preferred one. We won't make use of it in this jira, but will be in the future.\n1.8 initLeaders():\n1.8.1 We should log a warning if no assigned replica is live.\n1.8.2 Similar to onBrokerChange (), we should try to batch LeaderAndISRRequest to each broker.\n1.8.3 We should set noInit in LeaderAndISRRequest. IsInit will only be used in leaderAndISRRecovery().\n1.9 in all ZK event handlers, let's add try/catch for all throwables and log them.\n1.10 leaderAndISRRecovery(): we should try to batch LeaderAndISRRequest to each broker.\n1.11 tryToBecomeController(): I would add explicitly return keyword for each of the return statements to make things clear since they are not the last one in the function.\n1.12 handleDeletedTopics():\n1.12.1 delete partitions are already removed from allTopicPartitionAssignment. So we need to pass the assignment in.\n1.12.2 we should try to batch stopReplicaRequest to each broker.\n \n2. KafkaApis.handleLeaderAndISRRequest(): don't need to check if(leaderAndISR.ISR.contains(brokerId)). This allows the controller to bootstrap a new replica (from changing #partitions or partition assignments).\n\n3. LeaderAndISRRequest: It's better to change isInit to boolean and send the right byte under the cover.\n\n4. KafkaZookeeper.initLocalReplicas(): should remove local replicas no longer assigned to this broker. Also, this function needs to be called before registerBrokerInZk(). Otherwise, a deleted topic can be recreated and assigned to this broker before local replica is cleaned up first.\n\n5. AdminUtils: Remove the following line:\n  scala.Seq\n\n6. KafkaRequestHandler: Doesn't seem to require any change.\n\n7. LeaderAndISRRequest: It seems that CurrentVersion is more appropriate than InitialLeaderAndISRRequestVersion since the version could evolve in the future.\n\n8. Partition.updateISR(): This function is used in 3 places: ReplicaManager.makeLeader, maybeShrinkISR and recordFollowerPosition. In the first case, we only need to update the ISR in cache since we know the controller already updated the ISR in ZK. However, the latter 2 cases are initiated by the leader. So we need to update ISR in both cache and ZK. So, we will need 2 versions of updateISR(), one just updating the cache, another updating both cache and ZK.\n\n9. Replica.isLeader(): need to handle the case when leader doesn't exist\n\n10. ReplicaManager:\n10.1 The indentation for some logging statements cross 2 lines is wrong.\n10.2 stopReplica: The deletion of a log should probably be done in LogManager with a new function deleteLog.\n\n11. StopReplicaRequest: It seems that CurrentVersion is more appropriate than InitialVersion since the version could evolve in the future.\n\n12. ControllerBasicTest: Why is the test commented out?\n\n13. LeaderElectionTest: why is testEpoch() removed?\n\n14. log4j.properties: Is the change just for debugging?\n", "In response to last review:\n\n1. KafkaController:\n1.1 Is the following global import needed since it's imported in a nested class already?\n   import collection.JavaConversions._\n\nFixed\n\n1.2 ControllerChannelManager: brokers in allBrokers are added twice in the constructor.\n\n\nFixed\n\n1.3 BrokerChangeListener:\n  If you want to do sth for each item in a collection but don't need an output collection, use collection.foreach instead of collection.map. For example, the following usage of map should be changed to foreach.\n        addedBrokersSeq.map(controllerChannelManager.addBroker(_))        deletedBrokerIds.map(controllerChannelManager.removeBroker(_))\n\n\nFixed\n\n1.4 onBrokerChange():\n1.4.1 We should use cached eplica assignment allTopicPartitionAssignment in controller, instead of re-reading from ZK on every broker failure.\n\nFixed\n\n1.4.2 Instead of sending a leaderAndISRRequest per topicPartition, we should batch them and send only 1 leaderAndISRRequest per broker for each onBrokerChange() call.\n\nFixed\n\n1.4.3 If no assigned replica is in liveBrokerIds, we should log an error for the affected topic/partition.\n\nFixed\n\n1.4.4 Instead of using curZkPathVersion + 1 as the new version of a ZK path, we should use the new ZK version returned in conditionalUpdate. There is no guarantee that the new version is always curZkPathVersion + 1.\n\nFixed\n\n1.4.5 We should add an info level log for the final leaderAndISRRequest that we send to each broker.\n\nFixed\n\n1.4.6 If we select a new leader that's not in ISR, we should log a warning and indicate that it can cause potential data loss.\n\nFixed\n\n1.4.7 ZkUtils.getLeaderAndISRForPartition may return none. We need to handle it.\n\ndid that\n\n\n1.5 TopicChangeListener.handleChildChange(): We only need to update allTopicPartitionAssignment for the new and deleted topics.\n\ndid that\n\n\n1.6 handleNewTopics(): We should use allTopicPartitionAssignment, instead of reading from ZK.\n\nfixed\n\n1.7 allTopicPartitionAssignment: the replicas should be a list, instead of a set. This is because the ordering of the replicas is important and the 1st replica is the preferred one. We won't make use of it in this jira, but will be in the future.\n\nfixed\n\n1.8 initLeaders():\n1.8.1 We should log a warning if no assigned replica is live.\nfixed\n\n1.8.2 Similar to onBrokerChange (), we should try to batch LeaderAndISRRequest to each broker.\n\nfixed\n\n\n1.8.3 We should set noInit in LeaderAndISRRequest. IsInit will only be used in leaderAndISRRecovery().\nfixed\n\n1.9 in all ZK event handlers, let's add try/catch for all \nthrowables and log them.\n\nMaybe not necessary because ZkClient already did that\n\n1.10 leaderAndISRRecovery(): we should try to batch LeaderAndISRRequest to each broker.\nfixed\n\n\n1.11 tryToBecomeController(): I would add explicitly return keyword for each of the return statements to make things clear since they are not the last one in the function.\n\nfixed\n\n1.12 handleDeletedTopics():\n1.12.1 delete partitions are already removed from allTopicPartitionAssignment. So we need to pass the assignment in.\n\nfixed\n\n1.12.2 we should try to batch stopReplicaRequest to each broker.\n \nfixed\n\n2. KafkaApis.handleLeaderAndISRRequest(): don't need to check if(leaderAndISR.ISR.contains(brokerId)). This allows the controller to bootstrap a new replica (from changing #partitions or partition assignments).\n\n3. LeaderAndISRRequest: It's better to change isInit to boolean and send the right byte under the cover.\n\nDid it\n\n4. KafkaZookeeper.initLocalReplicas(): should remove local replicas no longer assigned to this broker. Also, this function needs to be called before registerBrokerInZk(). Otherwise, a deleted topic can be recreated and assigned to this broker before local replica is cleaned up first.\n\nFixed as we discussed\n\n5. AdminUtils: Remove the following line:\n  scala.Seq\n\ndid it\n\n6. KafkaRequestHandler: Doesn't seem to require any change.\n\n7. LeaderAndISRRequest: It seems that CurrentVersion is more appropriate than InitialLeaderAndISRRequestVersion since the version could evolve in the future.\n\nfixed it\n\n\n8. Partition.updateISR(): This function is used in 3 places: ReplicaManager.makeLeader, maybeShrinkISR and recordFollowerPosition. In the first case, we only need to update the ISR in cache since we know the controller already updated the ISR in ZK. However, the latter 2 cases are initiated by the leader. So we need to update ISR in both cache and ZK. So, we will need 2 versions of updateISR(), one just updating the cache, another updating both cache and ZK.\n\nDid that\n\n9. Replica.isLeader(): need to handle the case when leader doesn't exist\n\n10. ReplicaManager:\n10.1 The indentation for some logging statements cross 2 lines is wrong.\nfixed them\n\n\n10.2 stopReplica: The deletion of a log should probably be done in LogManager with a new function deleteLog.\n\ndid that\n\n\n11. StopReplicaRequest: It seems that CurrentVersion is more appropriate than InitialVersion since the version could evolve in the future.\nFixed it\n\n12. ControllerBasicTest: Why is the test commented out?\n\nAdded it back\n\n13. LeaderElectionTest: why is testEpoch() removed?\n\nnow it's mixed with  \"testLeaderElection\" \n\n\n14. log4j.properties: Is the change just for debugging? \nchanged back\n\nreturned it back\n\n\n", "Thanks for patch v2. It's in a much better shape now. Some new comments:\n\n20. KafkaController:\n20.1 tryToBecomeController(): It seems that we don't need to first call ZkUtils.readDataMaybeNull and check if the controller exists or not. Instead, we can just call ZkUtils.createEphemeralPathExpectConflict directly.\n20.2 allTopicPartitionAssignment and allPartitionReplicaAssignment are representing the same data in a slightly different form. Can we just keep one of them?\n20.3 leaderAndISRRecovery(): We need to turn on the Init flag for LeaderAndISRRequest since this method is the first request sent during controller failover. Also, instead of including all partitions in the request, we should only include partitions assigned to each broker.\n20.4 initLeaders(): Instead of sending the same leaderAndISRRequest to each broker, we should only send to a broker partitions assigned to it.\n20.5 onBrokerChanges:\n20.5.1 It seems that the logic for handling new brokers in the same as leaderAndISRRecovery() and we can just reuse the logic.\n20.5.2 liveBrokerIds is allBrokerIds.\n20.5.3 Instead of trying to elect the leader of all partitions from ZK, the controller should cache the current leader of each partition and only try to elect the leader for partitions whose current leader is no long alive. This will save the # of ZK reads during broker failure.\n20.5.4 similar to  initLeaders(), instead of sending the same leaderAndISRRequest to each broker, we should only send to a broker partitions assigned to it.\n\n21. BrokerChangeListener:\n21.1 handleChildChange(): should we remove the TODO comment?\n21.2 handleDeletedTopics(): similar to  initLeaders(), instead of sending the same StopReplicaRequest to each broker, we should only send to a broker partitions assigned to it.\n\n22. KafkaApis.handleLeaderAndISRRequest(): If the IsInit flag is on, we should just call stopReplicaCbk to remove partitions that are to be deleted, instead of rewriting the logic already in stopRelicaCbk.\n\n23. KafkaZookeeper: remove unused imports\n\n24. LeaderAndISR: Just to be consistent with ZK versioning, should initialLeaderEpoc start from 0?\n\n25. Log.deleteWholeLog() is not needed since there is already LogManager.deleteLog\n\n26. Replica: isLeader() is not used.\n\n27. ReplicaManager.maybeShrinkISR(): fix the indentation of the closing bracket\n\n28. StopReplicaRequest: DefaultAckTimeout: 1000 ms seems too long. How about 100ms?\n\n29. FetcherTest: tearDown(): there is no need to call fetcher.stopAllConnections() since fetcher.shutdown() does that already.\n\n30. LeaderElectionTest.testLeaderElectionAndEpoch(): instead of adding sleep, could we change waitUntilLeaderIsElected so that it waits until the leader is a live broker. If you feel this is better handled in a separate jira, that fine too. Just create a new jira and provide enough details there.\n\n ", "20. KafkaController:\n20.1 tryToBecomeController(): It seems that we don't need to first call ZkUtils.readDataMaybeNull and check if the controller exists or not. Instead, we can just call ZkUtils.createEphemeralPathExpectConflict directly.\n\ndid that\n\n20.2 allTopicPartitionAssignment and allPartitionReplicaAssignment are representing the same data in a slightly different form. Can we just keep one of them?\n\nDid that\n\n20.3 leaderAndISRRecovery(): We need to turn on the Init flag for LeaderAndISRRequest since this method is the first request sent during controller failover. Also, instead of including all partitions in the request, we should only include partitions assigned to each broker.\n\ndid it\n\n\n20.4 initLeaders(): Instead of sending the same leaderAndISRRequest to each broker, we should only send to a broker partitions assigned to it.\n\nchanged\n\n\n20.5 onBrokerChanges:\n20.5.1 It seems that the logic for handling new brokers in the same as leaderAndISRRecovery() and we can just reuse the logic.\n\nmerged into a function \"recoverLeaderAndISRFromZookeeper\"\n\n20.5.2 liveBrokerIds is allBrokerIds.\n\nfixed\n\n20.5.3 Instead of trying to elect the leader of all partitions from ZK, the controller should cache the current leader of each partition and only try to elect the leader for partitions whose current leader is no long alive. This will save the # of ZK reads during broker failure.\n\nfixed\n\n\n20.5.4 similar to initLeaders(), instead of sending the same leaderAndISRRequest to each broker, we should only send to a broker partitions assigned to it.\n\nfixed\n\n21. BrokerChangeListener:\n21.1 handleChildChange(): should we remove the TODO comment?\nremoved\n\n21.2 handleDeletedTopics(): similar to initLeaders(), instead of sending the same StopReplicaRequest to each broker, we should only send to a broker partitions assigned to it.\nfixed\n\n\n22. KafkaApis.handleLeaderAndISRRequest(): If the IsInit flag is on, we should just call stopReplicaCbk to remove partitions that are to be deleted, instead of rewriting the logic already in stopRelicaCbk.\n\nfixed\n\n\n23. KafkaZookeeper: remove unused imports\nfixed\n\n24. LeaderAndISR: Just to be consistent with ZK versioning, should initialLeaderEpoc start from 0?\n\nchanged to start from 0\n\n25. Log.deleteWholeLog() is not needed since there is already LogManager.deleteLog\n\nThis is not a problem since LogManager.deleteLog calls Log.deleteWholeLog()  internally \n\n\n26. Replica: isLeader() is not used.\nremoved\n\n27. ReplicaManager.maybeShrinkISR(): fix the indentation of the closing bracket\nfixed\n\n28. StopReplicaRequest: DefaultAckTimeout: 1000 ms seems too long. How about 100ms?\nchagned\n\n29. FetcherTest: tearDown(): there is no need to call fetcher.stopAllConnections() since fetcher.shutdown() does that already.\n\nchanged\n\n30. LeaderElectionTest.testLeaderElectionAndEpoch(): instead of adding sleep, could we change waitUntilLeaderIsElected so that it waits until the leader is a live broker. If you feel this is better handled in a separate jira, that fine too. Just create a new jira and provide enough details there. \n\nI'm going to open a new jira for it.", "Sorry for coming late to this, I'd like to review this tomorrow. ", "Thanks for patch v3. A few more comments:\n\n40. RequestSendThread: brokerId is not used. If the intention is to have another broker id representing the source, it probably should be named fromBrokerId.\n\n41. KafkaController:\n41.1 recoverLeaderAndISRFromZookeeper(), initLeaders(): brokerToLeaderAndISRInfosMap.get(b).get can just be  brokerToLeaderAndISRInfosMap(b)\n41.2 recoverLeaderAndISRFromZookeeper: brokerIds is not used, but should be.\n41.3 onBrokerChange(): We should batch-send the leaderAndISRRequest to each broker.\n\n42. SimpleConsumer,Cluster: no change needed.\n\nAlso, just to clarify for other reviewers. The code for broker startup changes a bit from the v3 design. In the v3 design, a broker will on startup, (1) read all existing topics from ZK, (2) delete those local topics not in ZK, and then (3) register itself in ZK. However, a topic can be deleted between steps (1) and (3). Since the deletion of those topics won't be communicated from the controller (since the broker wasn't registered then), those deleted topics may not be cleaned up in the broker. In this patch, on startup, a broker just registers itself in ZK. On detecting a new broker, the controller will first send the full list of current topics to the new broker with the isInit flag on. On receiving the request from the controller, the broker can remove local topics not in the request. This avoids the problem in v3 design.", "Here are some initial review comments. \n\n1. ZkUtils\n1. Remove the debug statement in getEpochForPartition that says âget data, see mâ. Debug statements should be carefully added and should be added to be useful during debugging.\n2. Another debug statement that can be improved - \n3. âCheck the leaderEpocAndISR raw string:â. Can you please change this to something like trace(âThe leader and isr info %s read from zk path %sâ)\n4. Change debug(âthe leader....â) to debug(âLeader %d, Epoch %d, isr %s, zk path version %d for topic %s and partition %dâ). Without knowing the topic and partition, this debug statement will not be useful \n5. Overall, change any log statement that says âget this and check thatâ. For example, getLeaderForPartition() \n6. Change epoc to epoch everywhere in your code\n7. In getPartitionLeaderAndISRForTopics, the input is a list of topics for which leader and isr is to be fetched, but in the output, some partitions for which leader and isr is not available are missing from the return value. This is hard to understand from the caller's POV. Either it should return None or throw an exception so that the caller can handle Option value or an exception to know that some partitions don't have leader and isr info.\n8. What is the value of change readData to readDataMaybeNull in getBrokerInfoFromIds ? Basically you are returning invalid Broker objects with null broker info from that API, which doesn't seem useful to the caller. The fix is same as above. Either throw exception (like before) or change it to return Option.\n\n2. KafkaController.scala\n1. Rename recoverLeaderAndISRFromZookeeper to getLeaderAndISRFromZookeeper\n2. In recoverLeaderAndISRFromZookeeper () API, get is blindly invoked on an Option variable returned from allPartitionReplicaAssignment. Scala options force us to handle invalid/missing values cleanly, please use Option correctly everywhere in your code.\n3. The following info statement is unclear - âOn leaderAndISR recover, ....â. Please change it to âAfter reading leader and ISR from zookeeper, ...â\n4. What is the purpose of leaderAndISRRecovery since it just calls recoverLeaderAndISRFromZookeeper anyways ? Let's remove this API\n5. All info statements in controllerRegisterOrFailover() API are unclear. Please follow the info statements in leaderElection() in KafkaZookeeper(the code you deleted) and fix these. \n\n3. Partition.scala\n1. Change updateISR back to take in an optional zkclient instead of null and a separate boolean value. That way it makes it easier to handle a missing zk client instead of running into NPE\n2. Parentheses should be on the same line in updateISR. Try to follow consistent coding convention.\n3. Why is the try-catch-finally clause removed while acquiring/releasing the leaderAndISRLock ? This is very dangerous and can lead to the lock never being released in some failure cases. \n4. Keep the info/error statements like those in updateISRInZk (code that you deleted)\n\n4. ReplicaManager.scala\n1. The deleteLog change seems pretty hacky. What happens if the log was being written to while processing the stop replica request ? It doesn't seem like that this scenario is handled and tested in this patch. Please revert all log deletion related changes, leave a TODO comment in stopReplica. We have another JIRA filed for delete topic, maybe we can handle it cleanly as part of that ? This will also help reduce the scope of this patch which is supposed to only handle become leader and become follower. \n2. Change all log4j statements that say âOn broker, blahâ to âBlah on brokerâ\n3. Does makeLeader and makeFollower ever return an error code other than NoError ?\n\n5. Log.scala\n1. Revert \n\n6. LogManager.scala\n1. Revert \n\n7. FileMessageSet.scala\n1. Revert \n\n8. SimpleConsumer\n1. Revert \n\n9. AbstractFetcherManager\n1. Revert\n\n10. KafkaServer.scala\n1. Remove commented out code in addReplica\n\n11. KafkaController.scala\n1. Change all log4j statements of this format âController %d see blahâ to something meaningful like âBlah is %s on controller %dâ\n\n12. Testing\n\n12.1. In TestUtils, \n1. change variables ISR1 and ISR2 to lower case. Also make the same change in ZkUtils and anywhere else where you've used all-uppercase to denote a variable. Also, rename createSampleLeaderAndISRResponse to createTestLeaderAndISRResponse\n2. waitUntilLeaderIsElected should be improved with this patch to handle new leader election as well as existing leader change. \n\n12.2. In ControllerBasicTest, change assertEquals to say âController should be on broker 1â. You might want to remove the debug statement that says âcommand send test finishesâ\n12.3. Why are all tests deleted from ControllerToBrokerRequestTest.scala ?\n12.4. LeaderElectionTest and ControllerBasicTest throws couple of new errors with this patch -\n[2012-07-16 15:14:16,515] ERROR Uncaught exception in thread 'kafka-request-handler-3': (kafka.utils.Utils$:99)\njava.lang.IllegalStateException: Trying to set the leo 0 for local log\n        at kafka.cluster.Replica.logEndOffset(Replica.scala:35)\n        at kafka.cluster.Partition.updateReplicaLEO(Partition.scala:72)\n        at kafka.server.ReplicaManager.updateReplicaLEO(ReplicaManager.scala:150)\n        at kafka.server.ReplicaManager.recordFollowerPosition(ReplicaManager.scala:251)\n        at kafka.server.KafkaApis$$anonfun$maybeUpdatePartitionHW$1$$anonfun$apply$17.apply(KafkaApis.scala:309)\n        at kafka.server.KafkaApis$$anonfun$maybeUpdatePartitionHW$1$$anonfun$apply$17.apply(KafkaApis.scala:308)\n        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)\n        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)\n        at kafka.server.KafkaApis$$anonfun$maybeUpdatePartitionHW$1.apply(KafkaApis.scala:308)\n        at kafka.server.KafkaApis$$anonfun$maybeUpdatePartitionHW$1.apply(KafkaApis.scala:305)\n        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)\n        at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:32)\n        at kafka.server.KafkaApis.maybeUpdatePartitionHW(KafkaApis.scala:305)\n        at kafka.server.KafkaApis.handleFetchRequest(KafkaApis.scala:248)\n        at kafka.server.KafkaApis.handle(KafkaApis.scala:59)\n        at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:35)\n        at java.lang.Thread.run(Thread.java:662)\n[2012-07-16 15:14:16,525] WARN EndOfStreamException: Unable to read additional data from client sessionid 0x13891d7ebb40019, likely client has closed socket (org.apache.zookeeper.server.NIOServerCnxn:634)\n[2012-07-16 15:14:16,539] WARN EndOfStreamException: Unable to read additional data from client sessionid 0x13891d7ebb40014, likely client has closed socket (org.apache.zookeeper.server.NIOServerCnxn:634)\n[2012-07-16 15:14:16,841] ERROR Closing socket for /127.0.0.1 because of error (kafka.network.Processor:99)\njava.io.IOException: Connection reset by peer\n        at sun.nio.ch.FileDispatcher.write0(Native Method)\n        at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:29)\n        at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:69)\n        at sun.nio.ch.IOUtil.write(IOUtil.java:40)\n        at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:334)\n        at kafka.api.PartitionDataSend.writeTo(FetchResponse.scala:66)\n        at kafka.network.MultiSend.writeTo(Transmission.scala:93)\n        at kafka.network.Send$class.writeCompletely(Transmission.scala:74)\n        at kafka.network.MultiSend.writeCompletely(Transmission.scala:86)\n        at kafka.api.TopicDataSend.writeTo(FetchResponse.scala:142)\n        at kafka.network.MultiSend.writeTo(Transmission.scala:93)\n        at kafka.network.Send$class.writeCompletely(Transmission.scala:74)\n        at kafka.network.MultiSend.writeCompletely(Transmission.scala:86)\n        at kafka.api.FetchResponseSend.writeTo(FetchResponse.scala:219)\n        at kafka.network.Processor.write(SocketServer.scala:321)\n        at kafka.network.Processor.run(SocketServer.scala:215)\n        at java.lang.Thread.run(Thread.java:662)\n[2012-07-16 15:14:16,938] WARN EndOfStreamException: Unable to read additional data from client sessionid 0x13891d7ebb4001b, likely client has closed socket (org.apache.zookeeper.server.NIOServerCnxn:634)\n[2012-07-16 15:14:16,991] ERROR Cannot truncate log to 0 since the log start offset is 0 and end offset is 0 (kafka.log.Log:93)\n[2012-07-16 15:14:16,993] ERROR Uncaught exception in thread 'kafka-request-handler-1': (kafka.utils.Utils$:99)\njava.lang.IllegalArgumentException: Broker id 0 does not exist\n        at kafka.cluster.Broker$.createBroker(Broker.scala:30)\n        at kafka.utils.ZkUtils$$anonfun$getBrokerInfoFromIds$1.apply(ZkUtils.scala:552)\n        at kafka.utils.ZkUtils$$anonfun$getBrokerInfoFromIds$1.apply(ZkUtils.scala:552)\n        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)\n        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)\n        at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)\n        at scala.collection.immutable.List.foreach(List.scala:45)\n        at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)\n        at scala.collection.immutable.List.map(List.scala:45)\n        at kafka.utils.ZkUtils$.getBrokerInfoFromIds(ZkUtils.scala:552)\n        at kafka.server.ReplicaManager.makeFollower(ReplicaManager.scala:198)\n        at kafka.server.KafkaServer.makeFollower(KafkaServer.scala:155)\n        at kafka.server.KafkaServer$$anonfun$startup$6.apply(KafkaServer.scala:92)\n        at kafka.server.KafkaServer$$anonfun$startup$6.apply(KafkaServer.scala:92)\n        at kafka.server.KafkaApis$$anonfun$handleLeaderAndISRRequest$1.apply(KafkaApis.scala:97)\n        at kafka.server.KafkaApis$$anonfun$handleLeaderAndISRRequest$1.apply(KafkaApis.scala:73)\n        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)\n        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)\n        at scala.collection.Iterator$class.foreach(Iterator.scala:631)\n        at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:161)\n        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:194)\n        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)\n        at scala.collection.mutable.HashMap.foreach(HashMap.scala:80)\n        at kafka.server.KafkaApis.handleLeaderAndISRRequest(KafkaApis.scala:73)\n        at kafka.server.KafkaApis.handle(KafkaApis.scala:62)\n        at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:35)\n        at java.lang.Thread.run(Thread.java:662)\n[2012-07-16 15:14:20,124] ERROR Error in controller request send thread to broker 1 down due to  (kafka.server.RequestSendThread:99)\njava.nio.channels.AsynchronousCloseException\n        at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:185)\n        at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:223)\n        at kafka.utils.Utils$.read(Utils.scala:603)\n        at kafka.network.BoundedByteBufferReceive.readFrom(BoundedByteBufferReceive.scala:54)\n        at kafka.network.Receive$class.readCompletely(Transmission.scala:55)\n        at kafka.network.BoundedByteBufferReceive.readCompletely(BoundedByteBufferReceive.scala:29)\n        at kafka.network.BlockingChannel.receive(BlockingChannel.scala:92)\n        at kafka.server.RequestSendThread.run(KafkaController.scala:64)\n12.5. This patch is supposed to make very significant changes to the replication state machine. The existing tests don't pass cleanly. I would prefer to see many more unit tests and a running system test before accepting this patch. You can probably wait until KAFKA-350 is checked in since that enables replication with failures. This will allow us to run the system test on this patch.\n", "Answering Neha's reviews\n\n1. ZkUtils\n1. Remove the debug statement in getEpochForPartition that says âget data, see mâ. Debug statements should be carefully added and should be added to be useful during debugging.\n2. Another debug statement that can be improved -\n3. âCheck the leaderEpocAndISR raw string:â. Can you please change this to something like trace(âThe leader and isr info %s read from zk path %sâ)\n4. Change debug(âthe leader....â) to debug(âLeader %d, Epoch %d, isr %s, zk path version %d for topic %s and partition %dâ). Without knowing the topic and partition, this debug statement will not be useful\n5. Overall, change any log statement that says âget this and check thatâ. For example, getLeaderForPartition()\n\n\nabove comments about the logging statement fixed \n\n\n\n6. Change epoc to epoch everywhere in your code\n\nfixed\n\n7. In getPartitionLeaderAndISRForTopics, the input is a list of topics for which leader and isr is to be fetched, but in the output, some partitions for which leader and isr is not available are missing from the return value. This is hard to understand from the caller's POV. Either it should return None or throw an exception so that the caller can handle Option value or an exception to know that some partitions don't have leader and isr info.\n\nThis is reasonable, because this function is only called at getLeaderAndISRFromZookeeper() in controller, as recovering the leader and ISR info from zookeeper (if there is). It's possible that some partitions don't have the leader and ISR in zookeeper, which does not matter. (will be initialezed later)\n\n\n\n8. What is the value of change readData to readDataMaybeNull in getBrokerInfoFromIds ? Basically you are returning invalid Broker objects with null broker info from that API, which doesn't seem useful to the caller. The fix is same as above. Either throw exception (like before) or change it to return Option.\n\nInside createBroker(), I handle the case where the brokerInfo is null, a BrokerNotExistException (this exception is added by me) is thrown, add caught at makeFollower() function of ReplicaManager, when a broker intends to follow a leader which was down, this excpetion is thrown and caught, and corresponding error code is returned. \n\n\n\n2. KafkaController.scala\n1. Rename recoverLeaderAndISRFromZookeeper to getLeaderAndISRFromZookeeper\n\nOK\n\n\n2. In recoverLeaderAndISRFromZookeeper () API, get is blindly invoked on an Option variable returned from allPartitionReplicaAssignment. Scala options force us to handle invalid/missing values cleanly, please use Option correctly everywhere in your code.\n\nActually it was implicitely handled (with a waste of function call, before it was like:\n\n        val brokersAssignedToThisPartitionOpt = allPartitionReplicaAssignment.get(topicPartition)\n        if(brokersAssignedToThisPartitionOpt == None){\n          warn(\"during leaderAndISR recovery, there's no replica assignment for partition [%s, %d] with allPartitionReplicaAssignment: %s\".format(topicPartition._1, topicPartition._2, allPartitionReplicaAssignment))\n        } else{\n          val relatedBrokersAssignedToThisPartition = allPartitionReplicaAssignment.get(topicPartition).get.filter(brokerIds.contains(_))\n\t  ....\n\t}\n\nNow it is:\n        val brokersAssignedToThisPartitionOpt = allPartitionReplicaAssignment.get(topicPartition)\n        if(brokersAssignedToThisPartitionOpt == None){\n          warn(\"during leaderAndISR recovery, there's no replica assignment for partition [%s, %d] with allPartitionReplicaAssignment: %s\".format(topicPartition._1, topicPartition._2, allPartitionReplicaAssignment))\n        } else{\n          val relatedBrokersAssignedToThisPartition = brokersAssignedToThisPartitionOpt.filter(brokerIds.contains(_))\n\t  ....\n\t}\n\n\n\n\n3. The following info statement is unclear - âOn leaderAndISR recover, ....â. Please change it to âAfter reading leader and ISR from zookeeper, ...â\n\nI've changed into better class level log indention\n\n\n\n4. What is the purpose of leaderAndISRRecovery since it just calls recoverLeaderAndISRFromZookeeper anyways ? Let's remove this API\n\nOK\n\n\n5. All info statements in controllerRegisterOrFailover() API are unclear. Please follow the info statements in leaderElection() in KafkaZookeeper(the code you deleted) and fix these.\n\nWith class level log indention, it should be better\n\n\n3. Partition.scala\n1. Change updateISR back to take in an optional zkclient instead of null and a separate boolean value. That way it makes it easier to handle a missing zk client instead of running into NPE\n\nDone\n\n\n2. Parentheses should be on the same line in updateISR. Try to follow consistent coding convention.\n\n\nI use \"LinkedIn style\" auto indention of IntelliJ, It will automatically indent the follow code snippet to:\n\n      inSyncReplicas = newISR.map {r =>\n        getReplica(r) match {\n          case Some(replica) => replica\n          case None => throw new KafkaException(\"ISR update failed. No replica for id %d\".format(r))\n        }\n                                  }\n\nWhile I prefer:\n      \n      inSyncReplicas = newISR.map\n      {\n        r =>\n          getReplica(r) match {\n            case Some(replica) => replica\n            case None => throw new KafkaException(\"ISR update failed. No replica for id %d\".format(r))\n          }\n      }      \n\n\n\n3. Why is the try-catch-finally clause removed while acquiring/releasing the leaderAndISRLock ? This is very dangerous and can lead to the lock never being released in some failure cases.\n\nNow they're back\n\n4. Keep the info/error statements like those in updateISRInZk (code that you deleted)\n\nDone\n\n\n\n\n4. ReplicaManager.scala\n1. The deleteLog change seems pretty hacky. What happens if the log was being written to while processing the stop replica request ? It doesn't seem like that this scenario is handled and tested in this patch. Please revert all log deletion related changes, leave a TODO comment in stopReplica. We have another JIRA filed for delete topic, maybe we can handle it cleanly as part of that ? This will also help reduce the scope of this patch which is supposed to only handle become leader and become follower.\n\nI think that's a good idea\n\n\n\n2. Change all log4j statements that say âOn broker, blahâ to âBlah on brokerâ\nNow I'm using class level log indent\n\n\n\n3. Does makeLeader and makeFollower ever return an error code other than NoError ?\nNow it does for makeFollower, I add \"TODO\" for makeLeader()\n\n\n\n5. Log.scala\n1. Revert\nMay be not applicable now\n\n\n6. LogManager.scala\n1. Revert\nMay be not applicable now\n\n\n7. FileMessageSet.scala\n1. Revert\n\nReverted\n\n\n8. SimpleConsumer\n1. Revert\n\nReverted\n\n\n9. AbstractFetcherManager\n\nMay be not applicable now\n\n\n10. KafkaServer.scala\n1. Remove commented out code in addReplica\n\ndone\n\n\n\n11. KafkaController.scala\n1. Change all log4j statements of this format âController %d see blahâ to something meaningful like âBlah is %s on controller %dâ\n\nChanged\n\n\n\n12. Testing\n\n12.1. In TestUtils,\n1. change variables ISR1 and ISR2 to lower case. Also make the same change in ZkUtils and anywhere else where you've used all-uppercase to denote a variable. Also, rename createSampleLeaderAndISRResponse to createTestLeaderAndISRResponse\n\nChanged\n\n\n2. waitUntilLeaderIsElected should be improved with this patch to handle new leader election as well as existing leader change.\n\nNow improved with a new function waitUntilLeaderIsElectedOrChanged()\n\n\n\n12.2. In ControllerBasicTest, change assertEquals to say âController should be on broker 1â. You might want to remove the debug statement that says âcommand send test finishesâ\n\nFixed\n\n\n\n12.3. Why are all tests deleted from ControllerToBrokerRequestTest.scala ?\nThis test is now included in ControllerBasicTest() and not valuable at all\n\n\n\n12.4. LeaderElectionTest and ControllerBasicTest throws couple of new errors with this patch - \nNow they're either identified as not actual error or fixed.\n\n\n\nMore descrption:\n\nThis patch basically addresses various kinds of exceptions during handling leader and ISR requests. Now the logic of the controller side is pretty clear. \n\n\nAlso I changed the watiUnitLeaderIsElected() to watiUnitLeaderIsElectedOrChanged()\n\nI added class level log indention for KafkaController, KafkaServer, LogManager, Log, ReplicaManager, ReplicaFetcherThread, etc.. now the output of the log should be easier to read.\n\nI made quite a few changes to the ZkUtils class, most importantly, the partition is always treated as integer, unlike before it's String in a lot of cases. \n\nUnit tests all passed. But I find with low probability, broker failure system test will fail with losing a few messages, also I find latest code in 0.8 also fails at broker failure system test by losing messages. We need to look in detail of this problem. But the general controller side logic in this patch is clear. \n\n\n\n", "Thanks for patch v4. Some more comments:\n\n50. RequestSendThread.run(): Not sure if we need to handle AsynchronousCloseException specially. If we hit any exception while sending the request, we should probably just log the exception at the info level and let it go. If the exception is due to a failed broker, the controller will eventually shutdown this thread.\n\n51. KafkaController:\n51.1 getLeaderAndISRFromZookeeper():\n51.1.1 Can we rename it to deliverLeaderAndISRFromZookeeper()?\n51.1.2 brokerToLeaderAndISRInfosMap.get(b).get can be brokerToLeaderAndISRInfosMap.(b)\n51.2 tryToBecomeController(): doesn't need to be synchronized since all callers are already synchronized.\n\n52. KafkaApis:\n52.1 handleStopReplicaRequest: we should put in a TODO saying that this will be handled in kafka-330\n52.2 The check of replicaManager.hwCheckPointThreadStarted should be done in replicaManager.startHighWaterMarksCheckPointThread, not in KafkaApis.\n52.3 handleLeaderAndISRRequest(): Could you fix the indentation of the following line?\n              (!replica.partition.leaderId().isDefined || replica.partition.leaderId().get != brokerId)){\n52.4 handleDeletedTopics: During controller failover, we need to remove unneeded leaderAndISRPath that the previous controller didn't get a chance to remove. This can be done as part of kafka-330. Please add this note to kafka-330.\n\n53. ReplicaManager:\n53.1 addLocalReplica(): The local replica should only be created if it doesn't exist.\n53.2 fix the indentation of the following line in makeFollower()\n      ErrorMapping.UnknownCode\n53.3 partitionExists(): Can we just use getPartition() instead?\n\n54.AbstractFetcherThread: It's useful to see the exception in the log message. This is ok since it's only in debug mode. So, just revert the change.\n\n55. ConsumerFetcherManager, DefaultEventHandler,ProducerRequest,ProducerSendThread: It's actually better to put the long statement in 2 lines. So, revert this change.\n\n56. LogManager.flushAllLogs(): It's actually better to put the long statement in 2 lines.\n\n57. Partition.updateISR(): Updating ISR in ZK should be done conditionally using zk version. Also, we should update ISR in ZK first before updating in memory. This can be done in a separate jira. Please file a new one.\n\n58. RequestPurgatory: name in constructor. Change it logPrefix. Ditto for ExpiredRequestReaper. Also there were lots of changes that seem to due to white spaces.\n\n59. LeaderExistsOrChangedListener: The 1st is too long. Break it into 2 lines.\n\n60. TestUtils: There are changes that seem to due to white spaces.\n", "50. RequestSendThread.run(): Not sure if we need to handle AsynchronousCloseException specially. If we hit any exception while sending the request, we should probably just log the exception at the info level and let it go. If the exception is due to a failed broker, the controller will eventually shutdown this thread.\n\nWithout catching this exception, there will a lot of erro messages in the log, no other difference\n\n\n\n51. KafkaController:\n51.1 getLeaderAndISRFromZookeeper():\n51.1.1 Can we rename it to deliverLeaderAndISRFromZookeeper()?\nOK\n\n51.1.2 brokerToLeaderAndISRInfosMap.get(b).get can be brokerToLeaderAndISRInfosMap.(b)\nchanged all occurrences like this in the code\n\n\n51.2 tryToBecomeController(): doesn't need to be synchronized since all callers are already synchronized.\nOK\n\n52. KafkaApis:\n52.1 handleStopReplicaRequest: we should put in a TODO saying that this will be handled in kafka-330\nI added *TODO* in the deleteReplica function of ReplicaManager\n\n\n52.2 The check of replicaManager.hwCheckPointThreadStarted should be done in replicaManager.startHighWaterMarksCheckPointThread, not in KafkaApis.\n\nOK\n\n\n52.3 handleLeaderAndISRRequest(): Could you fix the indentation of the following line?\n              (!replica.partition.leaderId().isDefined || replica.partition.leaderId().get != brokerId)){\n52.4 handleDeletedTopics: During controller failover, we need to remove unneeded leaderAndISRPath that the previous controller didn't get a chance to remove. This can be done as part of kafka-330. Please add this note to kafka-330.\n\nok\n\n53. ReplicaManager:\n53.1 addLocalReplica(): The local replica should only be created if it doesn't exist.\nok\n\n53.2 fix the indentation of the following line in makeFollower()\n      ErrorMapping.UnknownCode\nfixed\n\n53.3 partitionExists(): Can we just use getPartition() instead?\nsure\n\n\n54.AbstractFetcherThread: It's useful to see the exception in the log message. This is ok since it's only in debug mode. \nreverted\n\nSo, just revert the change.\n\n55. ConsumerFetcherManager, DefaultEventHandler,ProducerRequest,ProducerSendThread: It's actually better to put the long statement in 2 lines. So, revert this change.\nOK\nI've done that\n\n56. LogManager.flushAllLogs(): It's actually better to put the long statement in 2 lines.\nOK\n\n\n\n57. Partition.updateISR(): Updating ISR in ZK should be done conditionally using zk version. Also, we should update ISR in ZK first before updating in memory. This can be done in a separate jira. Please file a new one.\n\nAlready fixed in latest patch\n\n\n\n58. RequestPurgatory: name in constructor. Change it logPrefix. Ditto for ExpiredRequestReaper. Also there were lots of changes that seem to due to white spaces.''\n\nold  changed \n\n\n59. LeaderExistsOrChangedListener: The 1st is too long. Break it into 2 lines.\nOK\n\n\n60. TestUtils: There are changes that seem to due to white spaces. \nIt's indention problem with existing code", "I've made changes to the waitUntilLeaderIsElectedOrChanged() to waitUntilLiveLeaderIsElected()", "Thanks for patch v6. Committed to 0.8. Filed kafka-427 and kafka-428 as followup jiras.", "KAFKA-343 checkin broke some unit tests. Unit tests hang on the KAFKA-384 patch. ", "Revisit the patch, from V5, the updateISR() and RequestSendThread has been changed, but the waitUntilLeaderIselectedOrChanged is recovered. ", "Revert commit due to unit test failure.", "Rebase from kafka-384", "Thanks for patch v8. Committed to 0.8.", "Yang,\n\nDo we know the impact of this patch on the leader election latency ? The idea of having a controller do state changes was the possible reduction in leader election latency. Curious to know if there are any numbers to support this assumption ?\n\n"], "derived": {"summary": "We need to reimplement become leader/follower using the controller model described in https://cwiki. apache.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "revisit the become leader and become follower state change operations using V3 design - We need to reimplement become leader/follower using the controller model described in https://cwiki. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "Yang,\n\nDo we know the impact of this patch on the leader election latency ? The idea of having a controller do state changes was the possible reduction in leader election latency. Curious to know if there are any numbers to support this assumption ?"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-344", "title": "migration tool from 0.7 to 0.8", "status": "Closed", "priority": "Blocker", "reporter": "Jun Rao", "assignee": "Yang Ye", "labels": ["tools"], "created": "2012-05-16T16:29:40.000+0000", "updated": "2012-11-23T00:40:50.000+0000", "description": "Since 0.8 is a non-backward compatible release, we need a migration tool. One way to do that is to have a tool that can mirror data from an 0.7 Kafka cluster into an 0.8 Kafka cluster. Once the mirror is set up, we can then first migrate the consumers to the 0.8 cluster, followed by the producers. After that, we can decommission the 0.7 cluster. ", "comments": ["\nSorry the system test is still not done yet. The current system test framework assume a global \"cluster_config.json\" file, it's used by all testSuites and all test cases under each suite. But current cluster config (3 broker, 1 zookeeper, 1 consumer, 1 producer) does not apply to migration test, so I have to create a new one (1 07 zookeeper, 1 08 zookeeper, 3 07 brokers, 3 08 brokers, 1 07 producer, 1 migration tool, 1 08 consumer), \n\nBut the current replication_testSuite cannot work with this configration, I have to delete it temporarily. (I've talked with John to enable local cluster configuration instead of just global one)\n\nI created new \"role\"s of \"broker07\", \"zookeeper07\" and \"producer_performance07\", and changed a few files in the system test framework. Currently it is still not working, continuing on it...\n\nPlease help review the migration tool itself", "Thanks for the draft patch. Very tricky to get right. It does seem to work. Good job! Look forward to the final patch.", "1. The current system test assume there is kafka 0.7 project at location, \"~/Dev/kafka_0.7\" (otherwise you need to specify the kafka 07 location at \"cluster_config.json\" file\n\n2. The patch make the replication_testsuite not working because they share the same cluster_config.json, I've talked with John to create three level cluster configuration: testcase, testsuite and global level. If configuration file is found at \"testcase\" level, the framework won't search the other two level, otherwise if it's found in testsuite level, the framework won't search the global level. Will rebase after John added this feature\n\n3. Added a few roles: \"broker07\", \"zookeeper07\", \"producer_performance07\", and \"migration_tool\" as specified in the cluster_config.json file\n\n4. add a few functions in Kafka_system_test_utils.py in accordance with the new roles, like start_zookeeper07(), start_producer_performance07(), etc\n\n5. patch the kafka 07 producer performance, by adding message id to the produced messages.\n\n6. please manually add the two jar files to \"system_test/migration_tool_testsuite\" directory as they cannot be included in the patch file\n\n ", "Thanks for patch v1. Some comments.\n\n1. main: \n1.1 Add some comments at the beginning that describe how the tool is built.\n1.2 print out an error message on exception in addition to the stack trace. \n1.3 Most required command line options are not enforced, e.g. consumer.config, zkclient.01.jar, kafka.07.jar. Also, at least one of the whilelist/blacklist needs to be specified.\n1.4 There is a weird regex to subscribe to all topics. Could you add that in the description of whitelist? I can't seem that get wildcard working. Tried things like --whilelist \"test*\". However, it seems to only pick up topic \"test\", but not \"test1\".\n\n2. MigrationThread: \n2.1 The following line should be written to log4j under DEBUG level, instead of stdout. Also, since message is binary, printing out the content is useless. We can probably just output the topic name and the message length.\n          System.out.println(\"send 08 message: \" + bytes.toString());\n2.2 Normally, we should never finish the while loop. However, it would be useful to add an info level logging to indicate if the loop is finished.\n2.3 IterableNextMethod is not used.\n\n", "All comments address. Also in the code, force rewrite the 08 producer's property \"serializer.class\" to \"kafka.serializer.DefaultEncoder\", and the property \"producer.type\" to \"sync\"", "Thanks for patch v2. There are still comments in the previous review that are not addressed:\n\n1.2 Since this is a command line tool, if the program stops because of an error, we need to print out the exception message and the stacktrace to stdout, instead of log4j so that users can see what's wrong. log4j output may not always be in stdout. \n\n1.3. Currently, one can still omit both whitelist and blacklist without being warned. We need to fix this.\n\n2.1 We shouldn't print the message in the following line since it's binary. Instead, we should log the size and the topic.\n          logger.debug(\"send 08 message: \" + bytes.toString());\n\nGood change to force write \"serializer.class\" to \"kafka.serializer.DefaultEncoder\". We shouldn't force write the property \"producer.type\" to \"sync\" though. In most cases, we should probably use async for better throughput. However, we can let users make that decision. Please change the comment in the file accordingly. \n\nAlso, broker.list in config/producer.properties has the wrong format and comment. Could you fix it in this patch too?\n\n \n", "\nThank you for the comments, it's really careful of you and careless of me...", "Thanks for the patch. Committed the tool itself and the change to config/producer.properties to 0.8. Will close the jira once the system test change is done.", "This is done now, right?", "Yes, this is done now"], "derived": {"summary": "Since 0. 8 is a non-backward compatible release, we need a migration tool.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "migration tool from 0.7 to 0.8 - Since 0. 8 is a non-backward compatible release, we need a migration tool."}, {"q": "What updates or decisions were made in the discussion?", "a": "Yes, this is done now"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-345", "title": "Add a listener to ZookeeperConsumerConnector to get notified on rebalance events", "status": "Resolved", "priority": "Major", "reporter": "Peter Romianowski", "assignee": "Jiangjie Qin", "labels": [], "created": "2012-05-18T14:34:08.000+0000", "updated": "2015-04-08T00:30:37.000+0000", "description": "A sample use-case\n\nIn our scenario we partition events by userid and then apply these to some kind of state machine, that modifies the actual state of a user. So events trigger state transitions. In order to avoid the need of loading user's state upon each event processed, we cache that. But if a user's partition is moved to another consumer and then back to the previous consumer we have stale caches and hell breaks loose. I guess the same kind of problem occurs in other scenarios like counting numbers by user, too.", "comments": ["Added a patch against trunk.", "I tried to add a test to verify that the listener gets called if an error occurs (ConsumerListener#afterRebalance(false)) but I failed to introduce some error into ZookeeperConsumerConnector.ZKRebalancerListener#syncedRebalance. \n\nAny ideas how to provoke an error during rebalancing?", "Peter, thanks for the patch. Some comments:\n\n1. It seems that we should add setListener in the scala version of ConsumerConnector too. Instead of calling it setListener, should we call it setRebalanceListener?\n\n2. Since ConsumerListener needs to be used in both the java and scala version of ConsumerConnector, should we put it in the consumer package, instead of javaapi.consumer? Also, we probably should rename it to ConsumerRebalanceListener.\n\n3. ZKLoadBalanceTest:\n3.1 testLoadBalance(): There seems to be a mix of space and tab. Maybe that's in the original code already, but could you fix that?\n3.2 remove unused imports\n\nI am not sure if there is an easy way to fail rebalance in unit test. The unit test in the patch seem sufficient to me.", "Jun,\n\nregarding your comments:\n\n1. Adding it to the scala-version makes sense, of course. Bear with me, I'm just a Java-guy. I'll add that.\n\n2. I'll move the listener. I did name it \"ConsumerListener\" and \"setListener\" intentionally, because I thought it would be a good extension point for future stuff, that is not necessarily related to rebalancing. That's also why I made it an abstract class and not an interface. If you still think it should be renamed, I'll do it.\n\n3. I'll remove the spaces and unused imports.", "Peter,\n\nRegarding #2, naming it as ConsumerListener is fine.", "Created reviewboard https://reviews.apache.org/r/28025/diff/\n against branch origin/trunk", "[~becket_qin] Given that we are close to starting development on the new consumer that exposes such a listener, just wondering what is the motivation behind including one in the older consumer at this moment?", "[~nehanarkhede] It is mainly because recently we have many mirror maker issue that leads to a hard kill, which cause data loss. So we want to eliminate the data loss in mirror maker as soon as possible. KAFKA-1650 is opened for this issue. In order to do that, we need to turn off auto offset commit. That means we are almost guaranteed to have duplicates on consumer rebalance. So we want to add a callback to avoid duplicates on consumer rebalance.", "There are some other considerations regarding adding the callback to old consumer as well. First, it's a backward compatible patch, if user does not wire in the callback, there is no impact. So current user will not be affected. Secondly, it is not too complicated to add the callback and it might take some time for the new producer to be ready for production, hence it seems to worth making this available for the transitional period. I think it could also potentially provide some references for how the callback could be used in new producer. ", "Updated reviewboard https://reviews.apache.org/r/28025/diff/\n against branch origin/trunk", "Updated reviewboard https://reviews.apache.org/r/28025/diff/\n against branch origin/trunk", "Updated reviewboard https://reviews.apache.org/r/28025/diff/\n against branch origin/trunk", "Looking at trunk, I think this got in through a different patch... closing this one."], "derived": {"summary": "A sample use-case\n\nIn our scenario we partition events by userid and then apply these to some kind of state machine, that modifies the actual state of a user. So events trigger state transitions.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add a listener to ZookeeperConsumerConnector to get notified on rebalance events - A sample use-case\n\nIn our scenario we partition events by userid and then apply these to some kind of state machine, that modifies the actual state of a user. So events trigger state transitions."}, {"q": "What updates or decisions were made in the discussion?", "a": "Looking at trunk, I think this got in through a different patch... closing this one."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-346", "title": "Don't call commitOffsets() during rebalance", "status": "Resolved", "priority": "Major", "reporter": "Peter Romianowski", "assignee": null, "labels": [], "created": "2012-05-18T14:57:01.000+0000", "updated": "2015-04-08T01:23:29.000+0000", "description": "A sample use-case\n\nIf I read the source correctly then offsets can be committed at any\ntime (whenever there is a change in consumer or broker zk registry).\nOur application doesn't use auto-commit in order to batch some\nmessages together, process them and if everything went fine, we call\ncommitOffsets(). If, for any reason, the processing of messages does\nnot succeed, we rely on Kafka's promise to re-deliver the messages.\n\nBut if ZKRebalancerListener triggers a rebalance before our \"batch\" of\nmessages is full, then offsets will be committed even if the messages\nhave not been processed yet by our application. So if then processing\nof these messages fails, we basically lost them, right?\n\n(See discussion at http://www.mail-archive.com/kafka-users@incubator.apache.org/msg01415.html)", "comments": ["I could now write a patch that - as Jun suggested - does not commit offsets, if autoCommit has been turned off. Together with KAFKA-345 that adds a listener for rebalance-events, we could simply use a listener and throw away all messages in our batch upon a rebalance. That would work fine for us.\n\nMy question is: Do you think this is the behavior others would expect? Even though the current behavior is broken IMHO, the new one would introduce quite some duplicate messages which have to be handled by the application.\n\nAre there better ideas on how to fix this?", "Personally I think this makes sense. If you set auto-commit we commit for you. If you say no to auto-commit you handle commits.", "Added a patch against trunk. In order to compile KAFKA-345 has to be applied.", "This is already fixed on trunk. "], "derived": {"summary": "A sample use-case\n\nIf I read the source correctly then offsets can be committed at any\ntime (whenever there is a change in consumer or broker zk registry). Our application doesn't use auto-commit in order to batch some\nmessages together, process them and if everything went fine, we call\ncommitOffsets().", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Don't call commitOffsets() during rebalance - A sample use-case\n\nIf I read the source correctly then offsets can be committed at any\ntime (whenever there is a change in consumer or broker zk registry). Our application doesn't use auto-commit in order to batch some\nmessages together, process them and if everything went fine, we call\ncommitOffsets()."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is already fixed on trunk."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-347", "title": "change number of partitions of a topic online", "status": "Closed", "priority": "Major", "reporter": "Jun Rao", "assignee": "Sriram", "labels": ["features"], "created": "2012-05-18T16:31:07.000+0000", "updated": "2013-08-29T02:51:41.000+0000", "description": "We will need an admin tool to change the number of partitions of a topic online.", "comments": ["This is more of a nice-to-have, no? Can't we postpone this beyond 0.8? Changing partition # will scramble the partitioning from the producer, and we don't really have this capability now.", "It can probably be pushed beyond 0.8. The only concern is that consumer parallelism is currently tied to # partitions.", "It's sort of out of scope but I think that using some sort of consistent hashing for partition assignment could really help here. The process could be that if you activate \"automatic partition assignment\", you don't need to send the ReassignPartition admin command to manage partitions but you over-partition and spread partitions for each topics over brokers using consistent hashing. This would:\n\n- Minimize the number of partitions moved when you add/remove brokers\n- Partitions would stick to a broker when some partitions are added/deleted\n- Could be run on a regular basis by brokers or other means, and would be perfectly consistent\n\nThis would greatly improve operations", "Well i think there are really two mappings here:\nkey => partition\nand\npartition => broker\n\nThis is the generalization of consistent hashing that most persistent data systems use.\n\nIn Kafka key=>partition is user-defined (Partitioner interface) and defaults to just hash(key)%num_partitions. partition=>broker is assigned at topic creation time and from then on is semi-static (changing it is an admin command). So when adding a broker we already can move just the number of partitions we need by having the tool compute the best set of partitions to migrate or choosing at random.\n\nSo the idea is that you over-partition and then the partition count doesn't change and hence the key=>partition assignment doesn't change.\n\nThe question is, do we need to support changing the number of partitions to handle the case where you don't over-partition by enough? If you do this then the change in mapping would be large. That could be helped a bit by a consistent hash partitioner for the key=>partition mapping on the client side, but even in that case you would still have lots of values that are now in the wrong partition, so any code that depended on the partitioning would be broken.\n\nAlternately you could do the hard work of actually implementing partition splitting on the broker by having the broker split a partition into two and then migrating the new partitions.\n\nThe question I would ask is, is any of this worth it? Many data system don't support partition splitting they just say \"choose your partition count wisely or else delete it and start fresh\". Arguably most messaging use cases are particularly concerned with recent data so this might be a fine answer. So an alternate strategy would just be to spend the time working on scaling the number of partitions we can handle and over-partitioning heavily.\n", "Agreed. Except you also have to think about people not using keyed partitioning. It would be very convenient to add (maybe not remove) partitions as your topic gets larger and you increase your cluster size. And that could be done online since the number of partitions in this case is only used for scaling and distribution.\n\n (and I think the auto partition balancing is off-topic and I should probably open another JIRA)", "Going to look into this", "1. Added an isolated change to dynamically add partitions for messages without key\n2. The only common code change is the replica layout strategy\n3. Unit tests", "A few comments:\n\n1. In addPartitions function, we can set partitionStartIndex as the last partition of the partition instead of partition 0 of the topic, since by doing so as more and more of this function gets called, the broker owning partition 0's leader will have more leader partitions assigned to it.\n\n2. Some of the changes in ZookeeperConsumerConnector for KAFKA-969 is also included in the patch. Better trim them out for this patch.\n\n3. Are there two getManualReplicaAssignment in AddPartitionsCommand and CreateTopicCommand with one parameter difference? If yes could we combine them into one?\n\nSome minor stuff:\n\n1. In AdminUtils, there are two more space indents for line of \"ZkUtils.updatePersistentPath(zkClient, zkPath, jsonPartitionData)\"\n\n2. In createOrUpdateTopicPartitionAssignmentPathInZK, use\n\nif {\n  //\n} else {\n  //\n}", "1. That would not give you the right result. The start index is for the replica of the first partition we create. This is to continue from where we left for that topic.\n\n2. That is on purpose. There is no change. Just if else fix.\n\n3. This is on purpose to prevent any regression for 0.8.", "Thank you for the patch. Couple of comments, all very minor:\n\nAddPartitionsCommand:\n- IMO it is more intuitive for the option to be: \"total partitions desired\"\n  as opposed to \"num partitions to add\"\n- It is a bit odd that we can allow some partitions with a different\n  replication factor from what's already there. I don't see any issues with\n  it though. I just think it's a bit odd. One potential issue is if\n  producers explicitly want to set acks to say 3 when there are some\n  partitions with a replication factor of 2 and some with 3 (However,\n  producers really should be using -1 in which case it would be fine).\n- I think the command can currently allow an unintentional reassignment of\n  replicas since the persistent path is always updated. (or no?) I think\n  this can be easily checked for and avoided.\n- Apart from start partition id I think getManualReplicaAssignment is\n  identical to CreateTopicCommand's - maybe that code can be moved into\n  AdminUtils?\n\nKafkaController:\n- nitpick: ZkUtils.getAllTopics(zkClient).foreach(p =>\n  partitionStateMachine.registerPartitionChangeListener(p)) (can you change\n  p to t :) - p really looks like a partition but it is a topic )\n\nAdminUtils:\n- the //\"for testing only\" comment is now misplaced.\n- This code is pre-existing, but would prefer changing secondReplicaShift to\n  nextReplicaShift.\n\n- Any reason why AddPartitionsTest should not be within AdminTest?\n- Finally, can you rebase? Sorry for not getting to this patch sooner :(\n\n", "Overall, this looks great and sorry for not coming to this patch earlier. Few minor comments -\n\n1. KafkaController\n\n1.1 We can read the list of all topics from cache instead of reading from zookeeper, since initializeControllerContext already does that\n1.2 In onNewTopicCreation, what was the motivation to move onNewPartitionCreation to before the registration of the listener\n2. PartitionStateMachine.AddPartitionsListener\n\ncontrollerContext.partitionReplicaAssignment gets populated during the NewPartition state transition.\nDue to this, it is best to get rid of the following in the listener, it should happen as part of the NewPartition state change\ncontrollerContext.partitionReplicaAssignment.++=(partitionsRemainingToBeAdded)\n\n3. AdminUtils\n\nIn createOrUpdateTopicPartitionAssignmentPathInZK, please change topic creation -> Topic creation\n\n4. There is a conflict in ZookeeperConsumerConnector\n", "Joel\n\n- I did not see a big difference in the meaning\n- good point. I now check if the replication factor specified is the same as that of the topic.\n- I dont think so. We only add new rows.\n- This was done to reduce regression.\n- Done\n- Done\n- Done\n- I have a bunch to initial setup that is not present in that test and they dont require it\n\nNeha\n- Done\n- It seemed safer and intuitive to create a listener to a topic after it is created\n- Done\n- Done\n- Done", "Thanks for patch v2. I'm +1 on this as is, but if you can address some of these minor comments that would be great.\n\nv2.1 - For \"num partitions to add\" vs \"partitions desired\" - all I meant was that most of the time users would think\nof \"desired number of partitions\" vs \"how many more to add\". E.g., I have eight partitions for a topic, I now want\n20 instead. It is more convenient to just say I want \"20\" partitions instead of thinking of how many to add. OTOH since\nwe don't support reducing partitions treating it as a \"num partitions to add\" is safer. So I don't feel very strongly\nabout it either way.\n\nv2.2 - Re: unintentional reassignment of partitions. Yes you are right.\n\nv2.3 - Your patch still has ZookeeperConsumerConnector changes in it, so it did not apply cleanly.\n\nv2.4 - On checking the replication factor: if we don't allow having a different replication factor for the new partitions\nwe should not even expose it as an option.\n\nv2.5 - AddPartitionsListener: no need to change it now, but just a comment: we can directly parse the replica assignment\nfrom the data object (instead of reading from zookeeper again) right?\n\nv2.6 - On moving getManualReplicaAssignment to AdminUtils - I think it would be good to do that here, but either way is\nfine.\n", "v2.4 the reason to expose it is for manual replica assignment. It is more explicit to specify the rep factor and the assignments for those. \nRebased without the zkconsumer connector change.", "Thanks for patch v2. Looks good overall. Some comments.\n\n20. AdminUtils.createOrUpdateTopicPartitionAssignmentPathInZK(): The info logging should be different, depending on whether the topic is created or updated. Also, fix the indentation in the else clause.\n\n21. AddPartitionsCommand:\n21.1 remove unused imports\n21.2 Is there any value in having the \"replica\" option? It seems that it should always be the existing replication factor.\n21.3 For the \"replica-assignment-list\" option, could we make it clear in the description that this is for newly added partitions.\n21.4 getManualReplicaAssignment(): We need to make sure the replica factor is the same as the existing one.\n\n22. KafkaController.onNewTopicCreation(): Could you explain why the onNewPartition statement is moved to before the watcher registration? Normally, in order not to miss any watchers, one needs to register the watcher before reading the associated nodes in ZK.\n\n23. PartitionStateMachine.AddPartitionsListener.handleDataChange():\n23.1 In the following statement, we are actually returning all replica assignments.\n           val addedPartitionReplicaAssignment = ZkUtils.getReplicaAssignmentForTopics(zkClient, List(topic))\n23.2 Instead of using controllerContext.partitionLeadershipInfo to filter out existing partitions, it's probably better to use controllerContext.partitionReplicaAssignment, since leaders may not always exist.\n23.3 In the error logging, could we add the affected data path?\n\n24. AddPartitionsTest: remove unused imports\n\n25. Did we do any test to make sure that existing consumers can pick up the new partitions?\n\n26. The patch needs to be rebased.", "20. Indentation seems fine to me.\n21.2 It is present to make manual assignment more clear.\n25 Yes the test was done. I will do another sanity check after the patch is commited.", "added a script for addpartitions", "Thanks for the patch. Committed to 0.8. Could you also provide a patch to trunk since in trunk, all topic related commands are consolidated to a kafka-topic tool.", "Is there some operational documentation on how to use this?", "You just need to run bin/kafka-add-partitions.sh. We will add the docs when 0.8 final is released. "], "derived": {"summary": "We will need an admin tool to change the number of partitions of a topic online.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "change number of partitions of a topic online - We will need an admin tool to change the number of partitions of a topic online."}, {"q": "What updates or decisions were made in the discussion?", "a": "You just need to run bin/kafka-add-partitions.sh. We will add the docs when 0.8 final is released."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-348", "title": "rebase 0.8 branch from trunk", "status": "Resolved", "priority": "Critical", "reporter": "Joe Stein", "assignee": "Joe Stein", "labels": [], "created": "2012-05-25T02:12:22.000+0000", "updated": "2012-06-17T15:32:34.000+0000", "description": null, "comments": ["a few issues\n\n1) conflict in ZookeeperConsumerConnector related to releasePartitionOwnership and kafka tickets 300,239 and 286\n\nand a bunch of errors from merge related to to other tickets stepping on each other though no specific conflict\n\n[info] Compiling main sources...\n[error] /Users/josephstein/apache/0.8_kafka/core/src/main/scala/kafka/consumer/ZookeeperConsumerConnector.scala:556: type mismatch;\n[error]  found   : kafka.utils.Pool[String,kafka.utils.Pool[Int,kafka.consumer.PartitionTopicInfo]]\n[error]  required: kafka.utils.Pool[String,kafka.utils.Pool[kafka.cluster.Partition,kafka.consumer.PartitionTopicInfo]]\n[error]               addPartitionTopicInfo(currentTopicRegistry, topicDirs, partition, topic, consumerThreadId)\n[error]                                     ^\n[error] /Users/josephstein/apache/0.8_kafka/core/src/main/scala/kafka/consumer/ZookeeperConsumerConnector.scala:661: not found: value partition\n[error]       val leaderOpt = getLeaderForPartition(zkClient, topic, partition.toInt)\n[error]                                                              ^\n[error] /Users/josephstein/apache/0.8_kafka/core/src/main/scala/kafka/consumer/ZookeeperConsumerConnector.scala:664: not found: value partition\n[error]           format(partition, topic))\n[error]                  ^\n[error] /Users/josephstein/apache/0.8_kafka/core/src/main/scala/kafka/consumer/ZookeeperConsumerConnector.scala:665: not found: value partition\n[error]         case Some(l) => debug(\"Leader for partition %s for topic %s is %d\".format(partition, topic, l))\n[error]                                                                                   ^\n[error] /Users/josephstein/apache/0.8_kafka/core/src/main/scala/kafka/consumer/ZookeeperConsumerConnector.scala:669: not found: value partition\n[error]       val znode = topicDirs.consumerOffsetDir + \"/\" + partition\n[error]                                                       ^\n[error] /Users/josephstein/apache/0.8_kafka/core/src/main/scala/kafka/consumer/ZookeeperConsumerConnector.scala:676: not found: value partition\n[error]                   earliestOrLatestOffset(topic, leader, partition.toInt, OffsetRequest.EarliestTime)\n[error]                                                         ^\n[error] /Users/josephstein/apache/0.8_kafka/core/src/main/scala/kafka/consumer/ZookeeperConsumerConnector.scala:678: not found: value partition\n[error]                   earliestOrLatestOffset(topic, leader, partition.toInt, OffsetRequest.LatestTime)\n[error]                                                         ^\n[error] /Users/josephstein/apache/0.8_kafka/core/src/main/scala/kafka/consumer/ZookeeperConsumerConnector.scala:691: not found: value partition\n[error]                                                  partition.toInt,\n[error]                                                  ^\n[error] /Users/josephstein/apache/0.8_kafka/core/src/main/scala/kafka/consumer/ZookeeperConsumerConnector.scala:696: not found: value partition\n[error]       partTopicInfoMap.put(partition.toInt, partTopicInfo)\n[error]                            ^\n[error] /Users/josephstein/apache/0.8_kafka/core/src/main/scala/kafka/producer/SyncProducer.scala:62: value produces is not a member of kafka.api.ProducerRequest\n[error]           for (produce <- request.produces) {\n[error]                                   ^\n[error] /Users/josephstein/apache/0.8_kafka/core/src/main/scala/kafka/producer/SyncProducer.scala:145: not found: type MessageSizeTooLargeException\n[error]         throw new MessageSizeTooLargeException\n[error]                   ^\n[error] /Users/josephstein/apache/0.8_kafka/core/src/main/scala/kafka/server/KafkaApis.scala:100: type mismatch;\n[error]  found   : kafka.message.MessageSet\n[error]  required: kafka.message.ByteBufferMessageSet\n[error]           log.append(partitionData.messages)\n[error]                                    ^\n[error] 12 errors found\n\n\nall other conflicts resolved I will take a look at the errors tomorrow but if someone else can take a look also would be great seems like integrating these last few files is going to be a little more involved making sure not to lose any features and such", "Joe,\n\nThanks for helping out. The patch seems to miss some of the new classes created in trunk (e.g., KafkaStream, TopicFilter, etc).", "For addPartitionTopicInfo(currentTopicRegistry, topicDirs, partition, topic, consumerThreadId):\nIn 0.8, the method doesn't take topicRegistry in the input any more, but uses it in the method directly. Also, in 0.8, we are using the partitionId as the key in the inner map of topicRegistry, instead Partition object.\n", "<< The patch seems to miss some of the new classes created in trunk (e.g., KafkaStream, TopicFilter, etc).\n\nodd, I have them local and when trying to add them I get a warning saying they are already under version control I wonder if it is some odd svn thing not getting picked up with the diff since the merge has them in the branch now and\n\nsvn status shows them in there so they will end up getting commited\n\nA  +    core/src/main/scala/kafka/consumer/KafkaStream.scala\nA  +    core/src/main/scala/kafka/consumer/TopicFilter.scala\n\nalong with the other added files are in svn status very weird they are not showing through the diff... ugh will look into that but want to get these errors cleared first\n\n<< For addPartitionTopicInfo(currentTopicRegistry, topicDirs, partition, topic, consumerThreadId): \n<<In 0.8, the method doesn't take topicRegistry in the input any more, but uses it in the method directly. Also, in 0.8, we are using the partitionId as the key in the inner map of topicRegistry, instead Partition object. \n\nthanks this is helpful going through some of the nuances I should be able to work through these more later this evening along with the conflict for releasePartitionOwnership", "ok, I started again from scratch and took detailed notes for all conflicts from the merge and what how it was resolved.  \n\nAttached are \n\n1) my notes for all the conflicts encountered\n2) the latest patch.  Please note I figured out why the \"added\" files to trunk are not showing up in the diff from the merge.  the reason is because these files are actually not new but actually a direct descendant of a real file in the repo.  supposedly a new version of SVN client has a new flag called --show-copies-as-adds so that these files will show up in the diff.  I will update a machine for the latest client version of SVN and make a new patch so it is holy complete before commit \n\nI did want to post everything for where it is now because:\n\n1) there is one last error\n\n[info] Compiling main sources...\n[error] /Users/josephstein/rebase_kafka/rebase_0.8/core/src/main/scala/kafka/server/KafkaApis.scala:100: type mismatch;\n[error]  found   : kafka.message.MessageSet\n[error]  required: kafka.message.ByteBufferMessageSet\n[error]           log.append(partitionData.messages)\n[error]                                    ^\n[error] one error found\n\nthis is a result of KAFKA-310 on the trunk changing the append function in Log.scala https://github.com/apache/kafka/commit/2bcc91134316a546322bf59a422bec1934613607\n\nand the use of that function in KAFKA-49\n\nhttps://github.com/apache/kafka/commit/1461a3877f34db9ac5ce1f809d53bc353df24b01 \n\nany thoughts?\n\n2) review of everything going in\n\nI will work on updating svn so i can produce a patch with the merged copies that are from trunk as new to this branch and also mull on this error maybe someone knows a fix here? ", "I updated to the latest svn and produced a patch that is inclusive of the new files that were on the trunk (using --show-copies-as-adds) since they are not technically new to the tree the older SVN could not deal with that in a diff.\n\nI also updated and integrated Prashanth commit from this morning so the rebase is complete (added that to the notes)\n\nStill hung up on how to fix this error \n\n\n[error] /Users/josephstein/rebase_kafka/rebase_0.8/core/src/main/scala/kafka/server/KafkaApis.scala:99: type mismatch;\n[error]  found   : kafka.message.MessageSet\n[error]  required: kafka.message.ByteBufferMessageSet\n[error]           log.append(partitionData.messages)\n[error]                                    ^\n[error] one error found\n\nI don't know enough about KAFKA-310 but if there is a way to change that back to use MessageSet again we don't have to re-wire the new PartitionData structures unless someone knows of a way or I can spend time to figure out a way to conver them (seems clunky converting an abstract base class into the child class...)", "I got past this error making \n\nlog.append(partitionData.messages)\n\ninto\n\nlog.append(new ByteBufferMessageSet(partitionData.messages.getSerialized))\n\nnow the rest of tests are compiling 46 more errors working through them now (likely just messed up the merge of the conflicted imports is all)", "Joe,\n\nPartitionData has to use MessageSet since FetchRequest creates PartitionData with FileMessageSet, instead of ByteBufferMessageSet. However, we know that in ProduceRequest, PartitionData always uses ByteBufferMessageSet. So, we can just do log.append( (ByteBufferMessageSet) partitionData.messages).", "Looks like I am down to 1 test case failing\n\nwanted to post where I am probably not able to look at this again until tomorrow evening or the next day\n\n\n[error] Test Failed: testQueueTimeExpired(kafka.producer.AsyncProducerTest)\njava.lang.AssertionError: \n  Expectation failure on verify:\n    handle(ListBuffer(ProducerData(topic1,null,List(msg0)), ProducerData(topic1,null,List(msg1)))): expected: 1, actual: 0\n\tat org.easymock.internal.MocksControl.verify(MocksControl.java:184)\n\tat org.easymock.EasyMock.verify(EasyMock.java:2038)\n\tat kafka.producer.AsyncProducerTest.testQueueTimeExpired(AsyncProducerTest.scala:163)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat junit.framework.TestCase.runTest(TestCase.java:164)\n\tat junit.framework.TestCase.runBare(TestCase.java:130)\n\tat junit.framework.TestResult$1.protect(TestResult.java:110)\n\tat junit.framework.TestResult.runProtected(TestResult.java:128)\n\tat junit.framework.TestResult.run(TestResult.java:113)\n\tat junit.framework.TestCase.run(TestCase.java:120)\n\tat junit.framework.TestSuite.runTest(TestSuite.java:228)\n\tat junit.framework.TestSuite.run(TestSuite.java:223)\n\tat junit.framework.TestSuite.runTest(TestSuite.java:228)\n\tat junit.framework.TestSuite.run(TestSuite.java:223)\n\tat org.scalatest.junit.JUnit3Suite.run(JUnit3Suite.scala:309)\n\tat org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:40)\n\tat sbt.TestRunner.run(TestFramework.scala:53)\n\tat sbt.TestRunner.runTest$1(TestFramework.scala:67)\n\tat sbt.TestRunner.run(TestFramework.scala:76)\n\tat sbt.TestFramework$$anonfun$10$$anonfun$apply$11.runTest$2(TestFramework.scala:194)\n\tat sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)\n\tat sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)\n\tat sbt.NamedTestTask.run(TestFramework.scala:92)\n\tat sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)\n\tat sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)\n\tat sbt.TaskManager$Task.invoke(TaskManager.scala:62)\n\tat sbt.impl.RunTask.doRun$1(RunTask.scala:77)\n\tat sbt.impl.RunTask.runTask(RunTask.scala:85)\n\tat sbt.impl.RunTask.sbt$impl$RunTask$$runIfNotRoot(RunTask.scala:60)\n\tat sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)\n\tat sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)\n\tat sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)\n\tat sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)\n\tat sbt.Control$.trapUnit(Control.scala:19)\n\tat sbt.Distributor$Run$Worker.run(ParallelRunner.scala:131)\n", "Joe,\n\nThanks a lot for your help. Really appreciate it. I fixed the broken unit test and my earlier comment on converting MessageSet to ByteBufferMessageSet. To get the fix, please\na. apply your patch\nb. revert ProducerSendThread.scala and KafkaApis.scala\nc. apply the new patch I attached.\n\nAnother comment. Please remove unused imports in the following classes:\nBackwarsCompatibilityTest\nKafkaETLContext\nKafkaLog4jAppender\nKafkaLog4jAppenderTest\nKafkaRequetHandler\nSeverShutdownTest\nSimpleConsumerDemo\nTestUtils\n\nOther than that, the patch is ready for commit. Since this patch has a lot of changes, we probably should let you commit this before kafka-46.", "committed\n\n+1 to dual commits ", "Reopen the jira. Found a few missed merges and included them in kafka-348-delta.patch.", "thanks Jun, reviewed and commited"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "rebase 0.8 branch from trunk"}, {"q": "What updates or decisions were made in the discussion?", "a": "thanks Jun, reviewed and commited"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-349", "title": "Create individual \"Response\" types for each kind of request and wrap them with \"BoundedByteBufferSend\", remove \"xxResponseSend\" types for all requests except \"FetchRequest\"", "status": "Resolved", "priority": "Major", "reporter": "Yang Ye", "assignee": "Yang Ye", "labels": [], "created": "2012-05-25T22:59:20.000+0000", "updated": "2012-06-06T01:30:17.000+0000", "description": null, "comments": ["1. Replace \"Requst\" type with \"RequestOrResponse\" and move it to \"kafka.api\" from \"kafka.network\"\n\n2. Create \"FetchResponse\", \"ProducerResponse\", \"TopicMetaDataResponse\", \"OffsetResponse\" class\n\n3. Move the error code inside the response classes \n\n4. Change the code in \"KafkaApis\" in response to the change in response formats\n\n5. Change the Blocking channel receive function to receive just the response (with error code inside) instead of tuple of response and error code.\n\n6. Remove unnecessary \"serialization\" and \"deserialization\" functions\n\n7. Change corresponding test classes", "That's a good cleanup patch. Thanks. Some comments:\n1. Maybe we should use this opportunity to make the error code length in all responses consistent (use short int instead of int).\n2. The response classes don't need to store CurrentVersion since their version is always the same as the corresponding request.", "1. Change all the error code in the system to Short type\n2. Remove the \"version id\" type from all the response classes", "Thanks for patch v5. One more comment:\n\n51. FetchResponse,ProducerResponse: We will still need to send the versionId to the client so that the client knows if it gets the right version of the response. We just don't need CurrentVersion in FetchResponse/ProducerResponse since we should just refer to the version constant in FetchRequest/ProducerRequest.\n", "Also, unit tests seem to fail due to the wire protocol changes.", "1. Added the versionId in ProducerResponse and FetchResponse\n2. Fix one error that leads to test failures. All unit tests been tested through, all passed except two (\"testCleanupExpiredSegments\" in \"LogManagerTest\") and \"BackwardsCompatibilityTest\", but they also fail at the Kafka 0.8 repo. (So it's irrelevant to this patch)", "Thanks for patch v6. Some new comments:\n\n61. FetchResponse, ProducerResponse: VersionId should be the first item is serialized data since we will be depending on VersionId to deserialize the right version of the data in the future. \n\n62. Could you rebase since trunk has moved?\n\n63. The only test that fails for me is testCleanupExpiredSegments, which is fixed as part of merging from trunk to 0.8. If you rebase, that test should pass.", "I'm sorry but it's really hard to decouple the RPC patch from the \"cleanup\" patch, since the latter depends on the first. Jun, can you help check them in all together? This submitted patch include three parts:\n\n1. Rebase from Neha's latest commit\n2. the cleanup patch\n3. the RPC patch.\n\nAll tests passed. ", "Thanks for patch v7. Some comments:\n\n71. LeaderAndISRRequest:\n71.1 constructor: can we put versionId and clientId before the rest of the fields?\n71.2 writeTo(): we should put versionId first and clientId second.\n\n72. KafkaApis: in handleLeaderAndISR() and handleStopReplica(), add a TODO comment that the actually logic will be put in later.\n\n73: ProducerReponse.writeTo: Let's put correlationId before errorcode, to be consistent.\n\n74. ControllerToBrokerRequestTest seems to fail for me. You need to remove unit. from the package name.\n", "75. ControllerToBrokerRequestTest should use TestUtils.createBrokerConfigs to create broker properties.", "71. LeaderAndISRRequest:\n71.1 constructor: can we put versionId and clientId before the rest of the fields?\n\ndid that\n\n71.2 writeTo(): we should put versionId first and clientId second.\n\ndid that\n\n72. KafkaApis: in handleLeaderAndISR() and handleStopReplica(), add a TODO comment that the actually logic will be put in later.\n\ndid that\n\n\n73: ProducerReponse.writeTo: Let's put correlationId before errorcode, to be consistent.\n\ndid that\n\n\n74. ControllerToBrokerRequestTest seems to fail for me. You need to remove unit. from the package name.\n\ndid that\n\n75. ControllerToBrokerRequestTest should use TestUtils.createBrokerConfigs to create broker properties.\n\ndid that\n\n\nAll tests passed on my local machine", "A few more comments for patch v8.\n\n81. LeaderAndISRRequest and StopReplicaRequest: Could we add a default value for versionId and clientId in the constructor (like in FetchRequest)?\n\n82. ProducerResponse and OffsetResponse: remove val CurrentVersion.\n\n", "81. LeaderAndISRRequest and StopReplicaRequest: Could we add a default value for versionId and clientId in the constructor (like in FetchRequest)?\n\n\nDid that\n\n\n82. ProducerResponse and OffsetResponse: remove val CurrentVersion. \n\nDid that", "Thanks for patch v9. Just committed to 0.8."], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Create individual \"Response\" types for each kind of request and wrap them with \"BoundedByteBufferSend\", remove \"xxResponseSend\" types for all requests except \"FetchRequest\""}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for patch v9. Just committed to 0.8."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-350", "title": "Enable message replication in the presence of controlled failures", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "labels": [], "created": "2012-05-26T02:22:52.000+0000", "updated": "2012-07-24T18:13:32.000+0000", "description": "KAFKA-46 introduced message replication feature in the absence of server failures. This JIRA will improve the log recovery logic and fix other bugs to enable message replication to happen in the presence of controlled server failures", "comments": ["Handle the logic of deleting segments that have start offset > highwatermark during log recovery. ", "This patch contains fixes to bugs in various components to make message replication work in the presence of controlled server failures. The system test under system_test/single_host_multiple_brokers passes with server failures enabled. \n\nThis patch contains the following changes -\n\n1. Topic metadata request bug\n\n1.1. While responding to a topic metadata request bug, the server uses the AdminUtils to query ZK for the host-port info for the leader and other replicas. However, it can happen that one of the brokers in the replica is offline and hence its broker registration in ZK is unavailable. Since the system test simulates exactly this scenario, the server's request handler thread was exiting due to a NoNodeException while reading the /brokers/ids/[id] path. The general problem seems to be handling error codes correctly in the request handlers and sending them to the client. I think once KAFKA-402 is resolved, all error codes will be handled cleanly by the server. For now, I've modified topic metadata response to have an error code per topic as well as per partition. So, if the leader is not available for some partitions, it will set the LeaderNotAvailable error code at the partition level. If some other replica is not available, it will set the ReplicaNotAvailable error code in the response instead of throwing an exception and exiting.\n1.2. On the producer side, it fails the produce request retry if the leader for that partition is not available. It logs all other kinds of errors and ignores them. \n1.3. Changed some unit tests in AdminTest to elect a leader before making topic metadata requests, so that the test passes.\n1.4. Fixed another bug in the deserialization of the topic metadata response that read the versionId and errorCode in incorrect order.\n1.5. In DefaultEventHandler, during a retry, it relied on the topic cache inside BrokerPartitionInfo for refreshing topic metadata. However, if the topic cache is empty (when no previous send request has succeeded), it doesn't end up refreshing the metadata for the required topics in the produce request. Fixed this by passing the list of topics explicitly in the call to updateInfo()\n1.6. In general, it seems like a good idea to have a global error code for the entire response (to handle global errors like illegal request format), then a per topic error code (to handle error codes like UnknownTopic), a per-partition error code (to handle partition-level error codes like LeaderNotAvailable) and also a per-replica error code (to handle ReplicaNotAvailable). Jun had a good suggestion about the format of TopicMetadata response. I would like to file another JIRA to improve the format of topic metadata request.\n\n2. Bug in TestUtils waitUntilLeaderIsElected was signalling a condition object without acquiring the corresponding lock. This error message was probably getting masked since we had turned off ERROR log4j level for unit tests. Fixed this.\n\n3. Log recovery\n3.1. Removed the highWatermark() API in Log.scala, since we used âhighwatermarkâ to indicate the offset of the latest message flushed to disk. This was causing the server to prevent the replicas from fetching unflushed data on the leader. With replication, we use highwatermark to denote the offset of the latest committed message. I removed all references to the older API (highWatermark). To remain consistent with setHW, I added an API called getHW. As I understand it, these APIs will be refactored/renamed to match conventions, when KAFKA354 is resolved\n3.2. To handle truncating a log segment, I added a truncateUpto API that takes in the checkpointed highwatermark value for that partition,  computes the correct end offset for that log segment and truncates data after the computed end offset\n3.3. Improved log recovery to delete segments that have start offset > highwatermark\n3.4. Fixed logEndOffset to return the absolute offset of the last message in the log for that partition.\n3.5. To limit the changes in this patch, it does not move the highwatermarks for all partitions to a single file. This will be done as part of KAFKA-405\n3.6. Added a LogRecoveryTest to test recovery of log segments with and without failures\n\n4. Config options\n4.1. We might want to revisit the defaults for all config options. For example, the isr.keep.in.sync.time.ms defaulted to 30s which seems way too optimistic.  While running the system tests, most messages timed out since the isr.keep.in.sync.time.ms and the frequency of bouncing the replicas was also roughly 30s. The socket timeout for the producer also defaults to 30s which seems very long to block a producer for.\n4.2.  The socket.timeout.ms should probably be set to producer.request.ack.timeout.ms, if it is a non-negative value. If producer.request.ack.timeout.ms  = -1, socket.timeout.ms should probably default to a meaningful value. While running the system test, I observed that the socket timeout was 30s and the producer.request.ack.timeout.ms was -1, which means that the producer would always block for 30s if the server failed to send back an ACK. Since the frequency of bouncing the brokers was also 30s, most produce requests were timing out.\n\n5. Socket server bugs\n5.1. A bug in processNewResponses() causes the SocketServer to process responses inspite of it going through a shutdown. It probably makes sense to let outstanding requests timeout and shutdown immediately\n5.2. Another bug in processNewResponses() causes it to go in an infinite loop when the selection key processing throws an exception. It failed to move to the next key in this case. I fixed it by moving the next key processing in the finally block.\n\n6. Moved the zookeeper client connection from startup() API in KafkaZookeeper to startup() API  in KafkaServer.scala. This is because the ReplicaManager is instantiated right after KafkaZookeeper and was passed in the zkclient object created by KafkaZookeeper. Since KafkaZookeeper started the zkclient only in startup() API, ReplicaManager's API's got NPE while trying to access the passed-in zkclient. With the fix, we can create the zookeeper client connection once in the KafkaServer startup, pass it around and tear it down in the shutdown API of KafkaServer. \n\n7. System testing\n7.1. Hacked ProducerPerformance to turn off forceful use of the async producer. I guess ProducerPerformance has grown over time into a complex blob of if-else statements. Will file another JIRA to refactor it and fix it so that all config options work well with both sync and async producer.\n7.2. Added producer ACK config options to ProducerPerformance. Right now, they are hardcoded. I'm hoping this can be fixed with the above JIRA. \n7.3. The single_host_multiple_brokers system test needs to be changed after KAFKA-353. Basically, it needs to count the successfully sent messages correctly. Right now, there is no good way to do this in the script. One way is to have the system test script grep the logs to find the successfully ACKed producer requests. To get it working for now, hacked it to use sync producer. Hence, before these issues are fixed it will be pretty slow.\n7.4. Improved the system test to start the console consumer at the very end of the test for verification\n\n8. Another thing that can be fixed in KAFKA-402 is the following -\n8.1. When a broker gets a request for a partition for which it is not the leader, it should sent back a response with an error code immediately. Right now, I see a produce request timing out in LogRecoveryTest since the producer never gets a response. It doesn't break the test since the producer retries, but it is adding unnecessary delay.\n8.2. In quite a few places, we log an error and then retry, or log an error which is actually meant to be a warning. Due to this, it is hard to spot real errors during testing. We can probably fix this too as part of KAFKA-402. \n\n8. Found a NPE while running ReplicaFetchTest. Would like to file a bug for it\n\n9. Controller bugs \n9.1. Unit tests fail intermittently due to NoSuchElementException thrown by testZKSendWithDeadBroker() in KafkaController. Due to this, the zookeeper server doesn't shut down and rest of the unit tests fail. The root cause is absence of a broker id as a key in the java map. I think Scala maps should be used as it forces the user to handle invalid values cleanly through Option variables.\n9.2. ControllerBasicTest tests throw tons of zookeeper warnings that complain a client not cleanly closing a zookeeper session. The sessions are forcefully closed only when the zookeeper server is shutdown. These warnings should be paid attention to and fixed. LogTest is also throwing similar zk warnings.\n9.3. Since controller bugs were not in the critical path to getting replication-with-failures to work, I'd like to file another JIRA to fix it.\n\nIf the above future work sounds good, I will go ahead and file the JIRAs", "This is a pretty hard to review due to the large number of changes and also because I don't know some of this code well.\n\nA lot of things like bad logging/naming that I think you could probably catch just perusing the diff.\n\nLog:\n- Log should not know about hw right? We seem to be adding more hw stuff there?\n- This adds a getHW() that just returns a private val, why not make the val public? Please fix these. Regardless of cleanup being done get/set methods have been against the style guide for a long time, lets not add more. Ditto getEndOffset() which in addition to being a getter is inconsistent with Log.logEndOffset\n- There is debug statement in a for loop in Log.scala that needs to be removed\n- I don't understand the difference between nextAppendOffset and logEndOffset. Can you make it clear in the javadoc and explain on why we need both of these. Our public interface to Log is getting incredibly complex, which is really sad so I think we should really think through deeply what is added here and why.\n- The javadoc on line 138 of Log.scala doesn't match the style of javadoc for the preceeding 5 variables.\n\n- Does making isr.keep.in.sync.time.ms more stringent actually make sense? 10 seconds is pretty tight. I think what you are saying is that every server bounce will introduce 30 seconds of latency. But I think that is kind of a weakness of our design. If we lower that timeout we may just get spurious dropped replicas, no?\n- Can we change the name of isr.keep.in.sync.time.ms to replica.max.lag.time.ms?\n- Good point about the socket timeouts. We can't set socket timeout equal to request timeout, though, as there may be a large network latency. I recommend we just default the socket timeout to something large (like 10x the request timeout), and throw an exception if it is less than the request timeout (since that is certainly wrong). I don't think we should be using the socket timeout except as an emergency escape for a totally hung broker now that we have the nice per-request timeout.\n- Can we change producer.request.ack.timeout.ms to producer.request.timeout.ms so it is more intuitive? I don't think the word \"ack\" is going to be self-explanatory to users.\n\nSocketServer.scala\n- Please remove: info(\"Shut down acceptor completed\")\n- Is there a reason to add the port into the thread name? That seems extremely odd...is it to simplify testing where there are multiple servers on a machine?\n- Why is it a bug for processNewResponses() to happen while a shutdown is occuring. I don't think that is a bug. That is called in the event loop. It is the loop that should stop, no? Is there any ill effect of this?\n- Good catch on the infinite loop\n\nSystem testing\n- I think we should fix the performance test hacks. The performance tool is critical. I have watched this play out before. No one ever budgets time for making the performance test stuff usable and then it just gets re-written umpteen times and never does what is needed. Most of these are just a matter of some basic cleanup and adding options. Let's work clean.\n\nAdminUtils\n- I don't understand the change in getTopicMetaDataFromZK\n\nReplica.scala\n- Can you remove trace(\"Returning leader replica for leader \" + leaderReplicaId) unless you think it is of value going forward\n\nErrorMapping.scala\n- getMaxErrorCodeValue - seems to be recomputed for each TopicMetadata. Let's get rid of this, I don't think we need it. We already have an unknown entry in the mapping, we should use that and get rid of the Utils.getShortInRange\n- If we do want to keep it, fix the name\n- We should really give a base class KafkaException to all exceptions so the client can handle them more easily\n- Instead of having the client get an IllegalArgumentException we should just throw new KafkaException(\"Unknown server error (error code \" + code + \")\")\n- The file NoLeaderForPartitionException seems to be empty now, I think you meant to delete it\n\nConsoleConsumer\n- What does moving those things into a finally block do? We just caught all exceptions...\n\nFileMessageSet\n- You added more log spam. Please format it so it is intelligible to someone not working on the code or remove: debug(\"flush size:\" + sizeInBytes())\n- Ditto info(\"recover upto size:\" + sizeInBytes())\n\nBrokerPartitionInfos is a really weird class\n\nDefaultEventHandler\n- This seems to have grown into a big glump of proceedural logic.\n- Inconsistent spacing of parens should be cleaned up\n- partitionAndCollate is extemely complex\n- Option[Map[Int, Map[(String, Int), Seq[ProducerData[K,Message]]]]]\n\nZkUtils\n- LeaderExists class needs a name that is a noun\n- Also we have a class with the same name in TopicMetadata\n\nI really like TestUtils.waitUntilLeaderIsElected. God bless you for not adding sleeps. We should consider repeating this pattern for other cases like this.\n\nZooKeeperTestHarness.scala\n- Can we replace the thread.sleep with a waitUntilZkIsUp call?\n\n", "Would it be better to have a general read/write formatting for boolean, like writeBoolean/readBoolean just like writeShortString/readShortString? Then we do not need to specify, for example, in TopicMetadata:\n\nsealed trait LeaderRequest { def requestId: Byte }\ncase object LeaderExists extends LeaderRequest { val requestId: Byte = 1 }\ncase object LeaderDoesNotExist extends LeaderRequest { val requestId: Byte = 0 }\n\n\nwhich would need to be done for every request/response that has a boolean.", "Thanks for the review, Jay ! Here is another patch fixing almost all review comments -\n\n1. Log\n1. Refactoring of the hw logic is part of KAFKA-405. It is moved to a new file HighwaterMarkCheckpoint and is controlled by the ReplicaManager. The Log does not and should not know about high watermarks.\n2. Getter/setter for hw is removed as part of KAFKA-405 anyways.\n3. Agree with you on the weak public interface, especially Log needs a cleanup. I think you attempted that as part of KAFKA-371. I've cleaned up quite a few things as part of KAFKA-405 and this patch. Nevertheless, fixed the nextAppendOffset as part of this patch. It is not required when we have logEndOffset. Also, removed the getEndOffset from FileMessageSet. Added endOffset() API to the LogSegment in addition to a size() API. This is useful during truncation of the log based on high watermark.\n4. Fixed the javadoc and removed the debug statement.\n\n2. Config options\n1. The isr.keep.in.sync.time.ms set to 10 seconds is also very lax. A healthy follower should be able to catch up in 100s of milliseconds even with a lot of topics, with a worst case of maybe 4-5 seconds. We will know the latency better when we run some large scale tests. But yeah, the issue with the system test is independent of what the right value should be. I was just explaining how I discovered this issue. :)\n2. Good point about renaming it to replica.max.lag.time.ms. Also changed isr.keep.in.sync.bytes to replica.max.lag.bytes.\n3. Since the producer does a blocking read on the socket, the socket timeout cannot be greater than the request timeout. If it is, then the request timeout guarantee would be violated, no ?\n\n3. SocketServer.scala\n1. Removed info log statement\n2. Yes, wanted to simplify testing/debugging (thread dumps) when there are multiple servers on one machine. Not sure if this is the best way to do that.\n3. processNewResponses() doesn't have to process outstanding requests during shutdown. It can shutdown by ignoring them and those requests will timeout anyways. But yes, good point about the event loop doing it instead. Fixed it.\n\n4. System testing\n1. Fixed the hacky change. I still need ProducerPerformance needs a complete redo. Filed bug KAFKA-408 to do that.\n\n5. AdminUtils\n1. Need this change to return appropriate error codes for topic metadata request. Without this, all produce requests are timing out while fetching metadata, since in the system test, at any point of time, one replica is always down.\n\n6. Partition.scala\n1. Removed the trace statement.\n\n7. ErrorMapping\n1. Removed getMaxErrorCode and its usage from ErrorMapping\n2. Good point. Introduced a new class KafkaException and converted IllegalStateException and IllegalArgumentException to it.\n3. NoLeaderForPartitionException is in fact marked as deleted in the patch\n\n8. ConsoleConsumer\n1. Good point. The finally block there didn't really make any sense.\n\n9. FileMessageSet\n1. I haven't added the log statements in this patch, we always had it. \n2. The purpose of adding that was help with debugging producer side queue full exceptions. We turned on DEBUG to see what the server side flush sizes and latencies were. I think these statements were added when we didn't have an ability to monitor these. However, whoever added the log flush monitoring maybe forgot to remove these statements. I removed it in this patch.\n3. Removed the ârecover uptoâ log statement too\n\n10. DefaultEventHandler\n1. Agree with the unwieldly procedural code. Filed bug KAFKA-409 to clean this up. \n\n11. ZkUtils\n1. Renamed LeaderExists -> LeaderExistsListener\n\n12. ZookeeperTestHarness\n1. The sleep is actually not required. The EmbeddedZookeeper constructor returns only after the zk server is completely started.", "Thanks for patch v2. Some comments:\n\n20. PartitionMetadata: Could we make getLeaderRequest() private?\n\n21. DefaultEventHandler.partitionAndCollate(): Is it necessary for this method to return an Option? It seems that if this method hits any exception, it's simpler to just pass on the exception and let the caller deal with it. \n\n22. LogSegment: Ideally this class should only deal with offsets local to the segment. Global offsets are only used at the Log level. So it's probably better to use local offset in the input of truncateUpto(). Similarly, we probably don't need endOffset since it returns global offset. If we do want to keep it, it probably should be named globalEndOffset.\n\n23. Log: Since we removed the HW check in FileMessageSet.read. We will need to add a guard in Log.read() so that we only expose messages up to HW to the consumer.\n\n24. LogManager: remove unused import\n\n25. ControllerChannelManager.removeBroker: the error message is not correct since this method is called in places other than shutdown too.\n\n26. system_test/single_host_multi_brokers/bin/run-test.sh: We should remove comments saying \"If KAFKA-350 is fixed\".\n\n27. Unit tests pass on my desktop but fail on my laptop at the following test consistently. Without the patch, unit tests pass on my laptop too. This seems to be due to the change in ZooKeeperTestHarness.\n[info] Test Starting: testFetcher(kafka.consumer.FetcherTest)\n[error] Test Failed: testFetcher(kafka.consumer.FetcherTest)\norg.I0Itec.zkclient.exception.ZkTimeoutException: Unable to connect to zookeeper server within timeout: 2000\n\tat org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)\n\tat org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)\n\tat org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)\n\tat kafka.zk.ZooKeeperTestHarness$class.setUp(ZooKeeperTestHarness.scala:31)\n\tat kafka.consumer.FetcherTest.kafka$integration$KafkaServerTestHarness$$super$setUp(FetcherTest.scala:35)\n\tat kafka.integration.KafkaServerTestHarness$class.setUp(KafkaServerTestHarness.scala:35)\n\tat kafka.consumer.FetcherTest.setUp(FetcherTest.scala:57)\n", "1.1, 1.2, 2.1, 2.2 Awesome!\n\n2.3 Hmm, well so but that doesn't make sense because we would always pop the socket timeout and fully close out the connection, which is not good. Basically the socket timeout would ALWAYS go off before the server-side request processing timeout due to the network latency both ways. I think that timeout should just be for \"emergencies\". I agree it is a bit of a muddle now that we have two.\n\n7.2 Hmm, but that changed ALL IllegalStateExceptions to KafkaException. IllegalStateException is used to say \"this should not be possible\". For example in SocketServer those checks are in the form of assertions. I like the idea of KafkaException, and I think we should make all the other exceptions extend it to ease error handling on the client, but I don't think we should get rid of IllegalStateException or IllegalArgumentException, they are informative.\n", "Thanks for the review !\n\nJun's comments -\n\n20. Made getLeaderRequest and getLogSegmentMetadataRequest private\n21. partitionAndCollate() is used by dispatchSerializedData() API. The contract of dispatchSerializedData() is to return the list of messages that were not sent successfully so that they can be used on the next retry. Now, if partitionAndCollate() fails, we need dispatchSerializedData to return the entire input list of messages. If something else fails after partitionAndCollate, we need to return a filtered list of messages back. To handle this, partitionAndCollate returns an Option. I agree that this class has very complex code that needs a cleanup. Have filed KAFKA-409 for that.\n22. We need to know the absolute end offset of the segment for truncation during make follower state change. Have changed the name of the API to absoluteEndOffset. \n23. We have KAFKA-376 filed to ensure that the consumer sees data only after the HW. This patch exposes data upto log end offset to replicas as well as consumers. So FileMessageSet.read() exposes all data upto log end offset.\n24. Removed unused import.\n25. Hmm, fixed the error message\n26. Removed the TODOs related to KAFKA-350\n27. Introduced the connection timeout and session timeout variables in the test harness and set it to 6s. \n\nJay's comments -\n\n2.3 So we talked about this offline, stating it here so that others can follow. There are 2 choices here -\n2.3.1. One is to keep request timeout = socket timeout. This would make us fully dependent on the socket for the timeout. So, any requests that took longer than that on the server for some reason (GC, bugs etc), would throw SocketTimeoutException on the producer client. The only downside to this is that the producer client wouldn't know why it failed or whether it got written to 0 or 1 or n replicas. Al though, this should be very rare and in the normal case, the server process the request and be able to send the response within the timeout. This is assuming the timeout value was set counting the network delay as well. So if the request is travelling cross-colo, it is expected that you set the timeout keeping in mind the cross-colo network latency. \n2.3.2. The second choice is to set the socket timeout >> request timeout. This would ensure that in some rare failure cases on the server, the producer client would still be able to get back a response (most of the times) with a descriptive error message explaining the cause of the failure. However, it would also mean that under some failures like (server side kernel bug crashing the server etc), the timeout would actually be the socket timeout, which is usually set to a much higher value. This can confuse users who might expect the timeout to be the request timeout. Also, having two different timeouts also seems to complicate the guarantees provided by Kafka \n2.3.3. I personally think option 1 is simpler and provides no worse guarantees than option 2. This patch just sets the socket timeout to be the request timeout. \n\n2.4 Yeah, I think differentiating between âthis should not be possibleâ and âthis should not be possible in Kafkaâ is a little tricky. On one hand, it seems nicer to know that any exception thrown by Kafka will either be KafkaException or some exception that extends KafkaException. On the other hand, some states are just impossible and must be treated like assertions, for example, a socketchannel key that is not readable or writable or even valid. And in such cases, it might be slightly more convenient to have IllegalStateException. And I'm not sure I know the right answer here. I took a pass over all the IllegalStateException usages and converted the ones I think should be KafkaException, but I might not have done it in the best way possible.\n", "Thanks for patch v3. \n\nFor 21, my point is that the exceptions that can be thrown in partitionAndCollate() are non-recoverable and therefore retries won't help. partitionAndCollate() won't throw NoBrokersForPartitionException since only BrokerPartitionInfo.updateInfo can throw such an exception and updateInfo is not called here. partitionAndCollate() throws InvalidPartitionException that indicates a wrong partition of a topic. If we just retry, we will hit the same exception and fail again. It's simpler to just throw an exception and treat it as a failed case. Ditto for other exceptions that partitionAndCollate() may throw.\n\nA few new comments:\n30. SyncProducerConfig.requestTimeoutMs: We should make sure that the value is a positive integer and change the comment accordingly.\n\n31. IteraterTemplate: The two KafkaExceptions are better reverted to IllegalStateException.\n\n32. ProducerPerformance: We should remove socketTimeoutMsOpt.\n\n33. SocketServer: no need to import illegalStateException\n\n34. I got the following exception when running system_test/single_host_multi_brokers/bin/run-test.sh\n\n2012-07-24 00:22:51 cleaning up kafka server log/data dir\n2012-07-24 00:22:53 starting zookeeper\n2012-07-24 00:22:55 starting cluster\n2012-07-24 00:22:55 starting kafka server\n2012-07-24 00:22:55   -> kafka_pids[1]: 75282\n2012-07-24 00:22:55 starting kafka server\n2012-07-24 00:22:55   -> kafka_pids[2]: 75286\n2012-07-24 00:22:55 starting kafka server\n2012-07-24 00:22:55   -> kafka_pids[3]: 75291\n2012-07-24 00:22:57 creating topic [mytest] on [localhost:2181]\ncreation failed because of org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/ids\norg.I0Itec.zkclient.exception.ZkNoNodeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/ids\n\tat org.I0Itec.zkclient.exception.ZkException.create(ZkException.java:47)\n\tat org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:685)\n\tat org.I0Itec.zkclient.ZkClient.getChildren(ZkClient.java:413)\n\tat org.I0Itec.zkclient.ZkClient.getChildren(ZkClient.java:409)\n\tat kafka.utils.ZkUtils$.getChildren(ZkUtils.scala:363)\n\tat kafka.utils.ZkUtils$.getSortedBrokerList(ZkUtils.scala:80)\n\tat kafka.admin.CreateTopicCommand$.createTopic(CreateTopicCommand.scala:86)\n\tat kafka.admin.CreateTopicCommand$.main(CreateTopicCommand.scala:73)\n\tat kafka.admin.CreateTopicCommand.main(CreateTopicCommand.scala)\nCaused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/ids\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:102)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:42)\n\tat org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1249)\n\tat org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1277)\n\tat org.I0Itec.zkclient.ZkConnection.getChildren(ZkConnection.java:99)\n\tat org.I0Itec.zkclient.ZkClient$2.call(ZkClient.java:416)\n\tat org.I0Itec.zkclient.ZkClient$2.call(ZkClient.java:413)\n\tat org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:675)\n\t... 7 more\n", "\n21. Not really. updateInfo() is not the only API that throws NoBrokersForPartitionException. partitionAndCollate can encounter that exception while calling the getPartitionListForTopic() API.  But you raised a good point here. What partitionAndCollate() should do (given its current design) is to log a retry warning for recoverable exceptions and return None, so that they are included in the retry. For non-recoverable exceptions, it should just throw the exception to the handle API and mark it failed.\n\n30. Added an error message for non-positive values for request timeout. Al though a value of zero means infinite timeout\n31. I guess so. Changed it back to IllegalStateException\n32. Removed socket timeout related stuff from ProducerPerformance\n33. Removed the import.\n34. Hmm, that can happen if you try to create a topic when the brokers haven't yet registered in zookeeper. The system tests waits for 2 seconds, which should be enough for couple of brokers to startup in the normal case. I haven't been able to reproduce this. Its possible that this is something that can be fixed in the system test. File a bug if you see it.", "Thanks for patch v4. We are almost there.\n\n21. In partitionAndCollate(), UnknownTopicException doesn't seem to recoverable.\n\n30. It doesn't look like the broker handles requestTimeoutMs of 0 through RequestPurgatory. It's probably simpler if we just require timeout to be positive. If someone wants infinite timeout, MAX_INT can be used instead.\n\nIf those issues are addressed, the patch can be committed without another round of review.\n\n34. This seems to only happen on my laptop. Will file another jira. ", "21. It is recoverable when there is auto create topic enabled on the server. Until we get rid of auto create, I think we can keep this. \n30. Changed it to be non-negative and non-zero\n\nCommitting it with these changes."], "derived": {"summary": "KAFKA-46 introduced message replication feature in the absence of server failures. This JIRA will improve the log recovery logic and fix other bugs to enable message replication to happen in the presence of controlled server failures.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Enable message replication in the presence of controlled failures - KAFKA-46 introduced message replication feature in the absence of server failures. This JIRA will improve the log recovery logic and fix other bugs to enable message replication to happen in the presence of controlled server failures."}, {"q": "What updates or decisions were made in the discussion?", "a": "21. It is recoverable when there is auto create topic enabled on the server. Until we get rid of auto create, I think we can keep this. \n30. Changed it to be non-negative and non-zero\n\nCommitting it with these changes."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-351", "title": "Refactor some new components introduced for replication ", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Jun Rao", "labels": ["optimization"], "created": "2012-05-26T02:26:50.000+0000", "updated": "2017-10-30T12:18:23.000+0000", "description": "Jay had some good refactoring suggestions as part of the review for KAFKA-46. I'd like to file this umbrella JIRA with individual sub tasks to cover those suggestions", "comments": ["From KAFKA-405 -\n\nReplicaManager:\n\nThis class has a very odd public interface. Instead of managing replicas it has a bunch of passive calls--addLocalReplica(), addRemoteReplica(), etc. Who calls these? KafkaServer seems to have its own wrapper for these. Then it passes these methods on as arguments to KafkaZookeeper. Does this make sense? I think it would be good if ReplicaManager handled replica management, even if that means it depends on zookeeper.\n", "Attach patch v1. An overview of the patch.\nA. Use synchronized instead of lock for synchronization. The latter has more functionality, but more overhead. See\nhttp://blog.rapleaf.com/dev/2011/06/16/java-performance-synchronized-vs-lock/\n\nB. Partition: Consolidated all reads/writes to leader/ISR in Partition and all accessed are synchronized. This makes sure that leader/ISR values don't change while doing the following operations.\n- makeLeader\n- makerFollower\n- maybeShrinkISR\n- updateLeaderHWAndMaybeExpandISR (for maintaining remote replicas)\n- checkEnoughReplicasReachAnOffset (for checking if a produce request is satisfied)\nThis means that Partition has to access ReplicaManager. In some sense, Partition probably should be a nested class under ReplicaManager since it needs to access several members of ReplicaManager. However, this will make ReplicaManager too big.\n\nC. RepicaManager:\n- Moved most per partition operations to Partition.\n- Cleaned up public methods and added a few helper methods for getting partition and replica.\n- Use ConcurrentHashMap for managing all partitions.\n\nD. KafkaApis: Removed callbacks in the constructer. Instead, call methods in ReplicaManager directly.\n\nE. Replica:\n- Changed to the new getter/setter style for logEndOffset and highWatermark.\n- Local replica doesn't need to set logEndOffset anymore since the logEndOffsetUpdateTime for local replica is never used.\n\nF. BrokerPartitionInfo: Partition is now a complex structure and is supposed to be used only on the server side. Create a simpler PartitionAndLeader class for client usage.\n\nG. KafkaZookeeper: Removed ensurePartitionLeaderOnThisBroker(). The checking of the existence of a leader is now done by replicaManager.leaderReplicaIfLocalOrException(), which is cheaper since it doesn't access ZK.\n\nH. ISRExpirationTest: Remove the test testISRExpirationForMultiplePartitions. It doesn't seem to add additional value since ISR expiration is always done on a per partition basis.\n\nI. SyncProducerTest: Remove the test testProducRequestForUnknowTopic since the logic is always covered by #1 in testProduceCorrectlyReceivesResponse.\n\nJ. TestUtils: Added a helper method leaderLocalOnBroker() that more reliably ensures that a leader exists on a broker.\n\nK. TopicCounTest: removed since it's not doing any useful test.\n\nL. ZookeeperConsumerConnectorTest.testCompressionSetConsumption(): removed the part that tests consumer timeout since it's covered in testBasic already.\n", "I still see some transient failures in unit tests, most of which seem to be due to leader not ready. I will probably wait until kafka-369 is committed since it fixes some of those issues.", "Another change in the patch:\nM. Currently, on leaderAndISR request, the broker gets and creates all assigned replicas. In this patch, the broker only creates replicas in ISR (since they are required in the logic for shrinking ISR). Other remote replicas are created on demand during the handling for follower fetch requests. This will make implementing kafka-42 a bit easier since newly bootstrapped replicas can be added on demand.", "Jun, The patch didn't apply cleanly on a fresh checkout of the 0.8 branch. Do you mind uploading another patch after rebasing ?", "Attach patch v2 after rebase. Made one more change:\nN. Make logEndOffset and highWaterMark in replica atomic long. This is because java doesn't guarantee consistency of long value if not synchronized. ", "Attach patch v3. Just a rebase.", "Thanks for the patch! Overall, this refactoring is a good change. Here are a few review comments -\n\n1. TestUtils. How about renaming leaderLocalOnBroker to isLeaderLocalOnBroker ?\n\n2. LogOffsetTest:\n2.1. The change in testGetOffsetsForUnknownTopic doesn't look right. Since the topic \"foo\" doesn't exist, the client should get back UnknownTopicException. The partition is not invalid, it doesn't even exist.\n\n3. SyncProducerTest\n3.1 Same here, client should get back UnknownTopicException, not InvalidPartitionException\n\n4. ISRExpirationTest\n4.1 getLogWithLogEndOffset expected 6 calls for log.logEndOffset since the test exercised that API 6 times during correct operation. If you change it to anyTimes, it will hide problems with either the test or the code. Was it changed to\nget rid of some transient test failure ?\n4.2 Minor formatting: For consistency, you might want to change to first letter caps for error messages. So far, I don't think everyone quite followed this. So some log statements have first letter caps, others don't. I personally prefer\n first letter caps for all log statements.\n\n5. Replica\n5.1 Is there a reason why logEndOffsetUpdateTimeMs is not AtomicLong ? It's access is not protected by a lock.\n5.2 What is the difference between private[this] var and private var ?\n5.3 It's great that you changed logEndOffset to use the new getter/setter API convention. I think there is only one drawback to using that. I don't know a way to search the code to list all places that use the setter. Do you ?\n\n6. Partition\n6.1 Rename addReplicaIfNotExist to addReplicaIfNotExists.\n6.2 In getOrCreateLog, it is better to use case match, since in Scala case match always evaluates to some value. Since this API needs to return the Replica object, using case match will protect against code bugs. Instead of if-else that checks isDefined, case-match handles Options naturally, since it forces you to handle all the cases. Same for makeLeader, makeFollower, checkEnoughReplicasReachAnOffset since they also return some value.\n6.3 Looks like assignedReplicaMap is meant to be a map of replica_id->Replica. It might be a good idea to change Pool to handle Options. Options are much easier to use than handling null values. For example, getReplica can reduce to just returning assignedReplicaMap.get(replicaId), instead of the if-else checking for nulls.\n6.5 Minor formatting comment same as 4.2\n6.6. maybeIncrementLeaderHW: Since you are trying to access inSyncReplicas here, this method should be synchronized on the leaderAndIsrUpdateLock\n6.7 getOutOfSyncReplicas, updateISR: Same as 6.6\n6.8 checkEnoughReplicasReachAnOffset: \n6.8.1 We should probably rename this to checkEnoughReplicasReachOffset. \n\n7. ReplicaManager\n7.1 I think leaderReplicas was a poorly chosen name by me in the past. It should be renamed to leaderPartitions since it is the set of partitions with their leader hosted on the local broker. Also, that would mean we should rename leaderReplicasLock to leaderPartitionsLock\n7.2 Same as 6.3 for allPartitions. This will greatly simplify getOrCreatePartition\n7.3 Same as 4.2 for some of the APIs\n7.4 Fix typo: shuttedd down \n7.5 Fix identation and parenthesis style for checkpointHighWatermarks. \n7.6 Same as 6.2 for becomeLeaderOrFollower \n7.7 I wonder if it is better to rename leaderReplicaIfLocalOrException to getLeaderReplicaIfLocal ?\n", "Thanks for the review. \n\n2,3. The difficulty is that a broker currently doesn't cache all topic/partitions (only controller does that). It only knows about topic/partitions assigned to itself. So, it's hard for a broker distinguish between a missing topic and a missing partition. We could cache all topic/partitions in all brokers, but we need to add additional ZK logic in each broker. So, in this patch, just combined UnknownTopicException and InvailidPartitionException into a more general UnknowTopicPartitionException. It's not ideal, but probably not too painful for the user to understand.\n\n5.1 That's a good point. Moved the update (which updates logEndOffsetUpdateTimeMs) of logEndOffset into Partition.updateLeaderHWAndMaybeExpandISR(). This way, both the reader and the writer of logEndOffsetUpdateTimeMs is synchronized by leaderISRUpdateLock. So, there is no need to make it an AtomicLong.\n\n6.3 Just not to make this patch too large. I will create a separate jira for changing the Pool api to use Option.\n\n6.6 and 6.7 Both methods are only called in Partition and the caller already synchronizes on leaderISRUpdateLock.\n\nThe rest of the comments have been addressed.", "Upload patch v5 to fix an svn rename issue.", "Thanks for incorporating review suggestions!\n\n2.3 Agreed that it is a bit of work to get meaningful error codes in the client. Often this is ignored, but client contract should be very well thought out and easy to understand. It is best if we give the most descriptive error code, but if we feel it takes significant amount of work, we can start with a simple solution. We went through a pretty detailed review of the new request formats, but not error codes. It will be good to go through this before the release.\n\n5.1 That change is correct. However, in Replica.scala, highWatermarkValue and logEndOffsetValue are synchronized via AtomicLong, but not logEndOffsetUpdateTime. Right now, like you said, there is only one API that updates/reads logEndOffsetUpdateTime and it synchronizes those accesses. But since these are Replica APIs, I'm pretty sure there will be more places in the code that will either update or read the logEndOffset/logEndOffsetUpdateTime and each of those APIs would have to synchronize those accesses. For what it's worth, changing it to AtomicLong actually protects us from future synchronization errors and is not much of a performance hit as well. \n\n8. HighwatermarkPersistenceTest. Fix fail error message to say KafkaException instead of IllegalStateException. I forgot to do this in my patch when I added this test, it will be great if you can include this minor change.\n\n9. Minor comment - Probably better to rename UnknownTopicPartition to UnknownTopicOrPartitionException.\n", "Thanks for the review. Attach patch v6.\n\n5.1 Changed logEndOffsetUpdateTime to AtomicLong.\n\n8,9: Fixed.\n\nOptimized Replica.getOutOfSyncReplicas() a bit to avoid the unnecessary check for leader replica.\n\nRebased.", "+1 \n\nMinor comments before checking it in - \n\n1. Fix comment in UnknownTopicOrPartitionException.scala. Right now it describes InvalidPartitionException\n2. Replica - Fix error message \"shouldn't set logEndOffset for replica %d topic %s partition %d since it's local\" to first letter capital\n", "Thanks for the review. Addressed the issues in the last review and committed to 0.8.\n\nCreate kafka-476 to track using Option in Pool.\n\nFor priviate[this] var, it restricts the usage of a member field to only this instance of the class. This way, one is always forced to use the public api to access the member field in other instances of the class. \n\nYes, Intellij seems to have an issue finding references of x_=(), which is inconvenient. Not sure if it has been addressed in a new version."], "derived": {"summary": "Jay had some good refactoring suggestions as part of the review for KAFKA-46. I'd like to file this umbrella JIRA with individual sub tasks to cover those suggestions.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "Refactor some new components introduced for replication  - Jay had some good refactoring suggestions as part of the review for KAFKA-46. I'd like to file this umbrella JIRA with individual sub tasks to cover those suggestions."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for the review. Addressed the issues in the last review and committed to 0.8.\n\nCreate kafka-476 to track using Option in Pool.\n\nFor priviate[this] var, it restricts the usage of a member field to only this instance of the class. This way, one is always forced to use the public api to access the member field in other instances of the class. \n\nYes, Intellij seems to have an issue finding references of x_=(), which is inconvenient. Not sure if it has been addressed in a new version."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-352", "title": "Throw exception to client if it makes a produce/consume request to a Kafka broker for a topic that hasn't been created", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "labels": [], "created": "2012-05-30T02:04:13.000+0000", "updated": "2012-07-19T19:51:30.000+0000", "description": "It will be good to inform the client if it makes a produce/consume request to a Kafka broker for a topic that hasn't been created. The exception should be something like UnknownTopicException that is descriptive.", "comments": ["1. Improved error handling for produce, fetch, get offsets request to handle unknown topic error code\n\n2. Topic metadata request error handling is fixed as part of KAFKA-350.\n\n3. I understand that we have KAFKA-402 filed to go over all other error handling. This patch only handled unknown topics and unit tested it for all types of requests.", "+1\n\nA few minor things:\n1. Remove the println in KafkaApis.scala\n2. In LogOffsetTest, I think there is no need to define val part = 0 since it is only used once\n3. What does the Thread.sleep(500) in SyncProducerTest.testProduceCorrectlyReceivesResponse wait for?\n\n\n", "Thanks for the review, Jay ! Uploading another patch with the review comments addressed -\n1. Deleted the println\n2. Removed the definition of part variable\n3. Removed the sleep. It seems like one of those unnecessary sleep() calls we have in our unit tests.\n", "+1", "Thanks for the review!"], "derived": {"summary": "It will be good to inform the client if it makes a produce/consume request to a Kafka broker for a topic that hasn't been created. The exception should be something like UnknownTopicException that is descriptive.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Throw exception to client if it makes a produce/consume request to a Kafka broker for a topic that hasn't been created - It will be good to inform the client if it makes a produce/consume request to a Kafka broker for a topic that hasn't been created. The exception should be something like UnknownTopicException that is descriptive."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for the review!"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-353", "title": "tie producer-side ack with high watermark and progress of replicas", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Joel Jacob Koshy", "labels": [], "created": "2012-05-30T20:37:01.000+0000", "updated": "2012-06-29T22:47:44.000+0000", "description": null, "comments": ["We need to support different levels of acks required by the producer. By default (ack=-1), the response to the producer request is only sent when every replica in ISR has received the message. If a producer specifies an ack >=0 (0 will be treated the same as 1), the response is sent after ack replicas have received the message. The caveat is that if the specified ack is less than ISR, the produced message could be lost if some broker fails.", "Patch is not too big, but a bit tricky. To help with review, here is an\noverview:\n\n- DelayedProduce:\n  - Contains the logic to determine if it can be unblocked or not. The\n    partitionStatus map is necessary to keep track of the local log's\n    offset, error status, and whether acks are still pending.  The comments\n    in the code should make the remaining logic clear.\n  - Handling delayed producer requests brings in dependencies on the fetch\n    request purgatory, replica manager and KafkaZookeeper - which is why I\n    had to pass in kafkaApis to the DelayedProduce which is a bit ugly.\n- KafkaApis:\n  - The existing code for handling produce requests would respond when the\n    leader persists the data to disk. We still do that if requiredAcks is 0\n    or 1. For other values, we now create a DelayedProduce request which\n    will be satisfied when requiredAcks followers are caught up.  If the\n    |ISR| < requiredAcks then the request will time out.\n  - handleFetchRequest: if a request comes from a follower replica, check\n    the produceRequestPurgatory and see if DelayedProduce requests can be\n    unblocked.\n- ReplicaManager\n  - only change is to try and unblock DelayedProduce requests to partitions\n    for which the leader changed.\n\nNote that even if a request times out, some of the partitions may have been\nsuccessfully acked - so even if one partition times out the global error\ncode is NoError. The receiver must check the errors array to determine if\nthere are any failures. I think this brings in the need for a\n\"PartialSuccess\" global error code in the ProducerResponse. Thoughts on\nthis?\n\nI think there was a bug in checking satisfied DelayedFetchRequests: the\ncheckSatisfied method would take a produceRequest's TopicData and see if the\ntotal number of bytes in that produce request could satisfy the remaining\nbytes to be filled in the DelayedFetchRequest. However, that could count\ndata for partitions that were not part of the DelayedFetchRequest.  This\npatch fixes that issue as well - changed the FetchRequestPurgatory key from\nTopicData to PartitionData and check on a per-partition basis.\n\nAnother potential issue is that the DelayedFetchRequest satisfied counts\nusing MessageSet's sizeInBytes, which could include incomplete messages - as\nopposed to iterating over the valid messages and getting the size. I left\nthat as is. I think it is debatable which approach is correct in this case.\n\nI added some trace logs at individual request level - e.g., \"Produce request\nto topic unblocked n delayedfetchrequests\". These would be more useful if we\nadd a uuid to each produce request - I think this idea was tossed around on\nthe mailing list sometime before. Doesn't even have to be uuid or part of\nthe produceRequest's wire format - even an atomicLong counter (internal to\nthe broker) may be helpful- thoughts?\n\nThere is this corner case that I think is handled correctly but want to make\nsure:\n- leader receives a producer request and adds it to the\n  ProduceRequestPurgatory.\n- leadership changes while it is pending, so the error code for that\n  partition is set to NotLeaderErrorForPartitionCode\n- leadership changes back to this broker while the DelayedProduce is\n  pending.\n- In this scenario, the partition remains in the error state.\n- I think it is correct because the leader would have become a follower\n  (before it became a leader again), and would have truncated its log to the\n  intermediate leader's HW.\n\nIf requiredAcks == |ISR| and the |ISR| shrinks while the DelayedProduce is\npending, the request may timeout. However, if the |ISR| expands back to its\noriginal size while it is still pending it will get satisfied.\n\nLet me know if you can think of other corner cases that need to be\nconsidered - I wouldn't be surprised if there are quite a few.\n\nI only did limited testing with the ProducerTest.\n\nI think this opens up the following future work (separate jiras):\n- Enhance system test to test all corner cases (leader changing while\n  request pending, ISR shrinking while request pending, etc.\n- ProducerResponse currently uses two separate arrays for errors and\n  offsets; and ProducerRequest uses as array of TopicData each of which\n  contains an array of PartitionData. It may be a good idea to improve these\n  classes to use maps/something else as I had to resort to using find and\n  indexOf to locate partition-level data in the original request.\n- We should add some mbeans for request purgatory stats - avg. hold time,\n  outstanding requests, etc.\n- We should try and get rid of sleeps and fix all intermittent test\n  failures.\n\nIf the above list sounds good I'll file the jiras.\n", "Thanks for patch v1. Looks good overall. Some comments:\n\n1. TopicData.partitionDatas: data is the plural form of datum. So datas feels weird. How about partitionDataArray?\n\n2. KafkaApis:\n2.1 maybeUnblockDelayedRequests: put fetchRequestPurgatory.update in 1 line\n2.2 handleFetchRequest: \nFor the following line:\n    if(fetchRequest.replicaId != -1) {\nInstead of using -1 , could we create a constant like NoneFollowerReplicaId?\n2.3 handleFetchRequest: When creating responses, we should fill in the elapsed time instead of passing in -1. Note that the elapsed time is in nano-secs. So, we probably should rename Response.elapsed to sth like elapsesNs. Ditto for handleProduceRequest.\n\n3. DelayedProduce:\n3.1 isSatisfied(): need to handle the case when requiredAcks is default (probably any value <0). This means whenever we get the ack from every replica in the current ISR, the request is satisfied. This can be done by simply making sure leader.HW >= delayedProduce.partitionStatus(followerFetchPartition).localOffset.\n3.2 Could we change localOffsets to sth like requiredOffsets?\n3.3 respond(): need to compute elapse time in Response \n3.4 We need to be aware that isSatisfied can be called concurrently on the same DelayedProduce object. I am not sure if it's really necessary, but maybe we should consider using AtomicBoolean for PartitionStatus.acksPending?\n\n4. ProducerRequest: actTimeoutSecs should be actTimeoutMs.\n\n5. ReplicaManager: I would vote for not checking ProducerRequestPurgatory when there is a leadership change, since the logic is simpler. We just let the producer requests timeout. If we do want to do the check here, we should share the ProducerRequestPurgatory object used in KafkaApis.\n\n6. SyncProducerConfigShared: We should probably make the default requiredAcks to be -1 (i.e., wait for ack from each replica in ISR).\n\n7. TopicMetadataTest: no need to change\n\n8. 0.8 has moved. So need to rebase\n", "The suggestions for future improvements all make sense. We can create new jiras to track them.", "This doesn't seem to apply cleanly on 0.8, I get conflicts on ReplicaFetcherThread and ReplicaManager.\n\nA few comments:\n1. There is a race condition between adding to the purgatory and acknowledgement, I think. We first produce locally and then add the watcher, which allows a window where the message is available for replication with no watcher on ack. If a replica acknowledges a given produce before the DelayedProduce is added as a watcher that will not be recorded, I think, and the request will potentially timeout waiting for ISR acks. This would certainly happen and lead to sporadic timeouts. This is tricky, let's discuss. One solution would be to reverse the logic--add the watcher first then produce it locally, but this means having to handle the case where the local produce fails (we don't want to leak watchers).\n2. As you say, you definitely can't pass KafkaApis to anything, making DelayedProduceReuqest an inner class of KafkaApis will fix this, I think.\n3. I think the way we are tying the api handling and replica manager is not good. replica manager shouldn't know about DelayedProduce requests if possible, it should only know about replica management and be oblivous to the api level.\n4. Why is the ack timeout in seconds? It should be milliseconds like everything else, right? One would commonly want a sub-second timeout.\n5. As a stylistic thing it would be good to avoid direct use of Tuple2[A,B] and instead use (A,B) (e.g. myMethod: List[(A,B)] instead of myMethod: List[Tuple2[A,B]]\n6. I don't understand why we are passing (topic, partition) to the purgatory as BOTH the key and request. This is a bit hacky since this is not the request. Let's discuss, we may need to clean up the purgatory api to make it gracefully handle this case.\n7. I think the ProducerRequestPurgatory should be instantiated in KafkaApis not KafkaServer--it is an implementation detail of the api layer not a major subsystem.\n8. The doubly nested map/flatMap in KafkaApis.handleProduceRequest is a little bit tricky could you clean that up or if that is not possible comment what it does (I think it just makes a list of (topic, partition) pairs, but the variable name doesn't explain it). Same with KafkaApis.handleFetchRequest.\n9. DelayedProduce should definitely not be in the kafka.api package, that package is for request/response \"structs\". DelayedProduce is basically a bunch of server internals and not something the producer or consumer should be aware of. Likewise, the code for ProduceRequestPurgatory/FetchRequestPurgatory is kept differently, but these things are mirror images of one another. FetchRequestPurgatory is in KafkaApis, ProduceRequestPurgatory in its own file. These should match each other. The reasoning behind keeping things in KafkaApis was that those classes contained part of the logic for processing a request, so splitting it into two files makes it harder to read (the counter argument is that KafkaApis is getting pretty big). The other reason was so it could access the variables in KafkaApis as a nested class. Either way is probably fine, but they should be symetric between the two.\n10. KafkaApis.handleProducerRequest and KafkaApis.produce are a little hard to differentiate. I think the later is effectively \"produce to local log\" and the former does the purgatory stuff. Would be good to call this out in the method name or comment.\n11. I am not so sure about the logic of proactively responding to all outstanding requests with failure during a leadership change. Is this the right thing to do?\n12. The logic in DelayedProduce.isSatisfied is very odd. I think what we want to do is respond to all outstanding requests in the event of a leadership change. But we do this by passing in the ReplicaManager and having logic that keys off whether or not the local broker is the leader. This seems quite convoluted. Wouldn't it make more sense to do this: (1) Have the replica manager allow \"subscribers\" to its state changes. (2) Add a subscriber that loops over all in-flight requests and expires them with an error when the local broker ceases to be the leader. There is a description of this pattern here http://codingjunkie.net/guava-eventbus/.\n13. Nice catch with the request size. I think it is okay that we are using sizeInBytes, since it is not expected that the producer send fragments, though a fully correct implementation would check that.", "The race condition that Jay raised seems like a potential issue. However, it may be tricky to add the watcher before writing to the local log in the leader. This is because the watcher needs to know the end offset of messages in the leader's log. Another way to address the race condition is to still add messages to leader's log first, followed by adding the watcher. However, after the watcher is added, we can explicitly trigger a watcher check by calling ProducerRequestPurgatory.update.", "Also, another thought. This implementation is keeping a Map[(topic, partition)] => # acks. I wonder if it would be better to do this in terms of the acknowledged hw. Basically keep a global structure as part of replica manager that tracks the position of all replicas for all partitions. Then the isSatisfied topic would just check this structure to see if the replicas had advanced far enough. This seems like it would not require a per-request map of state and would be less fragile. Somehow this structure seems generally useful for monitoring and other purposes as well.", "Jun/Jay, thanks a lot for the very helpful reviews. Here are follow-ups to\nsome of your comments - just wanted to make sure I fully address them in v2,\nso let me know if I missed anything. One major thing to decide is what to do\non a leader change during a delayedProduce - should it be handled\nimmediately or let the full request timeout?\n\n> 1. TopicData.partitionDatas: data is the plural form of datum. So datas\n> feels weird. How about partitionDataArray? \n\nThat works - no wonder 'datas' sounded odd. :)\n\n> 3. DelayedProduce: \n> 3.1 isSatisfied(): need to handle the case when requiredAcks is default\n> (probably any value <0). This means whenever we get the ack from every\n> replica in the current ISR, the request is satisfied. This can be done by\n> simply making sure leader.HW >=\n> delayedProduce.partitionStatus(followerFetchPartition).localOffset. \n\nThanks for pointing that out - forgot to handle < 0.\n\n> 3.4 We need to be aware that isSatisfied can be called concurrently on the\n> same DelayedProduce object. I am not sure if it's really necessary, but\n> maybe we should consider using AtomicBoolean for\n> PartitionStatus.acksPending? \n\nI don't think isSatisfied can be called concurrently on the same\nDelayedProduce - the ProduceRequestPurgatory's watcher map is from key ->\nWatchers. The isSatisfied method is synchronized (through\ncollectSatisfiedRequests) on Watchers. I do access partitionStatus for\nrespond(), but that's after the delayedProduce has been kicked out of the\npurgatory. Or am I missing some other scenario? If so, can you elaborate?\n\n> 4. ProducerRequest: actTimeoutSecs should be actTimeoutMs.\n\nForgot to bring this up in the v1 patch - it was simply named ackTimeout and\ntreated as seconds (i.e., the producer request object uses an Int instead of\nLong) - that's why I added the secs suffix and left it as Int.  Will switch\nit to use long and ms instead.\n\n> 5. ReplicaManager: I would vote for not checking ProducerRequestPurgatory\n> when there is a leadership change, since the logic is simpler. We just let\n> the producer requests timeout. If we do want to do the check here, we\n> should share the ProducerRequestPurgatory object used in KafkaApis. \n\nOk - that's also fine, and the code comment indicated it was optional. Not\ndoing the check should help significantly with the code structure. Right now\nthere is a circular dependency which forced me to put DelayedProduce and\nProduceRequestPurgatory outside KafkaApis and pass in KafkaApis to\nDelayedProduce - can probably lay it out differently if we want to do this\ncheck later, but for now this makes things easier.\n\n> 6. SyncProducerConfigShared: We should probably make the default\n> requiredAcks to be -1 (i.e., wait for ack from each replica in ISR). \n\nOk - either way sounds good. The existing default is (almost) the same\nbehavior as 0.7 except that the producer response is sent after the message\nis persisted at the leader.\n\n> 1. There is a race condition between adding to the purgatory and\n> acknowledgement, I think. We first produce locally and then add the\n> watcher, which allows a window where the message is available for\n> replication with no watcher on ack. If a replica acknowledges a given\n> produce before the DelayedProduce is added as a watcher that will not be\n> recorded, I think, and the request will potentially timeout waiting for\n> ISR acks. This would certainly happen and lead to sporadic timeouts. This\n> is tricky, let's discuss. One solution would be to reverse the logic--add\n> the watcher first then produce it locally, but this means having to handle\n> the case where the local produce fails (we don't want to leak watchers). \n\n> The race condition that Jay raised seems like a potential issue. However,\n> it may be tricky to add the watcher before writing to the local log in the\n> leader. This is because the watcher needs to know the end offset of\n> messages in the leader's log. Another way to address the race condition is\n> to still add messages to leader's log first, followed by adding the\n> watcher. However, after the watcher is added, we can explicitly trigger a\n> watcher check by calling ProducerRequestPurgatory.update.\n\nExcellent points - will think about how best to handle this.\n\n> 2. As you say, you definitely can't pass KafkaApis to anything, making\n> DelayedProduceReuqest an inner class of KafkaApis will fix this, I think. \n\nRight - that's actually what I started out with (i.e., the purgatory and\ndelayed request inside kafkaApis), but as I described above, there is a\ncircular dependency and I was forced to move it out. However, the conclusion\nseems to be that we can do away with purgatory update on becoming follower\nso we can break this dependency loop.\n\n> 3. I think the way we are tying the api handling and replica manager is\n> not good. replica manager shouldn't know about DelayedProduce requests if\n> possible, it should only know about replica management and be oblivous to\n> the api level. \n\nCovered above (2).\n\n> 4. Why is the ack timeout in seconds? It should be milliseconds like\n> everything else, right? One would commonly want a sub-second timeout. \n\nYes - it should be ms. (described above)\n\n> 5. As a stylistic thing it would be good to avoid direct use of\n> Tuple2[A,B] and instead use (A,B) (e.g. myMethod: List[(A,B)] instead of\n> myMethod: List[Tuple2[A,B]] \n\nSounds good.\n\n> 6. I don't understand why we are passing (topic, partition) to the\n> purgatory as BOTH the key and request. This is a bit hacky since this is\n> not the request. Let's discuss, we may need to clean up the purgatory api\n> to make it gracefully handle this case. \n\nIt does seem a bit unintuitive on the surface, but I think it is correct:\ni.e., the produce request is to a set of (topic, partition) and potentially\nunblocks DelayedFetch requests to those (topic, partition).\n\n> 7. I think the ProducerRequestPurgatory should be instantiated in\n> KafkaApis not KafkaServer--it is an implementation detail of the api layer\n> not a major subsystem. \n\nCovered above (2).\n\n> 8. The doubly nested map/flatMap in KafkaApis.handleProduceRequest is a\n> little bit tricky could you clean that up or if that is not possible\n> comment what it does (I think it just makes a list of (topic, partition)\n> pairs, but the variable name doesn't explain it). Same with\n> KafkaApis.handleFetchRequest. \n\nThat is correct - will rename/comment.\n\n# 9. DelayedProduce should definitely not be in the kafka.api package, that\n# package is for request/response \"structs\". DelayedProduce is basically a\n# bunch of server internals and not something the producer or consumer\n# should be aware of. Likewise, the code for\n# ProduceRequestPurgatory/FetchRequestPurgatory is kept differently, but\n# these things are mirror images of one another. FetchRequestPurgatory is in\n# KafkaApis, ProduceRequestPurgatory in its own file. These should match\n# each other. The reasoning behind keeping things in KafkaApis was that\n# those classes contained part of the logic for processing a request, so\n# splitting it into two files makes it harder to read (the counter argument\n# is that KafkaApis is getting pretty big). The other reason was so it could\n# access the variables in KafkaApis as a nested class. Either way is\n# probably fine, but they should be symetric between the two. \n\nCovered above (2).\n\n> 11. I am not so sure about the logic of proactively responding to all\n> outstanding requests with failure during a leadership change. Is this the\n> right thing to do? \n\nRelated to (2) above - we can discuss this more. The benefit of doing this\nis that a DelayedProduce can be unblocked sooner (if the timeout is high)\nand has a more accurate error code - i.e., the produce request failed for\nthis partition because its leader changed. However, not doing anything and\nthrowing a timeout is also fine - we just need to decide on one or the other\nand document it for the producer API.\n\n> 12. The logic in DelayedProduce.isSatisfied is very odd. I think what we\n> want to do is respond to all outstanding requests in the event of a\n> leadership change. But we do this by passing in the ReplicaManager and\n> having logic that keys off whether or not the local broker is the leader.\n> This seems quite convoluted. Wouldn't it make more sense to do this: (1)\n> Have the replica manager allow \"subscribers\" to its state changes. (2) Add\n> a subscriber that loops over all in-flight requests and expires them with\n> an error when the local broker ceases to be the leader. There is a\n> description of this pattern here http://codingjunkie.net/guava-eventbus/. \n\nI think there are two concerns you raise here:\n\n(i) Responding to requests in the event of a leadership change: This was\nintended as a safety net - the existing patch deals with the leadership\nchange in replicaManager (which is why replicaManager needs access to the\nproduceRequestPurgatory). However, if we choose not to handle the leader\nchange and just timeout, then we don't need that code in replicaManager and\nwe can remove this logic as well.\n\n(ii) Passing in replicaManager - this is required because I need to get the\ninSyncReplicas for a partition, and the replicaManager maintains that.\nHowever, as described in (2) I think this can be directly accessed when I\nmove this back to KafkaApis.\n\n> 13. Nice catch with the request size. I think it is okay that we are using\n> sizeInBytes, since it is not expected that the producer send fragments,\n> though a fully correct implementation would check that.\n\nThat's a good point - using sizeInBytes is (more) correct then.\n\n> Also, another thought. This implementation is keeping a Map[(topic,\n> partition)] => # acks. I wonder if it would be better to do this in terms\n> of the acknowledged hw. Basically keep a global structure as part of\n> replica manager that tracks the position of all replicas for all\n> partitions. Then the isSatisfied topic would just check this structure to\n> see if the replicas had advanced far enough. This seems like it would not\n> require a per-request map of state and would be less fragile. Somehow this\n> structure seems generally useful for monitoring and other purposes as\n> well.\n\nThis is exactly how it is implemented now - let me know if I misunderstood\nyour comment. The acksPending is a boolean. The actual ack count is\ndetermined from the acknowledged HW that the replicaManager keeps track of.\nThe map is still required to maintain other information such as the error\ncode (e.g., if there was an error in writing to the log), the offset for\neach partition that followers need to fetch beyond for this request to be\nunblocked, and if there are any acks pending (so we don't bother re-counting\nunnecessarily).\n", "Will respond to other points, but the intention of the timeout was that it was milliseconds. It is only 4 bytes because an infinite timeout can be specified by -1 and Int.MaxValue is already a lot of milliseconds.", "3.4 I overlooked that watcher.collectSatisfiedRequests is synchronized. So, this is not an issue.", "Here is v2 - changes from v1:\n\n- Rebased.\n- Addressed all the \"minor\" comments.\n- Using NonFollowerId (alias of DefaultReplicaId) instead of -1\n- For request timing: added the duration computation to the Response\n  class and removed all the default -1.\n- Added handling for requiredAcks < 0\n- To deal with the race condition that Jay raised, did an explicit update\n  after adding to the produceRequestPurgatory.\n- Removed the proactive update to produceRequestPurgatory on a leader\n  change. This facilitated moving the ProduceRequestPurgatory and\n  DelayedProduce back to KafkaApis where they belong.\n- I'm still doing the isLocal check in the isSatisfied method because we get\n  it for free and NotLeaderForPartitionCode is more accurate than giving a\n  timeout error code. We can discuss whether to remove this altogether or\n  not.\n- One thing v1 did not handle was negative timeouts which should be\n  interpreted as forever, so I converted -ve values to Long.MaxValue.  In\n  doing this I hit what I thought was a bug in DelayQueue but turned out to\n  be an overflow issue with DelayedItem - added checks for this.\n\nAlso, Jun asked about a race condition on partitionStatus - although\nisSatisfied cannot be called simultaneously on the same DelayedProduce, I\nthink there is a race condition between expire/checkSatisfied in the\nrequestPurgatory.  I can switch to a ConcurrentHashMap for the\npartitionStatus map, but I think this is an issue in the RequestPurgatory.\nI think it should be easy to fix, but wanted to call it out first to see if\nI'm incorrect:\n- An incoming request triggers checkSatisfied on a DelayedItem\n- During checkSatisfied the DelayedItem expires and the expiration loop\n  calls expire().\n- There is a satisfied guard on the DelayedItem but that is checked only\n  after expire() and after checkSatisfied() so it is possible for both\n  checkSatisfied and expire to be called and it doesn't seem to make sense\n  to allow both - it's one or the other.\n", "The guard is checked before the call to expire. The invariant here is everyone must do a test-and-set on the satisfied variable and only proceed with handling the request if it is satisfied. I don't think there is any problem with checkSatisfied being called on an item that is going to be expired it is only a problem if we try to process the request twice (the call to check satisfied or expire will end up being redundant work but that is not a major problem).", "Thanks for patch v2. A couple of more comments:\n\n21. DelayedProduce.isSatisfied(): The following check seems to be wrong. If  produce.requiredAcks < 0, numAcks is always going to be >=  produce.requiredAcks.\n              if (numAcks >= produce.requiredAcks ||\n                (numAcks >= isr.size && produce.requiredAcks < 0)) {\n\n22. AsyncProducerTest seems to keep failing for me.", "Thanks for catch-21 :) Fixed that - (note that requiredAcks cannot be 0 in isSatisfied.)\n\nFor 22, turned out to be due to inconsistent default values for the producer config. Test passes now.\n\nRe: Jay's comment. It is true that the requests that collectSatisfied returns will not go through expiration (by virtue of the satisfied guard). The issue is that the client needs to be aware that the call to expire and checkSatisfied can be simultaneous and so may need to synchronize on data structures that their implementations may access. So we can either synchronize on the purgatory or clearly document this. Personally, I'm leaning toward fixing it in purgatory because:\n- if checkSatisfied is called (and it will return true), it seems we should consider the deadline to have been met (although this is not clear cut) so expiration should not proceed.\n- it makes the purgatory a little harder to use as the client needs to be aware of the race condition.\n", "Here is an incremental patch over v2 that makes the diff clearer.", "A few more comments on patch v3:\n\n31. DelayedProduce.isSatisfied(): \n31.1 We iterate leader.partition.inSyncReplicas here. However, ISR can be updated concurrently. We probably need to make leader.partition.inSyncReplicas an AtomicReference.\n31.2 Could you add a comment that explains what produce.requiredAcks < 0 means?\n\nFor the race condition between expire() and checkSatisfied, I agree that it's probably easier if RequestPurgatory does the synchronization on DelayedRequest.", "Added that comment on requiredAcks < 0.\n\nThat's a good point about inSyncReplicas. Looks like this is used incorrectly from other places, so I would prefer doing that (and the additional synchronization for expire/checkSatisfied) in a separate jira if that's okay with others.\n", "It's ok to handle the remaining issues in separate jiras. +1 from me on committing patch v3.", "Thanks for the review - committed to 0.8 and opened jiras for the other issues."], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "tie producer-side ack with high watermark and progress of replicas"}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for the review - committed to 0.8 and opened jiras for the other issues."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-354", "title": "Refactor getter and setter API to conform to the new convention", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Joe Stein", "labels": ["optimization"], "created": "2012-05-31T22:39:20.000+0000", "updated": "2017-10-30T12:18:14.000+0000", "description": "We just agreed on a new convention for getter/setter APIs. It will be good to refactor code to conform to that.\n\n> We can actually go with public vals or vars - there is not much point in\n> defining a custom getter/setter as that is redundant.\n>\n> For example:\n> - start with \"val x\"\n> - over time, we determine that it needs to be mutable - change it to \"var\n> x\"\n> - if you need something more custom (e.g., enforce constraints on the\n> values that you can assign) then we can add the custom setter\n>  private[this] var underyling: T = ...\n>  def  x = underlying\n>  def x_=(update: T)  { if (constraint satisfied) {underlying = update}\n> else {throw new Exception} }\n>\n> All of the above changes will be binary compatible since under the covers,\n> reads/assignments are all through getter/setter methods.", "comments": ["Hey Joe, did you get a chance to look into this yet ?", "Closing inactive issue."], "derived": {"summary": "We just agreed on a new convention for getter/setter APIs. It will be good to refactor code to conform to that.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Refactor getter and setter API to conform to the new convention - We just agreed on a new convention for getter/setter APIs. It will be good to refactor code to conform to that."}, {"q": "What updates or decisions were made in the discussion?", "a": "Closing inactive issue."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-355", "title": "Move high watermark maintenance logic out of Log", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "labels": [], "created": "2012-05-31T22:40:39.000+0000", "updated": "2012-07-27T17:57:09.000+0000", "description": "Jay had a good suggestion as part of the review for KAFKA-46 -\n\nMaybe HW mark should move out of Log since now it really indicates something about the state of other servers and Log is meant to be just a simple stand alone log implementation.", "comments": ["Fixed as part of KAFKA-405"], "derived": {"summary": "Jay had a good suggestion as part of the review for KAFKA-46 -\n\nMaybe HW mark should move out of Log since now it really indicates something about the state of other servers and Log is meant to be just a simple stand alone log implementation.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Move high watermark maintenance logic out of Log - Jay had a good suggestion as part of the review for KAFKA-46 -\n\nMaybe HW mark should move out of Log since now it really indicates something about the state of other servers and Log is meant to be just a simple stand alone log implementation."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed as part of KAFKA-405"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-356", "title": "Create a generic Kafka thread class that includes basic boiler plate code of instantiating and shutting down threads cleanly", "status": "Closed", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Yang Ye", "labels": ["optimization"], "created": "2012-06-01T01:27:29.000+0000", "updated": "2012-08-21T01:11:14.000+0000", "description": "We have a lot of threads that basically run in a loop and use an isRunning atomic boolean and count down latch. It will be useful to refactor it out into a helper runnable that these can extend. Verifying the lifecycle details for each is kind of a pain and it pretty easy to either not cleanly shutdown all the threads.", "comments": ["Build a KafkaThread class which wraps shutting down with interruption and exception handling; and let the leaderFinderThread, AbstractFetcherThread and RequestSendThread classes extend it. ", "The patch seems to miss a new class KafkaThread.", "\nAdded the KafkaThread.scala file into the svn", "Thanks for patch v2. Some comments:\n\n1. KafkaThread:\n1.1 Since this thread controls shutdown itself, it should always be a non-daemon thread. So, we just need 1 construct that takes name and logindent. The logindent can probably just to a string, instead of an option.\n1.2 Can we rename fun() to something like doWork()?\n\n2. ConsumerFetcherManager: remove unused imports", "You attached the new patch to a wrong jira (kakfa-365). One more comment:\n\n10. KafkaThread: We always need to specify thread name. So we can remove the constructor with just logIdentStr.", "Also,\n\n11. KafkaThread: In the following catch statement, we should just catch all throwables and log an error if isRunning is true (otherwise, the exception is normal due to shutdown).\n\n   } catch{\n     case e: InterruptedException => warn(\"intterrupted. Shutting down\")\n     case e1 => error(\"Error due to \", e1)\n   }\n\n12. Could you rebase?", "1. Can we make name a required parameter? No one ever thinks to name the threads until we are trying to debug some prod issue and then we can't figure out what is what easily.\n2. What is logIdent do?\n3. Can you add full scaladoc to the KafkaThread class that describes what it is for and how to use it?\n4. What is the logging class that gets used for things this class logs out? It is important that it not log them under KafkaThread but rather under whatever the extending class is, otherwise it will not be informative.\n5. Your log messages need some work. Please make them complete sentences, fix the spelling errors, and make them something that would be understandable in a busy log with lots of messages in it. For example if I see a message \"stopped\" mixed in with many other messages, what should I interpret that to mean? There is a guide to the logging levels in the coding style guide. Can you make sure the resulting logging after this conversion makes sense and everything is logged at the right level?\n6. Can we change the name to BackgroundThread or WorkerThread or something since not all Kafka threads extend this?\n7. Agree with Jun that we need a better name then fun. Maybe \"process()\"?", "\nJay, thanks for the comments.\n1. Yeah, as also commented by Jun\n2. logIdent is a field inherited from the Thread class, which is printed out first, then the log messages you specify. Say if you have logIdent \"A class, \" and it says info(\"blabla\") in your code, the actual log will be \"A class, blabla\", this also answers comment 4, 5\n3. Yeah I can, but it seems that in our whole project, we don't use scaladoc syntaxes other than \"/**, */\": https://wiki.scala-lang.org/display/SW/Syntax, so I won't use fancy syntaxes like bold, italic, link, etc.\n6. I have thought about that, but neither BackgroundThread nor WorkerThread  carry much meaning. So what about \"InterruptableThread\"\n7. I've changed it to doWork()\n", "Can we just give the logger the right class name?", "\nchange as suggested by Jay, and added square bracket to most of the log indentations. ", "\nrebase from the latest svn", "Thanks for patch v5. system_test/single_host_multiple_brokers now hangs. It seems that the unit test can't find the leader broker id. This is likely because some the of log messages have changed and the unit test script is not updated according.\n\n2012-08-17 19:01:41 =======================================\n2012-08-17 19:01:41 Iteration 1 of 3\n2012-08-17 19:01:41 =======================================\n\n2012-08-17 19:01:41 looking up leader\n2012-08-17 19:01:41 found the log line: [2012-08-17 19:01:36,221] INFO [Replica Manager on Broker 1], completed the leader state transition for topic mytest partition 0 (kafka.server.ReplicaManager)\nbin/run-test.sh: line 155: return: [2012-08-17: numeric argument required\n2012-08-17 19:01:41 current leader's broker id : 255\n2012-08-17 19:01:41 stopping server: 255\n2012-08-17 19:01:41 sleeping for 10s\n", "\nFix the log so that the system test passes\n", "Thanks for patch v6. Committed to 0.8 after removing unused imports."], "derived": {"summary": "We have a lot of threads that basically run in a loop and use an isRunning atomic boolean and count down latch. It will be useful to refactor it out into a helper runnable that these can extend.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "Create a generic Kafka thread class that includes basic boiler plate code of instantiating and shutting down threads cleanly - We have a lot of threads that basically run in a loop and use an isRunning atomic boolean and count down latch. It will be useful to refactor it out into a helper runnable that these can extend."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for patch v6. Committed to 0.8 after removing unused imports."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-357", "title": "Refactor zookeeper code in KafkaZookeeper into reusable components ", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": null, "labels": [], "created": "2012-06-01T01:29:31.000+0000", "updated": "2017-08-17T12:18:13.000+0000", "description": "Currently, we stuck a lot of zookeeper code in KafkaZookeeper. This includes leader election, ISR maintenance etc. However, it will be good to wrap up related code in separate components that make logical sense. A good example of this is the ZKQueue data structure. ", "comments": ["Zookeeper related code is getting refactored in KAFKA-5027/KAFKA-5501"], "derived": {"summary": "Currently, we stuck a lot of zookeeper code in KafkaZookeeper. This includes leader election, ISR maintenance etc.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Refactor zookeeper code in KafkaZookeeper into reusable components  - Currently, we stuck a lot of zookeeper code in KafkaZookeeper. This includes leader election, ISR maintenance etc."}, {"q": "What updates or decisions were made in the discussion?", "a": "Zookeeper related code is getting refactored in KAFKA-5027/KAFKA-5501"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-358", "title": "python producer causes \"Unreasonable length = -69523947\"", "status": "Closed", "priority": "Major", "reporter": "Jim Lim", "assignee": null, "labels": [], "created": "2012-06-01T02:51:08.000+0000", "updated": "2012-06-01T16:49:19.000+0000", "description": "- Download KAFKA 0.7\n- Start zookeeper and kafka servers as per quickstart\n\nUse the following commands:\n\n$> python clients/python/kafka.py localhost 2181 test\nEnter comma seperated messages: a,b,c\n\nThe following output is obtained:\nException causing close of session 0x0 due to java.io.IOException: Unreasonable length = -1102880512", "comments": ["Also happens with Ruby client.", "This is my fault.\n\nThe python and ruby clients do not have broker discovery features yet. The port number (and host) should be the actual KAFKA port, not the zookeeper port."], "derived": {"summary": "- Download KAFKA 0. 7\n- Start zookeeper and kafka servers as per quickstart\n\nUse the following commands:\n\n$> python clients/python/kafka.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "python producer causes \"Unreasonable length = -69523947\" - - Download KAFKA 0. 7\n- Start zookeeper and kafka servers as per quickstart\n\nUse the following commands:\n\n$> python clients/python/kafka."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is my fault.\n\nThe python and ruby clients do not have broker discovery features yet. The port number (and host) should be the actual KAFKA port, not the zookeeper port."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-359", "title": "Add message constructor that takes payload as a byte buffer", "status": "Resolved", "priority": "Major", "reporter": "Chris Riccomini", "assignee": null, "labels": [], "created": "2012-06-01T15:31:21.000+0000", "updated": "2017-08-17T12:10:44.000+0000", "description": "Currently, if a ByteBuffer is passed into Message(), it treats the buffer as the message's buffer (including magic byte, meta data, etc) rather than the payload. If you wish to construct a Message and provide just the payload, you have to use a byte array, which results in an extra copy if your payload data is already in a byte buffer.\n\nFor optimization, it would be nice to also provide a constructor like:\n\nthis(payload: ByteBuffer, isPayload: Boolean)\n\nThe existing this(buffer: ByteBuffer) constructor could then just be changed to this(buffer, false).", "comments": ["This has been fixed in newer Kafka versions."], "derived": {"summary": "Currently, if a ByteBuffer is passed into Message(), it treats the buffer as the message's buffer (including magic byte, meta data, etc) rather than the payload. If you wish to construct a Message and provide just the payload, you have to use a byte array, which results in an extra copy if your payload data is already in a byte buffer.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add message constructor that takes payload as a byte buffer - Currently, if a ByteBuffer is passed into Message(), it treats the buffer as the message's buffer (including magic byte, meta data, etc) rather than the payload. If you wish to construct a Message and provide just the payload, you have to use a byte array, which results in an extra copy if your payload data is already in a byte buffer."}, {"q": "What updates or decisions were made in the discussion?", "a": "This has been fixed in newer Kafka versions."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-360", "title": "Add ability to disable rebalancing in ZooKeeper consumer", "status": "Resolved", "priority": "Major", "reporter": "Chris Riccomini", "assignee": null, "labels": [], "created": "2012-06-01T21:13:55.000+0000", "updated": "2016-04-18T09:14:59.000+0000", "description": "There is a need for a ZooKeeper-based Kafka consumer that does not re-balance. This is needed because we may be handling partitioning outside of Kafka. For example, I may have a stateful process that is meant to consume only from Partition 7 of a given Kafka topic. When that process stops, I don't want another consumer to take over partition 7.\n\nThe benefits we get from using the ZooKeeper-based consumer (vs the Simple Consumer) without rebalancing is that offsets will still be handled by Kafka/ZK, as will failover when a partition's leader disappears/fails.\n\nI think the way to do this is to add a consumer config parameter that disables a consumer group's rebalancing. That way, the first consumer in the group to connect (when the ZK node is created) can specify if rebalancing should be enabled for the consumer group. If rebalancing is disabled, the consumers should be forced to supply a list of partition IDs that they wish to read from. Perhaps this can be done during the createMessageStreams call?", "comments": ["It seems that the main request is to control which partition a consumer consumes. I can see 3 approaches. (1) Patch ZookeeperConsumerConnector as proposed in the description of the jira. First, we need to disable rebalance. However, it seems that you want to control the partition assignment youself. So, we will need a way for a consumer to pass in what partitions it wants to consume. (2) If you want a consumer to stick to a partition, you can model each partition as a separate topic and let consumers consume each individual topic. (3) Changing SimpleConsumer so that it's not tied to a particular broker. Instead, it can take requests containing partitions hosted on any broker. Under the cover, it figures out the right brokers to connect to and handles partition leader changes. The consumer still needs to do the offset management itself. There are tradeoffs among those approaches and I am not sure which one is best. For approach (1), it seems that we won't be using much of the functionality of ZookeeperConsumerConnector except for offset management. For approach (2), it seems to be a bit ad hoc. For approach (3), it will make the SimpleConsumer implementation more complicated. We probably need to discuss this a bit more in the jira. "], "derived": {"summary": "There is a need for a ZooKeeper-based Kafka consumer that does not re-balance. This is needed because we may be handling partitioning outside of Kafka.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Add ability to disable rebalancing in ZooKeeper consumer - There is a need for a ZooKeeper-based Kafka consumer that does not re-balance. This is needed because we may be handling partitioning outside of Kafka."}, {"q": "What updates or decisions were made in the discussion?", "a": "It seems that the main request is to control which partition a consumer consumes. I can see 3 approaches. (1) Patch ZookeeperConsumerConnector as proposed in the description of the jira. First, we need to disable rebalance. However, it seems that you want to control the partition assignment youself. So, we will need a way for a consumer to pass in what partitions it wants to consume. (2) If you want a consumer to stick to a partition, you can model each partition as a separate topic and let consumers consume each individual topic. (3) Changing SimpleConsumer so that it's not tied to a particular broker. Instead, it can take requests containing partitions hosted on any broker. Under the cover, it figures out the right brokers to connect to and handles partition leader changes. The consumer still needs to do the offset management itself. There are tradeoffs among those approaches and I am not sure which one is best. For approach (1), it seems that we won't be using much of the functionality of ZookeeperConsumerConnector except for offset management. For approach (2), it seems to be a bit ad hoc. For approach (3), it will make the SimpleConsumer implementation more complicated. We probably need to discuss this a bit more in the jira."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-361", "title": "Add ability to get offsets from ZK-based consumer", "status": "Resolved", "priority": "Major", "reporter": "Chris Riccomini", "assignee": null, "labels": [], "created": "2012-06-01T21:17:01.000+0000", "updated": "2016-04-18T09:15:16.000+0000", "description": "It would be nice to get the partition/offset that a message is associated with when reading from the \"complex\" (zk-based) Kafka consumer. This is useful when we want to tie Kafka stream offsets to state outside of Kafka (e.g. checkpointing consumer state).", "comments": [], "derived": {"summary": "It would be nice to get the partition/offset that a message is associated with when reading from the \"complex\" (zk-based) Kafka consumer. This is useful when we want to tie Kafka stream offsets to state outside of Kafka (e.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add ability to get offsets from ZK-based consumer - It would be nice to get the partition/offset that a message is associated with when reading from the \"complex\" (zk-based) Kafka consumer. This is useful when we want to tie Kafka stream offsets to state outside of Kafka (e."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-362", "title": "ZookeeperConsumerConnector needs to connect to new leader after leadership change", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Jun Rao", "labels": [], "created": "2012-06-04T00:18:30.000+0000", "updated": "2012-07-11T16:17:31.000+0000", "description": "Currently, if the leader of a partition changes, the fetcher in ZookeeperConsumerConnector won't switch to the new leader automatically. ", "comments": ["One question: in the trunk code that I saw (which I think does not have many updates including broker replication), when consumers computes partition assignment they just consider everyone. With broker replication, they will only consider leader's partitions?", "In trunk, a partition only exists in 1 broker. So, once the partition assignment is done, the set of brokers from which a consumer fetches data doesn't change. In 0.8, each partition can have several replicas and data can only be fetched from the leader broker, which can change over time. So, a consumer needs to handle leader change properly in 0.8.", "Attach patch v1. The patch replaces Fetcher and FetcherRunnable with ConsumerFetcherManager and ConsumerFetcherThread. ConsumerFetcherManager maintains a list of partitions that have lost the leader. It keeps trying to get the new leader of the partition and add the partition to the corresponding fetcher thread. ", "Attache patch v2 with minor changes to make sure that no more fetcher threads are started after stopAllConnections() is called in  ConsumerFetcherManager.", "1 - ConsumerConfig:\n    a - Instead of NoneReplicaFetcherId, use FetchRequest.NonFollowerId (or\n      make it an alias of that)\n    b - So fetch.wait.time.ms and fetch.min.byte deprecate the existing\n      min.fetch.bytes and max.fetch.wait.ms configs? - ConsoleConsumer needs\n      to be updated (as it still uses the old ones). Actually, why rename\n      the existing configs?\n\n2 - PartitionTopicInfo: Seems that enqueue does not need to return anything\n    now. Actually, it's unclear: the comment above processPartitionData in\n    AbstractFetcherManager says that it returns the new fetch offset but the\n    implementation does not return anything.\n\n3 - AbstractFetcherThread: Line 77: is the check on isRunning necessary?\n\n4 - AbstractFetcherManager: rename numReplicaFetchers to numFetchers.\n\n5 - ConsumerFetcherManager: just a thought (I have not looked carefully\n  enough for feasibility): for noLeaderPartitionSet - would it be possible\n  to use a SynchronousQueue[Seq[(String, Int)] instead? That way, you may be\n  able to get rid of the refreshLeaderBackOff, and the lock and its\n  associated condition (although you would need to use an atomic reference\n  for the partitionMap).\n\n- completes -> completed\n  core/src/main/scala/kafka/consumer/ConsumerFetcherManager.scala: info(\"shutdown completes\")\n  core/src/main/scala/kafka/server/ReplicaFetcherManager.scala: info(\"shutdown completes\")\n", "Thanks for the review. Attach patch v3.\n1a. fixed\n1b. Didn't notice those configs already exist. Fixed.\n\n2. Changed to not return a value in enqueue. Changed comments above processPartitionData.\n\n3. It's needed. If we shut down the fetcher thread, we could get an Interrupted exception. In this case, we don't need to add the partitions to the list.\n\n4. fixed.\n\n5. It's possible to use a BlockingQueue. However, we probably still need to use a lock to synchronize the update of partitionMap and noLeaderPartitionSet (or queue). For example, if partitionMap is null, we don't want to update noLeaderPartitionSet any more. This is a bit hard to guarantee just with an atomic value since the value could change immediately after you do the check. So keeping the current implementation for now.\n\n", "+1 on v3", "Thanks for the view. Just committed to 0.8."], "derived": {"summary": "Currently, if the leader of a partition changes, the fetcher in ZookeeperConsumerConnector won't switch to the new leader automatically.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "ZookeeperConsumerConnector needs to connect to new leader after leadership change - Currently, if the leader of a partition changes, the fetcher in ZookeeperConsumerConnector won't switch to the new leader automatically."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for the view. Just committed to 0.8."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-363", "title": "Replace numerical compression codes in config with something human readable", "status": "Closed", "priority": "Minor", "reporter": "Jay Kreps", "assignee": "Jay Kreps", "labels": ["newbie"], "created": "2012-06-08T19:13:08.000+0000", "updated": "2012-10-03T19:51:46.000+0000", "description": "Currently we have compression codes like 1 or 2, which is pretty unintuitive. What does 1 mean?\n\nWe should replace these with human-readable codes like \"snappy\", \"gzip\", or \"none\" and change the documentation. We can continue to support the existing integer codes in addition for backwards compatibility with existing configurations.", "comments": ["Here's a go at a simple fix for this. I added a \"name\" attribute to the CompressionCodec case objects and added getCompressionCodec(String). ", "Thanks for the patch, David.\n\n1. Can we make the name match case insensitive? i.e., just name.toLowerCase.match\n2. Any particular reason to change the config name to compression.name? I think compression.codec looks better.\n3. Can you make it backward compatible? i.e., allow both int and string in ProducerConfig. (Also, there are other usages of compression.codec - e.g., in system tests and contrib/hadoop* that still use the int).\n", "Joel, thanks for the review.\n\nI've changed the config property back to \"compression.codec\". It accepts the short names (none, gzip, snappy) as well as the old int values. I also made the name match case insensitive like you suggested. New patch is attached", "Thanks for making that change. ProducerConfig: There's a compilation error due to getString default being given an int. Also, the second call to getCompressionCodec should have the default as well (or some unit tests will fail). Looks good otherwise.", "Compilation error - embarrassing. Maybe I should be more careful since I'm still pretty new to Scala.\n\nFixed those two issues. `sbt compile test` is happy", "+1\n(I don't think we need to apply this to trunk.) Committed this to 0.8 with a very minor edit.\n"], "derived": {"summary": "Currently we have compression codes like 1 or 2, which is pretty unintuitive. What does 1 mean?\n\nWe should replace these with human-readable codes like \"snappy\", \"gzip\", or \"none\" and change the documentation.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Replace numerical compression codes in config with something human readable - Currently we have compression codes like 1 or 2, which is pretty unintuitive. What does 1 mean?\n\nWe should replace these with human-readable codes like \"snappy\", \"gzip\", or \"none\" and change the documentation."}, {"q": "What updates or decisions were made in the discussion?", "a": "+1\n(I don't think we need to apply this to trunk.) Committed this to 0.8 with a very minor edit."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-364", "title": "Consumer re-design", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "labels": [], "created": "2012-06-11T23:49:30.000+0000", "updated": "2016-04-18T09:14:40.000+0000", "description": "We've received quite a lot of feedback on the consumer side features over the past few months. Some of them are improvements to the current consumer design and some are simply new feature/API requests. I have attempted to write up the requirements that I've heard on this wiki -\n\nhttps://cwiki.apache.org/confluence/display/KAFKA/Consumer+Client+Re-Design\n\nThis would involve some significant changes to the consumer APIs, so we would like to collect feedback on the proposal from our community. Since the list of changes is not small, we would like to understand if some features are preferred over others, and more importantly, if some features are not required at all. ", "comments": ["I would like to throw in a couple use cases:\n\n\n  - Allow the new consumer to reset its offset to either the current\n  largest or smallest.  This would be a great way to restart a process that\n  has fallen behind.  The only way I know how to do this today, with the\n  high-level consumer, is to delete the ZK nodes manually and restart the\n  consumer.\n  - Allow the consumer to reset its offset to some arbitrary value, and\n  then write that offset into ZK.    Kind of like the first case, but would\n  make rewinding/replays much easier.\n\nModularity (the ability to layer the ZK infrastructure on top of the simple\ninterface) would be great.\n\nthanks,\nEvan", "Throwing a +1 on \"Allow the consumer to reset its offset to some arbitrary value, and then write that offset into ZK\".\n\nWe're currently running into a scenario where we would like to have 100% reliability, and we're losing a few messages when a connection is broken, but there were still a few messages in the OS TCP buffers. So, we're planning on shifting the ZK offset by a few seconds \"back in time\" if we detect a broker has gone down, to make sure all the messages will be actually delivered to the end consumer when that broker comes back up, even if there's a small amount of overlapping messages.\n\nThanks,\n\nMarcos", "In the API redesign, it would be nice to somehow allow for flexible/pluggable control of the allocation of [broker:partition] from producers and to consumers when using zookeeper management.\n(I was not certain whether \"Manual partition assignment\" covered this - it did not mention producer partition control)\n\nI currently use SyncProducer and SimpleConsumer to directly control the set of [broker:partition] that a producer writes to and that a consumer reads from.\n\nI need this for a scenario where the consumer holds some state (like a cache) on local disk.  It is expensive to discard the local state - the consumer must then instead perform a remote lookup with a very high latency. (> 5mins).  I need the partitioning performed by a producer to remain fixed until explicitly changed (the number of producers is relatively static, and each producer sends messages into a dedicated broker).  I need each consumer to fetch the same partitions unless a consumer has failed for more than some period of time (approx 5 mins) so that if it recovers quickly I have not wastefully discarded local state.\n\nCurrently if I use Producer with zookeeper, the Partitioner API allows me to partition messages, but then kafka code in the Producer controls allocation of the Partitioner result to a physical [broker:partition].  If I use Producer with fixed brokers, messages are allocated to random partitions.  If I use the high level consumer, kafka code in ZookeeperConsumerConnector controls the allocation of [broker:partition] to available consumers.\n\nI understand if this is an over-specialised use-case to cater for.  At minimum I would like the equivalent functionality of SyncProducer and SimpleConsumer to be preserved in a public API.\n\nThanks,\nRoss\n\n", "I send batched messages with compression, and use the offsets retrieved by the consumer to get exactly-once semantics (by persisting consumer state with the offsets).  When using the message set iterator, for a e.g. batch of 5 messages the offset returned for messages 1-4 is the start of the *current* batch, and the offset for message 5 is the start of the *next* batch.  My code has to wait for the offset to change from the previous message before it persists (so that my consumer state is only persisted when a batch has been completed).  To me, this feels awkward in that it is not very explicit in the API (you have to know about internals to understand the processing required).  I think it could be useful to expose a flag that indicated batch-end, or to directly expose message batches (similar to the way shallowIterator does?).\n\nThanks,\nRoss\n"], "derived": {"summary": "We've received quite a lot of feedback on the consumer side features over the past few months. Some of them are improvements to the current consumer design and some are simply new feature/API requests.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Consumer re-design - We've received quite a lot of feedback on the consumer side features over the past few months. Some of them are improvements to the current consumer design and some are simply new feature/API requests."}, {"q": "What updates or decisions were made in the discussion?", "a": "I send batched messages with compression, and use the offsets retrieved by the consumer to get exactly-once semantics (by persisting consumer state with the offsets).  When using the message set iterator, for a e.g. batch of 5 messages the offset returned for messages 1-4 is the start of the *current* batch, and the offset for message 5 is the start of the *next* batch.  My code has to wait for the offset to change from the previous message before it persists (so that my consumer state is only persisted when a batch has been completed).  To me, this feels awkward in that it is not very explicit in the API (you have to know about internals to understand the processing required).  I think it could be useful to expose a flag that indicated batch-end, or to directly expose message batches (similar to the way shallowIterator does?).\n\nThanks,\nRoss"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-365", "title": "change copyright in NOTICE to 2012", "status": "Resolved", "priority": "Major", "reporter": "Joe Stein", "assignee": "Joe Stein", "labels": [], "created": "2012-06-14T16:57:48.000+0000", "updated": "2012-08-15T14:46:08.000+0000", "description": "need to change the copyright in NOTICE file to 2012\n", "comments": ["patch for the 2012 copyright to trunk, 0.7.1 and 0.8 branches", "+1 on the patch.", "committed to trunk and 0.7.1 and 0.8 branches", "changed according to the comments", "The patch is for kafka-356, not kafka-365."], "derived": {"summary": "need to change the copyright in NOTICE file to 2012.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "change copyright in NOTICE to 2012 - need to change the copyright in NOTICE file to 2012."}, {"q": "What updates or decisions were made in the discussion?", "a": "The patch is for kafka-356, not kafka-365."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-366", "title": "add jmx beans in broker to track # bytes in consumer", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Jun Rao", "labels": [], "created": "2012-06-18T20:47:40.000+0000", "updated": "2012-06-19T13:06:13.000+0000", "description": null, "comments": ["Patch for trunk attached. Will port to 0.8 branch after committing to trunk.", "+1", "Thanks for the review. Double committed to trunk and 0.8.", "also committed to 0.7.1 branch"], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "add jmx beans in broker to track # bytes in consumer"}, {"q": "What updates or decisions were made in the discussion?", "a": "also committed to 0.7.1 branch"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-367", "title": "StringEncoder/StringDecoder use platform default character set", "status": "Closed", "priority": "Major", "reporter": "Jay Kreps", "assignee": "Eli Reisman", "labels": ["newbie"], "created": "2012-06-19T05:13:19.000+0000", "updated": "2014-06-19T05:16:56.000+0000", "description": "StringEncoder and StringDecoder take the platform default character set. This is bad since the messages they produce are sent off that machine. We should\n-- add a new required argument to these that adds the character set and default to UTF-8 rather than the machine setting\n-- add a commandline parameter for the console-* tools to let you specify the correct encoding.", "comments": ["First attempt here. Defaulting StringEncoder/Decoder to \"UTF-8\" is done, should be able to get user-defined alternate charsets from CLI and use from Consumer side, not so sure about Producer side.\n\nLet me know if there's stuff to fix here, thanks again!\n", "Producer side is messed up, working on patch #2 here...\n", "Another attempt. Also not working quite yet. I'm new to Scala and not quite sure which syntax trick allows you to call a default constructor with and without parens from client code, and to have an override with a string in the constructor without upsetting Scala or the test code. This is happening in my modifications to StringEncoder. I would ideally like to make the character encoding setting a \"val\" as well. But all this will be solved soon I hope...\n\nFeel free to take a look and advise a wayward Java refugee...", "This looks good. The only issue is that the encoder and decoder are pluggable (you just give the class name). This makes hard coding a property as a variable in the config object for a particular serializer a little awkward. I think this is actually a general need for any kind of serializer not just strings, the serializer might need access to various URLs, or to a schema string or all kinds of other things. Since we want people to be able to make their own serializers without modifying any kafka code, this is not very good.\n\nCurrently we require a no-arg constructor for the serializer. What if, instead, we required a constructor that took a single argument, the java.util.Properties instance? This way you could write a custom serializer and pass any properties you liked to it without needing to modify any kafka code. Perhaps ideally we could look for either the Properties only constructor or the no-arg constructor preferring the properties one, allowing backwards compatibility with any encoders people have already. ", "Love it. I'll get right on this, thanks for the advice!\n", "Is this more like it? This seems right and the decoder related tests seem to be passing, but when I run the suite in SBT on my local machine I get several errors in tests regarding ZooKeeper. I am unaware whether these are due to local test config or genuine trouble. They seem to culminate at the Kafka end in Utils.getObject() at line 675.\n", "Is this more like it? I am still getting an error on several tests on SBT but they are ZooKeeper tests and I'm not certain if they relate directly to this patch or my local setup is not right for running the tests? They culminate on the Kafka side of the code in 2 ZK tests at Utils.getObject() line 675. Does this ring any bells?\n\nThanks!\n\n", "This looks good, but I don't see where we make use of the new constructor. I think we need to pass in the properties in the producer and consumer, right?", "Sorry, that makes sense. Wasn't sure if that was \"to be added later when someone needs it\" or if that should be wired up now. Will do. Kind of felt like the constructor should have been in the trait rather than the classes too but the compiler didn't agree. Might try another swipe at getting that to happen too. ", "Yeah I think the contract now becomes \"your class needs to implement the interface and provide a constructor that takes Properties, so it does make sense to have this in the class rather than the trait since we won't know what to do with the properties except in the class.\n\nI think we do need to use the new constructor to make this bug fixed since otherwise we are still going to pick up the default char set.", "Thanks, sounds good. Will do.\n", "I still need to add console opts for encoding and write these new constructors into the producer and consumer stuff, but the way I have the patch now I believe the properties calls have hardcoded defaults to \"UTF-8\" so acutally I do think even the no-arg constructors would be safe with this in the case where no character encoding option is chosen. Am I wrong about that? \n\nAnyway there's more to do here regardless, will get to it and post something ASAP\n", "This seems to be working/passing all the tests. If this is looking like what you had in mind, I can add a command line option to set the encoding manually and maybe we're good to go.\n\n", "This includes the Encoder/Decoder improvement as in the 367-4 patch, and the Consumer and Producer console options to set character encoding manually and place it into the relevant Properties objects for consumption inside Kafka. Passes SBT tests etc. should be good to go?", "Eli, I was also working on the same area for KAFKA-544 so I attempted to integrate this patch into that one. I have committed that to the 0.8 branch. If you get a chance take a look at the current 0.8 branch HEAD and let me know if I got it right."], "derived": {"summary": "StringEncoder and StringDecoder take the platform default character set. This is bad since the messages they produce are sent off that machine.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "StringEncoder/StringDecoder use platform default character set - StringEncoder and StringDecoder take the platform default character set. This is bad since the messages they produce are sent off that machine."}, {"q": "What updates or decisions were made in the discussion?", "a": "Eli, I was also working on the same area for KAFKA-544 so I attempted to integrate this patch into that one. I have committed that to the 0.8 branch. If you get a chance take a look at the current 0.8 branch HEAD and let me know if I got it right."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-368", "title": "use the pig core jar from maven instead of distributing it", "status": "Resolved", "priority": "Major", "reporter": "Joe Stein", "assignee": "Joe Stein", "labels": [], "created": "2012-06-19T17:27:00.000+0000", "updated": "2012-06-20T15:05:24.000+0000", "description": null, "comments": ["+1\nJust make sure that the pig-0.8.0 jar files are removed from lib.", "+1 with the same suggestion as Jun", "committed to trunk and the 0.8 and 0.7.1 branches"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "use the pig core jar from maven instead of distributing it"}, {"q": "What updates or decisions were made in the discussion?", "a": "committed to trunk and the 0.8 and 0.7.1 branches"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-369", "title": "remove ZK dependency on producer", "status": "Resolved", "priority": "Major", "reporter": "Jun Rao", "assignee": "Yang Ye", "labels": [], "created": "2012-06-20T01:48:56.000+0000", "updated": "2015-10-21T20:39:19.000+0000", "description": "Currently, the only place that ZK is actually used is in BrokerPartitionInfo. We use ZK to get a list of brokers for making TopicMetadataRequest requests. Instead, we can provide a list of brokers in the producer config directly. That way, the producer client is no longer dependant on ZK.", "comments": ["\n1. Added a function getBrokerListStrFromConfigs() in class TestUtils, which creates the broker list string from a sequence of KafkaConfig's\n\n2. remove every part of zk.connect in the producer side\n\n3. Added three wait-until-topic-is-registered-in-zookeeper block in ProducerTest to eliminate potential transient failure.", "Thanks for patch v1. Some comments:\n\n1. ProducerConfig:\n1.1 change broker.list's format to host:port; Also, change the comment and explain this is just for bootstrapping and it just needs to be a subset of all brokers in the cluster.\n1.2 We should make broker.list a required property by not providing a default value. If we do the following, an exception will be thrown if the property is not specified. There is no need to test for null any more.\n     Utils.getString(props, \"broker.list\")\n1.3 There is no need for ProducerConfig to extend ZKConfig.\n\n2. Producer: There is no need to check the existence of brokerList since it will be handled in ProducerConfig.\n\n3. BrokerPartitionInfo:\n3.1 It's better to rename it to something like MetaDataCache\n3.2 This class shouldn't depend on ProducerPool, which is supposed to be used for caching SyncProducer objects for sending produce request. Instead, this class only needs to know broker.list. The reason is that broker.list is just for bootstrapping and we can pass in a VIP instead of a real host. Each time we need to send a getMetaData request, we can just pick a host from broker.list, instantiate a SyncProducer, send the request, get the response, and close the SyncProducer. Since getMetaData is going to be used rarely, there is no need to cache the connection. It also avoids the timeout problem with VIP. For new brokers that we get in updateInfo(), we can instantiate a new SyncProducer and add it to ProducerPool.\n\n4. ProducerPool\n4.1 If we do the above in BrokerPartitionInfo, we can get rid of the following methods.\n  def addProducers(config: ProducerConfig) \n  def getAnyProducer: SyncProducer \n\n5. KafkaConfig: The default value of hostName should be InetAddress.getLocalHost.getHostAddress. We can then get rid of the hostName null check in KafkaZookeeper.registerBrokerInZk().\n\n6. KafkaKig4jAppender: get rid of the commented out lines related to ZkConnect; get rid of host and port\n\n7. KafkaKig4jAppenderTest: testZkConnectLog4jAppends should be renamed since there is no ZK connect any more.\n\n8. ProducerTest: \n8.1 all method of testZK* should be renamed properly.\n8.2  Quite a few places have the following checks. Can't we just combine them using a check for leaderIsLive?\n    assertTrue(\"Topic new-topic not created after timeout\", TestUtils.waitUntilTrue(() =>\n      AdminUtils.getTopicMetaDataFromZK(List(\"new-topic\"),\n        zkClient).head.errorCode != ErrorMapping.UnknownTopicCode, zookeeper.tickTime))\n    TestUtils.waitUntilLeaderIsElectedOrChanged(zkClient, \"new-topic\", 0, 500)\n\n9. Utils.getAllBrokersFromBrokerList(): There is no need to get the broker id from broker.list. We can just assign some sequential ids.\n", "\nIn AsyncProducerTest.testFailedSendRetryLogic(), we may see following error messges, it won't affect the success of the test. It's because in handle() function of DefaultEventHandler,\n\n        if (outstandingProduceRequests.size > 0)  {\n          // back off and update the topic metadata cache before attempting another send operation\n          Thread.sleep(config.producerRetryBackoffMs)\n          // get topics of the outstanding produce requests and refresh metadata for those          Utils.swallowError(brokerPartitionInfo.updateInfo(outstandingProduceRequests.map(_.getTopic)))\n\nit will refresh the cached metadata, but the broker is not up.\n\nThis doesn't affect the correctness of the test. Maybe we need try to eliminate the error messages.  \n\n\n\n\n\n[2012-08-03 19:11:44,182] ERROR Connection attempt to 127.0.0.1:52955 failed, next attempt in 100 ms (kafka.producer.SyncProducer:99)\njava.net.ConnectException: Connection refused\n\tat sun.nio.ch.Net.connect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:500)\n\tat kafka.network.BlockingChannel.connect(BlockingChannel.scala:57)\n\tat kafka.producer.SyncProducer.connect(SyncProducer.scala:161)\n\tat kafka.producer.SyncProducer.getOrMakeConnection(SyncProducer.scala:182)\n\tat kafka.producer.SyncProducer.doSend(SyncProducer.scala:74)\n\tat kafka.producer.SyncProducer.send(SyncProducer.scala:116)\n\tat kafka.producer.BrokerPartitionInfo$$anonfun$updateInfo$1.apply(BrokerPartitionInfo.scala:86)\n\tat kafka.producer.BrokerPartitionInfo$$anonfun$updateInfo$1.apply(BrokerPartitionInfo.scala:81)\n\tat scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)\n\tat scala.collection.immutable.List.foreach(List.scala:45)\n\tat scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:44)\n\tat scala.collection.mutable.ListBuffer.foreach(ListBuffer.scala:42)\n\tat kafka.producer.BrokerPartitionInfo.updateInfo(BrokerPartitionInfo.scala:81)\n\tat kafka.producer.async.DefaultEventHandler$$anonfun$handle$1.apply$mcV$sp(DefaultEventHandler.scala:53)\n\tat kafka.utils.Utils$.swallow(Utils.scala:429)\n\tat kafka.utils.Logging$class.swallowError(Logging.scala:102)\n\tat kafka.utils.Utils$.swallowError(Utils.scala:40)\n\tat kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:53)\n\tat kafka.producer.AsyncProducerTest.testFailedSendRetryLogic(AsyncProducerTest.scala:438)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat junit.framework.TestCase.runTest(TestCase.java:164)\n\tat junit.framework.TestCase.runBare(TestCase.java:130)\n\tat junit.framework.TestResult$1.protect(TestResult.java:110)\n\tat junit.framework.TestResult.runProtected(TestResult.java:128)\n\tat junit.framework.TestResult.run(TestResult.java:113)\n\tat junit.framework.TestCase.run(TestCase.java:120)\n\tat junit.framework.TestSuite.runTest(TestSuite.java:228)\n\tat junit.framework.TestSuite.run(TestSuite.java:223)\n\tat org.junit.internal.runners.OldTestClassRunner.run(OldTestClassRunner.java:35)\n\tat org.junit.runner.JUnitCore.run(JUnitCore.java:121)\n\tat com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:71)\n\tat com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:199)\n\tat com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:62)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat com.intellij.rt.execution.application.AppMain.main(AppMain.java:120)", "Thanks for patch v2. Some more comments:\n\n20. ProducerConfig:\n20.1 change the comment and explain this is just for bootstrapping and it just needs to be a subset of all brokers in the cluster. \n20.2 use Utils.getString(props, \"broker.list\"). It will make sure the property exists.\n\n21. Producer:  get rid of the following lines since ProducerConfig makes sure that brokerlist exists.\n  if(!Utils.propertyExists(config.brokerList))\n    throw new InvalidConfigException(\"broker.list property must be specified in the producer\")\n\n22. BrokerPartitionInfo:\n22.1 Why is the following line there in the constructor?\n  error(\"The broker list is : \" + brokerList)\n22.2 updateInfo:\n22.2.1 we should close the producer even when there are exceptions\n22.2..2 if we get any socket level exception when getting metadata, we should retry getting the metadata using the next broker in the list.\n22.2.3 should call producerPool.updateProducer after the error checking\n\n23. ProducerPool:\n23.1 remove unused import\n23.2 updateProducer: need to sync on the lock.\n23.3 updateProducer: newBroker.+= get rid of the dot\n\n24. KafkaZookeeper:\n24.1 registerBrokerInZk: The following line should be val hostName = config.hostName.\n    val hostName = if (config.hostName == null) InetAddress.getLocalHost.getHostAddress else config.hostName\n\n25. ProducerConfig,Utils: remove unused import\n\n26. DefaultEventHandler: \n26.1 constructor: get rid of the comment \"this api is for testing\"\n26.2 send(): get rid if the following line:\n        error(\"pool broker id: \" + brokerId)\n", "Basically fix all the issues raised in the comments. I didn't use the \"warmUp\" function because it gets back \"LeaderNotAvaiable\" metadata info from the server. We may add a new jira to look into this problem and adding the batching warm up", "\nChange the error handling of \"broker.list\" config and refactoring the updateInfo() function", "\nI am thinking about the exception handling in updateInfo() function. The topic level error code maybe NoError while there may still be partition-level error code. In this case, it's not appropriate for us to put the (topic, topicMetadata) into the cache. Of course we can put in the \"tailed\" topicMetatData, which only contains partitionMetadata whose error code is NoError, but this is kind of hacky.\n\nAlso it's not bad to just throw exception to the client, because this function is called on demand --- when the client wants to send messages to a topic. If some topic is erroneous, the send need to be failed. ", "\nHandling the error code in the topicMetadataResponse", "Thanks for patch v5. A few more comments:\n\n50. There are a bunch of places where zk.connect is still used in ProducerConfig, could you fix them?\nProducerPerformance, ReplayLogProducer, ConsoleProducer, TestEndToEndLatency, AsyncProducerTest, KafkaOutputFormat.java, Producer.java\n\n51. Single host multi broker system test seems to hang:\n2012-08-08 09:08:22 =======================================\n2012-08-08 09:08:22 Iteration 1 of 3\n2012-08-08 09:08:22 =======================================\n\n2012-08-08 09:08:22 looking up leader\n2012-08-08 09:08:22 found the log line: [2012-08-08 09:08:17,434] INFO Controller 1, at initializing leaders for new partitions, the leaderAndISR request sent to broker 3 is LeaderAndISRRequest(1,,false,1000,Map((mytest,0) -> { \"ISR\": \"1,2,3\",\"leader\": \"1\",\"leaderEpoch\": \"0\" })) (kafka.server.KafkaController)\nbin/run-test.sh: line 151: return: [2012-08-08: numeric argument required\n2012-08-08 09:08:22 current leader's broker id : 255\n2012-08-08 09:08:22 stopping server: 255\n2012-08-08 09:08:22 sleeping for 10s\n\n52. remove unused imports\n\n53. ProducerPool.updateProducer: use a HashSet instead of ListBuffer for storing newBrokers.\n\n54. The following unit test seems to fail occasionally for me. Is that related to this patch?\n[error] Test Failed: testMultiProduceResend(kafka.integration.LazyInitProducerTest)\nkafka.common.KafkaException: fetching broker partition metadata for topics [List(test1)] from broker [ArrayBuffer(id:0,creatorId:192.168.1.111-1344443455425,host:192.168.1.111,port:61693)] failed\n\tat kafka.producer.BrokerPartitionInfo.updateInfo(BrokerPartitionInfo.scala:109)\n\tat kafka.producer.BrokerPartitionInfo.getBrokerPartitionInfo(BrokerPartitionInfo.scala:49)\n\tat kafka.producer.async.DefaultEventHandler.kafka$producer$async$DefaultEventHandler$$getPartitionListForTopic(DefaultEventHandler.scala:140)\n\tat kafka.producer.async.DefaultEventHandler$$anonfun$partitionAndCollate$1.apply(DefaultEventHandler.scala:101)\n\tat kafka.producer.async.DefaultEventHandler$$anonfun$partitionAndCollate$1.apply(DefaultEventHandler.scala:100)\n\tat scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)\n\tat scala.collection.immutable.List.foreach(List.scala:45)\n\tat kafka.producer.async.DefaultEventHandler.partitionAndCollate(DefaultEventHandler.scala:100)\n\tat kafka.producer.async.DefaultEventHandler.dispatchSerializedData(DefaultEventHandler.scala:65)\n\tat kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:48)\n\tat kafka.producer.Producer.send(Producer.scala:65)\n\tat kafka.integration.LazyInitProducerTest.testMultiProduceResend(LazyInitProducerTest.scala:163)\n\n55. Could you change the comment for broker.list in ProducerConfig to the following?\n\n  /** This is for bootstrapping and the producer will only use it for getting metadata (topics, partitions and replicas).\n   *  The socket connections for sending the actual data will be established based on the broker information\n   *  returned in the metadata. The format is host1:por1,host2:port2, and the list can be a subset of brokers or\n   *  a VIP pointing to a subset of brokers.\n   */\n", "\nBasically act according to the last review comments. The system test seems to hang, I'll check it, maybe file another Jira", "Didn't check this in last time due to Jira failure", "Didn't check this in last time due to Jira failure", "\nMajor changes from v8:\n\nChange the system test by specifying the broker list of all three brokers. ", "ran system test passes after v9 patch for quite a few times, all pass", "Thanks for the patch. Committed v9 to 0.8.", "I'm completely late on this and don't have the full context, so my apologies if this is a naive question, but why would you want to have addresses of brokers in producer's configs and not have producer get that from Zookeeper?  I thought using ZK for cluster-wide config sharing was a good thing, but there must be some reason why you chose not to do that here.  Can anyone shed some light on this.... 14 months after the commit? :)\n", "I guess all distributed systems have a problem of bootstrapping knowledge about the cluster.\n\nIn 0.7 the client had a list of hard-coded zk brokers which you connect to, and from this you can discover the state of the kafka cluster.\n\nThere are a number of problems with embedding zookeeper in clients at large scale:\n1. Makes language support hard\n2. GC in clients causes havoc\n3. Massive zookeeper load\n\nInstead we use the brokers for cluster discovery. As before you need a couple hardcoded urls to bootstrap metadata from. These can be any three random kafka brokers (or else use DNS or a vip) this is by no means intended to be a complete list of your brokers and the producer is not limited in any way to producing to the brokers it uses to get cluster metadata. These hosts are used only to find out the state of the cluster the first time. This isn't worse then the zookeeper situation (you still hardcode a couple urls) but gets rid of all the other zk issues.", "In addition to the above, the cluster metadata approach is more scalable since you get all metadata for several topics in one RPC roundtrip vs reading several paths per topic from zookeeper.", "Interesting.  Sounds like brokers act as \"portals into the whole Kafka cluster\", so to speak.  I wasn't aware of these ZK issues.  Just for my edification - are those issues specific to the ZK client that was used or how it was used in Kafka producer?", "[~otis] Those zk issues exist with any zookeeper client. ", "Thanks [~nehanarkhede].  Wouldn't all apps that talk to ZK have issues then?  Hadoop, HBase, SolrCloud, etc. etc.  Weird that I've never seen these issues reported anywhere else.... (honest comment, not trying to say anything bad here)", "So In producer we need a \"metadata.broker.list\" string, and in consumer we need a different zookeeper connect string, quite confusing...I want only one string for both of them...", "We are adding a new java consumer (org.apache.kafka.clients.consumer) in trunk and it will take the same broker list as the producer for bootstrapping."], "derived": {"summary": "Currently, the only place that ZK is actually used is in BrokerPartitionInfo. We use ZK to get a list of brokers for making TopicMetadataRequest requests.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "remove ZK dependency on producer - Currently, the only place that ZK is actually used is in BrokerPartitionInfo. We use ZK to get a list of brokers for making TopicMetadataRequest requests."}, {"q": "What updates or decisions were made in the discussion?", "a": "We are adding a new java consumer (org.apache.kafka.clients.consumer) in trunk and it will take the same broker list as the producer for bootstrapping."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-370", "title": "Exception \"java.util.NoSuchElementException: None.get\" appears inconsistently in Mirror Maker log.", "status": "Closed", "priority": "Major", "reporter": "John Fung", "assignee": "Jun Rao", "labels": [], "created": "2012-06-20T18:28:47.000+0000", "updated": "2012-06-22T20:16:01.000+0000", "description": "Exception in Mirror Maker log:\n=========================\n[2012-06-20 10:56:04,364] DEBUG Getting broker partition info for topic test01 (kafka.producer.BrokerPartitionInfo)\n[2012-06-20 10:56:04,365] INFO Fetching metadata for topic test01 (kafka.producer.BrokerPartitionInfo)\n[2012-06-20 10:56:04,366] ERROR Error in handling batch of 200 events (kafka.producer.async.ProducerSendThread)\njava.util.NoSuchElementException: None.get\n        at scala.None$.get(Option.scala:185)\n        at scala.None$.get(Option.scala:183)\n        at kafka.producer.ProducerPool.getAnyProducer(ProducerPool.scala:76)\n        at kafka.producer.BrokerPartitionInfo.updateInfo(BrokerPartitionInfo.scala:73)\n        at kafka.producer.BrokerPartitionInfo.getBrokerPartitionInfo(BrokerPartitionInfo.scala:45)\n        at kafka.producer.async.DefaultEventHandler.kafka$producer$async$DefaultEventHandler$$getPartitionListForTopic(DefaultEventHandler.scala:129)\n        at kafka.producer.async.DefaultEventHandler$$anonfun$partitionAndCollate$1.apply(DefaultEventHandler.scala:95)\n        at kafka.producer.async.DefaultEventHandler$$anonfun$partitionAndCollate$1.apply(DefaultEventHandler.scala:94)\n        at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)\n        at scala.collection.immutable.List.foreach(List.scala:45)\n        at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:44)\n        at scala.collection.mutable.ListBuffer.foreach(ListBuffer.scala:42)\n        at kafka.producer.async.DefaultEventHandler.partitionAndCollate(DefaultEventHandler.scala:94)\n        at kafka.producer.async.DefaultEventHandler.dispatchSerializedData(DefaultEventHandler.scala:65)\n        at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:49)\n        at kafka.producer.async.ProducerSendThread.tryToHandle(ProducerSendThread.scala:96)\n        at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:82)\n        at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:60)\n        at scala.collection.immutable.Stream.foreach(Stream.scala:254)\n        at kafka.producer.async.ProducerSendThread.processEvents(ProducerSendThread.scala:59)\n        at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:37)\n\nSteps to reproduce\n=================\nIt cannot be reproduced consistently. However, running the following script 2 or 3 times (step 2) will show the error:\n\n1. Apply kafka-306-v2.patch to 0.8 branch (revision 1352192 is used to reproduce this)\n\n2. Under the directory <kafka home>/system_test/broker_failure, execute the following command:\n=> $ bin/run-test.sh 5 0\n\n3. Check the log under the directory <kafka home>/system_test/broker_failure:\n=> $ grep Exception `ls kafka_mirror_maker*.log`\n=>    kafka_mirror_maker2.log:java.util.NoSuchElementException: None.get\n\n4. Also the kafka log sizes between source and target will not match:\n\n[/tmp]  $ find kafka* -name *.kafka -ls\n19400444 6104 -rw-r--r--   1 jfung    eng       6246655 Jun 20 10:56 kafka-source4-logs/test01-0/00000000000000000000.kafka\n19400819 5356 -rw-r--r--   1 jfung    eng       5483627 Jun 20 10:56 kafka-target3-logs/test01-0/00000000000000000000.kafka\n\nNotes about the patch kafka-306-v2.patch\n===============================\nThis patch fix the broker_failure test suite to do the followings:\n\na. Start 4 kafka brokers as source cluster\nb. Start 3 kafka brokers as target cluster\nc. Start 3 mirror maker to enable mirroring\nd. Send n messages to source cluster\ne. No bouncing is performed in this test for simplicity\nf. After the producer is stopped, validate the data count is matched between source & target\n", "comments": ["Attach patch v1. The issue is that the code relies on the broker ids in the hashmap to be always between 0 and size - 1, which is not always true, especially when there are failures.", "+1", "Thanks for the review. Committed to 0.8", "Downloaded rev. 1353005 and tried the test a few times. The issue is fixed."], "derived": {"summary": "Exception in Mirror Maker log:\n=========================\n[2012-06-20 10:56:04,364] DEBUG Getting broker partition info for topic test01 (kafka. producer.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Exception \"java.util.NoSuchElementException: None.get\" appears inconsistently in Mirror Maker log. - Exception in Mirror Maker log:\n=========================\n[2012-06-20 10:56:04,364] DEBUG Getting broker partition info for topic test01 (kafka. producer."}, {"q": "What updates or decisions were made in the discussion?", "a": "Downloaded rev. 1353005 and tried the test a few times. The issue is fixed."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-371", "title": "Creating topic of empty string puts broker in a bad state", "status": "Resolved", "priority": "Major", "reporter": "Martin Kleppmann", "assignee": "Jay Kreps", "labels": [], "created": "2012-06-21T01:09:43.000+0000", "updated": "2012-08-14T17:23:27.000+0000", "description": "Using the Java client library, I accidentally published a message where the topic name was the empty string. This put the broker in a bad state where publishing became impossible, and the following exception was logged 10-20 times per second:\n\n2012-06-21 00:41:30,324 [kafka-processor-3] ERROR kafka.network.Processor  - Closing socket for /127.0.0.1 because of er\nror\nkafka.common.InvalidTopicException: topic name can't be empty\n        at kafka.log.LogManager.getOrCreateLog(LogManager.scala:165)\n        at kafka.server.KafkaRequestHandlers.kafka$server$KafkaRequestHandlers$$handleProducerRequest(KafkaRequestHandle\nrs.scala:75)\n        at kafka.server.KafkaRequestHandlers.handleProducerRequest(KafkaRequestHandlers.scala:58)\n        at kafka.server.KafkaRequestHandlers$$anonfun$handlerFor$1.apply(KafkaRequestHandlers.scala:43)\n        at kafka.server.KafkaRequestHandlers$$anonfun$handlerFor$1.apply(KafkaRequestHandlers.scala:43)\n        at kafka.network.Processor.handle(SocketServer.scala:289)\n        at kafka.network.Processor.read(SocketServer.scala:312)\n        at kafka.network.Processor.run(SocketServer.scala:207)\n        at java.lang.Thread.run(Thread.java:679)\n\nRestarting Kafka did not help. I had to manually clear out the bad state in Zookeeper to resolve the problem.\n\nThe broker should not accept a message that would put it in such a bad state.", "comments": ["Modifed createLog() to fail if it has been asked to create a log that would fail on the getLogPool() function.\n\nI am not sure if the partition check is correct here, it appears that the partition could be any >0 partition and be OK since the file would get created. If there is no where else that partition would need to exist that would be OK, I think that this may not be the case though as it would need to be in Zk if my current understanding is correct. \n", "Probably should set fix version to 0.7.1", "Committed the 0.8 patch. Thanks.\n\nAt the moment I don't know if we are doing a 0.7.2 so I am going to hold off on the 0.7.1 patch.", "I think in the 0.8 patch, the changes to createLog are unnecessary. This is because the same check happens inside getLogPool, which is always called before createLog in getOrCreateLog, no ?", "Well, I didn't see any way for the bug to have occurred unless createLog was being called outside of where getLogPool gets called. So, it does appear to be unnecessary, perhaps these checks were added and the ticket was never updated? \n\n", "Hmm, good point. This is a little odd, though, no? Why would getting a log pool check the validity of a partition? Not sure how that came about. I recommend we delete that and keep the check in create--if you can't create a bad log you don't need to check on every access. Objections?\n\nActually I have a bunch of clean-ups I would like to do in LogManager. Let me post a patch with all these while I am in there.", "Yes, that will work too. I guess the API was added so that every access to the log was guarded with the check. But if we ensure that you can't create one in the first place, we can get rid of the check in getLogPool and keep the one in createLog. ", "Reopening for cleanups of LogManager", "Neha can you sanity check this? Here are the changes:\n\n- Remove some unneeded getter getServerConfig\n- Rename getTopicIterator to topics()\n- Simplify getLogIterator() and rename to allLogs()\n- Remove awaitStartup latch and awaitStartup latch acquisition. The comment on startup() says it registers in zookeeper, but this doesn't seem to be true\n- Remove getLogPool. createLog checks correctness of creation, and fails if it fails. getLog returns null if the log doesn't exist, as per the contract. getOrCreateLog will fail when createLog fails. I think this is more sensible, and elimantes the getter\n- Remove the word \"Map\" from things of type Map\n- MS should be Ms by our usual conventions\n- Remove helper getLogRetentionMSMap\n\nI think the latch was there because log manager was somehow doing zk registration (according to comments). But I don't see that code there at all now, and logManager shouldn't be talking to zk, so i think it got cleaned up. So now theoretically we shouldn't need that and the weird ordering where we take requests before log manager is initiatialized but then block should not be needed. \n\nAlso, I removed the EasyMock verification on logmanager (I think all it does is check that the config was fetched), which I don't think is useful? But EasyMock is kind of a blackbox to me.", "+1. \n\nMinor comment -\n\nChange logCleanupThresholdMS to logCleanupThresholdMs while you're in there", "Does it make sense to have a function to \"validate a log\" and call that any time you want to validate that the log you are reading/writing is valid? Potentially some of the validation would be redundant and called at times it didn't need to be, reducing capacity. \n\nSo the balance is between consistent checking and efficient execution.", "Got it, committed.", "Hey Jonathan, I don't think so, but I may have misunderstood. Basically a log has two states\n OPEN\n CLOSED\n\nEvery time we create a log object we run recovery on it which validates any messages since the last know flush point and truncates any partial writes to put the log in a known state. This is done as part of log construction so there is no way to have a reference to a log until it is known to be valid and ready for writes.\n\nOur policy is that we actually kill the instance of the broker if we see an IOException while writing to disk (STONITH, if you will), so there is effectively no invalid state. The reason for this is that an IO error is effectively the same as a broker failure in that it indicates a potentially partial or corrupt write. You cannot append to the log in this state, so continuing to accept traffic only makes thing worse, the best policy is to die and let the other brokers cover things.\n\nAttempt to read or write to a closed log would be a programming error in the broker, and should just give an exception about the file being closed, so I don't know if we need additional checks there."], "derived": {"summary": "Using the Java client library, I accidentally published a message where the topic name was the empty string. This put the broker in a bad state where publishing became impossible, and the following exception was logged 10-20 times per second:\n\n2012-06-21 00:41:30,324 [kafka-processor-3] ERROR kafka.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Creating topic of empty string puts broker in a bad state - Using the Java client library, I accidentally published a message where the topic name was the empty string. This put the broker in a bad state where publishing became impossible, and the following exception was logged 10-20 times per second:\n\n2012-06-21 00:41:30,324 [kafka-processor-3] ERROR kafka."}, {"q": "What updates or decisions were made in the discussion?", "a": "Hey Jonathan, I don't think so, but I may have misunderstood. Basically a log has two states\n OPEN\n CLOSED\n\nEvery time we create a log object we run recovery on it which validates any messages since the last know flush point and truncates any partial writes to put the log in a known state. This is done as part of log construction so there is no way to have a reference to a log until it is known to be valid and ready for writes.\n\nOur policy is that we actually kill the instance of the broker if we see an IOException while writing to disk (STONITH, if you will), so there is effectively no invalid state. The reason for this is that an IO error is effectively the same as a broker failure in that it indicates a potentially partial or corrupt write. You cannot append to the log in this state, so continuing to accept traffic only makes thing worse, the best policy is to die and let the other brokers cover things.\n\nAttempt to read or write to a closed log would be a programming error in the broker, and should just give an exception about the file being closed, so I don't know if we need additional checks there."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-372", "title": "Consumer doesn't receive all data if there are multiple segment files", "status": "Resolved", "priority": "Major", "reporter": "John Fung", "assignee": null, "labels": [], "created": "2012-06-21T18:30:08.000+0000", "updated": "2012-06-26T16:24:14.000+0000", "description": "This issue happens inconsistently but could be reproduced by following the steps below (repeat step 4 a few times to reproduce it):\n\n1. Check out 0.8 branch (currently reproducible with rev. 1352634)\n\n2. Apply kafka-306-v4.patch\n\n3. Please note that the log.file.size is set to 10000000 in system_test/broker_failure/config/server_*.properties (small enough to trigger multi segment files)\n\n4. Under the directory <kafka home>/system_test/broker_failure, execute command:\n$ bin/run-test.sh 20 0\n\n5. After the test is completed, the result will probably look like the following:\n\n========================================================\nno. of messages published            : 14000\nproducer unique msg rec'd            : 14000\nsource consumer msg rec'd            : 7271\nsource consumer unique msg rec'd     : 7271\nmirror consumer msg rec'd            : 6960\nmirror consumer unique msg rec'd     : 6960\ntotal source/mirror duplicate msg    : 0\nsource/mirror uniq msg count diff    : 311\n========================================================\n\n6. By checking the kafka log files, the sum of the sizes of the source cluster segments files are equal to those in the target cluster.\n\n[/tmp] $  find kafka* -name *.kafka -ls\n\n18620155 9860 -rw-r--r--   1 jfung    eng      10096535 Jun 21 11:09 kafka-source3-logs/test01-0/00000000000000000000.kafka\n18620161 9772 -rw-r--r--   1 jfung    eng      10004418 Jun 21 11:11 kafka-source3-logs/test01-0/00000000000020105286.kafka\n18620160 9776 -rw-r--r--   1 jfung    eng      10008751 Jun 21 11:10 kafka-source3-logs/test01-0/00000000000010096535.kafka\n18620162 4708 -rw-r--r--   1 jfung    eng       4819067 Jun 21 11:11 kafka-source3-logs/test01-0/00000000000030109704.kafka\n19406431 9920 -rw-r--r--   1 jfung    eng      10157685 Jun 21 11:10 kafka-target2-logs/test01-0/00000000000010335039.kafka\n19406429 10096 -rw-r--r--   1 jfung    eng      10335039 Jun 21 11:09 kafka-target2-logs/test01-0/00000000000000000000.kafka\n19406432 10300 -rw-r--r--   1 jfung    eng      10544850 Jun 21 11:11 kafka-target2-logs/test01-0/00000000000020492724.kafka\n19406433 3800 -rw-r--r--   1 jfung    eng       3891197 Jun 21 11:12 kafka-target2-logs/test01-0/00000000000031037574.kafka\n\n7. If the log.file.size in target cluster is configured to a very large value such that there is only 1 data file, the result would look like this:\n\n========================================================\nno. of messages published            : 14000\nproducer unique msg rec'd            : 14000\nsource consumer msg rec'd            : 7302\nsource consumer unique msg rec'd     : 7302\nmirror consumer msg rec'd            : 13750\nmirror consumer unique msg rec'd     : 13750\ntotal source/mirror duplicate msg    : 0\nsource/mirror uniq msg count diff    : -6448\n========================================================\n\n8. The log files are like these:\n\n[/tmp] $ find kafka* -name *.kafka -ls\n\n18620160 9840 -rw-r--r--   1 jfung    eng      10075058 Jun 21 11:24 kafka-source2-logs/test01-0/00000000000010083679.kafka\n18620155 9848 -rw-r--r--   1 jfung    eng      10083679 Jun 21 11:23 kafka-source2-logs/test01-0/00000000000000000000.kafka\n18620162 4484 -rw-r--r--   1 jfung    eng       4589474 Jun 21 11:26 kafka-source2-logs/test01-0/00000000000030269045.kafka\n18620161 9876 -rw-r--r--   1 jfung    eng      10110308 Jun 21 11:25 kafka-source2-logs/test01-0/00000000000020158737.kafka\n19406429 34048 -rw-r--r--   1 jfung    eng      34858519 Jun 21 11:26 kafka-target3-logs/test01-0/00000000000000000000.kafka\n", "comments": ["** Uploaded a patch with a simplified scenario to reproduce the data loss in multi segment files. \n\n** This patch provides a script \"run-test-debug.sh\" to do the following:\n1. Start 1 broker\n2. Start a modified version of Producer to send 300 messages with user specified message string length (500 chars will reproduce the issue while 50 chars will not). This producer produces messages with sequence ID and send the messages in sequence starting from 1, 2, 3, â¦ Etc.\n3. Start ConsoleConsumer to receive data\n\n** To reproduce the issue, under <kafka home>/system_test/broker_failure, execute the following command:\n\n$ bin/run-test-debug.sh 500 (which means each message string is 500 chars long)\n\nThe consumer only receives the first 120 messages. (This is verified by checking kafka.tools.DumpLogSegments.\n========================================================\nno. of messages published            : 300\nproducer unique msg rec'd            : 300\nsource consumer msg rec'd            : 120\nsource consumer unique msg rec'd     : 120\n========================================================\n\nThe number of segment files are \n\n$ ls -l /tmp/kafka-source1-logs/test01-0/\n-rw-r--r--   1 jfung  wheel  10431 Jun 24 20:59:21 2012 00000000000000000000.kafka\n-rw-r--r--   1 jfung  wheel  10440 Jun 24 20:59:22 2012 00000000000000010431.kafka\n-rw-r--r--   1 jfung  wheel  10440 Jun 24 20:59:23 2012 00000000000000020871.kafka\n-rw-r--r--   1 jfung  wheel  10440 Jun 24 20:59:24 2012 00000000000000031311.kafka\n-rw-r--r--   1 jfung  wheel  10441 Jun 24 20:59:26 2012 00000000000000041751.kafka\n-rw-r--r--   1 jfung  wheel  10460 Jun 24 20:59:27 2012 00000000000000052192.kafka\n-rw-r--r--   1 jfung  wheel  10460 Jun 24 20:59:28 2012 00000000000000062652.kafka\n-rw-r--r--   1 jfung  wheel  10460 Jun 24 20:59:29 2012 00000000000000073112.kafka\n-rw-r--r--   1 jfung  wheel  10460 Jun 24 20:59:31 2012 00000000000000083572.kafka\n-rw-r--r--   1 jfung  wheel  10460 Jun 24 20:59:32 2012 00000000000000094032.kafka\n-rw-r--r--   1 jfung  wheel  10460 Jun 24 20:59:33 2012 00000000000000104492.kafka\n-rw-r--r--   1 jfung  wheel  10460 Jun 24 20:59:34 2012 00000000000000114952.kafka\n-rw-r--r--   1 jfung  wheel  10460 Jun 24 20:59:35 2012 00000000000000125412.kafka\n-rw-r--r--   1 jfung  wheel  10460 Jun 24 20:59:37 2012 00000000000000135872.kafka\n-rw-r--r--   1 jfung  wheel  10460 Jun 24 20:59:38 2012 00000000000000146332.kafka\n-rw-r--r--   1 jfung  wheel      0 Jun 24 20:59:38 2012 00000000000000156792.kafka\n-rw-r--r--   1 jfung  wheel      8 Jun 24 21:00:08 2012 highwatermark\n\n\n** However, if the length of each message string is changed to a lower value 50, the issue won't be showing:\n\n$ bin/run-test-debug.sh 50\n\nThe consumer receives all data:\n========================================================\nno. of messages published            : 300\nproducer unique msg rec'd            : 300\nsource consumer msg rec'd            : 300\nsource consumer unique msg rec'd     : 300\n========================================================\n\nThe number of segment files are\n\n$  ls -l /tmp/kafka-source1-logs/test01-0\ntotal 64\n-rw-r--r--  1 jfung  wheel  10039 Jun 24 20:29:26 2012 00000000000000000000.kafka\n-rw-r--r--  1 jfung  wheel  10001 Jun 24 20:29:34 2012 00000000000000010039.kafka\n-rw-r--r--  1 jfung  wheel   1752 Jun 24 20:29:36 2012 00000000000000020040.kafka\n-rw-r--r--  1 jfung  wheel      8 Jun 24 20:30:06 2012 highwatermark\n", "There were several issues that caused the problem.\n\n1. Log.nextAppendOffset() calls flush each time. Since this method is called for every produce request, we force a disk flush for every produce request independent of the flush interval in the broker config. This makes producers very slow.\n\n2. The default value for MaxFetchWaitMs in consumer config is 3 secs, which is too long.\n\n3. The script runs console consumer in background and only waits for 20 secs, which is too short. What we should do is to run console consumer in foreground and wait until it finishes (since it has consumer timeout).\n\nAttach patch v1 that fixes items 1 and 2. The test now passes. However, we should address item 3 in the script too.", "Thanks Jun. It is working correctly after applying kafka-372-v1.patch.", "Thanks John for reviewing the patch. Committed to 0.8."], "derived": {"summary": "This issue happens inconsistently but could be reproduced by following the steps below (repeat step 4 a few times to reproduce it):\n\n1. Check out 0.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Consumer doesn't receive all data if there are multiple segment files - This issue happens inconsistently but could be reproduced by following the steps below (repeat step 4 a few times to reproduce it):\n\n1. Check out 0."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks John for reviewing the patch. Committed to 0.8."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-373", "title": "Broker Failure Test needs to be fixed to work with Mirror Maker in TRUNK", "status": "Resolved", "priority": "Major", "reporter": "John Fung", "assignee": "John Fung", "labels": [], "created": "2012-06-22T21:53:14.000+0000", "updated": "2012-07-06T20:45:27.000+0000", "description": "Embedded consumer is no longer supported and Mirror Maker is introduced to TRUNK. Broker Failure test needs to be fixed to use Mirror Maker for mirroring.", "comments": ["Uploaded kafka-373-v1.patch. The changes are:\n\n1. Modified mirror_producer*.properties files to use zk.connect as \"broker.list\" is no longer supported\n2. The main test script run-test.sh is modified to invoke Mirror Maker for mirroring.\n3. The bouncing option now supports source broker, target broker and mirror maker.\n4. Some methods which can be shared between tests such as \"info\", \"kill_child_processes\" are moved to a util.sh script located under \"system_test/common\"\n5. Updated README", "John, thanks for the patch.  Some of these comments would also apply to the\ntest in 0.8:\n\n- The header of the script is mostly in the README which is more\n  appropriate. i.e., we should just have a README.\n- Non-blocker: I think it would be good to switch to getopt for the test\n  parameters - I think we should do this in our system tests going forward...  and some\n  of the variable parameters in the script (i.e., the ones you mark as\n  \"change as needed\") can be made command-line parameters.\n- The script is getting to be big, so a number of functions can/should be\n  collapsed, and use arguments to handle the variants:\n  - wait_for_zero.. functions\n  - start_source/target cluster/server functions\n  - start_console_consumer functions\n- Shouldn't shutdown_producer do a kill_child_processes? in which case why\n  have the file to notify? FWIW, there's some bug in the script's shutdown -\n  don't remember what I did, but I had run the system test in 0.8 and my\n  hard-disk was full in the morning.\n- Not sure why you need to check the num_iterations ge iter inside the\n  start_test loop - also you can move the increment outside those blocks -\n  or switch to a for loop over a range.\n- I don't think you need to sleep after starting the mirror makers.\n- Can you elaborate on the hanging console consumer/mirror maker issue?\n", "Hi Joel,\n\nThanks for reviewing this patch.\n\nUploaded kafka-373-v2.patch and here are the changes:\n\n1. In \"initialize\" function, added code to find the location of the zk & kafka log4j log files. \n\n2. In \"cleanup\" function, added code to remove the zk & kafka log4j log files \n\n3. The header of the script is now removed and the description are in README \n\n4. Use getopt to process command line arguments \n\n5. Consolidated the following functions: \n\n    * start_console_consumer_for_source_producer \n    * start_console_consumer_for_mirror_producer \n    * wait_for_zero_source_console_consumer_lags \n    * wait_for_zero_mirror_console_consumer_lags \n\n6. The file to notify producer to stop: \nThe producer is sent to the background to run in a while-loop. If a file is used to notify the producer process in the background, the producer will exit properly inside the while loop. \n\n7. The following check is required for each of the source, target and mirror maker. It is because the following 2 lines are needed for: \n\n    * Line 1: find out if the $bounce_source_id is a char in the string $svr_to_bounce \n    * Line 2: check to see if $num_iterations is already reached and if $svr_idx > 0 (meaning this server needs to be bounced) \n\n    * Line 1: svr_idx=`expr index $svr_to_bounce $bounce_source_id` \n    * Line 2: if [[ $num_iterations -ge $iter && $svr_idx -gt 0 ]]; then", "+1\n\nThe script can be made more concise (basically, a lot more functions can and\nshould be moved to utils).\n\nHowever, I just realized that since development is primarily on 0.8 and\nbecause there are a couple of differences in this script between 0.8 and\ntrunk, there does not seem to be much point in putting in much effort for\nthis in trunk.\n\nSo if there are no objections, I think we can check this in and work on such\nimprovements in 0.8 only.\n", "Committed to trunk."], "derived": {"summary": "Embedded consumer is no longer supported and Mirror Maker is introduced to TRUNK. Broker Failure test needs to be fixed to use Mirror Maker for mirroring.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Broker Failure Test needs to be fixed to work with Mirror Maker in TRUNK - Embedded consumer is no longer supported and Mirror Maker is introduced to TRUNK. Broker Failure test needs to be fixed to use Mirror Maker for mirroring."}, {"q": "What updates or decisions were made in the discussion?", "a": "Committed to trunk."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-374", "title": "Move to java CRC32 implementation", "status": "Closed", "priority": "Minor", "reporter": "Jay Kreps", "assignee": "Jay Kreps", "labels": ["newbie"], "created": "2012-06-26T15:37:07.000+0000", "updated": "2014-06-19T05:17:01.000+0000", "description": "We keep a per-record crc32. This is fairly cheap algorithm, but the java implementation uses JNI and it seems to be a bit expensive for small records. I have seen this before in Kafka profiles, and I noticed it on another application I was working on. Basically with small records the native implementation can only checksum < 100MB/sec. Hadoop has done some analysis of this and replaced it with a Java implementation that is 2x faster for large values and 5-10x faster for small values. Details are here HADOOP-6148.\n\nWe should do a quick read/write benchmark on log and message set iteration and see if this improves things.", "comments": ["The following is a draft patch that transliterates the Hadoop CRC32 implementation into scala and swaps it in for our own crc32 implementation. I ran a sanity check on a few hundred million values to check I got the same crcs.\n\nThe CRC code is essentially unreadable. So it is not something you can really edit.\n\nI wrote a simple test that just sequentially creates messages (which requires computing the checksum). On this test I see about a 30-40% improvement, roughly across the board on Linux. I will include full results when it completes.\n\nI suspect this really only helps very trivial cases: benchmarks, mirroring, or other applications which do no real processing. It should speed up both the broker, the producer, and the consumer.", "Here are full performance results\n\nThe size is in bytes and the value for native/java is the nanoseconds per message averaged over a large number of messages:\n\nsize\tnative\tjava\timprovement\n16\t149.47\t108.11\t27.7%\n32\t197.8\t149.78\t24.3%\n64\t291.01\t219.89\t24.4%\n128\t487.36\t357.64\t26.6%\n256\t892.78\t631.15\t29.3%\n512\t1774.22\t1251.4\t29.5%\n1024\t3412.79\t2470.58\t27.6%\n2048\t6594.28\t4421.38\t33.0%\n4096\t13121.85\t8751.19\t33.3%\n8192\t25689.03\t18173.61\t29.3%\n16384\t51258.21\t36278.3\t29.2%\n32768\t103584.61\t73240.5\t29.3%\n65536\t207569.05\t146748.51\t29.3%\n131072\t415893.86\t292083.12\t29.8%\n\nI suspect there is still some scala numeric boxing magic happening here that would be good to get rid of.", "Should the native and the java column be reversed? Now, it reads that java is faster than native.", "Yeah, that's a little unclear. The \"java\" column is the \"pure jvm\" scala implementation in the patch and the native version is java.util.zip.CRC32 which uses JNI to call a C CRC library.", "I pulled in the pure-Java implementation from Hadoop for comparison:\n\nhttps://docs.google.com/spreadsheet/pub?key=0AksaPvYfWJQFdG5fZFNyWnpOUzZfZEtnVl9YZ21FWUE&output=html\n\nPure Java has a slight advantage over pure Scala, both have good speedup over JNI. This was done against 0.8 using the same test code in the patch", "Ooo very nice.", "Not sure how you guys feel about having Java in the source tree, but I attached a patch with the pure Java implementation (and the other stuff from [~jkreps] original patch).", "Should this be a post 0.8 item?", "Yes, this should go on trunk.", "Awesome.  I was just profiling some Kafka 0.7.1 stuff and noticed the CRC eating up time, and since I co-authored the pure java hadoop one, was just about to file a JIRA here....\n\n\nOn a related note, it would be nice to offload the CRC and decompression to a different thread than the user's thread.  Does 0.8's client do all of this work on the user thread like 0.7.1 ?  Our 0.7.1 consumers are often throughput bound by CPU including kafka code.    If the client is currently single-threaded I can file a JIRA with some ideas.", "Hey Scott, currently the implementation of the consumer is that there is a background thread that fetches data and populates a queue of data chunks which are handed off to the user's iterators. A single consumer client can feed many iterators in (potentially) many threads. The goal of this design was specifically to support a large thread pool of processors while still maintaining ordered consumption on a per-partition basis. So although a background thread would be one way to increase parallelism I think it is actually the harder one to reason about since now there are multiple thread pools to tune. I think just increasing the number of user threads/iterators is probably the best approach. ", "I was thinking of using Akka Actors, as they can provide the necessary ordering guarantees, do not require thread pool tuning, and should be easy to reason about.  The performance would likely be better than producer/consumer thread chains too, since chained actors often run in the same thread and avoid processor cache thrashing.", "Akka seems a bit overkill for this (although it does have some nice properties). It would be interesting to refactor the threading in Kafka with Akka and see what kind of performance differences there are (certainly beyond the scope of this JIRA).\n\nAs for the CRC implementation, is there consensus of what do here - Java or Scala?\n\nI say +1 for Java since no one will need to modify this code and it doesn't really matter that it's not Scala.", "Checked in the java version."], "derived": {"summary": "We keep a per-record crc32. This is fairly cheap algorithm, but the java implementation uses JNI and it seems to be a bit expensive for small records.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Move to java CRC32 implementation - We keep a per-record crc32. This is fairly cheap algorithm, but the java implementation uses JNI and it seems to be a bit expensive for small records."}, {"q": "What updates or decisions were made in the discussion?", "a": "Checked in the java version."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-375", "title": "update the site page for the 0.7.1 release download and related changes", "status": "Resolved", "priority": "Major", "reporter": "Joe Stein", "assignee": null, "labels": [], "created": "2012-06-26T19:43:44.000+0000", "updated": "2012-06-27T23:48:57.000+0000", "description": "approved release https://www.apache.org/dyn/closer.cgi/incubator/kafka/kafka-0.7.1-incubating/", "comments": ["Joe, thanks for pulling this together. There are some small edits required to the quick start as well (due to the consumer API change). I can add a diff for that in a bit.", "This is great. Joel, do you think we should add a few lines to the design document anywhere to mention the wildcard support?", "Yes - added a few more comments on topic filters.", "Looks good. The only thing is that message format has changed in 0.7.1 slightly (an optional attribute field) and we need to reflect that in design. Once that's fixed, we can commit. \n\n/**\n * A message. The format of an N byte message is the following:\n *\n * If magic byte is 0\n *\n * 1. 1 byte \"magic\" identifier to allow format changes\n *\n * 2. 4 byte CRC32 of the payload\n *\n * 3. N - 5 byte payload\n *\n * If magic byte is 1\n *\n * 1. 1 byte \"magic\" identifier to allow format changes\n *\n * 2. 1 byte \"attributes\" identifier to allow annotations on the message independent of the version (e.g. compression enabled, type of codec used)\n *\n * 3. 4 byte CRC32 of the payload\n *\n * 4. N - 6 byte payload\n * \n */\n", "v3 patch attached, covers the other patches together + the message format updated in design.html", "commited "], "derived": {"summary": "approved release https://www. apache.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "update the site page for the 0.7.1 release download and related changes - approved release https://www. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "v3 patch attached, covers the other patches together + the message format updated in design.html"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-376", "title": "expose different data to fetch requests from the follower replicas and consumer clients", "status": "Closed", "priority": "Major", "reporter": "Jun Rao", "assignee": "Prashanth Menon", "labels": ["bugs"], "created": "2012-06-26T23:40:00.000+0000", "updated": "2012-09-10T15:29:42.000+0000", "description": "Currently, the broker always uses highwatermark to calculate the available bytes to a fetch request, no matter where the request is from. Instead, we should use highwatermark for requests coming from real consumer clients and use logendoffset for requests coming from follower replicas.", "comments": ["I can take a look at this, probably this weekend.", "Hey Prashanth, how goes it? Let me know if you don't have time and I can pick this up.", "I actually didn't get a chance to look at this, though I'd really like to.  Tell you what, I'll try to start this weekend and if I'm unable to, you can take it off my hands :)", "Awesome, sounds great! No pressure, just didn't want it to fall through the cracks. :-)", "Minor update: Started some of the work involved, didn't get a chance to put as much time to it as I'd like.  Hoping to get a patch out by mid-end of this week.", "Hi all, I feel like I should add a draft patch because the code has grown much more complex since the last time I touched it.  The patch itself is minimal but a little nasty, so I'd like to clean it up a little but I figured I'd submit a draft so more eyes can get on it.  Here goes ... an outline:\n\n1. Modified KafkaApis.availableFetchBytes to check the leader replicas highwaterMark if the request is coming from a follower, otherwise use the logEndOffset.  \n2. Modified readMessageSet to not read beyond the highwaterMark of the replica's log (which is local because it's the leader) if the request is coming from a regular, non-follower, consumer.\n\nOne thing I'm considering doing is putting the log read logic in the replica itself since it is aware of hw and leo.  There's also a ReplicaManager.getLeaderReplica that throws an exception if the leader replica doesn't exist but returns an Option[Replica] - can this just return a Replica?  \n\nAs for the test, I couldn't find a decent place to put it because it includes both replication and simple consumer tests.  Rather than split the tests into two separate classes, I thought it'd be better to group them together.\n\nLooking forward to the suggestions.", "Prashanth,\n\nThanks for the patch. Some comments:\n\n1. In KafkaApis, currently we allow fetch requests on follower replicas, in addition to the leader replica. This is potentially useful for debugging purpose and it would be good to keep this. For normal consumer clients, the fetch requests will still be routed to the leader. However, for tools like ConsoleConsumer, we can potentially consume from any replica.\n\n2. SimpleFetchTest:\n2.1 Could you add the Apache header and remove the author comment?\n2.2 Since you mocked ReplicaManager, could we do the test with just 1 replica?\n2.3 Not sure if bigFetch tests anything more than goodFetch in testNonReplicaSeesHwWhenFetching(). In addition to goodFetch, we should add a fetchRequest with an offset at HW and expect an empty MessageSet in the response.\n2.4 In testNonReplicaSeesHwWhenFetching(), the second comment of \" // there should be no response\" is incorrect.\n2.5 For verification, it's probably simpler if we just verify that all log.read requests made are correct.\n\nFinally, we probably should have kafka-434 committed first since it's a big patch and has been rebased quite a few times. It's almost there and should be committed this week.", "Thanks for the comments, Jun.\n\n1. Do we currently support this?  The fetch requests always check that the leader is on the broker handling the request before actually reading from the log, so I'm not sure how a read on a follower succeeds?\n\n2.1, 2.2, 2.3, 2.4, 2.5 All valid.  Will upload new patch later this week.\n\nI assume you mean, KAFKA-343?  I've been following it, and yes, it's grown quite large/complex hahaha.  I'll definitely wait for that to get ironed out and committed.", "For 1, yes, you are right. We actually ensure that fetch requests can only be made on the leader right now. So, we can relax it in a separate jira.\n\nYes, I meant 343.", "Sigh, I took way too long to take a look at this again.  Regardless, I've attached a new patch that addresses the points.  I'm still a little unhappy with the logic to determine offsets - though I believe it's functionality correct, something about it seems off.  I'll mull it over while comments come in, hopefully someone points something out.\n\n", "Hi Prashanth, the patch didn't apply cleanly for me. Can you svn up? ", "Thanks for patch v1. It looks good overall. Some comments:\n\n20. KafkaApis.availableFetchBytes: getting the leo of a replica can be done as leader.logEndOffset.\n\n21. Could you rebase and make sure that single_host_multi_brokers under system_test passes?", "Hi all,\n\nFind a second patch that addresses 20 and 21 (and some line spacing cleanup).  It passes all the tests except for BackwardsCompatibilityTest; that test uses a pre-constructed log-data file that's loaded up when the Kafka server starts.  The test then issues an offset request and pulls all the data back and expected 100 messages total.  There are three issues here:\n\n1) It looks like we need to handle the case where a topic/partition has a replication factor of one.  In such cases, updating the LEO of the log should also update the HW to the same value.  I took care of this in the patch, but I'd like some feedback on if it's the correct approach.\n2) What should the server do on startup if it is a leader of a specific topic/partition and finds local data available for it.  Presumable, it should truncate it's log to the value in the checkpoint file, but it doesn't seem to be done now.  Is this thinking correct? \n3) KafkaApis should use the leader's HW to perform a bounds check when responding to an offset request.  It currently checks for leader presence, adding a check in should be relatively simple.  I can attach another patch with that (and any other issues that come up) mid-week.  ", "Prashanth, thanks for the updates. \n\n1) We have a separate jira kafka-420 to track the issue of maintaining HW with just 1 replica. It probably needs a bit more work than what's in your patch. First of all, we should increase HW as long as ISR (not assigned replica) has only 1 replica. Second, we need to check if we can increase HW in at least two other places (a) when a replica becomes the leader, (b) when ISR shrinks to size 1.\n\n2) A new leader should never truncate its log. Only a follower truncates its log. This is because the new leader is guaranteed to have all committed data in its log and truncating any data from its log may risk losing a committed message.\n\n3) Yes, we should fix the getOffset api. We will need to distinguish requests from a follower and a regular client, so that we can return either LEO or HW accordingly. We can add a replicaId to the getOffsetRequest like FetchRequest.\n\n4) There are a couple other jiras that complicate things a bit. (a) In kafka-461, we are trying to get rid of the support of magic byte 0 in Message, which will end up remove BackwardsCompatibilityTest. However, if BackwardsCompatibilityTest is the only test that fails for this jira, that means we don't have enough unit test coverage for this jira and kafka-420. So, we will need to add a new unit test. (b) kafka-351 is doing some refactoring of ReplicaManager, which will change a bit how KafkaApis interacts with ReplicaManager.\n\nIdeally, we probably should fix things in the following order. (a) get kafka-351 (I hope to check in mid this week) in first so that we can use the cleaner api in ReplicaManager; (b) fix kafka-420 and add a new unit test; (c) fix this jira and patch OffsetRequest. Does this make sense to you? We can probably have someone else work on kafka-420 if you don't have the time. ", "Prashanth,\n\nkafka-461, kafka-351 and kafka-420 have all been committed to 0.8. This jira should be unblocked now. Could you rebase your patch?", "Here we go, I've attached v3 of the patch.  It's effectively the same, but with some adjustments to compile against latest trunk.  Let me know what you guys think.\n\nMinor edit, thanks for taking care of the other JIRA's.  The last few weeks have been fairly busy, but the good news is that I see a break which I hope to use to get back into the swing of things :)  As for the offsets API, can we take care of that in a separate JIRA?", "Thanks for patch v3. +1 on the patch. The following are some minor comments. Once they are addressed, the patch can be checked in without another review.\n\n30. KafkaApis:\n30.1 availableFetchBytes(): Not all exceptions are no leader exceptions. So, we should log the exception and change the error message to be more general.\n30.2 readMessageSets(fetchRequest): The comment about replica id value of -1 can be removed. There is no need to pass in brokerId for the first replicaManager.getReplica call since it defaults to local replica. There is no need to make the second replicaManager.getReplica call for the remote replica since topic and partitionId are available locally. \n30.3 readMessageSet(): Leader's log should always to be available. If not, we should log an error in addition to returning an empty set.\n\n31. SimpleFetchTest: There is no need to mock KafkaZooKeeper any more.\n\nAlso, could you create another jira to fix the getOffset api?", "Thanks for the review Jun.  30 and 31 are all silly mistakes by me.  I'll attach a patch later today to address them.", "Hi all,\n\nSo I've attached a new patch that addresses all the points except the second point in 30.2 .  It looks like using EasyMock to mock methods with optional arguments is tricky; the call to ReplicaManager.getReplica is causing issues because the brokerId is optional.  I suspect the Scala compiler produces several versions of the method that are chained together at runtime, but which EasyMock can't resolve for some reason.  I've tried some trickery to no avail.\n\nIt's a bad idea to modify code to accomadate faulty test/tools, but I figured I'd attach a patch for review while I check out other options.  Let me know what you think,", "Thanks for patch v4. The EasyMock issue is interesting. Maybe you have to mock getReplica(topic, partition) directly. \n\nAnother thought. The only reason that we call getReplica in KafkaApis.readMessageSets(fetchRequest) is to get the highWatermark. We can change readMessageSet(topic: String, partition: Int, offset: Long, maxSize: Int, fromFollower: Boolean) to return Either[Short, (MessageSet, highWatermark]) instead. This way, we can avoid calling getReplica in the first readMessageSets(). In general, the fewer times that we call getReplica the better since there are few places for error handling.", "Also, in availableFetchBytes(), we should probably distinguish UnknownTopicOrPartitionException from other exceptions. For the former, we just need to add an info level log. For the latter, we need to add an error level log with the stacktrace.", "Thanks for taking a look, Jun.  I've attached a new patch that ddresses the following items:\n- readMessageSet now returns Either[Short, (MessageSet, Long)] where, on success, we return both the messages and the highwatermark of the leader.\n- availableFetchBytes now logs info when it gets an UnknownTopicOrPartitionException exception, and logs an error for any other exceptions.\n\nI tried to fiddle with EasyMock to get around the optional argument issue, but had no luck with it.  Seems like Mockito get's around it by mocking out all possible invocations permuting the parameters which is a little nasty.\n\nLet me know what you think.", "Thanks for patch v5, Prashanth. +1 on the patch.", "Actually, there is one other issue:\n\nIn readMessageSet(topic: String, partition: Int, offset: Long, maxSize: Int, fromFollower: Boolean), could we make sure that actualSize is always >=0 ?", "Good catch, Jun.  V6 patch attached; if all is good, I'll go ahead and commit to the 0.8 branch.", "Thanks for patch v6. We probably should do the length check inside log.read() , instead of readMessageSet. This way, we make sure that the caller's offset is checked by Log.findRange and an OffsetOutOfRangeException can be thrown if needed. In log.read(), after Log.findRange, we can check if length is <=0  and if so, immediately return an empty set.", "Argh, should have run the test suite before submitting the patch.  I've attached a new one that moves the logic into Log.read().", "Thanks for patch v7. +1 Please commit to 0.8.", "Thanks Jun.  Comitted to 0.8."], "derived": {"summary": "Currently, the broker always uses highwatermark to calculate the available bytes to a fetch request, no matter where the request is from. Instead, we should use highwatermark for requests coming from real consumer clients and use logendoffset for requests coming from follower replicas.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "expose different data to fetch requests from the follower replicas and consumer clients - Currently, the broker always uses highwatermark to calculate the available bytes to a fetch request, no matter where the request is from. Instead, we should use highwatermark for requests coming from real consumer clients and use logendoffset for requests coming from follower replicas."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks Jun.  Comitted to 0.8."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-377", "title": "some of the dist mirrors are giving a 403", "status": "Closed", "priority": "Major", "reporter": "Joe Stein", "assignee": "Joe Stein", "labels": [], "created": "2012-06-28T04:30:01.000+0000", "updated": "2012-09-19T16:40:09.000+0000", "description": "https://issues.apache.org/jira/browse/INFRA-4975\n\nwill make patch to change the links to by pass this", "comments": ["+1", "have an update here, don't think we need to patch so holding off the commit\n\n\"Somehow new incubator directories get bad permissions on the rsync servers eos, aurora. \nThe perms on mino are ok ; some stupid bug we can't track down. The perms were corrected \non eos & aurora. \n\nSome mirrors rsync with bad options ; missing '-p' (preserve permissions), so the initial bad \npermissions aren't corrected on the mirror. \"\n\nI mailed the mirror in the example (reverse.net). \"\n\nhttps://issues.apache.org/jira/browse/INFRA-4975?focusedCommentId=13403334&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13403334", "INFRA-4975 is closed now, we good here?", "nope, i found a few mirrors still bad will re-open the ticket in INFRA\n\nhttp://newverhost.com/pub/incubator/kafka \nhttp://mirror.symnds.com/software/Apache/incubator/kafka \nhttp://apache.osuosl.org/incubator/kafka\nhttp://ftp.wayne.edu/apache/incubator/kafka \nhttp://mirror.atlanticmetro.net/apache/incubator/kafka\nhttp://mirror.nyi.net/apache/incubator/kafka\nhttp://apache.mesi.com.ar/incubator/kafka \nhttp://apache.deathculture.net/incubator/kafka\nhttp://mirror.candidhosting.com/pub/apache/incubator/kafka \nhttp://apache.ziply.com/incubator/kafka\nhttp://mirrors.axint.net/apache/incubator/kafka\nhttp://apache.mirrors.tds.net/incubator/kafka", "I clicked through everything at https://www.apache.org/dyn/closer.cgi/incubator/kafka/kafka-0.7.1-incubating/, got one (presumably transient) timeout and no 403s.  I think we are good here.", "Can we close this?"], "derived": {"summary": "https://issues. apache.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "some of the dist mirrors are giving a 403 - https://issues. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "Can we close this?"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-378", "title": "mirrors but must never be used for verification", "status": "Resolved", "priority": "Major", "reporter": "Joe Stein", "assignee": null, "labels": [], "created": "2012-06-28T12:49:04.000+0000", "updated": "2012-06-28T16:14:20.000+0000", "description": "http://incubator.apache.org/guides/releasemanagement.html#understanding-distribution\n\nMirrored copies of checksums, KEYS and signature files (.asc and .md5 files) will be present on the mirrors but must never be used for verification. So, all links from the podling website to signatures, sums and KEYS need to refer to the original documents on www.apache.org\n\n", "comments": ["patch pointing to the original asc key and md5 for verification", "+1", "committed and updated svn on site box"], "derived": {"summary": "http://incubator. apache.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "mirrors but must never be used for verification - http://incubator. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "committed and updated svn on site box"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-379", "title": "TopicCount.constructTopicCount isn't thread-safe", "status": "Closed", "priority": "Major", "reporter": "Nick Howard", "assignee": "Jun Rao", "labels": ["bugs"], "created": "2012-06-28T21:01:29.000+0000", "updated": "2012-09-18T05:06:14.000+0000", "description": "TopicCount uses scala.util.parsing.json.JSON, which isn't thread-safe https://issues.scala-lang.org/browse/SI-4929\n\nIf you have multiple consumers within the same JVM, and they all rebalance at the same time, you can get errors like the following:\n\n[...] kafka.consumer.TopicCount$.constructTopicCount:39] ERROR: error parsing consumer json string [...]\njava.lang.NullPointerException\n        at scala.util.parsing.combinator.Parsers$NoSuccess.<init>(Parsers.scala:131)\n        at scala.util.parsing.combinator.Parsers$Failure.<init>(Parsers.scala:158)\n        at scala.util.parsing.combinator.Parsers$$anonfun$acceptIf$1.apply(Parsers.scala:489)\n        ...\n        at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:742)\n        at scala.util.parsing.json.JSON$.parseRaw(JSON.scala:71)\n        at scala.util.parsing.json.JSON$.parseFull(JSON.scala:85)\n        at kafka.consumer.TopicCount$.constructTopicCount(TopicCount.scala:32)\n        at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.kafka$consumer$ZookeeperConsumerConnector$ZKRebalancerListener$$getTopicCount(ZookeeperConsumerConnector.scala:422)\n        at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.kafka$consumer$ZookeeperConsumerConnector$ZKRebalancerListener$$rebalance(ZookeeperConsumerConnector.scala:460)\n        at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener$$anonfun$syncedRebalance$1.apply$mcVI$sp(ZookeeperConsumerConnector.scala:437)\n        at scala.collection.immutable.Range$ByOne$class.foreach$mVc$sp(Range.scala:282)\n        at scala.collection.immutable.Range$$anon$2.foreach$mVc$sp(Range.scala:265)\n        at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.syncedRebalance(ZookeeperConsumerConnector.scala:433)\n        at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.handleChildChange(ZookeeperConsumerConnector.scala:375)\n        at org.I0Itec.zkclient.ZkClient$7.run(ZkClient.java:568)\n        at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)\n\nI ran into this on 0.7.0, but the code in trunk appears to be vulnerable to the same issue.", "comments": ["I patched our fork like this.\n\nThe patch takes the parsing code from scala.util.parsing.json.JSON and puts it in a class instead of an object, so it's instantiable.", "Thanks for reporting this - and the SI reference.\n\nThis likely affects 0.8 in other places as well:\ngit grep parseFull\ncore/src/main/scala/kafka/consumer/TopicCount.scala:        JSON.parseFull(topicCountString) match {\ncore/src/main/scala/kafka/server/StateChangeCommand.scala:      JSON.parseFull(requestJson) match {\ncore/src/main/scala/kafka/utils/ZkUtils.scala:        JSON.parseFull(jsonPartitionMap) match {", "Thanks for the patch. In 0.7, the only usage of JSON is in TopicCount and all usages are called from ZookeeperConsumerConnector.rebalance, which is synchronized. So, it seems that we just need to create a new JSONParser instance per ZookeeperConsumerConnctor instance. Creating 1 JSONParser instance each time a parser is needed may be too expensive.", "Attach patch v1 in 0.8. Just created a synchronized singleton that wraps scala JSON. Since json parsing is used rarely in Kafka, this is likely not a performance concern.", "+1", "+1. Looks good.", "Forgot to add - can we get this patched on trunk as well?", "Thanks for the review. Rebased and committed to 0.8. Will port to 0.7.", "Do you guys want a patch for 0.6?   I just created one and can attach it.   I know, we should really upgrade to 0.7.", "Evan,\n\nThe 0.6 release is pre Apache. So, we won't be able to patch it. Sorry.", "Patched and committed to trunk too."], "derived": {"summary": "TopicCount uses scala. util.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "TopicCount.constructTopicCount isn't thread-safe - TopicCount uses scala. util."}, {"q": "What updates or decisions were made in the discussion?", "a": "Patched and committed to trunk too."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-380", "title": "Enhance single_host_multi_brokers test with failure to trigger leader re-election in replication", "status": "Resolved", "priority": "Major", "reporter": "John Fung", "assignee": "John Fung", "labels": [], "created": "2012-06-28T21:33:44.000+0000", "updated": "2012-08-06T17:28:12.000+0000", "description": null, "comments": ["Uploaded kafka-380-v1.patch with the following changes:\n\nA. Introduce failure to the brokers in a round robin fashion (as described in README):\n1. Start the Kafka cluster\n2. Create topic\n3. Find the leader\n4. Stop the broker in Step 3\n5. Send n messages\n6. Consume the messages\n7. Start the broker in Step 3\n8. Goto Step 3 for all servers in the cluster\n9. Validate test results\n\nB. Keep track of the leader re-election latency (the difference between the broker shutdown and leader re-elected timestamp).\n\nC. Report the max and min latency values\n", "- Looks good overall, but will revisit after KAFKA-350 is done.\n- Instead of sleeping 30s after shutting down a server, pgrep for it.\n- We should get rid of as many sleeps as possible. E.g., with producer acks\n  we don't need to sleep after producer-performance. Likewise for all other\n  sleeps, let see why they are needed and provide tooling (if necessary)\n  to eliminate/reduce them.\n", "Filed KAFKA-392 which includes the two points above.\n", "Thanks Joel for reviewing kafka-380-v1.patch. I have uploaded kafka-380-v2.patch (not the changes you suggested for KAFKA-392) with additional minor changes:\n\nThe changes made in kafka-380-v2.patch is to fix the problem to run this test in MacOS. The problem is due to the different argument options syntax between Linux and Darwin (MacOS) for the shell command \"date\".", "Patch v2 doesn't apply after kafka-306 is committed. Could you rebase?", "Thanks Jun. Uploaded kafka-380-v3.patch with the following changes:\n\n1. Rebased from 0.8 branch after KAFKA-306 is checked in.\n2. Fixed running issue in MacOS", "Thanks for patch v3. Shouldn't we set invoke_failures to true by default? However, when I do that, the test seems to hang after the following:\n\n\n2012-07-11 09:57:54 ---------------------------------------\n2012-07-11 09:57:54 leader re-election latency : 36 ms\n2012-07-11 09:57:54 ---------------------------------------\n2012-07-11 09:57:54 starting console consumer\n2012-07-11 09:57:54 sleeping for 5s\n2012-07-11 09:57:59 starting producer performance\n", "Thanks Jun for reviewing. The test is hanging due to the following exception thrown by ProducerPerformance:\n\n[2012-07-10 15:38:31,510] INFO Beging shutting down ProducerSendThread (kafka.producer.async.ProducerSendThread)\n[2012-07-10 15:39:01,491] INFO Disconnecting from 127.0.0.1:9093 (kafka.producer.SyncProducer)\n[2012-07-10 15:39:01,491] INFO Disconnecting from 127.0.0.1:9093 (kafka.producer.SyncProducer)\n[2012-07-10 15:39:01,491] INFO Disconnecting from 127.0.0.1:9093 (kafka.producer.SyncProducer)\n[2012-07-10 15:39:01,491] INFO Disconnecting from 127.0.0.1:9093 (kafka.producer.SyncProducer)\n[2012-07-10 15:39:01,495] ERROR Error in handling batch of 1 events (kafka.producer.async.ProducerSendThread)\njava.net.SocketTimeoutException\n        at sun.nio.ch.SocketAdaptor$SocketInputStream.read(SocketAdaptor.java:201)\n        at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:86)\n        at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:221)\n        at kafka.utils.Utils$.read(Utils.scala:603)\n        at kafka.network.BoundedByteBufferReceive.readFrom(BoundedByteBufferReceive.scala:54)\n        at kafka.network.Receive$class.readCompletely(Transmission.scala:55)\n        at kafka.network.BoundedByteBufferReceive.readCompletely(BoundedByteBufferReceive.scala:29)\n        at kafka.network.BlockingChannel.receive(BlockingChannel.scala:92)\n        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:78)\n        at kafka.producer.SyncProducer.doSend(SyncProducer.scala:76)\n        at kafka.producer.SyncProducer.send(SyncProducer.scala:115)\n        at kafka.producer.BrokerPartitionInfo.updateInfo(BrokerPartitionInfo.scala:76)\n        at kafka.producer.BrokerPartitionInfo.getBrokerPartitionInfo(BrokerPartitionInfo.scala:45)\n        at kafka.producer.async.DefaultEventHandler.kafka$producer$async$DefaultEventHandler$$getPartitionListForTopic(DefaultEventHandler.scala:129)\n        at kafka.producer.async.DefaultEventHandler$$anonfun$partitionAndCollate$1.apply(DefaultEventHandler.scala:95)\n        at kafka.producer.async.DefaultEventHandler$$anonfun$partitionAndCollate$1.apply(DefaultEventHandler.scala:94)\n        at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)\n        at scala.collection.immutable.List.foreach(List.scala:45)\n        at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:44)\n        at scala.collection.mutable.ListBuffer.foreach(ListBuffer.scala:42)\n        at kafka.producer.async.DefaultEventHandler.partitionAndCollate(DefaultEventHandler.scala:94)\n        at kafka.producer.async.DefaultEventHandler.dispatchSerializedData(DefaultEventHandler.scala:65)\n        at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:49)\n        at kafka.producer.async.ProducerSendThread.tryToHandle(ProducerSendThread.scala:96)\n        at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:82)\n        at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:60)\n        at scala.collection.immutable.Stream.foreach(Stream.scala:254)\n        at kafka.producer.async.ProducerSendThread.processEvents(ProducerSendThread.scala:59)\n        at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:37)", "Uploaded kafka-380-v4.patch with the following changes:\n\n1. Rebased with the latest 0.8 branch\n2. Fixed an issue running in MacOS\n3. Added validation for re-election latency", "1. We can probably get rid of the following information from the test output -\n\n2012-07-26 09:21:47 server_shutdown_unix_timestamp      : 1343319697\n2012-07-26 09:21:47 server_shutdown_unix_timestamp_ms   : 743\n2012-07-26 09:21:47 elected_leader_unix_timestamp       : 1343319697\n2012-07-26 09:21:47 elected_leader_unix_timestamp_ms    : 794\n2012-07-26 09:21:47 full_elected_leader_unix_timestamp  : 1343319697.794\n2012-07-26 09:21:47 full_server_shutdown_unix_timestamp : 1343319697.743\n\n2. Can you make the test output also get logged to some file called test-output.log ? This will be helpful for debugging. \n\nOther than that, this patch looks good.\n", "Hi Neha,\n\nThanks for reviewing. Uploaded kafka-380-v5.patch which has the changes suggested.", "+1. ", "Thanks for the patch John !"], "derived": {"summary": "", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Enhance single_host_multi_brokers test with failure to trigger leader re-election in replication"}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for the patch John !"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-381", "title": "Changes made by a request do not affect following requests in the same packet.", "status": "Resolved", "priority": "Minor", "reporter": "Samir Jindel", "assignee": null, "labels": [], "created": "2012-06-29T14:43:36.000+0000", "updated": "2017-11-16T14:50:42.000+0000", "description": "If a packet contains a produce request followed immediately by a fetch request, the fetch request will not have access to the data produced by the prior request.", "comments": ["Interesting. Is this actually true in 0.7.x? I think because we use a single selector per connection and a single connection per client, provided they use the same client ordering should be maintained and anything produced on a given socket will be visible on that socket. An exception would be the async producer, but that makes sense, because, well, it is asynchronous.\n\nIn 0.8 we have changed this model so that there are multiple I/O threads. Now that I think about it, this is problematic from an ordering guarantee pov. We may need to revisit this because this guarantee does need to be maintained.", "I thought it is because only messages that are flushed to the disk hence below the high water mark will be returned in 0.7? As long as the flush process is async the following fetch request will not be able to get the message that are still in the buffer.", "The problem is likely due to what Guozhang described. In 0.7, if you sent flush.interval to 1 on the broker, a fetch request should see the previously produced data from the same client.", "Ah, yes, makes sense.", "I think we can safely close this issue, the behavior was sufficiently investigated and explained.\r\nBehavior today would still be like this and is the expected behavior."], "derived": {"summary": "If a packet contains a produce request followed immediately by a fetch request, the fetch request will not have access to the data produced by the prior request.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Changes made by a request do not affect following requests in the same packet. - If a packet contains a produce request followed immediately by a fetch request, the fetch request will not have access to the data produced by the prior request."}, {"q": "What updates or decisions were made in the discussion?", "a": "I think we can safely close this issue, the behavior was sufficiently investigated and explained.\r\nBehavior today would still be like this and is the expected behavior."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-382", "title": "Write ordering guarantee violated", "status": "Resolved", "priority": "Major", "reporter": "Jay Kreps", "assignee": "Jay Kreps", "labels": [], "created": "2012-06-29T15:54:28.000+0000", "updated": "2012-08-14T16:35:45.000+0000", "description": "The guarantee is that if the producer does\n  send(X)\n  send(Y)\nthe client see X first and Y second, but this may not actually happen in 0.8. The reason is because of the parallel I/O threads and the single queue in the network server. The current model is one work queue and one response queue per selector. The single queue is great from a parallelism point of view--if one thread is blocked another can do the work--but this actually breaks the ordering guarantee. Not sure how I missed this in the initial work. :-(\n\nThe reason for the single work queue was to avoid blocking a whole selector when one thread does a flush. But I wonder now how relevant that is now. If the durability guarantee comes from replication I think there is not much reason to have a blocking flush, we can rely on pdflush to do it in the background so doing the write synchronously may be fine.\n\nI think the solution is to modify RequestChannel to have one work queue per I/O thread and hash into the work queue by connection id. In this solution a blocked I/O thread only blocks clients that hash onto it. This retains the current async model but no longer has the property that a blocked thread doesn't block everyone. (At first I thought we didn't need a RequestChannel at all any more and could just synchronously return zero or more requests from KafkaApis, but in reality because of the possibility of request timeout from a background thread, this won't work.)\n\nIt would also be possible to be smarter still and attempt a non-blocking solution that only preserves the write-ordering guarantees. One solution would be as follows. Each request from a given connection would be assigned an increasing number starting with 0 by the network layer. KafkaApi would keep a \"last processed\" number for each connection. Any request which is more than the current number for that connection + 1 would be re-enqueued. I don't like this solution because it is more complex and because I don't think blocking flushes are needed now that we have replication (e.g. you can just turn on replication and rely on pdflush which is async), so optimizing this case is not useful imo.", "comments": ["Currently, each produce request blocks on a response, which means that a client can't send the next request until it has received a response from the current request. This will guarantee that all writes are processed in send order at the leader. So, we are fine now. If we want to make the producer non-blocking, then the shared work queue approach can violate the ordering. But if the producer is non-blocking, maybe you can argue that it doesn't care about ordering.", "Yes, that makes sense."], "derived": {"summary": "The guarantee is that if the producer does\n  send(X)\n  send(Y)\nthe client see X first and Y second, but this may not actually happen in 0. 8.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "Write ordering guarantee violated - The guarantee is that if the producer does\n  send(X)\n  send(Y)\nthe client see X first and Y second, but this may not actually happen in 0. 8."}, {"q": "What updates or decisions were made in the discussion?", "a": "Yes, that makes sense."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-383", "title": "copied the 0.7.0 release to the dist folder and update the site", "status": "Resolved", "priority": "Major", "reporter": "Joe Stein", "assignee": null, "labels": [], "created": "2012-06-29T18:04:08.000+0000", "updated": "2012-07-06T19:40:09.000+0000", "description": "I copied the 0.7.0 release to the dist folder\n\nit will take sometime to show up and be on the mirrors\n", "comments": ["links now point to mirror cgi and the apache dist site i copied the 0.7.0 release to and everything showing up under incubator/kafka now synced", "+1", "U    downloads.html\nU    faq.html\nUpdated to revision 1358369."], "derived": {"summary": "I copied the 0. 7.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "copied the 0.7.0 release to the dist folder and update the site - I copied the 0. 7."}, {"q": "What updates or decisions were made in the discussion?", "a": "U    downloads.html\nU    faq.html\nUpdated to revision 1358369."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-384", "title": "Fix intermittent test failures and remove unnecessary sleeps", "status": "Resolved", "priority": "Major", "reporter": "Joel Jacob Koshy", "assignee": "Neha Narkhede", "labels": [], "created": "2012-06-29T22:29:29.000+0000", "updated": "2012-07-31T23:28:30.000+0000", "description": "Seeing intermittent failures in 0.8 unit tests. Also, many sleeps can be removed (with producer acks in place) and I think MockTime isn't used in some places where it should.\n", "comments": ["This patch removes sleep statements from the unit tests. Changes include -\n\n1. All sleep statements except those related to a scheduler are removed. So 2-3 sleep statements that exercise the producer side queue expiration logic have not been removed\n2. For the Log tests, passed in a Time parameter to the Log. Kafka server already takes in an optional Time parameter that defaults to SystemTime. However, it wasn't passed around in LogManager. Fixed it so KafkaServer passes its Time\n variable to LogManager which passes it to Log.\nThis is useful in removing all sleep statements from unit tests for the log manager and logs", "Thanks for the patch. It looks good. Just 1 comment:\n\n1. AsyncProducerTest.testQueueTimeExpired(): This is an existing issue, but probably can be fixed in this patch too. It seems that we should do     producerSendThread.shutdown after EasyMock.verify(mockHandler). Shutdown always sends all remaining messages in the buffer. If we call shutdown before verify, it's not clear if the send was triggered by timeout or shutdown.", "Thanks for the review, Jun. That is a good point. I fixed that and also removed the reference to mockTime since that is not useful here.", "+1 for v2.", "You are my hero...", "+1 on v2 too.", "KAFKA-343 checkin broke some unit tests and cause others to hang. I think I might have to hold off on the checkin until that is either fixed or reverted. ", "Thanks all for the review! Committed the v2 patch."], "derived": {"summary": "Seeing intermittent failures in 0. 8 unit tests.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Fix intermittent test failures and remove unnecessary sleeps - Seeing intermittent failures in 0. 8 unit tests."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks all for the review! Committed the v2 patch."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-385", "title": "RequestPurgatory enhancements - expire/checkSatisfy issue; add jmx beans", "status": "Closed", "priority": "Major", "reporter": "Joel Jacob Koshy", "assignee": "Joel Jacob Koshy", "labels": ["bugs"], "created": "2012-06-29T22:40:23.000+0000", "updated": "2012-08-16T23:35:24.000+0000", "description": "As discussed in KAFKA-353:\n1 - There is potential for a client-side race condition in the implementations of expire and checkSatisfied. We can just synchronize on the DelayedItem.\n2 - Would be good to add jmx beans to facilitate monitoring RequestPurgatory stats.\n", "comments": ["Summary of changes and notes:\n\n1 - Fixed the synchronization issue (raised in KAFKA-353) between\n  checkSatisfied and expire by synchronizing on the DelayedItem.\n\n2 - Added request purgatory metrics using the metrics-core library. Also\n  added support for csv/ganglia/graphite reporters which I think is useful -\n  e.g., I attached a graphite dashboard that was pretty easy to whip up. It\n  should be a breeze to use metrics-core for other stats in Kafka.\n\n3 - This brings in dependencies on metrics and slf4j, both with Apache\n  compatible licenses. I don't know of any specific best-practices in using\n  metrics-core as I have not used it before, so it would be great if people\n  with experience using it glance over this patch.\n\n4 - It's a bit hard to tell right now which metrics are useful and which are\n  pointless/redundant.  We can iron that out over time.\n\n5 - Some metrics are only global and both global and per-key (which I think\n  is useful to have, e.g., to get a quick view of which partitions are\n  slower).  E.g., it helped to see (in the attached screen shots) that fetch\n  requests were all expiring - and it turned out to be a bug in how\n  DelayedFetch requests from followers are checked for satisfaction.  The\n  issue is that maybeUnblockDelayedFetch is only called if required acks is\n  0/1. We need to call it always - in the FetchRequestPurgatory\n  checkSatisfied method, if it is a follower request then we need to use\n  logendoffset to determine the available bytes to the fetch request, and HW\n  if it is a non-follower request. I fixed it to always check\n  availableFetchBytes, but it can be made a little more efficient by having\n  the DelayedFetch request keep track of currently available bytes in each\n  topic-partition key.\n\n6 - I realized that both the watchersForKey and per-key metrics pools keep\n  growing.  It may be useful to have a simple garbage collector in the Pool\n  class that garbage collects entries that are stale (e.g., due to a\n  leader-change), but this is non-critical.\n\n7 - I needed to maintain DelayedRequest metrics outside the purgatory:\n  because the purgatory itself is abstract and does not have internal\n  knowledge of delayed requests and their keys. Note that these metrics are\n  for delayed requests - i.e., these metrics are not updated for those\n  requests that are satisfied immediately without going through the\n  purgatory.\n\n8 - There is one subtlety with producer throughput: I wanted to keep per-key\n  throughput, so the metric is updated on individual key satisfaction. This\n  does not mean that the DelayedProduce itself will be satisfied - i.e,.\n  what the metric reports is an upper-bound since some DelayedProduce\n  requests may have expired.\n\n9 - I think it is better to wait for Kafka-376 to go in first. In this\n  patch, I hacked a simpler version of that patch - i.e., in\n  availableFetchBytes, I check the logEndOffset instead of the\n  high-watermark. Otherwise, follower fetch requests would see zero\n  available bytes. Of course, this hack now breaks non-follower fetch\n  requests.\n\n10 - KafkaApis is getting pretty big - I can try and move DelayedMetrics out\n  if that helps although I prefer having it inside since all the\n  DelayedRequests and purgatories are in there.\n\n11 - There may be some temporary edits to start scripts/log4j that I will\n  revert in the final patch.\n\nWhat's left to do:\n\na - This was a rather painful rebase, so I need to review in case I missed\n  something.\n\nb - Optimization described above: DelayedFetch should keep track of\n  bytesAvailable for each key and FetchRequestPurgatory's checkSatisfied\n  should take a topic, partition and compute availableBytes for just that\n  key.\n\nc - The JMX operations to start and stop the reporters are not working\n  properly. I think I understand the issue, but will fix later.", "Thanks for the patch. Overall, it seems that metrics-core is easy to use. Some comments:\n\n1. KafkaMetricsGroup.metricName: add a comment on the following statement and explain what it does\n          actualPkg.replaceFirst(\"\"\"\\.\"\"\", \".%s.\".format(ident))\n\n2. Unused imports: KafkaApis, KafkaConfig\n\n3. Pool.getAndMaybePut(): This seems to force the caller to create a new value object on each call and in most cases, the new object is not needed.\n\n4. FetchRequestKey and ProduceRequestKey: can they be shared?\n\n5. Is it better to put all metrics classes in one package metrics?\n\n6. It's useful to have topic level stats. I am wondering if we can just keep stats at the global and the topic level, but not at topic/partition level. \n\n7. DelayedProducerRequestMetrics,DelayedFetchRequestMetrics : Can we name the stats consistently? Something like FetchSatisfiedTime and ProduceStatisfiedTime,  FetchSatisfiedRequest and ProduceStatisfiedRequest. Also, there is some overlap with the stats in BrokerTopicStat.\n\n8. ProducerPerformance: Why forcing producer acks to be 2? Shouldn't that come from the command line?\n\n9. The changes and the new files in config/: Are they needed?", "Thanks for the review.\n\n1. Will do.\n\n2. Do you remember which class? I can run an optimize imports everywhere, so\n   there may be some noise in the final patch.\n\n3. Oops - yes that's true. I'll fix this to accept a factory method that\n   creates the object if absent.\n\n4. Yes - I'll combine them.\n\n5. I had considered this, but I think it is better to keep only generic    stuff in the metrics package and keep concrete metrics close to where    they are used. I think we can decide this as part of KAFKA-203 since that\n   may result in a more elaborate wrapper around metrics-core than I have\n   now in the kafka.metrics package.\n\n6/7. The issue is that produce/fetch request stats are asymmetric. E.g.,   per-key expiration stats do not make sense for DelayedFetch requests, but   they do make sense for DelayedProduce requests. Also, the caught-up fetch\n   request rps and duration stats for the producer are updated on a per-key\n   basis so that's why they are named as such - i.e., it does not exactly   equal the satisfaction time. I can add an additional stat that does track\n   satisfaction time for completely satisfied produce requests. There is\n   *some* overlap with BrokerTopicStat but as I mentioned in (7) in my first\n   comment these stats are only account for DelayedRequests. As for topic\n   level stats, throughput stats would help, but are satisfaction/expiration\n   stats (especially with multi-produce) useful since those events occur at\n   the key(partition)-level?\n\n8. Yes - will revert. This was done before Neha added that option in\n   KAFKA-350.\n\n9. Not required - I used it for testing only. Will revert.\n", "Also, re: my comment concerning the dependency on KAFKA-376: this is not strictly required - KAFKA-350 actually made a similar temporary fix to use the log end offset instead of hw.", "\nThe jira system was down around the time of my last comment and it was not\nsent to the mailing list. That further explains some of these updates.\n\n1 - Added produce satisfied stats.\n2 - Common request key for fetch/produce.\n3 - Factory method for getAndMaybePut.\n4 - Added doc describing fudged name for KafkaMetricsGroup.\n5 - Fixed the issues with the JMX operations I had earlier.\n6 - Reverted the ProducerPerformance and temporary config changes.\n\nI decided against doing the optimization for the FetchRequestPurgatory\ncheckSatisfied as I don't think it makes a significant difference, as the\navailable bytes computation is pretty much in memory.\n\nOne more comment on the usage of metrics: it is possible to combine\nrate/histogram metrics into a metrics Timer object. I chose not to do this\nbecause the code turns out a little cleaner by using a meter/histogram, but\nwill revisit later.\n", "Just want to add a comment on metrics overheads of using histograms, meters\nand gauges.\n\nMeters use an exponential weighted moving average and don't need to maintain\npast data.  Histograms are implemented using reservoir sampling and need to\nmaintain a sampling array. There is only one per-key histogram - the\n\"follower catch-up time\" histogram in DelayedProducerRequestMetrics. There\nare four more global histograms.\n\nThe sampling array is 1028 atomic longs by default. So if we have say, 500\ntopics with four partitions each, four brokers, assume that leadership is\nevenly spread out, a *lower* bound (if we ignore object overheads and the\nglobal histograms) on memory would be ~ 4MB per broker.\n\nAlso, after looking at the metrics code a little more, I think we should use\nthe exponentially decaying sampling option. By default, the histogram uses a\nuniform sample - i.e., it effectively takes a uniform sample from all the\nseen data points. Exponential decaying sampling gives more weight to the\npast five minutes of data - but it would use at least double the memory of\nuniform sampling which pushes memory usage to over 8MB.\n", "Patch v2 doesn't apply cleanly on a fresh checkout of 0.8 -\n\nnnarkhed-ld:kafka-385 nnarkhed$ patch -p0 -i ~/Projects/kafka-patches/KAFKA-385-v2.patch \npatching file core/src/main/scala/kafka/Kafka.scala\npatching file core/src/main/scala/kafka/api/FetchResponse.scala\npatching file core/src/main/scala/kafka/metrics/KafkaMetrics.scala\npatching file core/src/main/scala/kafka/metrics/KafkaMetricsConfigShared.scala\npatching file core/src/main/scala/kafka/metrics/KafkaMetricsGroup.scala\npatching file core/src/main/scala/kafka/server/KafkaApis.scala\nHunk #4 FAILED at 160.\nHunk #12 FAILED at 360.\n2 out of 24 hunks FAILED -- saving rejects to file core/src/main/scala/kafka/server/KafkaApis.scala.rej\npatching file core/src/main/scala/kafka/server/KafkaConfig.scala\nHunk #1 succeeded at 24 with fuzz 2 (offset 2 lines).\nHunk #2 succeeded at 36 with fuzz 1 (offset 2 lines).\nHunk #3 succeeded at 139 (offset 2 lines).\npatching file core/src/main/scala/kafka/server/RequestPurgatory.scala\npatching file core/src/main/scala/kafka/utils/Pool.scala\npatching file core/src/main/scala/kafka/utils/Utils.scala\nHunk #1 succeeded at 502 (offset 1 line).\npatching file core/src/test/scala/unit/kafka/integration/LogCorruptionTest.scala\npatching file core/src/test/scala/unit/kafka/integration/TopicMetadataTest.scala\npatching file core/src/test/scala/unit/kafka/server/RequestPurgatoryTest.scala\npatching file project/build/KafkaProject.scala\n", "Yes - looks like KAFKA-369 went in yesterday. I will need to rebase now.", "1. Can we make the reporters pluggable? We shouldn't hard code those, you should just give something like\n  metrics.reporters=com.xyz.MyReporter, com.xyz.YourReporter\n2. Please remove the reference to scala class names from the logging (e.g. DelayedProduce)\n3. How are you measuring the performance impact of the change you made to synchronization?\n4. It would be good to cut-and-paste the scala timer class they provide in the scala wrapper. That is really nice.\n5. Size.\nI think the overhead is the following:\n8 byte pointer to the value\n12 byte object header\n8 byte value\nTotal: 28 bytes\n\nThis is too much memory for something that should just be monitoring. I think we should not do per-key histograms.\n\n", "I'm about to upload a rebased patch so I'd like to incorporate as much of this as I can.\n\n> 1. Can we make the reporters pluggable? We shouldn't hard code those, you should just give something like \n  metrics.reporters=com.xyz.MyReporter, com.xyz.YourReporter \n\nThese (and ConsoleReporter) are the only reporters currently available. It would be good to make it pluggable, but their constructors are different. i.e., I don't think it is possible to do this without using a spring-like framework.\n\n> 2. Please remove the reference to scala class names from the logging (e.g. DelayedProduce)\n\nI don't follow this comment - can you clarify? Which file are you referring to?\n\n> 3. How are you measuring the performance impact of the change you made to synchronization?\n\nI did not - I can measure the satisfaction time with and without the synchronization but I doubt it would add much overhead. Also, we have to add synchronization one way or the other - either inside the purgatory or outside (i.e,. burden the client's usage).\n\n> 4. It would be good to cut-and-paste the scala timer class they provide in the scala wrapper. That is really nice. \n\nThey == ? Not clear on this - can you clarify?\n\n> 5. Size. \nI think the overhead is the following: \n8 byte pointer to the value \n12 byte object header \n8 byte value \nTotal: 28 bytes \n\n> This is too much memory for something that should just be monitoring. I think we should not do per-key histograms. \n\nThis is certainly a concern. So with the scenario I have in my previous comment, this would be > 13MB per broker and > double that if I use exponentially decaying sampling. That said, there is only one per-key histogram (which is the follower catch up time). OTOH since the main use I can think of is to see which followers are slower we can achieve that with grep's in the log. So I guess the visual benefit comes at a prohibitive memory cost.\n", "Changes over v2:\n- Rebased (twice!)\n- For the remaining (global) histograms switched to biased histograms.\n- Addressed Jay's comments:\n  - 1: actually there's a workaround - basically pass through the properties\n    to the custom reporter. (I provided an example\n    (KafkaCSVMetricsReporter). If JMX operations need to be exposed the\n    custom reporter will need to implement an mbean trait that extends from\n    KafkaMetricsReporterMBean. I did this to avoid having to implement the\n    DynamicMBean interface. Since we now have pluggable reporters I removed\n    the KafkaMetrics class and the dependency on metrics-ganglia and\n    metrics-graphite.\n  - 2: changed the logging statements in KafkaApis to just say producer\n    requests/fetch requests.\n  - 3: I did a quick test as described above, but couldn't see any\n    measurable impact.\n  - 4: Added KafkaTimer and a unit test (which I'm thinking of removing as\n    it is pretty dumb other than showing how to use it).\n  - 5: Got rid of the per-key follower catch up time histogram from\n    DelayedProduceMetrics.  Furthemore, meters are inexpensive and the\n    per-key caught up follower request meter should be sufficient.\n", "+1", "I forgot to update the running flag in the example reporter - will fix that before check-in.", "Jun had brought up one more problem - the factory method to getAndMaybePut doesn't really\nfix the problem of avoiding instantiation of objects if they are already present in the Pool since the\nanonymous function that I use needs to instantiate the object. I tweaked the code to use lazy vals\nand used logging to verify that each object in the Pool is instantiated only once.\n\nFrom what I understand, it seems lazy val's implementation effectively uses a synchronized bitmap\nto keep track of whether a particular val has been created or not. However, I'm not so sure how it works\nif the val involves a parameter. e.g., lazy val factory = new MyClass(param) as opposed to\nlazy val factory = new MyClass The concern is that scala may need to create some internal wrapper\nclasses (at runtime). I tried disassembling the bytecode but did not want to spend too much time on\nit - so I thought I'd ask if anyone know details of how lazy vals work when the actual instance is only\nknown at runtime?\n", "Attached an incremental patch over v3 to illustrate.", "For the issue with getAndMaybePut(), can we add an optional createValue() method to the constructor of Pool? If an object doesn't exist for a key, Pool can call createValue() to create a new value object from key.\n\nA few other comments:\n30. KafkaConfig: brokerid should be a required property. So we shouldn't put a default value.\n\n31. satisfiedRequestMeter: Currently, it doesn't include the time for expired requests. I prefer to have a stat that gives the time for all requests, whether expired or not. \n\n32. I am not sure how useful the metric CaughtUpFollowerFetchRequestsPerSecond is.", "That's a good suggestion, and should work as well. I'll make that change tomorrow. It would be good to understand the lazy val implementation as well - will try and dig into some toy examples.\n\n30. Correct - I had reverted that in the last attachment. There was a (temporary I think) reason I needed that default but I don't remember.\n\nFor 31 and 32 I think we should defer this to a later discussion. I actually had expiration time before but removed it. I'm not sure it makes a lot of sense. It would perhaps be useful to detect producers that are setting a very low expiration period, but even so it is driven by producer configs and would be a mash-up of values from different producers with expiring requests. Stats can be asymmetric between delayed-produce/delayed-fetch and also between expired/satisfied.\n", "Moved the valueFactory to Pool's constructor.\nUnit tests/system tests pass.", "+1 on patch v4.", "Thanks for the reviews. Committed to 0.8."], "derived": {"summary": "As discussed in KAFKA-353:\n1 - There is potential for a client-side race condition in the implementations of expire and checkSatisfied. We can just synchronize on the DelayedItem.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "RequestPurgatory enhancements - expire/checkSatisfy issue; add jmx beans - As discussed in KAFKA-353:\n1 - There is potential for a client-side race condition in the implementations of expire and checkSatisfied. We can just synchronize on the DelayedItem."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for the reviews. Committed to 0.8."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-386", "title": "Race condition in accessing ISR", "status": "Closed", "priority": "Major", "reporter": "Joel Jacob Koshy", "assignee": null, "labels": ["bugs"], "created": "2012-06-29T22:45:37.000+0000", "updated": "2012-08-23T16:51:00.000+0000", "description": "Also brought up in KAFKA-353 - Partition's inSyncReplicas is used by both KafkaApis and ReplicaManager; and is subject to concurrent writes. Should be able to just switch it to an AtomicReference, but need to look at the code more carefully to determine if that is sufficient.\n", "comments": ["This should be fixed as part of kafka-351. All accesses to ISR are now synchronized."], "derived": {"summary": "Also brought up in KAFKA-353 - Partition's inSyncReplicas is used by both KafkaApis and ReplicaManager; and is subject to concurrent writes. Should be able to just switch it to an AtomicReference, but need to look at the code more carefully to determine if that is sufficient.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Race condition in accessing ISR - Also brought up in KAFKA-353 - Partition's inSyncReplicas is used by both KafkaApis and ReplicaManager; and is subject to concurrent writes. Should be able to just switch it to an AtomicReference, but need to look at the code more carefully to determine if that is sufficient."}, {"q": "What updates or decisions were made in the discussion?", "a": "This should be fixed as part of kafka-351. All accesses to ISR are now synchronized."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-387", "title": "Add a centralized co-ordinator for consumer rebalancing", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "labels": [], "created": "2012-06-30T00:06:42.000+0000", "updated": "2016-04-18T09:05:33.000+0000", "description": "Proposal for the consumer co-ordinator is here - https://cwiki.apache.org/confluence/display/KAFKA/Central+Consumer+Coordination\n\n", "comments": [], "derived": {"summary": "Proposal for the consumer co-ordinator is here - https://cwiki. apache.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add a centralized co-ordinator for consumer rebalancing - Proposal for the consumer co-ordinator is here - https://cwiki. apache."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-388", "title": "Add a highly available consumer co-ordinator to a Kafka cluster", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "labels": ["patch"], "created": "2012-06-30T00:09:04.000+0000", "updated": "2015-07-21T16:08:09.000+0000", "description": "This JIRA will add a highly available co-ordinator for consumer rebalancing. Detailed design of the co-ordinator leader election and startup procedure is documented here - https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Detailed+Consumer+Coordinator+Design\n", "comments": ["Patch_v1 attached.\n\nThe coordinator keeps the following static information in cache:\n\ninterestGroup: Map[String, Set[String]] // For each topic, the set of groups that are interested in it, hence will be involved\n// in rebalancing once the partitions for that topic has changed\n\nThen on startup, it does the following:\n\n1. Set watchers for topic-broker changes (ChildChanges) on /brokers/topics/[topic], and initialize the interestGroup for each topic\n\n2. Read the consumers' intended interests for topicCounts within each topic, and as long as one consumer are interested in some topic A, put the group as A's interested group\n\n3. Set watchers for group member changes (ChildChanges) on /consumers/[group]/ids for each group\n\n4. Set watchers for group changes (ChildChanges) on /consumers (Do we really care if a whole group is gone?)\n\n5. Set StateChange watcher for itself for trying to re-elect as the coordinator\n\nIn addition, it keeps a underRebalance AtomicBoolean for each group indicating if the group is under rebalance. This is used to batch multiple rebalance request for the same group\n\n", "Is this still relevant or can we close?", "This is now handled in KAFKA-1326."], "derived": {"summary": "This JIRA will add a highly available co-ordinator for consumer rebalancing. Detailed design of the co-ordinator leader election and startup procedure is documented here - https://cwiki.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add a highly available consumer co-ordinator to a Kafka cluster - This JIRA will add a highly available co-ordinator for consumer rebalancing. Detailed design of the co-ordinator leader election and startup procedure is documented here - https://cwiki."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is now handled in KAFKA-1326."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-389", "title": "Add a consumer client that connects to the co-ordinator for rebalancing", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": null, "labels": [], "created": "2012-06-30T00:12:40.000+0000", "updated": "2016-04-18T09:06:02.000+0000", "description": "This JIRA will add a new Kafka consumer that does not depend on zookeeper for rebalancing. This consumer will eventually replace the ZookeeperConsumerConnector. On startup, the consumer connects to the current co-ordinator and listens for rebalancing requests.\n\nDetailed design is here - https://cwiki.apache.org/confluence/display/KAFKA/Central+Consumer+Coordination", "comments": [], "derived": {"summary": "This JIRA will add a new Kafka consumer that does not depend on zookeeper for rebalancing. This consumer will eventually replace the ZookeeperConsumerConnector.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add a consumer client that connects to the co-ordinator for rebalancing - This JIRA will add a new Kafka consumer that does not depend on zookeeper for rebalancing. This consumer will eventually replace the ZookeeperConsumerConnector."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-390", "title": "Implement the rebalancing protocol between co-ordinator and consumer", "status": "Resolved", "priority": "Major", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "labels": [], "created": "2012-06-30T00:13:49.000+0000", "updated": "2016-04-18T09:06:24.000+0000", "description": "This JIRA will implement the rebalancing protocol between the co-ordinator and the consumer. Detailed protocol is here - https://cwiki.apache.org/confluence/display/KAFKA/Central+Consumer+Coordination", "comments": [], "derived": {"summary": "This JIRA will implement the rebalancing protocol between the co-ordinator and the consumer. Detailed protocol is here - https://cwiki.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Implement the rebalancing protocol between co-ordinator and consumer - This JIRA will implement the rebalancing protocol between the co-ordinator and the consumer. Detailed protocol is here - https://cwiki."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-391", "title": "Producer request and response classes should use maps", "status": "Closed", "priority": "Blocker", "reporter": "Joel Jacob Koshy", "assignee": "Joel Jacob Koshy", "labels": ["optimization"], "created": "2012-07-02T18:32:38.000+0000", "updated": "2014-11-07T06:57:57.000+0000", "description": "Producer response contains two arrays of error codes and offsets - the ordering in these arrays correspond to the flattened ordering of the request arrays.\n\nIt would be better to switch to maps in the request and response as this would make the code clearer and more efficient (right now, linear scans are used in handling producer acks).\n\nWe can probably do the same in the fetch request/response.", "comments": ["Attached a draft patch. I was out of action for the last two weeks and 0.8\nhas moved along quite a bit, so this will need a significant rebase. (This\npatch should cleanly apply on r1374069.) Before I spend time rebasing I was\nhoping to get some high-level feedback on this change. I think it makes the\ncode clearer and likely more efficient (although an extensive perf test is\npending).\n\nIt is pretty straightforward and should be quick to review at least for this\nround. The main changes are as follows:\n\n- Switched to maps for the data/status in the producer request/response\n  classes.\n- I'm using SortedMap as that helps with the serializing to the wire format.\n  The maps of course are only in memory. The wire format is mostly the same\n  although I modified the response format to include topic names (similar to\n  the request format). This is not strictly required but I think it is\n  clearer this way.\n- Made some of the related code simpler/clearer (e.g., produceToLocalLog,\n  DelayedProduce.respond, etc.)\n- Minor fixes in the tests due to the above changes.\n", "One more comment I forgot to add above: we can/should probably get rid of the global\nerror code in ProducerResponse as it is unused and does not seem to make sense\nanyway in the absence of a \"generic error\" code.", "Thanks for the patch. Some comments:\n\n1. We probably shouldn't use scala sortedMap in kafka.javaapi.ProducerRequest. On that thought, why can't ProducerRequest take a regular map in the constructor? If we want some ordering on the serialized data, we can sort the map before serialization. SortedMap seems to reveal an implementation detail that clients don't (and shouldn't) really care. Ditto for ProducerResponse.\n\n2. To be consistent, should we change FetchResponse (and maybe FetchRequest) to use map, instead of array too?\n\n3. ProducerRequest.writeTo() can use foreach like the following:\n   groupedData.foreach{ case(topic, TopciAndPartitionData) => ... }\n", "(1) Yes that's a good point. I should have thought through it more carefully. We only need the sorted property once (serialization).\n(2) I had considered this, but decided to punt to see how the producer-side came out. I'll take a stab at the fetch side as well as part of this jira.\n(3) Nice.\n\nSo I'll rebase now and incorporate the above.\n", "(Accidentally deleted the description.)", "Also, I agree that we can remove the global errorcode from both the fetch and the produce responses.", "2.1 - Rebased after the first review, but I will need to rebase again. Unit\n  tests and system tests pass. This patch applies cleanly to svn revision\n  1381858 - it would be great if this can be reviewed against that revision.   I can provide an incremental patch if that helps after I rebase.\n\n2.2 - Removed all the equals methods in classes that were using Arrays earlier.\n\n2.3 - Switched fetch request/response to maps. It makes the code a little\n  cleaner, but I think the improvement was greater with the refactoring on\n  the producer side. Anyway, this makes the APIs more consistent.  One small\n  observation: previously, fetch requests would throw a\n  FetchRequestFormatException if the TopicData array for a fetch request\n  contained the same topic multiple times. Right now we don't do any checks\n  since we use a map. We can add it to the FetchRequestBuilder, but not sure\n  if it is required/worth it. Also, I think it previously would allow\n  fetches from different offsets in the same partition in the same fetch\n  request. That is no longer allowed, although I don't know why anyone would\n  need that.\n\n2.4 - Removed the global error code.\n\nAnother minor detail: I wonder if it would help to have a case class for\nTopicAndPartition. We use (topic, partition) and the associated tuple\naddressing all over the place. That would make the Map declarations slightly\nclearer, although now we're accustomed to understanding that (String, Int)\nmust mean (topic, partitionId).\n", "BTW, I forgot to remove OffsetDetail from FetchRequest - we don't need that anymore.", "Thanks for patch v2. Some comments.\n\n20. It's a good idea to create a case class of TopicPartition. This helps 2 things: (1) Tuple doesn't exist in java and javaapi.ProducerRequest currently uses tuple. (2) This avoids things like x._1._1, which is harder to understand. Similarly, should we create a case class of ProducerResponsStatus that wraps (errrocode, offset)?\n\n21. javaapi.ProduceRequest should use java map, instead of scala map. javaapi.SyncProducer should return a java version of ProducerResponse. Thinking about it. Should we even provide a javaapi for SyncProducer since everyone should really be using the high level Producer api? In other words, SyncProducer probably is not our public api for clients. For consumers, we likely still need to provide a java version of SimpleConsumer since there may be applications that want to control the fetch offset. \n\n23. It doesn't look like that we have a java version of FetchRequest. We should add that  and use it in the java version of SimpleConsumer.\n\n24. FetchResponse: We probably should add a helper method errorCode(topic, partition)?\n\n25. AbstractFetchThread: can use the pattern response.data.foreach{ case(key, partitionData) => }\n\n26. KafkaApis.handleFetchRequest(): can use the pattern in #25 too.\n\n", "Marking as blocker since I ended up changing the wire format.", "Overview of changes in v3:\n\n(3.1) - (20) - This actually caused bulk of the changes in v3. I did this \nfor just the topic-partition pairs in producer/consumer request handling;\nsame for producer response status. There are a lot more places where we\ncould move from tuples to case classes. I think (as mentioned on the mailing\nlist) it would be good to do this as part of cleanup but we should defer\nthat for later since such changes cut across a lot of files. Going forward I\nthink this brings out a convention that we might want to follow. The \"scala\nbook\" has a reasonable guideline. I'll send out an email to kafka-dev for\ndiscussion and add it to our coding convention depending on how that pans\nout.\n\n(3.2) (21)-(23) - Thanks for catching the javaapi issue. Couple of changes\nhere: \na - Added javaapi for FetchRequest. (I needed to provide both java/non-java\n  fetchrequest to the simpleconsumer since FetchBuilder returns a scala\n  FetchRequest.) \nb - Java map for all the Fetch/Produce request/response in the javaapi.\nc - Removed SyncProducer: agreed that is unnecessary since Producer supports\n  both sync and async; made the hadoop producer code use the high level\n  producer. I think that's safe - i.e., I don't see a good reason why anyone\n  would \"depend\" on the data going to a single broker.\nd - Got rid of the unreferenced ProducerConsumerTestHarness in javaapi.\ne - Fixed the equals method in javaapi ProducerRequest; added one to\nFetchResponse - actually we can abstract this out into a trait, but that is\na minor improvement that I punted on.\nf - Made the ProducerRequest use Java map in javaapi.\ng - (I did not add Java versions of ProducerResponse since the SyncProducer\n  has been removed.)\n\n(3.3) (24) - added the helper method, although I don't think I'm using it\nanywhere.\n\n(3.4) - Got rid of OffsetDetail.\n\nFor (25)(26) - I tried to use the pattern wherever I could, but may have \nmissed a few.\n\nI did not rebase this (i.e., it still applies on r1381858). I'll rebase \nafter we are done with reviews.\n", "V3 looks good overall. Some additional comments:\n\n30. remove javaapi.ProducerRequest\n31. We probably should call TopicPartition TopicAndPartition.\n32. javaapi.SimpleConsumer: It's bit confusing to the scala version of send here. Let's at least explain in the comment how to use it.\n", "30 - Why should it be removed?", "Ok nm - I see it's because we removed SyncProducer and only need ProducerData. Ok - so I'll rebase now and upload the final patch in a bit.", "Here is the rebased patch. I also had to include a small edit to ReplicaFetcherThread to address the issue in KAFKA-517 which affects our system tests.", "Thanks for patch v4. +1 Could you fix the following minor issues before checking in?\n\n41. scala version of FetchResponse: We throw an exception in errorCode if the map key doesn't exist. To be consistent, we should do the same for messageSet and highWatermark.\n\n42. PartitionStatus: Since this is a case class, there is no need to define requiredOffset as val.\n\n43. DefaultEventHandler.serialize(): This is not introduced in this patch, but could you change  error(\"Error serializing message \" + t) to error(\"Error serializing message \", t)\n\n\n\n\n", "Thanks for the review. Checked-in to 0.8 after addressing the minor issues.\n", "Why this fix add this line code?\nSometimes got 2 responses for one request,   Why fall into this situation, will there be duplicate data in Kafka?\n\nhttps://git-wip-us.apache.org/repos/asf?p=kafka.git;a=commitdiff;h=b688c3ba045df340bc32caa40ba1909eddbcbec5 \n+        if (response.status.size != producerRequest.data.size)+          throw new KafkaException(\"Incomplete response (%s) for producer request (%s)\"+                                           .format(response, producerRequest))\n\n", "The client should always receive one response per request. Are you see otherwise?", "Yes,  after add more information to the error message ,  sometimes see two response for one request.", "Which version of Kafka are you using? Is that easily reproducible?", "You mean two responses to the caller of send? I don't see why those two lines would cause two responses. Can you explain further and provide steps to reproduce if there really is an issue? Do you see errors/warns in the logs?", "Not that two line cause two responses.\nWe just hit that two line while run.\nAnd after change that line to output the value of response.status.size and  producerRequest.data.size ,   we found that sometimes  response.status.size=2 and producerRequest.data.size =1.\n\nWant to ask initially why add this line?   when will response.status.size != producerRequest.data.size happen?", "Yeah I see your point. That's interesting - I actually don't remember why that was added but it appears there must have been a legitimate reason (since you ran into it:)  ).\n\nSince you are able to reproduce it can you actually print the full original request and response itself? It should be in the exception that is thrown.\n\nAlso, what is your broker Kafka version? Also, what is the version of the producer? Is it the same?\n", "This situation happen under below scenario:\none broker is leader for several partitions, for example 3,   when send one messageset which has message for all of the 3 partitions of this broker ,      the response.status.size is 3 and  the producerRequest.data.size is 1.    then it hit this exception.   Any idea for fix?  Do we need compare response.status.size  with messagesPerTopic.Count instead of producerRequest.data.size ?\n\n\n  private def send(brokerId: Int, messagesPerTopic: collection.mutable.Map[TopicAndPartition, ByteBufferMessageSet]) = {\n    if(brokerId < 0) {\n      warn(\"Failed to send data since partitions %s don't have a leader\".format(messagesPerTopic.map(_._1).mkString(\",\")))\n      messagesPerTopic.keys.toSeq\n    } else if(messagesPerTopic.size > 0) {\n      val currentCorrelationId = correlationId.getAndIncrement\n      val producerRequest = new ProducerRequest(currentCorrelationId, config.clientId, config.requestRequiredAcks,\n        config.requestTimeoutMs, messagesPerTopic)\n      var failedTopicPartitions = Seq.empty[TopicAndPartition]\n      try {\n        val syncProducer = producerPool.getProducer(brokerId)\n        debug(\"Producer sending messages with correlation id %d for topics %s to broker %d on %s:%d\"\n          .format(currentCorrelationId, messagesPerTopic.keySet.mkString(\",\"), brokerId, syncProducer.config.host, syncProducer.config.port))\n        val response = syncProducer.send(producerRequest)\n        debug(\"Producer sent messages with correlation id %d for topics %s to broker %d on %s:%d\"\n          .format(currentCorrelationId, messagesPerTopic.keySet.mkString(\",\"), brokerId, syncProducer.config.host, syncProducer.config.port))\n        if(response != null) {\n          if (response.status.size != producerRequest.data.size)\n            throw new KafkaException(\"Incomplete response (%s) for producer request (%s)\".format(response, producerRequest))\n\n", "If there are three partitions, then there will be three message-sets. i.e., producerRequest.data.size will be three, not one. Can you give example _application_ code that reproduces the issue that you are seeing?", "Many thanks for you help.\nAfter debugging and testing, seemly I can't hit that exception.\nActually we're using one c# version client which is inherit from https://github.com/precog/kafka/tree/master/clients/csharp/src/Kafka/Kafka.Client , and after debug and compare it's code with java version, finally prove that it's the bug of the C# code.\nIn java version, when create ProducerRequest, it set produceRequest.data as messagesPerTopic,  and do group by topic just before send binary.\nBut in our c# version,  it group it first and set the produceRequest.data as dictionary of <Topic,Data>, so we hit this exception wrongly, we fixed it.\n\nMany thanks for your time.\nBut anyway, can't find our related open source version from internet, our version has DefaultCallbackHandler.cs, but the version on https://github.com/precog/kafka/tree/master/clients/csharp/src/Kafka/Kafka.Client has no, so can't provide the link here.\n\nThe java link:\nhttps://git-wip-us.apache.org/repos/asf?p=kafka.git;a=blob;f=core/src/main/scala/kafka/api/ProducerRequest.scala;h=570b2da1d865086f9830aa919a49063abbbe574d;hb=HEAD\nhttps://git-wip-us.apache.org/repos/asf?p=kafka.git;a=blob;f=core/src/main/scala/kafka/producer/async/DefaultEventHandler.scala;h=821901e4f434dfd9eec6eceabfc2e1e65507a57c;hb=HEAD#l260\n"], "derived": {"summary": "Producer response contains two arrays of error codes and offsets - the ordering in these arrays correspond to the flattened ordering of the request arrays. It would be better to switch to maps in the request and response as this would make the code clearer and more efficient (right now, linear scans are used in handling producer acks).", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Producer request and response classes should use maps - Producer response contains two arrays of error codes and offsets - the ordering in these arrays correspond to the flattened ordering of the request arrays. It would be better to switch to maps in the request and response as this would make the code clearer and more efficient (right now, linear scans are used in handling producer acks)."}, {"q": "What updates or decisions were made in the discussion?", "a": "Many thanks for you help.\nAfter debugging and testing, seemly I can't hit that exception.\nActually we're using one c# version client which is inherit from https://github.com/precog/kafka/tree/master/clients/csharp/src/Kafka/Kafka.Client , and after debug and compare it's code with java version, finally prove that it's the bug of the C# code.\nIn java version, when create ProducerRequest, it set produceRequest.data as messagesPerTopic,  and do group by topic just before send binary.\nBut in our c# version,  it group it first and set the produceRequest.data as dictionary of <Topic,Data>, so we hit this exception wrongly, we fixed it.\n\nMany thanks for your time.\nBut anyway, can't find our related open source version from internet, our version has DefaultCallbackHandler.cs, but the version on https://github.com/precog/kafka/tree/master/clients/csharp/src/Kafka/Kafka.Client has no, so can't provide the link here.\n\nThe java link:\nhttps://git-wip-us.apache.org/repos/asf?p=kafka.git;a=blob;f=core/src/main/scala/kafka/api/ProducerRequest.scala;h=570b2da1d865086f9830aa919a49063abbbe574d;hb=HEAD\nhttps://git-wip-us.apache.org/repos/asf?p=kafka.git;a=blob;f=core/src/main/scala/kafka/producer/async/DefaultEventHandler.scala;h=821901e4f434dfd9eec6eceabfc2e1e65507a57c;hb=HEAD#l260"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-392", "title": "Refactor and optimize system tests", "status": "Resolved", "priority": "Minor", "reporter": "Joel Jacob Koshy", "assignee": null, "labels": ["replication-testing"], "created": "2012-07-03T21:52:00.000+0000", "updated": "2015-12-18T17:18:20.000+0000", "description": "It would be helpful to refactor the system tests to make them as concise and\nclear as possible, and get them to complete faster. Obviously this is\nnon-urgent but should ideally be done prior to adding new system tests if\nany.\n\n1 - Several functions (e.g., wait_for_zero_consumer_lag, start_cluster,\n  etc.) are all util methods that are relevant across system tests. Also,\n  these util functions can be switched to getopt (preferrably with long\n  options) which will make the scripts more readable. Anything that is not\n  super-specific to any test should be moved to the global util script.\n2 - Might help to have all test output and logs sent to a directory in tmp\n  (use mktemp) so everything is contained there - right now some files are\n  local and some are in tmp. Also, clean up would simply be rm -rf the temp\n  directory instead of handling multiple locations.\n3 - We should get rid of as many sleeps as possible. E.g., with producer\n  acks I don't think we need to sleep after producer-performance. Likewise\n  for all other sleeps, let us see if they are really needed and provide\n  tooling (if necessary) to eliminate/reduce them.\n4 - User-variable test parameters should be command-line options (getopt) to\n  the script.\n", "comments": ["Probably handled by other jiras in 0.8 but will keep open for now.", "This is for the old system tests, which have been removed. Also, most of these issues (common utilities, log collection to a single directory, get rid of sleeps) are already addressed by the new ducktape system tests."], "derived": {"summary": "It would be helpful to refactor the system tests to make them as concise and\nclear as possible, and get them to complete faster. Obviously this is\nnon-urgent but should ideally be done prior to adding new system tests if\nany.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "Refactor and optimize system tests - It would be helpful to refactor the system tests to make them as concise and\nclear as possible, and get them to complete faster. Obviously this is\nnon-urgent but should ideally be done prior to adding new system tests if\nany."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is for the old system tests, which have been removed. Also, most of these issues (common utilities, log collection to a single directory, get rid of sleeps) are already addressed by the new ducktape system tests."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-393", "title": "Add constructor for message which takes both byte array offset and length", "status": "Closed", "priority": "Major", "reporter": "graham sanderson", "assignee": null, "labels": [], "created": "2012-07-04T21:29:11.000+0000", "updated": "2012-08-28T18:43:23.000+0000", "description": "This would avoid an extra copying step for clients that (for whatever reason) have multiple messages in a byte[] to push into kafka.", "comments": ["Since this is not very critical, we probably should only fix it in 0.8.", "Attached patch which fixes this issue", "Thanks for the patch. Committed to 0.8."], "derived": {"summary": "This would avoid an extra copying step for clients that (for whatever reason) have multiple messages in a byte[] to push into kafka.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add constructor for message which takes both byte array offset and length - This would avoid an extra copying step for clients that (for whatever reason) have multiple messages in a byte[] to push into kafka."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for the patch. Committed to 0.8."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-394", "title": "update site with steps and notes for doing a release under developer", "status": "Resolved", "priority": "Major", "reporter": "Joe Stein", "assignee": "Joe Stein", "labels": [], "created": "2012-07-06T19:42:12.000+0000", "updated": "2013-11-25T18:35:53.000+0000", "description": "steps in release process including updating the dist directory", "comments": ["https://cwiki.apache.org/confluence/display/KAFKA/Release+Process"], "derived": {"summary": "steps in release process including updating the dist directory.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "update site with steps and notes for doing a release under developer - steps in release process including updating the dist directory."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://cwiki.apache.org/confluence/display/KAFKA/Release+Process"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-395", "title": "kafka.tools.MirrorMaker black/white list improvement", "status": "Resolved", "priority": "Minor", "reporter": "Dave DeMaagd", "assignee": null, "labels": [], "created": "2012-07-09T19:52:55.000+0000", "updated": "2018-10-25T13:14:34.000+0000", "description": "Current black/white list topics are specified directly on the command line, while functional, this has two drawbacks:\n1) Changes become unwieldy if there are a large number of running instances - potentially many instances to restart, which can have implications for data stream lag\n2) Maintaining the list itself can become increasingly complex if there are a large number of elements in the list (particularly if they are complex expressions)\n\nSuggest extending the way that black/white lists can be fed to the mirror maker application, in particular, being able to specify the black/white list as a file (or possibly a generic URI).  Thinking that this could be accomplished either by adding '--whitelistfile' and '--blacklistfile' command line parameters, or modifying the existing '--blacklist' and '--whitelist' parameters to include a 'is this a valid file?' test and decide how to handle it based on that (if it is a file, read it, if not, use current behavior). \n\nFollow up suggestion would be to have the mirror maker process check for updates to the list file, and on change, validate and reload it, and run from that point with the new list information. ", "comments": ["As this issue has not been touched in more than 6 years I think it is fairly safe to assume that we can close this.\r\n\r\nAny discussionsÂ around mirroring functionality are betterÂ addressed in the MirrorMaker 2.0 KIP discussion.\r\n\r\nRegarding the specifics of this issue, that can be worked around by placing whitelists topics into a file and the paste-ing that file into the command as shown below. I believe that this should be sufficient as a workaround.\r\n{code:bash}\r\n./kafka-mirror-maker.sh --consumer.config ../config/consumer.properties --producer.config ../config/producer.properties --whitelist \"$(paste whitelist.topics -d'|' -s)\" --blacklist \"$(paste blacklist.topics -d'|' -s)\"\r\n{code}"], "derived": {"summary": "Current black/white list topics are specified directly on the command line, while functional, this has two drawbacks:\n1) Changes become unwieldy if there are a large number of running instances - potentially many instances to restart, which can have implications for data stream lag\n2) Maintaining the list itself can become increasingly complex if there are a large number of elements in the list (particularly if they are complex expressions)\n\nSuggest extending the way that black/white lists can be fed to the mirror maker application, in particular, being able to specify the black/white list as a file (or possibly a generic URI). Thinking that this could be accomplished either by adding '--whitelistfile' and '--blacklistfile' command line parameters, or modifying the existing '--blacklist' and '--whitelist' parameters to include a 'is this a valid file?' test and decide how to handle it based on that (if it is a file, read it, if not, use current behavior).", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "kafka.tools.MirrorMaker black/white list improvement - Current black/white list topics are specified directly on the command line, while functional, this has two drawbacks:\n1) Changes become unwieldy if there are a large number of running instances - potentially many instances to restart, which can have implications for data stream lag\n2) Maintaining the list itself can become increasingly complex if there are a large number of elements in the list (particularly if they are complex expressions)\n\nSuggest extending the way that black/white lists can be fed to the mirror maker application, in particular, being able to specify the black/white list as a file (or possibly a generic URI). Thinking that this could be accomplished either by adding '--whitelistfile' and '--blacklistfile' command line parameters, or modifying the existing '--blacklist' and '--whitelist' parameters to include a 'is this a valid file?' test and decide how to handle it based on that (if it is a file, read it, if not, use current behavior)."}, {"q": "What updates or decisions were made in the discussion?", "a": "As this issue has not been touched in more than 6 years I think it is fairly safe to assume that we can close this.\r\n\r\nAny discussionsÂ around mirroring functionality are betterÂ addressed in the MirrorMaker 2.0 KIP discussion.\r\n\r\nRegarding the specifics of this issue, that can be worked around by placing whitelists topics into a file and the paste-ing that file into the command as shown below. I believe that this should be sufficient as a workaround.\r\n{code:bash}\r\n./kafka-mirror-maker.sh --consumer.config ../config/consumer.properties --producer.config ../config/producer.properties --whitelist \"$(paste whitelist.topics -d'|' -s)\" --blacklist \"$(paste blacklist.topics -d'|' -s)\"\r\n{code}"}]}}
{"project": "KAFKA", "issue_id": "KAFKA-396", "title": "Mirroring system test fails on 0.8", "status": "Resolved", "priority": "Major", "reporter": "Joel Jacob Koshy", "assignee": "Joel Jacob Koshy", "labels": ["replication-testing"], "created": "2012-07-09T22:58:27.000+0000", "updated": "2013-06-14T03:59:47.000+0000", "description": "Just making a note of this - will look into this later.", "comments": ["I think this no longer happens, may be closed", "Did you check? It may be fixed but needs to be verified - the script is obsolete due to changes in the arguments to tools such as producer performance.", "This is fixed in 0.8."], "derived": {"summary": "Just making a note of this - will look into this later.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Mirroring system test fails on 0.8 - Just making a note of this - will look into this later."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is fixed in 0.8."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-397", "title": "kafka.common.InvalidMessageSizeException: null", "status": "Resolved", "priority": "Blocker", "reporter": "David Siegel", "assignee": null, "labels": [], "created": "2012-07-10T17:21:39.000+0000", "updated": "2013-07-12T14:29:47.000+0000", "description": "I've just gotten the following error while running the zookeeper consumer.  \n\nI made a backup of the kafka log directory and wiped the logs.  I restarting kafka and the consumer.  After processing a few hundred messages successfully I got the same error again.  I restarted the consumer again and got the same error immediately.  I wiped the logs yet again and reproduced the error again.  I will attach the logs from this final run.\n\nI'm running Kafka 0.7.1\n\nI have the following message size configurations:\n  producer config:\n    max.message.size: 1000000\n  consumer config:\n    fetch.size: 2072000\n\nDoes it matter that I'm only wiping the logs and not wiping the zookeeper offsets?\n\n2012-07-10 02:31:21,998 ERROR [Consumer1] c.k.h.c.k.KafkaConsumerServiceWorker: Failed to get next student event\nkafka.common.InvalidMessageSizeException: null\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[na:1.6.0_30]\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39) ~[na:1.6.0_30]\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27) ~[na:1.6.0_30]\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:513) ~[na:1.6.0_30]\n        at java.lang.Class.newInstance0(Class.java:355) ~[na:1.6.0_30]\n        at java.lang.Class.newInstance(Class.java:308) ~[na:1.6.0_30]\n        at kafka.common.ErrorMapping$.maybeThrowException(ErrorMapping.scala:53) ~[KPIP-0.4.birdy.jar:na]\n        at kafka.message.ByteBufferMessageSet.kafka$message$ByteBufferMessageSet$$internalIterator(ByteBufferMessageSet.scala:99) ~[KPIP-0.4.birdy.jar:na]\n        at kafka.message.ByteBufferMessageSet.iterator(ByteBufferMessageSet.scala:82) ~[KPIP-0.4.birdy.jar:na]\n        at kafka.consumer.ConsumerIterator.makeNext(ConsumerIterator.scala:81) ~[KPIP-0.4.birdy.jar:na]\n        at kafka.consumer.ConsumerIterator.makeNext(ConsumerIterator.scala:32) ~[KPIP-0.4.birdy.jar:na]\n        at kafka.utils.IteratorTemplate.maybeComputeNext(IteratorTemplate.scala:59) ~[KPIP-0.4.birdy.jar:na]\n        at kafka.utils.IteratorTemplate.hasNext(IteratorTemplate.scala:51) ~[KPIP-0.4.birdy.jar:na]\n        at kafka.utils.IteratorTemplate.next(IteratorTemplate.scala:36) ~[KPIP-0.4.birdy.jar:na]\n        at kafka.consumer.ConsumerIterator.next(ConsumerIterator.scala:43) ~[KPIP-0.4.birdy.jar:na]\n        at java.lang.Thread.run(Thread.java:662) [na:1.6.0_30]\n2012-07-10 02:31:21,998 ERROR [Consumer1] c.k.h.c.k.KafkaConsumerServiceWorker: Iterator got into bad state.  Thread exiting\njava.lang.IllegalStateException: Iterator is in failed state\n        at kafka.utils.IteratorTemplate.hasNext(IteratorTemplate.scala:47) ~[KPIP-0.4.birdy.jar:na]\n        at kafka.utils.IteratorTemplate.next(IteratorTemplate.scala:36) ~[KPIP-0.4.birdy.jar:na]\n        at kafka.consumer.ConsumerIterator.next(ConsumerIterator.scala:43) ~[KPIP-0.4.birdy.jar:na]\n        at java.lang.Thread.run(Thread.java:662) [na:1.6.0_30]\n", "comments": ["When we set the number of partitions per topic to 100 and set the consumer to use 100 threads, we don't see this error. \n\nWhen we set the number of partitions per topic to 100 and set the consumers to use 2 threads, we see the error consistently.", "Yes, if you wipe off the log. You need to clean up offsets in ZK too. \n\nCould you also check if the broker has any error log with the following text?\n\"error when processing request \"", "2012-07-10T18:07:27.27635 [2012-07-10 18:07:27,275] ERROR error when processing request FetchRequest(topic:tmp, part:0 offset:2411 maxSize:2072000) (kafka.server\n.KafkaRequestHandlers)\n2012-07-10T18:07:27.27638 kafka.common.OffsetOutOfRangeException: offset 2411 is out of range\n2012-07-10T18:07:27.27639       at kafka.log.Log$.findRange(Log.scala:46)\n2012-07-10T18:07:27.27640       at kafka.log.Log.read(Log.scala:247)\n2012-07-10T18:07:27.27640       at kafka.server.KafkaRequestHandlers.kafka$server$KafkaRequestHandlers$$readMessageSet(KafkaRequestHandlers.scala:108)\n2012-07-10T18:07:27.27641       at kafka.server.KafkaRequestHandlers$$anonfun$2.apply(KafkaRequestHandlers.scala:97)\n2012-07-10T18:07:27.27641       at kafka.server.KafkaRequestHandlers$$anonfun$2.apply(KafkaRequestHandlers.scala:96)\n2012-07-10T18:07:27.27642       at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)\n2012-07-10T18:07:27.27642       at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)\n2012-07-10T18:07:27.27643       at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)\n2012-07-10T18:07:27.27644       at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:34)\n2012-07-10T18:07:27.27644       at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)\n2012-07-10T18:07:27.27645       at scala.collection.mutable.ArrayOps.map(ArrayOps.scala:34)\n2012-07-10T18:07:27.27646       at kafka.server.KafkaRequestHandlers.handleMultiFetchRequest(KafkaRequestHandlers.scala:96)\n2012-07-10T18:07:27.27647       at kafka.server.KafkaRequestHandlers$$anonfun$handlerFor$3.apply(KafkaRequestHandlers.scala:40)\n2012-07-10T18:07:27.27648       at kafka.server.KafkaRequestHandlers$$anonfun$handlerFor$3.apply(KafkaRequestHandlers.scala:40)\n2012-07-10T18:07:27.27648       at kafka.network.Processor.handle(SocketServer.scala:296)\n2012-07-10T18:07:27.27649       at kafka.network.Processor.read(SocketServer.scala:319)\n2012-07-10T18:07:27.27649       at kafka.network.Processor.run(SocketServer.scala:214)\n2012-07-10T18:07:27.27650       at java.lang.Thread.run(Thread.java:662)\n", "Try cleaning out ZooKeeper as well as the Kafka logs (as Jun suggested). Based on your exception, I'd guess that your consumer is using the last offset stored in ZooKeeper which exceeds the current size of your logs. I have seen exceptions like this when screwing around with the log files.\n", "So is there anything we should be fixing here?", "I'm pretty sure this was my fault not clearing zookeeper properly."], "derived": {"summary": "I've just gotten the following error while running the zookeeper consumer. I made a backup of the kafka log directory and wiped the logs.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "kafka.common.InvalidMessageSizeException: null - I've just gotten the following error while running the zookeeper consumer. I made a backup of the kafka log directory and wiped the logs."}, {"q": "What updates or decisions were made in the discussion?", "a": "I'm pretty sure this was my fault not clearing zookeeper properly."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-398", "title": "Enhance SocketServer to Enable Sending Requests", "status": "Resolved", "priority": "Major", "reporter": "Guozhang Wang", "assignee": null, "labels": ["features"], "created": "2012-07-10T19:56:54.000+0000", "updated": "2015-02-07T23:41:52.000+0000", "description": "Currently the SocketServer is only used for reactively receiving requests and send responses but not used for pro-actively send requests and receive responses. Hence it does not need to remember which channel/key correspond to which consumer.\n\nOn the other hand, there are cases such as consumer coordinator that needs SocketServer to send requests and receive responses to the consumers.\n\nIt would be nice to add this functionality such that an API can be called with the id string and the request message, and the SocketServer will figure out which channel to use and write that message to the key's attachment and set the flag as WRITABLE so that the processor can then send it.", "comments": ["In 0.7 SocketServer, the handler can only take the readin buffer as the parameter. However, to handle RegisterConsumer request, the coordinator needs to remember the map of the consumer id string to processor id, and then to the specific key. In 0.8 SocketServer handlers can take the request object which contains the key as the metadata, which makes recording the key possible.\n\nCurrently I hacked the SocketServer so that at the handleRegisterConsumerRequest procedure, it records the (consumerId, processorId) in the coordinator, and each processor keeps the (consumerId, key) itself. Then when the coordinator needs to send a request to certain consumer, it first finds the processor which maintains that channel from its map, sends its request to the processor, and then the processor will finds its corresponding key from its own map, and then atttach the request and set the flag WRITABLE.", "This patch is based on 0.7, which is from my current implementation. Currently I work around this issue by:\n\n1. Keeping a [consumerId, socket-server-processor-id, key] on the coordinator, which will be updated for handling the consumer registering request.\n\n2. Add one more handlers class for coordinator requests, which require key and processor id in addition to the byte buffer\n\n3. One thing that is still missing is an additional function for processors, which take the message and the key as the input, and try to send it by first clear the key's attachment, put the message as its new attachment, and then mark the key opt as WRITABLE.", "Added the API function of SocketServer: prepareWrite", "Hey Guozhang, I think we can't take these socket server changes as is. The idea behind the network/ sub-package is that it should be stand-alone and have no knowledge that it is part of kafka. So it is not appropriate to special case individual request types there. That is fine though, I think this is a side point for the code you are writing. I recommend you just hack away, when it comes time to port it over to 0.8 I will gladly do the refactoring to the socket server to do this in a clean way. If you get stuck for more than 10 mins on anything grab me and I will help you hack it further.\n\nI think the following things probably need to change in 0.8:\n1. RequestChannel probably needs some renaming as the request/response terminology no longer makes sense.\n2. The run() loop in SocketServer.Processor needs a fix for the read/write logic. Currently we have:\n            if(key.isReadable)\n              read(key)\n            else if(key.isWritable)\n              write(key)\nBut now a given socket may be both readable and writable for different requests. To make this work we will need to tweak the way we handle in-flight partitially written requests. We assume there can be only one partitial request for any given socket at one time, but now there could be two--one inbound and one outbound. These are very simple changes but need some care and thought.\n3. We need to always be ready for reading even if we are currently writing.\n\nIncidently I think this is the same fix that would be required to allow the client to multi-plex requests over a single connection which is a valuable feature too.\n", "Hi Jay, thanks for the comments. One thing I do not quite understand about \"it is not appropriate to special case individual request types\": currently the SocketServer just receive a Request object, and attached that to the corresponding key. It still knows nothing about the Request type, hence any knowledge about Kafka. All it knows is to \"go prepare to send a request\", which I think should be fine for a general socket server?", "I meant this:\n\n+    // Here we have to hack the socket server a little bit since the request\n+    // does not have the key information for the coordinator to store the map\n+    //\n+    // In 0.8 socket server this is unnecessary since the key info is included\n+    // in the request\n+    var start: Long = -1\n+    var maybeSend: Option[Send] = null\n+    if (RequestKeys.isCoordinatorRequest(requestTypeId)) {\n+      val handler = handlerMapping.coordinatorHandleFor(requestTypeId, request)\n+      if(handler == null)\n+        throw new InvalidRequestException(\"No handler found for request\")\n+      start = time.nanoseconds\n+      maybeSend = handler(request, this, key)\n+    }\n+    else {\n+      val handler = handlerMapping.handlerFor(requestTypeId, request)\n+      if(handler == null)\n+        throw new InvalidRequestException(\"No handler found for request\")\n+      start = time.nanoseconds\n+      maybeSend = handler(request)\n+    }\n+", "Oh I see. Yeah, I totally agree with you."], "derived": {"summary": "Currently the SocketServer is only used for reactively receiving requests and send responses but not used for pro-actively send requests and receive responses. Hence it does not need to remember which channel/key correspond to which consumer.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Enhance SocketServer to Enable Sending Requests - Currently the SocketServer is only used for reactively receiving requests and send responses but not used for pro-actively send requests and receive responses. Hence it does not need to remember which channel/key correspond to which consumer."}, {"q": "What updates or decisions were made in the discussion?", "a": "Oh I see. Yeah, I totally agree with you."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-399", "title": "0.7.1 seems to show less performance than 0.7.0", "status": "Resolved", "priority": "Minor", "reporter": "Dongmin Yu", "assignee": null, "labels": [], "created": "2012-07-11T00:55:31.000+0000", "updated": "2013-07-11T22:26:31.000+0000", "description": "On a test, 0.7.1 seems to show less performance than 0.7.0.\n\nProducer is on a machine A, and Broker is on machine B.\n\nMachine's spec is\n- 8 core Intel(R) Xeon(R) CPU           E5405  @ 2.00GHz\n- 16G RAM\n\nBroker's configuration is\n- num.threads=8\n- socket.send.buffer=1048576\n- socket.receive.buffer=1048576\n- max.socket.request.bytes=104857600\n- log.flush.interval=10000\n- log.default.flush.interval.ms=1000\n- log.default.flush.scheduler.interval.ms=1000\n- log.file.size=536870912\n- enable.zookeeper=true\n\nAdditional note\n- no compression\n\n\nAttached dstat result and used producer code", "comments": ["0.8 is different enough that I think it is safe to mark this \"won't fix\"."], "derived": {"summary": "On a test, 0. 7.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "0.7.1 seems to show less performance than 0.7.0 - On a test, 0. 7."}, {"q": "What updates or decisions were made in the discussion?", "a": "0.8 is different enough that I think it is safe to mark this \"won't fix\"."}]}}
{"project": "KAFKA", "issue_id": "KAFKA-400", "title": "kafka-producer-shell.sh throws exceptions trying to send data.", "status": "Resolved", "priority": "Major", "reporter": "Edward Capriolo", "assignee": null, "labels": [], "created": "2012-07-11T18:29:43.000+0000", "updated": "2012-07-21T12:03:15.000+0000", "description": "The producer and consumer shell are very useful for testing. The producer shell is broken and I end up doing all testing with kafka-producer-perf-test.sh which is less then idea.\n{noformat}\n[edward@ec kafka-mirror]$ bin/kafka-producer-shell.sh --props config/consumer2.properties --topic topic_in_two\n[2012-07-11 14:27:11,926] INFO Creating sync producer for broker id = 0 at 127.0.0.2:9092 (kafka.producer.ProducerPool)\nyo\nException in thread \"main\" java.lang.ClassCastException: java.lang.String cannot be cast to kafka.message.Message\n\tat kafka.serializer.DefaultEncoder.toMessage(Encoder.scala:26)\n\tat kafka.producer.ProducerPool$$anonfun$send$1$$anonfun$3$$anonfun$apply$1.apply(ProducerPool.scala:107)\n\tat kafka.producer.ProducerPool$$anonfun$send$1$$anonfun$3$$anonfun$apply$1.apply(ProducerPool.scala:107)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)\n\tat scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)\n\tat scala.collection.immutable.List.foreach(List.scala:45)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:206)\n\tat scala.collection.immutable.List.map(List.scala:45)\n\tat kafka.producer.ProducerPool$$anonfun$send$1$$anonfun$3.apply(ProducerPool.scala:107)\n\tat kafka.producer.ProducerPool$$anonfun$send$1$$anonfun$3.apply(ProducerPool.scala:105)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:206)\n\tat scala.collection.mutable.ArrayBuffer.map(ArrayBuffer.scala:43)\n\tat kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:105)\n\tat kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)\n\tat kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)\n\tat kafka.producer.ProducerPool.send(ProducerPool.scala:100)\n\tat kafka.producer.Producer.zkSend(Producer.scala:137)\n\tat kafka.producer.Producer.send(Producer.scala:99)\n\tat kafka.tools.ProducerShell$.main(ProducerShell.scala:65)\n\tat kafka.tools.ProducerShell.main(ProducerShell.scala)\n{noformat}", "comments": ["hey Ed, try\n\nbin/kafka-producer-shell.sh --props config/producer.properties --topic topic_in_two \n\nthe consumer.property file is missing the serialize.class property but if you use the producer.properties you should be good to go", "You are correct wrong parameters"], "derived": {"summary": "The producer and consumer shell are very useful for testing. The producer shell is broken and I end up doing all testing with kafka-producer-perf-test.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "kafka-producer-shell.sh throws exceptions trying to send data. - The producer and consumer shell are very useful for testing. The producer shell is broken and I end up doing all testing with kafka-producer-perf-test."}, {"q": "What updates or decisions were made in the discussion?", "a": "You are correct wrong parameters"}]}}
