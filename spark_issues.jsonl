{"project": "SPARK", "issue_id": "SPARK-515", "title": "Make it easier to run external code on a Nexus cluster", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0010-04-03T23:04:00.000+0000", "updated": "2020-09-14T19:26:26.000+0000", "description": "Right now, one must modify the Makefile and run script to point to external directories, which is ugly. This will get easier when a Nexus features is added to make it possible to pass the classpath to the Spark executor through environment variables (https://github.com/nexusproject/nexus/issues/18).", "comments": ["Github comment from tjhunter: I have a (partial) implementation of this feature. Please consider reviewing for future merging.\n\nhttps://github.com/tjhunter/spark/commit/8587497497dc825e0d0a312ff156f1853348aebf", "Github comment from mateiz: Closing this now that it is possible to pass jars and sparkHome to SparkContext's constructor.", "Imported from Github issue spark-1, originally reported by mateiz"], "derived": {"summary": "Right now, one must modify the Makefile and run script to point to external directories, which is ugly. This will get easier when a Nexus features is added to make it possible to pass the classpath to the Spark executor through environment variables (https://github.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Make it easier to run external code on a Nexus cluster - Right now, one must modify the Makefile and run script to point to external directories, which is ugly. This will get easier when a Nexus features is added to make it possible to pass the classpath to the Spark executor through environment variables (https://github."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-1, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-514", "title": "Merge fault tolerance code into master branch", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0010-04-03T23:05:00.000+0000", "updated": "2012-10-22T14:55:35.000+0000", "description": null, "comments": ["Github comment from mateiz: Done.", "Imported from Github issue spark-2, originally reported by mateiz"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Merge fault tolerance code into master branch"}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-2, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-513", "title": "Make NexusScheduler more efficient by keeping a list of tasks for each node", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0010-04-03T23:06:00.000+0000", "updated": "2012-10-22T14:55:32.000+0000", "description": "Right now, it performs an O(N) scan to find local tasks on each node. We could make this O(1) (amortized) by having a separate queue per node with the preferred tasks for that node, plus a global queue in case the node queue is empty.", "comments": ["Github comment from mateiz: Fixed in master.", "Imported from Github issue spark-3, originally reported by mateiz"], "derived": {"summary": "Right now, it performs an O(N) scan to find local tasks on each node. We could make this O(1) (amortized) by having a separate queue per node with the preferred tasks for that node, plus a global queue in case the node queue is empty.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Make NexusScheduler more efficient by keeping a list of tasks for each node - Right now, it performs an O(N) scan to find local tasks on each node. We could make this O(1) (amortized) by having a separate queue per node with the preferred tasks for that node, plus a global queue in case the node queue is empty."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-3, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-512", "title": "Shuffle operation", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0010-04-03T23:07:00.000+0000", "updated": "2012-10-22T14:55:36.000+0000", "description": "This is a big issue... the shuffle operation needs to be designed first!", "comments": ["Github comment from mateiz: This is fixed in master now.", "Imported from Github issue spark-4, originally reported by mateiz"], "derived": {"summary": "This is a big issue. the shuffle operation needs to be designed first!.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Shuffle operation - This is a big issue. the shuffle operation needs to be designed first!."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-4, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-511", "title": "Unify the API and implementation of ParallelArrays and HdfsFiles", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0010-04-03T23:09:00.000+0000", "updated": "2012-10-22T14:55:33.000+0000", "description": "Right now, ParallelArrays support fewer operations simply because HdfsFile was written later. It would be good to rewrite them to provide the same low-level API as files (maybe pulled into a trait called DistributedDataset) and to reuse classes such as MappedFile so that they work on both arrays and HDFS files. In other words, implement the internal structure described in the Spark paper.", "comments": ["Github comment from mateiz: Done in master branch.", "Imported from Github issue spark-5, originally reported by mateiz"], "derived": {"summary": "Right now, ParallelArrays support fewer operations simply because HdfsFile was written later. It would be good to rewrite them to provide the same low-level API as files (maybe pulled into a trait called DistributedDataset) and to reuse classes such as MappedFile so that they work on both arrays and HDFS files.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Unify the API and implementation of ParallelArrays and HdfsFiles - Right now, ParallelArrays support fewer operations simply because HdfsFile was written later. It would be good to rewrite them to provide the same low-level API as files (maybe pulled into a trait called DistributedDataset) and to reuse classes such as MappedFile so that they work on both arrays and HDFS files."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-5, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-509", "title": "Merge Mosharaf's broadcast code into master branch", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0010-06-17T17:07:00.000+0000", "updated": "2012-10-22T14:55:30.000+0000", "description": null, "comments": ["Imported from Github issue spark-7, originally reported by mateiz"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Merge Mosharaf's broadcast code into master branch"}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-7, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-510", "title": "Save operation", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0010-06-17T17:07:00.000+0000", "updated": "2012-10-22T14:55:34.000+0000", "description": "RDDs should support a save() call that saves each partition to a distributed filesystem.", "comments": ["Github comment from ijuma: This would be handy indeed. Also, it would be nice to support Hadoop's OutputFormat (similar to how InputFormat is supported as a source of data).", "Github comment from mateiz: I've looked into this a bit in the past, and the annoying part about Hadoop OutputFormats is that they are tied pretty tightly to running inside a Hadoop task (or at least the file-based ones are). They need a \"task attempt context\" to figure out a temp name for their file, as well as a JobConf that passes various parameters around. I'll probably do this though because it seems useful to support the existing Hadoop formats.\n\nIn the meantime, if you have a specific format you want (e.g. text or SequenceFile), you can probably use runJob() directly to do it. The quickest way would be to pick some output directory to write to and have each task create a file with a random name in there. However, this puts you in trouble if some task fails, as a retry attempt will choose a different filename (and in general, when we add speculative execution, there may be 2+ concurrent copies of each task active). The \"right\" way then is to have each task create a file in some other (temp) directory and return its filename when it completes, and then have the master rename the output file from each task to a location in the final directory when done.", "Github comment from ijuma: I see.\n\nThanks for the explanation. I had indeed figured out that runJob would allow me to do some of what I need now. It is why I filed issue #56 (I've patched the version I am using, of course).", "Github comment from mateiz: TD implemented this with pull request 64 (https://github.com/mesos/spark/pull/64), so I'm closing this issue. It should work for text files, SequenceFiles, and other Hadoop output formats of your choice.", "Imported from Github issue spark-6, originally reported by mateiz"], "derived": {"summary": "RDDs should support a save() call that saves each partition to a distributed filesystem.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Save operation - RDDs should support a save() call that saves each partition to a distributed filesystem."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-6, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-508", "title": "Union operation on RDDs", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0010-06-18T09:10:00.000+0000", "updated": "2012-10-22T14:55:37.000+0000", "description": "It should be possible to \"union\" two RDDs with the same element type and run some operation (e.g. a reduce) on the combined dataset.", "comments": ["Github comment from mateiz: This is mostly in already, except that the implementation is somewhat inefficient for large unions (it stores a chain of UnionRDD objects, rather than trying to coalesce them). Might want to fix that before closing this issue.", "Github comment from mateiz: Closed in 0e2adecdabe4b1b8f5df21ec16cea182b72d5626.", "Imported from Github issue spark-8, originally reported by mateiz"], "derived": {"summary": "It should be possible to \"union\" two RDDs with the same element type and run some operation (e. g.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Union operation on RDDs - It should be possible to \"union\" two RDDs with the same element type and run some operation (e. g."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-8, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-507", "title": "Add save() operation", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0010-06-27T10:41:00.000+0000", "updated": "2012-10-22T14:55:37.000+0000", "description": null, "comments": ["Github comment from mateiz: Duplicate of previous issue...", "Imported from Github issue spark-9, originally reported by mateiz"], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add save() operation"}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-9, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-506", "title": "Load-balance tasks better across nodes", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0010-06-27T10:42:00.000+0000", "updated": "2012-10-22T14:55:32.000+0000", "description": "In the current scheduler, each node in a resource offer is filled up individually, rather than tasks being spread out. Therefore, some nodes might get many more tasks than others, increasing latency.", "comments": ["Github comment from mateiz: Justin fixed this in one of his scheduler commits.", "Imported from Github issue spark-10, originally reported by mateiz"], "derived": {"summary": "In the current scheduler, each node in a resource offer is filled up individually, rather than tasks being spread out. Therefore, some nodes might get many more tasks than others, increasing latency.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "Load-balance tasks better across nodes - In the current scheduler, each node in a resource offer is filled up individually, rather than tasks being spread out. Therefore, some nodes might get many more tasks than others, increasing latency."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-10, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-505", "title": "Consider using a better build tool than make", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0010-06-27T14:00:00.000+0000", "updated": "2012-10-22T14:55:35.000+0000", "description": "Either Ant or SimpleBuildTool would be good. The latter is nicer but less likely to be installed on peoples' machines.", "comments": ["Github comment from mateiz: Closed by using SBT.", "Imported from Github issue spark-11, originally reported by mateiz"], "derived": {"summary": "Either Ant or SimpleBuildTool would be good. The latter is nicer but less likely to be installed on peoples' machines.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Consider using a better build tool than make - Either Ant or SimpleBuildTool would be good. The latter is nicer but less likely to be installed on peoples' machines."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-11, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-504", "title": "Add support for Hadoop InputFormats other than TextInputFormat", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0010-07-19T09:48:00.000+0000", "updated": "2012-10-22T14:55:36.000+0000", "description": null, "comments": ["Github comment from mateiz: Added support for generic Hadoop InputFormats and refactored textFile to\nuse this. Closed by 74bbfa91c252150811ce9617424cdeea3711bdf9.", "Imported from Github issue spark-12, originally reported by mateiz"], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add support for Hadoop InputFormats other than TextInputFormat"}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-12, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-503", "title": "Make it possible to run spark-shell without a shared NFS", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0010-07-22T16:54:00.000+0000", "updated": "2012-10-22T14:55:32.000+0000", "description": null, "comments": ["Github comment from mateiz: Fixed now that the REPL uses HTTP to serve classes to executors (commit e5e9edeeb3612663b885643cccbc4aaeae8c00be merged this in).", "Imported from Github issue spark-13, originally reported by mateiz"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Make it possible to run spark-shell without a shared NFS"}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-13, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-502", "title": "Make per-task CPU and memory configurable", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0010-08-03T14:30:00.000+0000", "updated": "2012-10-22T14:55:32.000+0000", "description": null, "comments": ["Github comment from mateiz: Fixed in master in commit 28d6f231968a1ee7bf17c890fc7a4cc850ed55c7.", "Imported from Github issue spark-14, originally reported by mateiz"], "derived": {"summary": "", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Make per-task CPU and memory configurable"}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-14, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-501", "title": "Don't keep trying to run tasks on a slave if they all fail", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0010-08-05T13:26:00.000+0000", "updated": "2012-10-22T14:55:35.000+0000", "description": "Justin ran into this on the R cluster when there was a misconfigured slave", "comments": ["Github comment from mateiz: This is fixed now.", "Imported from Github issue spark-15, originally reported by mateiz"], "derived": {"summary": "Justin ran into this on the R cluster when there was a misconfigured slave.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Don't keep trying to run tasks on a slave if they all fail - Justin ran into this on the R cluster when there was a misconfigured slave."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-15, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-500", "title": "Add a single spark.shared.fs option to use for REPL classes, broadcast, etc", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0010-08-13T19:01:00.000+0000", "updated": "2012-10-22T14:55:30.000+0000", "description": null, "comments": ["Github comment from mateiz: This is fixed now that the REPL uses HTTP to serve classes to the workers rather than relying on a file system -- this change means that spark.dfs is the only option used to specify a distributed file system to use. We could use a distributed FS for the REPL too if necessary, but won't do that yet (the generated classes will almost surely all be tiny).", "Imported from Github issue spark-16, originally reported by mateiz"], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add a single spark.shared.fs option to use for REPL classes, broadcast, etc"}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-16, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-578", "title": "Fix interpreter code generation to only capture needed dependencies", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0010-08-16T13:02:00.000+0000", "updated": "2016-01-18T10:20:41.000+0000", "description": null, "comments": ["Imported from Github issue spark-17, originally reported by mateiz", "[~matei] is this related to slimming down he assembly?", "This is so old that I think it's obsolete"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Fix interpreter code generation to only capture needed dependencies"}, {"q": "What updates or decisions were made in the discussion?", "a": "This is so old that I think it's obsolete"}]}}
{"project": "SPARK", "issue_id": "SPARK-499", "title": "Write up a Spark tutorial for wiki", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0010-09-01T22:46:00.000+0000", "updated": "2012-10-22T14:55:35.000+0000", "description": null, "comments": ["Github comment from tjhunter: I started a page on EC2 but I am blocked for now.", "Github comment from mateiz: This is fixed now.", "Imported from Github issue spark-18, originally reported by mateiz"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Write up a Spark tutorial for wiki"}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-18, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-498", "title": "Make Spark use log4j / slf4j for logging", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0010-09-27T15:01:00.000+0000", "updated": "2012-10-22T14:55:33.000+0000", "description": null, "comments": ["Github comment from mateiz: This has been fixed in master.", "Imported from Github issue spark-19, originally reported by mateiz"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Make Spark use log4j / slf4j for logging"}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-19, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-497", "title": "Increase default locality wait from 1s to 3-5s", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0010-09-28T23:23:00.000+0000", "updated": "2012-10-22T14:55:35.000+0000", "description": null, "comments": ["Github comment from mateiz: Increase default locality wait to 3s. Closed by f50b23b825dfbaa7ad65a8c27db2cc7eb0b2ebfa.", "Imported from Github issue spark-20, originally reported by mateiz"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Increase default locality wait from 1s to 3-5s"}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-20, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-496", "title": "java crashes when trying to run spark+EC2", "status": "Resolved", "priority": null, "reporter": "tjhunter", "assignee": null, "labels": [], "created": "0010-10-08T18:44:00.000+0000", "updated": "2012-10-19T22:50:32.000+0000", "description": "Mesos and Spark compile fine, the EC2 computing nodes are launched without a problem, however when I want to run the following command:\n\n ./run SparkPi \"1@ec2-184-73-97-35.compute-1.amazonaws.com\"\n\nThe program stops with the following error:\n\njava: ev.c:2210: ev_loop: Assertion `(\"libev: ev_loop recursion during release detected\", ((loop)->loop_done) != 0x80)' failed.\nAborted\n\nRunning the example locally has no issue:\n\ntjhunter@paris:~/mm/spark> ./run SparkPi \"local[10]\"\n10/10/08 19:42:07 INFO SparkContext: Running 2 tasks in parallel\n10/10/08 19:42:07 INFO LocalScheduler: Running task 0\n10/10/08 19:42:07 INFO LocalScheduler: Running task 1\n10/10/08 19:42:07 INFO LocalScheduler: Size of task 0 is 933 bytes\n10/10/08 19:42:07 INFO LocalScheduler: Size of task 1 is 933 bytes\n10/10/08 19:42:07 INFO ForeachTask: Processing spark.ParallelArraySplit@691\n10/10/08 19:42:07 INFO ForeachTask: Processing spark.ParallelArraySplit@692\n10/10/08 19:42:07 INFO LocalScheduler: Finished task 1\n10/10/08 19:42:07 INFO LocalScheduler: Finished task 0\n10/10/08 19:42:07 INFO SparkContext: Tasks finished in 0.1540033 s\nPi is roughly 3.13712\n\nLooking at the log of the mesos master, nothing happened. Do you have any advice? I can provide some logs and the specs of the machine.", "comments": ["Github comment from mateiz: I think this might be because you didn't include the port in your address. That is, it should've been 1@ec2-184-73-97-35.compute-1.amazonaws.com:5050. This is a pretty horrible error message to get for that though.\n\nIf you're using a relatively new version of the EC2 scripts, they will write the master URL in ~/mesos-ec2/cluster-url, so you can just cat that file to see the master.", "Github comment from tjhunter: No difference :(\n\nI just tried to run cpp-test-framework from the mesos examples and it fails. I suggest we close this ticket and I reopen a new one on mesos.", "Github comment from mateiz: I think this is because you're trying to submit jobs from your laptop to the EC2 cluster (see my email). Let me know if it also happens when you submit them from the cluster.", "Github comment from tjhunter: Thanks Matei,\nI had a compilation problem with mesos (solved), I will try again later this week end.\nOne point that is not clear to me is why I cannot send a spark job from my local computer to the cluster by passing the adress of the master. Is it a missing feature or simply a wrong way to use spark and mesos?", "Github comment from mateiz: It's just something we're not supporting yet. There are a few reasons for it:\n\n1) For your computer to be able to submit jobs to the EC2 cluster, the cluster would need to have a port open to the outside world on which jobs can be submitted. This could be a security problem -- random people would be able to submit jobs to your EC2 cluster. Some kind of authentication is required to prevent this.\n\n2) Spark also requires your worker nodes to be able to send messages to the master. This might require your computer to have a public IP address (i.e. a way to be accessible from behind a firewall). This will not be true for most users.\n\n3) Right now, different versions (builds) of Mesos will not play well with each other. If you happen to have a different version on your machine than the one on the Amazon image, then the Spark built against your local version may not be able to talk to the one on EC2. This will be fixed later but it's not yet fixed.\n\n4) In general the latency between your machine and EC2 might make it unattractive to run a job there. (I.e. the job will perform better if the master and workers are close).\n\nFor now, the recommended approach is to log into your master EC2 node, check out any code you want, and build it and run it from there. With EBS-backed clusters, you can have your master, or your entire cluster, persist so that you can restart it later. I can show you how to use that in person sometime.", "Github comment from mateiz: I'm going to close this issue now because it isn't something we're aiming to support at the moment.", "Imported from Github issue spark-21, originally reported by tjhunter"], "derived": {"summary": "Mesos and Spark compile fine, the EC2 computing nodes are launched without a problem, however when I want to run the following command:. /run SparkPi \"1@ec2-184-73-97-35.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "java crashes when trying to run spark+EC2 - Mesos and Spark compile fine, the EC2 computing nodes are launched without a problem, however when I want to run the following command:. /run SparkPi \"1@ec2-184-73-97-35."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-21, originally reported by tjhunter"}]}}
{"project": "SPARK", "issue_id": "SPARK-495", "title": "Spark/Mesos crashes after completion of jobs", "status": "Resolved", "priority": null, "reporter": "Mosharaf Chowdhury", "assignee": null, "labels": [], "created": "0010-10-12T14:51:00.000+0000", "updated": "2012-10-19T22:50:25.000+0000", "description": "It used to work before I came back. So I am not really sure whether its due to the new version of Mesos, or Scala 2.8.0, or something else that changed in Spark itself. The following error message shows up in the console.\n\n<pre>\n====================ERROR_MESSAGE====================\n#\n# A fatal error has been detected by the Java Runtime Environment:\n#\n#  SIGSEGV (0xb) at pc=0xb3fc70b0, pid=18408, tid=3029719920\n#\n# JRE version: 6.0_20-b02\n# Java VM: Java HotSpot(TM) Client VM (16.3-b01 mixed mode, sharing linux-x86 )\n# Problematic frame:\n# C  [libmesos.so+0x440b0]  _ZNSt8_Rb_treeISsSt4pairIKSsSsESt10_Select1stIS2_ESt4lessISsESaIS2_EE8_M_eraseEPSt13_Rb_tree_nodeIS2_E+0x20\n#\n# An error report file with more information is saved as:\n# /home/mosharaf/Work/spark/hs_err_pid18408.log\n#\n# If you would like to submit a bug report, please visit:\n#   http://java.sun.com/webapps/bugreport/crash.jsp\n# The crash happened outside the Java Virtual Machine in native code.\n# See problematic frame for where to report the bug.\n#\nAborted\n====================ERROR_MESSAGE====================\n</pre>\n\nNote that the problematic frame in most cases is something related to libmesos* (like this one), but it can be libstd* as well.", "comments": ["Github comment from mosharaf: The problem has propagated to all the branches out of multi-tracker. Most probably it has something to do with threads that just hang around (to timeout after a default 60s) even after the job is over.", "Github comment from mosharaf: This has not yet been observed in Mesos on EC2. Most probably it is specific to Mesos on local machine.", "Github comment from mateiz: Try updating Mesos.. it might be a bug that was fixed in there in the past. Also check whether there are any upgrades for your JVM. Some old versions on Ubuntu apparently crash with this type of error pretty often.", "Github comment from mosharaf: This actually happens when I am doing a small broadcast. If the broadcast takes long enough (say >20s) then it does not crash.", "Github comment from mosharaf: Similar things happen during LocalFileShuffle in local VM if there are more than 2 mesos slaves. Now I think all these are due to insufficient memory in my VM.", "Github comment from mateiz: Seems like this was a memory problem, closing the issue.", "Imported from Github issue spark-22, originally reported by mosharaf"], "derived": {"summary": "It used to work before I came back. So I am not really sure whether its due to the new version of Mesos, or Scala 2.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Spark/Mesos crashes after completion of jobs - It used to work before I came back. So I am not really sure whether its due to the new version of Mesos, or Scala 2."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-22, originally reported by mosharaf"}]}}
{"project": "SPARK", "issue_id": "SPARK-577", "title": "Rename Split to Partition", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "labels": [], "created": "0010-10-16T16:10:00.000+0000", "updated": "2014-02-03T07:18:55.000+0000", "description": null, "comments": ["Imported from Github issue spark-23, originally reported by mateiz", "This was done in 0.7, by https://github.com/mesos/spark/commit/06e5e6627f3856b5c6e3e60cbb167044de9ef6d4", "\"split\" is still there, e.g. in RDD class' scaladoc and parameter names. (spark 0.9.0)"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Rename Split to Partition"}, {"q": "What updates or decisions were made in the discussion?", "a": "\"split\" is still there, e.g. in RDD class' scaladoc and parameter names. (spark 0.9.0)"}]}}
{"project": "SPARK", "issue_id": "SPARK-576", "title": "Design and develop a more precise progress estimator", "status": "Resolved", "priority": null, "reporter": "Mosharaf Chowdhury", "assignee": null, "labels": [], "created": "0010-10-17T12:06:00.000+0000", "updated": "2014-10-20T18:34:06.000+0000", "description": "In addition to <task_completed>/<total_tasks>, we need to have something that says <estimated_time_remaining>.", "comments": ["Imported from Github issue spark-24, originally reported by mosharaf", "I've created a PR for this: https://github.com/apache/spark/pull/2837/", "Closing this as \"Won't Fix\"; see our discussion at https://github.com/apache/spark/pull/2837.\n\n{quote}\nI'm not sure whether this is a good idea - since estimates like this are likely to be very inaccurate due to the presence of stragglers in most jobs. I think it's better to just give people a slider as we have now. If someone sees an estimate but we are off by more than 100%, it could end up frustrating users.\n{quote}"], "derived": {"summary": "In addition to <task_completed>/<total_tasks>, we need to have something that says <estimated_time_remaining>.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Design and develop a more precise progress estimator - In addition to <task_completed>/<total_tasks>, we need to have something that says <estimated_time_remaining>."}, {"q": "What updates or decisions were made in the discussion?", "a": "Closing this as \"Won't Fix\"; see our discussion at https://github.com/apache/spark/pull/2837.\n\n{quote}\nI'm not sure whether this is a good idea - since estimates like this are likely to be very inaccurate due to the presence of stragglers in most jobs. I think it's better to just give people a slider as we have now. If someone sees an estimate but we are off by more than 100%, it could end up frustrating users.\n{quote}"}]}}
{"project": "SPARK", "issue_id": "SPARK-494", "title": "Print milliseconds in the log timestamps", "status": "Resolved", "priority": null, "reporter": "Mosharaf Chowdhury", "assignee": null, "labels": [], "created": "0010-10-18T09:59:00.000+0000", "updated": "2012-10-19T22:50:33.000+0000", "description": null, "comments": ["Github comment from mateiz: You can get this by editing conf/log4j.properties. Lets not commit it as the default though because it can get annoying.", "Imported from Github issue spark-25, originally reported by mosharaf"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Print milliseconds in the log timestamps"}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-25, originally reported by mosharaf"}]}}
{"project": "SPARK", "issue_id": "SPARK-493", "title": "Organize broadcast implementations", "status": "Resolved", "priority": null, "reporter": "Mosharaf Chowdhury", "assignee": null, "labels": [], "created": "0010-10-18T14:06:00.000+0000", "updated": "2012-10-19T22:50:25.000+0000", "description": "Everything related to Broadcast right now is in the Broadcast.scala file, which is rapidly growing.\n\nWe should create a separate broadcast directory and have separate source files for different implementations and shared classes.\n\nAlso we need to put them inside spark.broadcast package (similar to spark.repl)", "comments": ["Imported from Github issue spark-26, originally reported by mosharaf"], "derived": {"summary": "Everything related to Broadcast right now is in the Broadcast. scala file, which is rapidly growing.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Organize broadcast implementations - Everything related to Broadcast right now is in the Broadcast. scala file, which is rapidly growing."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-26, originally reported by mosharaf"}]}}
{"project": "SPARK", "issue_id": "SPARK-575", "title": "Maintain a cache of JARs on each node to avoid unnecessary copying", "status": "Closed", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0010-10-18T17:27:00.000+0000", "updated": "2015-02-12T07:25:07.000+0000", "description": null, "comments": ["Imported from Github issue spark-27, originally reported by mateiz", "What was the actual issue here?  I'm pretty sure that JARs are only downloaded once per machine during the lifetime of a SparkContext, so was the idea to maintain a cache of JARs that would be re-used across contexts?  That could become non-trivial if different jobs use different versions of JAR dependencies that have the same filename (but this could be addressed by tracking files by SHA sums, for example).", "[~joshrosen] is quite correct.\n\nthis issue looks inactive. i'm going to close it out, but as always feel free to re-open. i can think of a few ways this could be done, and not all need spark code to be changed.", "User 'mengxr' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/4555"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Maintain a cache of JARs on each node to avoid unnecessary copying"}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'mengxr' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/4555"}]}}
{"project": "SPARK", "issue_id": "SPARK-574", "title": "Put license notice and copyrights headers to all files", "status": "Resolved", "priority": null, "reporter": "tjhunter", "assignee": "Matei Alexandru Zaharia", "labels": [], "created": "0010-10-19T14:29:00.000+0000", "updated": "2013-09-01T15:24:45.000+0000", "description": "There is no license indication right now, and this may cause some potential issues regarding copyright and licensing rights of the code in the future.\nAlso, what is the license? Apache? BSD? LGPL? Public domain? GPL?", "comments": ["Imported from Github issue spark-28, originally reported by tjhunter", "0.8 adds Apache license headers"], "derived": {"summary": "There is no license indication right now, and this may cause some potential issues regarding copyright and licensing rights of the code in the future. Also, what is the license? Apache? BSD? LGPL? Public domain? GPL?.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Put license notice and copyrights headers to all files - There is no license indication right now, and this may cause some potential issues regarding copyright and licensing rights of the code in the future. Also, what is the license? Apache? BSD? LGPL? Public domain? GPL?."}, {"q": "What updates or decisions were made in the discussion?", "a": "0.8 adds Apache license headers"}]}}
{"project": "SPARK", "issue_id": "SPARK-573", "title": "Clarify semantics of the parallelized closures", "status": "Resolved", "priority": "Minor", "reporter": "tjhunter", "assignee": null, "labels": [], "created": "0010-10-28T09:56:00.000+0000", "updated": "2016-01-08T14:11:54.000+0000", "description": "I do not think there is any guideline about which features of scala are allowed/forbidden in the closure that gets sent to the remote nodes. Two examples I have are a return statement and updating mutable variables of singletons.\nIdeally, a compiler plugin could give an error at compile time, but a good error message at run time would be good also.\nAre there any other cases that should not be allowed?", "comments": ["Imported from Github issue spark-29, originally reported by tjhunter"], "derived": {"summary": "I do not think there is any guideline about which features of scala are allowed/forbidden in the closure that gets sent to the remote nodes. Two examples I have are a return statement and updating mutable variables of singletons.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Clarify semantics of the parallelized closures - I do not think there is any guideline about which features of scala are allowed/forbidden in the closure that gets sent to the remote nodes. Two examples I have are a return statement and updating mutable variables of singletons."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-29, originally reported by tjhunter"}]}}
{"project": "SPARK", "issue_id": "SPARK-572", "title": "Forbid update of static mutable variables", "status": "Closed", "priority": null, "reporter": "tjhunter", "assignee": null, "labels": [], "created": "0010-10-28T10:00:00.000+0000", "updated": "2015-07-29T11:55:41.000+0000", "description": "Consider the following piece of code:\n{code}\nobject Foo {\n var xx = -1\n\n def main() {\n   xx = 1\n   val sc = new SparkContext(...)\n   sc.broadcast(xx)\n   sc.parallelize(0 to 10).map(i=>{ ... xx ...})\n }\n}\n{code}\nCan you guess the value of xx? It is 1 when you use the local scheduler and -1 when you use the mesos scheduler. Given the complications, it should probably just be forbidden for now...", "comments": ["Github comment from rxin: Spark can't really tell what you are updating outside the closures. I think it is more important to document this behavior for global, static variables.", "Github comment from tjhunter: +1 for documentation in spark\nI still think the best solution would be a compiler plugin that would detect a few dangerous patterns like these. With the new macro system coming in scala 2.1x, that should not be too hard.", "Imported from Github issue spark-30, originally reported by tjhunter", "Static mutable variables are now a standard way of having code run on a per-executor basis.\n\nTo run per-entry, you can use map(), for per-partition you can use mapPartitions(), but for per-executor you need static variables or initializers.  If for example you want to open a connection to another data storage system and write all of an executor's data into that system, a static connection object is the common way to do that.\n\nI would propose closing this ticket as \"Won't Fix\".  Using this technique is confusing, but prohibiting it is difficult and introduces additional roadblocks to Spark power users.\n\ncc [~rxin]", "Closing this as won't fix since it is very hard to enforce and we do \"abuse\" it to run stateful computation."], "derived": {"summary": "Consider the following piece of code:\n{code}\nobject Foo {\n var xx = -1\n\n def main() {\n   xx = 1\n   val sc = new SparkContext(. )\n   sc.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Forbid update of static mutable variables - Consider the following piece of code:\n{code}\nobject Foo {\n var xx = -1\n\n def main() {\n   xx = 1\n   val sc = new SparkContext(. )\n   sc."}, {"q": "What updates or decisions were made in the discussion?", "a": "Closing this as won't fix since it is very hard to enforce and we do \"abuse\" it to run stateful computation."}]}}
{"project": "SPARK", "issue_id": "SPARK-571", "title": "Forbid return statements when cleaning closures", "status": "Resolved", "priority": null, "reporter": "tjhunter", "assignee": "William Benton", "labels": [], "created": "0010-10-28T10:07:00.000+0000", "updated": "2014-05-13T20:45:32.000+0000", "description": "By mistake, I wrote some code like this:\n{code}\nobject Foo {\n def main() {\n   val sc = new SparkContext(...)\n   sc.parallelize(0 to 10,10).map({  ... return 1 ... }).collect\n }\n}\n{code}\nThis compiles fine and actually runs using the local scheduler. However, using the mesos scheduler throws a NotSerializableException in the CollectTask . I agree the result of the program above should be undefined or it should be an error. Would it be possible to have more explicit messages?", "comments": ["Imported from Github issue spark-31, originally reported by tjhunter", "I have a patch for this and will submit a PR later today; can someone please assign this to me?"], "derived": {"summary": "By mistake, I wrote some code like this:\n{code}\nobject Foo {\n def main() {\n   val sc = new SparkContext(. )\n   sc.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Forbid return statements when cleaning closures - By mistake, I wrote some code like this:\n{code}\nobject Foo {\n def main() {\n   val sc = new SparkContext(. )\n   sc."}, {"q": "What updates or decisions were made in the discussion?", "a": "I have a patch for this and will submit a PR later today; can someone please assign this to me?"}]}}
{"project": "SPARK", "issue_id": "SPARK-570", "title": "Easy way to set remote node memory", "status": "Resolved", "priority": null, "reporter": "tjhunter", "assignee": null, "labels": [], "created": "0010-10-28T10:15:00.000+0000", "updated": "2013-12-07T14:54:11.000+0000", "description": "By default, spark accepts a number of java arguments (like for the repl, etc)\nFor specifying the memory on the remote nodes, maybe some options like:\n\nspark.remote.Xmx\nspark.remote.Xms\n\nwould seem more intuitive than SPARK_MEM?", "comments": ["Imported from Github issue spark-32, originally reported by tjhunter", "This is addressed by the {{spark.executor.memoy}} system property."], "derived": {"summary": "By default, spark accepts a number of java arguments (like for the repl, etc)\nFor specifying the memory on the remote nodes, maybe some options like:\n\nspark. remote.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Easy way to set remote node memory - By default, spark accepts a number of java arguments (like for the repl, etc)\nFor specifying the memory on the remote nodes, maybe some options like:\n\nspark. remote."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is addressed by the {{spark.executor.memoy}} system property."}]}}
{"project": "SPARK", "issue_id": "SPARK-492", "title": "Add unit tests for shuffle operations", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0010-11-08T00:48:00.000+0000", "updated": "2012-10-22T14:55:37.000+0000", "description": "Need to add these in the shuffle branch.", "comments": ["Github comment from mateiz: Unit tests for shuffle operations. Closed by d0a99665551ed6ecbe29583983198e27beb7200e.", "Imported from Github issue spark-33, originally reported by mateiz"], "derived": {"summary": "Need to add these in the shuffle branch.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Add unit tests for shuffle operations - Need to add these in the shuffle branch."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-33, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-491", "title": "Support other serialization mechanisms than Java Serialization", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0010-11-08T23:55:00.000+0000", "updated": "2014-11-10T17:44:48.000+0000", "description": "Currently looking at Kryo (http://code.google.com/p/kryo/).", "comments": ["Github comment from mateiz: Done in master.", "Imported from Github issue spark-34, originally reported by mateiz", "[~matei] I can't track down the original GitHub issue with commentary where Kryo was first used in Spark. I'm trying to understand why it was perceived to be a good idea, and what advantages it has over other solutions. If you can point me to where the old GitHub issue can be found, I'd love to read through the discussion on the original pull request. Thanks!"], "derived": {"summary": "Currently looking at Kryo (http://code. google.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support other serialization mechanisms than Java Serialization - Currently looking at Kryo (http://code. google."}, {"q": "What updates or decisions were made in the discussion?", "a": "[~matei] I can't track down the original GitHub issue with commentary where Kryo was first used in Spark. I'm trying to understand why it was perceived to be a good idea, and what advantages it has over other solutions. If you can point me to where the old GitHub issue can be found, I'd love to read through the discussion on the original pull request. Thanks!"}]}}
{"project": "SPARK", "issue_id": "SPARK-490", "title": "Clean up temporary files after LocalFileShuffle", "status": "Resolved", "priority": null, "reporter": "Mosharaf Chowdhury", "assignee": null, "labels": [], "created": "0010-11-28T11:29:00.000+0000", "updated": "2012-10-19T22:50:38.000+0000", "description": "Otherwise, disks run out of space and tasks start to fail resulting in job failure.", "comments": ["Github comment from mateiz: This is fixed now.", "Imported from Github issue spark-35, originally reported by mosharaf"], "derived": {"summary": "Otherwise, disks run out of space and tasks start to fail resulting in job failure.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Clean up temporary files after LocalFileShuffle - Otherwise, disks run out of space and tasks start to fail resulting in job failure."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-35, originally reported by mosharaf"}]}}
{"project": "SPARK", "issue_id": "SPARK-489", "title": "Pluggable broadcast implementations", "status": "Resolved", "priority": null, "reporter": "Mosharaf Chowdhury", "assignee": null, "labels": [], "created": "0010-11-30T19:01:00.000+0000", "updated": "2012-10-19T22:50:20.000+0000", "description": "Summary\n=======\nThree different implementations of broadcast that pluggable through the config option \"spark.broadcast.Factory\":\n\n1. DfsBroadcast:\n   * Write to DFS from master and read back from everywhere else\n\n2. ChainedBroadcast:\n   * Divide the broadcast variable into blocks and form a O(n)-length chain from the master.\n   * Can be and should be generalized to any d-ary tree instead of an 1-ary tree.\n\n3. BitTorrentBroadcast: Stripped-down bit torrent with the master as the seed\n   * Very fast and fairly distributed (upload:download=~1:~1) distribution\n   * Special case with only a few leechers have been resolved\n\nNotes\n======\n* Code files as well broadcast related config files should be restructured\n* There are still lots of logging going on in Chained and BitTorrent implementations. Should be turned OFF at some point.\n* conf/* files are included in the 'mos-bt' branch. Should be ignored/removed before committing the merge with 'master'", "comments": ["Github comment from mateiz: This looks pretty good overall, but I noticed a few small things:\n\n- Can you remove the conf files (git rm) to make the merge happen more easily? You're free to copy them and leave them around, just don't have them be under git's control.\n- I noticed that you changed the default shuffle implementation in RDD.scala (see the \"files changed\" view). Change it back to DfsShuffle.", "Github comment from mosharaf: Done. Couple of things about the config files: before using Streaming or BitTorrent, one must update spark.broadcast.MasterHostAddress with Master's IP address and will want to update spark.broadcast.BlockSize to their liking.", "Github comment from mateiz: Noticed a few other things:\n\n- Why is sendBroadcast part of the public interface of trait Broadcast? It's only called internally, so it doesn't need to be in the Broadcast trait.\n- The default broadcast implementation should be the HDFS one, not the BitTorrent one, until we get some more testing of BT. This is especially true if BT requires you to configure those variables.\n- When initializing BitTorrent broadcast on the master, figure out your local IP address using code such as http://www.exampledepot.com/egs/java.net/Local.html and set the spark.broadcast.MasterHostAddress system property to that. It will automatically be transferred to the slaves when they get launched. This way the user doesn't need to configure it manually.\n- Your naming of system properties is a bit inconsistent with existing ones. You capitalize the first letter of the last word (e.g. spark.broadcast.BlockSize), whereas all the existing properties don't (e.g. spark.broadcast.blockSize). It would be good to uncapitalize those.", "Github comment from mosharaf: Done. Let me know if you find anything else.", "Imported from Github issue spark-36, originally reported by mosharaf"], "derived": {"summary": "Summary\n=======\nThree different implementations of broadcast that pluggable through the config option \"spark. broadcast.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Pluggable broadcast implementations - Summary\n=======\nThree different implementations of broadcast that pluggable through the config option \"spark. broadcast."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-36, originally reported by mosharaf"}]}}
{"project": "SPARK", "issue_id": "SPARK-569", "title": "Consolidate/reorganize configuration options", "status": "Resolved", "priority": null, "reporter": "Mosharaf Chowdhury", "assignee": null, "labels": [], "created": "0010-12-01T10:54:00.000+0000", "updated": "2013-12-11T09:32:03.000+0000", "description": "Organize broadcast, shuffle, cache, and general Spark configuration options into separate files.", "comments": ["Imported from Github issue spark-37, originally reported by mosharaf", "Duplicates  SPARK-544"], "derived": {"summary": "Organize broadcast, shuffle, cache, and general Spark configuration options into separate files.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Consolidate/reorganize configuration options - Organize broadcast, shuffle, cache, and general Spark configuration options into separate files."}, {"q": "What updates or decisions were made in the discussion?", "a": "Duplicates  SPARK-544"}]}}
{"project": "SPARK", "issue_id": "SPARK-568", "title": "Document configuration options", "status": "Resolved", "priority": null, "reporter": "Mosharaf Chowdhury", "assignee": null, "labels": [], "created": "0010-12-01T10:55:00.000+0000", "updated": "2021-06-15T07:07:05.000+0000", "description": "Way too many configuration options already. Even though, in most cases, there names are descriptive enough, things can and will go out of control.", "comments": ["Github comment from tjhunter: +1 . A list of all the command line variables / options would be appreciated.", "Imported from Github issue spark-38, originally reported by mosharaf", "We now have docs for the configuration options: https://spark.incubator.apache.org/docs/latest/configuration.html"], "derived": {"summary": "Way too many configuration options already. Even though, in most cases, there names are descriptive enough, things can and will go out of control.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Document configuration options - Way too many configuration options already. Even though, in most cases, there names are descriptive enough, things can and will go out of control."}, {"q": "What updates or decisions were made in the discussion?", "a": "We now have docs for the configuration options: https://spark.incubator.apache.org/docs/latest/configuration.html"}]}}
{"project": "SPARK", "issue_id": "SPARK-488", "title": "Replacing the native lzf compression code with ning's lzf library", "status": "Resolved", "priority": null, "reporter": "Joshua Hartman", "assignee": null, "labels": [], "created": "0010-12-05T21:31:00.000+0000", "updated": "2012-10-19T22:50:31.000+0000", "description": "Hi Matei (and others),\n\nI replaced the native code with ning's apache 2.0 compression library. This way, spark doesn't have to maintain the LZF code, lzf compression can be used in places (like corporate environments) where native code is a no-go for deployment issues, and the JNI performance penalty doesn't have to be paid.\n\nCode compiles and tests pass.\n\n- Josh", "comments": ["Github comment from mateiz: This looks great, but I have one minor request: Can you include the license for Ning's LZF library in its directory in third_party, just for future reference?", "Github comment from jhartman: There you are. I'll merge this into the sbt branch soon as well.", "Github comment from mateiz: Thanks Josh. I've merged this into master. Do you want to add another pull request for sbt or should I merge it myself?", "Github comment from jhartman: I'll send a pull request for sbt. I'm not sure when it'll be ready - I really haven't started on it yet.\n\nBy the way, it looks like Spark doesn't have an actual license. What is the license(s) you plan on using?\n\n- Josh", "Github comment from mateiz: I'm going to put it under the BSD license. I'll add that soon. In the long term, after more core functionality is done, I'm also thinking of putting Spark in the Apache Incubator.\n\n(All the projects we work on at Berkeley get released under BSD or BSD-like licenses as part of our agreement with our industry sponsors.)", "Imported from Github issue spark-39, originally reported by jhartman"], "derived": {"summary": "Hi Matei (and others),\n\nI replaced the native code with ning's apache 2. 0 compression library.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "Replacing the native lzf compression code with ning's lzf library - Hi Matei (and others),\n\nI replaced the native code with ning's apache 2. 0 compression library."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-39, originally reported by jhartman"}]}}
{"project": "SPARK", "issue_id": "SPARK-487", "title": "Serialization issue", "status": "Resolved", "priority": null, "reporter": "tjhunter", "assignee": null, "labels": [], "created": "0010-12-10T19:42:00.000+0000", "updated": "2012-10-19T22:50:23.000+0000", "description": "The following code fails at deserialization:\n\n    import spark.Utils\n    @serializable class XXX\n    val xx=new XXX\n    val cc=Utils.serialize(xx)\n    val xx2=Utils.deserialize[Any](cc)\n\nReported error is below. In particular, it causes an exception to be thrown when the simple job class gets some results back to the master. I am looking at a workaround using my own class loader.\n\njava.lang.ClassNotFoundException: XXX\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:217)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:205)\n        at scala.tools.nsc.util.ScalaClassLoader$URLClassLoader.scala$tools$nsc$util$ScalaClassLoader$$super$findClass(ScalaClassLoader.scala:86)\n        at scala.tools.nsc.util.ScalaClassLoader$class.findClass(ScalaClassLoader.scala:51)\n        at scala.tools.nsc.util.ScalaClassLoader$URLClassLoader.findClass(ScalaClassLoader.scala:86)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:321)\n        at scala.tools.nsc.util.ScalaClassLoader$URLClassLoader.scala$tools$nsc$util$ScalaClassLoader$$super$loadClass(ScalaClassLoader.scala:86)\n        at scala.tools.nsc.util.ScalaClassLoader$class.loadClass(S...", "comments": ["Github comment from tjhunter: I have a patch in my branch that solves a related issue: using classes not used by spark (and thus not in spark's class loader).\nEventually, the deserialization routines should be the same as the one used in the executor.", "Github comment from mateiz: That makes sense, thanks for the patch. I didn't realize that deserialize is also called here when I added support for JARs. Should I just cherry-pick this commit, or do you have other stuff in your branch that you want merged in? I noticed there's a synchronized statement but if I recall correctly that had to do with some behavior of MM code.", "Github comment from mateiz: BTW, if you want to use Utils.deserialize in your own code, the easiest thing to pass in is Thread.currentThread.getContextClassLoader. That will be the ClassLoader that loaded your code to begin with. Is this what you're trying to do? Utils.serialize and deserialize aren't actually meant to be part of the public Spark API, but they will probably work if you do this.", "Github comment from tjhunter: Actually, I am not very concerned with the example I provided  in the first message (I am not using the deserializer directly from the console, it was just for narrowing down the issue). I am closing this bug since it is solved in my branch.\nSince my branch has this synchronized statement, you should cherry-pick the relevant commits, I have to find a workaround in the MM code to remove it.\nAlso, I committed another fix related to SPARK_MEM.", "Github comment from mateiz: Hey Tim,\n\nI merged in your SPARK_MEM fix, but for the other one, I'll check to see if there are cleaner ways to pass that classloader around throughout the code.", "Imported from Github issue spark-40, originally reported by tjhunter"], "derived": {"summary": "The following code fails at deserialization:\n\n    import spark. Utils\n    @serializable class XXX\n    val xx=new XXX\n    val cc=Utils.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Serialization issue - The following code fails at deserialization:\n\n    import spark. Utils\n    @serializable class XXX\n    val xx=new XXX\n    val cc=Utils."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-40, originally reported by tjhunter"}]}}
{"project": "SPARK", "issue_id": "SPARK-567", "title": "Unified directory structure for temporary data", "status": "Closed", "priority": null, "reporter": "Mosharaf Chowdhury", "assignee": null, "labels": [], "created": "0010-12-13T14:23:00.000+0000", "updated": "2014-09-21T15:45:09.000+0000", "description": "Broadcast, shuffle, and unforeseen use cases should use the same directory structure.", "comments": ["Imported from Github issue spark-41, originally reported by mosharaf", "please re-open with additional details for how this could be implemented"], "derived": {"summary": "Broadcast, shuffle, and unforeseen use cases should use the same directory structure.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Unified directory structure for temporary data - Broadcast, shuffle, and unforeseen use cases should use the same directory structure."}, {"q": "What updates or decisions were made in the discussion?", "a": "please re-open with additional details for how this could be implemented"}]}}
{"project": "SPARK", "issue_id": "SPARK-486", "title": "Passing masterHostAddress to workers is not working", "status": "Resolved", "priority": null, "reporter": "Mosharaf Chowdhury", "assignee": null, "labels": [], "created": "0010-12-17T12:12:00.000+0000", "updated": "2012-10-19T22:50:32.000+0000", "description": "Both BitTorrent and Chained broadcast implementations are not working because the workers are not receiving masterHostAddress properly. \n\nCurrent code in place is obviously wrong. But using System.setProperty (\"spark.broadcast.masterHostAddress\", InetAddress.getLocalHost.getHostAddress) in master and reading it everywhere using System.setProperty (\"spark.broadcast.masterHostAddress\", \"127.0.0.1\") is also not working. \n\n<pre>\n        if (isMaster) {\n          System.setProperty(\"spark.broadcast.masterHostAddress\", InetAddress.getLocalHost.getHostAddress)\n        }\n        MasterHostAddress_ =\n          System.getProperty(\"spark.broadcast.masterHostAddress\", \"127.0.0.1\")\n</pre>", "comments": ["Github comment from mosharaf: The main reason its not working is because in the SparkContext.scala, scheduler is started before the Broadcast is initialized; it already creates the args for the workers before Broadcast gets a chance to update masterHostAddress. \n\nSame is true for Shuffle codes.\n\nStraightforward solution is to initialize Broadcast and Shuffle subsystems before starting the scheduler. Is there any dependency to not change the order?", "Imported from Github issue spark-42, originally reported by mosharaf"], "derived": {"summary": "Both BitTorrent and Chained broadcast implementations are not working because the workers are not receiving masterHostAddress properly. Current code in place is obviously wrong.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Passing masterHostAddress to workers is not working - Both BitTorrent and Chained broadcast implementations are not working because the workers are not receiving masterHostAddress properly. Current code in place is obviously wrong."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-42, originally reported by mosharaf"}]}}
{"project": "SPARK", "issue_id": "SPARK-566", "title": "Replace polling+sleeping with semaphores in broadcast and shuffle", "status": "Resolved", "priority": null, "reporter": "Mosharaf Chowdhury", "assignee": null, "labels": [], "created": "0010-12-21T19:19:00.000+0000", "updated": "2014-10-21T07:48:02.000+0000", "description": "All the FixedThreadPools are doing this right now. Must be fixed.", "comments": ["Imported from Github issue spark-43, originally reported by mosharaf", "Thew shuffle and broadcast implementations have been re-written at least twice since this was created. I'm gonna close it as it's no longer relevant."], "derived": {"summary": "All the FixedThreadPools are doing this right now. Must be fixed.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Replace polling+sleeping with semaphores in broadcast and shuffle - All the FixedThreadPools are doing this right now. Must be fixed."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thew shuffle and broadcast implementations have been re-written at least twice since this was created. I'm gonna close it as it's no longer relevant."}]}}
{"project": "SPARK", "issue_id": "SPARK-485", "title": "Concurrency bug in BlockedShuffle implementations in mos-shuffle-tracked branch", "status": "Resolved", "priority": null, "reporter": "Mosharaf Chowdhury", "assignee": null, "labels": [], "created": "0011-01-03T17:36:00.000+0000", "updated": "2012-10-19T22:50:30.000+0000", "description": "Since BlockedShuffle implementations started reading multiple blocks per connection to the server/mapper instead of just one (Commit 1e26fb39534cc5b79c6980e0974bedbf72e19c69), a new bug has been introduced where they are reading more blocks than they are supposed to. \n\nWe use hasBlocksInSplit(i) and totalBlocksInSplit(i) to keep track of how many blocks have been received and this reducer is supposed to receive from mapper i. A particular index i of these two variables are accessed by only one thread at a time which is ensured by hasSplitsBitVector and splitsInRequestBitVector. In short, when we access hasBlocksInSplit(i) and totalBlocksInSplit(i), we do so without any synchronize blocks and use them for if/while -related comparisons. While totalBlocksInSplit(i) is written only once, hasBlocksInSplit(i) is incremented after every block is received. This can possibly create concurrency bug if some other thread is accessing/incrementing some other index hasBlocksInSplit(j) (not sure if thats likely though).\n\nIn any case, if we instead copy hasBlocksInSplit(i) and totalBlocksInSplit(i) to local variables in each thread and copy back at the end using synchronize block the problem should be away and it does. Reducers receiver exact number of blocks they are supposed to. But this creates a completely unrelated problem in ShuffleConsumer threads. Some of them just keep waiting on a take() call on a LinkedBlockingQueue even though they are not supposed to get to the take() call unless the size of the queue is non-zero. Essentially, they see that the size of queue is non-zero, they proceed, but find the queue to be empty. Since take() is a blocking call they just halt there.", "comments": ["Github comment from mosharaf: splitsInRequestBitVector was not being properly updated. Fixed by 7eb334d97ce3731d6397136e1df4c2c989a665f3.", "Imported from Github issue spark-44, originally reported by mosharaf"], "derived": {"summary": "Since BlockedShuffle implementations started reading multiple blocks per connection to the server/mapper instead of just one (Commit 1e26fb39534cc5b79c6980e0974bedbf72e19c69), a new bug has been introduced where they are reading more blocks than they are supposed to. We use hasBlocksInSplit(i) and totalBlocksInSplit(i) to keep track of how many blocks have been received and this reducer is supposed to receive from mapper i.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Concurrency bug in BlockedShuffle implementations in mos-shuffle-tracked branch - Since BlockedShuffle implementations started reading multiple blocks per connection to the server/mapper instead of just one (Commit 1e26fb39534cc5b79c6980e0974bedbf72e19c69), a new bug has been introduced where they are reading more blocks than they are supposed to. We use hasBlocksInSplit(i) and totalBlocksInSplit(i) to keep track of how many blocks have been received and this reducer is supposed to receive from mapper i."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-44, originally reported by mosharaf"}]}}
{"project": "SPARK", "issue_id": "SPARK-484", "title": "Make sure slf4j is initialized before any piece of code tries to use it", "status": "Resolved", "priority": null, "reporter": "Mosharaf Chowdhury", "assignee": null, "labels": [], "created": "0011-01-06T17:05:00.000+0000", "updated": "2012-10-19T22:50:26.000+0000", "description": "Sometimes important pieces of log messages get lost that cause trouble during log parsing.\n\nIt does print warnings though.", "comments": ["Github comment from mateiz: This is fixed in commit 5166d76, so closing it.", "Imported from Github issue spark-45, originally reported by mosharaf"], "derived": {"summary": "Sometimes important pieces of log messages get lost that cause trouble during log parsing. It does print warnings though.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Make sure slf4j is initialized before any piece of code tries to use it - Sometimes important pieces of log messages get lost that cause trouble during log parsing. It does print warnings though."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-45, originally reported by mosharaf"}]}}
{"project": "SPARK", "issue_id": "SPARK-565", "title": "Integrate spark in scala standard collection API", "status": "Resolved", "priority": null, "reporter": "tjhunter", "assignee": null, "labels": [], "created": "0011-04-26T10:44:00.000+0000", "updated": "2014-11-06T06:53:11.000+0000", "description": "This is more a meta-bug / whish item than a real bug. \nScala 2.0 provides some API for parallel collections which might be interesting to leverage, but mostly as a user, I would like to be able to write a function like:\n\ndef contrived_example(xs:Seq[Int]) = xs.map(_ * 2).sum\n\nand not have to care if xs is an array, a scala parallel collection or a RDD. Given that RDDs already implement most of the API for Seq, it seems mostly a matter of standardization. I am probably missing some subtle details here?", "comments": ["Imported from Github issue spark-46, originally reported by tjhunter", "RDD.flatMap being inconsistent with the collections API is an issue here.\nSee:\nhttp://mail-archives.apache.org/mod_mbox/spark-user/201403.mbox/%3CCAKn3j0s7paRiWVjjweEYGHLsGwY269gADY2fzWjruMYSu3YpzQ@mail.gmail.com%3E", "FYI I'm going to close this because we've locked down the API for 1.X, and it's pretty clear that it can't fully fit into the Scala collections API (that has a lot of things we don't have, and vice versa). This is something we can investigate later but it's unlikely that we'll want to bind the API to Scala even if we change pieces of it in the future."], "derived": {"summary": "This is more a meta-bug / whish item than a real bug. Scala 2.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Integrate spark in scala standard collection API - This is more a meta-bug / whish item than a real bug. Scala 2."}, {"q": "What updates or decisions were made in the discussion?", "a": "FYI I'm going to close this because we've locked down the API for 1.X, and it's pretty clear that it can't fully fit into the Scala collections API (that has a lot of things we don't have, and vice versa). This is something we can investigate later but it's unlikely that we'll want to bind the API to Scala even if we change pieces of it in the future."}]}}
{"project": "SPARK", "issue_id": "SPARK-483", "title": "Cache to disk", "status": "Resolved", "priority": null, "reporter": "Ankur Dave", "assignee": null, "labels": [], "created": "0011-04-28T09:22:00.000+0000", "updated": "2012-10-19T22:50:32.000+0000", "description": "DiskSpillingCache is a cache implementation derived from BoundedMemoryCache that spills entries to disk when it runs out of space.\n\nThe location on disk where entries are stored is configurable through the spark.diskSpillingCache.cacheDir property.", "comments": ["Github comment from mateiz: Thanks Ankur, I committed this.", "Imported from Github issue spark-47, originally reported by ankurdave"], "derived": {"summary": "DiskSpillingCache is a cache implementation derived from BoundedMemoryCache that spills entries to disk when it runs out of space. The location on disk where entries are stored is configurable through the spark.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Cache to disk - DiskSpillingCache is a cache implementation derived from BoundedMemoryCache that spills entries to disk when it runs out of space. The location on disk where entries are stored is configurable through the spark."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-47, originally reported by ankurdave"}]}}
{"project": "SPARK", "issue_id": "SPARK-482", "title": "Bagel: Large-scale graph processing on Spark", "status": "Resolved", "priority": null, "reporter": "Ankur Dave", "assignee": null, "labels": [], "created": "0011-05-03T14:49:00.000+0000", "updated": "2012-10-19T22:50:20.000+0000", "description": "Bagel is an implementation of the Pregel graph processing framework on Spark.\n\nBagel currently supports basic graph computation, combiners, and aggregators. Future work includes support for mutating the graph topology. Tests exist but currently don't run due to a Spark bug.", "comments": ["Github comment from tjhunter: I would recommend you refactor your code before merging, it is always harder / less tempting to do after.", "Github comment from mateiz: This looks great, Ankur, except for two naming things: can you change the package name from bagel to spark.bagel, and can you rename the Pregel class to Bagel?", "Github comment from ankurdave: Sure, I've done so.", "Github comment from mateiz: Looks great, thanks. The one thing I should add is that maybe you should write a README documenting the examples, or a wiki page (and put a comment in the code pointing to this location).", "Imported from Github issue spark-48, originally reported by ankurdave"], "derived": {"summary": "Bagel is an implementation of the Pregel graph processing framework on Spark. Bagel currently supports basic graph computation, combiners, and aggregators.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Bagel: Large-scale graph processing on Spark - Bagel is an implementation of the Pregel graph processing framework on Spark. Bagel currently supports basic graph computation, combiners, and aggregators."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-48, originally reported by ankurdave"}]}}
{"project": "SPARK", "issue_id": "SPARK-481", "title": "Support Scala 2.9.x", "status": "Resolved", "priority": null, "reporter": "Ismael Juma", "assignee": null, "labels": [], "created": "0011-05-26T01:53:00.000+0000", "updated": "2012-10-19T22:50:37.000+0000", "description": "I am opening an issue to track this. The FAQ says:\n\n\"Spark currently works with Scala 2.8.1. We're working on a port to 2.9.\"\n\nIs this work published somewhere? I would be interested in helping out. I looked at the branch names and there wasn't an obvious one for Scala 2.9.0.", "comments": ["Github comment from mateiz: Right now there isn't anything pushable done here. It seems that Spark will compile with 2.9, but the part I'd like to change is the interpreter. We currently include a modified version of the Scala 2.8 interpreter and I know the 2.9 one has some nice changes. Ideally the interpreter would've been modular enough that Spark doesn't have to ship a modified version (and I've been meaning to ask Scala guys about this) but for now this will have to do. That part is kind of hairy so it's probably easiest for me to do.", "Github comment from ijuma: After moving the repl classes to a separate module, it was trivial to build for 2.9.0 without repl support. I pushed the work to the following branch:\n\nhttps://github.com/ijuma/spark/tree/scala-2.9", "Github comment from ijuma: I've rebased the scala-2.9 branch against master since it now contains the repl classes in a separate module.", "Github comment from mateiz: I'm still working on supporting the 2.9 interpreter. Is anything else required to run on 2.9, other than updating the build file?", "Github comment from ijuma: The build file and the run shell script. The branch above helps so that you don't have to figure out what version you need for the Scala dependencies so that they work with 2.9.0. Quite trivial though.", "Github comment from mateiz: I've merged all your changes (up to upgrading scalacheck to 2.9) and a bunch of work on the interpreter into branch scala-2.9 in the repo. Rebase against that one if you need to make more changes. I think this is working OK but I want to test it a bit more before merging it into master.", "Github comment from ijuma: Excellent, thanks!", "Github comment from ijuma: Matei merged the Scala 2.9 branch to master, so this is now done. Closing.", "Imported from Github issue spark-49, originally reported by ijuma"], "derived": {"summary": "I am opening an issue to track this. The FAQ says:\n\n\"Spark currently works with Scala 2.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support Scala 2.9.x - I am opening an issue to track this. The FAQ says:\n\n\"Spark currently works with Scala 2."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-49, originally reported by ijuma"}]}}
{"project": "SPARK", "issue_id": "SPARK-480", "title": "Fix deprecations when compiled with Scala 2.8.1", "status": "Resolved", "priority": null, "reporter": "Ismael Juma", "assignee": null, "labels": [], "created": "0011-05-26T02:08:00.000+0000", "updated": "2012-10-19T22:50:37.000+0000", "description": "There are a lot of deprecations when compiling Spark with Scala 2.8.1. There are several instances of:\n\n- +' with mutable collections and then assigning it to a var.\n- Math instead of math\n- List.-\n- Iterator.fromArray\n- first instead of head\n- List.--\n- Array.fromFunction\n\nIs there a reason to use the deprecated methods or is it OK for me to submit a pull request with the changes?", "comments": ["Github comment from mateiz: Sounds great, thanks for taking a look at these.", "Github comment from ijuma: Matei has merged these changes. Closing.", "Imported from Github issue spark-50, originally reported by ijuma"], "derived": {"summary": "There are a lot of deprecations when compiling Spark with Scala 2. 8.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Fix deprecations when compiled with Scala 2.8.1 - There are a lot of deprecations when compiling Spark with Scala 2. 8."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-50, originally reported by ijuma"}]}}
{"project": "SPARK", "issue_id": "SPARK-564", "title": "Publish to maven repository", "status": "Closed", "priority": null, "reporter": "Ismael Juma", "assignee": null, "labels": [], "created": "0011-05-26T03:05:00.000+0000", "updated": "2012-10-20T16:50:07.000+0000", "description": "Publishing the jar to a maven repository would make adoption easier. The first step towards that would be to include the dependencies via the SBT build file instead of having the libraries in lib. If there are no objections, I could have a go.", "comments": ["Github comment from mateiz: That sounds great. The only thing to watch out for will be kryo-1.04-mod.jar, which is a slightly modified version of the Kryo serialization library (http://code.google.com/p/kryo/). I could push that as a separate GitHub project too.", "Github comment from ijuma: I started on this and I have the following questions:\n\n- Is there no way to avoid modifying the kryo jar? Modified third-party jars cause problems to clients that mix them with the unmodified ones.\n- What version of the mesos jar is being used?\n- What version of jline is being used?\n- Is the native liblzf-3.5 still being used?\n\nThe work in progress can be seen here:\n\nhttps://github.com/ijuma/spark/tree/issue51", "Github comment from mateiz: Unfortunately the Kryo changes are needed to have it work with common Scala objects that lack a default constructor (like Pair, List, tuples, etc). I reported some of the issues to the Kryo developer but they aren't fixed yet. We could include Kryo-mod as a separate project somehow since Kryo serialization is an optional feature.\n\nFor the Mesos JAR, we are using the pre-protobuf branch at github.com/mesos/mesos. Would it be possible to give this a separate version number? We can call it something like 18039d (the latest commit in that branch).\n\nFor jline, this is a good question and I'm not sure how to find the answer. I took the jline.jar out of SCALA_HOME/lib in order to support the Scala interpreter, but I don't see where its version is documented. Maybe just try the latest one?\n\nliblzf is no longer being used and could be deleted from the repo.", "Github comment from ijuma: I see. I think we could leave Kryo in lib for now until upstream incorporates those features then. That would mean that someone who depends on the SBT build of Spark would not get that dependency automatically.\n\nOK.\n\nMaybe I'll leave this in lib too as this may cause conflicts with another jline in the user's classpath. Do you think it would make sense to move the REPL classes to a separate subproject?\n\nI'll delete it then. Thanks.", "Github comment from mateiz: Moving the REPL to a separate subproject is a great idea. It shouldn't be too bad.. it will just depend on core, and all the code for it is in the spark/repl packages anyway.", "Github comment from mateiz: (But for that make sure that ./spark-shell continues to work.)", "Github comment from ijuma: Hmm, when I try spark-shell from master I get a NoClassDefFoundError. It seems like the scala-compiler.jar needs to be added to the classpath:\n\n[ijuma@localhost]~/src/spark% ./spark-shell \njava.lang.NoClassDefFoundError: scala/tools/nsc/InterpreterResults$Result\n\tat spark.repl.Main$.main(Main.scala:13)\n\nDoes it work for you?\n\nI've rebased the branch and moved the REPL to its own module. I had to use reflection in two places where the core module optionally uses REPL classes:\n\nhttps://github.com/ijuma/spark/tree/issue51\n\nI don't have any other changes planned aside from the spark-shell issue (which I think was already there, but I don't mind fixing it by simply adding the scala-compiler.jar to the classpath if you agree that it's the way to go).", "Github comment from ijuma: Oh, the other thing left to discuss is what public Maven repository to publish the binaries too. Once we decide that, we need to add that to the build configuration.", "Github comment from mateiz: That InterpreterResult class should be in Scala 2.8.1's scala-compiler.jar. I believe it gets added to the classpath when you just run scala. Has spark-shell ever worked for you without changes to Spark?\n\nWhen I try your branch, I believe I get past that problem, but I see:\n\njava.lang.NoClassDefFoundError: org/eclipse/jetty/util/thread/ThreadPool\n\tat spark.repl.SparkInterpreter.<init>(SparkInterpreter.scala:111)\n\tat spark.repl.SparkInterpreterLoop$$anon$1.<init>(SparkInterpreterLoop.scala:131)\n...\n\nI'll figure out what the right dependency is called for that.\n\nIn terms of which Maven repository to push it to, I don't have a preference as I don't know much about Maven. What are the options?", "Github comment from mateiz: I fixed this by not making the Jetty dependency \"provided\" and making it be on jetty-server rather than jetty-webapp. (We're using Jetty to run an HTTP server, not to embed Spark in a webapp.) Just remove the jettyWebapp and the val jetty = ... lines and add the following in BaseProject:\n\n```val jettyServer = \"org.eclipse.jetty\" % \"jetty-server\" % \"7.4.1.v20110513\"```\n\nAlso, I'm not 100% sure about this, but shouldn't the REPL, examples, Bagel, etc projects just depend on BaseProject rather than being subclasses? Right now there is a separate lib_managed directory for each one with pretty much the same JARs in it.", "Github comment from ijuma: \"That InterpreterResult class should be in Scala 2.8.1's scala-compiler.jar. I believe it gets added to the classpath when you just run scala. Has spark-shell ever worked for you without changes to Spark?\"\n\nNo, the scala-compiler doesn't get added to the runtime classpath by default (only scala-library). Have you modified your Scala installation in any way? I had never tried spark-shell previously.\n\n\"I fixed this by not making the Jetty dependency \"provided\" and making it be on jetty-server rather than jetty-webapp.\"\n\nThanks for looking into the issue, makes sense.\n\n\"Also, I'm not 100% sure about this, but shouldn't the REPL, examples, Bagel, etc projects just depend on BaseProject rather than being subclasses? Right now there is a separate lib_managed directory for each one with pretty much the same JARs in it.\"\n\nBaseProject isn't an actual project, it's just there to avoid duplication (so you can't depend on it). Sadly, there's no way around the duplication of jars in lib_managed in SBT 0.7.x. SBT 0.9.x has additional options (you can use the ivy cache or you can use an improved version of lib_managed that includes a shared lib_managed for multi-module projects).\n\nOnce SBT 0.10.0 is out (0.9.x is development while 0.10.x is stable) and the plugins have been ported, I can submit a patch to move to that as it requires incompatible changes. I've already done that for my company's project which has a build file that is way more complicated than Spark.", "Github comment from mateiz: I always install Scala by unzipping the .tar.gz somewhere and setting SCALA_HOME to point to that. I've done this on multiple machines with different OSes and it always seems to have the compiler on the classpath. Are you running it some other way? For example, I believe the Eclipse plugin doesn't add the compiler JAR to the classpath. In any case, maybe there's a way to make SBT add it, or at least to make the run script look for it in SCALA_HOME.", "Github comment from ijuma: Sorry, you are indeed right that scala-compiler is included by default when something is executed with scala. I had never realised this because I tend to run my applications with the java launcher and I add the scala-library manually (or deploy to a container). It seems like it's done that way for ease of implementation (the scala command can also be used for scripts and the compiler jar is needed for that). I'd say it's a bug, but whatever. :)\n\nWith that out of the way, I now realise what was the issue I was having. My installation of scala 2.9.0 was being used instead of the 2.8.1 one. Mystery solved. :)\n\nI'll fix the jetty thing and I'll try to configure publishing to a private repository. Later, we can look into publishing to scala-tools.org or the central Maven repository.", "Github comment from ijuma: I pushed the jetty-server change and spark-shell works for me when I set SCALA_HOME to my 2.8.1 install.\n\nIn terms of publishing, it seems like the easiest option is to use scala-tools.org. At the bottom of their home page[1], there are instructions on what is required to publish there. It looks like it has to be done by someone that can represent the project. I suggest you email them when you have a chance.\n\nMeanwhile, the work that's been done here makes it easy for someone to publish spark to their internal Maven repositories (I've done it to the one used by my company), so I believe it should be merged to master. Is there anything you'd like me to do before that is done?\n\n[1] http://scala-tools.org/", "Github comment from mateiz: The branch looks good, so I've merged it in. I fixed one bug in the Executor code though (it was passing the wrong classes when looking for a constructor for ExecutorClassLoader). Thanks for spending the time to do this.\n\nI'll look into hosting on scala-tools.org; that seems like a good solution. Maybe before that we should also change the package name from spark to something like edu.berkeley.spark throughout.", "Github comment from ijuma: Thanks for merging and catching the bug.\n\nHave you considered org.spark_project?", "Github comment from patelh: Has this been pushed to maven yet?", "Github comment from mateiz: Not to the official Maven repo, but you can do sbt publish-local to publish it to your own local repository.\n\nI'm actually thinking of submitting Spark as an Apache incubator project soon. If that gets in we'd have a clear home and we'd change the packages to org.apache.spark and publish.", "Imported from Github issue spark-51, originally reported by ijuma", "From the latest releases:\n\n{quote}\nSpark is now available in Maven Central, making it easier to link into your programs without having to build it as a JAR. Use the following Maven identifiers to add it to a project:\n\ngroupId: org.spark-project\nartifactId: spark-core_2.9.2\nversion: 0.5.1\n\ngroupId: org.spark-project\nartifactId: spark-core_2.9.2\nversion: 0.6.0\n{quote}"], "derived": {"summary": "Publishing the jar to a maven repository would make adoption easier. The first step towards that would be to include the dependencies via the SBT build file instead of having the libraries in lib.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Publish to maven repository - Publishing the jar to a maven repository would make adoption easier. The first step towards that would be to include the dependencies via the SBT build file instead of having the libraries in lib."}, {"q": "What updates or decisions were made in the discussion?", "a": "From the latest releases:\n\n{quote}\nSpark is now available in Maven Central, making it easier to link into your programs without having to build it as a JAR. Use the following Maven identifiers to add it to a project:\n\ngroupId: org.spark-project\nartifactId: spark-core_2.9.2\nversion: 0.5.1\n\ngroupId: org.spark-project\nartifactId: spark-core_2.9.2\nversion: 0.6.0\n{quote}"}]}}
{"project": "SPARK", "issue_id": "SPARK-479", "title": "Fix deprecations when compiled with Scala 2.8.1", "status": "Resolved", "priority": null, "reporter": "Ismael Juma", "assignee": null, "labels": [], "created": "0011-05-26T13:25:00.000+0000", "updated": "2012-10-19T22:50:25.000+0000", "description": "See issue #50. The change for `--` and `-` are the only controversial ones in my opinion and there's no better alternative as far as I know (it's the suggested approach in the deprecated message).", "comments": ["Github comment from ijuma: Oh, I also upgraded the sbt-idea plugin to version 0.4.0.", "Github comment from mateiz: Thanks, looks great!", "Imported from Github issue spark-52, originally reported by ijuma"], "derived": {"summary": "See issue #50. The change for `--` and `-` are the only controversial ones in my opinion and there's no better alternative as far as I know (it's the suggested approach in the deprecated message).", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Fix deprecations when compiled with Scala 2.8.1 - See issue #50. The change for `--` and `-` are the only controversial ones in my opinion and there's no better alternative as far as I know (it's the suggested approach in the deprecated message)."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-52, originally reported by ijuma"}]}}
{"project": "SPARK", "issue_id": "SPARK-478", "title": "Use explicit asInstanceOf instead of misleading unchecked pattern matching.", "status": "Resolved", "priority": null, "reporter": "Ismael Juma", "assignee": null, "labels": [], "created": "0011-05-26T23:03:00.000+0000", "updated": "2012-10-19T22:50:37.000+0000", "description": "This makes the code a bit uglier, but lets someone easily spot where dangerous things are being done. It also silences the compiler so that real \"unchecked\" warnings can be spotted.\n\nAlso enabled -unchecked warnings in SBT build file and restructured it a bit to make that easier.", "comments": ["Github comment from mateiz: Looks good. It looks like your editor also made some formatting changes in the build file, but we can keep those.\n\nIs there any widely used Scala style guide out there? We've tried to adhere to http://www.codecommit.com/scala-style-guide.pdf but it doesn't cover everything.", "Github comment from ijuma: Thanks for merging. Sorry for the formatting changes in the build file, it seemed like two styles were being used in the same file so I changed it to be consistent with typical Scala style, but forgot to mention it.\n\nI think http://davetron5000.github.com/scala-style/ScalaStyleGuide.pdf is a more recent version of the style guide that Daniel started. In terms of what's used out there, it varies quite a lot and this variation happens even inside the Scala standard library and compiler. I'll have a look at the Style Guide once again (it's been a while since I looked at it) and try to stick to that for Spark then.", "Imported from Github issue spark-53, originally reported by ijuma"], "derived": {"summary": "This makes the code a bit uglier, but lets someone easily spot where dangerous things are being done. It also silences the compiler so that real \"unchecked\" warnings can be spotted.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Use explicit asInstanceOf instead of misleading unchecked pattern matching. - This makes the code a bit uglier, but lets someone easily spot where dangerous things are being done. It also silences the compiler so that real \"unchecked\" warnings can be spotted."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-53, originally reported by ijuma"}]}}
{"project": "SPARK", "issue_id": "SPARK-477", "title": "Remove merged branches", "status": "Resolved", "priority": null, "reporter": "Ismael Juma", "assignee": null, "labels": [], "created": "0011-05-30T20:38:00.000+0000", "updated": "2012-10-19T22:50:27.000+0000", "description": "It would be nice if branches that have already been merged were removed from the repository. This would help new contributors know which branches are worth looking at. Tags could also be created before deletion where relevant.", "comments": ["Github comment from mateiz: I've removed some old ones that I worked on now, but there are still 16 that I know are active or I need to ask people about. For people who want to check out recent activity, another way is to view https://github.com/mesos/spark/network.", "Github comment from ijuma: Thanks!", "Imported from Github issue spark-54, originally reported by ijuma"], "derived": {"summary": "It would be nice if branches that have already been merged were removed from the repository. This would help new contributors know which branches are worth looking at.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Remove merged branches - It would be nice if branches that have already been merged were removed from the repository. This would help new contributors know which branches are worth looking at."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-54, originally reported by ijuma"}]}}
{"project": "SPARK", "issue_id": "SPARK-563", "title": "Run findBugs and IDEA inspections in the codebase", "status": "Resolved", "priority": null, "reporter": "Ismael Juma", "assignee": null, "labels": [], "created": "0011-05-31T03:46:00.000+0000", "updated": "2014-09-03T11:39:44.000+0000", "description": "I ran into a few instances of unused local variables and unnecessary usage of the 'return' keyword (the recommended practice is to avoid 'return' if possible) and thought it would be good to run findBugs and IDEA inspections to clean-up the code.\n\nI am willing to do this, but first would like to know whether you agree that this is a good idea and whether this is the right time to do it. These changes tend to affect many source files and can cause issues if there is major work ongoing in separate branches.", "comments": ["Github comment from mateiz: Let's wait a bit on this. In the near future, I want to merge in some changes to make Spark run on top of the latest Mesos version (branch new-rdds-protobuf), finish the 2.9 changes, and merge in some improvements to broadcast and shuffle (branch mos-bt). I think this will all be done sometime next week.\n\nBy the way, are you using IDEA to browse/develop Spark? Does it work OK? I might try it out.", "Github comment from ijuma: Sounds good.\n\nYes, I am using IDEA with SBT for all my development at the moment. Compilation is handled by SBT. I also use sbt-idea and idea-sbt-plugin. The former creates the project and the latter provides a console for hyperlinked errors inside IDEA. I think it works pretty well as far as Scala IDEs go.", "Github comment from ijuma: I won't have time to do this for a couple of weeks, but I was curious about the status. Have the important branches been merged now?", "Imported from Github issue spark-55, originally reported by ijuma", "This appears to be obsolete/stale too."], "derived": {"summary": "I ran into a few instances of unused local variables and unnecessary usage of the 'return' keyword (the recommended practice is to avoid 'return' if possible) and thought it would be good to run findBugs and IDEA inspections to clean-up the code. I am willing to do this, but first would like to know whether you agree that this is a good idea and whether this is the right time to do it.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Run findBugs and IDEA inspections in the codebase - I ran into a few instances of unused local variables and unnecessary usage of the 'return' keyword (the recommended practice is to avoid 'return' if possible) and thought it would be good to run findBugs and IDEA inspections to clean-up the code. I am willing to do this, but first would like to know whether you agree that this is a good idea and whether this is the right time to do it."}, {"q": "What updates or decisions were made in the discussion?", "a": "This appears to be obsolete/stale too."}]}}
{"project": "SPARK", "issue_id": "SPARK-476", "title": "Make SparkContext.runJob public", "status": "Resolved", "priority": null, "reporter": "Ismael Juma", "assignee": null, "labels": [], "created": "0011-05-31T10:03:00.000+0000", "updated": "2012-10-19T22:50:30.000+0000", "description": "Is there a good reason to prevent clients from using it? It's useful to be able to implement methods that can schedule operations and work on the result provided by each split.", "comments": ["Github comment from mateiz: Sounds good, as a means to let people write their own parallel operations. I've committed it.", "Imported from Github issue spark-56, originally reported by ijuma"], "derived": {"summary": "Is there a good reason to prevent clients from using it? It's useful to be able to implement methods that can schedule operations and work on the result provided by each split.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Make SparkContext.runJob public - Is there a good reason to prevent clients from using it? It's useful to be able to implement methods that can schedule operations and work on the result provided by each split."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-56, originally reported by ijuma"}]}}
{"project": "SPARK", "issue_id": "SPARK-475", "title": "LocalScheduler should catch Throwable to avoid silent failures", "status": "Resolved", "priority": null, "reporter": "Ismael Juma", "assignee": null, "labels": [], "created": "0011-06-01T01:00:00.000+0000", "updated": "2012-10-19T22:50:27.000+0000", "description": "I was testing a job with the LocalScheduler and it consistently got stuck. After spending some time with the debugger, it became clear that tasks were being sent to LocalScheduler, but they would never finish. It turns out that Throwable subclasses (at one point NoClassDefFoundError and in another OutOfMemoryError) were being thrown and the thread would die silently.\n\nI suggest simply catching Throwable instead of Exception in this situation, since we just report the problem and then call System.exit. I also think we should remove the System.exit call, but that can be discussed in a separate issue.", "comments": ["Imported from Github issue spark-57, originally reported by ijuma"], "derived": {"summary": "I was testing a job with the LocalScheduler and it consistently got stuck. After spending some time with the debugger, it became clear that tasks were being sent to LocalScheduler, but they would never finish.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "LocalScheduler should catch Throwable to avoid silent failures - I was testing a job with the LocalScheduler and it consistently got stuck. After spending some time with the debugger, it became clear that tasks were being sent to LocalScheduler, but they would never finish."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-57, originally reported by ijuma"}]}}
{"project": "SPARK", "issue_id": "SPARK-474", "title": "Remove unnecessary toStream calls", "status": "Resolved", "priority": null, "reporter": "Ismael Juma", "assignee": null, "labels": [], "created": "0011-06-01T07:23:00.000+0000", "updated": "2012-10-19T22:50:25.000+0000", "description": "It could be that I am missing something, but it looks to me like the toStream calls are unnecessary. Used the scala-2.9 branch as I believe that will become master soon.", "comments": ["Github comment from mateiz: I'm pretty sure those were needed in 2.8, but it looks like they aren't in 2.9.", "Imported from Github issue spark-58, originally reported by ijuma"], "derived": {"summary": "It could be that I am missing something, but it looks to me like the toStream calls are unnecessary. Used the scala-2.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Remove unnecessary toStream calls - It could be that I am missing something, but it looks to me like the toStream calls are unnecessary. Used the scala-2."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-58, originally reported by ijuma"}]}}
{"project": "SPARK", "issue_id": "SPARK-473", "title": "Move managedStyle to SparkProject", "status": "Resolved", "priority": null, "reporter": "Ismael Juma", "assignee": null, "labels": [], "created": "0011-06-02T13:24:00.000+0000", "updated": "2012-10-19T22:50:27.000+0000", "description": "I had added it to DepJar by mistake. This is not urgent and it could be merged after the other work you have planned for the coming week. I just put it here as a heads-up.", "comments": ["Github comment from mateiz: Thanks for the fix.", "Imported from Github issue spark-59, originally reported by ijuma"], "derived": {"summary": "I had added it to DepJar by mistake. This is not urgent and it could be merged after the other work you have planned for the coming week.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Move managedStyle to SparkProject - I had added it to DepJar by mistake. This is not urgent and it could be merged after the other work you have planned for the coming week."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-59, originally reported by ijuma"}]}}
{"project": "SPARK", "issue_id": "SPARK-562", "title": "Add a way to ship files with as Spark job", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": "Denny Britz", "labels": [], "created": "0011-06-20T14:26:00.000+0000", "updated": "2013-01-20T12:38:36.000+0000", "description": "The new pipe() operation allows users to pass an RDD through an external shell command to enable the use of Python scripts, existing binaries, etc in Spark, but there should also be a way to ship the program to the working directory of the slave nodes as in Hadoop Streaming.", "comments": ["Github comment from rxin: Shark's issue number 7 ( https://github.com/amplab/shark/issues/7 ) actually depends on this.\n\nHive uses Hadoop distributed cache to distribute external files (e.g. scripts) across the cluster.", "Imported from Github issue spark-60, originally reported by mateiz", "This was added in 0.6.0 via addFile() / addJar()."], "derived": {"summary": "The new pipe() operation allows users to pass an RDD through an external shell command to enable the use of Python scripts, existing binaries, etc in Spark, but there should also be a way to ship the program to the working directory of the slave nodes as in Hadoop Streaming.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add a way to ship files with as Spark job - The new pipe() operation allows users to pass an RDD through an external shell command to enable the use of Python scripts, existing binaries, etc in Spark, but there should also be a way to ship the program to the working directory of the slave nodes as in Hadoop Streaming."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was added in 0.6.0 via addFile() / addJar()."}]}}
{"project": "SPARK", "issue_id": "SPARK-472", "title": "Better readme", "status": "Resolved", "priority": null, "reporter": "Olivier Grisel", "assignee": null, "labels": [], "created": "0011-06-22T16:29:00.000+0000", "updated": "2012-10-19T22:50:25.000+0000", "description": "Various improvements to the README file to make it look better on github.", "comments": ["Github comment from mateiz: Thanks Olivier!", "Imported from Github issue spark-61, originally reported by ogrisel"], "derived": {"summary": "Various improvements to the README file to make it look better on github.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Better readme - Various improvements to the README file to make it look better on github."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-61, originally reported by ogrisel"}]}}
{"project": "SPARK", "issue_id": "SPARK-471", "title": "Add missing test for RDD.groupWith", "status": "Resolved", "priority": null, "reporter": "Olivier Grisel", "assignee": null, "labels": [], "created": "0011-06-22T16:30:00.000+0000", "updated": "2012-10-19T22:50:38.000+0000", "description": "Test for the CoGroup operation.", "comments": ["Github comment from mateiz: Thanks!", "Imported from Github issue spark-62, originally reported by ogrisel"], "derived": {"summary": "Test for the CoGroup operation.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add missing test for RDD.groupWith - Test for the CoGroup operation."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-62, originally reported by ogrisel"}]}}
{"project": "SPARK", "issue_id": "SPARK-470", "title": "Implemented RDD.leftOuterJoin and RDD.rightOuterJoin", "status": "Resolved", "priority": null, "reporter": "Olivier Grisel", "assignee": null, "labels": [], "created": "0011-06-24T01:06:00.000+0000", "updated": "2012-10-19T22:50:36.000+0000", "description": "I decided to implement them from scratch on the `join` method model rather than filtering out the output of `groupWith` to avoid emitting intermediate results that are dropped right afterward.\n\nI think it's better to use the scala Option type to handle missing values rather that null values as in SQL or Pig, tell be if you don't aggree.\n\n*Note*: There is no scaladoc on those methods as they all lack such documentation in the RDD class. I really think this is missing though. Maybe the RDD should be fully documented at once in another pull request?", "comments": ["Github comment from mateiz: Looks good, thanks. A docs patch for RDD would also be appreciated.", "Imported from Github issue spark-63, originally reported by ogrisel"], "derived": {"summary": "I decided to implement them from scratch on the `join` method model rather than filtering out the output of `groupWith` to avoid emitting intermediate results that are dropped right afterward. I think it's better to use the scala Option type to handle missing values rather that null values as in SQL or Pig, tell be if you don't aggree.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Implemented RDD.leftOuterJoin and RDD.rightOuterJoin - I decided to implement them from scratch on the `join` method model rather than filtering out the output of `groupWith` to avoid emitting intermediate results that are dropped right afterward. I think it's better to use the scala Option type to handle missing values rather that null values as in SQL or Pig, tell be if you don't aggree."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-63, originally reported by ogrisel"}]}}
{"project": "SPARK", "issue_id": "SPARK-469", "title": "Functionality to save RDDs to Hadoop files", "status": "Resolved", "priority": null, "reporter": "Tathagata Das", "assignee": null, "labels": [], "created": "0011-06-27T11:00:00.000+0000", "updated": "2012-10-19T22:50:28.000+0000", "description": "Overview of changes\n1> Introduced class HadoopFileWriter that uses Hadoop's libraries (OutputFormat, OutputCommitter, etc) to save the records in a RDD, each split of the RDD being a file in the destination directory. While instantiating HadoopFileWriter, one can specify which classes to use for key, value, output-format and output-committer. The way it is written, all subclasses of outputformat and outputcommitter should be applicable.\n\n2> Introduced following functions in RDD\n(a) RDD[T].saveAsTextFile(<path>): toString will be used to convert records (type T) to String and saved as a text file.\n(b) RDD[T].saveAsObjectFile(<path>): Each record, if serializable, will be converted to bytes and saved as serialized objects.  \n(b) RDD[(K,V)].saveAsHadoopFile(<path>): Key-value pair RDDs will be saved as a Hadoop file using whatever OutputFormat and OutputCommitter class that the user specifies. This has a few different overloaded syntaxes. \n(c) RDD[(Writable, Writable)].saveAsSequenceFile(<path>): Key-value pair RDDs with keys and value being subclasses of Writable (like IntWritable) or can be implicitly converted to Writable (Int -> IntWritable, etc) can be saved as a Sequential File.\n \n3> Other changes\n(a) SparkContext.objectFile(<path>): opens files that have been saved by saveAsObjectFile\n(b) A number of implicit functions to convert basic data types to its Writable counterparts\n(c) Introduced class TaskContext, that provides a running Task with the contextual information like stage ID, task ID.\n(d) Scheduler and Task classes changed to generate and populate TaskContext information", "comments": ["Github comment from ijuma: Did you test this when using s3n Paths?", "Github comment from tdas: No, I havent tested it using s3n paths. I have tested it only with local and hdfs paths. It would be great if some one is able to test s3n paths.", "Github comment from ijuma: I tried it and ran into some issues during the set-up of OutputCommitter. I'll file an issue with more details if it hasn't been verified to work then.", "Github comment from tdas: Great, I will look into it once we have the details. Thanks!", "Github comment from ijuma: I filed https://github.com/mesos/spark/issues/81", "Imported from Github issue spark-64, originally reported by tdas"], "derived": {"summary": "Overview of changes\n1> Introduced class HadoopFileWriter that uses Hadoop's libraries (OutputFormat, OutputCommitter, etc) to save the records in a RDD, each split of the RDD being a file in the destination directory. While instantiating HadoopFileWriter, one can specify which classes to use for key, value, output-format and output-committer.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Functionality to save RDDs to Hadoop files - Overview of changes\n1> Introduced class HadoopFileWriter that uses Hadoop's libraries (OutputFormat, OutputCommitter, etc) to save the records in a RDD, each split of the RDD being a file in the destination directory. While instantiating HadoopFileWriter, one can specify which classes to use for key, value, output-format and output-committer."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-64, originally reported by tdas"}]}}
{"project": "SPARK", "issue_id": "SPARK-468", "title": "Change @serializable to extends Serializable in 2.9 branch", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0011-06-27T21:28:00.000+0000", "updated": "2012-10-22T14:55:33.000+0000", "description": "Looks like @serializable is deprecated now. It may make sense to do this in the 2.8 branch too (importing java.util.Serializable) but we can also wait until we no longer have a 2.8 branch.", "comments": ["Github comment from ijuma: This can be closed now (I don't have the privileges to do it myself).", "Github comment from mateiz: I added you as a committer to the project now; hopefully that means you can close issues too.", "Github comment from ijuma: Thanks!", "Imported from Github issue spark-65, originally reported by mateiz"], "derived": {"summary": "Looks like @serializable is deprecated now. It may make sense to do this in the 2.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Change @serializable to extends Serializable in 2.9 branch - Looks like @serializable is deprecated now. It may make sense to do this in the 2."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-65, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-561", "title": "Implement unit tests for RDD save functionality", "status": "Resolved", "priority": null, "reporter": "Tathagata Das", "assignee": null, "labels": [], "created": "0011-06-27T22:09:00.000+0000", "updated": "2012-10-22T15:10:58.000+0000", "description": "The RDD save functionality has been introduced with a number of assumptions about how the Hadoop library class OutputFormat and OutputCommitter is used to create Hadoop files. There is a chance that some assumptions are flawed and/or have been overlooked. To make sure things dont break, unit tests should be present.\n\nAlso, the functionality is expected to work with other custom OutputFormat and OutputCommiiter classes, along with custom JobConf parameters, but it has not been tested with any OutputFormat other than SequenceFileOutputFormat and FileOutputCommitter.", "comments": ["Github comment from mateiz: I've added some unit tests to check that files are created and can be read back, though nothing for failures. That's going to be harder to test without some way to instrument the LocalScheduler and make it fail tasks.", "Imported from Github issue spark-66, originally reported by tdas"], "derived": {"summary": "The RDD save functionality has been introduced with a number of assumptions about how the Hadoop library class OutputFormat and OutputCommitter is used to create Hadoop files. There is a chance that some assumptions are flawed and/or have been overlooked.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Implement unit tests for RDD save functionality - The RDD save functionality has been introduced with a number of assumptions about how the Hadoop library class OutputFormat and OutputCommitter is used to create Hadoop files. There is a chance that some assumptions are flawed and/or have been overlooked."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-66, originally reported by tdas"}]}}
{"project": "SPARK", "issue_id": "SPARK-560", "title": "Specialize RDDs / iterators", "status": "Closed", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0011-07-04T19:14:00.000+0000", "updated": "2015-02-06T19:33:37.000+0000", "description": "When you're working on in-memory data, the overhead of boxing / unboxing starts to matter, and it looks like specializing would give a 2-4x speedup. We can't just throw in @specialized though because Scala's Iterator is not specialized. We probably need to make our own and also ensure that the right methods get called remotely when you have a chain of RDDs (i.e. it doesn't \"lose\" its specialization).", "comments": ["Github comment from ijuma: Note that there's a plan to add @specialized to many of Scala collections going forward.", "Imported from Github issue spark-67, originally reported by mateiz", "[~matei], [~pwendell], [~rxin]: Is this issue still valid?", "We should close this one. It is much easier to do with DataFrame, and we will just make DataFrame optimized for this."], "derived": {"summary": "When you're working on in-memory data, the overhead of boxing / unboxing starts to matter, and it looks like specializing would give a 2-4x speedup. We can't just throw in @specialized though because Scala's Iterator is not specialized.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Specialize RDDs / iterators - When you're working on in-memory data, the overhead of boxing / unboxing starts to matter, and it looks like specializing would give a 2-4x speedup. We can't just throw in @specialized though because Scala's Iterator is not specialized."}, {"q": "What updates or decisions were made in the discussion?", "a": "We should close this one. It is much easier to do with DataFrame, and we will just make DataFrame optimized for this."}]}}
{"project": "SPARK", "issue_id": "SPARK-467", "title": "Add API for controlling the number of splits on a Hadoop file", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0011-07-06T16:42:00.000+0000", "updated": "2012-10-22T14:55:37.000+0000", "description": "Mostly useful when someone wants to increase the number of splits per block, since you must have at least one split per block right now.", "comments": ["Github comment from mateiz: This is now fixed.", "Imported from Github issue spark-68, originally reported by mateiz"], "derived": {"summary": "Mostly useful when someone wants to increase the number of splits per block, since you must have at least one split per block right now.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add API for controlling the number of splits on a Hadoop file - Mostly useful when someone wants to increase the number of splits per block, since you must have at least one split per block right now."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-68, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-466", "title": "Enable -optimize in the build", "status": "Resolved", "priority": null, "reporter": "Ismael Juma", "assignee": null, "labels": [], "created": "0011-07-14T17:18:00.000+0000", "updated": "2012-10-19T22:50:23.000+0000", "description": "The last time I tried this, I ran into some issues. I'll try again soon and investigate the issues (if they still exist).", "comments": ["Github comment from mateiz: Sounds good. I'm curious, have you seen it make a significant difference in performance?", "Github comment from ijuma: It can make a difference in some cases like http://dcsobral.blogspot.com/2011/05/scala-29-optimizes-for-comprehensions.html . Not as good as it could be though. In Scala trunk, Paul Phillips fixed a bug where -optimize would not inline private methods and that seems to help some too.", "Github comment from ijuma: Seems like this is OK now as all tests pass for me. See: https://github.com/mesos/spark/pull/77", "Github comment from mateiz: Thanks, I've merged it in.", "Imported from Github issue spark-69, originally reported by ijuma"], "derived": {"summary": "The last time I tried this, I ran into some issues. I'll try again soon and investigate the issues (if they still exist).", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "Enable -optimize in the build - The last time I tried this, I ran into some issues. I'll try again soon and investigate the issues (if they still exist)."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-69, originally reported by ijuma"}]}}
{"project": "SPARK", "issue_id": "SPARK-465", "title": "Port SBT build to SBT 0.10", "status": "Resolved", "priority": null, "reporter": "Ismael Juma", "assignee": null, "labels": [], "created": "0011-07-14T17:19:00.000+0000", "updated": "2012-10-19T22:50:33.000+0000", "description": "I have started this and should have something for review in the next few days.", "comments": ["Github comment from ijuma: By the way, what is XmlTestReport used for? I use junit_xml_listener for Hudson/Jenkins and it's a possible replacement that doesn't require forking.\n\n[1] http://henkelmann.eu/2011/02/02/junit_xml_listener_now_with_support_for_skipped_tests", "Github comment from ijuma: I've pushed a commit in a work-in-progress branch:\n\nhttps://github.com/ijuma/spark/commit/f686e3dacb02d42dd2bb9695a96cecd85786d7b5\n\nAlmost everything works. The two outstanding issues are sbt-assembly and XmlTestReport. I sent a message to the person who ported the former to SBT 0.10 asking for a release for SBT 0.10.1. Regarding XmlTestReport, waiting for an answer to comment #1", "Github comment from ijuma: dep-jar now works. Would it be possible to get an answer to the XmlTestReport question? That's the only thing left from a fully functional sbt 0.10.1 build.", "Github comment from mateiz: I don't think the XmlTestReport is critical. It was there just to output data to Hudson, but if junit_xml_listener can do this too, it's fine to replace it with that.", "Github comment from ijuma: Great, it should be easy then.", "Github comment from ijuma: I pushed the latest changes. The branch is ready for testing and review:\n\nhttps://github.com/ijuma/spark/tree/sbt-0.10", "Github comment from mateiz: I tried building this but got the following error when SBT tried to update itself. Is this a temporary thing due to a server being down?\n\n    Getting org.scala-tools.sbt sbt_2.8.1 0.10.1 ...\n    \n    :: problems summary ::\n    :::: WARNINGS\n    \t\t[NOT FOUND  ] commons-codec#commons-codec;1.2!commons-codec.jar (2ms)\n    \n  \t==== Maven2 Local: tried\n    \n    \t  file:///Users/matei/.m2/repository/commons-codec/commons-codec/1.2/commons-codec-1.2.jar\n      \n    download failed: commons-codec#commons-codec;1.2!commons-codec.jar\n    Error during sbt execution: Error retrieving required libraries\n      (see /Users/matei/workspace/spark/project/boot/update.log for complete log)\n    Error: Could not retrieve sbt 0.10.1", "Github comment from mateiz: I managed to fix this by removing my Ivy cache (~/.ivy2), so I guess it was something local to my machine (maybe some download cancelled earlier on). I'm going to merge this momentarily.", "Github comment from mateiz: Actually, now it builds but I see another problem: ScalaTest fails on every test with something like the following:\n\n    Could not run test spark.ParallelCollectionSplitSuite: java.lang.ClassCastException: scala.collection.immutable.Set$EmptySet$ cannot be cast to scala.collection.generic.Addable\n\nIt's not clear why this would happen, except perhaps for some mismatch between the ScalaTest API in Scala 2.8 and 2.9 (http://groups.google.com/group/scalatest-users/tree/browse_frm/month/2011-02?_done=%2Fgroup%2Fscalatest-users%2Fbrowse_frm%2Fmonth%2F2011-02%3F&) due to collection API changes. Is SBT trying to run the tests through the ScalaTest 2.8 API?", "Github comment from ijuma: If I run \"test\" in sbt shell on my branch, all tests pass, but my branch is based on the scala-2.9 branch. The ScalaTest version specified requires Scala 2.9.0.\n\nMaybe you merged the branch to master? That won't work without some additional work to make sure the dependencies work with Scala 2.8.1.\n\nMy intent was for this to be Scala 2.9.x only.", "Github comment from mateiz: I finally fixed the problem by doing a clean checkout -- there must've been some old JARs left behind that were compiled with Scala 2.8 and that SBT didn't decide to update. I've merged this into the repository now.", "Github comment from ijuma: Thanks!", "Imported from Github issue spark-70, originally reported by ijuma"], "derived": {"summary": "I have started this and should have something for review in the next few days.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Port SBT build to SBT 0.10 - I have started this and should have something for review in the next few days."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-70, originally reported by ijuma"}]}}
{"project": "SPARK", "issue_id": "SPARK-464", "title": "ClosureCleaner fails when instantiating with outer", "status": "Resolved", "priority": null, "reporter": "chrisx", "assignee": null, "labels": [], "created": "0011-07-19T14:00:00.000+0000", "updated": "2012-10-19T22:50:29.000+0000", "description": "Reproduction in the repl:\n\nscala> class T {                                                    \n     | def a(x:Int) = x.toString                                    \n     | sc.parallelize(Array(1,2,3), 2).map(v => a(v))               \n     | }\ndefined class T\n\nscala> class W extends T                                            \ndefined class W\n\nscala> new W\njava.lang.IllegalArgumentException: Can not set final $iwC field T.$outer to $iwC\n\tat sun.reflect.UnsafeFieldAccessorImpl.throwSetIllegalArgumentException(UnsafeFieldAccessorImpl.java:146)\n\tat sun.reflect.UnsafeFieldAccessorImpl.throwSetIllegalArgumentException(UnsafeFieldAccessorImpl.java:150)\n\tat sun.reflect.UnsafeQualifiedObjectFieldAccessorImpl.set(UnsafeQualifiedObjectFieldAccessorImpl.java:65)\n\tat java.lang.reflect.Field.set(Field.java:657)\n\tat spark.ClosureCleaner$.spark$ClosureCleaner$$instantiateClass(ClosureCleaner.scala:116)\n\tat spark.ClosureCleaner$$anonfun$clean$3.apply(ClosureCleaner.scala:79)\n\tat spark.ClosureCleaner$$anonfun$clean$3.apply(ClosureCleaner.scala:78)\n\tat scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)\n\tat scala.collection.immut...\nscala> new T\nres6: T = T@57c52e72\n\nscala> trait A {\n     | def a(x:Int) = x.toString                     \n     | sc.parallelize(Array(1,2,3), 2).map(v => a(v))\n     | }\ndefined trait A\n\nscala> class B extends A\ndefined class B\n\nscala> new B\njava.lang.InstantiationError: A\n\tat sun.reflect.GeneratedSerializationConstructorAccessor55.newInstance(Unknown Source)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\n\tat spark.ClosureCleaner$.spark$ClosureCleaner$$instantiateClass(ClosureCleaner.scala:111)\n\tat spark.ClosureCleaner$$anonfun$clean$3.apply(ClosureCleaner.scala:79)\n\tat spark.ClosureCleaner$$anonfun$clean$3.apply(ClosureCleaner.scala:78)\n\tat scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)\n\tat scala.collection.immutable.List.foreach(List.scala:45)\n\tat spark.ClosureCleaner$.clean(ClosureCleaner.scala:78)\n\tat spark.SparkContext.clean(SparkContext.scala:252)\n\tat spark.RDD.map(RDD.scala:85)\n\tat A$class.$init$(<console>:13)\n\tat B.<init>(<console>:14)\n\tat <init>(<console>:18)\n\tat...", "comments": ["Github comment from tjhunter: The closure cleaner has a number of issues. Meanwhile, you can define the function a in an object, that should work.", "Github comment from ijuma: Maybe there should be a way to disable the closure cleaner then (if there isn't already)?", "Github comment from tjhunter: I think the first task should be to clarify the semantics of what is accepted in the closure and what is not. In particular in this case, the cleaner is right to refuse to accept a method to the calling class (the calling class should not be serialzied since it is usually assumed to be the singleton containing the main function), but the error report is non intuitive at best. If we want to allow *any* scala construct in the closure, we may hit some hard corner cases that may be hard to get to work (this is already the case with some of the bug reports I opened). Eventually, I think the closure cleaner should be replaced by a plugin to the scala compiler, so that the compilation fails if the closure is not proper, but this is another iterm.\nThe workaround in the particular case above is to put the a() method in a separate singleton object.", "Github comment from mateiz: I've closed this issue now with some fixes to the ClosureCleaner that make the sample code work, but the closure cleaner is definitely not a finished project. The core problem is that if you have a class C in which you create a closure that calls methods/fields of C, we need to pass the entire instance of class C to remote nodes along with your closure. Otherwise, we have no way to tell which fields the methods you call will use. Unfortunately, this can lead to more fields being passed along than are really needed (and potentially things like NotSerializableException). Really the way users should be encouraged to work around it is by defining local variables that copy the fields they care about. However, with this fix, they can also use those variables/methods directly as in the example above, provided they also make the class C serializable. This should hopefully be enough warning that the whole of C will be passed along...", "Github comment from mateiz: Forgot to add: Commit 9e4c79a4d39b890b3eb55ffe8cef7d21eb31f0e6 also adds unit tests for the closure cleaner.", "Github comment from tjhunter: Ideally, if someone has some time, a compiler plugin should at least be able to warn you that you are going to build a potentially unsafe closure.", "Imported from Github issue spark-71, originally reported by chrisx"], "derived": {"summary": "Reproduction in the repl:\n\nscala> class T {                                                    \n     | def a(x:Int) = x. toString                                    \n     | sc.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "ClosureCleaner fails when instantiating with outer - Reproduction in the repl:\n\nscala> class T {                                                    \n     | def a(x:Int) = x. toString                                    \n     | sc."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-71, originally reported by chrisx"}]}}
{"project": "SPARK", "issue_id": "SPARK-463", "title": "Kryo serializer sometimes fails to use no-argument constructor", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0011-07-21T14:31:00.000+0000", "updated": "2012-10-22T14:55:34.000+0000", "description": "For example, if you serialize a HashSet, our modified Kryo serializer will not call HashSet's no-arg constructor, leading to a NullPointerException.", "comments": ["Imported from Github issue spark-72, originally reported by mateiz"], "derived": {"summary": "For example, if you serialize a HashSet, our modified Kryo serializer will not call HashSet's no-arg constructor, leading to a NullPointerException.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Kryo serializer sometimes fails to use no-argument constructor - For example, if you serialize a HashSet, our modified Kryo serializer will not call HashSet's no-arg constructor, leading to a NullPointerException."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-72, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-559", "title": "Automatically register all classes used in fields of a class with Kryo", "status": "Closed", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0011-07-21T14:32:00.000+0000", "updated": "2014-09-22T01:41:58.000+0000", "description": null, "comments": ["Github comment from rxin: We should close this ticket since we made kryo registration optional by default in this commit:\n\nhttps://github.com/mesos/spark/commit/968f75f6afc1383692f4f33d6a1a5f8ce2ac951d\n\nWhen we upgrade to Kryo 2.x, registration will also be automatic.", "Imported from Github issue spark-73, originally reported by mateiz", "the last comment on this, from 2 years ago, suggest this is resolved w/ an upgrade to kryo 2.x. i'm going to close this, but please re-open if you disagree.", "As of today in master we're using Twitter Chill version 0.3.6 which includes Kryo 2.21, so we are on the 2.x branch now"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Automatically register all classes used in fields of a class with Kryo"}, {"q": "What updates or decisions were made in the discussion?", "a": "As of today in master we're using Twitter Chill version 0.3.6 which includes Kryo 2.21, so we are on the 2.x branch now"}]}}
{"project": "SPARK", "issue_id": "SPARK-462", "title": "Add APIs to Serializer for streams of objects of the same type", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0011-07-22T23:47:00.000+0000", "updated": "2012-10-22T14:55:33.000+0000", "description": "This could be a substantial optimization for faster serializers, because they wouldn't need to encode (and later decode) the class of each object in the stream repeatedly.", "comments": ["Github comment from mateiz: Done in dev branch.", "Imported from Github issue spark-74, originally reported by mateiz"], "derived": {"summary": "This could be a substantial optimization for faster serializers, because they wouldn't need to encode (and later decode) the class of each object in the stream repeatedly.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add APIs to Serializer for streams of objects of the same type - This could be a substantial optimization for faster serializers, because they wouldn't need to encode (and later decode) the class of each object in the stream repeatedly."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-74, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-461", "title": "Workaround for scalac bug and publishTo configuration", "status": "Resolved", "priority": null, "reporter": "Ismael Juma", "assignee": null, "labels": [], "created": "0011-07-31T11:21:00.000+0000", "updated": "2012-10-19T22:50:23.000+0000", "description": "Both commits are simple and the commit message for each explain what they're about.", "comments": ["Github comment from mateiz: OK, merged. I've also merged the Scala 2.9 branch into master now.", "Github comment from ijuma: Thanks!", "Imported from Github issue spark-75, originally reported by ijuma"], "derived": {"summary": "Both commits are simple and the commit message for each explain what they're about.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Workaround for scalac bug and publishTo configuration - Both commits are simple and the commit message for each explain what they're about."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-75, originally reported by ijuma"}]}}
{"project": "SPARK", "issue_id": "SPARK-460", "title": "Fix code not to use code deprecated in Scala 2.9", "status": "Resolved", "priority": null, "reporter": "Ismael Juma", "assignee": null, "labels": [], "created": "0011-08-02T01:26:00.000+0000", "updated": "2012-10-19T22:50:30.000+0000", "description": "The commits are self-explanatory.", "comments": ["Github comment from mateiz: Awesome, thanks a lot for taking the time to do this!", "Imported from Github issue spark-76, originally reported by ijuma"], "derived": {"summary": "The commits are self-explanatory.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Fix code not to use code deprecated in Scala 2.9 - The commits are self-explanatory."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-76, originally reported by ijuma"}]}}
{"project": "SPARK", "issue_id": "SPARK-459", "title": "Enable -optimize in the build", "status": "Resolved", "priority": null, "reporter": "Ismael Juma", "assignee": null, "labels": [], "created": "0011-08-02T01:29:00.000+0000", "updated": "2012-10-19T22:50:28.000+0000", "description": "The commit is self-explanatory. This includes the deprecation fixes, so that pull request should be merged first.", "comments": ["Imported from Github issue spark-77, originally reported by ijuma"], "derived": {"summary": "The commit is self-explanatory. This includes the deprecation fixes, so that pull request should be merged first.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "Enable -optimize in the build - The commit is self-explanatory. This includes the deprecation fixes, so that pull request should be merged first."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-77, originally reported by ijuma"}]}}
{"project": "SPARK", "issue_id": "SPARK-458", "title": "Replace DepJar with sbt-assembly plugin", "status": "Resolved", "priority": null, "reporter": "Ismael Juma", "assignee": null, "labels": [], "created": "0011-08-02T02:50:00.000+0000", "updated": "2012-10-19T22:50:31.000+0000", "description": "With recent changes to the sbt-assembly plugin we can just use it instead of having our own DepJar class. See:\n\nhttps://github.com/eed3si9n/sbt-assembly/issues/4#issuecomment-1697831", "comments": ["Github comment from mateiz: Done.", "Imported from Github issue spark-78, originally reported by ijuma"], "derived": {"summary": "With recent changes to the sbt-assembly plugin we can just use it instead of having our own DepJar class. See:\n\nhttps://github.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Replace DepJar with sbt-assembly plugin - With recent changes to the sbt-assembly plugin we can just use it instead of having our own DepJar class. See:\n\nhttps://github."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-78, originally reported by ijuma"}]}}
{"project": "SPARK", "issue_id": "SPARK-558", "title": "Simplify run script by relying on sbt to launch app", "status": "Resolved", "priority": null, "reporter": "Ismael Juma", "assignee": null, "labels": [], "created": "0011-08-02T05:15:00.000+0000", "updated": "2014-09-12T08:04:58.000+0000", "description": "The run script replicates SBT's functionality in order to build the classpath.\n\nThis could be avoided by creating a task in sbt that is responsible for calling the appropriate main method, configuring the environment variables from the script and then invoking sbt with the task name and arguments.\n\nIs there a reason why we should not do this?", "comments": ["Github comment from mateiz: This sounds OK to me; my only concern is whether SBT will add a substantial amount of memory usage or startup time, or somehow mess up the classloaders and the classpath. Do you know in more detail what it does when you run a class?", "Github comment from ijuma: Fair points. SBT can either launch the app in the same process (like it does with tests) or it can fork. The latter is safer, but the original SBT process will continue running (and taking memory) until the forked process ends. Maybe we could configure the SBT JVM with a very small heap to minimise that. Still there will always be some overhead, so maybe the current approach is the best compromise even if a bit ugly.", "Github comment from ijuma: An alternative would be to use SBT to simply generate the classpath. A modified version of the following code by Mark Harrah:\n\nTaskKey[Unit](\"mkrun\") <<=\n  (fullClasspath in Runtime,\n    mainClass in (Compile,run),\n    baseDirectory) map {\n  (cp: Classpath,\n    main: Option[String],\n    base: File) =>\n      val mc = main.getOrElse(\"No main class specified\")\n      val script = \"java -cp '\" + cp.files.absString + \"' \" + mc\n      val out = base / \"run\"\n      IO.write(out, script)\n      out.setExecutable(true)\n}\nhttps://groups.google.com/d/msg/simple-build-tool/TEOjVzOwFOE/M5ggCPqjUFsJ", "Github comment from mateiz: That sounds good actually. Instead of writing the \"run\" script directly, we could also write a special file called \"classpath\", so that there can be other stuff in the script (e.g. the code that reads spark-env.sh.", "Github comment from ijuma: I intend to have a look at this once SBT 0.11.0 is out (should be soon).", "Github comment from mateiz: Hey Ismael,\n\nI'm still interested in getting this to work, as the run script is pretty unwieldy. Is there a feature for it in SBT 0.11?", "Github comment from ijuma: There is no specific feature, but the approach we discussed should work. I am swamped, unfortunately, so won't be able to look at this before next month. By the way, SBT 0.11.1 was released today and it has vastly improved memory usage characteristics.", "Github comment from mateiz: No worries; I'll try to implement it myself.", "Imported from Github issue spark-79, originally reported by ijuma", "Is this stale too? given that SBT is less used, I don't imagine the run scripts will start relying on it for classpath generation.", "Probably stale, yes."], "derived": {"summary": "The run script replicates SBT's functionality in order to build the classpath. This could be avoided by creating a task in sbt that is responsible for calling the appropriate main method, configuring the environment variables from the script and then invoking sbt with the task name and arguments.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Simplify run script by relying on sbt to launch app - The run script replicates SBT's functionality in order to build the classpath. This could be avoided by creating a task in sbt that is responsible for calling the appropriate main method, configuring the environment variables from the script and then invoking sbt with the task name and arguments."}, {"q": "What updates or decisions were made in the discussion?", "a": "Probably stale, yes."}]}}
{"project": "SPARK", "issue_id": "SPARK-457", "title": "Delete scala-2.9 branch", "status": "Resolved", "priority": null, "reporter": "Ismael Juma", "assignee": null, "labels": [], "created": "0011-08-03T00:31:00.000+0000", "updated": "2012-10-19T22:50:22.000+0000", "description": "The scala-2.9 branch has now been fully merged and can be deleted from the repository.", "comments": ["Github comment from mateiz: Done.", "Imported from Github issue spark-80, originally reported by ijuma"], "derived": {"summary": "The scala-2. 9 branch has now been fully merged and can be deleted from the repository.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Delete scala-2.9 branch - The scala-2. 9 branch has now been fully merged and can be deleted from the repository."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-80, originally reported by ijuma"}]}}
{"project": "SPARK", "issue_id": "SPARK-456", "title": "Save RDDs should work for s3n paths", "status": "Resolved", "priority": null, "reporter": "Ismael Juma", "assignee": null, "labels": [], "created": "0011-08-19T03:09:00.000+0000", "updated": "2012-10-19T22:50:25.000+0000", "description": "I tried a few variations and I could not get this to work. And since this was not tested (as mentioned here https://github.com/mesos/spark/pull/64#issuecomment-1721647), it's possible that it's a bug indeed.\n\nWhen I use a path like s3n://<access_key>:<secret_key>@<bucket_name>, I get a IllegalArgumentException caused by a java.net.URISyntaxException[1].\n\nIf I instead use a path like s3n://<access_key>:<secret_key>@<host>/<bucket_name>, I get a org.apache.hadoop.fs.s3.S3Exception caused by a org.jets3t.service.S3ServiceException[2].\n\nYou should be able to reproduce this by simply changing your test case to output to s3n instead of HDFS. Let me know if you have questions.\n\n[1] Exception in thread \"main\" java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: s3n://access_key:secret_key@bucket_name_temporary\n\tat org.apache.hadoop.fs.Path.initialize(Path.java:148)\n\tat org.apache.hadoop.fs.Path.<init>(Path.java:71)\n\tat org.apache.hadoop.fs.Path.<init>(Path.java:50)\n\tat org.apache.hadoop.mapred.FileOutputCommitter.setupJob(FileOutputCommitter.java:50)\n\tat org.apache.hadoop.mapred.HadoopWriter.preSetup(HadoopWriter.scala:44)\n\tat spark.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:243)\n        [...]\nCaused by: java.net.URISyntaxException: Relative path in absolute URI: s3n://access_key:secret_key@bucket_name_temporary\n\tat java.net.URI.checkPath(URI.java:1802)\n\tat java.net.URI.<init>(URI.java:752)\n\tat org.apache.hadoop.fs.Path.initialize(Path.java:145)\n\t... 14 more\n\n[2] Exception in thread \"main\" org.apache.hadoop.fs.s3.S3Exception: org.jets3t.service.S3ServiceException: Request Error. HEAD '/bucket_name' on Host 's3-eu-west-1.amazonaws.com.s3.amazonaws.com' @ 'Fri, 19 Aug 2011 11:03:22 GMT' -- ResponseCode: 404, ResponseStatus: Not Found, RequestId: 000B648BEF2C9519, HostId: 6ROVsyivgLBAceNLaoj5sv8uOvx9uY33RdsXeKoOuNCUTbY0QaM8yFB+cSojps/k\n\tat org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.handleServiceException(Jets3tNativeFileSystemStore.java:229)\n\tat org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.retrieveMetadata(Jets3tNativeFileSystemStore.java:111)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:601)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)\n\tat org.apache.hadoop.fs.s3native.$Proxy0.retrieveMetadata(Unknown Source)\n\tat org.apache.hadoop.fs.s3native.NativeS3FileSystem.getFileStatus(NativeS3FileSystem.java:392)\n\tat org.apache.hadoop.fs.s3native.NativeS3FileSystem.mkdir(NativeS3FileSystem.java:505)\n\tat org.apache.hadoop.fs.s3native.NativeS3FileSystem.mkdirs(NativeS3FileSystem.java:498)\n\tat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1226)\n\tat org.apache.hadoop.mapred.FileOutputCommitter.setupJob(FileOutputCommitter.java:52)\n\tat org.apache.hadoop.mapred.HadoopWriter.preSetup(HadoopWriter.scala:44)\n\tat spark.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:243)\n\t[...]\nCaused by: org.jets3t.service.S3ServiceException: Request Error. HEAD '/bucket_name' on Host 's3-eu-west-1.amazonaws.com.s3.amazonaws.com' @ 'Fri, 19 Aug 2011 11:03:22 GMT' -- ResponseCode: 404, ResponseStatus: Not Found, RequestId: 000B648BEF2C9519, HostId: 6ROVsyivgLBAceNLaoj5sv8uOvx9uY33RdsXeKoOuNCUTbY0QaM8yFB+cSojps/k\n\tat org.jets3t.service.impl.rest.httpclient.RestS3Service.performRequest(RestS3Service.java:520)\n\tat org.jets3t.service.impl.rest.httpclient.RestS3Service.performRestHead(RestS3Service.java:868)\n\tat org.jets3t.service.impl.rest.httpclient.RestS3Service.getObjectImpl(RestS3Service.java:2016)\n\tat org.jets3t.service.impl.rest.httpclient.RestS3Service.getObjectDetailsImpl(RestS3Service.java:1944)\n\tat org.jets3t.service.S3Service.getObjectDetails(S3Service.java:3059)\n\tat org.jets3t.service.S3Service.getObjectDetails(S3Service.java:1940)\n\tat org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.retrieveMetadata(Jets3tNativeFileSystemStore.java:103)\n\t... 23 more\nCaused by: org.jets3t.service.impl.rest.HttpException\n\tat org.jets3t.service.impl.rest.httpclient.RestS3Service.performRequest(RestS3Service.java:518)\n\t... 29 more", "comments": ["Github comment from mateiz: I haven't had a chance to test this yet, but here are a couple of pointers:\n\n1) According to https://issues.apache.org/jira/browse/HADOOP-5805, you might need to use a subdirectory rather than a top-level bucket. In particular, Hadoop's S3 library won't create the bucket if it doesn't exist.\n\n2) I'm not sure whether passing your credentials with @ is supposed to work in S3N URLs (it might well work), but if it doesn't, you can also try the versions of the save() method that take a JobConf and set your credentials in there through fs.s3.awsAccessKeyId and fs.s3.awsSecretAccessKey. Check out http://wiki.apache.org/hadoop/AmazonS3.", "Github comment from ijuma: Hi Matei. The bucket does exist and the syntax I used works fine for InputFormat. I will try to use a subfolder, but from a brief investigation it seemed the usage of OutputCommitter was the issue.", "Github comment from ijuma: I had a quick go at using a subfolder and that didn't help.", "Github comment from mateiz: OK, I'll try to take a look at this, but I'm not sure I'll have time in the next couple of days due to some other stuff I need to finish.", "Github comment from ijuma: Sure, there's no rush. I had written some code to upload to S3 before the OutputFormat code went in and I can continue to use that until you have a chance to look at this.", "Github comment from ijuma: Any progress on this?", "Github comment from mateiz: Sorry, haven't had a chance to look at it yet. I'll try to do that this weekend.", "Github comment from mateiz: Hi Ismael,\n\nSorry for the delay on this, but I think I figured out the problem. Saving with s3n:// URIs works fine unless your access key ID or secret access key contain a \"/\" character, in which case you get the error you had above. This is because the S3 code in Hadoop tries to get a hostname from the URI and fails (presumably passwords encoded into URLs in the :@ format can't have slashes). I figured this out by creating new access keys until I got one without a slash in it.\n\nOne way around this is to use the saveAsHadoopFile method and pass it a JobConf, but that's really ugly. For example:\n\n    val data = sc.parallelize(1 to 10)\n    val conf = new JobConf\n    conf.set(\"fs.s3n.awsAccessKeyId\", \"XXX\")\n    conf.set(\"fs.s3n.awsSecretAccessKey\", \"YYY\")\n    val mapped = data.map(x => (NullWritable.get(), new Text(x.toString)))\n    mapped.saveAsHadoopFile(\"s3n://matei-test/foo\", classOf[NullWritable], classOf[Text], classOf[TextOutputFormat[_, _]], conf)\n\nI'm also thinking of two other ways around it:\n* Give all the saveAsX() methods an optional JobConf parameter.\n* Give SparkContext's constructor an optional JobConf parameter, which will be used for all save operations in it.\n\nAny preferences?", "Github comment from ijuma: Thanks for looking into this and for the workaround.\n\nGood news that it's easy to solve. It's a bit weird because I also went through the same process of generating keys until I got one with no '/' in it. If it works for you though, then it looks like I probably had another issue in my code.\n\nAbout your question, I prefer the option of passing the optional JobConf to saveAsX methods.", "Github comment from mateiz: Maybe you had some other weird character in the new keys that was causing a similar problem. I just know that mine worked eventually. In any case, I'll add those JobConf parameters and write somewhere that that is the preferred way to use S3.", "Imported from Github issue spark-81, originally reported by ijuma"], "derived": {"summary": "I tried a few variations and I could not get this to work. And since this was not tested (as mentioned here https://github.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Save RDDs should work for s3n paths - I tried a few variations and I could not get this to work. And since this was not tested (as mentioned here https://github."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-81, originally reported by ijuma"}]}}
{"project": "SPARK", "issue_id": "SPARK-455", "title": "Upgrade to Scala 2.9.1.", "status": "Resolved", "priority": null, "reporter": "Ismael Juma", "assignee": null, "labels": [], "created": "0011-08-31T02:10:00.000+0000", "updated": "2012-10-19T22:50:23.000+0000", "description": "Scala 2.9.1 has been released and it includes a lot of improvements, including faster compilation:\n\nhttp://www.scala-lang.org/node/10780\n\nA couple of small changes in the REPL support were needed in addition to the changes in the build file and run script. Please review them to see if they're correct.\n\nAll tests pass and ./spark-shell launches successfully.", "comments": ["Github comment from mateiz: Looks good, thanks!", "Imported from Github issue spark-82, originally reported by ijuma"], "derived": {"summary": "Scala 2. 9.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Upgrade to Scala 2.9.1. - Scala 2. 9."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-82, originally reported by ijuma"}]}}
{"project": "SPARK", "issue_id": "SPARK-454", "title": "Upgrade to SBT 0.11.0", "status": "Resolved", "priority": null, "reporter": "Ismael Juma", "assignee": null, "labels": [], "created": "0011-09-27T14:18:00.000+0000", "updated": "2012-10-19T22:50:28.000+0000", "description": "Upgrade to SBT 0.11.0 and update a couple of dependencies (Jetty and compress-lzf). SBT 0.11.0 was released recently and it includes a number of improvements:\n\nRelease announcement: https://groups.google.com/d/topic/simple-build-tool/PiRxuWiuyic/discussion\nChanges: http://implicit.ly/simple-build-tool-0110", "comments": ["Github comment from mateiz: Thanks for the patch!", "Imported from Github issue spark-83, originally reported by ijuma"], "derived": {"summary": "Upgrade to SBT 0. 11.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Upgrade to SBT 0.11.0 - Upgrade to SBT 0. 11."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-83, originally reported by ijuma"}]}}
{"project": "SPARK", "issue_id": "SPARK-453", "title": "Report exceptions in tasks back to master to simplify debugging", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0011-10-14T09:50:00.000+0000", "updated": "2012-10-22T14:55:32.000+0000", "description": null, "comments": ["Github comment from tjhunter: +1", "Github comment from mateiz: This is fixed now.", "Imported from Github issue spark-84, originally reported by mateiz"], "derived": {"summary": "", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Report exceptions in tasks back to master to simplify debugging"}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-84, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-452", "title": "Wrong documentation for Mesos+Spark integration", "status": "Resolved", "priority": null, "reporter": "Yury Litvinov", "assignee": null, "labels": [], "created": "0011-10-17T04:03:00.000+0000", "updated": "2012-10-19T22:50:30.000+0000", "description": "It's not possible to build Mesos for Spark using instruction from\nhttps://github.com/mesos/spark/wiki/Running-spark-on-mesos\n\nParticularly, that\n```git checkout -b pre-protobuf --track origin/pre-protobuf```\nresults in \n```fatal: Not a git repository (or any of the parent directories): .git```", "comments": ["Github comment from mateiz: Thanks for reporting this. I think the issue is that you must do `cd mesos` to move into the directory you cloned into before calling `git checkout`. Try that out.", "Github comment from mateiz: By the way, those instructions will only work for release 0.3 and earlier (the versions of Spark you can download from spark-project.org). If you want to work with the master branch of Spark, you will need to use the master branch of Mesos (so just don't do the `git checkout`, but do the `git clone`).", "Github comment from ylitvinov: Thank you. I've tried\n```git checkout -b pre-protobuf --track origin/pre-protobuf```\nand it resulted in\n```Branch pre-protobuf set up to track remote branch pre-protobuf from origin.\nSwitched to a new branch 'pre-protobuf'```\n\nis it expected output?", "Github comment from mateiz: Yes, that looks good. You should be able to build Mesos now.", "Github comment from mateiz: I'm going to close this issue because I fixed the docs for this.", "Imported from Github issue spark-85, originally reported by ylitvinov"], "derived": {"summary": "It's not possible to build Mesos for Spark using instruction from\nhttps://github. com/mesos/spark/wiki/Running-spark-on-mesos\n\nParticularly, that\n```git checkout -b pre-protobuf --track origin/pre-protobuf```\nresults in \n```fatal: Not a git repository (or any of the parent directories):.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Wrong documentation for Mesos+Spark integration - It's not possible to build Mesos for Spark using instruction from\nhttps://github. com/mesos/spark/wiki/Running-spark-on-mesos\n\nParticularly, that\n```git checkout -b pre-protobuf --track origin/pre-protobuf```\nresults in \n```fatal: Not a git repository (or any of the parent directories):."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-85, originally reported by ylitvinov"}]}}
{"project": "SPARK", "issue_id": "SPARK-557", "title": "Provide scripts for launching Spark through Amazon Elastic MapReduce", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0011-10-17T12:24:00.000+0000", "updated": "2013-04-05T20:27:17.000+0000", "description": "We talked with some Amazon folks a while back that said this should be possible through the \"bootstrap actions\" they provide. It would make it very easy to scale clusters up/down, use spot instances, etc.", "comments": ["Imported from Github issue spark-86, originally reported by mateiz", "Amazon released a tutorial, \"Run Spark and Shark on Amazon Elastic MapReduce\", that does this:\n\nhttp://aws.amazon.com/articles/4926593393724923"], "derived": {"summary": "We talked with some Amazon folks a while back that said this should be possible through the \"bootstrap actions\" they provide. It would make it very easy to scale clusters up/down, use spot instances, etc.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Provide scripts for launching Spark through Amazon Elastic MapReduce - We talked with some Amazon folks a while back that said this should be possible through the \"bootstrap actions\" they provide. It would make it very easy to scale clusters up/down, use spot instances, etc."}, {"q": "What updates or decisions were made in the discussion?", "a": "Amazon released a tutorial, \"Run Spark and Shark on Amazon Elastic MapReduce\", that does this:\n\nhttp://aws.amazon.com/articles/4926593393724923"}]}}
{"project": "SPARK", "issue_id": "SPARK-451", "title": "Ranges of Longs don't get split properly by SparkContext.parallelize", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": ["Starter"], "created": "0011-11-02T09:49:00.000+0000", "updated": "2013-10-10T18:00:04.000+0000", "description": "We wanted Ranges to be split into smaller Range objects to avoid shipping the integers in them over the network, but Scala 2.9's Long ranges don't get matched as a Range by the ParallelCollection code and thus get converted to arrays, costing a lot of RAM.", "comments": ["Imported from Github issue spark-87, originally reported by mateiz"], "derived": {"summary": "We wanted Ranges to be split into smaller Range objects to avoid shipping the integers in them over the network, but Scala 2. 9's Long ranges don't get matched as a Range by the ParallelCollection code and thus get converted to arrays, costing a lot of RAM.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Ranges of Longs don't get split properly by SparkContext.parallelize - We wanted Ranges to be split into smaller Range objects to avoid shipping the integers in them over the network, but Scala 2. 9's Long ranges don't get matched as a Range by the ParallelCollection code and thus get converted to arrays, costing a lot of RAM."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-87, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-450", "title": "Implement a sort() operation / RDD", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0011-11-02T09:50:00.000+0000", "updated": "2012-10-22T14:55:31.000+0000", "description": null, "comments": ["Github comment from shawny: Re, i like it.", "Github comment from lgiorda: I would love this.", "Github comment from mateiz: We're working on it -- should be in soon!", "Imported from Github issue spark-88, originally reported by mateiz"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Implement a sort() operation / RDD"}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-88, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-449", "title": "Tab completion doesn't work in REPL with Scala 2.9.1", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0011-11-02T09:51:00.000+0000", "updated": "2012-10-22T14:55:37.000+0000", "description": "This will either be an easy fix or a really hard one.. need to look at what changed in the REPL between 2.9.0.1 and 2.9.1.", "comments": ["Imported from Github issue spark-89, originally reported by mateiz"], "derived": {"summary": "This will either be an easy fix or a really hard one. need to look at what changed in the REPL between 2.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Tab completion doesn't work in REPL with Scala 2.9.1 - This will either be an easy fix or a really hard one. need to look at what changed in the REPL between 2."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-89, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-448", "title": "Use Akka actors for communication", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0011-11-07T19:51:00.000+0000", "updated": "2012-10-22T14:55:31.000+0000", "description": "This might help avoid one annoying issue with Scala actors, which is that there's no way to just bind to a free port and figure out the port you bound to. (Instead, you must guess a free port number and call alive(port number), and there is no error if that port number in taken.)\n\nIn addition, it could provide some performance benefits, though I'd have to test this.", "comments": ["Github comment from mateiz: Done in dev branch, although we will likely switch away from Akka.", "Imported from Github issue spark-90, originally reported by mateiz"], "derived": {"summary": "This might help avoid one annoying issue with Scala actors, which is that there's no way to just bind to a free port and figure out the port you bound to. (Instead, you must guess a free port number and call alive(port number), and there is no error if that port number in taken.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Use Akka actors for communication - This might help avoid one annoying issue with Scala actors, which is that there's no way to just bind to a free port and figure out the port you bound to. (Instead, you must guess a free port number and call alive(port number), and there is no error if that port number in taken."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-90, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-447", "title": "Bagel unit tests broken after API change", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0011-11-07T23:08:00.000+0000", "updated": "2012-10-22T14:55:35.000+0000", "description": "It seems like the BagelSuite test doesn't compile, but this is hard to notice because SBT doesn't try to compile it when you aren't running a test. To reproduce it you can run sbt/sbt test (maybe typing project bagel first to limit its scope to that).", "comments": ["Github comment from ankurdave: Fixed by c5be7d2b2268e44e3eafb460d4bf0fb0badf9b22.", "Github comment from mateiz: Great, thanks!", "Imported from Github issue spark-91, originally reported by mateiz"], "derived": {"summary": "It seems like the BagelSuite test doesn't compile, but this is hard to notice because SBT doesn't try to compile it when you aren't running a test. To reproduce it you can run sbt/sbt test (maybe typing project bagel first to limit its scope to that).", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Bagel unit tests broken after API change - It seems like the BagelSuite test doesn't compile, but this is hard to notice because SBT doesn't try to compile it when you aren't running a test. To reproduce it you can run sbt/sbt test (maybe typing project bagel first to limit its scope to that)."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-91, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-446", "title": "Spark doesn't return offers if it doesn't have any tasks for them", "status": "Resolved", "priority": null, "reporter": "woggling", "assignee": null, "labels": [], "created": "0011-11-08T18:51:00.000+0000", "updated": "2012-10-19T22:50:24.000+0000", "description": "When Spark gets a resource offer for which it cannot launch a task, it never calls launchTask() for that offer. With the current (Apache trunk) implementation of Mesos, this appears to leave the offer pending rather than returning it so Spark can get more offers.", "comments": ["Github comment from mateiz: Ah, this explains some weird stuff that Eemil (a visitor from Sweden) was seeing. I'll fix it momentarily.", "Github comment from mateiz: Commit 07532021fee9e2d27ee954b21c30830687478d8b hopefully fixes this.", "Imported from Github issue spark-92, originally reported by woggling"], "derived": {"summary": "When Spark gets a resource offer for which it cannot launch a task, it never calls launchTask() for that offer. With the current (Apache trunk) implementation of Mesos, this appears to leave the offer pending rather than returning it so Spark can get more offers.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Spark doesn't return offers if it doesn't have any tasks for them - When Spark gets a resource offer for which it cannot launch a task, it never calls launchTask() for that offer. With the current (Apache trunk) implementation of Mesos, this appears to leave the offer pending rather than returning it so Spark can get more offers."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-92, originally reported by woggling"}]}}
{"project": "SPARK", "issue_id": "SPARK-445", "title": "Add a version of sample() that returns a fixed number of elements", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0011-11-17T13:57:00.000+0000", "updated": "2012-10-22T14:55:36.000+0000", "description": "Right now you can only sample an RDD by passing a fraction of elements to preserve. It would be nice to have a way to just pick exactly K elements out of it and return it as an array. This can be called collectSample().", "comments": ["Github comment from holdenk: I've added a collectSample() function (see pull request #107).", "Github comment from mateiz: Hey Holden,\n\nUnfortunately we recently committed very similar functionality called takeSample in https://github.com/mesos/spark/pull/98. Does collectSample do anything extra beyond that? If so maybe we can merge the two.", "Github comment from holdenk: I think my implementation is a bit more efficient, takeSample may re-sample\nthe same array repeatedly until it gets a sample of at least size k (\nhttps://github.com/edisontung/spark/commit/a3bc012af8a4f7c362a28bc1294942019d5a288d\nline\n113), whereas collectSample does a single pass through. I can update my\ncommit to replace takeSample rather than have a different name?\n\nOn Wed, Jan 25, 2012 at 1:42 PM, Matei Zaharia <\nreply@reply.github.com\n> wrote:\n\n> Hey Holden,\n>\n> Unfortunately we recently committed very similar functionality called\n> takeSample in https://github.com/mesos/spark/pull/98. Does collectSample\n> do anything extra beyond that? If so maybe we can merge the two.\n>\n> ---\n> Reply to this email directly or view it on GitHub:\n> https://github.com/mesos/spark/issues/93#issuecomment-3659689\n>\n\n\n\n-- \nCell : 425-233-8271", "Github comment from mateiz: Sure, that would be great.", "Github comment from holdenk: Cool, I'll update my commit tonight :)\n\nOn Wed, Jan 25, 2012 at 2:12 PM, Matei Zaharia <\nreply@reply.github.com\n> wrote:\n\n> Sure, that would be great.\n>\n> ---\n> Reply to this email directly or view it on GitHub:\n> https://github.com/mesos/spark/issues/93#issuecomment-3660242\n>\n\n\n\n-- \nCell : 425-233-8271", "Github comment from holdenk: I've updated my pull request :)", "Imported from Github issue spark-93, originally reported by mateiz"], "derived": {"summary": "Right now you can only sample an RDD by passing a fraction of elements to preserve. It would be nice to have a way to just pick exactly K elements out of it and return it as an array.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add a version of sample() that returns a fixed number of elements - Right now you can only sample an RDD by passing a fraction of elements to preserve. It would be nice to have a way to just pick exactly K elements out of it and return it as an array."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-93, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-556", "title": "Results from tasks should be returned through our own sockets rather than Mesos framework messages", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": "Kay Ousterhout", "labels": [], "created": "0011-11-19T14:56:00.000+0000", "updated": "2013-10-10T18:07:14.000+0000", "description": "With large results, the Mesos library might not like sending large framework messages. We can avoid this by sending the results to the master through a socket rather than through sendFrameworkMessage.\n\nAnother thing that would help is warning the user in this case.", "comments": ["Imported from Github issue spark-94, originally reported by mateiz"], "derived": {"summary": "With large results, the Mesos library might not like sending large framework messages. We can avoid this by sending the results to the master through a socket rather than through sendFrameworkMessage.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Results from tasks should be returned through our own sockets rather than Mesos framework messages - With large results, the Mesos library might not like sending large framework messages. We can avoid this by sending the results to the master through a socket rather than through sendFrameworkMessage."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-94, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-444", "title": "Add an accumulating fold operator", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0011-11-23T18:05:00.000+0000", "updated": "2012-10-22T14:55:32.000+0000", "description": "Often when reducing we want to add up objects of some type (e.g. vectors), in which case merging them with a sum function as in reduce(_ + _) is inefficient because it creates a new object each time. It would be nice to have a fold operator taking a \"zero value\" and a \"merge\" function that takes the result so far and adds an element onto it.", "comments": ["Imported from Github issue spark-95, originally reported by mateiz"], "derived": {"summary": "Often when reducing we want to add up objects of some type (e. g.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add an accumulating fold operator - Often when reducing we want to add up objects of some type (e. g."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-95, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-555", "title": "Write a \"Spark internals\" wiki page", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": "Josh Rosen", "labels": [], "created": "0011-11-23T18:06:00.000+0000", "updated": "2013-12-07T12:56:44.000+0000", "description": "This would help new developers jump into the codebase.", "comments": ["Github comment from shawny: good news", "Github comment from tsforce: +1", "Imported from Github issue spark-96, originally reported by mateiz", "The slides from the \"Introduction to Spark Internals\" talk (https://www.youtube.com/watch?v=49Hr5xZyTEA) probably serve as a good introduction to the internals.", "We now have a Spark Internals page on the Apache CWiki: https://cwiki.apache.org/confluence/display/SPARK/Spark+Internals"], "derived": {"summary": "This would help new developers jump into the codebase.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Write a \"Spark internals\" wiki page - This would help new developers jump into the codebase."}, {"q": "What updates or decisions were made in the discussion?", "a": "We now have a Spark Internals page on the Apache CWiki: https://cwiki.apache.org/confluence/display/SPARK/Spark+Internals"}]}}
{"project": "SPARK", "issue_id": "SPARK-554", "title": "Add aggregateByKey", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": "Sandy Ryza", "labels": [], "created": "0011-11-30T11:40:00.000+0000", "updated": "2014-06-12T15:15:34.000+0000", "description": "Similar to the new fold() and aggregate() methods in #95, we should have foldByKey and aggregateByKey for pair RDDs. The main thing that makes this slightly harder is that we'll have to change the combineByKey API and ShuffledRDD to allow taking in a \"zero value\".", "comments": ["Imported from Github issue spark-97, originally reported by mateiz", "foldByKey() was added in https://github.com/mesos/spark/pull/526.", "I've run into a couple situations where aggregateByKey would have been really helpful - I'm going to have a go at adding it.", "https://github.com/apache/spark/pull/705", "Fixed in:\nhttps://github.com/apache/spark/pull/705"], "derived": {"summary": "Similar to the new fold() and aggregate() methods in #95, we should have foldByKey and aggregateByKey for pair RDDs. The main thing that makes this slightly harder is that we'll have to change the combineByKey API and ShuffledRDD to allow taking in a \"zero value\".", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add aggregateByKey - Similar to the new fold() and aggregate() methods in #95, we should have foldByKey and aggregateByKey for pair RDDs. The main thing that makes this slightly harder is that we'll have to change the combineByKey API and ShuffledRDD to allow taking in a \"zero value\"."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed in:\nhttps://github.com/apache/spark/pull/705"}]}}
{"project": "SPARK", "issue_id": "SPARK-443", "title": "Added two examples. also added a method to RDD.scala", "status": "Resolved", "priority": null, "reporter": "edisontung", "assignee": null, "labels": [], "created": "0011-11-30T23:20:00.000+0000", "updated": "2012-10-19T22:50:34.000+0000", "description": "Added LocalKMeans and SparkLocalKMeans which are examples that compute K-Means clusters.\n\nMethod added to RDD.scala is takeSamples, which returns a set number of samples in an array (the number is specified as an argument to the method.", "comments": ["Imported from Github issue spark-98, originally reported by edisontung"], "derived": {"summary": "Added LocalKMeans and SparkLocalKMeans which are examples that compute K-Means clusters. Method added to RDD.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Added two examples. also added a method to RDD.scala - Added LocalKMeans and SparkLocalKMeans which are examples that compute K-Means clusters. Method added to RDD."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-98, originally reported by edisontung"}]}}
{"project": "SPARK", "issue_id": "SPARK-553", "title": "Write a tutorial for mining Wikipedia interactively on EC2", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0011-12-02T21:34:00.000+0000", "updated": "2013-05-05T11:40:42.000+0000", "description": "Would be a great way to get people started with Spark. It should basically let you do the following: http://www.youtube.com/user/iammatei#p/a/u/0/Jw9yNTJI8iM", "comments": ["Github comment from tjhunter: +1\n(Cool demo by the way)", "Github comment from tjhunter: Clustering on a WP dump is our current CS294 assignment :)\nI will write a tutorial and put the code online.", "Github comment from dennybritz: Have you gotten around to working on this yet? If not let me know and I'll write it up!", "Github comment from tjhunter: I started a tutorial on some geo tagged data. The idea is exactly the same as in the wikipedia demo, though.\nYou can check it out here:\nhttps://github.com/tjhunter/spark-geo-tutorial\nhttps://github.com/tjhunter/spark-geo-tutorial/blob/master/src/main/doc/tutorial.md\nIt should work once I figure out a way to make the dataset on S3 publicicly available. Do you know how to do it?", "Github comment from dennybritz: That's a cool tutorial. It should be linked from the Spark Wiki. Have you tried setting the permissions on the s3 files? In the S3 Console add \"Download/Open\" for everyone. \n\nEDIT: Also, make sure you give bucket listing permissions for everyone if you're using the wildcard in the input url.", "Github comment from tjhunter: I changed the ACL on the bucket and updated the tutorial. Do you want to give it a try?\nThe tutorial talks about caching, broadcast, map, reduce aggregate. Anything else that can easily be included? Feel free to send pull requests :-)", "Github comment from dennybritz: Just tried it out, I'm getting `org.apache.hadoop.fs.s3.S3Exception: org.jets3t.service.S3ServiceException: S3 HEAD request failed for '/2009-10-9.txt' - ResponseCode=403, ResponseMessage=Forbidden` for the S3 files. The directory listing permissions seem to be correct, but the individual file permissions not?", "Github comment from tjhunter: Fixed ACLs for the files.", "Github comment from mateiz: Tim, is this something we should link to from the Spark wiki? (That is, it is it more or less finished?) If so you can actually edit the wiki to add it, or I can do it for you. We can maybe add a Tutorials section on the main page, and another link at the bottom of the programming guide.", "Imported from Github issue spark-99, originally reported by mateiz", "The AMP Camp training materials have examples of mining Wikipedia: http://ampcamp.berkeley.edu/big-data-mini-course/data-exploration-using-spark.html"], "derived": {"summary": "Would be a great way to get people started with Spark. It should basically let you do the following: http://www.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Write a tutorial for mining Wikipedia interactively on EC2 - Would be a great way to get people started with Spark. It should basically let you do the following: http://www."}, {"q": "What updates or decisions were made in the discussion?", "a": "The AMP Camp training materials have examples of mining Wikipedia: http://ampcamp.berkeley.edu/big-data-mini-course/data-exploration-using-spark.html"}]}}
{"project": "SPARK", "issue_id": "SPARK-442", "title": "Add logging/notification when RDDs are evicted from cache", "status": "Resolved", "priority": null, "reporter": "Reynold Xin", "assignee": null, "labels": [], "created": "0011-12-07T01:47:00.000+0000", "updated": "2012-10-19T22:50:31.000+0000", "description": "As of now, there is no way for user and developer to know what RDDs are in the cache vs what have been evicted. It'd be useful to add some logging capabilities or even notification callbacks. This should be doable with reference queues.", "comments": ["Github comment from tjhunter: Is there a strong use case for callbacks? I though the caching was supposed to be transparent to the user.", "Github comment from rxin: Logging is higher priority for debugging. As for callbacks, you could imagine higher level applications built on top of Spark might like to provide its own feedback to power users.", "Github comment from mateiz: This is fixed in the code now (at least logging is there).", "Imported from Github issue spark-100, originally reported by rxin"], "derived": {"summary": "As of now, there is no way for user and developer to know what RDDs are in the cache vs what have been evicted. It'd be useful to add some logging capabilities or even notification callbacks.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add logging/notification when RDDs are evicted from cache - As of now, there is no way for user and developer to know what RDDs are in the cache vs what have been evicted. It'd be useful to add some logging capabilities or even notification callbacks."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-100, originally reported by rxin"}]}}
{"project": "SPARK", "issue_id": "SPARK-441", "title": "Update documentation for building with Scala 2.8", "status": "Resolved", "priority": null, "reporter": "Andy Konwinski", "assignee": null, "labels": [], "created": "0011-12-15T12:22:00.000+0000", "updated": "2012-10-19T22:50:27.000+0000", "description": "https://github.com/mesos/spark/wiki currently says under \"Building\" subheading: \"Spark requires Scala 2.9 or 2.8.\"\n\n... and then later: \"Once you have set up Scala, run sbt/sbt compile (in the scala-2.8 branch, sbt/sbt update compile) in the top-level Spark directory to build Spark. (Spark uses Simple Build Tool, which is bundled with it.)\"\n\nIt would be very helpful if the documentation more explicitly laid out the slightly different instructions you should follow for building with Scala 2.8 vs. 2.9. \n\nMaybe that final paragraph under \"Build\" can be reworked to be:\n\nOnce you have set up Scala, in the top-level Spark directory....\n\nIf you are using Scala 2.8.*:\n1. Check out the scala-2.8 git branch, by running: `git checkout -b scala-2.8 --track origin/scala-2.8`\n2. Update and compile, by running: `sbt/sbt update compile`\n3. Consider upgrading to Scala 2.9 to access the newest features of Spark! :)\n\nIf you are using Scala 2.9.*:\n1. Compile, by running: sbt/sbt compile\n\nNote: as you see above, Spark uses Simple Build Tool, which is bundled with it.", "comments": ["Github comment from lagerspetz: Sounds like a good idea. Check the wiki now.", "Github comment from andyk: Looks good to me!", "Imported from Github issue spark-101, originally reported by andyk"], "derived": {"summary": "https://github. com/mesos/spark/wiki currently says under \"Building\" subheading: \"Spark requires Scala 2.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Update documentation for building with Scala 2.8 - https://github. com/mesos/spark/wiki currently says under \"Building\" subheading: \"Spark requires Scala 2."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-101, originally reported by andyk"}]}}
{"project": "SPARK", "issue_id": "SPARK-440", "title": "Why \"./run spark.examples.SparkPi 1@localhost:5050\" can't get result of Pi", "status": "Resolved", "priority": null, "reporter": "songhe", "assignee": null, "labels": [], "created": "0011-12-16T07:10:00.000+0000", "updated": "2012-10-19T22:50:21.000+0000", "description": "I setup mesos on local machine:\n\n{\n1000     29724  0.0  0.4  49556  9728 pts/0    Sl   21:44   0:02 ./bin mesos-master \n1000     29774  0.0  0.4  48864  8756 pts/0    Sl   21:44   0:01 ./bin mesos-slave --resources=cpus:4;mem:916 -m mesos://master@localhost:5050\n}\n\nWhen i run \"./run spark.examples.SparkPi 1@localhost:5050\", it will stop at the following last line: \n\n11/12/16 23:04:45 INFO CacheTrackerActor: Registered actor on port 50501\n11/12/16 23:04:45 INFO MapOutputTrackerActor: Registered actor on port 50501\n11/12/16 23:04:45 INFO MesosScheduler: Temp directory for JARs: /tmp/spark-d10f5f39-4d21-4062-ac67-7e27ebd15794\n11/12/16 23:04:46 INFO MesosScheduler: JAR server started at http://127.0.0.1:40518\n11/12/16 23:04:46 INFO SparkContext: Starting job...\n11/12/16 23:04:46 INFO CacheTracker: Registering RDD ID 0 with cache\n11/12/16 23:04:46 INFO CacheTrackerActor: Registering RDD 0 with 2 partitions\n11/12/16 23:04:46 INFO CacheTrackerActor: Asked for current cache locations\n11/12/16 23:04:46 INFO MesosScheduler: Final stage: Stage 0\n11/12/16 23:04:46 INFO MesosScheduler: Parents of final stage: List()\n11/12/16 23:04:46 INFO MesosScheduler: Missing parents: List()\n11/12/16 23:04:46 INFO MesosScheduler: Submitting Stage 0, which has no missing parents\n11/12/16 23:04:46 INFO MesosScheduler: Got a job with 2 tasks\n\n\nBut when i run \"./run spark.examples.SparkPi mesos://master@localhost:5050\", i can get the result of Pi:\n\n11/12/16 22:57:44 INFO MapOutputTrackerActor: Registered actor on port 50501\n11/12/16 22:57:44 INFO CacheTrackerActor: Registered actor on port 50501\n11/12/16 22:57:44 INFO MesosScheduler: Temp directory for JARs: /tmp/spark-f173b70d-bf22-4dc7-a1c5-c241a67af3dc\n11/12/16 22:57:45 INFO MesosScheduler: JAR server started at http://127.0.0.1:40327\n11/12/16 22:57:45 INFO SparkContext: Starting job...\n11/12/16 22:57:45 INFO CacheTracker: Registering RDD ID 0 with cache\n11/12/16 22:57:45 INFO CacheTrackerActor: Registering RDD 0 with 2 partitions\n11/12/16 22:57:45 INFO CacheTrackerActor: Asked for current cache locations\n11/12/16 22:57:45 INFO MesosScheduler: Final stage: Stage 0\n11/12/16 22:57:45 INFO MesosScheduler: Parents of final stage: List()\n11/12/16 22:57:45 INFO MesosScheduler: Missing parents: List()\n11/12/16 22:57:45 INFO MesosScheduler: Submitting Stage 0, which has no missing parents\n11/12/16 22:57:45 INFO MesosScheduler: Got a job with 2 tasks\n11/12/16 22:57:45 INFO MesosScheduler: Registered as framework ID 201112162144-0-0002\n11/12/16 22:57:45 INFO MesosScheduler: Adding job with ID 0\n11/12/16 22:57:45 INFO SimpleJob: Starting task 0:0 as TID 0 on slave 201112162144-0-0: localhost (preferred)\n11/12/16 22:57:45 INFO SimpleJob: Starting task 0:1 as TID 1 on slave 201112162144-0-0: localhost (preferred)\n11/12/16 22:57:46 INFO SimpleJob: Finished TID 1 (progress: 1/2)\n11/12/16 22:57:47 INFO MesosScheduler: Completed ResultTask(0, 1)\n11/12/16 22:57:47 INFO SimpleJob: Finished TID 0 (progress: 2/2)\n11/12/16 22:57:47 INFO MesosScheduler: Completed ResultTask(0, 0)\n11/12/16 22:57:47 INFO SparkContext: Job finished in 1.827198029 s\nPi is roughly 3.14944\n\n==> but SparkPi still as active frameworks from UI\n(\nActive Frameworks\n\nID\tUser\tName\tRunning Tasks\tCPUs\tMEM\tMax Share\tConnected\n201112162144-0-0002\txuanhouysh\tSparkPi\t0\t0\t200.0 MB\t0.22\t2011-12-16 22:57:45\n)\n\nMy Spark config file as follows:\n\nexport MESOS_HOME=/home/xuanhouysh/software/mesos\nexport SCALA_HOME=/home/xuanhouysh/software/scala-2.9.1.final\nexport SPARK_MEM=200m", "comments": ["Github comment from mateiz: Hi Shawn,\n\nYou're supposed to use master@localhost:5050, not 1@localhost:5050. That's an older format.\n\nTo prevent the job remaining on the Mesos web UI, you need to add the argument --failover_timeout=0 to mesos-master. We're going to make it default to 0 soon because this is confusing.\n\nMatei\n\nOn Dec 16, 2011, at 4:10 PM, shawny wrote:\n\n> I setup mesos on local machine:\n> \n> {\n> 1000     29724  0.0  0.4  49556  9728 pts/0    Sl   21:44   0:02 ./bin mesos-master \n> 1000     29774  0.0  0.4  48864  8756 pts/0    Sl   21:44   0:01 ./bin mesos-slave --resources=cpus:4;mem:916 -m mesos://master@localhost:5050\n> }\n> \n> When i run \"./run spark.examples.SparkPi 1@localhost:5050\", it will stop at the following last line: \n> \n> 11/12/16 23:04:45 INFO CacheTrackerActor: Registered actor on port 50501\n> 11/12/16 23:04:45 INFO MapOutputTrackerActor: Registered actor on port 50501\n> 11/12/16 23:04:45 INFO MesosScheduler: Temp directory for JARs: /tmp/spark-d10f5f39-4d21-4062-ac67-7e27ebd15794\n> 11/12/16 23:04:46 INFO MesosScheduler: JAR server started at http://127.0.0.1:40518\n> 11/12/16 23:04:46 INFO SparkContext: Starting job...\n> 11/12/16 23:04:46 INFO CacheTracker: Registering RDD ID 0 with cache\n> 11/12/16 23:04:46 INFO CacheTrackerActor: Registering RDD 0 with 2 partitions\n> 11/12/16 23:04:46 INFO CacheTrackerActor: Asked for current cache locations\n> 11/12/16 23:04:46 INFO MesosScheduler: Final stage: Stage 0\n> 11/12/16 23:04:46 INFO MesosScheduler: Parents of final stage: List()\n> 11/12/16 23:04:46 INFO MesosScheduler: Missing parents: List()\n> 11/12/16 23:04:46 INFO MesosScheduler: Submitting Stage 0, which has no missing parents\n> 11/12/16 23:04:46 INFO MesosScheduler: Got a job with 2 tasks\n> \n> \n> But when i run \"./run spark.examples.SparkPi mesos://master@localhost:5050\", i can get the result of Pi:\n> \n> 11/12/16 22:57:44 INFO MapOutputTrackerActor: Registered actor on port 50501\n> 11/12/16 22:57:44 INFO CacheTrackerActor: Registered actor on port 50501\n> 11/12/16 22:57:44 INFO MesosScheduler: Temp directory for JARs: /tmp/spark-f173b70d-bf22-4dc7-a1c5-c241a67af3dc\n> 11/12/16 22:57:45 INFO MesosScheduler: JAR server started at http://127.0.0.1:40327\n> 11/12/16 22:57:45 INFO SparkContext: Starting job...\n> 11/12/16 22:57:45 INFO CacheTracker: Registering RDD ID 0 with cache\n> 11/12/16 22:57:45 INFO CacheTrackerActor: Registering RDD 0 with 2 partitions\n> 11/12/16 22:57:45 INFO CacheTrackerActor: Asked for current cache locations\n> 11/12/16 22:57:45 INFO MesosScheduler: Final stage: Stage 0\n> 11/12/16 22:57:45 INFO MesosScheduler: Parents of final stage: List()\n> 11/12/16 22:57:45 INFO MesosScheduler: Missing parents: List()\n> 11/12/16 22:57:45 INFO MesosScheduler: Submitting Stage 0, which has no missing parents\n> 11/12/16 22:57:45 INFO MesosScheduler: Got a job with 2 tasks\n> 11/12/16 22:57:45 INFO MesosScheduler: Registered as framework ID 201112162144-0-0002\n> 11/12/16 22:57:45 INFO MesosScheduler: Adding job with ID 0\n> 11/12/16 22:57:45 INFO SimpleJob: Starting task 0:0 as TID 0 on slave 201112162144-0-0: localhost (preferred)\n> 11/12/16 22:57:45 INFO SimpleJob: Starting task 0:1 as TID 1 on slave 201112162144-0-0: localhost (preferred)\n> 11/12/16 22:57:46 INFO SimpleJob: Finished TID 1 (progress: 1/2)\n> 11/12/16 22:57:47 INFO MesosScheduler: Completed ResultTask(0, 1)\n> 11/12/16 22:57:47 INFO SimpleJob: Finished TID 0 (progress: 2/2)\n> 11/12/16 22:57:47 INFO MesosScheduler: Completed ResultTask(0, 0)\n> 11/12/16 22:57:47 INFO SparkContext: Job finished in 1.827198029 s\n> Pi is roughly 3.14944\n> \n> ==> but SparkPi still as active frameworks from UI\n> (\n> Active Frameworks\n> \n> ID\tUser\tName\tRunning Tasks\tCPUs\tMEM\tMax Share\tConnected\n> 201112162144-0-0002\txuanhouysh\tSparkPi\t0\t0\t200.0 MB\t0.22\t2011-12-16 22:57:45\n> )\n> \n> My Spark config file as follows:\n> \n> export MESOS_HOME=/home/xuanhouysh/software/mesos\n> export SCALA_HOME=/home/xuanhouysh/software/scala-2.9.1.final\n> export SPARK_MEM=200m\n> \n> --- \n> Reply to this email directly or view it on GitHub:\n> https://github.com/mesos/spark/issues/102", "Github comment from shawny: Thanks for your quickly reply :)\n\nYou are right, i test it again using prefix master, success!", "Github comment from shawny: Sorry, about the \"failover_timeout\", it seems that assign it to 0 can't affect the active frameworks in UI.\n\nmesos master and slave process\n===============\n1000      4609  0.2  0.4  49332  9820 pts/0    Sl   00:02   0:00 ./bin mesos-master --failover_timeout=0\n1000      4659  0.1  0.4  48264  8876 pts/0    Sl   00:02   0:00 ./bin mesos-slave --resources=cpus:4;mem:916 -m mesos://master@localhost:5050\n\nUI\n======\nActive Frameworks\n\nID\tUser\tName\tRunning Tasks\tCPUs\tMEM\tMax Share\tConnected\n201112170008-0-0000\txuanhouysh\tSparkPi\t0\t0\t200.0 MB\t0.22\t2011-12-17 00:08:29", "Imported from Github issue spark-102, originally reported by shawny"], "derived": {"summary": "I setup mesos on local machine:\n\n{\n1000     29724  0. 0  0.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Why \"./run spark.examples.SparkPi 1@localhost:5050\" can't get result of Pi - I setup mesos on local machine:\n\n{\n1000     29724  0. 0  0."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-102, originally reported by shawny"}]}}
{"project": "SPARK", "issue_id": "SPARK-439", "title": "Made improvements to takeSample. Also changed SparkLocalKMeans to SparkKMeans", "status": "Resolved", "priority": null, "reporter": "edisontung", "assignee": null, "labels": [], "created": "0011-12-16T12:04:00.000+0000", "updated": "2012-10-19T22:50:25.000+0000", "description": "Instead of calling count on an RDD multiple times, stored the count so it can be used. Also, called the collect method for the array earlier also to reduce the number of times count() is called.", "comments": ["Github comment from mateiz: Hey Edison,\n\nThis looks good but I made a few comments on the diff, including finding a couple of bugs (another extra call to count() and a potential infinite loop).\n\nThe other thing I noticed is that your new kmeans_data.txt is huge -- 750K lines. It would be better to replace it with the simple one we had before (or just generate another small one), so that people can look at it more easily.", "Github comment from mateiz: Looks good. Thanks!", "Imported from Github issue spark-103, originally reported by edisontung"], "derived": {"summary": "Instead of calling count on an RDD multiple times, stored the count so it can be used. Also, called the collect method for the array earlier also to reduce the number of times count() is called.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Made improvements to takeSample. Also changed SparkLocalKMeans to SparkKMeans - Instead of calling count on an RDD multiple times, stored the count so it can be used. Also, called the collect method for the array earlier also to reduce the number of times count() is called."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-103, originally reported by edisontung"}]}}
{"project": "SPARK", "issue_id": "SPARK-552", "title": "spark api doc", "status": "Closed", "priority": null, "reporter": "songhe", "assignee": null, "labels": [], "created": "0011-12-19T20:26:00.000+0000", "updated": "2012-10-20T16:57:18.000+0000", "description": "If spark can provide the document of api, it would be much better for beginners.", "comments": ["Github comment from yiihsia: I think so", "Github comment from tjhunter: +1", "Github comment from rxin: +1", "Github comment from tsforce: +1", "Imported from Github issue spark-104, originally reported by shawny", "The Spark 0.6.0 release introduced Scaladoc API documentation for Spark: http://www.spark-project.org/docs/0.6.0/api/core/index.html"], "derived": {"summary": "If spark can provide the document of api, it would be much better for beginners.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "spark api doc - If spark can provide the document of api, it would be much better for beginners."}, {"q": "What updates or decisions were made in the discussion?", "a": "The Spark 0.6.0 release introduced Scaladoc API documentation for Spark: http://www.spark-project.org/docs/0.6.0/api/core/index.html"}]}}
{"project": "SPARK", "issue_id": "SPARK-438", "title": "Using 0 partition RDD in a shuffle causes crashes", "status": "Resolved", "priority": null, "reporter": "woggling", "assignee": null, "labels": [], "created": "0011-12-25T12:20:00.000+0000", "updated": "2012-10-19T22:50:36.000+0000", "description": "If one uses a zero partition RDD (e.g. the result of creating a Hadoop RDD from empty files with certain input formats) as part of a shuffle dependency, spark will get confused: DAGScheduler realizes that it doesn't need to realize the RDD, but it won't tell MapOutputTracker about it, so MapOutputTracker will return null when the reduce tasks ask for the shuffle locations.", "comments": ["Github comment from mateiz: Thanks for spotting this, Charles! A number of people had run into these errors. So I guess the solution is to (a) register those RDDs with the output tracker and (b) handle 0-partition RDDs on the reduce side?", "Github comment from mateiz: I've fixed this and added a test where I just create an RDD from an empty directory, but let me know if there is a better way to test it (any other case in which this happens).", "Imported from Github issue spark-105, originally reported by woggling"], "derived": {"summary": "If one uses a zero partition RDD (e. g.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Using 0 partition RDD in a shuffle causes crashes - If one uses a zero partition RDD (e. g."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-105, originally reported by woggling"}]}}
{"project": "SPARK", "issue_id": "SPARK-551", "title": "slave disconnected", "status": "Resolved", "priority": null, "reporter": "dukeecnu", "assignee": null, "labels": [], "created": "0011-12-26T00:37:00.000+0000", "updated": "2013-12-07T14:12:13.000+0000", "description": "I am a new mesos user. I run mesos on single machine. If I start the master and slave via the script deploy/start-mesos and run the example, it ends successfully but the slaves would be disconnected. However, if I start the master and slave as foreground process, it does work. What's wrong?", "comments": ["Github comment from mateiz: If you're not the one who posted about this on the mailing list, let me know what output you see from the slave. Look in the logs directory of Mesos for something called mesos-slave.xxx.INFO.", "Github comment from dukeecnu: I1229 11:17:20.043180  2678 process_based_isolation_module.cpp:114] Forked executor at = 2701\nI1229 11:17:20.106433  2678 slave.cpp:725] Got registration for executor 'default' of framework 201112291117-0-0000\nI1229 11:17:20.106547  2678 slave.cpp:779] Flushing queued tasks for framework 201112291117-0-0000\nI1229 11:17:20.231096  2678 slave.cpp:920] Status update: task 0 of framework 201112291117-0-0000 is now in state TASK_RUNNING\nI1229 11:17:20.231884  2678 slave.cpp:642] Got acknowledgement of status update for task 0 of framework 201112291117-0-0000\nI1229 11:17:21.234006  2678 slave.cpp:920] Status update: task 0 of framework 201112291117-0-0000 is now in state TASK_FINISHED\nI1229 11:17:21.234973  2678 slave.cpp:642] Got acknowledgement of status update for task 0 of framework 201112291117-0-0000\nI1229 11:17:21.236644  2678 slave.cpp:398] Got assigned task 1 for framework 201112291117-0-0000\nI1229 11:17:21.239364  2678 slave.cpp:920] Status update: task 1 of framework 201112291117-0-0000 is now in state TASK_RUNNING\nI1229 11:17:21.240097  2678 slave.cpp:642] Got acknowledgement of status update for task 1 of framework 201112291117-0-0000\nI1229 11:17:22.239802  2678 slave.cpp:920] Status update: task 1 of framework 201112291117-0-0000 is now in state TASK_FINISHED\nI1229 11:17:22.240749  2678 slave.cpp:642] Got acknowledgement of status update for task 1 of framework 201112291117-0-0000\nI1229 11:17:22.242307  2678 slave.cpp:398] Got assigned task 2 for framework 201112291117-0-0000\nI1229 11:17:22.244925  2678 slave.cpp:920] Status update: task 2 of framework 201112291117-0-0000 is now in state TASK_RUNNING\nI1229 11:17:22.245684  2678 slave.cpp:642] Got acknowledgement of status update for task 2 of framework 201112291117-0-0000\nI1229 11:17:23.245420  2678 slave.cpp:920] Status update: task 2 of framework 201112291117-0-0000 is now in state TASK_FINISHED\nI1229 11:17:23.246287  2678 slave.cpp:642] Got acknowledgement of status update for task 2 of framework 201112291117-0-0000\nI1229 11:17:23.248219  2678 slave.cpp:398] Got assigned task 3 for framework 201112291117-0-0000\nI1229 11:17:23.251009  2678 slave.cpp:920] Status update: task 3 of framework 201112291117-0-0000 is now in state TASK_RUNNING\nI1229 11:17:23.251930  2678 slave.cpp:642] Got acknowledgement of status update for task 3 of framework 201112291117-0-0000\nI1229 11:17:24.251518  2678 slave.cpp:920] Status update: task 3 of framework 201112291117-0-0000 is now in state TASK_FINISHED\nI1229 11:17:24.252471  2678 slave.cpp:642] Got acknowledgement of status update for task 3 of framework 201112291117-0-0000\nI1229 11:17:24.254112  2678 slave.cpp:398] Got assigned task 4 for framework 201112291117-0-0000\nI1229 11:17:24.256762  2678 slave.cpp:920] Status update: task 4 of framework 201112291117-0-0000 is now in state TASK_RUNNING\nI1229 11:17:24.257613  2678 slave.cpp:642] Got acknowledgement of status update for task 4 of framework 201112291117-0-0000\nI1229 11:17:25.257258  2678 slave.cpp:920] Status update: task 4 of framework 201112291117-0-0000 is now in state TASK_FINISHED\nI1229 11:17:25.259258  2678 slave.cpp:642] Got acknowledgement of status update for task 4 of framework 201112291117-0-0000\nI1229 11:17:25.259996  2678 slave.cpp:568] Asked to shut down framework 201112291117-0-0000\nI1229 11:17:25.260043  2678 slave.cpp:572] Shutting down framework 201112291117-0-0000\nI1229 11:17:25.260066  2678 slave.cpp:1306] Shutting down executor 'default' of framework 201112291117-0-0000\nI1229 11:17:26.216948  2678 process_based_isolation_module.cpp:217] Telling slave of lost executor default of framework 201112291117-0-0000\nF1229 11:17:26.217048  2678 utils.hpp:130] Expecting 'MESOS_HOME' in environment variables", "Github comment from dukeecnu: when I use one machine as master and another as slave, I run them as foreground process. Unfortunately, I fail too. At the end of the console terminal, there are\n\n*** Check failure stack trace: ***\n    @           0x59240d  google::LogMessage::Fail()\n    @           0x5980c7  google::LogMessage::SendToLog()\n    @           0x593cc4  google::LogMessage::Flush()\n    @           0x593f26  google::LogMessageFatal::~LogMessageFatal()\n    @           0x473605  mesos::internal::utils::process::killtree()\n    @           0x46dc0c  mesos::internal::slave::ProcessBasedIsolationModule::killExecutor()\n    @           0x46c5d4  mesos::internal::slave::ProcessBasedIsolationModule::processExited()\n    @           0x473fc9  std::tr1::_Function_handler<>::_M_invoke()\n    @           0x47453e  std::tr1::_Function_handler<>::_M_invoke()\n    @           0x5adbc5  process::ProcessBase::serve()\n    @           0x44c541  process::ProcessBase::operator()()\n    @           0x5b45f2  process::ProcessManager::run()\n    @           0x5b4760  process::trampoline()\n    @       0x3cbca41820  (unknown)", "Imported from Github issue spark-106, originally reported by dukeecnu"], "derived": {"summary": "I am a new mesos user. I run mesos on single machine.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "slave disconnected - I am a new mesos user. I run mesos on single machine."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-106, originally reported by dukeecnu"}]}}
{"project": "SPARK", "issue_id": "SPARK-437", "title": "Add support for fixed sized samples", "status": "Resolved", "priority": null, "reporter": "Holden Karau", "assignee": null, "labels": [], "created": "0012-01-23T23:38:00.000+0000", "updated": "2012-10-19T22:50:29.000+0000", "description": "Fix for issue #93, adding support for fixed sized samples.", "comments": ["Github comment from mateiz: Hey, sorry for being a bit late on this, I didn't see you posted a new pull request. Would you like me to grab the SortedRDD too or just the rest? As far as I understand, SortedRDD is not complete. We actually had our own effort to implement one so it might be better to use that instead.", "Github comment from holdenk: Oh crap I forgot about sortedrdd being in my branch, just the other stuff\nwould be cool :)\nOn Feb 5, 2012 8:55 PM, \"Matei Zaharia\" <\nreply@reply.github.com>\nwrote:\n\n> Hey, sorry for being a bit late on this, I didn't see you posted a new\n> pull request. Would you like me to grab the SortedRDD too or just the rest?\n> As far as I understand, SortedRDD is not complete. We actually had our own\n> effort to implement one so it might be better to use that instead.\n>\n> ---\n> Reply to this email directly or view it on GitHub:\n> https://github.com/mesos/spark/pull/107#issuecomment-3823642\n>", "Github comment from mateiz: Okay, so I looked at this more carefully, but unfortunately I think there are some problems:\n\n1) There is an array index out of bounds exception on CollectSampledRDD.scala line 41 (in sampleData.update(i,oldData(i))), because the for loop there goes over indices from 1 to sampleSize instead of 0 to sampleSize-1.\n\n2) It looks like the code is pulling out sampleSize items inside each split (partition) of the dataset, rather than sampleSize total items. This is not going to work as the user expects for datasets with more than 1 partition, unless we explicitly document this as the use of the function, which I don't like because I don't want the user to worry about the number of partitions. More generally, partitions might also have different numbers of items, so some might have fewer than sampleSize while some have more.\n\nI think it might be better to stick with the approach in the current collectSample for now, because it avoids these issues. In particular, it would be really hard to know in advance how many items are in each partition of the dataset, and pass the right number of items to the tasks that work on that object. The current code just tries to take a sample that is slightly bigger than the required number of items using independent coin flips on each element, and then pulls out the right number of items.", "Github comment from mateiz: I closed the pull request because the older collectSample seems to work okay, but thanks for trying this out!", "Imported from Github issue spark-107, originally reported by holdenk"], "derived": {"summary": "Fix for issue #93, adding support for fixed sized samples.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add support for fixed sized samples - Fix for issue #93, adding support for fixed sized samples."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-107, originally reported by holdenk"}]}}
{"project": "SPARK", "issue_id": "SPARK-436", "title": "Added immutable map registration in kryo serializer", "status": "Resolved", "priority": null, "reporter": "patelh", "assignee": null, "labels": [], "created": "0012-01-26T15:33:00.000+0000", "updated": "2012-10-19T22:50:20.000+0000", "description": "Reused existing map serializer.", "comments": ["Github comment from mateiz: Looks good, thanks!", "Imported from Github issue spark-108, originally reported by patelh"], "derived": {"summary": "Reused existing map serializer.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Added immutable map registration in kryo serializer - Reused existing map serializer."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-108, originally reported by patelh"}]}}
{"project": "SPARK", "issue_id": "SPARK-435", "title": "LocalFileShuffle should not be an object", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0012-02-05T22:55:00.000+0000", "updated": "2012-10-22T14:55:31.000+0000", "description": "Right now this creates problems in programs that use multiple SparkContexts running locally (not on Mesos), most notably our tests. We should instead have a separate ShuffleManager per local scheduler.", "comments": ["Github comment from mateiz: Fixed in commit 0e93891d3d7df849cff6", "Imported from Github issue spark-109, originally reported by mateiz"], "derived": {"summary": "Right now this creates problems in programs that use multiple SparkContexts running locally (not on Mesos), most notably our tests. We should instead have a separate ShuffleManager per local scheduler.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "LocalFileShuffle should not be an object - Right now this creates problems in programs that use multiple SparkContexts running locally (not on Mesos), most notably our tests. We should instead have a separate ShuffleManager per local scheduler."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-109, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-434", "title": "Bad behavior when saving to S3", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0012-02-09T22:36:00.000+0000", "updated": "2012-10-22T14:55:36.000+0000", "description": "Right now the output committer part of save() operations runs on the master, but unfortunately, when accessing S3 storage through Hadoop, this operation takes quite a long time. We should either run it in parallel (as a set of tasks) or not use the Hadoop IO libraries for S3.", "comments": ["Imported from Github issue spark-110, originally reported by mateiz"], "derived": {"summary": "Right now the output committer part of save() operations runs on the master, but unfortunately, when accessing S3 storage through Hadoop, this operation takes quite a long time. We should either run it in parallel (as a set of tasks) or not use the Hadoop IO libraries for S3.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Bad behavior when saving to S3 - Right now the output committer part of save() operations runs on the master, but unfortunately, when accessing S3 storage through Hadoop, this operation takes quite a long time. We should either run it in parallel (as a set of tasks) or not use the Hadoop IO libraries for S3."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-110, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-433", "title": "Adding sorting to RDDs", "status": "Closed", "priority": null, "reporter": "Antonio Lupher", "assignee": "Reynold Xin", "labels": [], "created": "0012-02-11T01:10:00.000+0000", "updated": "2014-06-18T07:58:27.000+0000", "description": "Fix for issue #88, adding sort to RDDs. This adds a sortByKey(ascending) method to RDDs of key/value pairs where keys have an Ordered trait. The 'ascending' argument is a boolean to specify order.\n\n(Note: still getting unchecked / eliminated by erasure warnings for getPartition in RangePartitioner class)", "comments": ["Github comment from mateiz: Hey Antonio,\n\nI made a few comments on the code (in the Diff tab). Also, do you mind adding a unit test for this? Add a file in core/src/test/spark. We use ScalaTest for testing (http://www.scalatest.org/). You can probably just copy and edit one of the existing tests to get the scaffolding right.", "Github comment from alupher: Great, thanks for your comments, Matei. I've made the changes in my fork and will add a unit test as well.", "Github comment from alupher: By the way, that does take care of the type erasure warnings :)", "Github comment from mateiz: Great. By the way, the fixes look good, but I think you forgot to git add the test file.\n\nMatei\n\nOn Feb 13, 2012, at 12:09 AM, Antonio Lupher wrote:\n\n> Great, thanks for your comments, Matei. I've made the changes in my fork and will add a unit test as well.\n> \n> --- \n> Reply to this email directly or view it on GitHub:\n> https://github.com/mesos/spark/pull/111#issuecomment-3935835", "Github comment from mateiz: Hey Antonio, have you had a chance to work on a unit test for sorting yet?\n\nMatei\n\nOn Feb 13, 2012, at 12:09 AM, Antonio Lupher wrote:\n\n> Great, thanks for your comments, Matei. I've made the changes in my fork and will add a unit test as well.\n> \n> --- \n> Reply to this email directly or view it on GitHub:\n> https://github.com/mesos/spark/pull/111#issuecomment-3935835", "Github comment from alupher: Hi Matei, just added some unit tests, let me know what you think.\n\n-Antonio", "Github comment from mateiz: Looks good. Thanks Antonio!", "Github comment from mateiz: Hey Antonio,\n\nJust FYI, I just made a commit to fix a few issues with sorting:\n- The range partitioner would crash with a division by zero if the RDD was smaller than the number of partitions requested.\n- Some stuff was inefficient -- in particular, all the local vals you declared to build up the rangeBounds remained as fields of the RangePartitioner object, which made it huge.\n- The unit test with high parallelism was taking way too long; we can speed it up further by replacing the getPartition code with a binary search but for now I've just left it out.\n\nHere's the commit: https://github.com/mesos/spark/commit/c7af538ac160727147eea6a1008dc16a2efd802e. Might be helpful for Shark too.\n\nMatei\n\nOn Feb 21, 2012, at 7:57 PM, Antonio Lupher wrote:\n\n> Hi Matei, just added some unit tests, let me know what you think.\n> \n> -Antonio\n> \n> --- \n> Reply to this email directly or view it on GitHub:\n> https://github.com/mesos/spark/pull/111#issuecomment-4097751", "Github comment from alupher: Hey Matei,\n\nThanks for your notes on the commit, that was very helpful. We'll make\nsure to incorporate them in the version of Spark we're using. Sorry\nfor missing some of those issues the first time around.\n\nWe also recently updated our Spark to use defaultParallelism as the\ndefault number of partitions for the sortedRDD (and defined a\nsortByKey that lets users choose this value). Here's the relevant\ncommit: https://github.com/alupher/spark/commit/13d5ffaf087e06e900ac9168d060a927274ff33e\n Do you think this would be good merge into the spark core?  If so, I\ncan send a pull request.\n\nAntonio\n\nOn Sat, Mar 17, 2012 at 1:54 PM, Matei Zaharia\n<reply@reply.github.com>\nwrote:\n> Hey Antonio,\n>\n> Just FYI, I just made a commit to fix a few issues with sorting:\n> - The range partitioner would crash with a division by zero if the RDD was smaller than the number of partitions requested.\n> - Some stuff was inefficient -- in particular, all the local vals you declared to build up the rangeBounds remained as fields of the RangePartitioner object, which made it huge.\n> - The unit test with high parallelism was taking way too long; we can speed it up further by replacing the getPartition code with a binary search but for now I've just left it out.\n>\n> Here's the commit: https://github.com/mesos/spark/commit/c7af538ac160727147eea6a1008dc16a2efd802e. Might be helpful for Shark too.\n>\n> Matei\n>\n> On Feb 21, 2012, at 7:57 PM, Antonio Lupher wrote:\n>\n>> Hi Matei, just added some unit tests, let me know what you think.\n>>\n>> -Antonio\n>>\n>> ---\n>> Reply to this email directly or view it on GitHub:\n>> https://github.com/mesos/spark/pull/111#issuecomment-4097751\n>\n> ---\n> Reply to this email directly or view it on GitHub:\n> https://github.com/mesos/spark/pull/111#issuecomment-4556990", "Imported from Github issue spark-111, originally reported by alupher"], "derived": {"summary": "Fix for issue #88, adding sort to RDDs. This adds a sortByKey(ascending) method to RDDs of key/value pairs where keys have an Ordered trait.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Adding sorting to RDDs - Fix for issue #88, adding sort to RDDs. This adds a sortByKey(ascending) method to RDDs of key/value pairs where keys have an Ordered trait."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-111, originally reported by alupher"}]}}
{"project": "SPARK", "issue_id": "SPARK-432", "title": "Changed HadoopRDD to get key and value containers from the RecordReader instead of through reflection", "status": "Resolved", "priority": null, "reporter": "Cliff Engle", "assignee": null, "labels": [], "created": "0012-02-29T16:44:00.000+0000", "updated": "2012-10-19T22:50:23.000+0000", "description": "This allows users to specify the key or value class of a HadoopRDD as Writable, whereas they needed to use a subclass of Writable before. I ran the FileSuite and everything passed.", "comments": ["Github comment from mateiz: Looks great, thanks!", "Imported from Github issue spark-112, originally reported by cengle"], "derived": {"summary": "This allows users to specify the key or value class of a HadoopRDD as Writable, whereas they needed to use a subclass of Writable before. I ran the FileSuite and everything passed.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Changed HadoopRDD to get key and value containers from the RecordReader instead of through reflection - This allows users to specify the key or value class of a HadoopRDD as Writable, whereas they needed to use a subclass of Writable before. I ran the FileSuite and everything passed."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-112, originally reported by cengle"}]}}
{"project": "SPARK", "issue_id": "SPARK-431", "title": "Update to work with Mesos 0.9 / 1.0 API", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0012-03-06T13:42:00.000+0000", "updated": "2012-10-22T14:55:35.000+0000", "description": "Mesos is going to make a first stable release as an Apache project soon, and as a result, there have been some API changes in it for long term evolvability. We need to update Spark\n\nIn the meantime, revision 1205738 of Mesos is what I recommend for people using the current Spark code. You can check it out from SVN with\n\nsvn checkout -r 1205738 http://svn.apache.org/repos/asf/incubator/mesos/trunk mesos", "comments": ["Github comment from mateiz: Branch mesos-0.9 (https://github.com/mesos/spark/tree/mesos-0.9) now supports the newest Mesos. It also uses Hadoop 0.20.205 by default instead of 0.20 as before.", "Github comment from mateiz: Just updated the branch with further fixes that work with Mesos 0.9 RC3.", "Github comment from tjhunter: Right now, mesos is in flux and some people are reporting some issues either with running spark or with building mesos. Can we make sure that the wiki states somewhere that casual users should use revision 1205738 of mesos?", "Github comment from tjhunter: FYI, the last working setup I have is: mesos r1205738 + spark branch 0.3-scala-2.9", "Github comment from mateiz: The master branch of Spark works fine with Mesos revision 1205738, and it's documented here: https://github.com/mesos/spark/wiki/Running-spark-on-mesos. I actually recommend using the master branch over 0.3 because it contains many fixes.\n\nWe'll make a new release as soon as there is an official Apache release of Mesos (which we're just waiting for some votes on from the Apache incubator PMC).", "Github comment from mateiz: This is released now.", "Imported from Github issue spark-113, originally reported by mateiz"], "derived": {"summary": "Mesos is going to make a first stable release as an Apache project soon, and as a result, there have been some API changes in it for long term evolvability. We need to update Spark\n\nIn the meantime, revision 1205738 of Mesos is what I recommend for people using the current Spark code.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Update to work with Mesos 0.9 / 1.0 API - Mesos is going to make a first stable release as an Apache project soon, and as a result, there have been some API changes in it for long term evolvability. We need to update Spark\n\nIn the meantime, revision 1205738 of Mesos is what I recommend for people using the current Spark code."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-113, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-430", "title": "Arthur: Replay debugger for Spark", "status": "Resolved", "priority": null, "reporter": "Ankur Dave", "assignee": null, "labels": [], "created": "0012-03-21T13:07:00.000+0000", "updated": "2012-10-19T22:50:33.000+0000", "description": "Arthur provides replay debugging for deterministic errors in Spark programs. Execution recording is turned off by default unless the user sets `spark.arthur.logPath`. The user can then load the log and replay the program using `EventLogReader`.", "comments": ["Imported from Github issue spark-114, originally reported by ankurdave"], "derived": {"summary": "Arthur provides replay debugging for deterministic errors in Spark programs. Execution recording is turned off by default unless the user sets `spark.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Arthur: Replay debugger for Spark - Arthur provides replay debugging for deterministic errors in Spark programs. Execution recording is turned off by default unless the user sets `spark."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-114, originally reported by ankurdave"}]}}
{"project": "SPARK", "issue_id": "SPARK-550", "title": "Hiding the default spark context in the spark shell creates serialization issues", "status": "Closed", "priority": null, "reporter": "tjhunter", "assignee": null, "labels": [], "created": "0012-04-03T20:41:00.000+0000", "updated": "2014-09-21T15:35:26.000+0000", "description": "I copy-pasted a piece of code along these lines in the spark shell:\n\n...\nval sc = new SparkContext(\"local[%d]\" format num_splits,\"myframework\")\nval my_rdd = sc.textFile(...)\nmy_rdd.count()\n\nThis leads to the shell crashing with a java.io.NotSerializableException: spark.SparkContext\n\nIt took me a while to realize it was due to the new spark context created. Maybe a warning/error should be triggered if the user tries to change the definition of sc?", "comments": ["Imported from Github issue spark-115, originally reported by tjhunter", "a lot of code has changed in this space over the past 2 years. i'm going to close this, but feel free to re-open if you feel it's still an issue."], "derived": {"summary": "I copy-pasted a piece of code along these lines in the spark shell:. val sc = new SparkContext(\"local[%d]\" format num_splits,\"myframework\")\nval my_rdd = sc.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Hiding the default spark context in the spark shell creates serialization issues - I copy-pasted a piece of code along these lines in the spark shell:. val sc = new SparkContext(\"local[%d]\" format num_splits,\"myframework\")\nval my_rdd = sc."}, {"q": "What updates or decisions were made in the discussion?", "a": "a lot of code has changed in this space over the past 2 years. i'm going to close this, but feel free to re-open if you feel it's still an issue."}]}}
{"project": "SPARK", "issue_id": "SPARK-549", "title": "Move mesos.jar to a maven repository", "status": "Resolved", "priority": null, "reporter": "tjhunter", "assignee": null, "labels": [], "created": "0012-04-04T13:28:00.000+0000", "updated": "2012-10-22T15:10:31.000+0000", "description": "The mesos jar file is embedded in the code and is not deployed when spark is published locally: using publish-local and then adding an sbt dependency to a project that depends on spark does not work correctly.\nI am not sure if it is a spark issue or a mesos issue at this point. Feel free to close the bug and to report it upstream to mesos.", "comments": ["Github comment from mateiz: Good idea; we'll try to do this when we make the first Apache release of Mesos, which should be soon.", "Imported from Github issue spark-116, originally reported by tjhunter", "Looks like this was done in the latest releases."], "derived": {"summary": "The mesos jar file is embedded in the code and is not deployed when spark is published locally: using publish-local and then adding an sbt dependency to a project that depends on spark does not work correctly. I am not sure if it is a spark issue or a mesos issue at this point.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Move mesos.jar to a maven repository - The mesos jar file is embedded in the code and is not deployed when spark is published locally: using publish-local and then adding an sbt dependency to a project that depends on spark does not work correctly. I am not sure if it is a spark issue or a mesos issue at this point."}, {"q": "What updates or decisions were made in the discussion?", "a": "Looks like this was done in the latest releases."}]}}
{"project": "SPARK", "issue_id": "SPARK-429", "title": "spark.master.host should be set to local IP address instead of hostname", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0012-04-05T13:47:00.000+0000", "updated": "2012-10-22T14:55:34.000+0000", "description": "In some clusters, the machine's short hostname, returned by InetAddress.getLocalHost().getHostName(), isn't its fully qualified name, which makes it not resolve from other machines. We should use the local IP instead.", "comments": ["Imported from Github issue spark-117, originally reported by mateiz"], "derived": {"summary": "In some clusters, the machine's short hostname, returned by InetAddress. getLocalHost().", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "spark.master.host should be set to local IP address instead of hostname - In some clusters, the machine's short hostname, returned by InetAddress. getLocalHost()."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-117, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-428", "title": "Update the examples to show how to ship the job's code to a cluster", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0012-04-06T09:08:00.000+0000", "updated": "2012-10-22T14:55:34.000+0000", "description": "They need to use the extended SparkContext constructor with a sparkHome and a list of JARs.", "comments": ["Imported from Github issue spark-118, originally reported by mateiz"], "derived": {"summary": "They need to use the extended SparkContext constructor with a sparkHome and a list of JARs.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Update the examples to show how to ship the job's code to a cluster - They need to use the extended SparkContext constructor with a sparkHome and a list of JARs."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-118, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-427", "title": "Report entry dropping in BoundedMemoryCache", "status": "Resolved", "priority": null, "reporter": "Ankur Dave", "assignee": null, "labels": [], "created": "0012-04-06T14:56:00.000+0000", "updated": "2012-10-19T22:50:28.000+0000", "description": "When BoundedMemoryCache dropped a key, it previously only logged the event to the local log on the slave. Now it reports that fact to the master as well.\n\nThis makes it possible to write tools that process the driver output and visualize cache usage across the cluster. It also will make it easier for Arthur to track this information.", "comments": ["Github comment from rxin: This is awesome :)", "Github comment from mateiz: Thanks!", "Imported from Github issue spark-119, originally reported by ankurdave"], "derived": {"summary": "When BoundedMemoryCache dropped a key, it previously only logged the event to the local log on the slave. Now it reports that fact to the master as well.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Report entry dropping in BoundedMemoryCache - When BoundedMemoryCache dropped a key, it previously only logged the event to the local log on the slave. Now it reports that fact to the master as well."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-119, originally reported by ankurdave"}]}}
{"project": "SPARK", "issue_id": "SPARK-548", "title": "Update Kryo to use 2.x", "status": "Resolved", "priority": null, "reporter": "Reynold Xin", "assignee": "Reynold Xin", "labels": [], "created": "0012-04-09T15:33:00.000+0000", "updated": "2013-01-20T12:29:48.000+0000", "description": "Kryo just released new 2.x versions. It has a number of useful changes. We should upgrade it soon. The only downside is that there is no maven pom for the new versions yet.\n\nA few useful things:\n\n1. Custom Serializer interface is now different. It no longer uses a ByteBuffer.\n\n2. Better performance for serializing byte arrays.", "comments": ["Github comment from mateiz: Cool! Let's wait until it's on Maven for now though because it seems like they want to do that soon. If you want to play with it before that, make your own separate branch.", "Github comment from rxin: I pushed a first implementation to kryo-2.x branch. Should still wait for this project to be updated: https://github.com/magro/kryo-serializers", "Github comment from velvia: By the way, I believe the code to serialize options (ie Some) does not work, because Kryo 1.x requires a no-arg constructor, and the Some() constructor requires a value.", "Github comment from rxin: de.javakaffee.kryoserializers.KryoReflectionFactorySupport should add the support for no-arg constructor.\n\nIt uses sun.reflect.ReflectionFactory to find the constructors ..", "Github comment from velvia: Ah, that won't work for openjdk will it?\n\n-Evan\nCarry your candle, run to the darkness\nSeek out the helpless, deceived and poor\nHold out your candle for all to see it\nTake your candle, and go light your world\n \n\n\nOn May 16, 2012, at 11:30 AM, Reynold Xin<reply@reply.github.com> wrote:\n\n> de.javakaffee.kryoserializers.KryoReflectionFactorySupport should add the support for no-arg constructor.\n> \n> It uses sun.reflect.ReflectionFactory to find the constructors ..\n> \n> ---\n> Reply to this email directly or view it on GitHub:\n> https://github.com/mesos/spark/issues/120#issuecomment-5748216", "Github comment from rxin: Fortunately openjdk implements this package too :)\n\nhttp://hg.openjdk.java.net/jdk6/jdk6/jdk/file/tip/src/share/classes/sun/reflect/ReflectionFactory.java", "Github comment from rxin: But yes - it is not a standard feature and it is plausible that not all JDKs have it.", "Imported from Github issue spark-120, originally reported by rxin", "Looks like this was done in https://github.com/mesos/spark/pull/341, so I'm resolving this issue."], "derived": {"summary": "Kryo just released new 2. x versions.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "Update Kryo to use 2.x - Kryo just released new 2. x versions."}, {"q": "What updates or decisions were made in the discussion?", "a": "Looks like this was done in https://github.com/mesos/spark/pull/341, so I'm resolving this issue."}]}}
{"project": "SPARK", "issue_id": "SPARK-426", "title": "Added an option (spark.closure.serializer) to specify the serializer for closures.", "status": "Resolved", "priority": null, "reporter": "Reynold Xin", "assignee": null, "labels": [], "created": "0012-04-09T21:02:00.000+0000", "updated": "2014-11-10T17:44:48.000+0000", "description": "This enables using Kryo as the closure serializer.", "comments": ["Github comment from mateiz: Cool! Does this mean that closures can be serialized properly this way?\n\nSince the parameter is called spark.closure.serializer, I would not change Utils.serialize and Utils.deserialize to use it, because the names for those would be misleading. Instead, create an instance of the closure serializer in SparkEnv and change the places that pass a closure or a TaskResult to use it.", "Github comment from rxin: I will make the change accordingly.\n\nI tested it and it worked. Previously the problem was that Class[_] was not serialized properly. This was solved by using a ClassSerializer in Kryo. (In 2.0, this becomes automatic.)", "Github comment from rxin: Updated. PTAL.", "Github comment from mateiz: Looks good. Thanks!", "Imported from Github issue spark-121, originally reported by rxin"], "derived": {"summary": "This enables using Kryo as the closure serializer.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Added an option (spark.closure.serializer) to specify the serializer for closures. - This enables using Kryo as the closure serializer."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-121, originally reported by rxin"}]}}
{"project": "SPARK", "issue_id": "SPARK-547", "title": "Provide a means to package Spark's executor into a tgz", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0012-04-13T21:39:00.000+0000", "updated": "2014-10-21T07:38:26.000+0000", "description": "Often the most convenient way to run Spark on Mesos would be to have a tar.gz that gets fetched by the nodes through HTTP or HDFS. We should have a sbt target that produces a streamlined tgz (probably just our JARs and the executor script).", "comments": ["Imported from Github issue spark-122, originally reported by mateiz", "This was fixed a long time ago."], "derived": {"summary": "Often the most convenient way to run Spark on Mesos would be to have a tar. gz that gets fetched by the nodes through HTTP or HDFS.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Provide a means to package Spark's executor into a tgz - Often the most convenient way to run Spark on Mesos would be to have a tar. gz that gets fetched by the nodes through HTTP or HDFS."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was fixed a long time ago."}]}}
{"project": "SPARK", "issue_id": "SPARK-546", "title": "Support full outer join and multiple join in a single shuffle", "status": "Resolved", "priority": null, "reporter": "Reynold Xin", "assignee": "Aaron Staple", "labels": [], "created": "0012-04-14T11:21:00.000+0000", "updated": "2014-12-04T17:51:56.000+0000", "description": "RDD[(K,V)] now supports left/right outer join but not full outer join.\n\nAlso it'd be nice to provide a way for users to join multiple RDDs on the same key in a single shuffle.", "comments": ["Imported from Github issue spark-123, originally reported by rxin", "We use a pimp-my-library pattern to add this functionality. Basically here's our code http://pastebin.com/EQ0Jm4Rj\n\nHope it helps :) (disclaimer - code in testing)", "I created a PR for a full outer join implementation here:\nhttps://github.com/apache/spark/pull/1395\n\nIf there is interest I can also implement multiJoin.", "Fixed by:\nhttps://github.com/apache/spark/pull/1395", "Hi, I think there are two features requested in this ticket:\n\n1) full outer join\n2) an RDD function to join >2 rdds in a single shuffle (e.g. multiJoin function)\n\nIve implemented #1 in my recent PR, but not #2. Im happy to implement #2 as well though.\n\nWould it make sense to reopen this ticket? File a new ticket?", "What about #2? Did you file a new ticket?\n\nI'm quite interested on this!", "Actually my experience implementing full join in a single shuffle is that it is fairly complicated and very hard to maintain. Since it is doable entirely in user code and given SparkSQL's SchemaRDD already supports it, I suggest not pulling this in Spark core. "], "derived": {"summary": "RDD[(K,V)] now supports left/right outer join but not full outer join. Also it'd be nice to provide a way for users to join multiple RDDs on the same key in a single shuffle.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support full outer join and multiple join in a single shuffle - RDD[(K,V)] now supports left/right outer join but not full outer join. Also it'd be nice to provide a way for users to join multiple RDDs on the same key in a single shuffle."}, {"q": "What updates or decisions were made in the discussion?", "a": "Actually my experience implementing full join in a single shuffle is that it is fairly complicated and very hard to maintain. Since it is doable entirely in user code and given SparkSQL's SchemaRDD already supports it, I suggest not pulling this in Spark core."}]}}
{"project": "SPARK", "issue_id": "SPARK-425", "title": "Added the ability to set environmental variables in piped rdd.", "status": "Resolved", "priority": null, "reporter": "Reynold Xin", "assignee": null, "labels": [], "created": "0012-04-17T15:41:00.000+0000", "updated": "2012-10-19T22:50:28.000+0000", "description": null, "comments": ["Github comment from mateiz: Looks great, thanks!", "Imported from Github issue spark-124, originally reported by rxin"], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Added the ability to set environmental variables in piped rdd."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-124, originally reported by rxin"}]}}
{"project": "SPARK", "issue_id": "SPARK-424", "title": "Fix issues with JAR server in mesos-0.9 branch", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0012-04-20T12:01:00.000+0000", "updated": "2012-10-22T14:55:33.000+0000", "description": "Found an issue testing this code at Twitter: in the mesos-0.9 branch, the ExecutorInfo in MesosScheduler is initialized before we have the right list of JARs to send, leading to none of the users' JARs being sent.", "comments": ["Github comment from mateiz: This is fixed now.", "Imported from Github issue spark-125, originally reported by mateiz"], "derived": {"summary": "Found an issue testing this code at Twitter: in the mesos-0. 9 branch, the ExecutorInfo in MesosScheduler is initialized before we have the right list of JARs to send, leading to none of the users' JARs being sent.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Fix issues with JAR server in mesos-0.9 branch - Found an issue testing this code at Twitter: in the mesos-0. 9 branch, the ExecutorInfo in MesosScheduler is initialized before we have the right list of JARs to send, leading to none of the users' JARs being sent."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-125, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-423", "title": "Spark executor should use Mesos-provided hostname for itself in cache tracker updates, etc", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0012-04-20T23:31:00.000+0000", "updated": "2012-10-22T14:55:36.000+0000", "description": "One user reported having machines where Mesos gets the FQDN and Spark only gets the unqualified host name. This was causing the scheduler to not realize that it can launch data-local tasks on those machines. Maybe we can also call getCanonicalHostname() instead, but this approach seems safer.", "comments": ["Github comment from mateiz: This is done in the dev branch.", "Imported from Github issue spark-126, originally reported by mateiz"], "derived": {"summary": "One user reported having machines where Mesos gets the FQDN and Spark only gets the unqualified host name. This was causing the scheduler to not realize that it can launch data-local tasks on those machines.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Spark executor should use Mesos-provided hostname for itself in cache tracker updates, etc - One user reported having machines where Mesos gets the FQDN and Spark only gets the unqualified host name. This was causing the scheduler to not realize that it can launch data-local tasks on those machines."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-126, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-422", "title": "End task instead of just exiting in LocalScheduler for tasks that throw exceptions", "status": "Resolved", "priority": null, "reporter": "Antonio Lupher", "assignee": null, "labels": [], "created": "0012-04-22T23:39:00.000+0000", "updated": "2012-10-19T22:50:20.000+0000", "description": "(our Shark testing suite driver was hanging on tests for which an exception was thrown on a worker)", "comments": ["Github comment from mateiz: Two comments:\n- Don't leave commented-out code in the repo (the //System.exit(1)); we can always see the old code using version control.\n- Do Spark's own unit tests (sbt/sbt test) still pass? Some of them do test exceptions thrown in tasks (but use the LocalScheduler setting that allows some failures).", "Github comment from alupher: Yep, the tests pass. I've removed the commented-out line.", "Github comment from mateiz: Looks good, thanks!", "Imported from Github issue spark-127, originally reported by alupher"], "derived": {"summary": "(our Shark testing suite driver was hanging on tests for which an exception was thrown on a worker).", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "End task instead of just exiting in LocalScheduler for tasks that throw exceptions - (our Shark testing suite driver was hanging on tests for which an exception was thrown on a worker)."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-127, originally reported by alupher"}]}}
{"project": "SPARK", "issue_id": "SPARK-421", "title": "Update spark-yarn project to support newest version of YARN", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0012-04-23T09:49:00.000+0000", "updated": "2012-10-22T14:55:34.000+0000", "description": "This is not a huge change but requires some work due to modifications to the YARN API.", "comments": ["Github comment from mateiz: Now fixed in the yarn branch of Spark.", "Imported from Github issue spark-128, originally reported by mateiz"], "derived": {"summary": "This is not a huge change but requires some work due to modifications to the YARN API.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Update spark-yarn project to support newest version of YARN - This is not a huge change but requires some work due to modifications to the YARN API."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-128, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-420", "title": "Force serialize/deserialize task results in local execution mode.", "status": "Resolved", "priority": null, "reporter": "Reynold Xin", "assignee": null, "labels": [], "created": "0012-04-24T13:58:00.000+0000", "updated": "2012-10-19T22:50:31.000+0000", "description": "In Shark, we were caught by surprise when we tested map join implementation on a cluster that it'd failed because of serialization. This commit forces a serialization/deserialization of task result data even for local modes to avoid such surprises.", "comments": ["Github comment from rxin: I added a new test for this too. Note that the test passing is contingent upon Antonio's patch (otherwise System.exit wouldn't give scala test a chance to intercept the exception).", "Github comment from mateiz: Looks good, thanks!", "Imported from Github issue spark-129, originally reported by rxin"], "derived": {"summary": "In Shark, we were caught by surprise when we tested map join implementation on a cluster that it'd failed because of serialization. This commit forces a serialization/deserialization of task result data even for local modes to avoid such surprises.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Force serialize/deserialize task results in local execution mode. - In Shark, we were caught by surprise when we tested map join implementation on a cluster that it'd failed because of serialization. This commit forces a serialization/deserialization of task result data even for local modes to avoid such surprises."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-129, originally reported by rxin"}]}}
{"project": "SPARK", "issue_id": "SPARK-545", "title": "support external sort", "status": "Resolved", "priority": null, "reporter": "Reynold Xin", "assignee": null, "labels": [], "created": "0012-05-02T13:17:00.000+0000", "updated": "2013-12-07T12:59:15.000+0000", "description": "Currently, sort operation can put too much pressure on memory and it is hard to guess what the right number of reduce partitions to use for sort. External sort can mitigate this problem.", "comments": ["Imported from Github issue spark-130, originally reported by rxin"], "derived": {"summary": "Currently, sort operation can put too much pressure on memory and it is hard to guess what the right number of reduce partitions to use for sort. External sort can mitigate this problem.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "support external sort - Currently, sort operation can put too much pressure on memory and it is hard to guess what the right number of reduce partitions to use for sort. External sort can mitigate this problem."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-130, originally reported by rxin"}]}}
{"project": "SPARK", "issue_id": "SPARK-419", "title": "Return size estimation, cache usage, and cache capacity from slave nodes to CacheTracker", "status": "Resolved", "priority": null, "reporter": "Reynold Xin", "assignee": null, "labels": [], "created": "0012-05-15T11:07:00.000+0000", "updated": "2012-10-19T22:50:39.000+0000", "description": "Also updated the log messages to give information on cache capacity when entries are added/dropped.", "comments": ["Github comment from rxin: I added another commit into this. It is the one that reduces the amount of data to serialize in closure serialization by making dependencies field transient.", "Github comment from mateiz: Great, thanks!", "Imported from Github issue spark-131, originally reported by rxin"], "derived": {"summary": "Also updated the log messages to give information on cache capacity when entries are added/dropped.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Return size estimation, cache usage, and cache capacity from slave nodes to CacheTracker - Also updated the log messages to give information on cache capacity when entries are added/dropped."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-131, originally reported by rxin"}]}}
{"project": "SPARK", "issue_id": "SPARK-418", "title": "Little refactoring and unit tests for CacheTrackerActor", "status": "Resolved", "priority": null, "reporter": "Benky", "assignee": null, "labels": [], "created": "0012-05-20T00:12:00.000+0000", "updated": "2012-10-19T22:50:24.000+0000", "description": null, "comments": ["Github comment from mateiz: Thanks a lot for these fixes! I had some comments on Utils.scala though -- in particular, leave the fast splitString method in because we've used it in some benchmarks, and revert the simplification of copyStream because it will be confusing to people without a lot of knowledge of how those FP operations work. If you make those changes, I'll merge this in.", "Github comment from mateiz: Thanks for the updates. I'll add in a program to use the splitString thing soon (we basically used it to build a fast WordCount program, but it's mostly been in demos we typed at the command line).", "Imported from Github issue spark-132, originally reported by Benky"], "derived": {"summary": "", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Little refactoring and unit tests for CacheTrackerActor"}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-132, originally reported by Benky"}]}}
{"project": "SPARK", "issue_id": "SPARK-417", "title": "BoundedMemoryCache.put should fail when estimated size of 'value' is larger than cache capacity", "status": "Resolved", "priority": null, "reporter": "Benky", "assignee": null, "labels": [], "created": "0012-05-20T13:30:00.000+0000", "updated": "2012-10-19T22:50:35.000+0000", "description": "Current implementation allows you to store object larger than BoundedMemoryCache.getCapacity into cache, which seems to be wrong.", "comments": ["Github comment from mateiz: Looks good, thanks!", "Imported from Github issue spark-133, originally reported by Benky"], "derived": {"summary": "Current implementation allows you to store object larger than BoundedMemoryCache. getCapacity into cache, which seems to be wrong.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "BoundedMemoryCache.put should fail when estimated size of 'value' is larger than cache capacity - Current implementation allows you to store object larger than BoundedMemoryCache. getCapacity into cache, which seems to be wrong."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-133, originally reported by Benky"}]}}
{"project": "SPARK", "issue_id": "SPARK-416", "title": "ShuffleManager & RDD refactored. Some unit tests added.", "status": "Resolved", "priority": null, "reporter": "Benky", "assignee": null, "labels": [], "created": "0012-05-27T14:42:00.000+0000", "updated": "2012-10-19T22:50:27.000+0000", "description": null, "comments": ["Github comment from mateiz: Thanks for posting this. Unfortunately this is a rather busy week for me, but I hope to look at it in the next few days.", "Github comment from mateiz: Finally got a chance to look at this. It looks great except for some formatting and a bug and possible performance regression in count() (you should check how Iterator.size is implemented -- I think it uses foreach() and ends up being slower than what I wrote).", "Github comment from mateiz: Closing this pull request because the code has diverged very far now. Let me know if you'd still like some of these to be added.", "Imported from Github issue spark-134, originally reported by Benky"], "derived": {"summary": "", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "ShuffleManager & RDD refactored. Some unit tests added."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-134, originally reported by Benky"}]}}
{"project": "SPARK", "issue_id": "SPARK-415", "title": "Make spark.repl.Main.interp_ publicly accessible", "status": "Resolved", "priority": null, "reporter": "Reynold Xin", "assignee": null, "labels": [], "created": "0012-05-30T17:45:00.000+0000", "updated": "2012-10-19T22:50:33.000+0000", "description": null, "comments": ["Github comment from mateiz: Looks good, thanks.", "Imported from Github issue spark-135, originally reported by rxin"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Make spark.repl.Main.interp_ publicly accessible"}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-135, originally reported by rxin"}]}}
{"project": "SPARK", "issue_id": "SPARK-413", "title": "Standalone deploy mode", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0012-06-02T12:22:00.000+0000", "updated": "2012-10-22T14:55:31.000+0000", "description": "Spark should provide a pure Java way of deploying on a cluster that people can try out if they only want to run just Spark, without Mesos or Hadoop alongside.", "comments": ["Github comment from mateiz: This is pretty much done in the dev branch now.", "Imported from Github issue spark-137, originally reported by mateiz"], "derived": {"summary": "Spark should provide a pure Java way of deploying on a cluster that people can try out if they only want to run just Spark, without Mesos or Hadoop alongside.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Standalone deploy mode - Spark should provide a pure Java way of deploying on a cluster that people can try out if they only want to run just Spark, without Mesos or Hadoop alongside."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-137, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-414", "title": "SizeEstimator's sampling should reuse SearchState", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0012-06-02T12:22:00.000+0000", "updated": "2012-10-22T14:55:37.000+0000", "description": "If you have an array where many elements share a pointer to the same object, the SizeEstimator currently double-counts that object, leading to overly large estimates.", "comments": ["Imported from Github issue spark-136, originally reported by mateiz"], "derived": {"summary": "If you have an array where many elements share a pointer to the same object, the SizeEstimator currently double-counts that object, leading to overly large estimates.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "SizeEstimator's sampling should reuse SearchState - If you have an array where many elements share a pointer to the same object, the SizeEstimator currently double-counts that object, leading to overly large estimates."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-136, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-412", "title": "run spark.examples.SparkPi  hangs with no results", "status": "Resolved", "priority": null, "reporter": "Parviz Deyhim", "assignee": null, "labels": [], "created": "0012-06-03T09:47:00.000+0000", "updated": "2012-10-19T22:50:24.000+0000", "description": "Hi there, \n\nI've been trying to make spark work on Mesos and have been stuck. I've tried everything I could  have think of. Unfortuantely I can't seem to figure out what's not setup right. \n\nWhen i run ./run spark.examples.SparkPi master@10.244.1.147:5050  Spark gets stuck at:\n\n12/06/03 17:42:27 INFO spark.MesosScheduler: Adding job with ID 0\n\nhere's the complete output. I'm also adding the output from my mesos-master. Any help would be truly appreciated.\n\n\n\nSpark Output: ./run spark.examples.SparkPi master@10.244.1.147:5050\n\n12/06/03 17:42:26 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 1358297825\n12/06/03 17:42:26 INFO spark.CacheTrackerActor: Registered actor on port 7077\n12/06/03 17:42:27 INFO spark.CacheTrackerActor: Started slave cache (size 1295.4MB) on ip-10-244-1-147.us-west-2.compute.internal\n12/06/03 17:42:27 INFO spark.MapOutputTrackerActor: Registered actor on port 7077\n12/06/03 17:42:27 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-71f91c61-c132-4abb-9437-ac736ca0ed8f/shuffle\n12/06/03 17:42:27 INFO server.Server: jetty-7.5.3.v20111011\n12/06/03 17:42:27 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:33864 STARTING\n12/06/03 17:42:27 INFO spark.ShuffleManager: Local URI: http://10.244.1.147:33864\n12/06/03 17:42:27 INFO spark.SparkContext: Starting job...\n12/06/03 17:42:27 INFO spark.CacheTracker: Registering RDD ID 1 with cache\n12/06/03 17:42:27 INFO spark.CacheTrackerActor: Registering RDD 1 with 2 partitions\n12/06/03 17:42:27 INFO spark.CacheTracker: Registering RDD ID 0 with cache\n12/06/03 17:42:27 INFO spark.CacheTrackerActor: Registering RDD 0 with 2 partitions\n12/06/03 17:42:27 INFO spark.CacheTrackerActor: Asked for current cache locations\n12/06/03 17:42:27 INFO spark.MesosScheduler: Final stage: Stage 0\n12/06/03 17:42:27 INFO spark.MesosScheduler: Parents of final stage: List()\n12/06/03 17:42:27 INFO spark.MesosScheduler: Missing parents: List()\n12/06/03 17:42:27 INFO spark.MesosScheduler: Submitting Stage 0, which has no missing parents\n12/06/03 17:42:27 INFO spark.MesosScheduler: Got a job with 2 tasks\n12/06/03 17:42:27 INFO spark.MesosScheduler: Registered as framework ID 201206031742-0-0000\n12/06/03 17:42:27 INFO spark.MesosScheduler: Adding job with ID 0\n\nMaster:\n\nI0603 17:42:18.808145 31518 logging.cpp:70] Logging to /tmp/install/mesos/logs\nI0603 17:42:18.809634 31518 main.cpp:95] Build: 2012-06-03 15:59:45 by hadoop\nI0603 17:42:18.809680 31518 main.cpp:96] Starting Mesos master\nI0603 17:42:18.810984 31519 master.cpp:264] Master started at mesos://master@10.244.1.147:5050\nI0603 17:42:18.811133 31519 master.cpp:279] Master ID: 201206031742-0\nI0603 17:42:18.811455 31519 master.cpp:462] Elected as master!\nI0603 17:42:22.068495 31519 master.cpp:814] Attempting to register slave 201206031742-0-0 at slave@10.244.1.196:52813\nI0603 17:42:22.069000 31519 master.cpp:1057] Master now considering a slave at ip-10-244-1-196.us-west-2.compute.internal:52813 as active\nI0603 17:42:22.069066 31519 master.cpp:1588] Adding slave 201206031742-0-0 at ip-10-244-1-196.us-west-2.compute.internal with cpus=1; mem=1024\nI0603 17:42:22.069295 31519 simple_allocator.cpp:71] Added slave 201206031742-0-0 with cpus=1; mem=1024\nI0603 17:42:27.985856 31519 master.cpp:492] Registering framework 201206031742-0-0000 at 2@10.244.1.147:55998\nI0603 17:42:27.986047 31519 simple_allocator.cpp:48] Added framework 201206031742-0-0000\nI0603 17:42:27.986146 31519 master.cpp:1184] Sending 1 offers to framework 201206031742-0-0000\nI0603 17:42:27.995363 31519 master.cpp:718] Reviving offers for framework 201206031742-0-0000\nI0603 17:42:27.995427 31519 simple_allocator.cpp:145] Filters removed for framework 201206031742-0-0000\nI0603 17:42:28.003842 31519 master.cpp:679] Received reply for offer 201206031742-0-0\nI0603 17:42:28.003937 31519 master.cpp:1403] Filtered slave 201206031742-0-0 for framework 201206031742-0-0000 for 1 seconds\nI0603 17:42:29.862329 31519 master.cpp:1184] Sending 1 offers to framework 201206031742-0-0000\nI0603 17:42:29.863867 31519 master.cpp:679] Received reply for offer 201206031742-0-1\nI0603 17:42:29.863937 31519 master.cpp:1403] Filtered slave 201206031742-0-0 for framework 201206031742-0-0000 for 1 seconds\nI0603 17:42:30.866348 31519 master.cpp:1184] Sending 1 offers to framework 201206031742-0-0000\nI0603 17:42:30.867856 31519 master.cpp:679] Received reply for offer 201206031742-0-2\nI0603 17:42:30.867918 31519 master.cpp:1403] Filtered slave 201206031742-0-0 for framework 201206031742-0-0000 for 1 seconds\nI0603 17:42:31.796823 31519 master.cpp:1118] Framework 201206031742-0-0000 disconnected", "comments": ["Github comment from mateiz: From this part:\n\n    I0603 17:42:29.863867 31519 master.cpp:679] Received reply for offer 201206031742-0-1\n    I0603 17:42:29.863937 31519 master.cpp:1403] Filtered slave 201206031742-0-0 for framework 201206031742-0-0000 for 1 seconds\n\nIt looks like Spark is not accepting to run on your slave node. Is it possible perhaps that your Mesos slave is reporting too little memory? What did you set SPARK_MEM to? The Mesos slave's memory should actually be slightly higher than SPARK_MEM, by a few hundred MB, or it won't allocate it.\n\nMatei\n\nOn Jun 3, 2012, at 10:47 AM, pdeyhim wrote:\n\n> Hi there, \n> \n> I've been trying to make spark work on Mesos and have been stuck. I've tried everything I could  have think of. Unfortuantely I can't seem to figure out what's not setup right. \n> \n> When i run ./run spark.examples.SparkPi master@10.244.1.147:5050  Spark gets stuck at:\n> \n> 12/06/03 17:42:27 INFO spark.MesosScheduler: Adding job with ID 0\n> \n> here's the complete output. I'm also adding the output from my mesos-master. Any help would be truly appreciated.\n> \n> \n> \n> Spark Output: ./run spark.examples.SparkPi master@10.244.1.147:5050\n> \n> 12/06/03 17:42:26 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 1358297825\n> 12/06/03 17:42:26 INFO spark.CacheTrackerActor: Registered actor on port 7077\n> 12/06/03 17:42:27 INFO spark.CacheTrackerActor: Started slave cache (size 1295.4MB) on ip-10-244-1-147.us-west-2.compute.internal\n> 12/06/03 17:42:27 INFO spark.MapOutputTrackerActor: Registered actor on port 7077\n> 12/06/03 17:42:27 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-71f91c61-c132-4abb-9437-ac736ca0ed8f/shuffle\n> 12/06/03 17:42:27 INFO server.Server: jetty-7.5.3.v20111011\n> 12/06/03 17:42:27 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:33864 STARTING\n> 12/06/03 17:42:27 INFO spark.ShuffleManager: Local URI: http://10.244.1.147:33864\n> 12/06/03 17:42:27 INFO spark.SparkContext: Starting job...\n> 12/06/03 17:42:27 INFO spark.CacheTracker: Registering RDD ID 1 with cache\n> 12/06/03 17:42:27 INFO spark.CacheTrackerActor: Registering RDD 1 with 2 partitions\n> 12/06/03 17:42:27 INFO spark.CacheTracker: Registering RDD ID 0 with cache\n> 12/06/03 17:42:27 INFO spark.CacheTrackerActor: Registering RDD 0 with 2 partitions\n> 12/06/03 17:42:27 INFO spark.CacheTrackerActor: Asked for current cache locations\n> 12/06/03 17:42:27 INFO spark.MesosScheduler: Final stage: Stage 0\n> 12/06/03 17:42:27 INFO spark.MesosScheduler: Parents of final stage: List()\n> 12/06/03 17:42:27 INFO spark.MesosScheduler: Missing parents: List()\n> 12/06/03 17:42:27 INFO spark.MesosScheduler: Submitting Stage 0, which has no missing parents\n> 12/06/03 17:42:27 INFO spark.MesosScheduler: Got a job with 2 tasks\n> 12/06/03 17:42:27 INFO spark.MesosScheduler: Registered as framework ID 201206031742-0-0000\n> 12/06/03 17:42:27 INFO spark.MesosScheduler: Adding job with ID 0\n> \n> Master:\n> \n> I0603 17:42:18.808145 31518 logging.cpp:70] Logging to /tmp/install/mesos/logs\n> I0603 17:42:18.809634 31518 main.cpp:95] Build: 2012-06-03 15:59:45 by hadoop\n> I0603 17:42:18.809680 31518 main.cpp:96] Starting Mesos master\n> I0603 17:42:18.810984 31519 master.cpp:264] Master started at mesos://master@10.244.1.147:5050\n> I0603 17:42:18.811133 31519 master.cpp:279] Master ID: 201206031742-0\n> I0603 17:42:18.811455 31519 master.cpp:462] Elected as master!\n> I0603 17:42:22.068495 31519 master.cpp:814] Attempting to register slave 201206031742-0-0 at slave@10.244.1.196:52813\n> I0603 17:42:22.069000 31519 master.cpp:1057] Master now considering a slave at ip-10-244-1-196.us-west-2.compute.internal:52813 as active\n> I0603 17:42:22.069066 31519 master.cpp:1588] Adding slave 201206031742-0-0 at ip-10-244-1-196.us-west-2.compute.internal with cpus=1; mem=1024\n> I0603 17:42:22.069295 31519 simple_allocator.cpp:71] Added slave 201206031742-0-0 with cpus=1; mem=1024\n> I0603 17:42:27.985856 31519 master.cpp:492] Registering framework 201206031742-0-0000 at 2@10.244.1.147:55998\n> I0603 17:42:27.986047 31519 simple_allocator.cpp:48] Added framework 201206031742-0-0000\n> I0603 17:42:27.986146 31519 master.cpp:1184] Sending 1 offers to framework 201206031742-0-0000\n> I0603 17:42:27.995363 31519 master.cpp:718] Reviving offers for framework 201206031742-0-0000\n> I0603 17:42:27.995427 31519 simple_allocator.cpp:145] Filters removed for framework 201206031742-0-0000\n> I0603 17:42:28.003842 31519 master.cpp:679] Received reply for offer 201206031742-0-0\n> I0603 17:42:28.003937 31519 master.cpp:1403] Filtered slave 201206031742-0-0 for framework 201206031742-0-0000 for 1 seconds\n> I0603 17:42:29.862329 31519 master.cpp:1184] Sending 1 offers to framework 201206031742-0-0000\n> I0603 17:42:29.863867 31519 master.cpp:679] Received reply for offer 201206031742-0-1\n> I0603 17:42:29.863937 31519 master.cpp:1403] Filtered slave 201206031742-0-0 for framework 201206031742-0-0000 for 1 seconds\n> I0603 17:42:30.866348 31519 master.cpp:1184] Sending 1 offers to framework 201206031742-0-0000\n> I0603 17:42:30.867856 31519 master.cpp:679] Received reply for offer 201206031742-0-2\n> I0603 17:42:30.867918 31519 master.cpp:1403] Filtered slave 201206031742-0-0 for framework 201206031742-0-0000 for 1 seconds\n> I0603 17:42:31.796823 31519 master.cpp:1118] Framework 201206031742-0-0000 disconnected\n> \n> ---\n> Reply to this email directly or view it on GitHub:\n> https://github.com/mesos/spark/issues/138", "Github comment from pdeyhim: Thanks for the quick reply Matei. SPARK_MEM is set to 2g. I checked the master and noticed that slave was getting resgistered with 1024m of memory. I started running slave with the following resources: --resources=cpus:2;mem:3449. I no longer get the \"filtered slave\" from master but I still get stuck here:\n\nfrom Master: \n\nI0604 01:35:00.843688 29562 logging.cpp:70] Logging to /tmp/install/mesos/logs\nI0604 01:35:00.845057 29562 main.cpp:95] Build: 2012-06-03 15:59:45 by hadoop\nI0604 01:35:00.845092 29562 main.cpp:96] Starting Mesos master\nI0604 01:35:00.846283 29563 master.cpp:264] Master started at mesos://master@10.244.1.147:5050\nI0604 01:35:00.846410 29563 master.cpp:279] Master ID: 201206040135-0\nI0604 01:35:00.846696 29563 master.cpp:462] Elected as master!\nI0604 01:35:01.642606 29563 master.cpp:814] Attempting to register slave 201206040135-0-0 at slave@10.244.1.196:43955\nI0604 01:35:01.643165 29563 master.cpp:1057] Master now considering a slave at ip-10-244-1-196.us-west-2.compute.internal:43955 as active\nI0604 01:35:01.643241 29563 master.cpp:1588] Adding slave 201206040135-0-0 at ip-10-244-1-196.us-west-2.compute.internal with cpus=2\nI0604 01:35:01.643457 29563 simple_allocator.cpp:71] Added slave 201206040135-0-0 with cpus=2\nI0604 01:35:13.313666 29563 master.cpp:492] Registering framework 201206040135-0-0000 at 2@10.244.1.147:45655\nI0604 01:35:13.313947 29563 simple_allocator.cpp:48] Added framework 201206040135-0-0000\nI0604 01:35:13.324192 29563 master.cpp:718] Reviving offers for framework 201206040135-0-0000\nI0604 01:35:13.324252 29563 simple_allocator.cpp:145] Filters removed for framework 201206040135-0-0000\n\n\nfrom Spark:\n\n12/06/04 01:35:12 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 1358297825\n12/06/04 01:35:12 INFO spark.CacheTrackerActor: Registered actor on port 7077\n12/06/04 01:35:12 INFO spark.CacheTrackerActor: Started slave cache (size 1295.4MB) on ip-10-244-1-147.us-west-2.compute.internal\n12/06/04 01:35:12 INFO spark.MapOutputTrackerActor: Registered actor on port 7077\n12/06/04 01:35:12 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-9b045c91-c9d5-403b-90a2-2a6bce617d63/shuffle\n12/06/04 01:35:12 INFO server.Server: jetty-7.5.3.v20111011\n12/06/04 01:35:12 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:39527 STARTING\n12/06/04 01:35:12 INFO spark.ShuffleManager: Local URI: http://10.244.1.147:39527\n12/06/04 01:35:13 INFO spark.SparkContext: Starting job...\n12/06/04 01:35:13 INFO spark.CacheTracker: Registering RDD ID 1 with cache\n12/06/04 01:35:13 INFO spark.CacheTrackerActor: Registering RDD 1 with 2 partitions\n12/06/04 01:35:13 INFO spark.CacheTracker: Registering RDD ID 0 with cache\n12/06/04 01:35:13 INFO spark.CacheTrackerActor: Registering RDD 0 with 2 partitions\n12/06/04 01:35:13 INFO spark.CacheTrackerActor: Asked for current cache locations\n12/06/04 01:35:13 INFO spark.MesosScheduler: Final stage: Stage 0\n12/06/04 01:35:13 INFO spark.MesosScheduler: Parents of final stage: List()\n12/06/04 01:35:13 INFO spark.MesosScheduler: Missing parents: List()\n12/06/04 01:35:13 INFO spark.MesosScheduler: Submitting Stage 0, which has no missing parents\n12/06/04 01:35:13 INFO spark.MesosScheduler: Got a job with 2 tasks\n12/06/04 01:35:13 INFO spark.MesosScheduler: Registered as framework ID 201206040135-0-0000\n12/06/04 01:35:13 INFO spark.MesosScheduler: Adding job with ID 0", "Github comment from mateiz: Oh, actually you need to put the \"--resources=cpus:2;mem:3449\" in quotes on your command line. Otherwise, bash thinks that the semicolon \";\" is the end of a statement, and treats the mem:3449 as a separate command.\n\nMatei\n\nOn Jun 3, 2012, at 6:36 PM, pdeyhim wrote:\n\n> Thanks for the quick reply Matei. I'm SPARK_MEM is set to 2g. I checked the master and noticed that slave was getting resgistered with 1024m of memory. I started running slave with the following resources: --resources=cpus:2;mem:3449. I no longer get the \"filtered slave\" from master but I still get stuck here:\n> \n> from Master: \n> \n> I0604 01:35:00.843688 29562 logging.cpp:70] Logging to /tmp/install/mesos/logs\n> I0604 01:35:00.845057 29562 main.cpp:95] Build: 2012-06-03 15:59:45 by hadoop\n> I0604 01:35:00.845092 29562 main.cpp:96] Starting Mesos master\n> I0604 01:35:00.846283 29563 master.cpp:264] Master started at mesos://master@10.244.1.147:5050\n> I0604 01:35:00.846410 29563 master.cpp:279] Master ID: 201206040135-0\n> I0604 01:35:00.846696 29563 master.cpp:462] Elected as master!\n> I0604 01:35:01.642606 29563 master.cpp:814] Attempting to register slave 201206040135-0-0 at slave@10.244.1.196:43955\n> I0604 01:35:01.643165 29563 master.cpp:1057] Master now considering a slave at ip-10-244-1-196.us-west-2.compute.internal:43955 as active\n> I0604 01:35:01.643241 29563 master.cpp:1588] Adding slave 201206040135-0-0 at ip-10-244-1-196.us-west-2.compute.internal with cpus=2\n> I0604 01:35:01.643457 29563 simple_allocator.cpp:71] Added slave 201206040135-0-0 with cpus=2\n> I0604 01:35:13.313666 29563 master.cpp:492] Registering framework 201206040135-0-0000 at 2@10.244.1.147:45655\n> I0604 01:35:13.313947 29563 simple_allocator.cpp:48] Added framework 201206040135-0-0000\n> I0604 01:35:13.324192 29563 master.cpp:718] Reviving offers for framework 201206040135-0-0000\n> I0604 01:35:13.324252 29563 simple_allocator.cpp:145] Filters removed for framework 201206040135-0-0000\n> \n> \n> from Spark:\n> \n> 12/06/04 01:35:12 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 1358297825\n> 12/06/04 01:35:12 INFO spark.CacheTrackerActor: Registered actor on port 7077\n> 12/06/04 01:35:12 INFO spark.CacheTrackerActor: Started slave cache (size 1295.4MB) on ip-10-244-1-147.us-west-2.compute.internal\n> 12/06/04 01:35:12 INFO spark.MapOutputTrackerActor: Registered actor on port 7077\n> 12/06/04 01:35:12 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-9b045c91-c9d5-403b-90a2-2a6bce617d63/shuffle\n> 12/06/04 01:35:12 INFO server.Server: jetty-7.5.3.v20111011\n> 12/06/04 01:35:12 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:39527 STARTING\n> 12/06/04 01:35:12 INFO spark.ShuffleManager: Local URI: http://10.244.1.147:39527\n> 12/06/04 01:35:13 INFO spark.SparkContext: Starting job...\n> 12/06/04 01:35:13 INFO spark.CacheTracker: Registering RDD ID 1 with cache\n> 12/06/04 01:35:13 INFO spark.CacheTrackerActor: Registering RDD 1 with 2 partitions\n> 12/06/04 01:35:13 INFO spark.CacheTracker: Registering RDD ID 0 with cache\n> 12/06/04 01:35:13 INFO spark.CacheTrackerActor: Registering RDD 0 with 2 partitions\n> 12/06/04 01:35:13 INFO spark.CacheTrackerActor: Asked for current cache locations\n> 12/06/04 01:35:13 INFO spark.MesosScheduler: Final stage: Stage 0\n> 12/06/04 01:35:13 INFO spark.MesosScheduler: Parents of final stage: List()\n> 12/06/04 01:35:13 INFO spark.MesosScheduler: Missing parents: List()\n> 12/06/04 01:35:13 INFO spark.MesosScheduler: Submitting Stage 0, which has no missing parents\n> 12/06/04 01:35:13 INFO spark.MesosScheduler: Got a job with 2 tasks\n> 12/06/04 01:35:13 INFO spark.MesosScheduler: Registered as framework ID 201206040135-0-0000\n> 12/06/04 01:35:13 INFO spark.MesosScheduler: Adding job with ID 0\n> \n> ---\n> Reply to this email directly or view it on GitHub:\n> https://github.com/mesos/spark/issues/138#issuecomment-6090958", "Github comment from pdeyhim: nice! getting very close :) Seeing a Jetty login error instead. Have no clue if its related to my issue though. Is Jetty the issue?\n\nSpark:\n\n12/06/04 04:18:32 INFO spark.MesosScheduler: Missing parents: List()\n12/06/04 04:18:32 INFO spark.MesosScheduler: Submitting Stage 0, which has no missing parents\n12/06/04 04:18:32 INFO spark.MesosScheduler: Got a job with 2 tasks\n12/06/04 04:18:32 INFO spark.MesosScheduler: Registered as framework ID 201206040418-0-0000\n12/06/04 04:18:32 INFO spark.MesosScheduler: Adding job with ID 0\n12/06/04 04:18:46 INFO spark.SimpleJob: Starting task 0:0 as TID 0 on slave 201206040418-0-0: ip-10-244-1-147.us-west-2.compute.internal (preferred)\n12/06/04 04:18:46 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 39 ms to serialize by spark.JavaSerializerInstance\n12/06/04 04:18:46 INFO spark.SimpleJob: Starting task 0:1 as TID 1 on slave 201206040418-0-0: ip-10-244-1-147.us-west-2.compute.internal (preferred)\n12/06/04 04:18:46 INFO spark.SimpleJob: Size of task 0:1 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance\n12/06/04 04:18:47 INFO spark.CacheTrackerActor: Started slave cache (size 2.5GB) on ip-10-244-1-147.us-west-2.compute.internal\n\nOn Slave:\n\nI0604 04:18:46.169631 17416 slave.cpp:340] Registered with master; given slave ID 201206040418-0-0\nI0604 04:18:46.226757 17416 slave.cpp:398] Got assigned task 0 for framework 201206040418-0-0000\nI0604 04:18:46.226852 17416 slave.cpp:1388] Generating a unique work directory for executor 'default' of framework 201206040418-0-0000\nI0604 04:18:46.227252 17416 slave.cpp:465] Using '/root/mesos/work/slaves/201206040418-0-0/frameworks/201206040418-0-0000/executors/default/runs/0' as work directory for executor 'default' of framework 201206040418-0-0000\nI0604 04:18:46.228713 17416 slave.cpp:398] Got assigned task 1 for framework 201206040418-0-0000\nI0604 04:18:46.228749 17416 slave.cpp:436] Queuing task '1' for executor default of framework '201206040418-0-0000\nI0604 04:18:46.228801 17416 process_based_isolation_module.cpp:91] Launching default (/root/spark/spark-executor) in /root/mesos/work/slaves/201206040418-0-0/frameworks/201206040418-0-0000/executors/default/runs/0 with resources mem=4096' for framework 201206040418-0-0000\nI0604 04:18:46.229663 17416 process_based_isolation_module.cpp:114] Forked executor at = 17419\nI0604 04:18:47.003084 17416 slave.cpp:725] Got registration for executor 'default' of framework 201206040418-0-0000\nI0604 04:18:47.003365 17416 slave.cpp:779] Flushing queued tasks for framework 201206040418-0-0000\n\nin /root/mesos/work/slaves/201206040418-0-0/frameworks/201206040418-0-0000/executors/default/runs/0/stderr i see this:\n\n12/06/04 04:18:47 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 2716595650\n12/06/04 04:18:47 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-54004563-055d-49fd-be07-537cebbe58aa/shuffle\n12/06/04 04:18:47 INFO server.Server: jetty-7.5.3.v20111011\n12/06/04 04:18:47 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:55566 STARTING\n12/06/04 04:18:47 INFO spark.ShuffleManager: Local URI: http://10.244.1.147:55566\njava.io.IOException: failure to login\n        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:433)\n        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:395)\n        at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:1435)\n        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1336)\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:244)", "Github comment from pdeyhim: looks like the issue was with HDFS. If I remove DFS from java_opt, everything seems to work a little better. I get \"Pi is roughly 3.13654\" back but the framework never disconnects from master. So looks like im in a better shape but two issues:\n\n1) HDFS issue\n2) Spark never disconnects from master \n\nmaster:\n\n\nI0604 21:23:26.347866  7125 master.cpp:902] Status update from slave@10.244.1.196:45099: task 1 of framework 201206042121-0-0001 is now in state TASK_RUNNING\nI0604 21:23:26.349199  7125 master.cpp:902] Status update from slave@10.244.1.196:45099: task 0 of framework 201206042121-0-0001 is now in state TASK_RUNNING\nI0604 21:23:26.520745  7125 master.cpp:902] Status update from slave@10.244.1.196:45099: task 0 of framework 201206042121-0-0001 is now in state TASK_FINISHED\nI0604 21:23:26.522171  7125 master.cpp:902] Status update from slave@10.244.1.196:45099: task 1 of framework 201206042121-0-0001 is now in state TASK_FINISHED\nI0604 21:23:27.134407  7125 master.cpp:1184] Sending 1 offers to framework 201206042121-0-0001\nI0604 21:23:27.135937  7125 master.cpp:679] Received reply for offer 201206042121-0-13\nI0604 21:23:27.136024  7125 master.cpp:1403] Filtered slave 201206042121-0-0 for framework 201206042121-0-0001 for 1 seconds\nI0604 21:23:28.138468  7125 master.cpp:1184] Sending 1 offers to framework 201206042121-0-0001\nI0604 21:23:28.219060  7125 master.cpp:1118] Framework 201206042121-0-0001 disconnected\n\n\nSpark:\n\n12/06/04 21:27:04 INFO spark.CacheTracker: Registering RDD ID 1 with cache\n12/06/04 21:27:04 INFO spark.CacheTrackerActor: Registering RDD 1 with 2 partitions\n12/06/04 21:27:04 INFO spark.CacheTracker: Registering RDD ID 0 with cache\n12/06/04 21:27:04 INFO spark.CacheTrackerActor: Registering RDD 0 with 2 partitions\n12/06/04 21:27:04 INFO spark.CacheTrackerActor: Asked for current cache locations\n12/06/04 21:27:04 INFO spark.MesosScheduler: Final stage: Stage 0\n12/06/04 21:27:04 INFO spark.MesosScheduler: Parents of final stage: List()\n12/06/04 21:27:04 INFO spark.MesosScheduler: Missing parents: List()\n12/06/04 21:27:04 INFO spark.MesosScheduler: Submitting Stage 0, which has no missing parents\n12/06/04 21:27:04 INFO spark.MesosScheduler: Got a job with 2 tasks\n12/06/04 21:27:04 INFO spark.MesosScheduler: Registered as framework ID 201206042121-0-0002\n12/06/04 21:27:04 INFO spark.MesosScheduler: Adding job with ID 0\n12/06/04 21:27:13 INFO spark.SimpleJob: Starting task 0:0 as TID 0 on slave 201206042121-0-1: ip-10-244-1-196.us-west-2.compute.internal (preferred)\n12/06/04 21:27:13 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 43 ms to serialize by spark.JavaSerializerInstance\n12/06/04 21:27:13 INFO spark.SimpleJob: Starting task 0:1 as TID 1 on slave 201206042121-0-1: ip-10-244-1-196.us-west-2.compute.internal (preferred)\n12/06/04 21:27:13 INFO spark.SimpleJob: Size of task 0:1 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance\n12/06/04 21:27:14 INFO spark.CacheTrackerActor: Started slave cache (size 1295.4MB) on ip-10-244-1-196.us-west-2.compute.internal\n12/06/04 21:27:15 INFO spark.SimpleJob: Finished TID 0 (progress: 1/2)\n12/06/04 21:27:15 INFO spark.SimpleJob: Finished TID 1 (progress: 2/2)\n12/06/04 21:27:15 INFO spark.MesosScheduler: Completed ResultTask(0, 0)\n12/06/04 21:27:15 INFO spark.MesosScheduler: Completed ResultTask(0, 1)\n12/06/04 21:27:15 INFO spark.SparkContext: Job finished in 10.738588868 s\nPi is roughly 3.1472", "Github comment from mateiz: Ah, could it be that your version of HDFS is different from the one Spark is compiled with? You can change the Hadoop version that Spark builds against in project/SparkBuild.scala. By default it's Hadoop 0.20.2. Note that 0.20.205 introduced security to HDFS, which is a major change in protocol. Maybe that's why this isn't working. If you look at the version of SparkBuild in the mesos-0.9 branch (https://github.com/mesos/spark/blob/mesos-0.9/project/SparkBuild.scala), it lists a few options for setting the Hadoop version.\n\nAs for the second issue, with the framework not disconnecting, you need to pass the parameter --failover_timeout=1 to the mesos-master command, or put it in Mesos's conf/mesos.conf file. This is documented at https://github.com/mesos/spark/wiki/Running-spark-on-mesos but it's easy to miss.", "Github comment from pdeyhim: I've tried the failover_timeout=1 with no luck. I see that on master my scala process lingers around even after I disconnect the Spark run command\n\nroot     28315 28298  0 15:52 ?        00:01:10 java -Djava.library.path=:/root/spark/lib:/root/spark/src/main/native:/tmp/install/mesos/lib/java -Xms4g -Xmx4g -Dspark.dfs=hdfs://10.244.1.147:9000 -Xbootclasspath/a:/tmp/install/scala-2.9.2/lib/jline.jar:/tmp/install/scala-2.9.2/lib/scala-compiler.jar:/tmp/install/scala-2.9.2/lib/scala-dbc.jar:/tmp/install/scala-2.9.2/lib/scala-library.jar:/tmp/install/scala-2.9.2/lib/scala-partest.jar:/tmp/install/scala-2.9.2/lib/scala-swing.jar:/tmp/install/scala-2.9.2/lib/scalacheck.jar:/tmp/install/scala-2.9.2/lib/scalap.jar -Dscala.usejavacp=true -Dscala.home=/tmp/install/scala-2.9.2 -Denv.emacs= scala.tools.nsc.MainGenericRunner -cp :/root/spark/core/target/scala-2.9.1/classes:/tmp/install/mesos/lib/java/mesos.jar:/root/spark/conf:/root/spark/repl/target/scala-2.9.1/classes:/root/spark/examples/target/scala-2.9.1/classes:/root/spark/core/lib/mesos.jar:/root/spark/lib_managed/jars/commons-net/commons\n\nis that normal?", "Github comment from mateiz: Oh, do you call System.exit(0) at the end of the program? Some of our example jobs unfortunately don't have that, because you used to not have to do it.\n\nMatei\n\nOn Jun 4, 2012, at 3:03 PM, pdeyhim wrote:\n\n> I've tried the failover_timeout=1 with no luck. I see that on master my scala process lingers around even after I disconnect the Spark run command\n> \n> root     28315 28298  0 15:52 ?        00:01:10 java -Djava.library.path=:/root/spark/lib:/root/spark/src/main/native:/tmp/install/mesos/lib/java -Xms4g -Xmx4g -Dspark.dfs=hdfs://10.244.1.147:9000 -Xbootclasspath/a:/tmp/install/scala-2.9.2/lib/jline.jar:/tmp/install/scala-2.9.2/lib/scala-compiler.jar:/tmp/install/scala-2.9.2/lib/scala-dbc.jar:/tmp/install/scala-2.9.2/lib/scala-library.jar:/tmp/install/scala-2.9.2/lib/scala-partest.jar:/tmp/install/scala-2.9.2/lib/scala-swing.jar:/tmp/install/scala-2.9.2/lib/scalacheck.jar:/tmp/install/scala-2.9.2/lib/scalap.jar -Dscala.usejavacp=true -Dscala.home=/tmp/install/scala-2.9.2 -Denv.emacs= scala.tools.nsc.MainGenericRunner -cp :/root/spark/core/target/scala-2.9.1/classes:/tmp/install/mesos/lib/java/mesos.jar:/root/spark/conf:/root/spark/repl/target/scala-2.9.1/classes:/root/spark/examples/target/scala-2.9.1/classes:/root/spark/core/lib/mesos.jar:/root/spark/lib_managed/jars/commons-net/commons\n> \n> is that normal?\n> \n> ---\n> Reply to this email directly or view it on GitHub:\n> https://github.com/mesos/spark/issues/138#issuecomment-6112775", "Github comment from pdeyhim: great! So looks like I've everything setup and working. Thanks again for your help so far!!\n\nThe only thing that I need to finish up is fixing the HDFS issue. What's the best way to build the mesos-0.9 branch? What Mesos branch do I have to use? prebuff? I pretty much tried every mesos build I could have think of but I keep getting the following error when I run Spark's example app:\n\n/run spark.examples.SparkPi master@10.244.1.147:5050\n12/06/05 21:46:24 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 339585269\n12/06/05 21:46:24 INFO spark.CacheTrackerActor: Registered actor on port 7077\n12/06/05 21:46:24 INFO spark.CacheTrackerActor: Started slave cache (size 323.9MB) on ip-10-244-1-147.us-west-2.compute.internal\n12/06/05 21:46:24 INFO spark.MapOutputTrackerActor: Registered actor on port 7077\n12/06/05 21:46:24 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-0131017e-70f3-46fc-9700-0e328062662c/shuffle\n12/06/05 21:46:24 INFO server.Server: jetty-7.5.3.v20111011\n12/06/05 21:46:24 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:35309 STARTING\n12/06/05 21:46:24 INFO spark.ShuffleManager: Local URI: http://10.244.1.147:35309\nException in thread \"Spark scheduler\" java.lang.UnsatisfiedLinkError: org.apache.mesos.MesosSchedulerDriver.initialize()V\n\tat org.apache.mesos.MesosSchedulerDriver.initialize(Native Method)\n\tat org.apache.mesos.MesosSchedulerDriver.<init>(MesosSchedulerDriver.java:89)\n\tat spark.MesosScheduler$$anon$2.run(MesosScheduler.scala:105)", "Github comment from mateiz: It will work with either with the trunk version of Mesos or the 0.9 release from http://www.mesosproject.org/download.html, but I think the error is because the configuration has changed a little. Take a look at https://github.com/mesos/spark/blob/mesos-0.9/conf/spark-env.sh.template -- you need to set a new variable called MESOS_NATIVE_LIBRARY to point to your libmesos.so, instead of the old MESOS_HOME variable. The Readme in this branch also describes this in more detail at the bottom: https://github.com/mesos/spark/blob/mesos-0.9/README.md. Sorry this isn't immediately apparent from the site -- I will make a new Spark release using 0.9 as the default soon and I'll update the docs then.", "Github comment from mateiz: By the way, you don't need the Mesos 0.9 branch to use a different version of HDFS! Just change the SparkBuild.scala in the old branch (search for hadoop in there). I was only showing the 0.9 stuff as an example because it lists the Maven dependency IDs for various Hadoop versions.", "Github comment from pdeyhim: Thanks! Done a few builds and still seeing the HDFS login issue which is very strange. Some googling got me here:\n\nhttps://issues.apache.org/jira/browse/HADOOP-7982\n\nAt the end of that thread, someone mentions this:\n\n\"Here's a test case which shows the issue. We tracked it down to a JNI issue  if when using libhdfs, the thread that started the JVM isn't the same thread that first uses libhdfs, it will fail with the error described in the JIRA. The fix as committed solves the problem.\"\n\nI'm not sure if that's whats going on here but wanted to post it here just in case if it rings any bell :)\n\n\n\n12/06/05 22:11:07 INFO spark.ShuffleManager: Local URI: http://10.244.1.196:57591\njava.io.IOException: failure to login\n        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:433)\n        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:395)\n        at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:1435)\n        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1336)\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:244)\n        at spark.broadcast.DfsBroadcast$.initialize(DfsBroadcast.scala:84)\n        at spark.broadcast.DfsBroadcastFactory.initialize(DfsBroadcast.scala:58)\n        at spark.broadcast.Broadcast$.initialize(Broadcast.scala:50)\n        at spark.Executor.init(Executor.scala:41)\nCaused by: javax.security.auth.login.LoginException: unable to find LoginModule class: org/apache/hadoop/security/UserGroupInformation$HadoopLoginModule\n        at javax.security.auth.login.LoginContext.invoke(LoginContext.java:808)\n        at javax.security.auth.login.LoginContext.access$000(LoginContext.java:186)\n        at javax.security.auth.login.LoginContext$4.run(LoginContext.java:683)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:680)\n        at javax.security.auth.login.LoginContext.login(LoginContext.java:579)\n        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:414)\n        ... 8 more\njava.lang.NullPointerException\n        at spark.Executor.launchTask(Executor.scala:53)\njava.lang.NullPointerException\n        at spark.Executor.launchTask(Executor.scala:53)", "Github comment from mateiz: Oh, I think I've seen something like this before. It's because we have two threads that use HDFS -- one would be the main thread, and one is this \"broadcast\" feature that can write files to HDFS in order to broadcast them to worker nodes. I would suggest removing the HDFS parameter from java-opts; that should make the second one not run. You can still access files in HDFS by passing the full hdfs://... URL. Or are you seeing the error despite having already removed this?", "Github comment from pdeyhim: interesting. Removing java_opt fixes the issue but I wasn't sure if removing namdenode's address from java_opt would still allows to me access HDFS. Without the java_opt, how would it know where namenode is?", "Github comment from mateiz: You can just specify the namenode as part of the hdfs:// URL. For example,\n\nval file = sparkContext.textFile(\"hdfs://my-machine:9000/user/foo/file.txt\")", "Github comment from pdeyhim: great! what's the best way to contact you? I'm working on getting spark deployed on potentially a large scale deployment and wanted to make sure I have everything covered. It'd be great to run some stuff with you.", "Github comment from mateiz: You can email me at matei@eecs.berkeley.edu. If you want we can arrange a phone call or something at some point.", "Github comment from mateiz: By the way, I'm going to close this issue because I've fixed the SparkPi example to call System.exit now and we figured out the other things. We can continue discussing over email.", "Imported from Github issue spark-138, originally reported by pdeyhim"], "derived": {"summary": "Hi there, \n\nI've been trying to make spark work on Mesos and have been stuck. I've tried everything I could  have think of.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "run spark.examples.SparkPi  hangs with no results - Hi there, \n\nI've been trying to make spark work on Mesos and have been stuck. I've tried everything I could  have think of."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-138, originally reported by pdeyhim"}]}}
{"project": "SPARK", "issue_id": "SPARK-411", "title": "The default broadcast implementation should not use HDFS", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0012-06-06T16:35:00.000+0000", "updated": "2012-10-22T14:55:37.000+0000", "description": "There seems to be a bug in some versions of HDFS that disallow it being initialized in two threads, and more generally, some users might not have HDFS. The default implementation should just launch an in-process HTTP server.", "comments": ["Imported from Github issue spark-139, originally reported by mateiz"], "derived": {"summary": "There seems to be a bug in some versions of HDFS that disallow it being initialized in two threads, and more generally, some users might not have HDFS. The default implementation should just launch an in-process HTTP server.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "The default broadcast implementation should not use HDFS - There seems to be a bug in some versions of HDFS that disallow it being initialized in two threads, and more generally, some users might not have HDFS. The default implementation should just launch an in-process HTTP server."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-139, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-544", "title": "Provide a Configuration class in addition to system properties", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "labels": [], "created": "0012-06-15T22:29:00.000+0000", "updated": "2014-04-30T00:43:06.000+0000", "description": "This is a much better option for people who want to connect to multiple Spark clusters in the same program, and for unit tests.", "comments": ["Github comment from tjhunter: In order to configure my applications, I have used the following system from \ntwitter:\nhttps://github.com/twitter/util/blob/master/util-\ncore/src/main/scala/com/twitter/util/Config.scala\nHere is a rationale:\nhttp://robey.lag.net/2012/03/26/why-config.html\n\nTim\n\nOn Friday, June 15, 2012 11:29:53 PM you wrote:\n> This is a much better option for people who want to connect to multiple\n> Spark clusters in the same program, and for unit tests.\n> \n> ---\n> Reply to this email directly or view it on GitHub:\n> https://github.com/mesos/spark/issues/140\n-- \nTimothy Hunter\nPh.D Student\nComputer Science\nUniversity of California - Berkeley\nwww.eecs.berkeley.edu/~tjhunter/\nT. 404 421 3075", "Imported from Github issue spark-140, originally reported by mateiz", "I like the approach in http://robey.lag.net/2012/03/26/why-config.html. The config classes in https://github.com/twitter/util/blob/master/util-core/src/main/scala/com/twitter/util/Config.scala is not much extra code either. \n\nMatei, are you okay with using this?\n\n\n", "I find this kind of confusing and too specific to Scala. I'd prefer something where you just say string key-value pairs. Then it will be easy to explain to existing users, and it will be easy to support it in Java, Python and Shark as well.", "Yeah, it is pretty Scala specific, that's a fair point.", "Why not just use regular commons-configuration or JSON based configuration. They are fairly universal and very readable.\nI would be more than happy to take this up if no one else is working on this already.\n\nI went ahead an did a simple grep of the `System.properies` and listed out all the properties as a json document and as a properties file.\nIMO JSON looks *nicer* but properties files are easier to handle. Any opinions?\n\nhttps://gist.github.com/ankurcha/5655646", "We started a discussion about the design of this on dev mailing list and collected a few opinions, which I summarized below. The link to the original discussion is http://www.mail-archive.com/dev@spark.incubator.apache.org/msg00446.html. \n\n1) Define a Configuration class which contains all the options available for Spark application. A Configuration instance can be de-/serialized from/to a formatted file. Most of us tend to agree that Typesafe Config library is a good choice for the Configuration class.\n2) Each application (SparkContext) has one Configuration instance and it is initialized by the application which creates it (either coded in app (apps could explicitly read from io stream or command line arguments), or system properties, or env vars). \n3) For an application the overriding rule should be code > system properties > env vars. Over time we will deprecate the env vars and maybe even system properties.\n4) When launching an Executor on a slave node, the Configuration is firstly initialized using the node-local configuration file as default  (instead of the env vars at present), and then the Configuration passed from application driver context will override specific options specified in default. Certain options in app's Configuration will always override those in node-local, because these options need to be the consistent across all the slave nodes, e.g. spark.serializer. In this case if any such options is not set in app's Config, a value will be provided by the system. On the other hand, some options in app's Config will never override those in node-local. as they're not meat to be set in app, e.g. spark.local.dir.  \n\n\n\n\n\n\n\n\n\n\n\n\n\n", "\"Inventing a class with its own interface to hold a piece of information is like inventing a new language to write every short story.\" http://www.codequarterly.com/2011/rich-hickey/\n\nIs there tech that can be leveraged here? It's- alas- a library on top of a rather sizable framework, but Chronos, for example, uses Dropwizard Configurable AssetsBundle for configuration (https://github.com/bazaarvoice/dropwizard-configurable-assets-bundle). I get the idea that the use case here, the need, is to serialize some state around, but Configuration is a massive ops concern and it'd be good to party up, I feel.", "Aurora- a Mesos scheduler from Twitter, the other Mesos scheduler- has code commited as of two days ago. They saw fit to create IDL definitions for tasks and their associate configuration. Not recommending any particular solution here nor there, but perhaps and if perhaps not maybe still interesting & useful reference- \nhttps://github.com/twitter/aurora/blob/master/thrift/src/main/thrift/com/twitter/aurora/gen/api.thrift#L130-L160\nor specifically now,\nhttps://github.com/twitter/aurora/commit/c248931344e46a0e99bfbad6fdf3e08d7473008b#L130-160", "re: @rektide\n\nTypesafe Config is used in Akka, Spray, Play, and other Scala frameworks.  We use it for all of our Scala apps.  If you look at how Akka uses it, they don't need to build a Configuration class on top.  Instead, you just pass in a com.typesafe.Config object into your class, say SparkContext.\n\nFor non-Java/Scala apps, they can write JSON config files, which Typesafe Config can parse easily.\n\nThe Config object can be initialized and created in multiple ways, but typically from a file, or from a Map, and you can merge it with a default file loaded from resources / jar, or even with system properties.\n\nFor example, let's say you have\n\n    class SparkContext(config: Config) {\n        val port = config.getInt(\"spark.port\")\n\nThat's pretty succinct for getting the value out.\n\nThe advantage of a config class is that you have a type-safe access to the properties, but the disadvantage is that you have to maintain the API.  I honestly feel like it's been OK to use the config without a formal class.\n\n-Evan\n", "By the way, I've started work on this, using Typesafe Config-based configuration objects (which can parse from JSON as well).\n\nThe first, primary goal is to move completely away from using System properties as global variables (there are multiple places that get, then set, for example, \"spark.driver.port\").   This will be a big step towards being able to safely have multiple contexts within the same process.\n\nThis will also allow much richer config options than simple strings.", "Cool, looking forward to it!", "Pull Request for the first part has been submitted, in case anybody wants to have a look:\nhttps://github.com/apache/incubator-spark/pull/55", "By the way, new progress is taking place in a new pull request:\nhttps://github.com/apache/incubator-spark/pull/230"], "derived": {"summary": "This is a much better option for people who want to connect to multiple Spark clusters in the same program, and for unit tests.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Provide a Configuration class in addition to system properties - This is a much better option for people who want to connect to multiple Spark clusters in the same program, and for unit tests."}, {"q": "What updates or decisions were made in the discussion?", "a": "By the way, new progress is taking place in a new pull request:\nhttps://github.com/apache/incubator-spark/pull/230"}]}}
{"project": "SPARK", "issue_id": "SPARK-543", "title": "Spark for Python", "status": "Resolved", "priority": null, "reporter": "Russell Jurney", "assignee": "Josh Rosen", "labels": [], "created": "0012-06-26T18:21:00.000+0000", "updated": "2013-01-20T12:34:51.000+0000", "description": "I want to use Spark from Python, not Scala.", "comments": ["Github comment from alexy: Patches welcome!", "Github comment from mateiz: Yeah, agree with Alexy :). If you're interested in this I'd be glad to provide some advice. We are also working on a Java API now.", "Github comment from tjhunter: Note that you can already use jython. You loose the benefit of using C libraries like numpy, though, so I found the benefit over scala to be quite marginal.\nIf you try to integrate cpython with some java code, there are non-trivial issues to solve. Maybe using piped rdds would work?", "Imported from Github issue spark-141, originally reported by rjurney", "Resolving this since PySpark has been merged into the master branch."], "derived": {"summary": "I want to use Spark from Python, not Scala.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Spark for Python - I want to use Spark from Python, not Scala."}, {"q": "What updates or decisions were made in the discussion?", "a": "Resolving this since PySpark has been merged into the master branch."}]}}
{"project": "SPARK", "issue_id": "SPARK-410", "title": "Scalacheck groupId has changed https://github.com/rickynils/scalacheck/i...", "status": "Resolved", "priority": null, "reporter": "rrmckinley", "assignee": null, "labels": [], "created": "0012-06-29T11:00:00.000+0000", "updated": "2012-10-19T22:50:25.000+0000", "description": "...ssues/24. Necessary to build with scalaVersion 2.9.2. Works with 2.9.1 too.", "comments": ["Github comment from mateiz: Thanks for the fix!", "Imported from Github issue spark-142, originally reported by rrmckinley"], "derived": {"summary": "ssues/24. Necessary to build with scalaVersion 2.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Scalacheck groupId has changed https://github.com/rickynils/scalacheck/i... - ssues/24. Necessary to build with scalaVersion 2."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-142, originally reported by rrmckinley"}]}}
{"project": "SPARK", "issue_id": "SPARK-409", "title": "Include Shark on default Spark AMI", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0012-06-29T15:45:00.000+0000", "updated": "2012-10-22T14:55:33.000+0000", "description": null, "comments": ["Github comment from mateiz: This is now done!", "Imported from Github issue spark-143, originally reported by mateiz"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Include Shark on default Spark AMI"}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-143, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-408", "title": "User's JARs are not on the classpath when instantiating custom serializer", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0012-07-06T15:12:00.000+0000", "updated": "2012-10-22T14:55:34.000+0000", "description": null, "comments": ["Github comment from mateiz: This has been fixed now by Denny's code.", "Imported from Github issue spark-144, originally reported by mateiz"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "User's JARs are not on the classpath when instantiating custom serializer"}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-144, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-407", "title": "add Accumulatable, add corresponding docs & tests for accumulators", "status": "Resolved", "priority": null, "reporter": "squito", "assignee": null, "labels": [], "created": "0012-07-12T09:05:00.000+0000", "updated": "2012-10-19T22:50:35.000+0000", "description": "Preliminary version of some updates to accumulators -- it works, but probably requires some discussion around the exact API.\n\nI didn't want to break all existing accumulators, so to add an `addInPlace(T,Y)`\nmethod meant creating a whole new type, which I've called `Accumulatable`. Also, I had to distinguish the old `+=` method from the new one.  I chose `+:=`, but that was rather arbitrary, I'm no expert on scala idioms.  (It seems that the old operator is really closer to `++=`, and the new one should be `+=`, but I guess we can't change that.)\n\nI've also added tests for accumulators.  These include an example where the current value of an accumulator is read during a task.  The example application is stochastic gradient descent, where seeing the current value is really critical.  Should `accumulator.value` throw an exception (so the user doesn't think they're getting the global value), but maybe instead we expose `accumulator.localValue`?", "comments": ["Github comment from mateiz: Hey, so one major comment on this. Instead of making Accumulable a subclass of Accumulator, I think it would be better to do things the other way around. That is, Accumulable[T, Y] should be the main class that Spark cares about, and Accumulator[T] should extend Accumulable[T, T]. This will require changing a bit more stuff in the rest of the Spark code that depends on accumulators, but it's worth it. Also, if you do this, then the +:= method can just be called +=. The += in Accumulator will be a special case for when T and Y are the same. Does this sound good?", "Github comment from squito: this is a great suggestion.  I'll make the change and update the pull request\n\nOn Thu, Jul 12, 2012 at 10:46 AM, Matei Zaharia\n<reply@reply.github.com>\nwrote:\n> Hey, so one major comment on this. Instead of making Accumulable a subclass of Accumulator, I think it would be better to do things the other way around. That is, Accumulable[T, Y] should be the main class that Spark cares about, and Accumulator[T] should extend Accumulable[T, T]. This will require changing a bit more stuff in the rest of the Spark code that depends on accumulators, but it's worth it. Also, if you do this, then the +:= method can just be called +=. The += in Accumulator will be a special case for when T and Y are the same. Does this sound good?\n>\n> ---\n> Reply to this email directly or view it on GitHub:\n> https://github.com/mesos/spark/pull/145#issuecomment-6942302", "Github comment from mateiz: This looks good! I made a few changes on the diff but once you have those I think it's good to merge in. The most important is keeping the name of the old addInPlace method on accumulator so that people don't have to modify their code.", "Github comment from mateiz: Hey, not sure if you saw, but I posted a line comment about the closure failing in the test. I think it's best to remove the whole test because the value being readable on slave nodes is actually not the intended behavior right now, and only happens in local mode, not on a cluster. See the explanation there. (Just posting here in case you don't get notified on line comments).", "Github comment from squito: Hi,\n\nsorry I hadn't gotten to this yet, just got sidetracked (I did get\nnotified of the inline comments too, btw).\n\nI can definitely make this change and resubmit.  I'd like to also make\nreading from accumulator.value throw an exception on slave nodes (the\nsame way that assigning a new value does), so that this is more clear.\n I know this will break some existing code (including some of our\nown), but really this is just silently waiting to explode when someone\ngoes from local mode to a cluster.  I think we'd prefer to prevent\nthose as soon as possible.\n\nI still don't really understand why the value shouldn't be readable,\nthough.  Its not any more efficient to make the value a local variable\nand send that workers, is it?  You'd just rather the user have to do\nit explicitly, so they realize how much data they are sending around?\n\nI'll try to write the SGD using mapPartitions example at some point in\na separate set of commits ...\n\nthanks\n\nOn Mon, Jul 23, 2012 at 8:26 PM, Matei Zaharia\n<reply@reply.github.com>\nwrote:\n> Hey, not sure if you saw, but I posted a line comment about the closure failing in the test. I think it's best to remove the whole test because the value being readable on slave nodes is actually not the intended behavior right now, and only happens in local mode, not on a cluster. See the explanation there. (Just posting here in case you don't get notified on line comments).\n>\n> ---\n> Reply to this email directly or view it on GitHub:\n> https://github.com/mesos/spark/pull/145#issuecomment-7197850", "Github comment from mateiz: Looks good, thanks a lot!\n\nThe reason that making value readable is tricky is because we actually try to initialize it to \"zero\" in each thread in readObject(), in order to add back all the changes at the end. This just didn't happen in local mode with one thread because we already have that accumulator in our thread. So we'd need to have two fields -- the \"real\" value and the \"accumulating\" value on this worker thread -- and two separate ways to read them. Anyway, I'll think more about including this. Having the exception in there will help a lot until then.", "Imported from Github issue spark-145, originally reported by squito"], "derived": {"summary": "Preliminary version of some updates to accumulators -- it works, but probably requires some discussion around the exact API. I didn't want to break all existing accumulators, so to add an `addInPlace(T,Y)`\nmethod meant creating a whole new type, which I've called `Accumulatable`.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "add Accumulatable, add corresponding docs & tests for accumulators - Preliminary version of some updates to accumulators -- it works, but probably requires some discussion around the exact API. I didn't want to break all existing accumulators, so to add an `addInPlace(T,Y)`\nmethod meant creating a whole new type, which I've called `Accumulatable`."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-145, originally reported by squito"}]}}
{"project": "SPARK", "issue_id": "SPARK-542", "title": "Cache Miss when machine have multiple hostname", "status": "Closed", "priority": "Minor", "reporter": "frankvictor", "assignee": null, "labels": [], "created": "0012-07-13T00:51:00.000+0000", "updated": "2014-11-06T06:57:08.000+0000", "description": "HI, I encountered a weird runtime of pagerank in last few day.\nAfter debugging the job, I found it was caused by the DNS name.\n\nThe machines of my cluster have multiple hostname, for example, slave 1 have name (c001 and c001.cm.cluster)\nwhen spark adding cache in cacheTracker, it get \"c001\" and add cache use it.\nBut when schedule task in SimpleJob, the msos offer give spark \"c001.cm.cluster\".\nso It will never get preferred location!\n\nI thinks spark should handle the multiple hostname case(by using ip instead of hostname, or some other methods).\n\nThanks!", "comments": ["Imported from Github issue spark-146, originally reported by frankvictor", "using ip has similar problem. A machine can have multiple ips.", "Spark uses only hostnames - not ip's.\nEven for hostnames, it should ideally pick only the canonical hostname - not the others.\n\nThis was done by design in 0.8 ... try to find if multiple host names/ip's are all referring to the same physical host/container is fraught with too many issues.", "New versions of Spark have ways to specify the hostname and IP address to bind to that should address this issue."], "derived": {"summary": "HI, I encountered a weird runtime of pagerank in last few day. After debugging the job, I found it was caused by the DNS name.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Cache Miss when machine have multiple hostname - HI, I encountered a weird runtime of pagerank in last few day. After debugging the job, I found it was caused by the DNS name."}, {"q": "What updates or decisions were made in the discussion?", "a": "New versions of Spark have ways to specify the hostname and IP address to bind to that should address this issue."}]}}
{"project": "SPARK", "issue_id": "SPARK-406", "title": "Broadcast refactoring/cleaning up", "status": "Resolved", "priority": null, "reporter": "Mosharaf Chowdhury", "assignee": null, "labels": [], "created": "0012-07-15T10:17:00.000+0000", "updated": "2012-10-19T22:50:25.000+0000", "description": "- Removed DfsBroadcast completely.\n- Removed ChainedBroadcast (TreeBroadcast is a superset of this)\n- Refactored code for TreeBroadcast and BitTorrentBroadcast (still there seems to be a lot of code overlapped, but they have differences in the details). The common object is called MultiTracker that basically acts as the torrent indexing service for ongoing broadcasts.\n- Added stop() for the broadcast subsystem. Haven't added it to SparkContext though\n- Merged several configuration options to one for simplicity. The ones that remain are setup to work without any changes. Just changing the broadcastFactory should do.\n- Changed \"isLocal\" in SparkContext to not consider localhost as local. To use local you have to use \"local\" and not \"localhost\". The reasoning is that using the localhost IP still acts like that, but localhost doesn't; plus, it helps in checking broadcast in the same machine without having to use the actual IP.\n\nTODO\n- Add a HttpBroadcast backup for TreeBroadcast and BitTorrentBroadcast methods just in case they fail. TreeBroadcast shouldn't. BitTorrentBroadcast has a weird corner case that will take some time in figuring out and hasn't been addressed in this patch.", "comments": ["Github comment from mateiz: Hey Mosharaf, did you ever get this to work with the block manager? Apart from that, the other thing we should do is make the broadcast objects be classes instead of singleton objects. You can then initialize and stop them in SparkContext. I've recently done that for the BlockManagerMaster too.", "Github comment from mosharaf: BroadcastManager is a class now with init() and stop() methods that are called from SparkEnv. Individual broadcast mechanisms still have their own objects though.", "Github comment from mateiz: Thanks. Are you planning to remove the individual objects too or is that too much work for right now?", "Github comment from mateiz: (Just wondering whether this is ready to merge)", "Github comment from mateiz: I've merged this to make it easier to develop new stuff. Thanks! But we should still remove the standalone objects for MultiTracker & co, for people who want to have multiple SparkContexts in the same app.", "Imported from Github issue spark-147, originally reported by mosharaf"], "derived": {"summary": "- Removed DfsBroadcast completely. - Removed ChainedBroadcast (TreeBroadcast is a superset of this)\n- Refactored code for TreeBroadcast and BitTorrentBroadcast (still there seems to be a lot of code overlapped, but they have differences in the details).", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Broadcast refactoring/cleaning up - - Removed DfsBroadcast completely. - Removed ChainedBroadcast (TreeBroadcast is a superset of this)\n- Refactored code for TreeBroadcast and BitTorrentBroadcast (still there seems to be a lot of code overlapped, but they have differences in the details)."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-147, originally reported by mosharaf"}]}}
{"project": "SPARK", "issue_id": "SPARK-405", "title": "Use test fixtures or setup/teardown methods in unit tests", "status": "Resolved", "priority": null, "reporter": "Josh Rosen", "assignee": null, "labels": [], "created": "0012-07-16T17:26:00.000+0000", "updated": "2012-10-19T22:50:26.000+0000", "description": "I've noticed that individual test failures can cause cascades of \"Failed to bind\" / \"Address already in use\" errors, because the `SparkContext.stop()` cleanup method is not called after the failure.\n\nOne solution would be to move the creation and destruction of `SparkContext` objects to `before()` and `after()`methods, using ScalaTests's [BeforeAndAfter](http://www.scalatest.org/scaladoc/1.8/#org.scalatest.BeforeAndAfter) trait.\n\nAnother option would be to override the [withFixture](http://www.scalatest.org/scaladoc/1.8/#org.scalatest.fixture.FunSuite) method to supply the `SparkContext` objects to the tests.", "comments": ["Imported from Github issue spark-148, originally reported by JoshRosen"], "derived": {"summary": "I've noticed that individual test failures can cause cascades of \"Failed to bind\" / \"Address already in use\" errors, because the `SparkContext. stop()` cleanup method is not called after the failure.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Use test fixtures or setup/teardown methods in unit tests - I've noticed that individual test failures can cause cascades of \"Failed to bind\" / \"Address already in use\" errors, because the `SparkContext. stop()` cleanup method is not called after the failure."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-148, originally reported by JoshRosen"}]}}
{"project": "SPARK", "issue_id": "SPARK-404", "title": "Instantiating custom serializer using user's classpath", "status": "Resolved", "priority": null, "reporter": "Denny Britz", "assignee": null, "labels": [], "created": "0012-07-17T13:14:00.000+0000", "updated": "2012-10-19T22:50:28.000+0000", "description": "- Fixes #144", "comments": ["Github comment from mateiz: Looks good, thanks!", "Imported from Github issue spark-149, originally reported by dennybritz"], "derived": {"summary": "- Fixes #144.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Instantiating custom serializer using user's classpath - - Fixes #144."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-149, originally reported by dennybritz"}]}}
{"project": "SPARK", "issue_id": "SPARK-403", "title": "Failing Test: FileSuite - Read SequenceFile using new Hadoop API", "status": "Resolved", "priority": null, "reporter": "Denny Britz", "assignee": null, "labels": [], "created": "0012-07-18T12:13:00.000+0000", "updated": "2012-10-19T22:50:35.000+0000", "description": null, "comments": ["Github comment from JoshRosen: A few days ago, I found the commit that introduced this bug and commented on it: https://github.com/mesos/spark/commit/6980b67557105490b6354dbb5331adace52d685c#core-src-main-scala-spark-sparkcontext-scala-P8", "Github comment from mateiz: Fixed this right now in commit 6f44c0db74cc065c676d4d8341da76d86d74365e.", "Imported from Github issue spark-150, originally reported by dennybritz"], "derived": {"summary": "", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Failing Test: FileSuite - Read SequenceFile using new Hadoop API"}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-150, originally reported by dennybritz"}]}}
{"project": "SPARK", "issue_id": "SPARK-402", "title": "Examples ship to to cluster", "status": "Resolved", "priority": null, "reporter": "Denny Britz", "assignee": null, "labels": [], "created": "0012-07-18T12:18:00.000+0000", "updated": "2012-10-19T22:50:36.000+0000", "description": "- Fixes #118\n- I wasn't sure how to best get the path to the example JAR, I didn't want to do it manually in every examples. I added another variable in the ./run script that's being used by the examples. Let me know if that's not the best way.\n- Should update the example documentation to tell people to run \"sbt/sbt package\" before running the examples.", "comments": ["Github comment from mateiz: This looks good, but can you also update the README to say that you should run sbt package instead of sbt compile? Otherwise the examples JAR won't get built. Also, rename the EXAMPLES_JAR variable to SPARK_EXAMPLES_JAR so that it doesn't conflict with other environment variables the user might make.", "Github comment from mateiz: Hey Denny, not sure if you saw my comment here, but can you rename the environment variable as mentioned above, and update the README to say sbt package? Otherwise it looks good.", "Github comment from dennybritz: I made the changes you suggested.", "Github comment from mateiz: Great, thanks!", "Imported from Github issue spark-151, originally reported by dennybritz"], "derived": {"summary": "- Fixes #118\n- I wasn't sure how to best get the path to the example JAR, I didn't want to do it manually in every examples. I added another variable in the.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Examples ship to to cluster - - Fixes #118\n- I wasn't sure how to best get the path to the example JAR, I didn't want to do it manually in every examples. I added another variable in the."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-151, originally reported by dennybritz"}]}}
{"project": "SPARK", "issue_id": "SPARK-401", "title": "Always destroy SparkContext in after block for the unit tests.", "status": "Resolved", "priority": null, "reporter": "Denny Britz", "assignee": null, "labels": [], "created": "0012-07-18T12:25:00.000+0000", "updated": "2012-10-19T22:50:33.000+0000", "description": "- Fixes #148\n- Tested. Tests still passing, except for one that wasn't passing before, see #150\n- I intentionally didn't instantiate a `SparkContext` in a before{} block, because every test may instantiate one with different parameters, or not use one at all.", "comments": ["Github comment from mateiz: Thanks! Merged this.", "Imported from Github issue spark-152, originally reported by dennybritz"], "derived": {"summary": "- Fixes #148\n- Tested. Tests still passing, except for one that wasn't passing before, see #150\n- I intentionally didn't instantiate a `SparkContext` in a before{} block, because every test may instantiate one with different parameters, or not use one at all.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Always destroy SparkContext in after block for the unit tests. - - Fixes #148\n- Tested. Tests still passing, except for one that wasn't passing before, see #150\n- I intentionally didn't instantiate a `SparkContext` in a before{} block, because every test may instantiate one with different parameters, or not use one at all."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-152, originally reported by dennybritz"}]}}
{"project": "SPARK", "issue_id": "SPARK-400", "title": "Java API", "status": "Resolved", "priority": null, "reporter": "Josh Rosen", "assignee": null, "labels": [], "created": "0012-07-18T19:07:00.000+0000", "updated": "2012-10-19T22:50:29.000+0000", "description": "An initial version of a Java API for Spark.  In addition to the Java API, this commit adds a `distinct()` method to RDDs and fixes some issues related to `DoubleRDDFunctions`.", "comments": ["Github comment from mateiz: This looks really good, but a couple of comments:\n* Can you rename the package to just spark.java? Seems a bit simpler.\n* It would be better to replace the JavaLR with a JavaHdfsLR that reads from a file. The reason is that this will be more common for people wanting to use Spark, so we might as well include it as an example.\n* In the WordCount example, I would inline the functions as anonymous inner classes just to show that syntax.\n* In your Scala function definitions (def), always add a return type instead of relying on type inference. This will ensure that we notice when we break binary compatibility by changing a type. We've tried to do this in the rest of the code.", "Github comment from JoshRosen: > Can you rename the package to just `spark.java`? Seems a bit simpler.\n\nScala package references are relative, so import statements like\n\n```scala\npackage spark\nimport java.util.List\n```\n\nwould be resolved against the `spark.java` package, causing compile errors.  A workaround is to use the top-level [`_root_`](http://stackoverflow.com/questions/687071/what-is-the-root-package-in-scala) package to import from the correct top-level `java` package:\n\n```scala\npackage spark\nimport _root_.java.util.List\n```\n\nIt's easy to add the `_root_` prefixes to the existing code, but the change would affect a lot of files.  Do you still want to rename the package?", "Github comment from mateiz: Oh, got it. Let's not rename it then.\n\nMatei\n\nOn Jul 22, 2012, at 1:08 PM, Josh Rosen <reply@reply.github.com> wrote:\n\n>> Can you rename the package to just `spark.java`? Seems a bit simpler.\n> \n> Scala package references are relative, so import statements like\n> \n> ```scala\n> package spark\n> import java.util.List\n> ```\n> \n> would be resolved against the `spark.java` package, causing compile errors.  A workaround is to use the top-level [`_root_`](http://stackoverflow.com/questions/687071/what-is-the-root-package-in-scala) package to import from the correct top-level `java` package:\n> \n> ```scala\n> package spark\n> import _root_.java.util.List\n> ```\n> \n> It's easy to add the `_root_` prefixes to the existing code, but the change would affect a lot of files.  Do you still want to rename the package?\n> \n> ---\n> Reply to this email directly or view it on GitHub:\n> https://github.com/mesos/spark/pull/153#issuecomment-7165012", "Github comment from JoshRosen: I pushed some changes that address your comments; let me know what you think.", "Github comment from JoshRosen: Thanks for the feedback; I pushed another commit addressing these comments.", "Github comment from JoshRosen: I removed the dependency on `StringOps` and added in a few RDD methods that were missing from the Java API (`persist`, `splits`, `glom`, and `mapPartitions`).", "Github comment from mateiz: Looks good. I'm going to merge it now, and we can make further changes as smaller commits. I'm also trying to get feedback on the API from a few other people. But this is an awesome start!", "Imported from Github issue spark-153, originally reported by JoshRosen"], "derived": {"summary": "An initial version of a Java API for Spark. In addition to the Java API, this commit adds a `distinct()` method to RDDs and fixes some issues related to `DoubleRDDFunctions`.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Java API - An initial version of a Java API for Spark. In addition to the Java API, this commit adds a `distinct()` method to RDDs and fixes some issues related to `DoubleRDDFunctions`."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-153, originally reported by JoshRosen"}]}}
{"project": "SPARK", "issue_id": "SPARK-399", "title": "Broadcast UUID cannot be casted to Integer", "status": "Resolved", "priority": null, "reporter": "Tathagata Das", "assignee": null, "labels": [], "created": "0012-07-23T00:35:00.000+0000", "updated": "2012-10-19T22:50:26.000+0000", "description": "When using a lot of broadcast variables, I sometimes get the following exception.\n\njava.lang.ClassCastException: java.util.UUID cannot be cast to java.lang.Integer\n\tat scala.runtime.BoxesRunTime.unboxToInt(Unknown Source)\n\tat scala.Tuple2._2$mcI$sp(Tuple2.scala:22)\n\tat spark.BoundedMemoryCache.reportEntryDropped(BoundedMemoryCache.scala:94)\n\tat spark.BoundedMemoryCache.ensureFreeSpace(BoundedMemoryCache.scala:84)\n\tat spark.BoundedMemoryCache.put(BoundedMemoryCache.scala:40)\n\tat spark.KeySpace.put(Cache.scala:60)\n\tat spark.broadcast.HttpBroadcast.readObject(HttpBroadcast.scala:38)\n\tat sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:616)\n\nHere is my guess.  The UUID assigned to a broadcasted variable using UUID.randomUUID is a 128 bit value. When a broadcast variable needs to be dropped, reportEntryDropped attempts to cast the UUID to Int for reporting to the cacheTracker. Thats where the casting fails.", "comments": ["Github comment from henryemilner: I encountered what looks like a related issue:\n\n2012-07-27 17:41:48,660 [main] INFO  spark.BoundedMemoryCache  - Dropping key ((1,4003b6c9-737f-4504-8144-ea39d1d0b529), 0) of size 255414 to make space\nscala.MatchError: (1,4003b6c9-737f-4504-8144-ea39d1d0b529) (of class scala.Tuple2)\n\tat spark.CacheTracker.dropEntry(CacheTracker.scala:228)\n\tat spark.BoundedMemoryCache.reportEntryDropped(BoundedMemoryCache.scala:93)\n\tat spark.BoundedMemoryCache.ensureFreeSpace(BoundedMemoryCache.scala:84)\n\tat spark.BoundedMemoryCache.put(BoundedMemoryCache.scala:40)\n\tat spark.KeySpace.put(Cache.scala:60)\n        [...]\n\nIt looks like dropEntry expects @datasetId to be an (Int, Int), but when evicting a broadcast variable it is an (Int, UUID).  It seems like this was just overlooked when broadcast variables were added - the rest of the code calls this thing an rddId and expects it to be an Int.\n\nIn fact, it appears to me that the master is never notified via an AddedToCache message when a slave fetches a broadcast variable - TreeBroadcast, for example, calls KeySpace.put() directly.  This means it would not make sense to send a DroppedFromCache message to the master anyway.  So I assume it is safe to work around this exception by adding a catchall case to CacheTracker.dropEntry().  But this means that the master doesn't include slaves' broadcast variables in its estimates of their memory usages.  That seems like a bigger bug to me, if it is one.  Can someone clarify what's intended here?", "Github comment from tdas: Here are my thoughts. The CacheTracker is designed to keep track of the locations of RDD partitions. So cacheTracker.dropEntry() is meant to notify the master when a RDD's partition falls off the cache. Now the same BoundedMemoryCache is also being using by the broadcast stuff. Since broadcast variables are not something that the CacheTracker's master cares about, it does not need to be notified if a broadcast variable falls off cache. In that case, a simple check that calls cacheTracker.dropEntry only when the datasetId is of type (Int, Int) should suffice. I fixed this today and it works. However, its in my own private repo that will eventually be pushed to mesos/spark/dev branch. For now, you can fix it immediately by this. Hope this helps!\n\n```scala\n protected def reportEntryDropped(datasetId: Any, partition: Int, entry: Entry) {\n    logInfo(\"Dropping key (%s, %d) of size %d to make space\".format(datasetId, partition, entry.size))\n    // TODO: remove BoundedMemoryCache    \n    val (keySpaceId, innerDatasetId) = datasetId.asInstanceOf[(Any, Any)] \n    innerDatasetId match {\n      case rddId: Int =>\n        SparkEnv.get.cacheTracker.dropEntry(rddId, partition)\n      case broadcastUUID: java.util.UUID =>\n        // TODO: Maybe something should be done if the broadcasted variable falls out of cache  \n      case _ => \n    }    \n  } \n```\n\nI agree that this would lead to miscalculation in the master's estimate of the cache usage of the slave. So yes there is a bigger problem that has been cause by Broadcast using BoundedMemoryCache. This will be correctly fixed in the dev branch (and therefore next version of Spark) when the BoundedMemoryCache will be replaced by the new BlockManager.", "Github comment from henryemilner: Great, thanks for the clarification.", "Github comment from mateiz: I've implemented a fix similar to TD's in master now too.", "Imported from Github issue spark-154, originally reported by tdas"], "derived": {"summary": "When using a lot of broadcast variables, I sometimes get the following exception. java.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Broadcast UUID cannot be casted to Integer - When using a lot of broadcast variables, I sometimes get the following exception. java."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-154, originally reported by tdas"}]}}
{"project": "SPARK", "issue_id": "SPARK-398", "title": "Support for external hashing and sorting", "status": "Resolved", "priority": null, "reporter": "Harvey Feng", "assignee": null, "labels": [], "created": "0012-07-23T15:43:00.000+0000", "updated": "2012-10-19T22:50:30.000+0000", "description": "Also includes a small fix in RangePartitioner - ascending/descending is accounted for in getPartition(), so sorting rddSample doesn't need the ascending check.", "comments": ["Github comment from harveyfeng: Sorry, meant this for the 'dev' branch...", "Imported from Github issue spark-155, originally reported by harveyfeng"], "derived": {"summary": "Also includes a small fix in RangePartitioner - ascending/descending is accounted for in getPartition(), so sorting rddSample doesn't need the ascending check.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support for external hashing and sorting - Also includes a small fix in RangePartitioner - ascending/descending is accounted for in getPartition(), so sorting rddSample doesn't need the ascending check."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-155, originally reported by harveyfeng"}]}}
{"project": "SPARK", "issue_id": "SPARK-397", "title": "Support for external sorting and hashing", "status": "Resolved", "priority": null, "reporter": "Harvey Feng", "assignee": null, "labels": [], "created": "0012-07-23T15:50:00.000+0000", "updated": "2012-10-19T22:50:24.000+0000", "description": "Also includes a small fix in RangePartitioner - ascending/descending is accounted for in getPartition(), so sorting rddSample doesn't need the ascending check.", "comments": ["Github comment from rxin: For a change of this size, it would be nice to have a high level writeup for it (1 - 5 pages) to explain the general structure / components. It'd also be very useful for reviewers and others to read when they want to modify this in the future.", "Imported from Github issue spark-156, originally reported by harveyfeng"], "derived": {"summary": "Also includes a small fix in RangePartitioner - ascending/descending is accounted for in getPartition(), so sorting rddSample doesn't need the ascending check.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support for external sorting and hashing - Also includes a small fix in RangePartitioner - ascending/descending is accounted for in getPartition(), so sorting rddSample doesn't need the ascending check."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-156, originally reported by harveyfeng"}]}}
{"project": "SPARK", "issue_id": "SPARK-396", "title": "Logging Throwables in Info and Debug", "status": "Resolved", "priority": null, "reporter": "Paul Cavallaro", "assignee": null, "labels": [], "created": "0012-07-30T09:44:00.000+0000", "updated": "2012-10-19T22:50:36.000+0000", "description": "Logging Throwables in logInfo and logDebug instead of swallowing them.", "comments": ["Github comment from mateiz: Thanks for catching this! Looks like a useful fix.", "Imported from Github issue spark-157, originally reported by paulcavallaro"], "derived": {"summary": "Logging Throwables in logInfo and logDebug instead of swallowing them.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Logging Throwables in Info and Debug - Logging Throwables in logInfo and logDebug instead of swallowing them."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-157, originally reported by paulcavallaro"}]}}
{"project": "SPARK", "issue_id": "SPARK-395", "title": "Merge Akka reference.conf files in sbt assembly task", "status": "Resolved", "priority": null, "reporter": "Josh Rosen", "assignee": null, "labels": [], "created": "0012-07-30T21:28:00.000+0000", "updated": "2012-10-19T22:50:27.000+0000", "description": "This fixes an issue when running Spark from the jar generated by the sbt `assembly` task.\n\nTo reproduce the issue:\n\n```java\nimport spark.SparkContext;\n\npublic class Test {\n    public static void main(String[] args) {\n        new SparkContext(\"local\", \"test\");\n        System.exit(0);\n    }\n}\n```\n\nPlace this code in a file (but not in the root of the `SPARK_HOME` directory), then compile and run it against the assembly jar:\n\n```\n$ javac -cp ../core/target/spark-core-assembly-0.6.0-SNAPSHOT.jar Test.java \n$ java -cp ../core/target/spark-core-assembly-0.6.0-SNAPSHOT.jar:. Test\nException in thread \"main\" com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'akka.version'\n\tat com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:115)\n\tat com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:135)\n\tat com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:140)\n\tat com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:108)\n\tat com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:146)\n\tat com.typesafe.config.impl.SimpleConfig.getString(SimpleConfig.java:188)\n\tat akka.actor.ActorSystem$Settings.<init>(ActorSystem.scala:116)\n\tat akka.actor.ActorSystemImpl.<init>(ActorSystem.scala:429)\n\tat akka.actor.ActorSystem$.apply(ActorSystem.scala:103)\n\tat spark.util.AkkaUtils$.createActorSystem(AkkaUtils.scala:36)\n\tat spark.SparkEnv$.createFromSystemProperties(SparkEnv.scala:58)\n\tat spark.SparkContext.<init>(SparkContext.scala:71)\n\tat spark.SparkContext.<init>(SparkContext.scala:55)\n\tat Test.main(Test.java:5)\n```\n\nThis issue is caused by the way that Akka `reference.conf` files are handled in the `assembly` task; the commit copies the solution provided by the Akka Team Blog: http://letitcrash.com/post/21025950392/howto-sbt-assembly-vs-reference-conf.", "comments": ["Github comment from mateiz: Woah, that's pretty ugly, but I guess we have to do it. I guess this reference.conf stuff was not a problem before because we weren't using Akka.", "Github comment from ijuma: No need for all this ugly code. Use a newer sbt-assembly. See http://letitcrash.com/post/21706121997/follow-up-sbt-assembly-now-likes-reference-conf", "Github comment from JoshRosen: It turns out that we're already using sbt-assembly 0.8.3.  The default `mergeStrategy` used by sbt-assembly should do the right thing for `reference.conf`, but it was not applied because commit ede615d7 added a custom `mergeStrategy` that did not extend the default.\n\nThe default `mergeStrategy` fails when trying to deduplicate `javax/servlet/SingleThreadModel.class`, which is probably why the custom `mergeStrategy` was added:\n\n```\n[info] Merging 'javax/servlet/SingleThreadModel.class' with strategy 'deduplicate'\n[error] {file:/Users/josh/Documents/programming/spark/spark/}repl/*:assembly: deduplicate: different file contents found in the following:\n[error] /Users/josh/Documents/programming/spark/spark/lib_managed/jars/javax.servlet/servlet-api/servlet-api-2.5.jar:javax/servlet/SingleThreadModel.class\n[error] /Users/josh/Documents/programming/spark/spark/lib_managed/jars/org.mortbay.jetty/servlet-api-2.5/servlet-api-2.5-6.1.14.jar:javax/servlet/SingleThreadModel.class\n[error] /Users/josh/Documents/programming/spark/spark/lib_managed/jars/org.mortbay.jetty/servlet-api/servlet-api-2.5-20081211.jar:javax/servlet/SingleThreadModel.class\n[error] {file:/Users/josh/Documents/programming/spark/spark/}core/*:assembly: deduplicate: different file contents found in the following:\n[error] /Users/josh/Documents/programming/spark/spark/lib_managed/jars/javax.servlet/servlet-api/servlet-api-2.5.jar:javax/servlet/SingleThreadModel.class\n[error] /Users/josh/Documents/programming/spark/spark/lib_managed/jars/org.mortbay.jetty/servlet-api-2.5/servlet-api-2.5-6.1.14.jar:javax/servlet/SingleThreadModel.class\n[error] /Users/josh/Documents/programming/spark/spark/lib_managed/jars/org.mortbay.jetty/servlet-api/servlet-api-2.5-20081211.jar:javax/servlet/SingleThreadModel.class\n[error] Total time: 93 s, completed Aug 1, 2012 11:04:10 PM\n```\nAdding `case \"reference.conf\" => MergeStrategy.concat` to our `mergeStrategy` solves the problem.", "Github comment from mateiz: Ah, makes sense. Do you mind sending another pull request with that then?", "Imported from Github issue spark-158, originally reported by JoshRosen"], "derived": {"summary": "This fixes an issue when running Spark from the jar generated by the sbt `assembly` task. To reproduce the issue:\n\n```java\nimport spark.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Merge Akka reference.conf files in sbt assembly task - This fixes an issue when running Spark from the jar generated by the sbt `assembly` task. To reproduce the issue:\n\n```java\nimport spark."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-158, originally reported by JoshRosen"}]}}
{"project": "SPARK", "issue_id": "SPARK-394", "title": "Spark WebUI", "status": "Resolved", "priority": null, "reporter": "Denny Britz", "assignee": null, "labels": [], "created": "0012-08-01T18:49:00.000+0000", "updated": "2012-10-19T22:50:29.000+0000", "description": "- Uses spray.twirl (https://github.com/spray/twirl)\n- CSS and JS files are included twice, once for the worker and once for the master resources. We may want to have just one shared resources folder since most people won't separate the code.\n- One problem that (I think, untested) remains is that the code uses the machine's local IP address (same as the spark server) in the URL links, even though the WebUI server is bound to 0.0.0.0/0. That means if you're running on a cluster with both a private and public ip address the private one will be used and you won't be able to follow the links from outside the cluster.\n- Jobs are always marked as completed. We need to add conditions under which a job counts as FAILED.", "comments": ["Github comment from mateiz: Thanks! Just to respond to the comments you raised:\n* It would be nice to add a shared \"static\" folder for the CSS and JS. I believe you can change the paths in Spray so that anything starting with /static is served out of that directory.\n* The IP address thing is tricky. For the Mesos scripts, we let the users set a \"public hostname\" parameter for each machine that was used in the web UI. Maybe we can do the same here. It will definitely matter when we run on EC2.\n* Figuring out what's FAILED is tricky as well due to disconnections. I think that right now, the client just disconnects. Ideally you would add an \"unregister\" message for a successful job, and add a shutdown hook on the client (Runtime.addShutdownHook) to send it. You'd also need a version of \"unregister\" that reports failure, because the Spark master can decide it's failed if a task fails too many times. And finally, for other disconnects, you would count them as failures.", "Github comment from dennybritz: - I'll look into the static folder.\n- I guess we don't need to worry about the public ip address too much. Let's instead modify the scripts to give the --ip argument to the master and workers. The user is then responsible for giving a public ip.", "Github comment from dennybritz: Files are now in a static folder.", "Github comment from mateiz: This looks good to merge except for a few formatting things (overly long lines of text).", "Github comment from dennybritz: Thanks, I made the changes you suggested.\n\nI couldn't get the non-blocking calls to work with Spray (I had actually tried that before), because of its built-in implicit Marshallers. However, I found that it actually supports implicit conversions of Futures.", "Github comment from mateiz: Thanks! Just merged it.", "Imported from Github issue spark-159, originally reported by dennybritz"], "derived": {"summary": "- Uses spray. twirl (https://github.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Spark WebUI - - Uses spray. twirl (https://github."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-159, originally reported by dennybritz"}]}}
{"project": "SPARK", "issue_id": "SPARK-393", "title": "Standalone cluster scripts", "status": "Resolved", "priority": null, "reporter": "Denny Britz", "assignee": null, "labels": [], "created": "0012-08-01T19:44:00.000+0000", "updated": "2012-10-19T22:50:37.000+0000", "description": "- Heavily influenced by Hadoop scripts. Basically works the same way, but with different argument names :) A list of slaves is expected to be in conf/slaves.\n- Added a  EC2 script, ec2-standalone, based on the mesos script, that starts a spark cluster in standalone mode. It works the same way as the mesos one (and takes the same arguments). The biggest differences are that there are no zoo keeper nodes, different security groups, and that it executes bin/start-all.sh to setup the cluster.\n- Created an AMI for the standalone mode, ami-1ff25976 (https://s3.amazonaws.com/spark-standalone-amis/latest-spark). All it has is the latest spark and scala.\n- Tested the scripts locally and on EC2.\n- We probably want to write up a tutorial on how to use the scripts, though it should be rather straightforward for people familiar with the hadoop ones.\n- One TODO is the configuration. How do we want users to specify things like master port, webUiPort, worker memory, worke cores, and so on? The simplest way would be to put these as env variables into the spark-env.sh, but that can get messy rather quickly.\n- Removed explicit netty dependency. This caused trouble because 2 netty jars were included. Akka already has a netty dependency.", "comments": ["Github comment from mateiz: Hey, just a couple of comments:\n* Regarding the configuration, I think environment variables are the best way to go for now, to be consistent with out other config stuff. Just call them things like SPARK_MASTER_PORT.\n* Would it be possible to reuse the spark-ec2 script from before, but just with a different AMI? Since a lot of the setup is handled by the script on the AMI, I believe it should be possible to reuse the client-side code. If necessary, you can add options to it. That's better than having two nearly identical files.", "Github comment from dennybritz: Yes, I could reuse the existing EC2 script. I'll just have to add another switch to it and make the security groups and ssh commands conditional.", "Github comment from dennybritz: I merged the two scripts into one. Still working on the configuration, but it's working fine with default settings right now.", "Github comment from dennybritz: Added configuration variables.", "Github comment from mateiz: Thanks. I looked at it some more and I have just two other suggestions:\n* If nontrivial parts of the code, such as the \"rotate logs\" bit, are copied from Hadoop, add Hadoop's Apache license notice to the the top of that file. If it's not copied but just based on reading the code, then there's probably no need.\n* Why didn't you make the master be configured in a separate \"masters\" file as well? I thought about this a bit and I think it's good to do it the same way as Hadoop because people will be familiar with that. It's also useful in case someone tries to stop or start the cluster from a machine other than the master (because the scripts will be everywhere).", "Github comment from dennybritz: * Added the Apache license to the two scripts that are a bit more complex and obviously taken from Hadoop.\n* The masters file in Hadoop is actually not what most people expect it to be. In Hadoop the NameNode and DataNode are only started on the machine where the script is executed. The masters file only specifies the locations where a SecondaryNameNode should be started. So, the naming of the *masters* file in Hadoop is a bit off. Since Spark doesn't have the notion of a SecondaryNameNode (which isn't a backup for the NameNode anyway) I thought a masters file was unnecessary.", "Github comment from mateiz: Oh, I see. That makes sense then. I'm going to merge this, and I guess the last thing that will be left is to document it.\n\nFor the documentation, I was thinking of switching away from the wiki to a set of HTML docs generated from markup files in our source tree, using a system such as Jekyll (https://github.com/mojombo/jekyll). I'll send an email later about that, but it can wait a bit.", "Imported from Github issue spark-160, originally reported by dennybritz"], "derived": {"summary": "- Heavily influenced by Hadoop scripts. Basically works the same way, but with different argument names :) A list of slaves is expected to be in conf/slaves.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Standalone cluster scripts - - Heavily influenced by Hadoop scripts. Basically works the same way, but with different argument names :) A list of slaves is expected to be in conf/slaves."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-160, originally reported by dennybritz"}]}}
{"project": "SPARK", "issue_id": "SPARK-392", "title": "Use sbt mergeStrategy for reference.conf files.", "status": "Resolved", "priority": null, "reporter": "Josh Rosen", "assignee": null, "labels": [], "created": "0012-08-02T09:33:00.000+0000", "updated": "2012-10-19T22:50:29.000+0000", "description": "Cleans up the code as discussed in #158.", "comments": ["Github comment from mateiz: Looks good, thanks!", "Imported from Github issue spark-161, originally reported by JoshRosen"], "derived": {"summary": "Cleans up the code as discussed in #158.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Use sbt mergeStrategy for reference.conf files. - Cleans up the code as discussed in #158."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-161, originally reported by JoshRosen"}]}}
{"project": "SPARK", "issue_id": "SPARK-391", "title": "Use maxMemory to better estimate memory available for BlockManager cache", "status": "Resolved", "priority": null, "reporter": "Shivaram Venkataraman", "assignee": null, "labels": [], "created": "0012-08-02T11:09:00.000+0000", "updated": "2012-10-19T22:50:30.000+0000", "description": null, "comments": ["Github comment from shivaram: Github seems to add any extra commits to my branch to the same pull request. Let me know if I should close this one and separate them out into two different pull requests.", "Github comment from mateiz: Yes, you can only have one pull request from each of your branches at a time. Further updates to the branch count as updates to that request. But anyway, it looks good.", "Imported from Github issue spark-162, originally reported by shivaram"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Use maxMemory to better estimate memory available for BlockManager cache"}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-162, originally reported by shivaram"}]}}
{"project": "SPARK", "issue_id": "SPARK-541", "title": "Passing bad master address to SparkContext results in unhelpful Mesos error message", "status": "Resolved", "priority": null, "reporter": "Josh Rosen", "assignee": "Patrick McFadin", "labels": [], "created": "0012-08-02T15:05:00.000+0000", "updated": "2013-01-24T16:00:07.000+0000", "description": "Passing an invalid master address to `SparkContext` results in an unhelpful error message from Mesos.  On the current (0.5.0) Spark EC2 release:\n\n```\n[root@ip-10-29-192-248 spark]# ./run spark.examples.SparkPi 127.0.0.1\n12/08/02 22:41:04 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 1370106101\n12/08/02 22:41:05 INFO spark.CacheTrackerActor: Registered actor on port 7077\n12/08/02 22:41:05 INFO spark.CacheTrackerActor: Started slave cache (size 1306.6MB) on ip-10-29-192-248.ec2.internal\n12/08/02 22:41:05 INFO spark.MapOutputTrackerActor: Registered actor on port 7077\n12/08/02 22:41:05 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-d7e47033-5684-41db-a5b8-83190eb3d983/shuffle\n12/08/02 22:41:05 INFO server.Server: jetty-7.5.3.v20111011\n12/08/02 22:41:05 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:33844 STARTING\n12/08/02 22:41:05 INFO spark.ShuffleManager: Local URI: http://10.29.192.248:33844\n12/08/02 22:41:05 INFO server.Server: jetty-7.5.3.v20111011\n12/08/02 22:41:05 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:48492 STARTING\n12/08/02 22:41:05 INFO broadcast.HttpBroadcast: Broadcast server started at http://10.29.192.248:48492\nI0802 22:41:06.010596  2834 logging.cpp:86] Logging to STDERR\njava: /root/mesos/build/../src/common/try.hpp:77: T Try<T>::get() const [with T = mesos::internal::MasterDetector*]: Assertion `state == SOME' failed.\n/root/scala-2.9.1.final/bin/scala: line 161:  2801 Aborted                 \"${JAVACMD:=java}\" $JAVA_OPTS \"${java_args[@]}\" ${CPSELECT}${TOOL_CLASSPATH} -Dscala.usejavacp=true -Dscala.home=\"$SCALA_HOME\" -Denv.emacs=\"$EMACS\" $CYGWIN_JLINE_TERMINAL scala.tools.nsc.MainGenericRunner \"$@\"\n```\n\nI see the same error with 71a958b on the dev branch.", "comments": ["Github comment from mateiz: Yeah, it's annoying, but it's hard to fix without modifying Mesos. I guess we could use a regex to check the format of the address in Spark.", "Imported from Github issue spark-163, originally reported by JoshRosen"], "derived": {"summary": "Passing an invalid master address to `SparkContext` results in an unhelpful error message from Mesos. On the current (0.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Passing bad master address to SparkContext results in unhelpful Mesos error message - Passing an invalid master address to `SparkContext` results in an unhelpful error message from Mesos. On the current (0."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-163, originally reported by JoshRosen"}]}}
{"project": "SPARK", "issue_id": "SPARK-390", "title": "Bug fix in RangePartitioner for partitioning when sorting in descending order.", "status": "Resolved", "priority": null, "reporter": "Harvey Feng", "assignee": null, "labels": [], "created": "0012-08-03T11:26:00.000+0000", "updated": "2012-10-19T22:50:33.000+0000", "description": null, "comments": ["Github comment from mateiz: Looks good. Thanks!", "Imported from Github issue spark-164, originally reported by harveyfeng"], "derived": {"summary": "", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Bug fix in RangePartitioner for partitioning when sorting in descending order."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-164, originally reported by harveyfeng"}]}}
{"project": "SPARK", "issue_id": "SPARK-389", "title": "Fix test checkpoint to reuse spark context defined in the class", "status": "Resolved", "priority": null, "reporter": "Shivaram Venkataraman", "assignee": null, "labels": [], "created": "0012-08-03T18:02:00.000+0000", "updated": "2012-10-19T22:50:19.000+0000", "description": "I sometimes see a crash in the after clause calling sc.stop and this seems to fix it", "comments": ["Github comment from mateiz: Looks good, but out of curiosity, what kind of crash is it? Is it because sc got reused from the previous test? Maybe we should set sc=null after to prevent this in the future.", "Github comment from shivaram: The stack trace I saw was:\n```\n[info] Exception encountered when attempting to run a suite with class name: spark.RDDSuite *** ABORTED ***\n[info]   java.lang.NullPointerException:\n[info]   at spark.SparkContext.stop(SparkContext.scala:303)\n[info]   at spark.RDDSuite$$anonfun$5.apply(RDDSuite.scala:14)\n[info]   at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:180)\n[info]   at spark.RDDSuite.runTest(RDDSuite.scala:8)\n[info]   at org.scalatest.FunSuite$$anonfun$runTests$1.apply(FunSuite.scala:1304)\n[info]   at org.scalatest.FunSuite$$anonfun$runTests$1.apply(FunSuite.scala:1304)\n[info]   at org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:226)\n[info]   at org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:215)\n[info]   at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59)\n[info]   at scala.collection.immutable.List.foreach(List.scala:45)\n```\n\nThe stack trace indicates that sc.stop was called but the dagScheduler was null (line 303 was dagScheduler.stop in my tree).  My guess is that somehow the spark context (sc) used by the after method was not null as sc had been created by the test \"checkpointing\".  So sc.stop was executed twice and that led to the NullPointerException.\nI am not sure why there could have been any confusion between the function local sc and the class variable sc though.", "Github comment from mateiz: I think the problem is just that it call the after() part twice, calling stop() twice on an old SparkContext that had been left over by a previous test. Each test initializes the class variable sc at the start.", "Github comment from shivaram: Do you think its a good idea to use the trait OneInstancePerTest to prevent tests from influencing each other ?", "Github comment from mateiz: It probably wouldn't hurt for most of our tests, but we should do it consistently, which will take a bit of work. It's also easy to forget to add on new tests, so from that point of view it may not be great.", "Github comment from shivaram: You were right - I added some debug information and the SparkContext from the previous test was being destructed twice. Adding sc = null in the after clause does handle those cases as well - so it might be safe to have it.", "Imported from Github issue spark-165, originally reported by shivaram"], "derived": {"summary": "I sometimes see a crash in the after clause calling sc. stop and this seems to fix it.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Fix test checkpoint to reuse spark context defined in the class - I sometimes see a crash in the after clause calling sc. stop and this seems to fix it."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-165, originally reported by shivaram"}]}}
{"project": "SPARK", "issue_id": "SPARK-388", "title": "Avoid a copy in ShuffleMapTask", "status": "Resolved", "priority": null, "reporter": "Shivaram Venkataraman", "assignee": null, "labels": [], "created": "0012-08-07T23:50:00.000+0000", "updated": "2012-10-19T22:50:24.000+0000", "description": "Avoid a copy in ShuffleMapTask by creating an iterator that will be used by the block manager.", "comments": ["Github comment from mateiz: Looks good, thanks!", "Github comment from shivaram: Thanks Josh for pointing this ! I don't think there is a performance concern as it just wraps the java map https://github.com/scala/scala/blob/2.9.x/src/library/scala/collection/JavaConversions.scala#L485 \n\nMaking a change to just use the JavaConversions call now...", "Github comment from shivaram: New pull request at https://github.com/mesos/spark/pull/168", "Imported from Github issue spark-166, originally reported by shivaram"], "derived": {"summary": "Avoid a copy in ShuffleMapTask by creating an iterator that will be used by the block manager.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Avoid a copy in ShuffleMapTask - Avoid a copy in ShuffleMapTask by creating an iterator that will be used by the block manager."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-166, originally reported by shivaram"}]}}
{"project": "SPARK", "issue_id": "SPARK-387", "title": "Detect non-zero exit status from PipedRDD process", "status": "Resolved", "priority": null, "reporter": "Josh Rosen", "assignee": null, "labels": [], "created": "0012-08-07T23:59:00.000+0000", "updated": "2012-10-19T22:50:38.000+0000", "description": "Changes PipedRDD to detect a non-zero exit status from its child process, rather than silently failing.", "comments": ["Github comment from JoshRosen: I added the exit status to the exception.", "Github comment from mateiz: Great, thanks.", "Imported from Github issue spark-167, originally reported by JoshRosen"], "derived": {"summary": "Changes PipedRDD to detect a non-zero exit status from its child process, rather than silently failing.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Detect non-zero exit status from PipedRDD process - Changes PipedRDD to detect a non-zero exit status from its child process, rather than silently failing."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-167, originally reported by JoshRosen"}]}}
{"project": "SPARK", "issue_id": "SPARK-386", "title": "Use JavaConversion to get a scala iterator", "status": "Resolved", "priority": null, "reporter": "Shivaram Venkataraman", "assignee": null, "labels": [], "created": "0012-08-08T13:13:00.000+0000", "updated": "2012-10-19T22:50:27.000+0000", "description": null, "comments": ["Github comment from mateiz: Isn't there an implicit conversion from Java iterators to Scala iterators or Seq? That would be cleaner.", "Github comment from mateiz: Great, thanks.", "Imported from Github issue spark-168, originally reported by shivaram"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Use JavaConversion to get a scala iterator"}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-168, originally reported by shivaram"}]}}
{"project": "SPARK", "issue_id": "SPARK-385", "title": "Changes to SizeEstimator more accurate", "status": "Resolved", "priority": null, "reporter": "Shivaram Venkataraman", "assignee": null, "labels": [], "created": "0012-08-11T17:07:00.000+0000", "updated": "2012-10-19T22:50:39.000+0000", "description": "Motivation:\nThis patch is motivated by an observation that the amount of heap space used by\nthe BoundedMemoryCache is often much larger than what we account for.\nFor example running with 10 files from the RITA dataset[1] and 4GB for the cache,\nwe see the currBytes variable to be 3917.67. However analyzing the memory heap\nwith Eclipse MAT[2] shows that the BoundedMemoryCache in fact occupies 4360.55 MB.\n\nChanges made:\nThis patch tries to address the discrepancy by making some changes to the\nSizeEstimator. The object size and reference size are changed based on\nthe architecture in use and if or not CompressedOops are in use by the JVM. This\nresults in the object size changing from 8 to 12 or 16 and references being\neither 4 or 8 bytes long.  We also account for the fact that arrays have an\nobject header + an int for the length. Lastly, this patch also account for the\nfact that fields and objects are aligned to 8-byte boundaries by the JVM.\nChanges are based on information from [3,4,5]\n\nTests:\nChanges are verified by comparing the results from spark.SizeEstimator.estimate\nto those from MAT. An example can be found in\nhttps://github.com/shivaram/spark/tree/size-estimate-test/res where the first\n100 lines from the 1990 dataset was used. The file spark-err-log.txt shows that\nthe size estimate was 25000 bytes which matches the size of the hashmap entry in\nBoundedMemoryCache entry found in spark-bounded-memory.txt.\n\nAlso, a simple script that can be used to run such a test with any text file can\nbe found in the `size-estimate-test` tree as `run_size_estimate_test` With this\npatch and the original dataset from the motivation, we get an estimate of\n3878.81MB while MAT reports memory usage of 3879.61MB. The difference is \nexplained below.\n\nCaveats:\nArrays are still sampled during estimation and this could lead to small\nvariations as seen with the example above. Also, the patch uses HotSpot\ndiagnostic MXBean to figure out if compressed oops are being used by the JVM and\nthis may fail on non-hotspot JVMs. Finally, the patch has been tested only on\n64-bit HotSpot JVMs (1.6.0_24 and 1.7.0_147-icedtea). I don't have access to a\n32-bit machine to test it, but we can do it on EC2.\n\n[1] http://stat-computing.org/dataexpo/2009/the-data.html\n[2] http://eclipse.org/mat\n[3] https://wikis.oracle.com/display/HotSpotInternals/CompressedOops\n[4] http://kohlerm.blogspot.com/2008/12/how-much-memory-is-used-by-my-java.html\n[5] http://lingpipe-blog.com/2010/06/22/the-unbearable-heaviness-jav-strings/", "comments": ["Github comment from mateiz: Looks great! One thing on the tests though: can you add a couple of tests with other settings of 64-bit and compressed oops? It does't need to be a lot, but just to cover those code paths.", "Github comment from shivaram: Added tests for the 32-bit and no-compressed oops cases. I had to move some of the variables to be initialized inside a method to get the tests to work correctly. \n\nAlso, this change should be applicable to the dev branch as well. Do you usually port patches from master to dev or should I use another pull request for that ?", "Github comment from mateiz: Thanks. If you have a pull request for dev that will save me some trouble, otherwise I'll cherry-pick these commits into it. (Unfortunately right now you can't merge master into dev.)", "Github comment from shivaram: I will open another pull request as I am in the process of applying these changes to dev.", "Imported from Github issue spark-169, originally reported by shivaram"], "derived": {"summary": "Motivation:\nThis patch is motivated by an observation that the amount of heap space used by\nthe BoundedMemoryCache is often much larger than what we account for. For example running with 10 files from the RITA dataset[1] and 4GB for the cache,\nwe see the currBytes variable to be 3917.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Changes to SizeEstimator more accurate - Motivation:\nThis patch is motivated by an observation that the amount of heap space used by\nthe BoundedMemoryCache is often much larger than what we account for. For example running with 10 files from the RITA dataset[1] and 4GB for the cache,\nwe see the currBytes variable to be 3917."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-169, originally reported by shivaram"}]}}
{"project": "SPARK", "issue_id": "SPARK-384", "title": "Launching Spark over YARN", "status": "Resolved", "priority": null, "reporter": "Denny Britz", "assignee": null, "labels": [], "created": "0012-08-13T12:28:00.000+0000", "updated": "2012-10-19T22:50:28.000+0000", "description": "**Usage Example:** \nSPARK_JAR=./core/target/spark-core-assembly-0.6.0-SNAPSHOT.jar ./run spark.deploy.yarn.Client **--jar** examples/target/scala-2.9.1/spark-examples_2.9.1-0.6.0-SNAPSHOT.jar **--class** spark.examples.SparkPi **--args** standalone **--num-workers** 3 **--worker-memory** 512M\n\n**Notes:**\n- Need to run sbt-assembly to generate the core-assembly jar used above.\n- YARN does not support requesting containers/resources by the number of cores. Using `Runtime.getRuntime.availableProcessors()` for each slave for now. \n- The user must use a the String \"standalone\" as the master's URL. That starts the scheduler without trying to connect to a cluster.\n- There are several TODOs. In particular 1. Checking for and handling container/app failure, 2. Letting the user set the context priority, and some code refactoring.\n- Tested locally, but not on a real cluster.", "comments": ["Github comment from mateiz: Hey Denny,\n\nLet me know when you've made the changes above and I'll merge this. It would be nice to have this in the main repo.", "Github comment from dennybritz: I made the changes. One thing that I couldn't quite figure out is why the application is marked as \"FAILED\" by YARN even though the ApplicationMaster exits with exit code 0. I am even setting the status to FinalApplicationStatus.SUCCEEDED explicitly. The error message isn't very enlightening: `Application application_1346087295431_0014 failed 1 times due to AM Container for appattempt_1346087295431_0014_000001 exited with  exitCode: 0 due to: .Failing this attempt.. Failing the application.` Any ideas?", "Github comment from mateiz: Thanks. Let's look into that later.", "Imported from Github issue spark-170, originally reported by dennybritz"], "derived": {"summary": "**Usage Example:** \nSPARK_JAR=. /core/target/spark-core-assembly-0.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Launching Spark over YARN - **Usage Example:** \nSPARK_JAR=. /core/target/spark-core-assembly-0."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-170, originally reported by dennybritz"}]}}
{"project": "SPARK", "issue_id": "SPARK-383", "title": "Size estimator changes for dev", "status": "Resolved", "priority": null, "reporter": "Shivaram Venkataraman", "assignee": null, "labels": [], "created": "0012-08-13T12:42:00.000+0000", "updated": "2012-10-19T22:50:35.000+0000", "description": "Based on pull request #169 - Additionally change BlockManagerSuite to make sure tests pass.", "comments": ["Github comment from mateiz: Thanks Shivaram!", "Github comment from rxin: BTW I think we should open source just this size estimator - others will find it useful in other projects.", "Github comment from mateiz: Yeah, it's a good idea.\n\nMatei\n\nOn Aug 14, 2012, at 8:43 AM, Reynold Xin wrote:\n\n> BTW I think we should open source just this size estimator - others will find it useful in other projects.\n> \n> \n> Reply to this email directly or view it on GitHub.\n> \n>", "Imported from Github issue spark-171, originally reported by shivaram"], "derived": {"summary": "Based on pull request #169 - Additionally change BlockManagerSuite to make sure tests pass.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Size estimator changes for dev - Based on pull request #169 - Additionally change BlockManagerSuite to make sure tests pass."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-171, originally reported by shivaram"}]}}
{"project": "SPARK", "issue_id": "SPARK-382", "title": "Rsync root directory in EC2 script", "status": "Resolved", "priority": null, "reporter": "Denny Britz", "assignee": null, "labels": [], "created": "0012-08-14T08:30:00.000+0000", "updated": "2012-10-19T22:50:37.000+0000", "description": "- Will modify the Standalone AMI to allow for root login as well.", "comments": ["Github comment from dennybritz: Modified Standalone AMI to use root instead of ec2-user.", "Github comment from mateiz: Great, thanks.", "Imported from Github issue spark-172, originally reported by dennybritz"], "derived": {"summary": "- Will modify the Standalone AMI to allow for root login as well.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Rsync root directory in EC2 script - - Will modify the Standalone AMI to allow for root login as well."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-172, originally reported by dennybritz"}]}}
{"project": "SPARK", "issue_id": "SPARK-381", "title": "make accumulator.localValue public, add tests", "status": "Resolved", "priority": null, "reporter": "squito", "assignee": null, "labels": [], "created": "0012-08-14T13:15:00.000+0000", "updated": "2012-10-19T22:50:34.000+0000", "description": null, "comments": ["Github comment from rxin: This seems to break the abstraction as the user can abuse it and make changes that are not associative. \n\nFor this usage, I think you can just implement an AccumulableParam for sets\n\nclass SetAccumulableParam[T] extends AccumulableParam[Set, T] {\n  def addAccumulator(t1: R, t2: T) : R = { t1 += t2 } \n  def addInPlace(t1: R, t2: R): R = { t1 ++= t2 }\n  def zero(initialValue: R): R = new Set\n}", "Github comment from mateiz: I think Imran had other usages, right? The main one is to read the value?\n\nI agree that the test shouldn't be using += on localValue though. That just encourages potentially error-prone use of accumulators.", "Github comment from squito: Yeah, the use of Set is not really a compelling reason for this feature, it was just the easiest to write a test case to verify the behavior.\n\nAs Matei said, one reason is to be able to read the current value of the accumulator, eg. something like SGD.\n\nI agree that the user certainly *could* abuse localValue, but I feel like the name & comment are a fair warning.\n\nI don't really understand the argument against calling += directly on localValue, though.  In fact, pretty much the only thing that calling += on most (all?) accumulators does is delegate to += on localValue.\n\nAnother reason to allow access to localValue is that you may get access to a richer api through localValue.  Eg., adding to a Set can tell you whether the element already existed or not, and adding to a Map can give you the previously stored value for the key.  That functionality isn't available via the accumulator interface.\n\nThe final reason is that if you want to call some method on localValue which takes multiple arguments, its just kind of pain (and requires needless object creation) to have to (a) define a helper \"update\" type and (b) create instances of the update type.  Eg., imagine you had:\n\nclass PartsOfSpeechCounts {\n  def addCounts(nouns:Int, verbs: Int) {...}\n...\n}\n\nto use this in an accumulator, you now have to create another object\nclass PartsOfSpeechUpdate(val nouns: Int, val verbs: Int)\n\nand then change all your updates from\n\nacc.localValue.addCounts(nouns,verbs)\nto\nacc += new PartsOfSpeechUpdate(nouns, verbs)\n\nIt isn't the end of the world, but seems like a headache with no benefit.", "Github comment from mateiz: Looks good, thanks!", "Imported from Github issue spark-173, originally reported by squito"], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "make accumulator.localValue public, add tests"}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-173, originally reported by squito"}]}}
{"project": "SPARK", "issue_id": "SPARK-380", "title": "Make spark-ec2 detect and handle VMs that fail to start correctly", "status": "Resolved", "priority": null, "reporter": "Andy Konwinski", "assignee": null, "labels": [], "created": "0012-08-16T16:25:00.000+0000", "updated": "2012-10-19T22:50:28.000+0000", "description": "See https://groups.google.com/forum/?fromgroups#!topic/spark-users/SpAHATspJTA\n\nThe script can check that the Mesos webui comes up and that the correct number of slaves come up and register with the Mesos master.", "comments": ["Github comment from shivaram: I have seen the same issue and quoting from the email, the problem is from the following lines:\n`[/sbin/fsck.ext3 (1) -- /mnt] fsck.ext3 -a /dev/xvdb \n[/sbin/fsck.ext3 (2) -- /mnt2] fsck.ext3 -a /dev/xvdc \nfsck.ext3: Device or resource busy while trying to open /dev/xvdc\nFilesystem mounted or opened exclusively by another program?\n/dev/xvdb: Adding dirhash hint to filesystem.\n`\nI think the problem is that sometimes /dev/xvdc is not available in the machine and fsck fails. A workaround is to disable fsck checking especially for the ephemeral drives as we don't have any data on them. We will need to regenerate the AMI when we make this change though.\n\nFinally the AMI assumes that we have all four ephemeral drives while setting up HDFS. This is not always true (http://docs.amazonwebservices.com/AWSEC2/latest/UserGuide/InstanceStorage.html) and we should probably use something like /proc/partitions to find this automatically.", "Github comment from shivaram: I was wrong about the last one - looking again I think spark_ec2 already sets the HDFS data dirs correctly.", "Github comment from mateiz: I think this is a problem with the latest AMI in particular. Maybe it got created at a time when it was very close to needing to fsck. Either way though, if we can disable that, that would be great.\n\nMatei\n\nOn Aug 17, 2012, at 9:14 AM, shivaram wrote:\n\n> I was wrong about the last one - looking again I think spark_ec2 already sets the HDFS data dirs correctly.\n> \n> \n> Reply to this email directly or view it on GitHub.\n> \n>", "Github comment from shivaram: I made some changes to avoid fsck as a part of creating the AMP Camp AMI. If things look good I'll make the same changes for the Spark AMI as well.", "Github comment from mateiz: I've updated the default Spark AMI with Shivaram's fix. Thanks Shivaram!", "Imported from Github issue spark-174, originally reported by andyk"], "derived": {"summary": "See https://groups. google.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Make spark-ec2 detect and handle VMs that fail to start correctly - See https://groups. google."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-174, originally reported by andyk"}]}}
{"project": "SPARK", "issue_id": "SPARK-379", "title": "add accumulators for mutable collections, with correct typing!", "status": "Resolved", "priority": null, "reporter": "squito", "assignee": null, "labels": [], "created": "0012-08-17T15:02:00.000+0000", "updated": "2012-10-19T22:50:30.000+0000", "description": "add accumulator params for all \"mutable collection\" types (anything that is growable and traversable).\n\nRather than create a bunch of implicit objects, there is a helper method which just creates the AccumulableParam needed.  This way, you also get correct typing of the accumulator.  This does require a different method, though -- I called it accumulableCollection, not really tied to that name.\n\n(this is completely independent from the other pull request on accumulable.localValue)", "comments": ["Github comment from squito: Hi Matei,\n\nany update on this?  I have some real use cases for this, so it would be nice to get it merged in.\n\nthanks!", "Github comment from mateiz: Sorry, I haven't had a ton of time to look at it, but I also haven't found a better way to do it, so we should probably just go with your approach (accumulableCollection). Can you update the pull request to be based on the latest dev branch though? Just git pull in your own repo. Right now it's not automatically mergeable.", "Github comment from mateiz: Oh actually I see your pull request is against master. Update it for the latest master code then, and I'll copy the commits into the dev branch too.", "Github comment from mateiz: Thanks for the update -- I've committed it. Just to let you know though, there was a bug that actually caused the test to fail sometimes: GrowableAccumulatorParam.zero() returned the initial collection, so if we had added some values into it before sending out the last few tasks, we ended up with multiple copies of those values. I've fixed it in commit 2498f95199bc642119ab52981d1c6508f71ff2ff.", "Imported from Github issue spark-175, originally reported by squito"], "derived": {"summary": "add accumulator params for all \"mutable collection\" types (anything that is growable and traversable). Rather than create a bunch of implicit objects, there is a helper method which just creates the AccumulableParam needed.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "add accumulators for mutable collections, with correct typing! - add accumulator params for all \"mutable collection\" types (anything that is growable and traversable). Rather than create a bunch of implicit objects, there is a helper method which just creates the AccumulableParam needed."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-175, originally reported by squito"}]}}
{"project": "SPARK", "issue_id": "SPARK-540", "title": "Add API to customize in-memory representation of RDDs", "status": "Resolved", "priority": "Minor", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0012-08-18T15:32:00.000+0000", "updated": "2016-01-18T10:21:10.000+0000", "description": "Right now the choice between serialized caching and just Java objects in dev is fine, but it might be cool to also support structures such as column-oriented storage through arrays of primitives without forcing it through the serialization interface.", "comments": ["Imported from Github issue spark-176, originally reported by mateiz", "[~matei], [~rxin]: Is this issue still valid?\n\n(I'm going through old issues that don't have an assigned component.)", "Yes it is.", "I assume this is subsumed by things like dataframes and the dataset API."], "derived": {"summary": "Right now the choice between serialized caching and just Java objects in dev is fine, but it might be cool to also support structures such as column-oriented storage through arrays of primitives without forcing it through the serialization interface.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add API to customize in-memory representation of RDDs - Right now the choice between serialized caching and just Java objects in dev is fine, but it might be cool to also support structures such as column-oriented storage through arrays of primitives without forcing it through the serialization interface."}, {"q": "What updates or decisions were made in the discussion?", "a": "I assume this is subsumed by things like dataframes and the dataset API."}]}}
{"project": "SPARK", "issue_id": "SPARK-378", "title": "Unresolved dependencies when running sbt to install spark", "status": "Resolved", "priority": null, "reporter": "Thomas Dudziak", "assignee": null, "labels": [], "created": "0012-08-21T09:03:00.000+0000", "updated": "2012-10-19T22:50:38.000+0000", "description": "When following the instructions on https://github.com/mesos/spark/wiki, I get this error:\n\n```\n[warn] \t::::::::::::::::::::::::::::::::::::::::::::::\n[warn] \t::          UNRESOLVED DEPENDENCIES         ::\n[warn] \t::::::::::::::::::::::::::::::::::::::::::::::\n[warn] \t:: com.typesafe.sbteclipse#sbteclipse-plugin;2.0.0-M2: not found\n[warn] \t::::::::::::::::::::::::::::::::::::::::::::::\n[warn] \n[warn] \tNote: Some unresolved dependencies have extra attributes.  Check that these dependencies exist with the requested attributes.\n[warn] \t\tcom.typesafe.sbteclipse:sbteclipse-plugin:2.0.0-M2 (sbtVersion=0.11.1, scalaVersion=2.9.1)\n[warn] \n[error] {file:/Users/tomdz/.sbt/plugins/}default-3ae66e/*:update: sbt.ResolveException: unresolved dependency: com.typesafe.sbteclipse#sbteclipse-plugin;2.0.0-M2: not found\nProject loading failed: (r)etry, (q)uit, (l)ast, or (i)gnore? \n```\nIgnore doesn't work, the session will not be initialized and hence compile won't be available.\n\nThe spark version is commit 680df96c433bc72713377e0f3ebb0a7cca7c11d8.\nThe scala version is `Scala code runner version 2.9.1.final -- Copyright 2002-2011, LAMP/EPFL`", "comments": ["Github comment from tomdz: Nvm, for some reason there was an entry for the eclipse plugin in `~/.sbt/plugins/plugins.sbt`.", "Imported from Github issue spark-177, originally reported by tomdz"], "derived": {"summary": "When following the instructions on https://github. com/mesos/spark/wiki, I get this error:\n\n```\n[warn] \t::::::::::::::::::::::::::::::::::::::::::::::\n[warn] \t::          UNRESOLVED DEPENDENCIES         ::\n[warn] \t::::::::::::::::::::::::::::::::::::::::::::::\n[warn] \t:: com.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Unresolved dependencies when running sbt to install spark - When following the instructions on https://github. com/mesos/spark/wiki, I get this error:\n\n```\n[warn] \t::::::::::::::::::::::::::::::::::::::::::::::\n[warn] \t::          UNRESOLVED DEPENDENCIES         ::\n[warn] \t::::::::::::::::::::::::::::::::::::::::::::::\n[warn] \t:: com."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-177, originally reported by tomdz"}]}}
{"project": "SPARK", "issue_id": "SPARK-377", "title": "Replay debugger for Spark", "status": "Resolved", "priority": null, "reporter": "Ankur Dave", "assignee": null, "labels": [], "created": "0012-08-23T17:55:00.000+0000", "updated": "2012-10-19T22:50:27.000+0000", "description": "This pull request provides replay debugging for deterministic errors in Spark programs. Execution recording is turned off by default unless the user sets spark.debugger.enable to \"true\". The user can then load the log and replay the program using spark.debugger.EventLogReader.", "comments": ["Imported from Github issue spark-178, originally reported by ankurdave"], "derived": {"summary": "This pull request provides replay debugging for deterministic errors in Spark programs. Execution recording is turned off by default unless the user sets spark.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Replay debugger for Spark - This pull request provides replay debugging for deterministic errors in Spark programs. Execution recording is turned off by default unless the user sets spark."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-178, originally reported by ankurdave"}]}}
{"project": "SPARK", "issue_id": "SPARK-376", "title": "Cache points in SparkLR example", "status": "Resolved", "priority": null, "reporter": "Josh Rosen", "assignee": null, "labels": [], "created": "0012-08-26T14:28:00.000+0000", "updated": "2012-10-19T22:50:35.000+0000", "description": "For consistency with the SparkHDFSLR example, the SparkLR example should cache the data points used to train its model.", "comments": ["Github comment from mateiz: Looks good, thanks.", "Imported from Github issue spark-179, originally reported by JoshRosen"], "derived": {"summary": "For consistency with the SparkHDFSLR example, the SparkLR example should cache the data points used to train its model.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Cache points in SparkLR example - For consistency with the SparkHDFSLR example, the SparkLR example should cache the data points used to train its model."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-179, originally reported by JoshRosen"}]}}
{"project": "SPARK", "issue_id": "SPARK-539", "title": "Permission denied(publickey)", "status": "Resolved", "priority": null, "reporter": "thikonom", "assignee": "Matei Alexandru Zaharia", "labels": [], "created": "0012-08-27T06:57:00.000+0000", "updated": "2012-12-11T10:06:30.000+0000", "description": "419   command = ((\"rsync -rv -e 'ssh -o StrictHostKeyChecking=no -i %s' \" +\n420       \"'%s/' 'root@%s:/'\") % (opts.identity_file, tmp_dir, active_master))\n\nThe script always produces an error when executing this command.\nand the error produced is:\nPermission denied (publickey)...(This error does not have to do with the permissions on the .pem file)\n\nAny ideas?", "comments": ["Github comment from mateiz: Weird, what OS is this on? And does the path to the key contain spaces by any chance?\n\nMatei\n\nOn Aug 27, 2012, at 7:57 AM, Theodoros Ikonomou wrote:\n\n> 419 command = ((\"rsync -rv -e 'ssh -o StrictHostKeyChecking=no -i %s' \" +\n> 420 \"'%s/' 'root@%s:/'\") % (opts.identity_file, tmp_dir, active_master))\n> \n> The script always produces an error when executing this command.\n> and the error produced is:\n> Permission denied (publickey)...(This error does not have to do with the permissions on the .pem file)\n> \n> Any ideas?\n> \n> \n> Reply to this email directly or view it on GitHub.\n> \n>", "Github comment from thikonom: I'm on a Mac with Snow Leopard and the path does not contain spaces.", "Github comment from mateiz: The only thing I can think of is that you might not have specified the -k parameter when launching the VM to tell Amazon to use the private key for that key pair. Can you SSH into the machine by hand using your .pem file?\n\nMatei\n\nOn Aug 27, 2012, at 8:35 AM, Theodoros Ikonomou wrote:\n\n> I'm on a Mac with Snow Leopard and the path does not contain spaces.\n> \n> \n> Reply to this email directly or view it on GitHub.\n> \n>", "Github comment from thikonom: I'm always using this command which includes the -k option:\n./spark-ec2 -i /Users/thikonom/Desktop/Big/key.pem -k key_name launch cluster_name\n\nI get the same error every time i'm trying to ssh from the command line:\nssh -i /Users/thikonom/Desktop/Big/key.pem root@...compute-1.amazonaws.com\nPermission denied (publickey).", "Imported from Github issue spark-180, originally reported by thikonom", "What fixed it for me is adding a \"chown 600\" line in spark_ec2.py:\n\nif deploy_ssh_key:\n    print \"Copying SSH key %s to master...\" % opts.identity_file\n    ssh(master, opts, 'mkdir -p ~/.ssh')\n    scp(master, opts, opts.identity_file, '~/.ssh/id_rsa')\n    ssh(master, opts, 'chown 600 ~/.ssh/id_rsa')\n  print \"Running setup on master...\"", "Thanks Alex -- I guess it's because the local file that we copied over did not have 600 permissions. Was this on Windows? On Linux/Mac I don't think you could even log into the node with a private key file that doesn't have 600 permissions set.", "I've fixed this in https://github.com/mesos/spark/commit/597520ae201513a51901c8e50f3815aff737d3d5, with Alexander's fix.", "Actually this was launched from another Linux EC2 host, but using different key file which had wrong permissions. \n\nSince for new cluster it allows to point to any private key as \"opts.identity_file\" (not the one that was already used for SSH before) we cant assume it will have correct permissions.", "Ah, makes sense. Thanks for the fix."], "derived": {"summary": "419   command = ((\"rsync -rv -e 'ssh -o StrictHostKeyChecking=no -i %s' \" +\n420       \"'%s/' 'root@%s:/'\") % (opts. identity_file, tmp_dir, active_master))\n\nThe script always produces an error when executing this command.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Permission denied(publickey) - 419   command = ((\"rsync -rv -e 'ssh -o StrictHostKeyChecking=no -i %s' \" +\n420       \"'%s/' 'root@%s:/'\") % (opts. identity_file, tmp_dir, active_master))\n\nThe script always produces an error when executing this command."}, {"q": "What updates or decisions were made in the discussion?", "a": "Ah, makes sense. Thanks for the fix."}]}}
{"project": "SPARK", "issue_id": "SPARK-375", "title": "Removed the deserialization cache for ShuffleMapTask", "status": "Resolved", "priority": null, "reporter": "Reynold Xin", "assignee": null, "labels": [], "created": "0012-08-27T21:35:00.000+0000", "updated": "2012-10-19T22:50:36.000+0000", "description": "Removed the deserialization cache for ShuffleMapTask becuase it was causing concurrency problems (some variables in Shark get set to null). The cost of task deserialization on slaves is trivial compared with the\nexecution time of the task anyway.", "comments": ["Github comment from mateiz: Thanks, looks good.", "Imported from Github issue spark-181, originally reported by rxin"], "derived": {"summary": "Removed the deserialization cache for ShuffleMapTask becuase it was causing concurrency problems (some variables in Shark get set to null). The cost of task deserialization on slaves is trivial compared with the\nexecution time of the task anyway.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Removed the deserialization cache for ShuffleMapTask - Removed the deserialization cache for ShuffleMapTask becuase it was causing concurrency problems (some variables in Shark get set to null). The cost of task deserialization on slaves is trivial compared with the\nexecution time of the task anyway."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-181, originally reported by rxin"}]}}
{"project": "SPARK", "issue_id": "SPARK-374", "title": "Add a limit on the number of parallel fetches in the reduce stage", "status": "Resolved", "priority": null, "reporter": "Harvey Feng", "assignee": null, "labels": [], "created": "0012-08-29T15:49:00.000+0000", "updated": "2012-10-19T22:50:39.000+0000", "description": "Made the numer of parallel fetches per reduce task \nSystem.getProperty(\"spark.default.parallelism\", \"8\")\nby default, but it can be set by setting the 'spark.blockManager.parallelFetches' property.", "comments": ["Github comment from mateiz: Hey Harvey,\n\nJust to understand, is this going to request the first 8 blocks, then wait until they've *all* been fetched, then request the next 8, etc? It would be nicer to start fetching the next block as soon as the previous one was obtained, so that you don't have times of idleness when waiting for the last one. The way you'd do this is by somehow creating one big queue of the block requests, and sending the next request whenever a Future completes and you've got the current one.", "Github comment from harveyfeng: Hi Matei,\n\nYeah, the parallel fetching is doing 8 requests at a time right now - I'll\nchange it.\nAlso, to confirm what you mentioned earlier, the serializing bug is\nbecause the callbacks are being executed on the same thread, so two\ndeserializeStreams are using the same ThreadLocal objectBuffer...I'll\ninclude a fix to this too.\n\nThanks!\n\nOn Wed, Aug 29, 2012 at 11:33 PM, Matei Zaharia <notifications@github.com>wrote:\n\n> Hey Harvey,\n>\n> Just to understand, is this going to request the first 8 blocks, then wait\n> until they've *all* been fetched, then request the next 8, etc? It would\n> be nicer to start fetching the next block as soon as the previous one was\n> obtained, so that you don't have times of idleness when waiting for the\n> last one. The way you'd do this is by somehow creating one big queue of the\n> block requests, and sending the next request whenever a Future completes\n> and you've got the current one.\n>\n> \n> Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/pull/182#issuecomment-8150506>.\n>\n>", "Github comment from mateiz: Hey Harvey,\n\nJust FYI, I pushed a fix to the deserializer issue, by doing all the deserialization in the caller's thread for getMultiple. It shouldn't affect your work in a major way but make sure you merge that in.", "Github comment from mateiz: Thanks Harvey! This looks good.", "Imported from Github issue spark-182, originally reported by harveyfeng"], "derived": {"summary": "Made the numer of parallel fetches per reduce task \nSystem. getProperty(\"spark.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Add a limit on the number of parallel fetches in the reduce stage - Made the numer of parallel fetches per reduce task \nSystem. getProperty(\"spark."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-182, originally reported by harveyfeng"}]}}
{"project": "SPARK", "issue_id": "SPARK-373", "title": "Disable running combiners on map tasks when mergeCombiners function is not specified by the user.", "status": "Resolved", "priority": null, "reporter": "Reynold Xin", "assignee": null, "labels": [], "created": "0012-08-29T22:06:00.000+0000", "updated": "2012-10-19T22:50:35.000+0000", "description": "I haven't done much testing yet beyond running sbt/sbt test. Want to get some early feedback.\n\nBasically if the mergeCombiners field is not set (null), combiners are not applied on map side. It doesn't break any existing API.", "comments": ["Imported from Github issue spark-183, originally reported by rxin"], "derived": {"summary": "I haven't done much testing yet beyond running sbt/sbt test. Want to get some early feedback.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Disable running combiners on map tasks when mergeCombiners function is not specified by the user. - I haven't done much testing yet beyond running sbt/sbt test. Want to get some early feedback."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-183, originally reported by rxin"}]}}
{"project": "SPARK", "issue_id": "SPARK-372", "title": "Disable running combiners on map tasks when mergeCombiners function is not specified by the user.", "status": "Resolved", "priority": null, "reporter": "Reynold Xin", "assignee": null, "labels": [], "created": "0012-08-29T22:33:00.000+0000", "updated": "2012-10-19T22:50:29.000+0000", "description": "I haven't done much testing yet beyond running sbt/sbt test. Want to get some early feedback.\n\nBasically if the mergeCombiners field is not set (null), combiners are not applied on map side. It doesn't break any existing API.", "comments": ["Github comment from rxin: Ok I added a test. Ready for review/commit.", "Github comment from mateiz: Looks good, thanks.", "Imported from Github issue spark-184, originally reported by rxin"], "derived": {"summary": "I haven't done much testing yet beyond running sbt/sbt test. Want to get some early feedback.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Disable running combiners on map tasks when mergeCombiners function is not specified by the user. - I haven't done much testing yet beyond running sbt/sbt test. Want to get some early feedback."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-184, originally reported by rxin"}]}}
{"project": "SPARK", "issue_id": "SPARK-371", "title": "run spark.examples.SparkPi 127.0.1.1:5050  spark gets error", "status": "Resolved", "priority": null, "reporter": "vince67", "assignee": null, "labels": [], "created": "0012-08-30T03:34:00.000+0000", "updated": "2012-10-19T22:50:29.000+0000", "description": "Hi there,\n              I've been trying to make spark work on Mesos and have got an error.\n              Any help would be truly appreciated.\n              When I run spark.examples.SparkPi 127.0.1.1:5050 ,sparks gives description like this:\n-------------------------\n\n12/08/30 18:33:37 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 1326463057\n12/08/30 18:33:37 INFO spark.CacheTrackerActor: Registered actor on port 7077\n12/08/30 18:33:37 INFO spark.CacheTrackerActor: Started slave cache (size 1265.0MB) on vince67-ThinkCentre-XXXX\n12/08/30 18:33:37 INFO spark.MapOutputTrackerActor: Registered actor on port 7077\n12/08/30 18:33:37 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-a3f754a1-cce3-4516-bf79-b2b423a8e6f0/shuffle\n12/08/30 18:33:38 INFO server.Server: jetty-7.5.3.v20111011\n12/08/30 18:33:38 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:53950 STARTING\n12/08/30 18:33:38 INFO spark.ShuffleManager: Local URI: http://127.0.1.1:53950\n12/08/30 18:33:38 INFO server.Server: jetty-7.5.3.v20111011\n12/08/30 18:33:38 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:39700 STARTING\n12/08/30 18:33:38 INFO broadcast.HttpBroadcast: Broadcast server started at http://127.0.1.1:39700\n12/08/30 18:33:38 INFO spark.MesosScheduler: Temp directory for JARs: /tmp/spark-1f3583ae-e06e-4e4e-97f3-7812abdf4db8\njava.lang.NullPointerException\n\tat java.io.File.<init>(File.java:222)\n\tat spark.MesosScheduler$$anonfun$createJarServer$3.apply(MesosScheduler.scala:334)\n\tat spark.MesosScheduler$$anonfun$createJarServer$3.apply(MesosScheduler.scala:333)\n\tat scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59)\n\tat scala.collection.immutable.List.foreach(List.scala:76)\n\tat spark.MesosScheduler.createJarServer(MesosScheduler.scala:333)\n\tat spark.MesosScheduler.createExecutorInfo(MesosScheduler.scala:125)\n\tat spark.MesosScheduler.<init>(MesosScheduler.scala:80)\n\tat spark.SparkContext.<init>(SparkContext.scala:78)\n\tat spark.examples.SparkPi$.main(SparkPi.scala:15)\n\tat spark.examples.SparkPi.main(SparkPi.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat scala.tools.nsc.util.ScalaClassLoader$$anonfun$run$1.apply(ScalaClassLoader.scala:78)\n\tat scala.tools.nsc.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:24)\n\tat scala.tools.nsc.util.ScalaClassLoader$URLClassLoader.asContext(ScalaClassLoader.scala:88)\n\tat scala.tools.nsc.util.ScalaClassLoader$class.run(ScalaClassLoader.scala:78)\n\tat scala.tools.nsc.util.ScalaClassLoader$URLClassLoader.run(ScalaClassLoader.scala:101)\n\tat scala.tools.nsc.ObjectRunner$.run(ObjectRunner.scala:33)\n\tat scala.tools.nsc.ObjectRunner$.runAndCatch(ObjectRunner.scala:40)\n\tat scala.tools.nsc.MainGenericRunner.runTarget$1(MainGenericRunner.scala:56)\n\tat scala.tools.nsc.MainGenericRunner.process(MainGenericRunner.scala:80)\n\tat scala.tools.nsc.MainGenericRunner$.main(MainGenericRunner.scala:89)\n\tat scala.tools.nsc.MainGenericRunner.main(MainGenericRunner.scala)\n\nmaster:\n--------------------------\nI0830 18:32:33.000991 11586 logging.cpp:72] Logging to <stderr>\nI0830 18:32:33.002667 11586 main.cpp:105] Build: 2012-08-30 18:28:46 by root\nI0830 18:32:33.002779 11586 main.cpp:106] Starting Mesos master\nI0830 18:32:33.003238 11588 master.cpp:268] Master started on 127.0.1.1:5050\nI0830 18:32:33.005064 11588 master.cpp:283] Master ID: 20120830183216842879-5050-11586\nI0830 18:32:33.005802 11588 master.cpp:483] Elected as master!\nI0830 18:32:33.046126 11590 webui_utils.cpp:45] Loading webui script at '/usr/local/mesos/share/mesos/webui/master/webui.py'\nBottle server starting up (using WSGIRefServer())...\nListening on http://0.0.0.0:8080/\nUse Ctrl-C to quit.\n\nI0830 18:32:59.907269 11587 master.cpp:844] Attempting to register slave 20120830183216842879-5050-11586-0 at slave@127.0.1.1:33321\nI0830 18:32:59.907335 11587 master.cpp:1097] Master now considering a slave at vince67-ThinkCentre-XXXX:33321 as active\nI0830 18:32:59.907357 11587 master.cpp:1633] Adding slave 20120830183216842879-5050-11586-0 at vince67-ThinkCentre-XXXX with mem=2000; cpus=4\nI0830 18:32:59.908043 11587 simple_allocator.cpp:69] Added slave 20120830183216842879-5050-11586-0 with mem=2000; cpus=4", "comments": ["Github comment from mateiz: Is this with the dev branch? You might need to do sbt/sbt package before running, in order to generate a JAR file with the example code.\n\nMatei\n\nOn Aug 30, 2012, at 4:34 AM, vince67 wrote:\n\n> Hi there,\n> I've been trying to make spark work on Mesos and have got an error.\n> Any help would be truly appreciated.\n> \n> When I run spark.examples.SparkPi 127.0.1.1:5050 ,sparks gives description like this:\n> \n> 12/08/30 18:33:37 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 1326463057\n> 12/08/30 18:33:37 INFO spark.CacheTrackerActor: Registered actor on port 7077\n> 12/08/30 18:33:37 INFO spark.CacheTrackerActor: Started slave cache (size 1265.0MB) on vince67-ThinkCentre-XXXX\n> 12/08/30 18:33:37 INFO spark.MapOutputTrackerActor: Registered actor on port 7077\n> 12/08/30 18:33:37 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-a3f754a1-cce3-4516-bf79-b2b423a8e6f0/shuffle\n> 12/08/30 18:33:38 INFO server.Server: jetty-7.5.3.v20111011\n> 12/08/30 18:33:38 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:53950 STARTING\n> 12/08/30 18:33:38 INFO spark.ShuffleManager: Local URI: http://127.0.1.1:53950\n> 12/08/30 18:33:38 INFO server.Server: jetty-7.5.3.v20111011\n> 12/08/30 18:33:38 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:39700 STARTING\n> 12/08/30 18:33:38 INFO broadcast.HttpBroadcast: Broadcast server started at http://127.0.1.1:39700\n> 12/08/30 18:33:38 INFO spark.MesosScheduler: Temp directory for JARs: /tmp/spark-1f3583ae-e06e-4e4e-97f3-7812abdf4db8\n> java.lang.NullPointerException\n> at java.io.File.(File.java:222)\n> at spark.MesosScheduler$$anonfun$createJarServer$3.apply(MesosScheduler.scala:334)\n> at spark.MesosScheduler$$anonfun$createJarServer$3.apply(MesosScheduler.scala:333)\n> at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59)\n> at scala.collection.immutable.List.foreach(List.scala:76)\n> at spark.MesosScheduler.createJarServer(MesosScheduler.scala:333)\n> at spark.MesosScheduler.createExecutorInfo(MesosScheduler.scala:125)\n> at spark.MesosScheduler.(MesosScheduler.scala:80)\n> at spark.SparkContext.(SparkContext.scala:78)\n> at spark.examples.SparkPi$.main(SparkPi.scala:15)\n> at spark.examples.SparkPi.main(SparkPi.scala)\n> at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n> at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n> at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n> at java.lang.reflect.Method.invoke(Method.java:597)\n> at scala.tools.nsc.util.ScalaClassLoader$$anonfun$run$1.apply(ScalaClassLoader.scala:78)\n> at scala.tools.nsc.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:24)\n> at scala.tools.nsc.util.ScalaClassLoader$URLClassLoader.asContext(ScalaClassLoader.scala:88)\n> at scala.tools.nsc.util.ScalaClassLoader$class.run(ScalaClassLoader.scala:78)\n> at scala.tools.nsc.util.ScalaClassLoader$URLClassLoader.run(ScalaClassLoader.scala:101)\n> at scala.tools.nsc.ObjectRunner$.run(ObjectRunner.scala:33)\n> at scala.tools.nsc.ObjectRunner$.runAndCatch(ObjectRunner.scala:40)\n> at scala.tools.nsc.MainGenericRunner.runTarget$1(MainGenericRunner.scala:56)\n> at scala.tools.nsc.MainGenericRunner.process(MainGenericRunner.scala:80)\n> at scala.tools.nsc.MainGenericRunner$.main(MainGenericRunner.scala:89)\n> \n> at scala.tools.nsc.MainGenericRunner.main(MainGenericRunner.scala)\n> \n> master:\n> \n> I0830 18:32:33.000991 11586 logging.cpp:72] Logging to \n> I0830 18:32:33.002667 11586 main.cpp:105] Build: 2012-08-30 18:28:46 by root\n> I0830 18:32:33.002779 11586 main.cpp:106] Starting Mesos master\n> I0830 18:32:33.003238 11588 master.cpp:268] Master started on 127.0.1.1:5050\n> I0830 18:32:33.005064 11588 master.cpp:283] Master ID: 20120830183216842879-5050-11586\n> I0830 18:32:33.005802 11588 master.cpp:483] Elected as master!\n> I0830 18:32:33.046126 11590 webui_utils.cpp:45] Loading webui script at '/usr/local/mesos/share/mesos/webui/master/webui.py'\n> Bottle server starting up (using WSGIRefServer())...\n> Listening on http://0.0.0.0:8080/\n> Use Ctrl-C to quit.\n> \n> I0830 18:32:59.907269 11587 master.cpp:844] Attempting to register slave 20120830183216842879-5050-11586-0 at slave@127.0.1.1:33321\n> I0830 18:32:59.907335 11587 master.cpp:1097] Master now considering a slave at vince67-ThinkCentre-XXXX:33321 as active\n> I0830 18:32:59.907357 11587 master.cpp:1633] Adding slave 20120830183216842879-5050-11586-0 at vince67-ThinkCentre-XXXX with mem=2000; cpus=4\n> \n> I0830 18:32:59.908043 11587 simple_allocator.cpp:69] Added slave 20120830183216842879-5050-11586-0 with mem=2000; cpus=4\n> \n> \n> Reply to this email directly or view it on GitHub.\n> \n>", "Github comment from mateiz: Actually you need to do sbt/sbt package even on the master branch. I'll update the docs to say that.\n\nOn Aug 30, 2012, at 4:34 AM, vince67 wrote:\n\n> Hi there,\n> I've been trying to make spark work on Mesos and have got an error.\n> Any help would be truly appreciated.\n> \n> When I run spark.examples.SparkPi 127.0.1.1:5050 ,sparks gives description like this:\n> \n> 12/08/30 18:33:37 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 1326463057\n> 12/08/30 18:33:37 INFO spark.CacheTrackerActor: Registered actor on port 7077\n> 12/08/30 18:33:37 INFO spark.CacheTrackerActor: Started slave cache (size 1265.0MB) on vince67-ThinkCentre-XXXX\n> 12/08/30 18:33:37 INFO spark.MapOutputTrackerActor: Registered actor on port 7077\n> 12/08/30 18:33:37 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-a3f754a1-cce3-4516-bf79-b2b423a8e6f0/shuffle\n> 12/08/30 18:33:38 INFO server.Server: jetty-7.5.3.v20111011\n> 12/08/30 18:33:38 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:53950 STARTING\n> 12/08/30 18:33:38 INFO spark.ShuffleManager: Local URI: http://127.0.1.1:53950\n> 12/08/30 18:33:38 INFO server.Server: jetty-7.5.3.v20111011\n> 12/08/30 18:33:38 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:39700 STARTING\n> 12/08/30 18:33:38 INFO broadcast.HttpBroadcast: Broadcast server started at http://127.0.1.1:39700\n> 12/08/30 18:33:38 INFO spark.MesosScheduler: Temp directory for JARs: /tmp/spark-1f3583ae-e06e-4e4e-97f3-7812abdf4db8\n> java.lang.NullPointerException\n> at java.io.File.(File.java:222)\n> at spark.MesosScheduler$$anonfun$createJarServer$3.apply(MesosScheduler.scala:334)\n> at spark.MesosScheduler$$anonfun$createJarServer$3.apply(MesosScheduler.scala:333)\n> at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59)\n> at scala.collection.immutable.List.foreach(List.scala:76)\n> at spark.MesosScheduler.createJarServer(MesosScheduler.scala:333)\n> at spark.MesosScheduler.createExecutorInfo(MesosScheduler.scala:125)\n> at spark.MesosScheduler.(MesosScheduler.scala:80)\n> at spark.SparkContext.(SparkContext.scala:78)\n> at spark.examples.SparkPi$.main(SparkPi.scala:15)\n> at spark.examples.SparkPi.main(SparkPi.scala)\n> at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n> at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n> at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n> at java.lang.reflect.Method.invoke(Method.java:597)\n> at scala.tools.nsc.util.ScalaClassLoader$$anonfun$run$1.apply(ScalaClassLoader.scala:78)\n> at scala.tools.nsc.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:24)\n> at scala.tools.nsc.util.ScalaClassLoader$URLClassLoader.asContext(ScalaClassLoader.scala:88)\n> at scala.tools.nsc.util.ScalaClassLoader$class.run(ScalaClassLoader.scala:78)\n> at scala.tools.nsc.util.ScalaClassLoader$URLClassLoader.run(ScalaClassLoader.scala:101)\n> at scala.tools.nsc.ObjectRunner$.run(ObjectRunner.scala:33)\n> at scala.tools.nsc.ObjectRunner$.runAndCatch(ObjectRunner.scala:40)\n> at scala.tools.nsc.MainGenericRunner.runTarget$1(MainGenericRunner.scala:56)\n> at scala.tools.nsc.MainGenericRunner.process(MainGenericRunner.scala:80)\n> at scala.tools.nsc.MainGenericRunner$.main(MainGenericRunner.scala:89)\n> \n> at scala.tools.nsc.MainGenericRunner.main(MainGenericRunner.scala)\n> \n> master:\n> \n> I0830 18:32:33.000991 11586 logging.cpp:72] Logging to \n> I0830 18:32:33.002667 11586 main.cpp:105] Build: 2012-08-30 18:28:46 by root\n> I0830 18:32:33.002779 11586 main.cpp:106] Starting Mesos master\n> I0830 18:32:33.003238 11588 master.cpp:268] Master started on 127.0.1.1:5050\n> I0830 18:32:33.005064 11588 master.cpp:283] Master ID: 20120830183216842879-5050-11586\n> I0830 18:32:33.005802 11588 master.cpp:483] Elected as master!\n> I0830 18:32:33.046126 11590 webui_utils.cpp:45] Loading webui script at '/usr/local/mesos/share/mesos/webui/master/webui.py'\n> Bottle server starting up (using WSGIRefServer())...\n> Listening on http://0.0.0.0:8080/\n> Use Ctrl-C to quit.\n> \n> I0830 18:32:59.907269 11587 master.cpp:844] Attempting to register slave 20120830183216842879-5050-11586-0 at slave@127.0.1.1:33321\n> I0830 18:32:59.907335 11587 master.cpp:1097] Master now considering a slave at vince67-ThinkCentre-XXXX:33321 as active\n> I0830 18:32:59.907357 11587 master.cpp:1633] Adding slave 20120830183216842879-5050-11586-0 at vince67-ThinkCentre-XXXX with mem=2000; cpus=4\n> \n> I0830 18:32:59.908043 11587 simple_allocator.cpp:69] Added slave 20120830183216842879-5050-11586-0 with mem=2000; cpus=4\n> \n> \n> Reply to this email directly or view it on GitHub.\n> \n>", "Github comment from vince67: In fact I've done sbt/sbt package,but it may not work perfectly. I re-download the spark comprssed package and do sbt/sbt again,Fortunaately,when I run the command ,the error has disappeared.", "Imported from Github issue spark-185, originally reported by vince67"], "derived": {"summary": "Hi there,\n              I've been trying to make spark work on Mesos and have got an error. Any help would be truly appreciated.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "run spark.examples.SparkPi 127.0.1.1:5050  spark gets error - Hi there,\n              I've been trying to make spark work on Mesos and have got an error. Any help would be truly appreciated."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-185, originally reported by vince67"}]}}
{"project": "SPARK", "issue_id": "SPARK-370", "title": "Spark HTTP FileServer", "status": "Resolved", "priority": null, "reporter": "Denny Britz", "assignee": null, "labels": [], "created": "0012-08-30T10:09:00.000+0000", "updated": "2012-10-19T22:50:37.000+0000", "description": "A general fileserver for serving both regular as well as JAR files.\n\n- I modified the existing JAR file server code to put everything into one fileserver.\n- Modified the LocalScheduler to mimic the ClusterScheduler/Executor in terms of getting files. That's useful for testing.\n- Test with standalone mode locally and local mode, but not on a mesos cluster.", "comments": ["Github comment from mateiz: I pointed out a few small formatting issues but I think there is also a problem with ShuffleMapTask's special serialization -- check that out and add a test for it. Otherwise it looks pretty good.", "Github comment from dennybritz: I made the changes you suggested and added the functionality of dynamically adding JAR files. Like you said, I had to modify the ShuffleMapTask serialization to make it work.", "Github comment from mateiz: This looks okay except for one big thing: creating a new ClassLoader for each task in the Executor is going to be very expensive, and is going to result in many copies of each task being loaded. Can you instead reuse the same ClassLoader, but give it more paths to search? One easy way would be to have a custom subclass of ClassLoader that holds an array of URLClassLoaders, and queries them to find the first one that contains a file.\n\nAs a more minor suggestion, in ShuffleMapTask, can you cache the serialized versions of the JAR and file HashMaps the same way we cache the RDDs? That serialization code becomes a bottleneck when you have a lot of tasks, and these maps will not change for subsequent runs of the task (or at least we shouldn't let a user use a JAR *before* they add it to the system). You may need to change the Executor to ignore older JARs as well in case it gets an old task. Also, in the serialization part, you may be better off doing a toArray on these and serializing that instead of serializing a HashMap; then, just do a toMap on the Executor.", "Github comment from dennybritz: Yep, that make sense. I'll make the changes.", "Github comment from dennybritz: I am closing this pull request here because I want to submit the request from another, clean, branch.", "Imported from Github issue spark-186, originally reported by dennybritz"], "derived": {"summary": "A general fileserver for serving both regular as well as JAR files. - I modified the existing JAR file server code to put everything into one fileserver.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Spark HTTP FileServer - A general fileserver for serving both regular as well as JAR files. - I modified the existing JAR file server code to put everything into one fileserver."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-186, originally reported by dennybritz"}]}}
{"project": "SPARK", "issue_id": "SPARK-369", "title": "sbin/mesos-start-cluster.sh      ulimit: error setting limit (Operation not permitted)", "status": "Resolved", "priority": null, "reporter": "vince67", "assignee": null, "labels": [], "created": "0012-08-31T04:28:00.000+0000", "updated": "2012-10-19T22:50:33.000+0000", "description": "Thanks for your attention.\nYou know, I have done everything following the steps on the web  (Runnin-Spark-on-Mesos) https://github.com/mesos/spark/wiki/Running-Spark-on-Mesos.\nBut,I have got this in the seventh step:\n\nusrname@ThinkCentre-XXXX:/usr/local/mesos$ sbin/mesos-start-cluster.sh\n-----------------------------------------------------------------------------------------\nStarting mesos-master on Host@Ip.ip.ip.ip\nssh -o StrictHostKeyChecking=no -o ConnectTimeout=2 surname@ip.ip.ip.ip  /usr/local/mesos/sbin/mesos-daemon.sh mesos-master </dev/null >/dev/null\nmasters@ip.ip.ip.ip's password: \n/usr/local/mesos/sbin/mesos-daemon.sh: 7: ulimit: error setting limit (Operation not permitted)\n--------------------------------------------------------------------------------------------------------------------------------\nStarting mesos-slave on spark@ip.ip.ip.ip\nssh -o StrictHostKeyChecking=no -o ConnectTimeout=2 spark@ip.ip.ip.ip  /usr/local/mesos/sbin/mesos-daemon.sh mesos-slave </dev/null >/dev/null\nspark@ip.ip.ip.ip's password: \n/usr/local/mesos/sbin/mesos-daemon.sh: 7: ulimit: error setting limit (Operation not permitted)\n--------------------------------------------------------------------------------------------------------------------------------\nEverything's started!\n\n\nAny words would be truly appreciated.", "comments": ["Imported from Github issue spark-187, originally reported by vince67"], "derived": {"summary": "Thanks for your attention. You know, I have done everything following the steps on the web  (Runnin-Spark-on-Mesos) https://github.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "sbin/mesos-start-cluster.sh      ulimit: error setting limit (Operation not permitted) - Thanks for your attention. You know, I have done everything following the steps on the web  (Runnin-Spark-on-Mesos) https://github."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-187, originally reported by vince67"}]}}
{"project": "SPARK", "issue_id": "SPARK-538", "title": "INFO spark.MesosScheduler: Ignoring update from TID 9 because its job is gone", "status": "Closed", "priority": null, "reporter": "vince67", "assignee": null, "labels": [], "created": "0012-09-02T01:09:00.000+0000", "updated": "2014-09-21T15:28:37.000+0000", "description": "Hi Matei,\n               Maybe I can't descibe it clearly.\n               We run masters or slaves on different machines,it is success.\n               But when we run spark.examples.SparkPi on the master , our process hangs,we have not got the result.\n               Descirption like these:\n \n               \n\n12/09/02 16:47:54 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 339585269\n12/09/02 16:47:54 INFO spark.CacheTrackerActor: Registered actor on port 7077\n12/09/02 16:47:54 INFO spark.CacheTrackerActor: Started slave cache (size 323.9MB) on vince67-ThinkCentre-XXXX\n12/09/02 16:47:54 INFO spark.MapOutputTrackerActor: Registered actor on port 7077\n12/09/02 16:47:54 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-3e79b235-1b94-44d1-823b-0369f6698688/shuffle\n12/09/02 16:47:54 INFO server.Server: jetty-7.5.3.v20111011\n12/09/02 16:47:54 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:49578 STARTING\n12/09/02 16:47:54 INFO spark.ShuffleManager: Local URI: http://ip.ip.ip.ip:49578\n12/09/02 16:47:55 INFO server.Server: jetty-7.5.3.v20111011\n12/09/02 16:47:55 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:49600 STARTING\n12/09/02 16:47:55 INFO broadcast.HttpBroadcast: Broadcast server started at http://ip.ip.ip.ip:49600\n12/09/02 16:47:55 INFO spark.MesosScheduler: Registered as framework ID 201209021640-74572372-5050-16898-0004\n12/09/02 16:47:55 INFO spark.SparkContext: Starting job...\n12/09/02 16:47:55 INFO spark.CacheTracker: Registering RDD ID 1 with cache\n12/09/02 16:47:55 INFO spark.CacheTrackerActor: Registering RDD 1 with 2 partitions\n12/09/02 16:47:55 INFO spark.CacheTracker: Registering RDD ID 0 with cache\n12/09/02 16:47:55 INFO spark.CacheTrackerActor: Registering RDD 0 with 2 partitions\n12/09/02 16:47:55 INFO spark.CacheTrackerActor: Asked for current cache locations\n12/09/02 16:47:55 INFO spark.MesosScheduler: Final stage: Stage 0\n12/09/02 16:47:55 INFO spark.MesosScheduler: Parents of final stage: List()\n12/09/02 16:47:55 INFO spark.MesosScheduler: Missing parents: List()\n12/09/02 16:47:55 INFO spark.MesosScheduler: Submitting Stage 0, which has no missing parents\n12/09/02 16:47:55 INFO spark.MesosScheduler: Got a job with 2 tasks\n12/09/02 16:47:55 INFO spark.MesosScheduler: Adding job with ID 0\n12/09/02 16:47:55 INFO spark.SimpleJob: Starting task 0:0 as TID 0 on slave 201209021640-74572372-5050-16898-2: lmrspark-G41MT-S2 (preferred)\n12/09/02 16:47:55 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 151 ms to serialize by spark.JavaSerializerInstance\n12/09/02 16:47:55 INFO spark.SimpleJob: Starting task 0:1 as TID 1 on slave 201209021640-74572372-5050-16898-2: lmrspark-G41MT-S2 (preferred)\n12/09/02 16:47:55 INFO spark.SimpleJob: Size of task 0:1 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance\n12/09/02 16:47:56 INFO spark.SimpleJob: Lost TID 0 (task 0:0)\n12/09/02 16:47:56 INFO spark.SimpleJob: Starting task 0:0 as TID 2 on slave 201209021640-74572372-5050-16898-2: lmrspark-G41MT-S2 (preferred)\n12/09/02 16:47:56 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance\n12/09/02 16:47:56 INFO spark.SimpleJob: Lost TID 1 (task 0:1)\n12/09/02 16:47:56 INFO spark.SimpleJob: Starting task 0:1 as TID 3 on slave 201209021640-74572372-5050-16898-2: lmrspark-G41MT-S2 (preferred)\n12/09/02 16:47:56 INFO spark.SimpleJob: Size of task 0:1 is 1606 bytes and took 5 ms to serialize by spark.JavaSerializerInstance\n12/09/02 16:47:57 INFO spark.SimpleJob: Lost TID 2 (task 0:0)\n12/09/02 16:47:57 INFO spark.SimpleJob: Starting task 0:0 as TID 4 on slave 201209021640-74572372-5050-16898-2: lmrspark-G41MT-S2 (preferred)\n12/09/02 16:47:57 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance\n12/09/02 16:47:57 INFO spark.SimpleJob: Lost TID 3 (task 0:1)\n12/09/02 16:47:57 INFO spark.SimpleJob: Starting task 0:1 as TID 5 on slave 201209021640-74572372-5050-16898-2: lmrspark-G41MT-S2 (preferred)\n12/09/02 16:47:57 INFO spark.SimpleJob: Size of task 0:1 is 1606 bytes and took 2 ms to serialize by spark.JavaSerializerInstance\n12/09/02 16:47:58 INFO spark.SimpleJob: Lost TID 4 (task 0:0)\n12/09/02 16:47:58 INFO spark.SimpleJob: Starting task 0:0 as TID 6 on slave 201209021640-74572372-5050-16898-2: lmrspark-G41MT-S2 (preferred)\n12/09/02 16:47:58 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance\n12/09/02 16:47:58 INFO spark.SimpleJob: Lost TID 5 (task 0:1)\n12/09/02 16:47:58 INFO spark.SimpleJob: Starting task 0:1 as TID 7 on slave 201209021640-74572372-5050-16898-2: lmrspark-G41MT-S2 (preferred)\n12/09/02 16:47:58 INFO spark.SimpleJob: Size of task 0:1 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance\n12/09/02 16:47:59 INFO spark.SimpleJob: Lost TID 6 (task 0:0)\n12/09/02 16:47:59 INFO spark.SimpleJob: Starting task 0:0 as TID 8 on slave 201209021640-74572372-5050-16898-2: lmrspark-G41MT-S2 (preferred)\n12/09/02 16:47:59 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance\n12/09/02 16:47:59 INFO spark.SimpleJob: Lost TID 7 (task 0:1)\n12/09/02 16:47:59 INFO spark.SimpleJob: Starting task 0:1 as TID 9 on slave 201209021640-74572372-5050-16898-2: lmrspark-G41MT-S2 (preferred)\n12/09/02 16:47:59 INFO spark.SimpleJob: Size of task 0:1 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance\n12/09/02 16:48:00 INFO spark.SimpleJob: Lost TID 8 (task 0:0)\n12/09/02 16:48:00 ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting job\n12/09/02 16:48:00 INFO spark.MesosScheduler: Ignoring update from TID 9 because its job is gone\n\n\n                 Your help will be appreciate.", "comments": ["Github comment from mateiz: Hi Vince,\n\nThe problem is the messages earlier, which say \"lost TID\". They mean that a task crashed, and most likely, that is because Spark or Scala was not installed on your worker nodes. On lmrspark-G41MT-S2 for example, did you download and build Spark in the same directory as on the master? It needs to be in the same place on all of them.\n\nYou can find a log of what happened by going to lmrspark-G41MT-S2 and looking in /tmp/mesos/slaveID/frameworkID/executors/0/stdout and stderr. (The frameworkID will be 201209021640-74572372-5050-16898-0004 for this job, which is printed towards the start of the log). It will likely say something like 'scala not found' or 'path ... does not exist'.", "Github comment from vince67: I try to go to the directory to find the log stderr on lmrspark-G41MT-S2,but it dose not exist.\nIn fact, when I run master and slave on the same machine, I can get the right result as well as the log stderr.", "Github comment from mateiz: Do you have any log output from mesos-slave on that machine? Maybe /tmp is not writable and that's why it can't run the tasks.", "Github comment from mateiz: By the way, you can pass a different work directory to mesos-slave with the --work-dir option. It will then run the tasks, and place the stdout/stderr files, in that directory.", "Imported from Github issue spark-188, originally reported by vince67", "this is a reasonable question for the user list, see http://spark.apache.org/community.html. i'm going to close this in favor of user list interaction. if you disagree, please re-open."], "derived": {"summary": "Hi Matei,\n               Maybe I can't descibe it clearly. We run masters or slaves on different machines,it is success.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "INFO spark.MesosScheduler: Ignoring update from TID 9 because its job is gone - Hi Matei,\n               Maybe I can't descibe it clearly. We run masters or slaves on different machines,it is success."}, {"q": "What updates or decisions were made in the discussion?", "a": "this is a reasonable question for the user list, see http://spark.apache.org/community.html. i'm going to close this in favor of user list interaction. if you disagree, please re-open."}]}}
{"project": "SPARK", "issue_id": "SPARK-368", "title": "Simulating a Spark standalone cluster locally", "status": "Resolved", "priority": null, "reporter": "Denny Britz", "assignee": null, "labels": [], "created": "0012-09-04T20:18:00.000+0000", "updated": "2012-10-19T22:50:39.000+0000", "description": "Title is pretty self-explanatory. The URL scheme is spark-cluster[N, coresPerSlave, memoryPerSlave]. There currently seems to be a problem with the Shutdown of the Executors, apparently they keep the JVM running when the user exits the Spark shell. I will fix that (let me know if you know a good way), but I submitted already because I wanted to get a quick code review.", "comments": ["Github comment from rxin: I think it is better to separate the master / slaves into multiple JVM processes so the local cluster setup better approximates a real cluster mode, which makes testing much easier.\n\n\nUpdated: never mind. I thought this was for the executor. As long as the executor is launched as a separate JVM from the master node, I am happy.", "Github comment from mateiz: I actually disagree with Reynold on that -- I think running the spark.deploy master and slaves in the same process is fine. They will still communicate over Akka and see the same issues they would in separate JVMs. I'd prefer to avoid spawning lots of JVMs so that we can use this effectively in unit tests.\n\nFor the executor JVMs, is the problem just that they stay running when you Ctrl-C? You should add a shutdown hook (Runtime.addShutdownHook) inside the spark.deploy.Worker that stops its child processes when the JVM is stopped, or even add a way to explicitly stop a Worker from outside. I thought I'd actually already added such a shutdown hook.", "Github comment from rxin: Oops - updated my comment. I didn't realize the executors would've been launched as separate JVM processes.", "Github comment from dennybritz: I looked into the executor problem. They exit fine if I quit the repl via \"ctrl+c\", but not when I type \"exit\" or \":quit\". Any ideas? Maybe that's not even a problem with the new code.", "Github comment from JoshRosen: I think that Ctrl-C is sending SIGINT to the REPL and all of its children (see http://www.vidarholen.net/contents/blog/?p=34).  If I send SIGINT directly to the REPL process using `kill -SIGINT`, then the executors keep running after the REPL exits.", "Github comment from mateiz: So shutdown hooks seem to work fine in the Scala and Spark shells:\n\n\n   scala> val r = Runtime.getRuntime\n   r: java.lang.Runtime = java.lang.Runtime@67439515\n   \n   scala> r.addShutdownHook(new Thread() { override def run() {println(\"!!!\")} })\n   \n   scala> [Ctrl-D] !!!\n\nTry printing some stuff in our shutdown hook and making sure it's called. You may also need to do a process.waitFor after killing it with kill().", "Github comment from dennybritz: Working now, see the commit above.", "Github comment from mateiz: Looks good, but instead of having the shutdown hook kill the master actor system, can you just add a shutdown hook in ExecutorRunner that does process.destroy()? I don't see that here.\n\nAlso, it would be nice if calling SparkContext.stop() when there is a local Spark cluster would shutdown that cluster. I think this would require some kind of wrapper or flag in SparkSchedulerBackend that lets you run some code when stop() is called, which in this case would stop the ActorSystems in the LocalSparkCluster, or send them a message. Basically the reason is that I'd like to use the local-cluster mode in unit tests, which will each start and stop a SparkContext.", "Github comment from mateiz: Looks good; thanks!", "Imported from Github issue spark-189, originally reported by dennybritz"], "derived": {"summary": "Title is pretty self-explanatory. The URL scheme is spark-cluster[N, coresPerSlave, memoryPerSlave].", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Simulating a Spark standalone cluster locally - Title is pretty self-explanatory. The URL scheme is spark-cluster[N, coresPerSlave, memoryPerSlave]."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-189, originally reported by dennybritz"}]}}
{"project": "SPARK", "issue_id": "SPARK-367", "title": "Log cache add/remove messages in block manager.", "status": "Resolved", "priority": null, "reporter": "Reynold Xin", "assignee": null, "labels": [], "created": "0012-09-05T15:02:00.000+0000", "updated": "2012-10-19T22:50:37.000+0000", "description": "Removed cache add/remove log messages from CacheTracker.\nAdded log messages on BlockManagerMaster to reflect block add/remove.\nAlso did some minor cleanup of storage package code.", "comments": ["Github comment from mateiz: Looks good, thanks.", "Imported from Github issue spark-190, originally reported by rxin"], "derived": {"summary": "Removed cache add/remove log messages from CacheTracker. Added log messages on BlockManagerMaster to reflect block add/remove.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Log cache add/remove messages in block manager. - Removed cache add/remove log messages from CacheTracker. Added log messages on BlockManagerMaster to reflect block add/remove."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-190, originally reported by rxin"}]}}
{"project": "SPARK", "issue_id": "SPARK-366", "title": "Broken build after typesafe ivy repo changes", "status": "Resolved", "priority": null, "reporter": "ivantopo", "assignee": null, "labels": [], "created": "0012-09-06T06:59:00.000+0000", "updated": "2012-10-19T22:50:36.000+0000", "description": "The build is broken since last night due to the following unresolved dependencies:\n\n[warn]  ::::::::::::::::::::::::::::::::::::::::::::::\n[warn]  ::          UNRESOLVED DEPENDENCIES         ::\n[warn]  ::::::::::::::::::::::::::::::::::::::::::::::\n[warn]  :: com.github.mpeltonen#sbt-idea;0.11.1: not found\n[warn]  :: com.typesafe.sbteclipse#sbteclipse;2.0: not found\n[warn]  :: com.eed3si9n#sbt-assembly;0.7.2: not found\n[warn]  ::::::::::::::::::::::::::::::::::::::::::::::\n\nI noticed that typesafe did some changes to what was available at their repo and this caused the build to fail. I suppose finding the dependencies in other repos will be enogh.", "comments": ["Github comment from mateiz: Thanks for pointing this out, but I can't reproduce the problem. I tried a build on a machine with a cleaned ~/.ivy2 and ~/.m2, from a fresh checkout of Spark. Are you sure that it's still happening? Maybe it was a transient thing.\n\n\nOn Sep 6, 2012, at 7:59 AM, ivantopo wrote:\n\n> The build is broken since last night due to the following unresolved dependencies:\n> \n> [warn] ::::::::::::::::::::::::::::::::::::::::::::::\n> [warn] :: UNRESOLVED DEPENDENCIES ::\n> [warn] ::::::::::::::::::::::::::::::::::::::::::::::\n> [warn] :: com.github.mpeltonen#sbt-idea;0.11.1: not found\n> [warn] :: com.typesafe.sbteclipse#sbteclipse;2.0: not found\n> [warn] :: com.eed3si9n#sbt-assembly;0.7.2: not found\n> [warn] ::::::::::::::::::::::::::::::::::::::::::::::\n> \n> I noticed that typesafe did some changes to what was available at their repo and this caused the build to fail. I suppose finding the dependencies in other repos will be enogh.\n> \n> \n> Reply to this email directly or view it on GitHub.\n> \n>", "Github comment from ivantopo: Hi Matei, the issue is no longer happening! I confirmed that there was a problem with the typesafe repo and also confirmed they are back to normal, thanks for your quick feedback, best regards!", "Imported from Github issue spark-191, originally reported by ivantopo"], "derived": {"summary": "The build is broken since last night due to the following unresolved dependencies:\n\n[warn]  ::::::::::::::::::::::::::::::::::::::::::::::\n[warn]  ::          UNRESOLVED DEPENDENCIES         ::\n[warn]  ::::::::::::::::::::::::::::::::::::::::::::::\n[warn]  :: com. github.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Broken build after typesafe ivy repo changes - The build is broken since last night due to the following unresolved dependencies:\n\n[warn]  ::::::::::::::::::::::::::::::::::::::::::::::\n[warn]  ::          UNRESOLVED DEPENDENCIES         ::\n[warn]  ::::::::::::::::::::::::::::::::::::::::::::::\n[warn]  :: com. github."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-191, originally reported by ivantopo"}]}}
{"project": "SPARK", "issue_id": "SPARK-537", "title": "driver.run() returned with code DRIVER_ABORTED", "status": "Resolved", "priority": null, "reporter": "yshaw", "assignee": null, "labels": [], "created": "0012-09-06T22:59:00.000+0000", "updated": "2014-09-21T15:25:42.000+0000", "description": "Hi there,\nWhen I try to run Spark on Mesos as a cluster, some error happen like this:\n\n```\n ./run spark.examples.SparkPi *.*.*.*:5050\n12/09/07 14:49:28 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 994836480\n12/09/07 14:49:28 INFO spark.CacheTrackerActor: Registered actor on port 7077\n12/09/07 14:49:28 INFO spark.CacheTrackerActor: Started slave cache (size 948.8MB) on shawpc\n12/09/07 14:49:28 INFO spark.MapOutputTrackerActor: Registered actor on port 7077\n12/09/07 14:49:28 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-81220c47-bc43-4809-ac48-5e3e8e023c8a/shuffle\n12/09/07 14:49:28 INFO server.Server: jetty-7.5.3.v20111011\n12/09/07 14:49:28 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:57595 STARTING\n12/09/07 14:49:28 INFO spark.ShuffleManager: Local URI: http://127.0.1.1:57595\n12/09/07 14:49:28 INFO server.Server: jetty-7.5.3.v20111011\n12/09/07 14:49:28 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:60113 STARTING\n12/09/07 14:49:28 INFO broadcast.HttpBroadcast: Broadcast server started at http://127.0.1.1:60113\n12/09/07 14:49:28 INFO spark.MesosScheduler: Temp directory for JARs: /tmp/spark-d541f37c-ae35-476c-b2fc-9908b0739f50\n12/09/07 14:49:28 INFO server.Server: jetty-7.5.3.v20111011\n12/09/07 14:49:28 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:50511 STARTING\n12/09/07 14:49:28 INFO spark.MesosScheduler: JAR server started at http://127.0.1.1:50511\n12/09/07 14:49:28 INFO spark.MesosScheduler: Registered as framework ID 201209071448-846324308-5050-26925-0000\n12/09/07 14:49:29 INFO spark.SparkContext: Starting job...\n12/09/07 14:49:29 INFO spark.CacheTracker: Registering RDD ID 1 with cache\n12/09/07 14:49:29 INFO spark.CacheTrackerActor: Registering RDD 1 with 2 partitions\n12/09/07 14:49:29 INFO spark.CacheTracker: Registering RDD ID 0 with cache\n12/09/07 14:49:29 INFO spark.CacheTrackerActor: Registering RDD 0 with 2 partitions\n12/09/07 14:49:29 INFO spark.CacheTrackerActor: Asked for current cache locations\n12/09/07 14:49:29 INFO spark.MesosScheduler: Final stage: Stage 0\n12/09/07 14:49:29 INFO spark.MesosScheduler: Parents of final stage: List()\n12/09/07 14:49:29 INFO spark.MesosScheduler: Missing parents: List()\n12/09/07 14:49:29 INFO spark.MesosScheduler: Submitting Stage 0, which has no missing parents\n12/09/07 14:49:29 INFO spark.MesosScheduler: Got a job with 2 tasks\n12/09/07 14:49:29 INFO spark.MesosScheduler: Adding job with ID 0\n12/09/07 14:49:29 INFO spark.SimpleJob: Starting task 0:0 as TID 0 on slave 201209071448-846324308-5050-26925-0: shawpc (preferred)\n12/09/07 14:49:29 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 52 ms to serialize by spark.JavaSerializerInstance\n12/09/07 14:49:29 INFO spark.SimpleJob: Starting task 0:1 as TID 1 on slave 201209071448-846324308-5050-26925-0: shawpc (preferred)\n12/09/07 14:49:29 INFO spark.SimpleJob: Size of task 0:1 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance\n12/09/07 14:49:30 INFO spark.SimpleJob: Lost TID 0 (task 0:0)\n12/09/07 14:49:30 INFO spark.SimpleJob: Starting task 0:0 as TID 2 on slave 201209071448-846324308-5050-26925-0: shawpc (preferred)\n12/09/07 14:49:30 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 0 ms to serialize by spark.JavaSerializerInstance\n12/09/07 14:49:30 INFO spark.SimpleJob: Lost TID 1 (task 0:1)\n12/09/07 14:49:30 INFO spark.SimpleJob: Lost TID 2 (task 0:0)\n12/09/07 14:49:30 INFO spark.SimpleJob: Starting task 0:0 as TID 3 on slave 201209071448-846324308-5050-26925-0: shawpc (preferred)\n12/09/07 14:49:30 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 2 ms to serialize by spark.JavaSerializerInstance\n12/09/07 14:49:32 INFO spark.SimpleJob: Starting task 0:1 as TID 4 on slave 201209071448-846324308-5050-26925-0: shawpc (preferred)\n12/09/07 14:49:32 INFO spark.SimpleJob: Size of task 0:1 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance\n12/09/07 14:49:32 INFO spark.SimpleJob: Lost TID 3 (task 0:0)\n12/09/07 14:49:32 INFO spark.SimpleJob: Starting task 0:0 as TID 5 on slave 201209071448-846324308-5050-26925-0: shawpc (preferred)\n12/09/07 14:49:32 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 0 ms to serialize by spark.JavaSerializerInstance\n12/09/07 14:49:32 INFO spark.SimpleJob: Lost TID 4 (task 0:1)\n12/09/07 14:49:32 INFO spark.SimpleJob: Lost TID 5 (task 0:0)\n12/09/07 14:49:32 INFO spark.SimpleJob: Starting task 0:0 as TID 6 on slave 201209071448-846324308-5050-26925-0: shawpc (preferred)\n12/09/07 14:49:32 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 0 ms to serialize by spark.JavaSerializerInstance\n12/09/07 14:49:34 INFO spark.SimpleJob: Starting task 0:1 as TID 7 on slave 201209071448-846324308-5050-26925-0: shawpc (preferred)\n12/09/07 14:49:34 INFO spark.SimpleJob: Size of task 0:1 is 1606 bytes and took 2 ms to serialize by spark.JavaSerializerInstance\n12/09/07 14:49:34 INFO spark.SimpleJob: Lost TID 6 (task 0:0)\n12/09/07 14:49:34 ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting job\nException in thread \"Thread-50\" java.io.EOFException\n\tat java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2280)\n\tat java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2749)\n\tat java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:779)\n\tat java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)\n\tat spark.JavaSerializerInstance$$anon$2.<init>(JavaSerializer.scala:39)\n\tat spark.JavaSerializerInstance.deserialize(JavaSerializer.scala:39)\n\tat spark.SimpleJob.taskLost(SimpleJob.scala:296)\n\tat spark.SimpleJob.statusUpdate(SimpleJob.scala:207)\n\tat spark.MesosScheduler.statusUpdate(MesosScheduler.scala:287)\n12/09/07 14:49:34 INFO spark.SimpleJob: Starting task 0:0 as TID 8 on slave 201209071448-846324308-5050-26925-0: shawpc (preferred)\n12/09/07 14:49:34 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance\n12/09/07 14:49:34 INFO spark.SimpleJob: Lost TID 7 (task 0:1)\n12/09/07 14:49:34 INFO spark.MesosScheduler: driver.run() returned with code DRIVER_ABORTED\n```", "comments": ["Github comment from yshaw: And the error information loged in the mesos slaves like this:\n\n```\njava.net.ConnectException: Connection refused\n\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n\tat java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:351)\n\tat java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:213)\n\tat java.net.PlainSocketImpl.connect(PlainSocketImpl.java:200)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)\n\tat java.net.Socket.connect(Socket.java:529)\n\tat java.net.Socket.connect(Socket.java:478)\n\tat sun.net.NetworkClient.doConnect(NetworkClient.java:163)\n\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:388)\n\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:523)\n\tat sun.net.www.http.HttpClient.<init>(HttpClient.java:227)\n\tat sun.net.www.http.HttpClient.New(HttpClient.java:300)\n\tat sun.net.www.http.HttpClient.New(HttpClient.java:317)\n\tat sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:970)\n\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:911)\n\tat sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:836)\n\tat sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1172)\n\tat java.net.URL.openStream(URL.java:1010)\n\tat spark.Executor.spark$Executor$$downloadFile(Executor.scala:161)\n\tat spark.Executor$$anonfun$createClassLoader$2.apply(Executor.scala:132)\n\tat spark.Executor$$anonfun$createClassLoader$2.apply(Executor.scala:129)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)\n\tat scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:38)\n\tat spark.Executor.createClassLoader(Executor.scala:129)\n\tat spark.Executor.registered(Executor.scala:42)\nException in thread \"Thread-0\" \n```\n\nAnd both in the master  and the slave, I can't open the 0.0.0.0:8080 or 0.0.0.0:8081 exactly, the error in the web page is :\n```\nError 500: Internal Server Error\n\nSorry, the requested URL http://0.0.0.0:8081/ caused an error:\n\nUnhandled exception\nException:\n\nIOError('socket error', error(111, 'Connection refused'))\nTraceback:\n\nTraceback (most recent call last):\n  File \"/home/shaw/mesos/share/mesos/webui/bottle-0.8.3/bottle.py\", line 499, in handle\n    return handler(**args)\n  File \"/home/shaw/mesos/share/mesos/webui/slave/webui.py\", line 57, in index\n    return template(\"index\", slave_port = slave_port, log_dir = log_dir)\n  File \"/home/shaw/mesos/share/mesos/webui/bottle-0.8.3/bottle.py\", line 1796, in template\n    return TEMPLATES[tpl].render(**kwargs)\n  File \"/home/shaw/mesos/share/mesos/webui/bottle-0.8.3/bottle.py\", line 1775, in render\n    self.execute(stdout, **args)\n  File \"/home/shaw/mesos/share/mesos/webui/bottle-0.8.3/bottle.py\", line 1763, in execute\n    eval(self.co, env)\n  File \"/home/shaw/mesos/share/mesos/webui/slave/index.tpl\", line 10, in <module>\n    % data = urllib.urlopen(url).read()\n  File \"/usr/lib/python2.7/urllib.py\", line 86, in urlopen\n    return opener.open(url)\n  File \"/usr/lib/python2.7/urllib.py\", line 207, in open\n    return getattr(self, name)(url)\n  File \"/usr/lib/python2.7/urllib.py\", line 344, in open_http\n    h.endheaders(data)\n  File \"/usr/lib/python2.7/httplib.py\", line 954, in endheaders\n    self._send_output(message_body)\nIOError: [Errno socket error] [Errno 111] Connection refused\n```", "Imported from Github issue spark-192, originally reported by yshaw", "this should be resolved by a number of fixes in 1.0. please re-open if it still reproduces."], "derived": {"summary": "Hi there,\nWhen I try to run Spark on Mesos as a cluster, some error happen like this:\n\n```. /run spark.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "driver.run() returned with code DRIVER_ABORTED - Hi there,\nWhen I try to run Spark on Mesos as a cluster, some error happen like this:\n\n```. /run spark."}, {"q": "What updates or decisions were made in the discussion?", "a": "this should be resolved by a number of fixes in 1.0. please re-open if it still reproduces."}]}}
{"project": "SPARK", "issue_id": "SPARK-365", "title": "Don't exit from the Examples since that stops the YARN ApplicationMaster.", "status": "Resolved", "priority": null, "reporter": "Denny Britz", "assignee": null, "labels": [], "created": "0012-09-07T08:58:00.000+0000", "updated": "2012-10-19T22:50:26.000+0000", "description": "....", "comments": ["Github comment from mateiz: Great, thanks.", "Imported from Github issue spark-193, originally reported by dennybritz"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Don't exit from the Examples since that stops the YARN ApplicationMaster. -"}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-193, originally reported by dennybritz"}]}}
{"project": "SPARK", "issue_id": "SPARK-536", "title": "Set SPARK_MEM based on instance type in EC2 scripts", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0012-09-07T11:45:00.000+0000", "updated": "2012-10-22T14:55:33.000+0000", "description": "Right now it's just 3 GB by default, which is often too small.", "comments": ["Imported from Github issue spark-194, originally reported by mateiz", "Closing this one since it is done in Spark 0.6 AMI."], "derived": {"summary": "Right now it's just 3 GB by default, which is often too small.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Set SPARK_MEM based on instance type in EC2 scripts - Right now it's just 3 GB by default, which is often too small."}, {"q": "What updates or decisions were made in the discussion?", "a": "Closing this one since it is done in Spark 0.6 AMI."}]}}
{"project": "SPARK", "issue_id": "SPARK-364", "title": "Spark HTTP FileServer", "status": "Resolved", "priority": null, "reporter": "Denny Britz", "assignee": null, "labels": [], "created": "0012-09-10T14:58:00.000+0000", "updated": "2012-10-19T22:50:38.000+0000", "description": "A general fileserver for serving both regular as well as JAR files.\n\n- I modified the existing JAR file server code to put everything into one fileserver.\n- Modified the LocalScheduler to mimic the ClusterScheduler/Executor in terms of getting files. That's useful for testing.\n- Includes test with local-cluster and local mode.\n- Modified serialization of ShuffleMapTask\n- I wasn't quite sure about the caching in ShuffleMapTask, is using the hashCode safe? There may be a better way to do this. Also, it doesn't seem like the serializedInfoCache (and fileCache) is cleared except when the SparkContext shuts down? What if it's a really long running task?", "comments": ["Github comment from dennybritz: Thinking about it, computing the hash code probably takes a long time as well, should I index the cache with the stageID?", "Github comment from mateiz: Yeah, do index it by shuffleID.", "Github comment from mateiz: Looks good; thanks!", "Imported from Github issue spark-195, originally reported by dennybritz"], "derived": {"summary": "A general fileserver for serving both regular as well as JAR files. - I modified the existing JAR file server code to put everything into one fileserver.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Spark HTTP FileServer - A general fileserver for serving both regular as well as JAR files. - I modified the existing JAR file server code to put everything into one fileserver."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-195, originally reported by dennybritz"}]}}
{"project": "SPARK", "issue_id": "SPARK-363", "title": "Log entire exception (including stack trace) in BlockManagerWorker.", "status": "Resolved", "priority": null, "reporter": "Reynold Xin", "assignee": null, "labels": [], "created": "0012-09-11T10:32:00.000+0000", "updated": "2012-10-19T22:50:36.000+0000", "description": "Previously only the error message is logged. This is not super helpful when you see an error...", "comments": ["Github comment from rxin: PTAL", "Github comment from mateiz: Great, thanks.", "Imported from Github issue spark-196, originally reported by rxin"], "derived": {"summary": "Previously only the error message is logged. This is not super helpful when you see an error.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Log entire exception (including stack trace) in BlockManagerWorker. - Previously only the error message is logged. This is not super helpful when you see an error."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-196, originally reported by rxin"}]}}
{"project": "SPARK", "issue_id": "SPARK-362", "title": "Adds a \"docs\" directory containing existing Spark documentation and doc build instructions", "status": "Resolved", "priority": null, "reporter": "Andy Konwinski", "assignee": null, "labels": [], "created": "0012-09-12T12:20:00.000+0000", "updated": "2012-10-19T22:50:21.000+0000", "description": "This change adds a directory called docs to the root Spark project directory. I used Jekyll as the framework for directory structure and compiling the docs, and used initializr.com for out-of-the-box generic look-and-feel (initializr generates pretty html wrapper leveraging Bootstrap/jquery, modernizr, ...). The Scaladoc API link in the global nav menu doesn't yet work.\n\nTo build the docs (install and) run `jekyll` and look in docs/_site or run `jekyll --server` and look at localhost:4000", "comments": ["Github comment from mateiz: Thanks Andy!", "Github comment from mateiz: By the way, is it possible to change the way this generates links so that they use relative paths? Right now if you open docs/_site/index.html in a browser, none of the links works because they have absolute paths.", "Imported from Github issue spark-197, originally reported by andyk"], "derived": {"summary": "This change adds a directory called docs to the root Spark project directory. I used Jekyll as the framework for directory structure and compiling the docs, and used initializr.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Adds a \"docs\" directory containing existing Spark documentation and doc build instructions - This change adds a directory called docs to the root Spark project directory. I used Jekyll as the framework for directory structure and compiling the docs, and used initializr."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-197, originally reported by andyk"}]}}
{"project": "SPARK", "issue_id": "SPARK-361", "title": "Fix links and make things a bit prettier.", "status": "Resolved", "priority": null, "reporter": "Andy Konwinski", "assignee": null, "labels": [], "created": "0012-09-12T18:34:00.000+0000", "updated": "2012-10-19T22:50:24.000+0000", "description": "Fixed links so that viewing html without running `jekyll --server` works better.\nAdded code syntax highlighting via pygments.\nUpdated the stylesheet a bit to make things a little easier to read.\nRemoved a duplicate doc file (EC2-Scripts.html) which existed because of capitalization weirdness (was a duplicate of ec2-scripts.html).", "comments": ["Github comment from mateiz: Thanks!", "Imported from Github issue spark-198, originally reported by andyk"], "derived": {"summary": "Fixed links so that viewing html without running `jekyll --server` works better. Added code syntax highlighting via pygments.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Fix links and make things a bit prettier. - Fixed links so that viewing html without running `jekyll --server` works better. Added code syntax highlighting via pygments."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-198, originally reported by andyk"}]}}
{"project": "SPARK", "issue_id": "SPARK-360", "title": "YARN and standalone documentation", "status": "Resolved", "priority": null, "reporter": "Denny Britz", "assignee": null, "labels": [], "created": "0012-09-13T08:48:00.000+0000", "updated": "2012-10-19T22:50:24.000+0000", "description": "YARN and standalone documentation", "comments": ["Github comment from dennybritz: Also, the navbar style was being messed up by the line-height property. I restricted the line-height to the body text.", "Github comment from mateiz: Great, thanks!", "Imported from Github issue spark-199, originally reported by dennybritz"], "derived": {"summary": "YARN and standalone documentation.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "YARN and standalone documentation - YARN and standalone documentation."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-199, originally reported by dennybritz"}]}}
{"project": "SPARK", "issue_id": "SPARK-359", "title": "Updates to docs", "status": "Resolved", "priority": null, "reporter": "Andy Konwinski", "assignee": null, "labels": [], "created": "0012-09-13T16:29:00.000+0000", "updated": "2015-06-14T00:09:58.000+0000", "description": "You can see what the generated site currently looks like here: http://www.cs.berkeley.edu/~andyk/spark-docs\n\n- Added jekyll plugin (in _plugin directory, written in Ruby) that makes sure scaladoc is built and copies over Scaladoc from Spark subprojects into docs directory before generating site. Also updated links to the scaladoc in api.md (which generates api.html) to point to these scaladoc directories.\n- Make logo look nicer (created a vector copy of it using illustrator magic and then used that to get a small version that looks a lot nicer)\n- Changed navbar to not collapse when page gets narrower (which was not super helpful for a source code documentation site) but left responsive css in so that the page looks good when the window gets smaller (turning responsive off in bootstrap unfortunately makes things somewhat ugly with narrow windows)\n- Adds back a file (ec2-scripts.md) that was mistakenly removed in previous commit.", "comments": ["Github comment from mateiz: Thanks!", "Imported from Github issue spark-200, originally reported by andyk"], "derived": {"summary": "You can see what the generated site currently looks like here: http://www. cs.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Updates to docs - You can see what the generated site currently looks like here: http://www. cs."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-200, originally reported by andyk"}]}}
{"project": "SPARK", "issue_id": "SPARK-535", "title": "Error with technique to find hostname in bin/start-slaves.sh in dev branch", "status": "Resolved", "priority": null, "reporter": "Andy Konwinski", "assignee": null, "labels": [], "created": "0012-09-16T13:11:00.000+0000", "updated": "2012-10-22T15:09:39.000+0000", "description": "I get an error when I run the new shell script to start slaves on my OS X Mountain Lion laptop (named Lucifer):\n\n    $ bin/start-slaves.sh\n    cd /Users/andyk/Development/spark/bin/.. ; /Users/andyk/Development/spark/bin/spark-daemon.sh start spark.deploy.worker.Worker spark://found::7077\n\nIt Looks like the lines getting hostname aren't working for me:\n\n~~~~~~~~~~~~\nLucifer:spark andyk$ hostname=`hostname`\nLucifer:spark andyk$ ip=`host \"$hostname\" | cut -d \" \" -f 4`\nLucifer:spark andyk$ echo $ip\nfound:\nLucifer:spark andyk$ hostname\nLucifer.local\nLucifer:spark andyk$ host Lucifer.local\nHost Lucifer.local not found: 3(NXDOMAIN)\n~~~~~~~~~~~~", "comments": ["Github comment from andyk: I poked around [1, 2], and one thing that worked for me that is probably somewhat portable but depends on having python is: <code>ip=\"\\`python -c \"import socket;print(socket.gethostbyname(socket.gethostname()))\"\\`\"</code>\n\n[1] http://stackoverflow.com/questions/2361709/efficient-way-to-get-your-ip-address-in-shell-scripts\n[2] http://stackoverflow.com/questions/166506/finding-local-ip-addresses-using-pythons-stdlib", "Github comment from mateiz: I couldn't get this to happen on my Mac, so it might have to do with the way you've asked Mac OS X to set your hostname. Since our scripts are just based on Hadoop's though, my guess is that the same thing would happen for Hadoop, so it can't be super common. Maybe one thing you can try to fix it is to add\n\n    export SPARK_MASTER_IP=<numeric IP>\n\nin your conf/spark-env.sh. Then you will force it to bind to a specific IP.", "Imported from Github issue spark-201, originally reported by andyk", "Reynold fixed this now."], "derived": {"summary": "I get an error when I run the new shell script to start slaves on my OS X Mountain Lion laptop (named Lucifer):\n\n    $ bin/start-slaves. sh\n    cd /Users/andyk/Development/spark/bin/.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Error with technique to find hostname in bin/start-slaves.sh in dev branch - I get an error when I run the new shell script to start slaves on my OS X Mountain Lion laptop (named Lucifer):\n\n    $ bin/start-slaves. sh\n    cd /Users/andyk/Development/spark/bin/."}, {"q": "What updates or decisions were made in the discussion?", "a": "Reynold fixed this now."}]}}
{"project": "SPARK", "issue_id": "SPARK-358", "title": "Updates to docs (including nav structure)", "status": "Resolved", "priority": null, "reporter": "Andy Konwinski", "assignee": null, "labels": [], "created": "0012-09-16T14:48:00.000+0000", "updated": "2012-10-19T22:50:33.000+0000", "description": "See what the compiled docs look like as of this commit at http://www.cs.berkeley.edu/~andyk/spark-docs-8f7dfcf33\n\n- Rework/expand the nav bar with more of the docs site\n- Removing parts of docs about EC2 and Mesos that differentiate between\n  running 0.5 and before\n    - Merged subheadings from running-on-amazon-ec2.html that are still relevant\n      (i.e., \"Using a newer version of Spark\" and \"Accessing Data in S3\") into\n      ec2-scripts.html and deleted running-on-amazon-ec2.html\n- Added some TODO comments to a few docs\n- Updated the blurb about AMP Camp\n- Renamed programming-guide to spark-programming-guide\n- Fixing typos/etc. in Standalone Spark doc\n- Add docs/api to .gitignore", "comments": ["Github comment from mateiz: Thanks!", "Imported from Github issue spark-202, originally reported by andyk"], "derived": {"summary": "See what the compiled docs look like as of this commit at http://www. cs.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Updates to docs (including nav structure) - See what the compiled docs look like as of this commit at http://www. cs."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-202, originally reported by andyk"}]}}
{"project": "SPARK", "issue_id": "SPARK-357", "title": "Java Programming Guide", "status": "Resolved", "priority": null, "reporter": "Josh Rosen", "assignee": null, "labels": [], "created": "0012-09-16T19:51:00.000+0000", "updated": "2012-10-19T22:50:38.000+0000", "description": "Adds a Java Programing Guide and fixes a pair of broken links to the Scala Programming Guide.", "comments": ["Github comment from mateiz: Thanks. I've merged this and we can add to it later.", "Imported from Github issue spark-203, originally reported by JoshRosen"], "derived": {"summary": "Adds a Java Programing Guide and fixes a pair of broken links to the Scala Programming Guide.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Java Programming Guide - Adds a Java Programing Guide and fixes a pair of broken links to the Scala Programming Guide."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-203, originally reported by JoshRosen"}]}}
{"project": "SPARK", "issue_id": "SPARK-356", "title": "When a file is downloaded, make it executable.", "status": "Resolved", "priority": null, "reporter": "Denny Britz", "assignee": null, "labels": [], "created": "0012-09-17T09:10:00.000+0000", "updated": "2012-10-19T22:50:26.000+0000", "description": "That's neccsary for scripts (e.g. in Shark).", "comments": ["Github comment from mateiz: Sounds like a good idea.", "Imported from Github issue spark-204, originally reported by dennybritz"], "derived": {"summary": "That's neccsary for scripts (e. g.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "When a file is downloaded, make it executable. - That's neccsary for scripts (e. g."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-204, originally reported by dennybritz"}]}}
{"project": "SPARK", "issue_id": "SPARK-534", "title": "Make SparkContext thread-safe", "status": "Resolved", "priority": "Blocker", "reporter": "tjhunter", "assignee": null, "labels": [], "created": "0012-09-17T09:49:00.000+0000", "updated": "2014-10-18T21:41:08.000+0000", "description": "SparkEnv (used by SparkContext) is not thread-safe and it causes issues with scala's Futures and parrallel collections.\nFor example, this will not work:\n\n{code}\nval f = Futures.future({\n  sc.textFile(\"hdfs://....\")\n})\nf.apply()\n{code}\n\nWorkaround for now:\n\n{code}\nval f = Futures.future({\n  SparkEnv.set(sc.env)\n  sc.textFile(\"hdfs://....\")\n})\nf.apply()\n{code}", "comments": ["Imported from Github issue spark-205, originally reported by tjhunter", "Some users have reported problems with the workaround:\n\nhttps://groups.google.com/d/topic/spark-users/iyV6gzrcajM/discussion\nhttps://groups.google.com/d/topic/spark-users/ROeSaelkhnY/discussion\n\nWe should try this ourselves and add a test case that documents the expected behavior.", "The thread safety problems stem from the fact that SparkEnv is a thread-local.  There's a discussion of this at https://groups.google.com/d/msg/spark-developers/GLx8yunSj0A/qAygH3hXyOoJ\n\nFrom Matei:\n\n{quote}SparkEnv was thread-local mostly because our RDD.compute() object didn't receive it as a parameter. If we make it passed through as part of TaskContext, we should be able to eliminate that requirement. It's something I've been meaning to do eventually, but if someone wants to step in and do it, go ahead.{quote}", "We will fix this as some of our use cases will require it.", "Specific use case for threadsafe Context. Executing jobs from Tomcat or other web container based on user requests.", "I'm going to resolve this as \"invalid\" since it's a really old issue and its title / description don't really match up.  The issue reported here isn't really thread-safety, it's that older versions of Spark required you to explicitly set SparkEnv before accessing SparkContext from a separate thread.\n\nI'm also going to unlink this as a blocker to SPARK-2243, since that issue is for supporting multiple active SparkContexts in the same JVM, whereas this is about concurrent use of the _same_ SparkContext.    As far as I know, we've always supported multiple threads submitting jobs to the same shared SparkContext.  The specific issue here of having to manually set SparkEnv has been fixed for a while (since at least Spark 1.0, I think)."], "derived": {"summary": "SparkEnv (used by SparkContext) is not thread-safe and it causes issues with scala's Futures and parrallel collections. For example, this will not work:\n\n{code}\nval f = Futures.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Make SparkContext thread-safe - SparkEnv (used by SparkContext) is not thread-safe and it causes issues with scala's Futures and parrallel collections. For example, this will not work:\n\n{code}\nval f = Futures."}, {"q": "What updates or decisions were made in the discussion?", "a": "I'm going to resolve this as \"invalid\" since it's a really old issue and its title / description don't really match up.  The issue reported here isn't really thread-safety, it's that older versions of Spark required you to explicitly set SparkEnv before accessing SparkContext from a separate thread.\n\nI'm also going to unlink this as a blocker to SPARK-2243, since that issue is for supporting multiple active SparkContexts in the same JVM, whereas this is about concurrent use of the _same_ SparkContext.    As far as I know, we've always supported multiple threads submitting jobs to the same shared SparkContext.  The specific issue here of having to manually set SparkEnv has been fixed for a while (since at least Spark 1.0, I think)."}]}}
{"project": "SPARK", "issue_id": "SPARK-355", "title": "SampledRDD produces incorrect samples when sampling with replacement", "status": "Resolved", "priority": null, "reporter": "henryem", "assignee": null, "labels": [], "created": "0012-09-17T21:34:00.000+0000", "updated": "2012-10-19T22:50:35.000+0000", "description": "The algorithm for sampling with replacement in SampledRDD is broken.  Each split is always guaranteed to get the same number of items in the sample output, so effectively it is performing a sample that is stratified by split.  A simple reproducing example:\n\n// This part isn't important - it's just a way to get an RDD with 2 splits, ((1, 1)) and ((2, 1)).\nval rdd = new spark.PairRDDFunctions(sc.parallelize(Seq((1, 1), (2, 1))))\nclass MyPartitioner extends spark.Partitioner {\n  override def numPartitions = 2\n  override def getPartition(key: Any) = key.asInstanceOf[Int] % 2\n}\nval twoSplits = rdd.reduceByKey(new MyPartitioner(), (x: Int, y: Int) => x)\n// Try this as many times as you like - you'll always get ((1, 1), (2, 1)) or ((2, 1), (1, 1)).\ntwoSplits.sample(true, 1, new java.util.Random().nextInt).collect\n\nA simple fix is to draw the number of samples for each split from a binomial(splitSize, desiredSampleFraction) distribution.  I'd like to fix this along with some other improvements to sampling.", "comments": ["Github comment from mateiz: This has been fixed now, though we still need an exact sampling algorithm too.", "Imported from Github issue spark-206, originally reported by henryem"], "derived": {"summary": "The algorithm for sampling with replacement in SampledRDD is broken. Each split is always guaranteed to get the same number of items in the sample output, so effectively it is performing a sample that is stratified by split.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "SampledRDD produces incorrect samples when sampling with replacement - The algorithm for sampling with replacement in SampledRDD is broken. Each split is always guaranteed to get the same number of items in the sample output, so effectively it is performing a sample that is stratified by split."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-206, originally reported by henryem"}]}}
{"project": "SPARK", "issue_id": "SPARK-354", "title": "RDD.takeSample() produces samples biased toward earlier splits", "status": "Resolved", "priority": null, "reporter": "henryem", "assignee": null, "labels": [], "created": "0012-09-17T21:43:00.000+0000", "updated": "2012-10-19T22:50:23.000+0000", "description": "Since SampledRDD doesn't support samples of fixed sizes, RDD.takeSample(k) unions together samples until the union's size exceeds size k, then truncates the union to the first k.  This truncation step biases the sample toward splits with lower indices.\n\nOne fix would be to choose k values at random during the truncation step.  Really, this should be fixed by implementing a good fixed-size sampling algorithm, which I plan to do soon.", "comments": ["Github comment from mateiz: This has been fixed now, though we still need an exact sampling algorithm too.", "Imported from Github issue spark-207, originally reported by henryem"], "derived": {"summary": "Since SampledRDD doesn't support samples of fixed sizes, RDD. takeSample(k) unions together samples until the union's size exceeds size k, then truncates the union to the first k.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "RDD.takeSample() produces samples biased toward earlier splits - Since SampledRDD doesn't support samples of fixed sizes, RDD. takeSample(k) unions together samples until the union's size exceeds size k, then truncates the union to the first k."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-207, originally reported by henryem"}]}}
{"project": "SPARK", "issue_id": "SPARK-353", "title": "Separated ShuffledRDD into multiple classes.", "status": "Resolved", "priority": null, "reporter": "Reynold Xin", "assignee": null, "labels": [], "created": "0012-09-19T11:38:00.000+0000", "updated": "2012-10-19T22:50:33.000+0000", "description": "RepartitionShuffledRDD, ShuffledSortedRDD, and ShuffledAggregatedRDD.\n\nMostly for performance reasons. In the future, we can also add Sort based aggregation to the code.", "comments": ["Github comment from mateiz: Hey Reynold,\n\nThis looks fine, but just to double-check, do we need new tests to verify all the new RDD subclasses? Or are they covered by the existing operations? \n\nMatei", "Github comment from rxin: They are mostly covered by existing tests if you assume the sort test pretty much covers RepartitionShuffledRDD.\n\nBasically there are 3 RDDs now:\n\nRepartitionShuffledRDD: Used by ShuffledSortedRDD\n\nShuffledSortedRDD: tested by sortByKey\n\nShuffledAggregatedRDD: tested by anything combineByKey as before.", "Github comment from mateiz: Alright, I've merged it. Let's see if we run into any issues.", "Imported from Github issue spark-208, originally reported by rxin"], "derived": {"summary": "RepartitionShuffledRDD, ShuffledSortedRDD, and ShuffledAggregatedRDD. Mostly for performance reasons.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Separated ShuffledRDD into multiple classes. - RepartitionShuffledRDD, ShuffledSortedRDD, and ShuffledAggregatedRDD. Mostly for performance reasons."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-208, originally reported by rxin"}]}}
{"project": "SPARK", "issue_id": "SPARK-533", "title": "Killing tasks in spark - request for comment", "status": "Resolved", "priority": null, "reporter": "Shivaram Venkataraman", "assignee": "Reynold Xin", "labels": [], "created": "0012-09-19T14:18:00.000+0000", "updated": "2013-11-17T14:45:09.000+0000", "description": "This patch is the first step towards introducing support for killing tasks in Spark.  The higher-level goal is to support killing tasks that are speculatively executed or unnecessary (e.g. limit queries in Shark).\n\nThe plan is to support task killing by interrupting threads that are executing the tasks to either generate an InterruptedException (if the task is waiting on a lock) or set the interrupted status for the thread.  As tasks in Spark can be CPU-bound while working on in-memory  data, we need to periodically check the interrupt bit from the thread.  \n\nThis is simple for ShuffleMap tasks where the user-defined function is invoked once per element. However as ResultTasks currently provide an RDD iterator to the user-defined-function, we have to change the interface to allow such tasks to be interrupted.\n\nThis patch shows an example of a new interface `OutputFunction` that makes ResultTasks use functions that can be called for each-element  of the RDD. \n\nNote: This is not a complete patch and only shows how OutputFunction will look for `collect` and `fold`. It would be great to get comments on the general approach and on the specific semantics used to create OutputFunctions.\n\nThis work was done jointly with @apanda", "comments": ["Github comment from rxin: So Shiv and I discussed this in person today, and we came up with a better proposal.\n\nWe can define a special RDD that handles the interrupted checking. Let's call it ThreadStatusCheckRDD:\n\n```scala\nclass ThreadStatusCheckRDD(parent: RDD) extends RDD {\n\n  def compute: Iterator = new Iterator {\n    val threadStopped = false\n    def hasNext = !threadStopped && parent.hasNext\n    def next = {\n      if (Thread.interrupted) {\n        threadStopped = true\n      }\n      parent.next()\n    }\n  }\n}\n```\n\nAnd then we can inject instances of this RDD in the lineage. We need to be smart about where to inject these RDDs, one example is to inject before a blocking operator (e.g. sort, or assume for any mapPartitions), and at the end of a chain.\n\nWe did a simple benchmark testing the cost of Thread.interrupted, and it is negligible.", "Imported from Github issue spark-209, originally reported by shivaram", "Submitted a pull request at : https://github.com/mesos/spark/pull/935"], "derived": {"summary": "This patch is the first step towards introducing support for killing tasks in Spark. The higher-level goal is to support killing tasks that are speculatively executed or unnecessary (e.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Killing tasks in spark - request for comment - This patch is the first step towards introducing support for killing tasks in Spark. The higher-level goal is to support killing tasks that are speculatively executed or unnecessary (e."}, {"q": "What updates or decisions were made in the discussion?", "a": "Submitted a pull request at : https://github.com/mesos/spark/pull/935"}]}}
{"project": "SPARK", "issue_id": "SPARK-352", "title": "Set a limited number of retry in standalone deploy mode.", "status": "Resolved", "priority": null, "reporter": "Reynold Xin", "assignee": null, "labels": [], "created": "0012-09-19T14:44:00.000+0000", "updated": "2012-10-19T22:50:34.000+0000", "description": "It took Denny and me 2 hrs today to figure out I had the wrong scala home set and the test (which uses the standalone cluster) goes into infinite retry and everything just hangs.\n\nThis commit limits the number of retries allowed.", "comments": ["Github comment from mateiz: Good idea! Just ran into this with someone else actually.", "Imported from Github issue spark-210, originally reported by rxin"], "derived": {"summary": "It took Denny and me 2 hrs today to figure out I had the wrong scala home set and the test (which uses the standalone cluster) goes into infinite retry and everything just hangs. This commit limits the number of retries allowed.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Set a limited number of retry in standalone deploy mode. - It took Denny and me 2 hrs today to figure out I had the wrong scala home set and the test (which uses the standalone cluster) goes into infinite retry and everything just hangs. This commit limits the number of retries allowed."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-210, originally reported by rxin"}]}}
{"project": "SPARK", "issue_id": "SPARK-351", "title": "run", "status": "Resolved", "priority": null, "reporter": "vince67", "assignee": null, "labels": [], "created": "0012-09-20T01:07:00.000+0000", "updated": "2012-10-19T22:50:34.000+0000", "description": null, "comments": ["Imported from Github issue spark-211, originally reported by vince67"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "run"}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-211, originally reported by vince67"}]}}
{"project": "SPARK", "issue_id": "SPARK-350", "title": "Log where in the user's code each RDD got created", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0012-09-20T16:08:00.000+0000", "updated": "2012-10-22T14:55:34.000+0000", "description": "Possible by just getting a stack trace of the current thread.", "comments": ["Github comment from ankurdave: Here's the solution currently in the debugger: https://github.com/ankurdave/spark/blob/dev/core/src/main/scala/spark/RDD.scala#L78", "Github comment from mateiz: This is now fixed! Thanks to pull #225.", "Imported from Github issue spark-213, originally reported by mateiz"], "derived": {"summary": "Possible by just getting a stack trace of the current thread.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Log where in the user's code each RDD got created - Possible by just getting a stack trace of the current thread."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-213, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-532", "title": "Log something when no resources have been offered in the cluster", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": "Patrick McFadin", "labels": [], "created": "0012-09-20T16:08:00.000+0000", "updated": "2013-03-05T16:11:24.000+0000", "description": "This is a behavior that's commonly confusing -- your job seems to hang because it didn't get offered any worker nodes, or it got them offered but they had too little RAM.", "comments": ["Imported from Github issue spark-212, originally reported by mateiz"], "derived": {"summary": "This is a behavior that's commonly confusing -- your job seems to hang because it didn't get offered any worker nodes, or it got them offered but they had too little RAM.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Log something when no resources have been offered in the cluster - This is a behavior that's commonly confusing -- your job seems to hang because it didn't get offered any worker nodes, or it got them offered but they had too little RAM."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-212, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-349", "title": "The --ebs-vol-size option doesn't work with the current EC2 AMI", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0012-09-20T16:32:00.000+0000", "updated": "2012-10-22T14:55:30.000+0000", "description": "Seems to be because it already has a /vol directory on the image.", "comments": ["Github comment from tdas: To add more information, the only the Spark AMI in us-east-1 has this problem. The us-west-1 AMI (ami-17f4d052) does not have this problem.", "Github comment from mateiz: This is now fixed.", "Imported from Github issue spark-214, originally reported by mateiz"], "derived": {"summary": "Seems to be because it already has a /vol directory on the image.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "The --ebs-vol-size option doesn't work with the current EC2 AMI - Seems to be because it already has a /vol directory on the image."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-214, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-348", "title": "HTTP File server fixes", "status": "Resolved", "priority": null, "reporter": "Denny Britz", "assignee": null, "labels": [], "created": "0012-09-21T10:02:00.000+0000", "updated": "2012-10-19T22:50:33.000+0000", "description": "The server returned the local file path instead of the URL, thus it was working when testing locally, but not on a cluster.", "comments": ["Github comment from mateiz: Thanks for the fix.", "Imported from Github issue spark-215, originally reported by dennybritz"], "derived": {"summary": "The server returned the local file path instead of the URL, thus it was working when testing locally, but not on a cluster.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "HTTP File server fixes - The server returned the local file path instead of the URL, thus it was working when testing locally, but not on a cluster."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-215, originally reported by dennybritz"}]}}
{"project": "SPARK", "issue_id": "SPARK-347", "title": "Logs from \"run\" disappear once you run the tests because test-classes are now on classpath", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0012-09-22T21:51:00.000+0000", "updated": "2012-10-22T14:55:32.000+0000", "description": "This is a slightly awkward side-effect of trying to run the distributed tests -- it places the test version of log4j.properties on the classpath.", "comments": ["Github comment from mateiz: On a related note, we should make the tests redirect all log4j output to a file, instead of leaving only the errors on stdout. People get confused by the errors and think tests failed.", "Imported from Github issue spark-216, originally reported by mateiz"], "derived": {"summary": "This is a slightly awkward side-effect of trying to run the distributed tests -- it places the test version of log4j. properties on the classpath.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Logs from \"run\" disappear once you run the tests because test-classes are now on classpath - This is a slightly awkward side-effect of trying to run the distributed tests -- it places the test version of log4j. properties on the classpath."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-216, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-346", "title": "Added a method to RDD to expose the ClassManifest.", "status": "Resolved", "priority": null, "reporter": "Reynold Xin", "assignee": null, "labels": [], "created": "0012-09-24T15:57:00.000+0000", "updated": "2012-10-19T22:50:25.000+0000", "description": null, "comments": ["Github comment from mateiz: Minor thing.. rename it to something like elementManifest to make it more obscure. It would've also been nice to make it private[spark] but I guess your Shark code is not in the Spark package, right?", "Github comment from rxin: Renamed it to elementClassManifest. PTAL.", "Github comment from mateiz: OK, thanks.", "Imported from Github issue spark-217, originally reported by rxin"], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Added a method to RDD to expose the ClassManifest."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-217, originally reported by rxin"}]}}
{"project": "SPARK", "issue_id": "SPARK-345", "title": "Scripts to start Spark under windows", "status": "Resolved", "priority": null, "reporter": "rnpandya", "assignee": null, "labels": [], "created": "0012-09-24T19:36:00.000+0000", "updated": "2012-10-19T22:50:24.000+0000", "description": "Windows Command Shell versions of run and sbt scripts", "comments": ["Github comment from mateiz: Awesome, thanks Ravi!", "Imported from Github issue spark-218, originally reported by rnpandya"], "derived": {"summary": "Windows Command Shell versions of run and sbt scripts.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Scripts to start Spark under windows - Windows Command Shell versions of run and sbt scripts."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-218, originally reported by rnpandya"}]}}
{"project": "SPARK", "issue_id": "SPARK-344", "title": "Add spark-shell.cmd", "status": "Resolved", "priority": null, "reporter": "rnpandya", "assignee": null, "labels": [], "created": "0012-09-25T06:32:00.000+0000", "updated": "2012-10-19T22:50:34.000+0000", "description": null, "comments": ["Github comment from mateiz: Thanks Ravi!", "Imported from Github issue spark-219, originally reported by rnpandya"], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add spark-shell.cmd"}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-219, originally reported by rnpandya"}]}}
{"project": "SPARK", "issue_id": "SPARK-343", "title": "One commit that makes nav dropdowns show on hover", "status": "Resolved", "priority": null, "reporter": "Andy Konwinski", "assignee": null, "labels": [], "created": "0012-09-25T14:45:00.000+0000", "updated": "2012-10-19T22:50:39.000+0000", "description": "One commit that makes nav dropdowns show on hover.", "comments": ["Imported from Github issue spark-220, originally reported by andyk"], "derived": {"summary": "One commit that makes nav dropdowns show on hover.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "One commit that makes nav dropdowns show on hover - One commit that makes nav dropdowns show on hover."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-220, originally reported by andyk"}]}}
{"project": "SPARK", "issue_id": "SPARK-342", "title": "Web UI should display memory in a nicer format", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0012-09-25T22:54:00.000+0000", "updated": "2012-10-22T14:55:35.000+0000", "description": "Can use Utils.memoryBytesToString for this.", "comments": ["Github comment from mateiz: Fixed now.", "Imported from Github issue spark-221, originally reported by mateiz"], "derived": {"summary": "Can use Utils. memoryBytesToString for this.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Web UI should display memory in a nicer format - Can use Utils. memoryBytesToString for this."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-221, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-341", "title": "Added MapPartitionsWithSplitRDD.", "status": "Closed", "priority": null, "reporter": "Reynold Xin", "assignee": "Reynold Xin", "labels": [], "created": "0012-09-26T16:12:00.000+0000", "updated": "2019-05-28T05:28:21.000+0000", "description": null, "comments": ["Github comment from mateiz: Awesome, thanks! One small thing I might change after is to rename this to just mapPartitions, since the compiler should be able to infer the type of function passed based on the number of arguments it expects.", "Github comment from rxin: Talked to Matei offline - it is best to keep the names separate because if they are the same name, scala compiler can be confused by situations like\n\nrdd.mapPartitions(myfunction)\n\nWhen the arguments are not specified to myfunction, it cannot reliably infer the type (whether it is a 2 arg closure or a 1 arg closure).", "Imported from Github issue spark-222, originally reported by rxin"], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Added MapPartitionsWithSplitRDD."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-222, originally reported by rxin"}]}}
{"project": "SPARK", "issue_id": "SPARK-340", "title": "Rename StorageLevels to something easier to remember", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0012-09-26T22:32:00.000+0000", "updated": "2012-10-22T14:55:36.000+0000", "description": "Something like this:\n\nMEMORY_ONLY -> deserialized, in-memory\nMEMORY_ONLY_SER -> serialized, in-memory\nMEMORY_AND_DISK -> deserialized, spills to disk\nMEMORY_AND_DISK_SER -> serialized, spills to disk", "comments": ["Github comment from jakajancar: To me, e.g. `.persist(disk=true, serialize=true)` would be even easier to remember (I don't see much point in named StorageLevels with different setting permutations).", "Imported from Github issue spark-223, originally reported by mateiz"], "derived": {"summary": "Something like this:\n\nMEMORY_ONLY -> deserialized, in-memory\nMEMORY_ONLY_SER -> serialized, in-memory\nMEMORY_AND_DISK -> deserialized, spills to disk\nMEMORY_AND_DISK_SER -> serialized, spills to disk.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Rename StorageLevels to something easier to remember - Something like this:\n\nMEMORY_ONLY -> deserialized, in-memory\nMEMORY_ONLY_SER -> serialized, in-memory\nMEMORY_AND_DISK -> deserialized, spills to disk\nMEMORY_AND_DISK_SER -> serialized, spills to disk."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-223, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-531", "title": "Create more explicit logs for cache registration and task completion", "status": "Resolved", "priority": null, "reporter": "Patrick Wendell", "assignee": null, "labels": [], "created": "0012-09-28T12:43:00.000+0000", "updated": "2013-01-24T13:09:36.000+0000", "description": "The logs are pretty confusing when you are getting started with spark:\n\nRegisterRDD(0,1)\nResultTask(0,0)\n\nThese parenthetical indices are used in both cases with very different meanings. I think similar patterns may also be used elsewhere, but I'm not sure. Without increasing verbosity too much, I propose just having:\n\nRegisterRDD(id=0, partitions=1)\nResultTask(stage=0, partition=0)", "comments": ["Imported from Github issue spark-224, originally reported by pwendell", "This is old, deleting it."], "derived": {"summary": "The logs are pretty confusing when you are getting started with spark:\n\nRegisterRDD(0,1)\nResultTask(0,0)\n\nThese parenthetical indices are used in both cases with very different meanings. I think similar patterns may also be used elsewhere, but I'm not sure.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Create more explicit logs for cache registration and task completion - The logs are pretty confusing when you are getting started with spark:\n\nRegisterRDD(0,1)\nResultTask(0,0)\n\nThese parenthetical indices are used in both cases with very different meanings. I think similar patterns may also be used elsewhere, but I'm not sure."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is old, deleting it."}]}}
{"project": "SPARK", "issue_id": "SPARK-339", "title": "Log message which records RDD origin", "status": "Resolved", "priority": null, "reporter": "Patrick Wendell", "assignee": null, "labels": [], "created": "0012-09-28T14:52:00.000+0000", "updated": "2012-10-19T22:50:36.000+0000", "description": "This adds tracking to determine the \"origin\" of an RDD. Origin is defined by\nthe boundary between the user's code and the spark code, during an RDD's\ninstantiation. It is meant to help users understand where a Spark RDD is\ncoming from in their code.\n\nThis patch also logs origin data when stages are submitted to the scheduler.\n\nFinally, it adds a new log message to fix an inconsitency in the way that\ndependent stages (those missing parents) and independent stages (those\nwithout) are logged during submission.", "comments": ["Github comment from mateiz: Thanks for the fixes!", "Imported from Github issue spark-225, originally reported by pwendell"], "derived": {"summary": "This adds tracking to determine the \"origin\" of an RDD. Origin is defined by\nthe boundary between the user's code and the spark code, during an RDD's\ninstantiation.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Log message which records RDD origin - This adds tracking to determine the \"origin\" of an RDD. Origin is defined by\nthe boundary between the user's code and the spark code, during an RDD's\ninstantiation."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-225, originally reported by pwendell"}]}}
{"project": "SPARK", "issue_id": "SPARK-338", "title": "Add a CoalescedRDD for decreasing the number of partitions in a map phase", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0012-09-28T18:04:00.000+0000", "updated": "2013-05-20T20:00:02.000+0000", "description": null, "comments": ["Github comment from mateiz: This has been done.", "Imported from Github issue spark-226, originally reported by mateiz"], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add a CoalescedRDD for decreasing the number of partitions in a map phase"}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-226, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-337", "title": "Allow controlling number of splits in distinct().", "status": "Resolved", "priority": null, "reporter": "Josh Rosen", "assignee": null, "labels": [], "created": "0012-09-28T22:47:00.000+0000", "updated": "2012-10-19T22:50:20.000+0000", "description": "Also documents `distinct()` in the Scala programming guide.", "comments": ["Github comment from mateiz: Alright, thanks!", "Imported from Github issue spark-227, originally reported by JoshRosen"], "derived": {"summary": "Also documents `distinct()` in the Scala programming guide.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Allow controlling number of splits in distinct(). - Also documents `distinct()` in the Scala programming guide."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-227, originally reported by JoshRosen"}]}}
{"project": "SPARK", "issue_id": "SPARK-336", "title": "Added mapPartitionsWithSplit to the programming guide.", "status": "Resolved", "priority": null, "reporter": "Reynold Xin", "assignee": null, "labels": [], "created": "0012-09-29T00:32:00.000+0000", "updated": "2012-10-19T22:50:24.000+0000", "description": null, "comments": ["Github comment from mateiz: Thanks!", "Imported from Github issue spark-228, originally reported by rxin"], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Added mapPartitionsWithSplit to the programming guide."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-228, originally reported by rxin"}]}}
{"project": "SPARK", "issue_id": "SPARK-530", "title": "Spark driver process doesn't exit after finishing", "status": "Resolved", "priority": null, "reporter": "Jaka Jancar", "assignee": null, "labels": [], "created": "0012-09-29T04:50:00.000+0000", "updated": "2013-01-22T23:28:36.000+0000", "description": "My driver program does not seem to exit after running.\n\nIf I place System.exit(0) at the end of my main function, it exits.\n\nThis is on the dev branch.", "comments": ["Github comment from mateiz: Thanks for mentioning this, I'll look into it. Does calling sc.stop() instead of System.exit(0) stop it by any chance?", "Github comment from jakajancar: Yes, it does:\n\n    12/10/01 09:00:27 INFO analyzer.App: Done!\n    12/10/01 09:00:27 INFO mesos.MesosSchedulerBackend: driver.run() returned with code DRIVER_STOPPED\n    12/10/01 09:00:27 INFO spark.MapOutputTrackerActor: MapOutputTrackerActor stopped!\n    12/10/01 09:00:27 INFO spark.CacheTrackerActor: Stopping CacheTrackerActor\n    12/10/01 09:00:27 INFO network.ConnectionManager: Selector selected 0 of 1 keys\n    12/10/01 09:00:27 INFO network.ConnectionManager: Selector thread was interrupted!\n    12/10/01 09:00:27 INFO network.ConnectionManager: ConnectionManager stopped\n    12/10/01 09:00:27 INFO storage.MemoryStore: MemoryStore cleared\n    12/10/01 09:00:27 INFO storage.MemoryStore: Shutting down block dropper\n    12/10/01 09:00:27 INFO storage.BlockManager: BlockManager stopped\n    12/10/01 09:00:27 INFO storage.BlockManagerMasterActor: Stopping BlockManagerMaster\n    12/10/01 09:00:27 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\n    12/10/01 09:00:27 INFO spark.SparkContext: Successfully stopped SparkContext\n\n(The first line is the \"last\" in my code. 0.5 would exit after that, 0.6 just hangs after it)", "Imported from Github issue spark-229, originally reported by jakajancar", "Closing this because I believe it was fixed in 0.6 (the report is from before that)."], "derived": {"summary": "My driver program does not seem to exit after running. If I place System.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Spark driver process doesn't exit after finishing - My driver program does not seem to exit after running. If I place System."}, {"q": "What updates or decisions were made in the discussion?", "a": "Closing this because I believe it was fixed in 0.6 (the report is from before that)."}]}}
{"project": "SPARK", "issue_id": "SPARK-335", "title": "SizeEstimator gives different sizes for Strings on Java 7", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0012-09-30T21:01:00.000+0000", "updated": "2012-10-22T14:55:34.000+0000", "description": "This is probably due to the fields of the String class changing somehow in Java 7. We should probably just not have tests with String, or maybe check for the JVM version too. Right now the current tests fail.", "comments": ["Github comment from shivaram: Is this on 32-bit or 64-bit arch ? I'll try to take a look at this sometime today/tomorrow on openjdk7", "Github comment from mateiz: It was 64-bit, but it doesn't really matter because our tests set the arch and compressed oops deterministically anyway.", "Github comment from shivaram: FWIW, I am not able to reproduce the failure on the dev branch with jdk-7 on debian\n\njava version \"1.7.0_03\"\nOpenJDK Runtime Environment (IcedTea7 2.1.2) (7u3-2.1.2-2)\nOpenJDK 64-Bit Server VM (build 22.0-b10, mixed mode)", "Github comment from mateiz: So this was the environment: Scala version 2.9.2 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_07)\n\nMaybe one difference is that it was Oracle Java?", "Github comment from shivaram: I can reproduce the failure with the same Oracle Java version and digging deeper this is an unfortunate case of the String class being modified in the Oracle JDK.\n\nDetails: The String.java file in open jdk has fields value, offset, count and hash. http://grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/7-b147/java/lang/String.java\n\nHowever the Oracle version gets rid of the offset and count fields. I can see this in the source for the java classes shipped along with the JDK (http://www.oracle.com/technetwork/java/javase/jdk-7-readme-429198.html - Source Code Unfortunately I can't find an online version of this String.java to link to).\n\nIn terms of solving this problem:\n\nWhenever we are estimating the size of classes defined outside our control, I don't think its possible to get a good test case. Testing the dummy classes works well as those are under our control. Strings are the only foreign class we test right now and we could remove those tests.\n\nOne workaround could be to define a local String-like class and use that for testing.  Thoughts ?", "Github comment from rxin: Can't we just add an \"OR\" condition to test string?\n\nI think it is important to have this test, and having it pass on sun + openjdk 1.6 / 1.7 are good enough.", "Github comment from mateiz: I think we could keep one test with String, and maybe make the failure say (\"are you maybe not on OpenJDK or Oracle?\"). However, right now a lot of other tests use Strings, so we should replace those.", "Github comment from shivaram: Looks like longer term this should land on OpenJDK as well (at 7u6, I was using openjdk-7u3 http://mail.openjdk.java.net/pipermail/core-libs-dev/2012-May/010257.html).  I like Reynold's idea of checking for both conditions.  Will create a patch soon.", "Github comment from mateiz: Cool, thanks!", "Imported from Github issue spark-230, originally reported by mateiz"], "derived": {"summary": "This is probably due to the fields of the String class changing somehow in Java 7. We should probably just not have tests with String, or maybe check for the JVM version too.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "SizeEstimator gives different sizes for Strings on Java 7 - This is probably due to the fields of the String class changing somehow in Java 7. We should probably just not have tests with String, or maybe check for the JVM version too."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-230, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-334", "title": "Added a new command \"pl\" in sbt to publish to both Maven and Ivy.", "status": "Resolved", "priority": null, "reporter": "Reynold Xin", "assignee": null, "labels": [], "created": "0012-09-30T23:18:00.000+0000", "updated": "2012-10-19T22:50:28.000+0000", "description": null, "comments": ["Github comment from mateiz: Awesome, thanks!", "Imported from Github issue spark-231, originally reported by rxin"], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Added a new command \"pl\" in sbt to publish to both Maven and Ivy."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-231, originally reported by rxin"}]}}
{"project": "SPARK", "issue_id": "SPARK-333", "title": "0.6: NPE in spark.storage.BlockManager", "status": "Resolved", "priority": null, "reporter": "Jaka Jancar", "assignee": null, "labels": [], "created": "0012-10-01T12:12:00.000+0000", "updated": "2012-10-19T22:50:20.000+0000", "description": "I'm getting the following (it's seems to happen only when I'm working with little data):\n\n\n    12/10/01 16:10:32 ERROR local.LocalScheduler: Exception in task 0\n    java.lang.NullPointerException\n        at spark.storage.BlockManager$.dispose(BlockManager.scala:709)\n        at spark.util.ByteBufferInputStream.cleanUp(ByteBufferInputStream.scala:58)\n        at spark.util.ByteBufferInputStream.read(ByteBufferInputStream.scala:30)\n        at com.ning.compress.lzf.LZFDecoder.readHeader(LZFDecoder.java:363)\n        at com.ning.compress.lzf.LZFDecoder.decompressChunk(LZFDecoder.java:176)\n        at com.ning.compress.lzf.LZFInputStream.readyBuffer(LZFInputStream.java:254)\n        at com.ning.compress.lzf.LZFInputStream.read(LZFInputStream.java:111)\n        at spark.ZigZag$.readInt(KryoSerializer.scala:57)\n        at spark.KryoDeserializationStream.readObject(KryoSerializer.scala:91)\n        at spark.DeserializationStream$$anon$1.getNext(Serializer.scala:84)\n        at spark.DeserializationStream$$anon$1.hasNext(Serializer.scala:94)\n        at spark.RDD$$anonfun$count$1.apply(RDD.scala:272)\n        at spark.RDD$$anonfun$count$1.apply(RDD.scala:270)\n        at spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:426)\n        at spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:426)\n        at spark.scheduler.ResultTask.run(ResultTask.scala:18)\n        at spark.scheduler.local.LocalScheduler.runTask$1(LocalScheduler.scala:74)\n        at spark.scheduler.local.LocalScheduler$$anon$1.run(LocalScheduler.scala:50)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:166)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\n        at spark.DaemonThread.run(DaemonThreadFactory.scala:15)\n    Exception in thread \"main\" spark.SparkException: Job failed: ResultTask(0, 0) failed: ExceptionFailure(java.lang.NullPointerException)\n        at spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:528)\n        at spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:526)\n        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60)\n        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n        at spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:526)\n        at spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:496)\n        at spark.scheduler.DAGScheduler.run(DAGScheduler.scala:268)\n        at spark.scheduler.DAGScheduler$$anon$1.run(DAGScheduler.scala:89)", "comments": ["Github comment from rxin: Also seeing this once I upgraded to the latest dev.\n\nI believe this problem did not exist a few days ago. Must be a very recent\ncommit.\n\n--\nReynold Xin\nAMPLab, UC Berkeley\nhttp://www.cs.berkeley.edu/~rxin/\n\n\n\nOn Mon, Oct 1, 2012 at 1:12 PM, Jaka Janar <notifications@github.com>wrote:\n\n> I'm getting the following (it's seems to happen only when I'm working with\n> little data):\n>\n> 12/10/01 16:10:32 ERROR local.LocalScheduler: Exception in task 0\n> java.lang.NullPointerException\n>     at spark.storage.BlockManager$.dispose(BlockManager.scala:709)\n>     at spark.util.ByteBufferInputStream.cleanUp(ByteBufferInputStream.scala:58)\n>     at spark.util.ByteBufferInputStream.read(ByteBufferInputStream.scala:30)\n>     at com.ning.compress.lzf.LZFDecoder.readHeader(LZFDecoder.java:363)\n>     at com.ning.compress.lzf.LZFDecoder.decompressChunk(LZFDecoder.java:176)\n>     at com.ning.compress.lzf.LZFInputStream.readyBuffer(LZFInputStream.java:254)\n>     at com.ning.compress.lzf.LZFInputStream.read(LZFInputStream.java:111)\n>     at spark.ZigZag$.readInt(KryoSerializer.scala:57)\n>     at spark.KryoDeserializationStream.readObject(KryoSerializer.scala:91)\n>     at spark.DeserializationStream$$anon$1.getNext(Serializer.scala:84)\n>     at spark.DeserializationStream$$anon$1.hasNext(Serializer.scala:94)\n>     at spark.RDD$$anonfun$count$1.apply(RDD.scala:272)\n>     at spark.RDD$$anonfun$count$1.apply(RDD.scala:270)\n>     at spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:426)\n>     at spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:426)\n>     at spark.scheduler.ResultTask.run(ResultTask.scala:18)\n>     at spark.scheduler.local.LocalScheduler.runTask$1(LocalScheduler.scala:74)\n>     at spark.scheduler.local.LocalScheduler$$anon$1.run(LocalScheduler.scala:50)\n>     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n>     at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\n>     at java.util.concurrent.FutureTask.run(FutureTask.java:166)\n>     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\n>     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\n>     at spark.DaemonThread.run(DaemonThreadFactory.scala:15)\n> Exception in thread \"main\" spark.SparkException: Job failed: ResultTask(0, 0) failed: ExceptionFailure(java.lang.NullPointerException)\n>     at spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:528)\n>     at spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:526)\n>     at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60)\n>     at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n>     at spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:526)\n>     at spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:496)\n>     at spark.scheduler.DAGScheduler.run(DAGScheduler.scala:268)\n>     at spark.scheduler.DAGScheduler$$anon$1.run(DAGScheduler.scala:89)\n>\n>  \n> Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/issues/232>.\n>\n>", "Github comment from rxin: I've isolated the problem to the following commit:\n\nlogcommit 9b326d01e9a9ec4a4a9abf293cf039c07d426293Author: Matei Zaharia <matei@eecs.berkeley.edu>Date:   Sat Sep 29 20:21:54 2012 -0700\n   Made BlockManager unmap memory-mapped files when necessary to reduce the\n   number of open files. Also optimized sending of disk-based blocks.\n\ncommit 56dcad593641ef8de211fcb4303574a9f4509f89\n\n\n\nThe problem is in BlockManager.scala dispose, the cleaner is null.\n\n      def dispose(buffer: ByteBuffer) {\n        if (buffer != null && buffer.isInstanceOf[MappedByteBuffer]) {\n          logDebug(\"Unmapping \" + buffer)\n          buffer.asInstanceOf[DirectBuffer].cleaner().clean()\n        }\n      }", "Github comment from rxin: Submitted a patch in pull request #233.", "Imported from Github issue spark-232, originally reported by jakajancar"], "derived": {"summary": "I'm getting the following (it's seems to happen only when I'm working with little data):\n\n\n    12/10/01 16:10:32 ERROR local. LocalScheduler: Exception in task 0\n    java.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "0.6: NPE in spark.storage.BlockManager - I'm getting the following (it's seems to happen only when I'm working with little data):\n\n\n    12/10/01 16:10:32 ERROR local. LocalScheduler: Exception in task 0\n    java."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-232, originally reported by jakajancar"}]}}
{"project": "SPARK", "issue_id": "SPARK-332", "title": "Fixed #232: DirectBuffer's cleaner was empty and Spark tried to invoke clean on it.", "status": "Resolved", "priority": null, "reporter": "Reynold Xin", "assignee": null, "labels": [], "created": "0012-10-01T13:08:00.000+0000", "updated": "2012-10-19T22:50:31.000+0000", "description": null, "comments": ["Github comment from mateiz: Thanks Reynold! I actually saw that check in some of the samples I read but never saw it happen on my box.", "Imported from Github issue spark-233, originally reported by rxin"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Fixed #232: DirectBuffer's cleaner was empty and Spark tried to invoke clean on it."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-233, originally reported by rxin"}]}}
{"project": "SPARK", "issue_id": "SPARK-331", "title": "Publish local maven", "status": "Resolved", "priority": null, "reporter": "Patrick Wendell", "assignee": null, "labels": [], "created": "0012-10-01T14:40:00.000+0000", "updated": "2012-10-19T22:50:35.000+0000", "description": "This just over-writes the default behavior of publish-local rather than having a separate \"pl\" target. It worked for me when I ran from a clean build.", "comments": ["Imported from Github issue spark-234, originally reported by pwendell"], "derived": {"summary": "This just over-writes the default behavior of publish-local rather than having a separate \"pl\" target. It worked for me when I ran from a clean build.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Publish local maven - This just over-writes the default behavior of publish-local rather than having a separate \"pl\" target. It worked for me when I ran from a clean build."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-234, originally reported by pwendell"}]}}
{"project": "SPARK", "issue_id": "SPARK-330", "title": "publish-local should go to maven + ivy by default", "status": "Resolved", "priority": null, "reporter": "Patrick Wendell", "assignee": null, "labels": [], "created": "0012-10-01T14:44:00.000+0000", "updated": "2012-10-19T22:50:35.000+0000", "description": "This makes publishing to ivy and maven the default behavior. Worked for me on a clean build.", "comments": ["Github comment from mateiz: Great, thanks.", "Github comment from RayRacine: FWIW, I don't see why the default should be to publish everything twice.  For  project builds SBT allows one to specify whether a particular dependency is M2 or Ivy.  No particular need to double publish locally.  Not arguing against having the build system support double local publishing, just making it the default behavior.  I am not aware of any other SBT / Scala project that follows this approach.", "Github comment from mateiz: Are you worried that the build will just take too long? I think that in that case we should do Maven only; the problem is that more tools seem to be aware of Maven than Ivy. But I'm not sure how easy it is to change the rules in sbt so that publish-local means Maven only. We'd probably have to give it a different name (e.g. maven-local).", "Github comment from pwendell: I think the ideal setup would be\n\nmaven-local\nivy-local\npublish-local (goes to both)\n\nWith regards to publish-local going to both: For people who have\nexpectations based on other projects, I don't see that they are\nparticularly harmed by having it publish to maven as well - it takes\nonly negligible time. For people who do not have expectations from other\nprojects, I think it's most intuitive that publish-local would publish to\nany local repo's that are publish-able. This project supports a Java API,\nso we expect to have a lot of people building for the purpose of hooking\ninto maven, including people who aren't active developers on several scala\nprojects.\n\n- Patrick\n\nOn Tue, Oct 2, 2012 at 11:50 AM, Matei Zaharia <notifications@github.com>wrote:\n\n> Are you worried that the build will just take too long? I think that in\n> that case we should do Maven only; the problem is that more tools seem to\n> be aware of Maven than Ivy. But I'm not sure how easy it is to change the\n> rules in sbt so that publish-local means Maven only. We'd probably have to\n> give it a different name (e.g. maven-local).\n>\n> \n> Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/pull/235#issuecomment-9082276>.\n>\n>", "Github comment from rxin: Patrick is best to respond to this, but I think the extra time is measured\nin seconds (just a few seconds) ....\n--\nReynold Xin\nAMPLab, UC Berkeley\nhttp://www.cs.berkeley.edu/~rxin/\n\n\n\nOn Tue, Oct 2, 2012 at 11:50 AM, Matei Zaharia <notifications@github.com>wrote:\n\n> Are you worried that the build will just take too long? I think that in\n> that case we should do Maven only; the problem is that more tools seem to\n> be aware of Maven than Ivy. But I'm not sure how easy it is to change the\n> rules in sbt so that publish-local means Maven only. We'd probably have to\n> give it a different name (e.g. maven-local).\n>\n> \n> Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/pull/235#issuecomment-9082276>.\n>\n>", "Github comment from RayRacine: Sorry, guys.  Did not intend to generate a tempest in a teapot on this one.\n\nCertainly there is a tacit expectation (principle of least surprise) that a standard Scala project built with SBT upon execution of publish-local will Ivy publish locally at a minimum.  I don't think that should change, which it has not.\n\nExtending the behavior to double publish on the surface may affect those who already customize their local-publish command (which I do).   In my case the double publishing had no deleterious impact.  (Changing publish-local to maven only would have.)\n\nAfter that the question comes down to time and space considerations.  In my case for example, I routinely disable ScalaDoc and Source(?) packaging in publish local to save time in my development cycle.\n\nAll-in-all, this is a minor concern.  I'd have left publish-local to default behavior and extended behavior with publish-local-maven.  It's a one-liner to put back single Ivy local publishing and I've already made that change in my Spark clone.  So all is well.  I withdraw my concerns.", "Imported from Github issue spark-235, originally reported by pwendell"], "derived": {"summary": "This makes publishing to ivy and maven the default behavior. Worked for me on a clean build.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "publish-local should go to maven + ivy by default - This makes publishing to ivy and maven the default behavior. Worked for me on a clean build."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-235, originally reported by pwendell"}]}}
{"project": "SPARK", "issue_id": "SPARK-329", "title": "A Spark \"Quick Start\" example", "status": "Resolved", "priority": null, "reporter": "Patrick Wendell", "assignee": null, "labels": [], "created": "0012-10-01T16:24:00.000+0000", "updated": "2012-10-19T22:50:33.000+0000", "description": "This commit includes a quick start example that covers:\n1) Basic usage of the Spark shell\n2) A simple Spark job in Scala\n3) A simple Spark job in Java", "comments": ["Github comment from mateiz: This looks pretty good overall! A few comments though:\n\n* For the standalone jobs, show them how to package their job into a JAR and pass that to SparkContext. Don't use the two-arguent SparkContext constructor. In fact it might be good to end the guide with a section called \"running on a cluster\" that points them to the standalone cluster doc and tells them to try running the job against that.\n\n* At the beginning, include a line on how to compile Spark (it's just sbt package)\n\n* Link to the programming guide when describing the operations. It might also be good to add an example of reduceByKey that does a word count, without fully explaining it, and say \"look in the programming guide for details of these operations\".\n\n* Separate the \"A Spark Job\" into two top-level sections: Writing a Standalone Scala Job and Writing a Standalone Java Job", "Github comment from pwendell: Hey Matei,\n\nI want to make sure I understand the first point, since I haven't used that\nconstructor yet. The job would look the same as it does now, except that it\nwould pass a reference to a packaged jar file (containing the job itself)\nto the constructor. We'd walk through pacaking the jar file in a way that's\nconsistent with the name passed in the constructor.\n\n- Patrick\n\nOn Tue, Oct 2, 2012 at 6:15 PM, Matei Zaharia <notifications@github.com>wrote:\n\n> This looks pretty good overall! A few comments though:\n>\n>    -\n>\n>    For the standalone jobs, show them how to package their job into a JAR\n>    and pass that to SparkContext. Don't use the two-arguent SparkContext\n>    constructor. In fact it might be good to end the guide with a section\n>    called \"running on a cluster\" that points them to the standalone cluster\n>    doc and tells them to try running the job against that.\n>    -\n>\n>    At the beginning, include a line on how to compile Spark (it's just\n>    sbt package)\n>    -\n>\n>    Link to the programming guide when describing the operations. It might\n>    also be good to add an example of reduceByKey that does a word count,\n>    without fully explaining it, and say \"look in the programming guide for\n>    details of these operations\".\n>    -\n>\n>    Separate the \"A Spark Job\" into two top-level sections: Writing a\n>    Standalone Scala Job and Writing a Standalone Java Job\n>\n>  \n> Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/pull/236#issuecomment-9092831>.\n>\n>", "Github comment from mateiz: Yes, exactly. Look at the part on Linking with Spark in the programing guide.", "Github comment from pwendell: This latest commit includes substantial changes based on Matei's feedback. It addresses all 4 comments.", "Github comment from mateiz: Thanks Patrick! Might do a few minor edits to this but it looks good.", "Github comment from pwendell: Sounds good. In your court now.\n\nOn Wed, Oct 3, 2012 at 8:31 AM, Matei Zaharia <notifications@github.com>wrote:\n\n> Thanks Patrick! Might do a few minor edits to this but it looks good.\n>\n> \n> Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/pull/236#issuecomment-9110467>.\n>\n>", "Imported from Github issue spark-236, originally reported by pwendell"], "derived": {"summary": "This commit includes a quick start example that covers:\n1) Basic usage of the Spark shell\n2) A simple Spark job in Scala\n3) A simple Spark job in Java.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "A Spark \"Quick Start\" example - This commit includes a quick start example that covers:\n1) Basic usage of the Spark shell\n2) A simple Spark job in Scala\n3) A simple Spark job in Java."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-236, originally reported by pwendell"}]}}
{"project": "SPARK", "issue_id": "SPARK-328", "title": "Don't build spark-repl-assembly in the assembly target by default", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0012-10-02T10:56:00.000+0000", "updated": "2012-10-22T14:55:35.000+0000", "description": "I think we needed it for Shark a while back, but it should not be necessary if we make Shark find Spark through Maven instead. Also it takes a while to build.", "comments": ["Github comment from mateiz: This is now fixed.", "Imported from Github issue spark-237, originally reported by mateiz"], "derived": {"summary": "I think we needed it for Shark a while back, but it should not be necessary if we make Shark find Spark through Maven instead. Also it takes a while to build.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Don't build spark-repl-assembly in the assembly target by default - I think we needed it for Shark a while back, but it should not be necessary if we make Shark find Spark through Maven instead. Also it takes a while to build."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-237, originally reported by mateiz"}]}}
{"project": "SPARK", "issue_id": "SPARK-327", "title": "Allow whitespaces in cluster URL configuration for local cluster.", "status": "Resolved", "priority": null, "reporter": "Reynold Xin", "assignee": null, "labels": [], "created": "0012-10-02T10:58:00.000+0000", "updated": "2012-10-19T22:50:29.000+0000", "description": "E.g. allow me to say (note the whitespace)\n\nexport MASTER=\"local-cluster[2, 1, 512]\"", "comments": ["Github comment from rxin: I piggybacked another commit into this. The other one checks to make sure SPARK_MEM <= memoryPerSlave for local cluster mode.", "Github comment from mateiz: Good idea, thanks.", "Imported from Github issue spark-238, originally reported by rxin"], "derived": {"summary": "E. g.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Allow whitespaces in cluster URL configuration for local cluster. - E. g."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-238, originally reported by rxin"}]}}
{"project": "SPARK", "issue_id": "SPARK-529", "title": "Have a single file that controls the environmental variables and spark config options", "status": "Resolved", "priority": "Major", "reporter": "Reynold Xin", "assignee": "Marcelo Masiero Vanzin", "labels": [], "created": "0012-10-02T14:41:00.000+0000", "updated": "2016-04-05T22:21:01.000+0000", "description": "E.g. multiple places in the code base uses SPARK_MEM and has its own default set to 512. We need a central place to enforce default values as well as documenting the variables.", "comments": ["Imported from Github issue spark-239, originally reported by rxin", "This looks obsolete and/or fixed, as variables like SPARK_MEM are deprecated, and I suppose there is spark-env.sh too.", "I'm reopening this since I believe it's a worthy addition; in fact, Spark SQL already has something similar, and I'm just refactoring that code a little for use in the other modules.\n\n(It's not a single file per-se, but the spirit is the same - one location where a particular config option is defined - name, type and default value).", "User 'vanzin' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/10205", "User 'vanzin' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/11570", "The new infrastructure is in place, although not all configs are currently using it. Those can be dealt with separately."], "derived": {"summary": "E. g.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Have a single file that controls the environmental variables and spark config options - E. g."}, {"q": "What updates or decisions were made in the discussion?", "a": "The new infrastructure is in place, although not all configs are currently using it. Those can be dealt with separately."}]}}
{"project": "SPARK", "issue_id": "SPARK-326", "title": "Package-Private Classes", "status": "Resolved", "priority": null, "reporter": "Denny Britz", "assignee": null, "labels": [], "created": "0012-10-02T18:06:00.000+0000", "updated": "2012-10-19T22:50:38.000+0000", "description": "Made all core classes except for *RDD and what's used by Shark package private. Haven't yet figured out how to make the HTML templates for the standalone UI private. \n\nI'll do the same for the other subprojects, just wanted to get comments.", "comments": ["Github comment from mateiz: Good start, but the following should *not* be private:\n* Accumulable, AccumulableParam, Accumulator, AccumulatorParam\n* Broadcast", "Github comment from mateiz: Also PairRDDFunctions, DoubleRDDFunctions, etc (all the implicit conversions).", "Github comment from mateiz: By the way, there's no need to do the same in repl and examples. We are not providing ScalaDoc for those. Maybe do it for internal classes in Bagel, but there aren't any of those AFAIK.", "Github comment from mateiz: One last thing: as a matter of style, maybe put the private[spark] on a separate line above the class name when the class name is long or has many parameters. Also, no need to include it for inner classes in a class that is already private[spark].", "Github comment from dennybritz: Made the changes. I thought I didn't mark any inner classes, at least not purposefully.", "Github comment from rxin: Denny - can you check if Shark still compiles with this?", "Github comment from dennybritz: Yes, Shark compiles. There are some classes (like OneToOneDependency) that I left purposefully public for Shark even though they probably shouldn't be.", "Github comment from JoshRosen: What about the Java API's classes?  The abstract traits `WrappedFunction1` and `WrappedFunction2` should probably be private, since they aren't part of the user-facing API.\n\nI think that `JavaRDDLike` should be private.  It doesn't really function like a proper RDD interface/base class since some common methods like `filter()` and `cache()` aren't defined in it, so leaving it public could be confusing.  This situation is due to a weird interaction between a Scala compiler bug and Java, which prevents me from completely following the [`TraversableLike` approach](http://docs.scala-lang.org/overviews/core/architecture-of-scala-collections.html) and having abstract methods that return RDDs of the instance's type.", "Github comment from dennybritz: Yeah, I wasn't sure about those. Seems like making them private is the right thing to do then.", "Github comment from JoshRosen: On the other hand, `JavaRDDLike` might be useful when implementing other language APIs on top of the Java API; removing it would force you to have variables of type `Object`  to hold Java RDDs of any type.  Both approaches would require casting, but `JavaRDDLike` provides slightly more type safety.  I suppose a third option would be to define an empty trait like `JavaRDDBase` for the sole purpose of being a superclass/interface for the Java RDDs.", "Github comment from dennybritz: Yeah, that makes sense. I left JavaRDDLike public for now.", "Github comment from mateiz: Looks good. I think we should leave the Dependency stuff public in case people want to subclass RDD themselves. We might instead make some of the RDD subclasses private (e.g. MappedRDD) but let's leave that for later. I'll merge this in now.", "Github comment from mateiz: I just made a few more things package-private, so try Shark again. Doubt it's affected though. I left all the shuffle RDDs Reynold added in.", "Imported from Github issue spark-240, originally reported by dennybritz"], "derived": {"summary": "Made all core classes except for *RDD and what's used by Shark package private. Haven't yet figured out how to make the HTML templates for the standalone UI private.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Package-Private Classes - Made all core classes except for *RDD and what's used by Shark package private. Haven't yet figured out how to make the HTML templates for the standalone UI private."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-240, originally reported by dennybritz"}]}}
{"project": "SPARK", "issue_id": "SPARK-325", "title": "First cut at adding documentation for GC tuning", "status": "Resolved", "priority": null, "reporter": "Shivaram Venkataraman", "assignee": null, "labels": [], "created": "0012-10-02T19:11:00.000+0000", "updated": "2012-10-19T22:50:32.000+0000", "description": "Created new sub-headings for cache-size tuning and GC tuning.", "comments": ["Github comment from mateiz: Thanks Shivaram!", "Imported from Github issue spark-241, originally reported by shivaram"], "derived": {"summary": "Created new sub-headings for cache-size tuning and GC tuning.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "First cut at adding documentation for GC tuning - Created new sub-headings for cache-size tuning and GC tuning."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-241, originally reported by shivaram"}]}}
{"project": "SPARK", "issue_id": "SPARK-324", "title": "Changing version of Scala in README", "status": "Resolved", "priority": null, "reporter": "Patrick Wendell", "assignee": null, "labels": [], "created": "0012-10-02T21:11:00.000+0000", "updated": "2012-10-19T22:50:27.000+0000", "description": null, "comments": ["Github comment from mateiz: Good catch!", "Imported from Github issue spark-242, originally reported by pwendell"], "derived": {"summary": "", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Changing version of Scala in README"}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-242, originally reported by pwendell"}]}}
{"project": "SPARK", "issue_id": "SPARK-323", "title": "Removes the included mesos-0.9.0.jar and pulls it from Maven Central instead", "status": "Resolved", "priority": null, "reporter": "Andy Konwinski", "assignee": null, "labels": [], "created": "0012-10-03T08:04:00.000+0000", "updated": "2012-10-19T22:50:23.000+0000", "description": "Removes the included mesos-0.9.0.jar and adds a libraryDependency to the build file so that mesos-0.9.0-incubating.jar (which contains the same class files, but has a silightly different name) will be pulled down from Maven Central instead.", "comments": ["Github comment from mateiz: Sweet! Thanks Andy.", "Imported from Github issue spark-243, originally reported by andyk"], "derived": {"summary": "Removes the included mesos-0. 9.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Removes the included mesos-0.9.0.jar and pulls it from Maven Central instead - Removes the included mesos-0. 9."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-243, originally reported by andyk"}]}}
{"project": "SPARK", "issue_id": "SPARK-322", "title": "Made Serializer and JavaSerializer non private.", "status": "Resolved", "priority": null, "reporter": "Reynold Xin", "assignee": null, "labels": [], "created": "0012-10-03T09:21:00.000+0000", "updated": "2012-10-19T22:50:26.000+0000", "description": null, "comments": ["Imported from Github issue spark-244, originally reported by rxin"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Made Serializer and JavaSerializer non private."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-244, originally reported by rxin"}]}}
{"project": "SPARK", "issue_id": "SPARK-321", "title": "Some additions to the Tuning Guide.", "status": "Resolved", "priority": null, "reporter": "Patrick Wendell", "assignee": null, "labels": [], "created": "0012-10-03T13:08:00.000+0000", "updated": "2012-10-19T22:50:38.000+0000", "description": "1. Slight change in organization\n2. Added pre-requisites\n3. Made a new section about determining memory footprint\n   of an RDD\n4. Other small changes", "comments": ["Imported from Github issue spark-245, originally reported by pwendell"], "derived": {"summary": "1. Slight change in organization\n2.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Some additions to the Tuning Guide. - 1. Slight change in organization\n2."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-245, originally reported by pwendell"}]}}
{"project": "SPARK", "issue_id": "SPARK-320", "title": "Fix SizeEstimator tests to work with String classes in JDK 6 and 7", "status": "Resolved", "priority": null, "reporter": "Shivaram Venkataraman", "assignee": null, "labels": [], "created": "0012-10-04T18:44:00.000+0000", "updated": "2012-10-19T22:50:34.000+0000", "description": "Fixes issue #230", "comments": ["Github comment from mateiz: Thanks Shivaram!", "Imported from Github issue spark-246, originally reported by shivaram"], "derived": {"summary": "Fixes issue #230.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Fix SizeEstimator tests to work with String classes in JDK 6 and 7 - Fixes issue #230."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-246, originally reported by shivaram"}]}}
{"project": "SPARK", "issue_id": "SPARK-319", "title": "Dev", "status": "Resolved", "priority": null, "reporter": "squito", "assignee": null, "labels": [], "created": "0012-10-05T07:39:00.000+0000", "updated": "2012-10-19T22:50:39.000+0000", "description": "This is just a repeat of\nhttps://github.com/mesos/spark/pull/173\nthat merge request didn't make it into the dev branch", "comments": ["Github comment from mateiz: Oh shoot, didn't realize that this didn't make it into dev.", "Imported from Github issue spark-247, originally reported by squito"], "derived": {"summary": "This is just a repeat of\nhttps://github. com/mesos/spark/pull/173\nthat merge request didn't make it into the dev branch.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Dev - This is just a repeat of\nhttps://github. com/mesos/spark/pull/173\nthat merge request didn't make it into the dev branch."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-247, originally reported by squito"}]}}
{"project": "SPARK", "issue_id": "SPARK-528", "title": "Provide a dist-like target that builds a binary distribution (JARs + scripts)", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "0012-10-05T12:23:00.000+0000", "updated": "2014-09-16T17:45:30.000+0000", "description": "So that Spark can be deployed without copying the whole built source folder.", "comments": ["Imported from Github issue spark-248, originally reported by mateiz", "Browsing old issues: I think this is long since fixed, along with https://issues.apache.org/jira/browse/SPARK-547 , because the build produces assembly JARs?", "Yeah thanks this was fixed a long time ago."], "derived": {"summary": "So that Spark can be deployed without copying the whole built source folder.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Provide a dist-like target that builds a binary distribution (JARs + scripts) - So that Spark can be deployed without copying the whole built source folder."}, {"q": "What updates or decisions were made in the discussion?", "a": "Yeah thanks this was fixed a long time ago."}]}}
{"project": "SPARK", "issue_id": "SPARK-318", "title": "Move RDD classes/files to their own package/directory", "status": "Resolved", "priority": null, "reporter": "Andy Konwinski", "assignee": null, "labels": [], "created": "0012-10-05T19:08:00.000+0000", "updated": "2012-10-19T22:50:27.000+0000", "description": "Refactors RDD classes/files to their own package/directory. All RDD files except for RDD.scala\nare now in src/main/scala/spark/rdd. Also fixes a bug in AccumulatorSuite.scala that was causing\nthe test named \"localValue readable in tasks\" to fail.", "comments": ["Github comment from mateiz: Awesome, thanks!", "Github comment from mateiz: By the way just FYI, your Eclipse seemed to have added one tab, so you might want to change it to spaces only. I'll fix that in another commit.", "Imported from Github issue spark-249, originally reported by andyk"], "derived": {"summary": "Refactors RDD classes/files to their own package/directory. All RDD files except for RDD.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Move RDD classes/files to their own package/directory - Refactors RDD classes/files to their own package/directory. All RDD files except for RDD."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-249, originally reported by andyk"}]}}
{"project": "SPARK", "issue_id": "SPARK-317", "title": "Fixed a bug in addFile that if the file is specified as \"file:///\", the symlink is created incorrectly for local mode.", "status": "Resolved", "priority": null, "reporter": "Reynold Xin", "assignee": null, "labels": [], "created": "0012-10-06T23:55:00.000+0000", "updated": "2012-10-19T22:50:39.000+0000", "description": null, "comments": ["Github comment from mateiz: Looks good, thanks.", "Imported from Github issue spark-250, originally reported by rxin"], "derived": {"summary": "", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Fixed a bug in addFile that if the file is specified as \"file:///\", the symlink is created incorrectly for local mode."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-250, originally reported by rxin"}]}}
{"project": "SPARK", "issue_id": "SPARK-316", "title": "Document Dependency classes and make minor interface improvements", "status": "Resolved", "priority": null, "reporter": "Josh Rosen", "assignee": null, "labels": [], "created": "0012-10-07T00:19:00.000+0000", "updated": "2012-10-19T22:50:26.000+0000", "description": "These commits add API documentation for the `Dependency` classes and make two interface changes:\n\n- Remove the unused `isShuffle` field from `Dependency`\n- Change the type of `ShuffleDependency.aggregator` to an `Option` to communicate that the aggregator is optional.  Currently, the absence of an aggregator is expressed using `Aggregator[K, V, V](null, null, null, false)`, which is a bit of a hack.\n\nWill these two changes introduce backwards-compatibility problems in the API?", "comments": ["Github comment from mateiz: Cool, thanks!", "Github comment from mateiz: I'm okay changing the RDD internal API for now, though this might affect Shark. We will lock it down more when we get to a version 1.0.", "Imported from Github issue spark-251, originally reported by JoshRosen"], "derived": {"summary": "These commits add API documentation for the `Dependency` classes and make two interface changes:\n\n- Remove the unused `isShuffle` field from `Dependency`\n- Change the type of `ShuffleDependency. aggregator` to an `Option` to communicate that the aggregator is optional.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Document Dependency classes and make minor interface improvements - These commits add API documentation for the `Dependency` classes and make two interface changes:\n\n- Remove the unused `isShuffle` field from `Dependency`\n- Change the type of `ShuffleDependency. aggregator` to an `Option` to communicate that the aggregator is optional."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-251, originally reported by JoshRosen"}]}}
{"project": "SPARK", "issue_id": "SPARK-315", "title": "Adding Sonatype releases to SBT.", "status": "Resolved", "priority": null, "reporter": "Patrick Wendell", "assignee": null, "labels": [], "created": "0012-10-07T00:21:00.000+0000", "updated": "2012-10-19T22:50:26.000+0000", "description": "This does a few things to get this branch ready for release:\n\n1. Upgrades the sbt and Scala version\n2. Sets the release number to 0.5.1\n3. Adds the Sonatype publishing target\n4. Installs the PGP signing plugin\n5. Removes the Mesos jar dependency", "comments": ["Github comment from mateiz: Looks good, but let me edit a few other files that point to Scala 2.9.1 before publishing. I'll let you know when I'm done.", "Imported from Github issue spark-252, originally reported by pwendell"], "derived": {"summary": "This does a few things to get this branch ready for release:\n\n1. Upgrades the sbt and Scala version\n2.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Adding Sonatype releases to SBT. - This does a few things to get this branch ready for release:\n\n1. Upgrades the sbt and Scala version\n2."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-252, originally reported by pwendell"}]}}
{"project": "SPARK", "issue_id": "SPARK-314", "title": "Changed the println to logInfo in Utils.fetchFile.", "status": "Resolved", "priority": null, "reporter": "Reynold Xin", "assignee": null, "labels": [], "created": "0012-10-07T00:54:00.000+0000", "updated": "2012-10-19T22:50:39.000+0000", "description": null, "comments": ["Imported from Github issue spark-253, originally reported by rxin"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Changed the println to logInfo in Utils.fetchFile."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-253, originally reported by rxin"}]}}
{"project": "SPARK", "issue_id": "SPARK-313", "title": "Removing one link in quickstart", "status": "Resolved", "priority": null, "reporter": "Patrick Wendell", "assignee": null, "labels": [], "created": "0012-10-08T07:55:00.000+0000", "updated": "2012-10-19T22:50:38.000+0000", "description": null, "comments": ["Imported from Github issue spark-254, originally reported by pwendell"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Removing one link in quickstart"}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-254, originally reported by pwendell"}]}}
{"project": "SPARK", "issue_id": "SPARK-312", "title": "Removes the annoying small gap above the nav menu dropdown boxes", "status": "Resolved", "priority": null, "reporter": "Andy Konwinski", "assignee": null, "labels": [], "created": "0012-10-08T08:35:00.000+0000", "updated": "2012-10-19T22:50:28.000+0000", "description": "Fixes the small gap above the nav menu dropdown boxes and the hoverable menu items that was causing the dropdowns to go away when the user moved their mouse down towards them.", "comments": ["Github comment from mateiz: Great, thanks!", "Imported from Github issue spark-255, originally reported by andyk"], "derived": {"summary": "Fixes the small gap above the nav menu dropdown boxes and the hoverable menu items that was causing the dropdowns to go away when the user moved their mouse down towards them.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Removes the annoying small gap above the nav menu dropdown boxes - Fixes the small gap above the nav menu dropdown boxes and the hoverable menu items that was causing the dropdowns to go away when the user moved their mouse down towards them."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-255, originally reported by andyk"}]}}
{"project": "SPARK", "issue_id": "SPARK-311", "title": "Adding new download instructions", "status": "Resolved", "priority": null, "reporter": "Patrick Wendell", "assignee": null, "labels": [], "created": "0012-10-08T08:44:00.000+0000", "updated": "2012-10-19T22:50:34.000+0000", "description": null, "comments": ["Github comment from pwendell: I updated this just now to reflect the templated versions. I also changed the wording a bit.", "Github comment from mateiz: Thanks but I think this is too long for an overview page. Just say \"this documentation corresponds to version X\" and they can figure out how to get others. Also, cut the part about Git.", "Github comment from pwendell: sure now this just says the version and the downloads page.", "Github comment from mateiz: OK, thanks.", "Imported from Github issue spark-256, originally reported by pwendell"], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Adding new download instructions"}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-256, originally reported by pwendell"}]}}
{"project": "SPARK", "issue_id": "SPARK-310", "title": "Adds special version variables to docs templating system", "status": "Resolved", "priority": null, "reporter": "Andy Konwinski", "assignee": null, "labels": [], "created": "0012-10-08T09:32:00.000+0000", "updated": "2012-10-19T22:50:36.000+0000", "description": "Adds liquid variables to docs templating system so that they can be used throughout the docs: SPARK_VERSION, SCALA_VERSION, and MESOS_VERSION.\n\nTo use them, e.g. use {{site.SPARK_VERSION}}.\n\nAlso removes uses of {{HOME_PATH}} which were being resolved to \"\"\nby the templating system anyway.", "comments": ["Github comment from mateiz: Can you pull the latest change from dev into your branch to make it mergeable? One of the earlier commits made it incompatible.", "Github comment from andyk: Ok, I think I've fixed the conflict. i also updated the README with instructions for building without scaladoc (because this feature can save people lots of time).", "Github comment from mateiz: Great, thanks!", "Imported from Github issue spark-257, originally reported by andyk"], "derived": {"summary": "Adds liquid variables to docs templating system so that they can be used throughout the docs: SPARK_VERSION, SCALA_VERSION, and MESOS_VERSION. To use them, e.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Adds special version variables to docs templating system - Adds liquid variables to docs templating system so that they can be used throughout the docs: SPARK_VERSION, SCALA_VERSION, and MESOS_VERSION. To use them, e."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-257, originally reported by andyk"}]}}
{"project": "SPARK", "issue_id": "SPARK-309", "title": "Readding yarn-standalone scheduler scheme", "status": "Resolved", "priority": null, "reporter": "Denny Britz", "assignee": null, "labels": [], "created": "0012-10-08T14:01:00.000+0000", "updated": "2012-10-19T22:50:31.000+0000", "description": "Also merged the latest dev branch in.", "comments": ["Github comment from mateiz: Thanks. Please reply to that thread on spark-developers too.", "Imported from Github issue spark-258, originally reported by dennybritz"], "derived": {"summary": "Also merged the latest dev branch in.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Readding yarn-standalone scheduler scheme - Also merged the latest dev branch in."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-258, originally reported by dennybritz"}]}}
{"project": "SPARK", "issue_id": "SPARK-308", "title": "Synchronization bug fix in broadcast implementations", "status": "Resolved", "priority": null, "reporter": "Mosharaf Chowdhury", "assignee": null, "labels": [], "created": "0012-10-08T15:34:00.000+0000", "updated": "2012-10-19T22:50:26.000+0000", "description": "- Synchronization bugs found during debugging the broadcast problem faced by one of the users (their problem couldn't be reproduced and still remain unresolved)\n- Removed some logging comments\n- Changed the broadcast example to call broadcast on multiple consecutive iterations, instead of just one.", "comments": ["Github comment from mateiz: Thanks Mosharaf. It would still be good to investigate and fix the full bug sometime though.", "Imported from Github issue spark-259, originally reported by mosharaf"], "derived": {"summary": "- Synchronization bugs found during debugging the broadcast problem faced by one of the users (their problem couldn't be reproduced and still remain unresolved)\n- Removed some logging comments\n- Changed the broadcast example to call broadcast on multiple consecutive iterations, instead of just one.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Synchronization bug fix in broadcast implementations - - Synchronization bugs found during debugging the broadcast problem faced by one of the users (their problem couldn't be reproduced and still remain unresolved)\n- Removed some logging comments\n- Changed the broadcast example to call broadcast on multiple consecutive iterations, instead of just one."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-259, originally reported by mosharaf"}]}}
{"project": "SPARK", "issue_id": "SPARK-307", "title": "Updates docs to use the new version num vars and adds Spark version in nav bar", "status": "Resolved", "priority": null, "reporter": "Andy Konwinski", "assignee": null, "labels": [], "created": "0012-10-08T16:25:00.000+0000", "updated": "2012-10-19T22:50:28.000+0000", "description": "Updating lots of docs to use the new special version number variables, and adding the version to the navbar so it is easy to tell which version of Spark these docs were compiled for.\n\nSee what the generated docs look like after this commit at: http://www.cs.berkeley.edu/~andyk/spark-docs-e1a724f39/", "comments": ["Github comment from mateiz: Thanks Andy.", "Imported from Github issue spark-260, originally reported by andyk"], "derived": {"summary": "Updating lots of docs to use the new special version number variables, and adding the version to the navbar so it is easy to tell which version of Spark these docs were compiled for. See what the generated docs look like after this commit at: http://www.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Updates docs to use the new version num vars and adds Spark version in nav bar - Updating lots of docs to use the new special version number variables, and adding the version to the navbar so it is easy to tell which version of Spark these docs were compiled for. See what the generated docs look like after this commit at: http://www."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-260, originally reported by andyk"}]}}
{"project": "SPARK", "issue_id": "SPARK-306", "title": "Adding documentation to public API's.", "status": "Resolved", "priority": null, "reporter": "Patrick Wendell", "assignee": null, "labels": [], "created": "0012-10-08T21:27:00.000+0000", "updated": "2012-10-19T22:50:31.000+0000", "description": null, "comments": ["Imported from Github issue spark-261, originally reported by pwendell"], "derived": {"summary": "", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Adding documentation to public API's."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-261, originally reported by pwendell"}]}}
{"project": "SPARK", "issue_id": "SPARK-305", "title": "Document RDD api (i.e. RDD.scala)", "status": "Resolved", "priority": null, "reporter": "Andy Konwinski", "assignee": null, "labels": [], "created": "0012-10-08T21:52:00.000+0000", "updated": "2012-10-19T22:50:22.000+0000", "description": "Adds scaladoc public API in RDD.scala", "comments": ["Github comment from mateiz: Thanks, looks good.", "Imported from Github issue spark-262, originally reported by andyk"], "derived": {"summary": "Adds scaladoc public API in RDD. scala.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Document RDD api (i.e. RDD.scala) - Adds scaladoc public API in RDD. scala."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-262, originally reported by andyk"}]}}
{"project": "SPARK", "issue_id": "SPARK-304", "title": "Add m1.medium node option to cluster management script.", "status": "Resolved", "priority": null, "reporter": "RayRacine", "assignee": null, "labels": [], "created": "0012-10-09T07:08:00.000+0000", "updated": "2012-10-19T22:50:23.000+0000", "description": "EC2 m1.medium node option is missing in the spark_ec2.py script.", "comments": ["Github comment from mateiz: Awesome, thanks.", "Imported from Github issue spark-263, originally reported by RayRacine"], "derived": {"summary": "EC2 m1. medium node option is missing in the spark_ec2.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add m1.medium node option to cluster management script. - EC2 m1. medium node option is missing in the spark_ec2."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-263, originally reported by RayRacine"}]}}
{"project": "SPARK", "issue_id": "SPARK-303", "title": "Support for Hadoop 2 distributions such as cdh4", "status": "Resolved", "priority": null, "reporter": "Thomas Dudziak", "assignee": null, "labels": [], "created": "0012-10-09T14:29:00.000+0000", "updated": "2012-10-19T22:50:23.000+0000", "description": "Hadoop 2 unfortunately introduced backwards-incompatible changes in classes used by Spark, `JobContext` and `TaskAttemptContext` in both the `mapred` and `mapreduce` packages. In the new codebase, these classes are now interfaces and thus cannot be instantiated directly anymore. Instead, you are now required to use the `JobContextImpl` and `TaskAttemptContextImpl` classes, respectively.\n\nThis pull requests extracts the code in Spark where it uses these, into separate traits that are located in Hadoop major version specific source folders that the sbt build then includes/excludes as necessary.\n\nIn addition, for Hadoop 2, Spark now also needs to depend on the `hadoop-client` artifact.", "comments": ["Github comment from mateiz: Thanks Tom, but we actually already have these fixes in the `yarn` branch. I don't think we want to merge them into master because that still relies on Hadoop 0.20.205. You can't use a Hadoop 2 client to connect to an older cluster, right?\n\nI think we'll just recommend using the `yarn` branch to people who use Hadoop 2, and we'll merge them when more people have moved to Hadoop 2.", "Github comment from tomdz: I don't know much about the yarn branch, but the name implies that it requires yarn, which we're not using with our cdh4 installation (we only use MRv1). The changes in this pull request seem to be sufficient to run current spark against the hdfs in our installation.", "Github comment from mateiz: My question is just, do these changes work when connecting to HDFS clusters on Hadoop 1.0 or 0.20.x? Since there's an API change in Hadoop, it seems that they wouldn't work.\n\nWe should probably just rename the \"yarn\" branch to \"hadoop2\". You're right that the name of that branch makes it sound like it requires YARN, but actually it runs with Mesos, the standalone deploy mode, etc as well.", "Github comment from tomdz: The change are supposed to make Spark work with the Hadoop/hdfs version that it is compiled against. I.e. if I use `2.0.0-mr1-cdh4.1.0` as the Hadoop version (and set the `HADOOP_MAJOR_VERSION` to 2 since cdh4 is a Hadoop 2 distribution), then the resulting Spark artifact will work with cdh4 (and presumably everything that is API compatible to that Hadoop version). The mechanism would allow to handle other Hadoop backwards-incompatible changes as well.\n\nOn a related note, we have a Maven POM that defines profiles for each of the popular Hadoop distributions (Hadoop 1 & 2) and generates Spark jars with corresponding classifiers. The benefit of that is that in my project I can depend on the right version of Spark (using a classifier in the dependency). If there is interest in that, I could create a pull request for it.", "Github comment from mateiz: Thanks, I didn't see that you're building differently based on the major version. I'll merge this in but probably after the release of 0.6.0 (in the next minor release after that). Maybe we can use the same mechanism to bring the YARN stuff into the main branch and only compile it conditionally.", "Github comment from tomdz: Regenerated the pull request against 0.6 dev branch in #285.", "Imported from Github issue spark-264, originally reported by tomdz"], "derived": {"summary": "Hadoop 2 unfortunately introduced backwards-incompatible changes in classes used by Spark, `JobContext` and `TaskAttemptContext` in both the `mapred` and `mapreduce` packages. In the new codebase, these classes are now interfaces and thus cannot be instantiated directly anymore.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support for Hadoop 2 distributions such as cdh4 - Hadoop 2 unfortunately introduced backwards-incompatible changes in classes used by Spark, `JobContext` and `TaskAttemptContext` in both the `mapred` and `mapreduce` packages. In the new codebase, these classes are now interfaces and thus cannot be instantiated directly anymore."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-264, originally reported by tomdz"}]}}
{"project": "SPARK", "issue_id": "SPARK-302", "title": "Making Spark version configurable in docs and updating Bagel doc", "status": "Resolved", "priority": null, "reporter": "Patrick Wendell", "assignee": null, "labels": [], "created": "0012-10-09T21:41:00.000+0000", "updated": "2012-10-19T22:50:29.000+0000", "description": null, "comments": ["Github comment from mateiz: Thanks Patrick!", "Imported from Github issue spark-265, originally reported by pwendell"], "derived": {"summary": "", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Making Spark version configurable in docs and updating Bagel doc"}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-265, originally reported by pwendell"}]}}
{"project": "SPARK", "issue_id": "SPARK-527", "title": "Support spark-shell when running on YARN", "status": "Resolved", "priority": null, "reporter": "Matei Alexandru Zaharia", "assignee": "Raymond Valencia", "labels": [], "created": "0012-10-10T10:35:00.000+0000", "updated": "2014-01-23T23:29:37.000+0000", "description": "Right now the YARN mode only allows standalone jobs.", "comments": ["Imported from Github issue spark-266, originally reported by mateiz", "Hi \n\nI have send a pull request on this one at : https://github.com/mesos/spark/pull/868 , please help to take a review\n", "Support for spark-shell under YARN was added in 0.8.1 using yarn-client mode: https://github.com/apache/incubator-spark/pull/101"], "derived": {"summary": "Right now the YARN mode only allows standalone jobs.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support spark-shell when running on YARN - Right now the YARN mode only allows standalone jobs."}, {"q": "What updates or decisions were made in the discussion?", "a": "Support for spark-shell under YARN was added in 0.8.1 using yarn-client mode: https://github.com/apache/incubator-spark/pull/101"}]}}
{"project": "SPARK", "issue_id": "SPARK-301", "title": "Fixed bug when fetching Jar dependencies.", "status": "Resolved", "priority": null, "reporter": "Denny Britz", "assignee": null, "labels": [], "created": "0012-10-10T15:11:00.000+0000", "updated": "2012-10-19T22:50:22.000+0000", "description": "Instead of checking currentFiles check currentJars. Came across this when testing some Shark queries.", "comments": ["Github comment from mateiz: Good catch; I probably miscopied this when I moved this code around to fetch JARs before deserializing the task.", "Imported from Github issue spark-267, originally reported by dennybritz"], "derived": {"summary": "Instead of checking currentFiles check currentJars. Came across this when testing some Shark queries.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Fixed bug when fetching Jar dependencies. - Instead of checking currentFiles check currentJars. Came across this when testing some Shark queries."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-267, originally reported by dennybritz"}]}}
{"project": "SPARK", "issue_id": "SPARK-300", "title": "Adding code for publishing to Sonatype.", "status": "Resolved", "priority": null, "reporter": "Patrick Wendell", "assignee": null, "labels": [], "created": "0012-10-10T16:27:00.000+0000", "updated": "2012-10-19T22:50:31.000+0000", "description": "By default - I'm leaving this commented out. This is because\nthere is a bug in the PGP signing plugin which causes it to active\neven duing a publish-local. So we'll just uncomment when we decide\nto publish.", "comments": ["Github comment from mateiz: Okay, thanks.", "Github comment from mateiz: By the way, the same issue seems to affect publish-local on branch 0.5. I'll fix it there too.", "Imported from Github issue spark-268, originally reported by pwendell"], "derived": {"summary": "By default - I'm leaving this commented out. This is because\nthere is a bug in the PGP signing plugin which causes it to active\neven duing a publish-local.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Adding code for publishing to Sonatype. - By default - I'm leaving this commented out. This is because\nthere is a bug in the PGP signing plugin which causes it to active\neven duing a publish-local."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-268, originally reported by pwendell"}]}}
{"project": "SPARK", "issue_id": "SPARK-299", "title": "Null pointer exception when RDD size is larger than cache size", "status": "Resolved", "priority": null, "reporter": "Shivaram Venkataraman", "assignee": null, "labels": [], "created": "0012-10-10T20:05:00.000+0000", "updated": "2012-10-19T22:50:27.000+0000", "description": "I got the null pointer exception today when I marked an rdd to be cached but the RDD size ended up being larger than memory available.  What I think is happening is that the CacheTracker calls blockManager.put and expects the put to succeed. But if a block is marked as memory only and if we don't have enough space, the block manager will compute the split but fail to put it in the cache.\n\nI think the right thing to do is to stream through the computation like we would do a non-cached RDD, but I am not exactly sure what the right code fix is.\n\n---------------------------------------\n12/10/10 23:57:33 INFO storage.MemoryStore: ensureFreeSpace(419262667)\ncalled with curMem=7152168890, maxMem=7470573158\n12/10/10 23:57:33 INFO storage.MemoryStore: Will not store rdd_2_230\nas it would require dropping another block from the same RDD\n12/10/10 23:57:33 INFO storage.BlockManager: Dropping block rdd_2_230\nfrom memory\n12/10/10 23:57:33 WARN storage.MemoryStore: Block rdd_2_230 could not\nbe removed as it does not exist\n12/10/10 23:57:33 WARN spark.CacheTracker: loading partition failed\nafter computing it rdd_2_230\n12/10/10 23:57:33 ERROR executor.Executor: Exception in task ID 233\njava.lang.NullPointerException\n        at spark.scheduler.OutputTask.run(OutputTask.scala:29)\n        at spark.executor.Executor$TaskRunner.run(Executor.scala:99)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:166)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\n        at java.lang.Thread.run(Thread.java:679)", "comments": ["Github comment from mateiz: I think we should not be calling BlockManager.get / put there. The reason it happened is probably because we didn't want to create an ArrayBuffer and pass that to the BlockManager (which would copy it), but the current way is silly; for example it will serialize and then deserialize the data if you're using serialized caching. We should probably just add a way for the block manager to take an ArrayBuffer and put it without cloning, or just give it one that it will copy. I'm not too worried about supporting cases where a single partition is bigger than the RAM right now so I think it's fine to materialize an ArrayBuffer even if we don't end up keeping it in the cache (e.g. due to the same-RDD replacement rule).", "Github comment from mateiz: Anyway let me know if you want to fix it, or I should. We should get it fixed before releasing 0.6.", "Github comment from shivaram: I agree with your take that the block manager should be able to take an array buffer rather than an iterator. And yes, this is not a case where a single partition is larger than memory available, but a case where I had 235 partitions and it happened that I didn't have memory for the last 4 or 5 partitions.\n\nI'll take a shot at fixing this tonight, but if I either don't get too far or run out of time, I'll let you by tomorrow for you to go ahead with a fix.", "Imported from Github issue spark-269, originally reported by shivaram"], "derived": {"summary": "I got the null pointer exception today when I marked an rdd to be cached but the RDD size ended up being larger than memory available. What I think is happening is that the CacheTracker calls blockManager.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Null pointer exception when RDD size is larger than cache size - I got the null pointer exception today when I marked an rdd to be cached but the RDD size ended up being larger than memory available. What I think is happening is that the CacheTracker calls blockManager."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-269, originally reported by shivaram"}]}}
{"project": "SPARK", "issue_id": "SPARK-298", "title": "Adding Java documentation", "status": "Resolved", "priority": null, "reporter": "Patrick Wendell", "assignee": null, "labels": [], "created": "0012-10-10T23:50:00.000+0000", "updated": "2012-10-19T22:50:37.000+0000", "description": null, "comments": ["Github comment from mateiz: Awesome, thanks a lot for doing this.", "Imported from Github issue spark-270, originally reported by pwendell"], "derived": {"summary": "", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Adding Java documentation"}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-270, originally reported by pwendell"}]}}
{"project": "SPARK", "issue_id": "SPARK-297", "title": "Change block manager to accept a ArrayBuffer", "status": "Resolved", "priority": null, "reporter": "Shivaram Venkataraman", "assignee": null, "labels": [], "created": "0012-10-10T23:56:00.000+0000", "updated": "2012-10-19T22:50:29.000+0000", "description": "Attempt to fix #269. Existing tests pass, but I haven't added a new test to make sure things are fixed. This is partly because I was not sure if I should extend CacheTrackerSuite or add something to the Block Manager test cases.", "comments": ["Github comment from mateiz: This looks good to me. I would add a test in DistributedSuite actually, to just launch a cluster where the memory limit per worker is low (e.g. set spark.storage.memoryFraction = 0.01) and try caching and using some data on it.", "Github comment from shivaram: Added the test with the assumption that 512MB of memory times 0.0001 is around 50KB and the array should be larger than that.", "Github comment from mateiz: Thanks. It's a good bet because we do set the JVMs to exactly 512 MB of memory. I might also add a test where half the data fits and half doesn't.", "Imported from Github issue spark-271, originally reported by shivaram"], "derived": {"summary": "Attempt to fix #269. Existing tests pass, but I haven't added a new test to make sure things are fixed.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Change block manager to accept a ArrayBuffer - Attempt to fix #269. Existing tests pass, but I haven't added a new test to make sure things are fixed."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-271, originally reported by shivaram"}]}}
{"project": "SPARK", "issue_id": "SPARK-526", "title": "sbt/sbt  --> eclipse won't work now", "status": "Resolved", "priority": null, "reporter": "tewr05", "assignee": null, "labels": [], "created": "0012-10-11T22:14:00.000+0000", "updated": "2013-01-20T12:33:47.000+0000", "description": "after a fresh clone of the repository, if you do sbt/sbt and\n> eclipse\n[error] Not a valid command: eclipse (similar: help, alias)\n...\n...", "comments": ["Github comment from mateiz: Interesting; see if you can find a version of the SBT Eclipse plugin for sbt 0.11.3 -- we recently updated our version of sbt. Or maybe the name of the command changed (e.g. it might be gen-eclipse or something).\n\nMatei\n\nOn Oct 11, 2012, at 11:14 PM, George Li wrote:\n\n> after a fresh clone of the repository, if you do sbt/sbt and\n> \n> eclipse\n> [error] Not a valid command: eclipse (similar: help, alias)\n> ...\n> ...\n> \n> \n> Reply to this email directly or view it on GitHub.\n> \n>", "Imported from Github issue spark-272, originally reported by tewr05", "\"sbt/sbt eclipse\" works for me on both the master branch and 0.6.1, so I'm marking this as resolved."], "derived": {"summary": "after a fresh clone of the repository, if you do sbt/sbt and\n> eclipse\n[error] Not a valid command: eclipse (similar: help, alias).", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "sbt/sbt  --> eclipse won't work now - after a fresh clone of the repository, if you do sbt/sbt and\n> eclipse\n[error] Not a valid command: eclipse (similar: help, alias)."}, {"q": "What updates or decisions were made in the discussion?", "a": "\"sbt/sbt eclipse\" works for me on both the master branch and 0.6.1, so I'm marking this as resolved."}]}}
{"project": "SPARK", "issue_id": "SPARK-296", "title": "Let the user specify environment variables to be passed to the Executors", "status": "Resolved", "priority": null, "reporter": "Denny Britz", "assignee": null, "labels": [], "created": "0012-10-13T13:02:00.000+0000", "updated": "2012-10-19T22:50:24.000+0000", "description": null, "comments": ["Github comment from mateiz: Hey Denny,\n\nAre you sure this will work for the standalone and Mesos coarse-grain modes? In those cases, it the Scheduler will launch executors *before* you get the chance to call putExecutorEnv on the SparkContext, so they won't see the new variables.\n\nI suggest that, instead, you make SparkContext's constructor take an `env` map as a fifth argument (and add a separate 4-argument constructor). That will also make it clear that you can't add or change the env after you start a job.\n\nMatei", "Github comment from mateiz: By the way if you do this, change JavaSparkContext in the same way.", "Github comment from dennybritz: Ah, you're right. I won't be back before tonight though so if you want to release today please feel free to make the changes yourself. If not I will change it tomorrow morning.", "Github comment from mateiz: Alright, I'll merge it and fix this myself; thanks.", "Imported from Github issue spark-273, originally reported by dennybritz"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Let the user specify environment variables to be passed to the Executors"}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-273, originally reported by dennybritz"}]}}
{"project": "SPARK", "issue_id": "SPARK-295", "title": "Access denied for LATEST_AMI_URL on 0.6 EC2 scripts", "status": "Resolved", "priority": null, "reporter": "Josh Rosen", "assignee": null, "labels": [], "created": "0012-10-13T14:26:00.000+0000", "updated": "2012-10-19T22:50:21.000+0000", "description": "When running the Spark 0.6 EC2 script, I received an  \"Access Denied\" message when trying to fetch LATEST_AMI_URL =  https://s3.amazonaws.com/mesos-images/ids/latest-spark-0.6.\n\nThe old URL, https://s3.amazonaws.com/mesos-images/ids/latest-spark-0.5 works fine, so this is probably a permissions issue.", "comments": ["Github comment from mateiz: Yup, it's because there's no AMI for 0.6 yet. You should use the 0.5 one manually for now, or use the 0.5 deploy script.", "Imported from Github issue spark-274, originally reported by JoshRosen"], "derived": {"summary": "When running the Spark 0. 6 EC2 script, I received an  \"Access Denied\" message when trying to fetch LATEST_AMI_URL =  https://s3.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Access denied for LATEST_AMI_URL on 0.6 EC2 scripts - When running the Spark 0. 6 EC2 script, I received an  \"Access Denied\" message when trying to fetch LATEST_AMI_URL =  https://s3."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-274, originally reported by JoshRosen"}]}}
{"project": "SPARK", "issue_id": "SPARK-525", "title": "Refactoring of shuffling-related classes", "status": "Closed", "priority": null, "reporter": "Josh Rosen", "assignee": null, "labels": [], "created": "0012-10-13T16:14:00.000+0000", "updated": "2012-10-23T23:58:58.000+0000", "description": "These commits refactor interfaces and classes related to shuffling.\n\nThe main change is to remove map-side combining and aggregation from the shuffling interfaces.  Map-side combining can be performed by calling `mapPartitions()` on an RDD, then passing that transformed RDD to `ShuffledRDD`.  The reduce-side combining of the combiners can be performed in a second `mapPartitions()` call.\n\nIn this branch,\n\n- `ShuffleMapTask` only performs shuffling, not map-side combining.\n- All of the specialized `ShuffledRDD` subclasses were removed and replaced with a single `ShuffledRDD` class, which is only responsible for fetching a shuffled RDD.\n- `ShuffleFetcher` returns an iterator over its fetched partitions, rather than accepting a function to apply to those partitions.\n- The map-side combiner tests were removed, since map-side combining no longer uses a special mechanism.\n- A `preservesPartitioning` option was added to `mapPartitions()` to allow users to specify that a transformation preserves the RDD's partitioning; this is used when implementing map-side combiners and reduce-side aggregation.\n- RDD methods were modified to use these new interfaces.  For some methods, like `sort()`, this had the effect of grouping the method's code into a single file rather than splitting it between the method and a special RDD subclass.\n\nThese changes haven't had a noticeable performance impact in the tests that I've run, but I'm still in the process of gathering benchmarks.  I'd appreciate any feedback on these changes, particularly the changes to `cogroup()` and `CoGroupedRDD`.", "comments": ["Imported from Github issue spark-275, originally reported by JoshRosen", "Closing since this pull request was merged."], "derived": {"summary": "These commits refactor interfaces and classes related to shuffling. The main change is to remove map-side combining and aggregation from the shuffling interfaces.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Refactoring of shuffling-related classes - These commits refactor interfaces and classes related to shuffling. The main change is to remove map-side combining and aggregation from the shuffling interfaces."}, {"q": "What updates or decisions were made in the discussion?", "a": "Closing since this pull request was merged."}]}}
{"project": "SPARK", "issue_id": "SPARK-294", "title": "Disable gpg by default.", "status": "Resolved", "priority": null, "reporter": "Reynold Xin", "assignee": null, "labels": [], "created": "0012-10-13T22:10:00.000+0000", "updated": "2012-10-19T22:50:26.000+0000", "description": null, "comments": ["Github comment from mateiz: I've already done this separately, so closing the pull request, but thanks!.", "Imported from Github issue spark-276, originally reported by rxin"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Disable gpg by default."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-276, originally reported by rxin"}]}}
{"project": "SPARK", "issue_id": "SPARK-293", "title": "SparkContext.newShuffleId should be public or ShuffleDependency should set its own id", "status": "Resolved", "priority": null, "reporter": "Josh Rosen", "assignee": null, "labels": [], "created": "0012-10-14T00:45:00.000+0000", "updated": "2012-10-19T22:50:34.000+0000", "description": "The `SparkContext.newShuffleId` method should probably be public, because `ShuffleDependency` is public and its constructor requires a valid `shuffleId`:\n\n```scala\nclass ShuffleDependency[K, V, C](\n    val shuffleId: Int,\n    @transient rdd: RDD[(K, V)],\n    val aggregator: Option[Aggregator[K, V, C]],\n    val partitioner: Partitioner)\n  extends Dependency(rdd)\n```\n\n`SparkContext.newShuffleId` is currently `private[spark]`, which prevents me from defining custom RDD subclasses in Shark that create their own `ShuffleDependency` instances.\n\nIn the current dev branch, `ShuffleDependency` is always supplied with `SparkContext.newShuffleId`.  If it's not important to be able to set a `ShuffleDependency`'s id to an arbitrary value, then perhaps the `shuffleId` field could be moved from its constructor into the class body, where it could be set to `rdd.context.newShuffleId`.", "comments": ["Github comment from mateiz: Good catch; I've fixed this now, by making the shuffle ID not be a constructor argument.", "Imported from Github issue spark-277, originally reported by JoshRosen"], "derived": {"summary": "The `SparkContext. newShuffleId` method should probably be public, because `ShuffleDependency` is public and its constructor requires a valid `shuffleId`:\n\n```scala\nclass ShuffleDependency[K, V, C](\n    val shuffleId: Int,\n    @transient rdd: RDD[(K, V)],\n    val aggregator: Option[Aggregator[K, V, C]],\n    val partitioner: Partitioner)\n  extends Dependency(rdd)\n```\n\n`SparkContext.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "SparkContext.newShuffleId should be public or ShuffleDependency should set its own id - The `SparkContext. newShuffleId` method should probably be public, because `ShuffleDependency` is public and its constructor requires a valid `shuffleId`:\n\n```scala\nclass ShuffleDependency[K, V, C](\n    val shuffleId: Int,\n    @transient rdd: RDD[(K, V)],\n    val aggregator: Option[Aggregator[K, V, C]],\n    val partitioner: Partitioner)\n  extends Dependency(rdd)\n```\n\n`SparkContext."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-277, originally reported by JoshRosen"}]}}
{"project": "SPARK", "issue_id": "SPARK-292", "title": "Adding dependency repos in quickstart example", "status": "Resolved", "priority": null, "reporter": "Patrick Wendell", "assignee": null, "labels": [], "created": "0012-10-14T10:50:00.000+0000", "updated": "2012-10-19T22:50:30.000+0000", "description": null, "comments": ["Imported from Github issue spark-278, originally reported by pwendell"], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Adding dependency repos in quickstart example"}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-278, originally reported by pwendell"}]}}
{"project": "SPARK", "issue_id": "SPARK-291", "title": "Removing credentials line in build.", "status": "Resolved", "priority": null, "reporter": "Patrick Wendell", "assignee": null, "labels": [], "created": "0012-10-14T18:35:00.000+0000", "updated": "2012-10-19T22:50:32.000+0000", "description": "This line isn't needed - in fact it messes with the right way of resolving credentials automatically.", "comments": ["Imported from Github issue spark-279, originally reported by pwendell"], "derived": {"summary": "This line isn't needed - in fact it messes with the right way of resolving credentials automatically.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Removing credentials line in build. - This line isn't needed - in fact it messes with the right way of resolving credentials automatically."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-279, originally reported by pwendell"}]}}
{"project": "SPARK", "issue_id": "SPARK-524", "title": "spark integration issue with Cloudera hadoop", "status": "Resolved", "priority": null, "reporter": "openreserach", "assignee": null, "labels": [], "created": "0012-10-15T19:17:00.000+0000", "updated": "2014-09-02T13:02:07.000+0000", "description": "Hi, \n\n1. I am using single EC2 instance with pre-built mesos (ami-0fcb7966) (Same issue if I build mesos from source code in locall VM)\n\n2. Follow instruction on https://github.com/mesos/spark/wiki/Running-spark-on-mesos with some tweaks.\n\n3. I install Cloudera cdhu5 by yum (not using pre-built hadoop due to lack of document)\n\n4. ./spartk-shell.sh\nimport spark._\nval sc = new SparkContext(\"localhost:5050\",\"passwd\")\nval ec2 = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\")\n\nIF I keep val HADOOP_VERSION = \"0.20.205.0\" in project/SparkBuild.scala\nat val file = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\")\nI am getting error\nProtocol org.apache.hadoop.hdfs.protocol.ClientProtocol version mismatch. (client = 61, server = 63)\n\nIF I set val HADOOP_VERSION = \"0.20.2-cdh3u5\" or val HADOOP_VERSION = \"0.20.2-cdh3u3\" \nI am getting error at  ec2.count()\nERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting job\nlike the one reported at http://mail-archives.apache.org/mod_mbox/incubator-mesos-dev/201108.mbox/%3CBD25AE7A-C9DC-4020-AD40-41C66DCAA56E@eecs.berkeley.edu%3E\n\nPlease let me know if you cannot replicate this error, and give more instruction on how Spark integrate with Cloudera Hadoop \n\nThanks\n\n-QH", "comments": ["Github comment from mateiz: Did you copy the code you built to the worker nodes too?\n\nIf so, look in /mnt/mesos-work on the worker to find the stdout and stderr output for your job, and let me know what's in there. But generally \"lost TID\" means that the JVM crashed, which likely means that it didn't have the right code or otherwise couldn't start Spark.\n\nMatei\n\nOn Oct 15, 2012, at 8:17 PM, Qiming He wrote:\n\n> Hi,\n> \n> I am using single EC2 instance with pre-built mesos (ami-0fcb7966) (Same issue if I build mesos from source code in locall VM)\n> \n> Follow instruction on https://github.com/mesos/spark/wiki/Running-spark-on-mesos with some tweaks.\n> \n> I install Cloudera cdhu5 by yum (not using pre-built hadoop due to lack of document)\n> \n> ./spartk-shell.sh\n> import spark._\n> val sc = new SparkContext(\"localhost:5050\",\"passwd\")\n> val ec2 = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\")\n> \n> IF I keep val HADOOP_VERSION = \"0.20.205.0\" in project/SparkBuild.scala\n> at val file = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\")\n> I am getting error\n> Protocol org.apache.hadoop.hdfs.protocol.ClientProtocol version mismatch. (client = 61, server = 63)\n> \n> IF I set val HADOOP_VERSION = \"0.20.2-cdh3u5\" or val HADOOP_VERSION = \"0.20.2-cdh3u3\" \n> I am getting error at ec2.count()\n> ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting job\n> like the one reported at http://mail-archives.apache.org/mod_mbox/incubator-mesos-dev/201108.mbox/%3CBD25AE7A-C9DC-4020-AD40-41C66DCAA56E@eecs.berkeley.edu%3E\n> \n> Please let me know if you cannot replicate this error, and give more instruction on how Spark integrate with Cloudera Hadoop\n> \n> Thanks\n> \n> -QH\n> \n> \n> Reply to this email directly or view it on GitHub.\n> \n>", "Github comment from openreserach: As stated in the issue description, it is a dummy *single* instance\ncluster, i.e., master and slave are co-hosted in one box. Where should I\ncopy code to?  -QH\n\nOn Mon, Oct 15, 2012 at 11:30 PM, Matei Zaharia <notifications@github.com>wrote:\n\n> Did you copy the code you built to the worker nodes too?\n>\n> If so, look in /mnt/mesos-work on the worker to find the stdout and stderr\n> output for your job, and let me know what's in there. But generally \"lost\n> TID\" means that the JVM crashed, which likely means that it didn't have the\n> right code or otherwise couldn't start Spark.\n>\n> Matei\n>\n> On Oct 15, 2012, at 8:17 PM, Qiming He wrote:\n>\n> > Hi,\n> >\n> > I am using single EC2 instance with pre-built mesos (ami-0fcb7966) (Same\n> issue if I build mesos from source code in locall VM)\n> >\n> > Follow instruction on\n> https://github.com/mesos/spark/wiki/Running-spark-on-mesos with some\n> tweaks.\n> >\n> > I install Cloudera cdhu5 by yum (not using pre-built hadoop due to lack\n> of document)\n> >\n> > ./spartk-shell.sh\n> > import spark._\n> > val sc = new SparkContext(\"localhost:5050\",\"passwd\")\n> > val ec2 = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\")\n> >\n> > IF I keep val HADOOP_VERSION = \"0.20.205.0\" in project/SparkBuild.scala\n> > at val file = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\")\n> > I am getting error\n> > Protocol org.apache.hadoop.hdfs.protocol.ClientProtocol version\n> mismatch. (client = 61, server = 63)\n> >\n> > IF I set val HADOOP_VERSION = \"0.20.2-cdh3u5\" or val HADOOP_VERSION =\n> \"0.20.2-cdh3u3\"\n> > I am getting error at ec2.count()\n> > ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting job\n> > like the one reported at\n> http://mail-archives.apache.org/mod_mbox/incubator-mesos-dev/201108.mbox/%3CBD25AE7A-C9DC-4020-AD40-41C66DCAA56E@eecs.berkeley.edu%3E\n> >\n> > Please let me know if you cannot replicate this error, and give more\n> instruction on how Spark integrate with Cloudera Hadoop\n> >\n> > Thanks\n> >\n> > -QH\n> >\n> > \n> > Reply to this email directly or view it on GitHub.\n> >\n> >\n>\n> \n> Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/issues/280#issuecomment-9472032>.\n>\n>\n\n\n\n-- \nDr. Qiming He\nQiming.He@openresearchinc.com\n301-525-6612 (Phone)\n815-327-2122 (Fax)", "Github comment from mateiz: Ah, got it. So how exactly did you start the cluster -- did you not use our EC2 script? In that case you probably need to configure some settings in conf/spark-env.sh, such as SCALA_HOME and MESOS_NATIVE_LIBRARY. Try starting a cluster with our script (https://github.com/mesos/spark/wiki/EC2-Scripts) to have them set up for you.\n\nAgain, to figure out the problem, you'd need to look at the logs of the worker process. Mesos places them in a \"work\" directory for the job. This is configured to be /mnt/mesos-work when you launch Mesos with our EC2 scripts, but if you launched it manually then it likely be in /tmp.\n\nMatei\n\nOn Oct 15, 2012, at 8:40 PM, Qiming He wrote:\n\n> As stated in the issue description, it is a dummy *single* instance \n> cluster, i.e., master and slave are co-hosted in one box. Where should I \n> copy code to? -QH \n> \n> On Mon, Oct 15, 2012 at 11:30 PM, Matei Zaharia <notifications@github.com>wrote: \n> \n> > Did you copy the code you built to the worker nodes too? \n> > \n> > If so, look in /mnt/mesos-work on the worker to find the stdout and stderr \n> > output for your job, and let me know what's in there. But generally \"lost \n> > TID\" means that the JVM crashed, which likely means that it didn't have the \n> > right code or otherwise couldn't start Spark. \n> > \n> > Matei \n> > \n> > On Oct 15, 2012, at 8:17 PM, Qiming He wrote: \n> > \n> > > Hi, \n> > > \n> > > I am using single EC2 instance with pre-built mesos (ami-0fcb7966) (Same \n> > issue if I build mesos from source code in locall VM) \n> > > \n> > > Follow instruction on \n> > https://github.com/mesos/spark/wiki/Running-spark-on-mesos with some \n> > tweaks. \n> > > \n> > > I install Cloudera cdhu5 by yum (not using pre-built hadoop due to lack \n> > of document) \n> > > \n> > > ./spartk-shell.sh \n> > > import spark._ \n> > > val sc = new SparkContext(\"localhost:5050\",\"passwd\") \n> > > val ec2 = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") \n> > > \n> > > IF I keep val HADOOP_VERSION = \"0.20.205.0\" in project/SparkBuild.scala \n> > > at val file = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") \n> > > I am getting error \n> > > Protocol org.apache.hadoop.hdfs.protocol.ClientProtocol version \n> > mismatch. (client = 61, server = 63) \n> > > \n> > > IF I set val HADOOP_VERSION = \"0.20.2-cdh3u5\" or val HADOOP_VERSION = \n> > \"0.20.2-cdh3u3\" \n> > > I am getting error at ec2.count() \n> > > ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting job \n> > > like the one reported at \n> > http://mail-archives.apache.org/mod_mbox/incubator-mesos-dev/201108.mbox/%3CBD25AE7A-C9DC-4020-AD40-41C66DCAA56E@eecs.berkeley.edu%3E \n> > > \n> > > Please let me know if you cannot replicate this error, and give more \n> > instruction on how Spark integrate with Cloudera Hadoop \n> > > \n> > > Thanks \n> > > \n> > > -QH \n> > > \n> > >  \n> > > Reply to this email directly or view it on GitHub. \n> > > \n> > > \n> > \n> >  \n> > Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/issues/280#issuecomment-9472032>. \n> > \n> > \n> \n> \n> \n> -- \n> Dr. Qiming He \n> Qiming.He@openresearchinc.com \n> 301-525-6612 (Phone) \n> 815-327-2122 (Fax)\n> \n> Reply to this email directly or view it on GitHub.\n> \n>", "Github comment from openreserach: Hi,\n\n1st-ly, I just use EC2 ami instance, I did not try EC2 script yet. So it is\njust dummy single instance, running in AWS (should be the same as local)\n\n2nd-ly, I cannot find /mnt/mesos-work. I am assuming it is\n/tmp/mesos-slave.INFO. I try Apache hadoop 1.0.4, setting val\nHADOOP_VERSION = \"1.0.4\" in project/SparkBuild.scala\n\nNo client mismatch problem, but still get error:\nERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting job\n\nHere is tail of  /tmp/mesos-slave.INFO\nI1017 04:15:27.429769  1421 slave.cpp:457] Got assigned task 0 for\nframework 201210170241-1056562698-5050-1405-0\n         005\nI1017 04:15:27.429924  1421 slave.cpp:1559] Generating a unique work\ndirectory for executor 'default' of framewo\n       rk 201210170241-1056562698-5050-1405-0005\nI1017 04:15:27.461669  1421 slave.cpp:522] Using\n'/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor\n\n ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/0' as\nwork directory for executor 'default' of\n      framework 201210170241-1056562698-5050-1405-0005\nI1017 04:15:27.463573  1421 process_based_isolation_module.cpp:93]\nLaunching default (/root/spark/spark-executor\n         ) in\n/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe\n                                       cutors/default/runs/0 with resources\nmem=512' for framework 201210170241-1056562698-5050-1405-0005\nI1017 04:15:27.469944  1421 process_based_isolation_module.cpp:129] Forked\nexecutor at 21143\nI1017 04:15:28.339529  1421 process_based_isolation_module.cpp:255] Telling\nslave of lost executor default of fr\n amework 201210170241-1056562698-5050-1405-0005\nI1017 04:15:28.339673  1421 process_utils.hpp:64] Stopping ... 21143\nSent signal to 21143\nI1017 04:15:28.425079  1421 slave.cpp:1383] Executor 'default' of framework\n201210170241-1056562698-5050-1405-00\n 05 has exited with status 127\nI1017 04:15:28.427103  1421 slave.cpp:989] Status update: task 0 of\nframework 201210170241-1056562698-5050-1405-\n         0005 is now in state TASK_LOST\nI1017 04:15:28.430907  1421 slave.cpp:1507] Scheduling executor directory\n/tmp/mesos/slaves/201210170241-1056562\n\n 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/0\nfor deletion\nI1017 04:15:28.434944  1421 slave.cpp:699] Got acknowledgement of status\nupdate for task 0 of framework 20121017\n   0241-1056562698-5050-1405-0005\nI1017 04:15:28.442648  1421 slave.cpp:457] Got assigned task 1 for\nframework 201210170241-1056562698-5050-1405-0\n         005\nI1017 04:15:28.442703  1421 slave.cpp:1559] Generating a unique work\ndirectory for executor 'default' of framewo\n       rk 201210170241-1056562698-5050-1405-0005\nI1017 04:15:28.444704  1421 slave.cpp:522] Using\n'/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor\n\n ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/1' as\nwork directory for executor 'default' of\n      framework 201210170241-1056562698-5050-1405-0005\nI1017 04:15:28.447643  1421 process_based_isolation_module.cpp:93]\nLaunching default (/root/spark/spark-executor\n         ) in\n/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe\n                                       cutors/default/runs/1 with resources\nmem=512' for framework 201210170241-1056562698-5050-1405-0005\nI1017 04:15:28.449476  1421 process_based_isolation_module.cpp:129] Forked\nexecutor at 21190\nI1017 04:15:29.340275  1421 process_based_isolation_module.cpp:255] Telling\nslave of lost executor default of fr\n amework 201210170241-1056562698-5050-1405-0005\nI1017 04:15:29.340428  1421 process_utils.hpp:64] Stopping ... 21190\nSent signal to 21190\nI1017 04:15:29.432467  1421 slave.cpp:1383] Executor 'default' of framework\n201210170241-1056562698-5050-1405-00\n 05 has exited with status 127\nI1017 04:15:29.434921  1421 slave.cpp:989] Status update: task 1 of\nframework 201210170241-1056562698-5050-1405-\n         0005 is now in state TASK_LOST\nI1017 04:15:29.501458  1421 slave.cpp:1507] Scheduling executor directory\n/tmp/mesos/slaves/201210170241-1056562\n\n 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/1\nfor deletion\nI1017 04:15:29.515281  1421 slave.cpp:699] Got acknowledgement of status\nupdate for task 1 of framework 20121017\n   0241-1056562698-5050-1405-0005\nI1017 04:15:29.516788  1421 slave.cpp:457] Got assigned task 2 for\nframework 201210170241-1056562698-5050-1405-0\n         005\nI1017 04:15:29.516840  1421 slave.cpp:1559] Generating a unique work\ndirectory for executor 'default' of framewo\n       rk 201210170241-1056562698-5050-1405-0005\nI1017 04:15:29.518450  1421 slave.cpp:522] Using\n'/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor\n\n ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/2' as\nwork directory for executor 'default' of\n      framework 201210170241-1056562698-5050-1405-0005\nI1017 04:15:29.520694  1421 process_based_isolation_module.cpp:93]\nLaunching default (/root/spark/spark-executor\n         ) in\n/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe\n                                       cutors/default/runs/2 with resources\nmem=512' for framework 201210170241-1056562698-5050-1405-0005\nI1017 04:15:29.522667  1421 process_based_isolation_module.cpp:129] Forked\nexecutor at 21237\nI1017 04:15:30.341961  1421 process_based_isolation_module.cpp:255] Telling\nslave of lost executor default of fr\n amework 201210170241-1056562698-5050-1405-0005\nI1017 04:15:30.342100  1421 process_utils.hpp:64] Stopping ... 21237\nSent signal to 21237\nI1017 04:15:30.427222  1421 slave.cpp:1383] Executor 'default' of framework\n201210170241-1056562698-5050-1405-00\n 05 has exited with status 127\nI1017 04:15:30.428863  1421 slave.cpp:989] Status update: task 2 of\nframework 201210170241-1056562698-5050-1405-\n         0005 is now in state TASK_LOST\nI1017 04:15:30.432663  1421 slave.cpp:1507] Scheduling executor directory\n/tmp/mesos/slaves/201210170241-1056562\n\n 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/2\nfor deletion\nI1017 04:15:30.432873  1421 slave.cpp:699] Got acknowledgement of status\nupdate for task 2 of framework 20121017\n   0241-1056562698-5050-1405-0005\nI1017 04:15:30.440033  1421 slave.cpp:457] Got assigned task 3 for\nframework 201210170241-1056562698-5050-1405-0\n         005\nI1017 04:15:30.440099  1421 slave.cpp:1559] Generating a unique work\ndirectory for executor 'default' of framewo\n       rk 201210170241-1056562698-5050-1405-0005\nI1017 04:15:30.441154  1421 slave.cpp:522] Using\n'/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor\n\n ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/3' as\nwork directory for executor 'default' of\n      framework 201210170241-1056562698-5050-1405-0005\nI1017 04:15:30.442700  1421 process_based_isolation_module.cpp:93]\nLaunching default (/root/spark/spark-executor\n         ) in\n/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe\n                                       cutors/default/runs/3 with resources\nmem=512' for framework 201210170241-1056562698-5050-1405-0005\nI1017 04:15:30.444150  1421 process_based_isolation_module.cpp:129] Forked\nexecutor at 21284\nI1017 04:15:31.343088  1421 process_based_isolation_module.cpp:255] Telling\nslave of lost executor default of fr\n amework 201210170241-1056562698-5050-1405-0005\nI1017 04:15:31.343229  1421 process_utils.hpp:64] Stopping ... 21284\nSent signal to 21284\nI1017 04:15:31.430294  1421 slave.cpp:1383] Executor 'default' of framework\n201210170241-1056562698-5050-1405-00\n 05 has exited with status 127\nI1017 04:15:31.432476  1421 slave.cpp:989] Status update: task 3 of\nframework 201210170241-1056562698-5050-1405-\n         0005 is now in state TASK_LOST\nI1017 04:15:31.434686  1421 slave.cpp:1507] Scheduling executor directory\n/tmp/mesos/slaves/201210170241-1056562\n\n 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/3\nfor deletion\nI1017 04:15:31.434814  1421 slave.cpp:699] Got acknowledgement of status\nupdate for task 3 of framework 20121017\n   0241-1056562698-5050-1405-0005\nI1017 04:15:31.449017  1421 slave.cpp:457] Got assigned task 4 for\nframework 201210170241-1056562698-5050-1405-0\n         005\nI1017 04:15:31.449122  1421 slave.cpp:1559] Generating a unique work\ndirectory for executor 'default' of framewo\n       rk 201210170241-1056562698-5050-1405-0005\nI1017 04:15:31.451181  1421 slave.cpp:522] Using\n'/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor\n\n ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/4' as\nwork directory for executor 'default' of\n      framework 201210170241-1056562698-5050-1405-0005\nI1017 04:15:31.452993  1421 process_based_isolation_module.cpp:93]\nLaunching default (/root/spark/spark-executor\n         ) in\n/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe\n                                       cutors/default/runs/4 with resources\nmem=512' for framework 201210170241-1056562698-5050-1405-0005\nI1017 04:15:31.455411  1421 process_based_isolation_module.cpp:129] Forked\nexecutor at 21331\nI1017 04:15:32.344189  1421 process_based_isolation_module.cpp:255] Telling\nslave of lost executor default of fr\n amework 201210170241-1056562698-5050-1405-0005\nI1017 04:15:32.344326  1421 process_utils.hpp:64] Stopping ... 21331\nSent signal to 21331\nI1017 04:15:32.431380  1421 slave.cpp:1383] Executor 'default' of framework\n201210170241-1056562698-5050-1405-00\n 05 has exited with status 127\nI1017 04:15:32.433527  1421 slave.cpp:989] Status update: task 4 of\nframework 201210170241-1056562698-5050-1405-\n         0005 is now in state TASK_LOST\nI1017 04:15:32.437247  1421 slave.cpp:1507] Scheduling executor directory\n/tmp/mesos/slaves/201210170241-1056562\n\n 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/4\nfor deletion\nI1017 04:15:32.441869  1421 slave.cpp:699] Got acknowledgement of status\nupdate for task 4 of framework 20121017\n   0241-1056562698-5050-1405-0005\n\nOn Mon, Oct 15, 2012 at 11:45 PM, Matei Zaharia <notifications@github.com>wrote:\n\n> Ah, got it. So how exactly did you start the cluster -- did you not use\n> our EC2 script? In that case you probably need to configure some settings\n> in conf/spark-env.sh, such as SCALA_HOME and MESOS_NATIVE_LIBRARY. Try\n> starting a cluster with our script (\n> https://github.com/mesos/spark/wiki/EC2-Scripts) to have them set up for\n> you.\n>\n> Again, to figure out the problem, you'd need to look at the logs of the\n> worker process. Mesos places them in a \"work\" directory for the job. This\n> is configured to be /mnt/mesos-work when you launch Mesos with our EC2\n> scripts, but if you launched it manually then it likely be in /tmp.\n>\n> Matei\n>\n> On Oct 15, 2012, at 8:40 PM, Qiming He wrote:\n>\n> > As stated in the issue description, it is a dummy *single* instance\n> > cluster, i.e., master and slave are co-hosted in one box. Where should I\n> > copy code to? -QH\n> >\n> > On Mon, Oct 15, 2012 at 11:30 PM, Matei Zaharia <\n> notifications@github.com>wrote:\n> >\n> > > Did you copy the code you built to the worker nodes too?\n> > >\n> > > If so, look in /mnt/mesos-work on the worker to find the stdout and\n> stderr\n> > > output for your job, and let me know what's in there. But generally\n> \"lost\n> > > TID\" means that the JVM crashed, which likely means that it didn't\n> have the\n> > > right code or otherwise couldn't start Spark.\n> > >\n> > > Matei\n> > >\n> > > On Oct 15, 2012, at 8:17 PM, Qiming He wrote:\n> > >\n> > > > Hi,\n> > > >\n> > > > I am using single EC2 instance with pre-built mesos (ami-0fcb7966)\n> (Same\n> > > issue if I build mesos from source code in locall VM)\n> > > >\n> > > > Follow instruction on\n> > > https://github.com/mesos/spark/wiki/Running-spark-on-mesos with some\n> > > tweaks.\n> > > >\n> > > > I install Cloudera cdhu5 by yum (not using pre-built hadoop due to\n> lack\n> > > of document)\n> > > >\n> > > > ./spartk-shell.sh\n> > > > import spark._\n> > > > val sc = new SparkContext(\"localhost:5050\",\"passwd\")\n> > > > val ec2 = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\")\n> > > >\n> > > > IF I keep val HADOOP_VERSION = \"0.20.205.0\" in\n> project/SparkBuild.scala\n> > > > at val file = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\")\n> > > > I am getting error\n> > > > Protocol org.apache.hadoop.hdfs.protocol.ClientProtocol version\n> > > mismatch. (client = 61, server = 63)\n> > > >\n> > > > IF I set val HADOOP_VERSION = \"0.20.2-cdh3u5\" or val HADOOP_VERSION\n> =\n> > > \"0.20.2-cdh3u3\"\n> > > > I am getting error at ec2.count()\n> > > > ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting\n> job\n> > > > like the one reported at\n> > >\n> http://mail-archives.apache.org/mod_mbox/incubator-mesos-dev/201108.mbox/%3CBD25AE7A-C9DC-4020-AD40-41C66DCAA56E@eecs.berkeley.edu%3E\n> > > >\n> > > > Please let me know if you cannot replicate this error, and give more\n> > > instruction on how Spark integrate with Cloudera Hadoop\n> > > >\n> > > > Thanks\n> > > >\n> > > > -QH\n> > > >\n> > > > \n> > > > Reply to this email directly or view it on GitHub.\n> > > >\n> > > >\n> > >\n> > > \n> > > Reply to this email directly or view it on GitHub<\n> https://github.com/mesos/spark/issues/280#issuecomment-9472032>.\n> > >\n> > >\n> >\n> >\n> >\n> > --\n> > Dr. Qiming He\n> > Qiming.He@openresearchinc.com\n> > 301-525-6612 (Phone)\n> > 815-327-2122 (Fax)\n> > \n> > Reply to this email directly or view it on GitHub.\n> >\n> >\n>\n> \n> Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/issues/280#issuecomment-9472214>.\n>\n>\n\n\n\n-- \nDr. Qiming He\nQiming.He@openresearchinc.com\n301-525-6612 (Phone)\n815-327-2122 (Fax)", "Github comment from mateiz: Got it. Actually the place to look for the worker process's output is\n\n/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/0\n\nIn there you'll find two files, stdout and stderr, with the output of the process. My guess is that it couldn't launch scala because you have not set up a conf/spark-env.sh file in your Spark folder with SCALA_HOME in it.\n\nMatei\n\nOn Oct 16, 2012, at 9:23 PM, Qiming He wrote:\n\n> Hi, \n> \n> 1st-ly, I just use EC2 ami instance, I did not try EC2 script yet. So it is \n> just dummy single instance, running in AWS (should be the same as local) \n> \n> 2nd-ly, I cannot find /mnt/mesos-work. I am assuming it is \n> /tmp/mesos-slave.INFO. I try Apache hadoop 1.0.4, setting val \n> HADOOP_VERSION = \"1.0.4\" in project/SparkBuild.scala \n> \n> No client mismatch problem, but still get error: \n> ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting job \n> \n> Here is tail of /tmp/mesos-slave.INFO \n> I1017 04:15:27.429769 1421 slave.cpp:457] Got assigned task 0 for \n> framework 201210170241-1056562698-5050-1405-0 \n> 005 \n> I1017 04:15:27.429924 1421 slave.cpp:1559] Generating a unique work \n> directory for executor 'default' of framewo \n> rk 201210170241-1056562698-5050-1405-0005 \n> I1017 04:15:27.461669 1421 slave.cpp:522] Using \n> '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor \n> \n> ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/0' as \n> work directory for executor 'default' of \n> framework 201210170241-1056562698-5050-1405-0005 \n> I1017 04:15:27.463573 1421 process_based_isolation_module.cpp:93] \n> Launching default (/root/spark/spark-executor \n> ) in \n> /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe \n> cutors/default/runs/0 with resources \n> mem=512' for framework 201210170241-1056562698-5050-1405-0005 \n> I1017 04:15:27.469944 1421 process_based_isolation_module.cpp:129] Forked \n> executor at 21143 \n> I1017 04:15:28.339529 1421 process_based_isolation_module.cpp:255] Telling \n> slave of lost executor default of fr \n> amework 201210170241-1056562698-5050-1405-0005 \n> I1017 04:15:28.339673 1421 process_utils.hpp:64] Stopping ... 21143 \n> Sent signal to 21143 \n> I1017 04:15:28.425079 1421 slave.cpp:1383] Executor 'default' of framework \n> 201210170241-1056562698-5050-1405-00 \n> 05 has exited with status 127 \n> I1017 04:15:28.427103 1421 slave.cpp:989] Status update: task 0 of \n> framework 201210170241-1056562698-5050-1405- \n> 0005 is now in state TASK_LOST \n> I1017 04:15:28.430907 1421 slave.cpp:1507] Scheduling executor directory \n> /tmp/mesos/slaves/201210170241-1056562 \n> \n> 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/0 \n> for deletion \n> I1017 04:15:28.434944 1421 slave.cpp:699] Got acknowledgement of status \n> update for task 0 of framework 20121017 \n> 0241-1056562698-5050-1405-0005 \n> I1017 04:15:28.442648 1421 slave.cpp:457] Got assigned task 1 for \n> framework 201210170241-1056562698-5050-1405-0 \n> 005 \n> I1017 04:15:28.442703 1421 slave.cpp:1559] Generating a unique work \n> directory for executor 'default' of framewo \n> rk 201210170241-1056562698-5050-1405-0005 \n> I1017 04:15:28.444704 1421 slave.cpp:522] Using \n> '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor \n> \n> ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/1' as \n> work directory for executor 'default' of \n> framework 201210170241-1056562698-5050-1405-0005 \n> I1017 04:15:28.447643 1421 process_based_isolation_module.cpp:93] \n> Launching default (/root/spark/spark-executor \n> ) in \n> /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe \n> cutors/default/runs/1 with resources \n> mem=512' for framework 201210170241-1056562698-5050-1405-0005 \n> I1017 04:15:28.449476 1421 process_based_isolation_module.cpp:129] Forked \n> executor at 21190 \n> I1017 04:15:29.340275 1421 process_based_isolation_module.cpp:255] Telling \n> slave of lost executor default of fr \n> amework 201210170241-1056562698-5050-1405-0005 \n> I1017 04:15:29.340428 1421 process_utils.hpp:64] Stopping ... 21190 \n> Sent signal to 21190 \n> I1017 04:15:29.432467 1421 slave.cpp:1383] Executor 'default' of framework \n> 201210170241-1056562698-5050-1405-00 \n> 05 has exited with status 127 \n> I1017 04:15:29.434921 1421 slave.cpp:989] Status update: task 1 of \n> framework 201210170241-1056562698-5050-1405- \n> 0005 is now in state TASK_LOST \n> I1017 04:15:29.501458 1421 slave.cpp:1507] Scheduling executor directory \n> /tmp/mesos/slaves/201210170241-1056562 \n> \n> 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/1 \n> for deletion \n> I1017 04:15:29.515281 1421 slave.cpp:699] Got acknowledgement of status \n> update for task 1 of framework 20121017 \n> 0241-1056562698-5050-1405-0005 \n> I1017 04:15:29.516788 1421 slave.cpp:457] Got assigned task 2 for \n> framework 201210170241-1056562698-5050-1405-0 \n> 005 \n> I1017 04:15:29.516840 1421 slave.cpp:1559] Generating a unique work \n> directory for executor 'default' of framewo \n> rk 201210170241-1056562698-5050-1405-0005 \n> I1017 04:15:29.518450 1421 slave.cpp:522] Using \n> '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor \n> \n> ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/2' as \n> work directory for executor 'default' of \n> framework 201210170241-1056562698-5050-1405-0005 \n> I1017 04:15:29.520694 1421 process_based_isolation_module.cpp:93] \n> Launching default (/root/spark/spark-executor \n> ) in \n> /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe \n> cutors/default/runs/2 with resources \n> mem=512' for framework 201210170241-1056562698-5050-1405-0005 \n> I1017 04:15:29.522667 1421 process_based_isolation_module.cpp:129] Forked \n> executor at 21237 \n> I1017 04:15:30.341961 1421 process_based_isolation_module.cpp:255] Telling \n> slave of lost executor default of fr \n> amework 201210170241-1056562698-5050-1405-0005 \n> I1017 04:15:30.342100 1421 process_utils.hpp:64] Stopping ... 21237 \n> Sent signal to 21237 \n> I1017 04:15:30.427222 1421 slave.cpp:1383] Executor 'default' of framework \n> 201210170241-1056562698-5050-1405-00 \n> 05 has exited with status 127 \n> I1017 04:15:30.428863 1421 slave.cpp:989] Status update: task 2 of \n> framework 201210170241-1056562698-5050-1405- \n> 0005 is now in state TASK_LOST \n> I1017 04:15:30.432663 1421 slave.cpp:1507] Scheduling executor directory \n> /tmp/mesos/slaves/201210170241-1056562 \n> \n> 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/2 \n> for deletion \n> I1017 04:15:30.432873 1421 slave.cpp:699] Got acknowledgement of status \n> update for task 2 of framework 20121017 \n> 0241-1056562698-5050-1405-0005 \n> I1017 04:15:30.440033 1421 slave.cpp:457] Got assigned task 3 for \n> framework 201210170241-1056562698-5050-1405-0 \n> 005 \n> I1017 04:15:30.440099 1421 slave.cpp:1559] Generating a unique work \n> directory for executor 'default' of framewo \n> rk 201210170241-1056562698-5050-1405-0005 \n> I1017 04:15:30.441154 1421 slave.cpp:522] Using \n> '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor \n> \n> ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/3' as \n> work directory for executor 'default' of \n> framework 201210170241-1056562698-5050-1405-0005 \n> I1017 04:15:30.442700 1421 process_based_isolation_module.cpp:93] \n> Launching default (/root/spark/spark-executor \n> ) in \n> /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe \n> cutors/default/runs/3 with resources \n> mem=512' for framework 201210170241-1056562698-5050-1405-0005 \n> I1017 04:15:30.444150 1421 process_based_isolation_module.cpp:129] Forked \n> executor at 21284 \n> I1017 04:15:31.343088 1421 process_based_isolation_module.cpp:255] Telling \n> slave of lost executor default of fr \n> amework 201210170241-1056562698-5050-1405-0005 \n> I1017 04:15:31.343229 1421 process_utils.hpp:64] Stopping ... 21284 \n> Sent signal to 21284 \n> I1017 04:15:31.430294 1421 slave.cpp:1383] Executor 'default' of framework \n> 201210170241-1056562698-5050-1405-00 \n> 05 has exited with status 127 \n> I1017 04:15:31.432476 1421 slave.cpp:989] Status update: task 3 of \n> framework 201210170241-1056562698-5050-1405- \n> 0005 is now in state TASK_LOST \n> I1017 04:15:31.434686 1421 slave.cpp:1507] Scheduling executor directory \n> /tmp/mesos/slaves/201210170241-1056562 \n> \n> 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/3 \n> for deletion \n> I1017 04:15:31.434814 1421 slave.cpp:699] Got acknowledgement of status \n> update for task 3 of framework 20121017 \n> 0241-1056562698-5050-1405-0005 \n> I1017 04:15:31.449017 1421 slave.cpp:457] Got assigned task 4 for \n> framework 201210170241-1056562698-5050-1405-0 \n> 005 \n> I1017 04:15:31.449122 1421 slave.cpp:1559] Generating a unique work \n> directory for executor 'default' of framewo \n> rk 201210170241-1056562698-5050-1405-0005 \n> I1017 04:15:31.451181 1421 slave.cpp:522] Using \n> '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor \n> \n> ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/4' as \n> work directory for executor 'default' of \n> framework 201210170241-1056562698-5050-1405-0005 \n> I1017 04:15:31.452993 1421 process_based_isolation_module.cpp:93] \n> Launching default (/root/spark/spark-executor \n> ) in \n> /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe \n> cutors/default/runs/4 with resources \n> mem=512' for framework 201210170241-1056562698-5050-1405-0005 \n> I1017 04:15:31.455411 1421 process_based_isolation_module.cpp:129] Forked \n> executor at 21331 \n> I1017 04:15:32.344189 1421 process_based_isolation_module.cpp:255] Telling \n> slave of lost executor default of fr \n> amework 201210170241-1056562698-5050-1405-0005 \n> I1017 04:15:32.344326 1421 process_utils.hpp:64] Stopping ... 21331 \n> Sent signal to 21331 \n> I1017 04:15:32.431380 1421 slave.cpp:1383] Executor 'default' of framework \n> 201210170241-1056562698-5050-1405-00 \n> 05 has exited with status 127 \n> I1017 04:15:32.433527 1421 slave.cpp:989] Status update: task 4 of \n> framework 201210170241-1056562698-5050-1405- \n> 0005 is now in state TASK_LOST \n> I1017 04:15:32.437247 1421 slave.cpp:1507] Scheduling executor directory \n> /tmp/mesos/slaves/201210170241-1056562 \n> \n> 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/4 \n> for deletion \n> I1017 04:15:32.441869 1421 slave.cpp:699] Got acknowledgement of status \n> update for task 4 of framework 20121017 \n> 0241-1056562698-5050-1405-0005 \n> \n> On Mon, Oct 15, 2012 at 11:45 PM, Matei Zaharia <notifications@github.com>wrote: \n> \n> > Ah, got it. So how exactly did you start the cluster -- did you not use \n> > our EC2 script? In that case you probably need to configure some settings \n> > in conf/spark-env.sh, such as SCALA_HOME and MESOS_NATIVE_LIBRARY. Try \n> > starting a cluster with our script ( \n> > https://github.com/mesos/spark/wiki/EC2-Scripts) to have them set up for \n> > you. \n> > \n> > Again, to figure out the problem, you'd need to look at the logs of the \n> > worker process. Mesos places them in a \"work\" directory for the job. This \n> > is configured to be /mnt/mesos-work when you launch Mesos with our EC2 \n> > scripts, but if you launched it manually then it likely be in /tmp. \n> > \n> > Matei \n> > \n> > On Oct 15, 2012, at 8:40 PM, Qiming He wrote: \n> > \n> > > As stated in the issue description, it is a dummy *single* instance \n> > > cluster, i.e., master and slave are co-hosted in one box. Where should I \n> > > copy code to? -QH \n> > > \n> > > On Mon, Oct 15, 2012 at 11:30 PM, Matei Zaharia < \n> > notifications@github.com>wrote: \n> > > \n> > > > Did you copy the code you built to the worker nodes too? \n> > > > \n> > > > If so, look in /mnt/mesos-work on the worker to find the stdout and \n> > stderr \n> > > > output for your job, and let me know what's in there. But generally \n> > \"lost \n> > > > TID\" means that the JVM crashed, which likely means that it didn't \n> > have the \n> > > > right code or otherwise couldn't start Spark. \n> > > > \n> > > > Matei \n> > > > \n> > > > On Oct 15, 2012, at 8:17 PM, Qiming He wrote: \n> > > > \n> > > > > Hi, \n> > > > > \n> > > > > I am using single EC2 instance with pre-built mesos (ami-0fcb7966) \n> > (Same \n> > > > issue if I build mesos from source code in locall VM) \n> > > > > \n> > > > > Follow instruction on \n> > > > https://github.com/mesos/spark/wiki/Running-spark-on-mesos with some \n> > > > tweaks. \n> > > > > \n> > > > > I install Cloudera cdhu5 by yum (not using pre-built hadoop due to \n> > lack \n> > > > of document) \n> > > > > \n> > > > > ./spartk-shell.sh \n> > > > > import spark._ \n> > > > > val sc = new SparkContext(\"localhost:5050\",\"passwd\") \n> > > > > val ec2 = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") \n> > > > > \n> > > > > IF I keep val HADOOP_VERSION = \"0.20.205.0\" in \n> > project/SparkBuild.scala \n> > > > > at val file = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") \n> > > > > I am getting error \n> > > > > Protocol org.apache.hadoop.hdfs.protocol.ClientProtocol version \n> > > > mismatch. (client = 61, server = 63) \n> > > > > \n> > > > > IF I set val HADOOP_VERSION = \"0.20.2-cdh3u5\" or val HADOOP_VERSION \n> > = \n> > > > \"0.20.2-cdh3u3\" \n> > > > > I am getting error at ec2.count() \n> > > > > ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting \n> > job \n> > > > > like the one reported at \n> > > > \n> > http://mail-archives.apache.org/mod_mbox/incubator-mesos-dev/201108.mbox/%3CBD25AE7A-C9DC-4020-AD40-41C66DCAA56E@eecs.berkeley.edu%3E \n> > > > > \n> > > > > Please let me know if you cannot replicate this error, and give more \n> > > > instruction on how Spark integrate with Cloudera Hadoop \n> > > > > \n> > > > > Thanks \n> > > > > \n> > > > > -QH \n> > > > > \n> > > > >  \n> > > > > Reply to this email directly or view it on GitHub. \n> > > > > \n> > > > > \n> > > > \n> > > >  \n> > > > Reply to this email directly or view it on GitHub< \n> > https://github.com/mesos/spark/issues/280#issuecomment-9472032>. \n> > > > \n> > > > \n> > > \n> > > \n> > > \n> > > -- \n> > > Dr. Qiming He \n> > > Qiming.He@openresearchinc.com \n> > > 301-525-6612 (Phone) \n> > > 815-327-2122 (Fax) \n> > >  \n> > > Reply to this email directly or view it on GitHub. \n> > > \n> > > \n> > \n> >  \n> > Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/issues/280#issuecomment-9472214>. \n> > \n> > \n> \n> \n> \n> -- \n> Dr. Qiming He \n> Qiming.He@openresearchinc.com \n> 301-525-6612 (Phone) \n> 815-327-2122 (Fax)\n> \n> Reply to this email directly or view it on GitHub.\n> \n>", "Github comment from openreserach: YES. That is the problem! After I set\n*export SCALA_HOME=/root/scala-2.9.1.final*\nit works perfectly.\n\nLast question: if scala is installed by package manager like yum or apt-get\nunder /usr/bin/scala.\nwhat is SCALA_HOME then?\n\nThanks\n\n-Qiming\n\n\nOn Wed, Oct 17, 2012 at 12:27 AM, Matei Zaharia <notifications@github.com>wrote:\n\n> Got it. Actually the place to look for the worker process's output is\n>\n> /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/0\n>\n>\n> In there you'll find two files, stdout and stderr, with the output of the\n> process. My guess is that it couldn't launch scala because you have not set\n> up a conf/spark-env.sh file in your Spark folder with SCALA_HOME in it.\n>\n> Matei\n>\n> On Oct 16, 2012, at 9:23 PM, Qiming He wrote:\n>\n> > Hi,\n> >\n> > 1st-ly, I just use EC2 ami instance, I did not try EC2 script yet. So it\n> is\n> > just dummy single instance, running in AWS (should be the same as local)\n> >\n> > 2nd-ly, I cannot find /mnt/mesos-work. I am assuming it is\n> > /tmp/mesos-slave.INFO. I try Apache hadoop 1.0.4, setting val\n> > HADOOP_VERSION = \"1.0.4\" in project/SparkBuild.scala\n> >\n> > No client mismatch problem, but still get error:\n> > ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting job\n> >\n> > Here is tail of /tmp/mesos-slave.INFO\n> > I1017 04:15:27.429769 1421 slave.cpp:457] Got assigned task 0 for\n> > framework 201210170241-1056562698-5050-1405-0\n> > 005\n> > I1017 04:15:27.429924 1421 slave.cpp:1559] Generating a unique work\n> > directory for executor 'default' of framewo\n> > rk 201210170241-1056562698-5050-1405-0005\n> > I1017 04:15:27.461669 1421 slave.cpp:522] Using\n> > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor\n> >\n> > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/0' as\n> > work directory for executor 'default' of\n> > framework 201210170241-1056562698-5050-1405-0005\n> > I1017 04:15:27.463573 1421 process_based_isolation_module.cpp:93]\n> > Launching default (/root/spark/spark-executor\n> > ) in\n> >\n> /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe\n>\n> > cutors/default/runs/0 with resources\n> > mem=512' for framework 201210170241-1056562698-5050-1405-0005\n> > I1017 04:15:27.469944 1421 process_based_isolation_module.cpp:129]\n> Forked\n> > executor at 21143\n> > I1017 04:15:28.339529 1421 process_based_isolation_module.cpp:255]\n> Telling\n> > slave of lost executor default of fr\n> > amework 201210170241-1056562698-5050-1405-0005\n> > I1017 04:15:28.339673 1421 process_utils.hpp:64] Stopping ... 21143\n> > Sent signal to 21143\n> > I1017 04:15:28.425079 1421 slave.cpp:1383] Executor 'default' of\n> framework\n> > 201210170241-1056562698-5050-1405-00\n> > 05 has exited with status 127\n> > I1017 04:15:28.427103 1421 slave.cpp:989] Status update: task 0 of\n> > framework 201210170241-1056562698-5050-1405-\n> > 0005 is now in state TASK_LOST\n> > I1017 04:15:28.430907 1421 slave.cpp:1507] Scheduling executor directory\n> > /tmp/mesos/slaves/201210170241-1056562\n> >\n> >\n> 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/0\n>\n> > for deletion\n> > I1017 04:15:28.434944 1421 slave.cpp:699] Got acknowledgement of status\n> > update for task 0 of framework 20121017\n> > 0241-1056562698-5050-1405-0005\n> > I1017 04:15:28.442648 1421 slave.cpp:457] Got assigned task 1 for\n> > framework 201210170241-1056562698-5050-1405-0\n> > 005\n> > I1017 04:15:28.442703 1421 slave.cpp:1559] Generating a unique work\n> > directory for executor 'default' of framewo\n> > rk 201210170241-1056562698-5050-1405-0005\n> > I1017 04:15:28.444704 1421 slave.cpp:522] Using\n> > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor\n> >\n> > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/1' as\n> > work directory for executor 'default' of\n> > framework 201210170241-1056562698-5050-1405-0005\n> > I1017 04:15:28.447643 1421 process_based_isolation_module.cpp:93]\n> > Launching default (/root/spark/spark-executor\n> > ) in\n> >\n> /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe\n>\n> > cutors/default/runs/1 with resources\n> > mem=512' for framework 201210170241-1056562698-5050-1405-0005\n> > I1017 04:15:28.449476 1421 process_based_isolation_module.cpp:129]\n> Forked\n> > executor at 21190\n> > I1017 04:15:29.340275 1421 process_based_isolation_module.cpp:255]\n> Telling\n> > slave of lost executor default of fr\n> > amework 201210170241-1056562698-5050-1405-0005\n> > I1017 04:15:29.340428 1421 process_utils.hpp:64] Stopping ... 21190\n> > Sent signal to 21190\n> > I1017 04:15:29.432467 1421 slave.cpp:1383] Executor 'default' of\n> framework\n> > 201210170241-1056562698-5050-1405-00\n> > 05 has exited with status 127\n> > I1017 04:15:29.434921 1421 slave.cpp:989] Status update: task 1 of\n> > framework 201210170241-1056562698-5050-1405-\n> > 0005 is now in state TASK_LOST\n> > I1017 04:15:29.501458 1421 slave.cpp:1507] Scheduling executor directory\n> > /tmp/mesos/slaves/201210170241-1056562\n> >\n> >\n> 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/1\n>\n> > for deletion\n> > I1017 04:15:29.515281 1421 slave.cpp:699] Got acknowledgement of status\n> > update for task 1 of framework 20121017\n> > 0241-1056562698-5050-1405-0005\n> > I1017 04:15:29.516788 1421 slave.cpp:457] Got assigned task 2 for\n> > framework 201210170241-1056562698-5050-1405-0\n> > 005\n> > I1017 04:15:29.516840 1421 slave.cpp:1559] Generating a unique work\n> > directory for executor 'default' of framewo\n> > rk 201210170241-1056562698-5050-1405-0005\n> > I1017 04:15:29.518450 1421 slave.cpp:522] Using\n> > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor\n> >\n> > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/2' as\n> > work directory for executor 'default' of\n> > framework 201210170241-1056562698-5050-1405-0005\n> > I1017 04:15:29.520694 1421 process_based_isolation_module.cpp:93]\n> > Launching default (/root/spark/spark-executor\n> > ) in\n> >\n> /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe\n>\n> > cutors/default/runs/2 with resources\n> > mem=512' for framework 201210170241-1056562698-5050-1405-0005\n> > I1017 04:15:29.522667 1421 process_based_isolation_module.cpp:129]\n> Forked\n> > executor at 21237\n> > I1017 04:15:30.341961 1421 process_based_isolation_module.cpp:255]\n> Telling\n> > slave of lost executor default of fr\n> > amework 201210170241-1056562698-5050-1405-0005\n> > I1017 04:15:30.342100 1421 process_utils.hpp:64] Stopping ... 21237\n> > Sent signal to 21237\n> > I1017 04:15:30.427222 1421 slave.cpp:1383] Executor 'default' of\n> framework\n> > 201210170241-1056562698-5050-1405-00\n> > 05 has exited with status 127\n> > I1017 04:15:30.428863 1421 slave.cpp:989] Status update: task 2 of\n> > framework 201210170241-1056562698-5050-1405-\n> > 0005 is now in state TASK_LOST\n> > I1017 04:15:30.432663 1421 slave.cpp:1507] Scheduling executor directory\n> > /tmp/mesos/slaves/201210170241-1056562\n> >\n> >\n> 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/2\n>\n> > for deletion\n> > I1017 04:15:30.432873 1421 slave.cpp:699] Got acknowledgement of status\n> > update for task 2 of framework 20121017\n> > 0241-1056562698-5050-1405-0005\n> > I1017 04:15:30.440033 1421 slave.cpp:457] Got assigned task 3 for\n> > framework 201210170241-1056562698-5050-1405-0\n> > 005\n> > I1017 04:15:30.440099 1421 slave.cpp:1559] Generating a unique work\n> > directory for executor 'default' of framewo\n> > rk 201210170241-1056562698-5050-1405-0005\n> > I1017 04:15:30.441154 1421 slave.cpp:522] Using\n> > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor\n> >\n> > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/3' as\n> > work directory for executor 'default' of\n> > framework 201210170241-1056562698-5050-1405-0005\n> > I1017 04:15:30.442700 1421 process_based_isolation_module.cpp:93]\n> > Launching default (/root/spark/spark-executor\n> > ) in\n> >\n> /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe\n>\n> > cutors/default/runs/3 with resources\n> > mem=512' for framework 201210170241-1056562698-5050-1405-0005\n> > I1017 04:15:30.444150 1421 process_based_isolation_module.cpp:129]\n> Forked\n> > executor at 21284\n> > I1017 04:15:31.343088 1421 process_based_isolation_module.cpp:255]\n> Telling\n> > slave of lost executor default of fr\n> > amework 201210170241-1056562698-5050-1405-0005\n> > I1017 04:15:31.343229 1421 process_utils.hpp:64] Stopping ... 21284\n> > Sent signal to 21284\n> > I1017 04:15:31.430294 1421 slave.cpp:1383] Executor 'default' of\n> framework\n> > 201210170241-1056562698-5050-1405-00\n> > 05 has exited with status 127\n> > I1017 04:15:31.432476 1421 slave.cpp:989] Status update: task 3 of\n> > framework 201210170241-1056562698-5050-1405-\n> > 0005 is now in state TASK_LOST\n> > I1017 04:15:31.434686 1421 slave.cpp:1507] Scheduling executor directory\n> > /tmp/mesos/slaves/201210170241-1056562\n> >\n> >\n> 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/3\n>\n> > for deletion\n> > I1017 04:15:31.434814 1421 slave.cpp:699] Got acknowledgement of status\n> > update for task 3 of framework 20121017\n> > 0241-1056562698-5050-1405-0005\n> > I1017 04:15:31.449017 1421 slave.cpp:457] Got assigned task 4 for\n> > framework 201210170241-1056562698-5050-1405-0\n> > 005\n> > I1017 04:15:31.449122 1421 slave.cpp:1559] Generating a unique work\n> > directory for executor 'default' of framewo\n> > rk 201210170241-1056562698-5050-1405-0005\n> > I1017 04:15:31.451181 1421 slave.cpp:522] Using\n> > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor\n> >\n> > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/4' as\n> > work directory for executor 'default' of\n> > framework 201210170241-1056562698-5050-1405-0005\n> > I1017 04:15:31.452993 1421 process_based_isolation_module.cpp:93]\n> > Launching default (/root/spark/spark-executor\n> > ) in\n> >\n> /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe\n>\n> > cutors/default/runs/4 with resources\n> > mem=512' for framework 201210170241-1056562698-5050-1405-0005\n> > I1017 04:15:31.455411 1421 process_based_isolation_module.cpp:129]\n> Forked\n> > executor at 21331\n> > I1017 04:15:32.344189 1421 process_based_isolation_module.cpp:255]\n> Telling\n> > slave of lost executor default of fr\n> > amework 201210170241-1056562698-5050-1405-0005\n> > I1017 04:15:32.344326 1421 process_utils.hpp:64] Stopping ... 21331\n> > Sent signal to 21331\n> > I1017 04:15:32.431380 1421 slave.cpp:1383] Executor 'default' of\n> framework\n> > 201210170241-1056562698-5050-1405-00\n> > 05 has exited with status 127\n> > I1017 04:15:32.433527 1421 slave.cpp:989] Status update: task 4 of\n> > framework 201210170241-1056562698-5050-1405-\n> > 0005 is now in state TASK_LOST\n> > I1017 04:15:32.437247 1421 slave.cpp:1507] Scheduling executor directory\n> > /tmp/mesos/slaves/201210170241-1056562\n> >\n> >\n> 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/4\n>\n> > for deletion\n> > I1017 04:15:32.441869 1421 slave.cpp:699] Got acknowledgement of status\n> > update for task 4 of framework 20121017\n> > 0241-1056562698-5050-1405-0005\n> >\n> > On Mon, Oct 15, 2012 at 11:45 PM, Matei Zaharia <\n> notifications@github.com>wrote:\n> >\n> > > Ah, got it. So how exactly did you start the cluster -- did you not\n> use\n> > > our EC2 script? In that case you probably need to configure some\n> settings\n> > > in conf/spark-env.sh, such as SCALA_HOME and MESOS_NATIVE_LIBRARY. Try\n> > > starting a cluster with our script (\n> > > https://github.com/mesos/spark/wiki/EC2-Scripts) to have them set up\n> for\n> > > you.\n> > >\n> > > Again, to figure out the problem, you'd need to look at the logs of\n> the\n> > > worker process. Mesos places them in a \"work\" directory for the job.\n> This\n> > > is configured to be /mnt/mesos-work when you launch Mesos with our EC2\n> > > scripts, but if you launched it manually then it likely be in /tmp.\n> > >\n> > > Matei\n> > >\n> > > On Oct 15, 2012, at 8:40 PM, Qiming He wrote:\n> > >\n> > > > As stated in the issue description, it is a dummy *single* instance\n> > > > cluster, i.e., master and slave are co-hosted in one box. Where\n> should I\n> > > > copy code to? -QH\n> > > >\n> > > > On Mon, Oct 15, 2012 at 11:30 PM, Matei Zaharia <\n> > > notifications@github.com>wrote:\n> > > >\n> > > > > Did you copy the code you built to the worker nodes too?\n> > > > >\n> > > > > If so, look in /mnt/mesos-work on the worker to find the stdout\n> and\n> > > stderr\n> > > > > output for your job, and let me know what's in there. But\n> generally\n> > > \"lost\n> > > > > TID\" means that the JVM crashed, which likely means that it didn't\n> > > have the\n> > > > > right code or otherwise couldn't start Spark.\n> > > > >\n> > > > > Matei\n> > > > >\n> > > > > On Oct 15, 2012, at 8:17 PM, Qiming He wrote:\n> > > > >\n> > > > > > Hi,\n> > > > > >\n> > > > > > I am using single EC2 instance with pre-built mesos\n> (ami-0fcb7966)\n> > > (Same\n> > > > > issue if I build mesos from source code in locall VM)\n> > > > > >\n> > > > > > Follow instruction on\n> > > > > https://github.com/mesos/spark/wiki/Running-spark-on-mesos with\n> some\n> > > > > tweaks.\n> > > > > >\n> > > > > > I install Cloudera cdhu5 by yum (not using pre-built hadoop due\n> to\n> > > lack\n> > > > > of document)\n> > > > > >\n> > > > > > ./spartk-shell.sh\n> > > > > > import spark._\n> > > > > > val sc = new SparkContext(\"localhost:5050\",\"passwd\")\n> > > > > > val ec2 = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\")\n> > > > > >\n> > > > > > IF I keep val HADOOP_VERSION = \"0.20.205.0\" in\n> > > project/SparkBuild.scala\n> > > > > > at val file = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\")\n> > > > > > I am getting error\n> > > > > > Protocol org.apache.hadoop.hdfs.protocol.ClientProtocol version\n> > > > > mismatch. (client = 61, server = 63)\n> > > > > >\n> > > > > > IF I set val HADOOP_VERSION = \"0.20.2-cdh3u5\" or val\n> HADOOP_VERSION\n> > > =\n> > > > > \"0.20.2-cdh3u3\"\n> > > > > > I am getting error at ec2.count()\n> > > > > > ERROR spark.SimpleJob: Task 0:0 failed more than 4 times;\n> aborting\n> > > job\n> > > > > > like the one reported at\n> > > > >\n> > >\n> http://mail-archives.apache.org/mod_mbox/incubator-mesos-dev/201108.mbox/%3CBD25AE7A-C9DC-4020-AD40-41C66DCAA56E@eecs.berkeley.edu%3E\n> > > > > >\n> > > > > > Please let me know if you cannot replicate this error, and give\n> more\n> > > > > instruction on how Spark integrate with Cloudera Hadoop\n> > > > > >\n> > > > > > Thanks\n> > > > > >\n> > > > > > -QH\n> > > > > >\n> > > > > > \n> > > > > > Reply to this email directly or view it on GitHub.\n> > > > > >\n> > > > > >\n> > > > >\n> > > > > \n> > > > > Reply to this email directly or view it on GitHub<\n> > > https://github.com/mesos/spark/issues/280#issuecomment-9472032>.\n> > > > >\n> > > > >\n> > > >\n> > > >\n> > > >\n> > > > --\n> > > > Dr. Qiming He\n> > > > Qiming.He@openresearchinc.com\n> > > > 301-525-6612 (Phone)\n> > > > 815-327-2122 (Fax)\n> > > > \n> > > > Reply to this email directly or view it on GitHub.\n> > > >\n> > > >\n> > >\n> > > \n> > > Reply to this email directly or view it on GitHub<\n> https://github.com/mesos/spark/issues/280#issuecomment-9472214>.\n> > >\n> > >\n> >\n> >\n> >\n> > --\n> > Dr. Qiming He\n> > Qiming.He@openresearchinc.com\n> > 301-525-6612 (Phone)\n> > 815-327-2122 (Fax)\n> > \n> > Reply to this email directly or view it on GitHub.\n> >\n> >\n>\n> \n> Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/issues/280#issuecomment-9515690>.\n>\n>\n\n\n\n-- \nDr. Qiming He\nQiming.He@openresearchinc.com\n301-525-6612 (Phone)\n815-327-2122 (Fax)", "Github comment from rxin: It is usually just a symlink in /usr/bin/scala.\n\nDo a ls -l to figure out where the symlink points to.\n\nE.g. for mine:\n\n~ $ ls -l `which scala`\nlrwxr-xr-x  1 root  wheel  39 Sep 26 15:31 /usr/local/bin/scala ->\n/usr/local/Cellar/scala/2.9.2/bin/scala\n\nIn this case, SCALA_HOME should be /usr/local/Cellar/scala/2.9.2/libexec\n\n\n\nOn Wed, Oct 17, 2012 at 6:46 PM, Qiming He <notifications@github.com> wrote:\n\n> YES. That is the problem! After I set\n> *export SCALA_HOME=/root/scala-2.9.1.final*\n> it works perfectly.\n>\n> Last question: if scala is installed by package manager like yum or\n> apt-get\n> under /usr/bin/scala.\n> what is SCALA_HOME then?\n>\n> Thanks\n>\n> -Qiming\n>\n>\n> On Wed, Oct 17, 2012 at 12:27 AM, Matei Zaharia <notifications@github.com>wrote:\n>\n>\n> > Got it. Actually the place to look for the worker process's output is\n> >\n> >\n> /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/0\n>\n> >\n> >\n> > In there you'll find two files, stdout and stderr, with the output of\n> the\n> > process. My guess is that it couldn't launch scala because you have not\n> set\n> > up a conf/spark-env.sh file in your Spark folder with SCALA_HOME in it.\n> >\n> > Matei\n> >\n> > On Oct 16, 2012, at 9:23 PM, Qiming He wrote:\n> >\n> > > Hi,\n> > >\n> > > 1st-ly, I just use EC2 ami instance, I did not try EC2 script yet. So\n> it\n> > is\n> > > just dummy single instance, running in AWS (should be the same as\n> local)\n> > >\n> > > 2nd-ly, I cannot find /mnt/mesos-work. I am assuming it is\n> > > /tmp/mesos-slave.INFO. I try Apache hadoop 1.0.4, setting val\n> > > HADOOP_VERSION = \"1.0.4\" in project/SparkBuild.scala\n> > >\n> > > No client mismatch problem, but still get error:\n> > > ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting job\n> > >\n> > > Here is tail of /tmp/mesos-slave.INFO\n> > > I1017 04:15:27.429769 1421 slave.cpp:457] Got assigned task 0 for\n> > > framework 201210170241-1056562698-5050-1405-0\n> > > 005\n> > > I1017 04:15:27.429924 1421 slave.cpp:1559] Generating a unique work\n> > > directory for executor 'default' of framewo\n> > > rk 201210170241-1056562698-5050-1405-0005\n> > > I1017 04:15:27.461669 1421 slave.cpp:522] Using\n> > > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor\n> > >\n> > > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/0' as\n> > > work directory for executor 'default' of\n> > > framework 201210170241-1056562698-5050-1405-0005\n> > > I1017 04:15:27.463573 1421 process_based_isolation_module.cpp:93]\n> > > Launching default (/root/spark/spark-executor\n> > > ) in\n> > >\n> >\n> /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe\n>\n> >\n> > > cutors/default/runs/0 with resources\n> > > mem=512' for framework 201210170241-1056562698-5050-1405-0005\n> > > I1017 04:15:27.469944 1421 process_based_isolation_module.cpp:129]\n> > Forked\n> > > executor at 21143\n> > > I1017 04:15:28.339529 1421 process_based_isolation_module.cpp:255]\n> > Telling\n> > > slave of lost executor default of fr\n> > > amework 201210170241-1056562698-5050-1405-0005\n> > > I1017 04:15:28.339673 1421 process_utils.hpp:64] Stopping ... 21143\n> > > Sent signal to 21143\n> > > I1017 04:15:28.425079 1421 slave.cpp:1383] Executor 'default' of\n> > framework\n> > > 201210170241-1056562698-5050-1405-00\n> > > 05 has exited with status 127\n> > > I1017 04:15:28.427103 1421 slave.cpp:989] Status update: task 0 of\n> > > framework 201210170241-1056562698-5050-1405-\n> > > 0005 is now in state TASK_LOST\n> > > I1017 04:15:28.430907 1421 slave.cpp:1507] Scheduling executor\n> directory\n> > > /tmp/mesos/slaves/201210170241-1056562\n> > >\n> > >\n> >\n> 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/0\n>\n> >\n> > > for deletion\n> > > I1017 04:15:28.434944 1421 slave.cpp:699] Got acknowledgement of\n> status\n> > > update for task 0 of framework 20121017\n> > > 0241-1056562698-5050-1405-0005\n> > > I1017 04:15:28.442648 1421 slave.cpp:457] Got assigned task 1 for\n> > > framework 201210170241-1056562698-5050-1405-0\n> > > 005\n> > > I1017 04:15:28.442703 1421 slave.cpp:1559] Generating a unique work\n> > > directory for executor 'default' of framewo\n> > > rk 201210170241-1056562698-5050-1405-0005\n> > > I1017 04:15:28.444704 1421 slave.cpp:522] Using\n> > > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor\n> > >\n> > > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/1' as\n> > > work directory for executor 'default' of\n> > > framework 201210170241-1056562698-5050-1405-0005\n> > > I1017 04:15:28.447643 1421 process_based_isolation_module.cpp:93]\n> > > Launching default (/root/spark/spark-executor\n> > > ) in\n> > >\n> >\n> /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe\n>\n> >\n> > > cutors/default/runs/1 with resources\n> > > mem=512' for framework 201210170241-1056562698-5050-1405-0005\n> > > I1017 04:15:28.449476 1421 process_based_isolation_module.cpp:129]\n> > Forked\n> > > executor at 21190\n> > > I1017 04:15:29.340275 1421 process_based_isolation_module.cpp:255]\n> > Telling\n> > > slave of lost executor default of fr\n> > > amework 201210170241-1056562698-5050-1405-0005\n> > > I1017 04:15:29.340428 1421 process_utils.hpp:64] Stopping ... 21190\n> > > Sent signal to 21190\n> > > I1017 04:15:29.432467 1421 slave.cpp:1383] Executor 'default' of\n> > framework\n> > > 201210170241-1056562698-5050-1405-00\n> > > 05 has exited with status 127\n> > > I1017 04:15:29.434921 1421 slave.cpp:989] Status update: task 1 of\n> > > framework 201210170241-1056562698-5050-1405-\n> > > 0005 is now in state TASK_LOST\n> > > I1017 04:15:29.501458 1421 slave.cpp:1507] Scheduling executor\n> directory\n> > > /tmp/mesos/slaves/201210170241-1056562\n> > >\n> > >\n> >\n> 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/1\n>\n> >\n> > > for deletion\n> > > I1017 04:15:29.515281 1421 slave.cpp:699] Got acknowledgement of\n> status\n> > > update for task 1 of framework 20121017\n> > > 0241-1056562698-5050-1405-0005\n> > > I1017 04:15:29.516788 1421 slave.cpp:457] Got assigned task 2 for\n> > > framework 201210170241-1056562698-5050-1405-0\n> > > 005\n> > > I1017 04:15:29.516840 1421 slave.cpp:1559] Generating a unique work\n> > > directory for executor 'default' of framewo\n> > > rk 201210170241-1056562698-5050-1405-0005\n> > > I1017 04:15:29.518450 1421 slave.cpp:522] Using\n> > > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor\n> > >\n> > > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/2' as\n> > > work directory for executor 'default' of\n> > > framework 201210170241-1056562698-5050-1405-0005\n> > > I1017 04:15:29.520694 1421 process_based_isolation_module.cpp:93]\n> > > Launching default (/root/spark/spark-executor\n> > > ) in\n> > >\n> >\n> /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe\n>\n> >\n> > > cutors/default/runs/2 with resources\n> > > mem=512' for framework 201210170241-1056562698-5050-1405-0005\n> > > I1017 04:15:29.522667 1421 process_based_isolation_module.cpp:129]\n> > Forked\n> > > executor at 21237\n> > > I1017 04:15:30.341961 1421 process_based_isolation_module.cpp:255]\n> > Telling\n> > > slave of lost executor default of fr\n> > > amework 201210170241-1056562698-5050-1405-0005\n> > > I1017 04:15:30.342100 1421 process_utils.hpp:64] Stopping ... 21237\n> > > Sent signal to 21237\n> > > I1017 04:15:30.427222 1421 slave.cpp:1383] Executor 'default' of\n> > framework\n> > > 201210170241-1056562698-5050-1405-00\n> > > 05 has exited with status 127\n> > > I1017 04:15:30.428863 1421 slave.cpp:989] Status update: task 2 of\n> > > framework 201210170241-1056562698-5050-1405-\n> > > 0005 is now in state TASK_LOST\n> > > I1017 04:15:30.432663 1421 slave.cpp:1507] Scheduling executor\n> directory\n> > > /tmp/mesos/slaves/201210170241-1056562\n> > >\n> > >\n> >\n> 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/2\n>\n> >\n> > > for deletion\n> > > I1017 04:15:30.432873 1421 slave.cpp:699] Got acknowledgement of\n> status\n> > > update for task 2 of framework 20121017\n> > > 0241-1056562698-5050-1405-0005\n> > > I1017 04:15:30.440033 1421 slave.cpp:457] Got assigned task 3 for\n> > > framework 201210170241-1056562698-5050-1405-0\n> > > 005\n> > > I1017 04:15:30.440099 1421 slave.cpp:1559] Generating a unique work\n> > > directory for executor 'default' of framewo\n> > > rk 201210170241-1056562698-5050-1405-0005\n> > > I1017 04:15:30.441154 1421 slave.cpp:522] Using\n> > > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor\n> > >\n> > > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/3' as\n> > > work directory for executor 'default' of\n> > > framework 201210170241-1056562698-5050-1405-0005\n> > > I1017 04:15:30.442700 1421 process_based_isolation_module.cpp:93]\n> > > Launching default (/root/spark/spark-executor\n> > > ) in\n> > >\n> >\n> /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe\n>\n> >\n> > > cutors/default/runs/3 with resources\n> > > mem=512' for framework 201210170241-1056562698-5050-1405-0005\n> > > I1017 04:15:30.444150 1421 process_based_isolation_module.cpp:129]\n> > Forked\n> > > executor at 21284\n> > > I1017 04:15:31.343088 1421 process_based_isolation_module.cpp:255]\n> > Telling\n> > > slave of lost executor default of fr\n> > > amework 201210170241-1056562698-5050-1405-0005\n> > > I1017 04:15:31.343229 1421 process_utils.hpp:64] Stopping ... 21284\n> > > Sent signal to 21284\n> > > I1017 04:15:31.430294 1421 slave.cpp:1383] Executor 'default' of\n> > framework\n> > > 201210170241-1056562698-5050-1405-00\n> > > 05 has exited with status 127\n> > > I1017 04:15:31.432476 1421 slave.cpp:989] Status update: task 3 of\n> > > framework 201210170241-1056562698-5050-1405-\n> > > 0005 is now in state TASK_LOST\n> > > I1017 04:15:31.434686 1421 slave.cpp:1507] Scheduling executor\n> directory\n> > > /tmp/mesos/slaves/201210170241-1056562\n> > >\n> > >\n> >\n> 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/3\n>\n> >\n> > > for deletion\n> > > I1017 04:15:31.434814 1421 slave.cpp:699] Got acknowledgement of\n> status\n> > > update for task 3 of framework 20121017\n> > > 0241-1056562698-5050-1405-0005\n> > > I1017 04:15:31.449017 1421 slave.cpp:457] Got assigned task 4 for\n> > > framework 201210170241-1056562698-5050-1405-0\n> > > 005\n> > > I1017 04:15:31.449122 1421 slave.cpp:1559] Generating a unique work\n> > > directory for executor 'default' of framewo\n> > > rk 201210170241-1056562698-5050-1405-0005\n> > > I1017 04:15:31.451181 1421 slave.cpp:522] Using\n> > > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor\n> > >\n> > > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/4' as\n> > > work directory for executor 'default' of\n> > > framework 201210170241-1056562698-5050-1405-0005\n> > > I1017 04:15:31.452993 1421 process_based_isolation_module.cpp:93]\n> > > Launching default (/root/spark/spark-executor\n> > > ) in\n> > >\n> >\n> /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe\n>\n> >\n> > > cutors/default/runs/4 with resources\n> > > mem=512' for framework 201210170241-1056562698-5050-1405-0005\n> > > I1017 04:15:31.455411 1421 process_based_isolation_module.cpp:129]\n> > Forked\n> > > executor at 21331\n> > > I1017 04:15:32.344189 1421 process_based_isolation_module.cpp:255]\n> > Telling\n> > > slave of lost executor default of fr\n> > > amework 201210170241-1056562698-5050-1405-0005\n> > > I1017 04:15:32.344326 1421 process_utils.hpp:64] Stopping ... 21331\n> > > Sent signal to 21331\n> > > I1017 04:15:32.431380 1421 slave.cpp:1383] Executor 'default' of\n> > framework\n> > > 201210170241-1056562698-5050-1405-00\n> > > 05 has exited with status 127\n> > > I1017 04:15:32.433527 1421 slave.cpp:989] Status update: task 4 of\n> > > framework 201210170241-1056562698-5050-1405-\n> > > 0005 is now in state TASK_LOST\n> > > I1017 04:15:32.437247 1421 slave.cpp:1507] Scheduling executor\n> directory\n> > > /tmp/mesos/slaves/201210170241-1056562\n> > >\n> > >\n> >\n> 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/4\n>\n> >\n> > > for deletion\n> > > I1017 04:15:32.441869 1421 slave.cpp:699] Got acknowledgement of\n> status\n> > > update for task 4 of framework 20121017\n> > > 0241-1056562698-5050-1405-0005\n> > >\n> > > On Mon, Oct 15, 2012 at 11:45 PM, Matei Zaharia <\n> > notifications@github.com>wrote:\n> > >\n> > > > Ah, got it. So how exactly did you start the cluster -- did you not\n> > use\n> > > > our EC2 script? In that case you probably need to configure some\n> > settings\n> > > > in conf/spark-env.sh, such as SCALA_HOME and MESOS_NATIVE_LIBRARY.\n> Try\n> > > > starting a cluster with our script (\n> > > > https://github.com/mesos/spark/wiki/EC2-Scripts) to have them set\n> up\n> > for\n> > > > you.\n> > > >\n> > > > Again, to figure out the problem, you'd need to look at the logs of\n> > the\n> > > > worker process. Mesos places them in a \"work\" directory for the job.\n> > This\n> > > > is configured to be /mnt/mesos-work when you launch Mesos with our\n> EC2\n> > > > scripts, but if you launched it manually then it likely be in /tmp.\n> > > >\n> > > > Matei\n> > > >\n> > > > On Oct 15, 2012, at 8:40 PM, Qiming He wrote:\n> > > >\n> > > > > As stated in the issue description, it is a dummy *single*\n> instance\n> > > > > cluster, i.e., master and slave are co-hosted in one box. Where\n> > should I\n> > > > > copy code to? -QH\n> > > > >\n> > > > > On Mon, Oct 15, 2012 at 11:30 PM, Matei Zaharia <\n> > > > notifications@github.com>wrote:\n> > > > >\n> > > > > > Did you copy the code you built to the worker nodes too?\n> > > > > >\n> > > > > > If so, look in /mnt/mesos-work on the worker to find the stdout\n> > and\n> > > > stderr\n> > > > > > output for your job, and let me know what's in there. But\n> > generally\n> > > > \"lost\n> > > > > > TID\" means that the JVM crashed, which likely means that it\n> didn't\n> > > > have the\n> > > > > > right code or otherwise couldn't start Spark.\n> > > > > >\n> > > > > > Matei\n> > > > > >\n> > > > > > On Oct 15, 2012, at 8:17 PM, Qiming He wrote:\n> > > > > >\n> > > > > > > Hi,\n> > > > > > >\n> > > > > > > I am using single EC2 instance with pre-built mesos\n> > (ami-0fcb7966)\n> > > > (Same\n> > > > > > issue if I build mesos from source code in locall VM)\n> > > > > > >\n> > > > > > > Follow instruction on\n> > > > > > https://github.com/mesos/spark/wiki/Running-spark-on-mesos with\n> > some\n> > > > > > tweaks.\n> > > > > > >\n> > > > > > > I install Cloudera cdhu5 by yum (not using pre-built hadoop\n> due\n> > to\n> > > > lack\n> > > > > > of document)\n> > > > > > >\n> > > > > > > ./spartk-shell.sh\n> > > > > > > import spark._\n> > > > > > > val sc = new SparkContext(\"localhost:5050\",\"passwd\")\n> > > > > > > val ec2 = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\")\n> > > > > > >\n> > > > > > > IF I keep val HADOOP_VERSION = \"0.20.205.0\" in\n> > > > project/SparkBuild.scala\n> > > > > > > at val file = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\")\n> > > > > > > I am getting error\n> > > > > > > Protocol org.apache.hadoop.hdfs.protocol.ClientProtocol\n> version\n> > > > > > mismatch. (client = 61, server = 63)\n> > > > > > >\n> > > > > > > IF I set val HADOOP_VERSION = \"0.20.2-cdh3u5\" or val\n> > HADOOP_VERSION\n> > > > =\n> > > > > > \"0.20.2-cdh3u3\"\n> > > > > > > I am getting error at ec2.count()\n> > > > > > > ERROR spark.SimpleJob: Task 0:0 failed more than 4 times;\n> > aborting\n> > > > job\n> > > > > > > like the one reported at\n> > > > > >\n> > > >\n> >\n> http://mail-archives.apache.org/mod_mbox/incubator-mesos-dev/201108.mbox/%3CBD25AE7A-C9DC-4020-AD40-41C66DCAA56E@eecs.berkeley.edu%3E\n> > > > > > >\n> > > > > > > Please let me know if you cannot replicate this error, and\n> give\n> > more\n> > > > > > instruction on how Spark integrate with Cloudera Hadoop\n> > > > > > >\n> > > > > > > Thanks\n> > > > > > >\n> > > > > > > -QH\n> > > > > > >\n> > > > > > > \n> > > > > > > Reply to this email directly or view it on GitHub.\n> > > > > > >\n> > > > > > >\n> > > > > >\n> > > > > > \n> > > > > > Reply to this email directly or view it on GitHub<\n> > > > https://github.com/mesos/spark/issues/280#issuecomment-9472032>.\n> > > > > >\n> > > > > >\n> > > > >\n> > > > >\n> > > > >\n> > > > > --\n> > > > > Dr. Qiming He\n> > > > > Qiming.He@openresearchinc.com\n> > > > > 301-525-6612 (Phone)\n> > > > > 815-327-2122 (Fax)\n> > > > > \n> > > > > Reply to this email directly or view it on GitHub.\n> > > > >\n> > > > >\n> > > >\n> > > > \n> > > > Reply to this email directly or view it on GitHub<\n> > https://github.com/mesos/spark/issues/280#issuecomment-9472214>.\n> > > >\n> > > >\n> > >\n> > >\n> > >\n> > > --\n> > > Dr. Qiming He\n> > > Qiming.He@openresearchinc.com\n> > > 301-525-6612 (Phone)\n> > > 815-327-2122 (Fax)\n> > > \n> > > Reply to this email directly or view it on GitHub.\n> > >\n> > >\n> >\n> > \n> > Reply to this email directly or view it on GitHub<\n> https://github.com/mesos/spark/issues/280#issuecomment-9515690>.\n> >\n> >\n>\n>\n>\n> --\n> Dr. Qiming He\n> Qiming.He@openresearchinc.com\n> 301-525-6612 (Phone)\n> 815-327-2122 (Fax)\n>\n> \n> Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/issues/280#issuecomment-9550652>.\n>\n>", "Imported from Github issue spark-280, originally reported by openreserach", "Can I ask a meta-question? This JIRA is an example, but just one. I see hundreds of JIRAs that likely have no further action.\n\nSome are likely obsoleted by time and subsequent changes, like this one -- CDH integration is much different now and presumably fixes this. Some are feature requests or changes that de facto don't have support and therefore won't be committed. These seem like they should be closed, for clarity. Bugs are riskier to close in case they identify a real issue that still exists.\n\nIs there any momentum for, or anything I can do, to help clean up things like this just to start?", "+1 for cleanup of issues that likely have no further action.", "I'm testing whether contributors can in fact close others JIRAs. It seems clear this one is stale and possibly already resolved.\n\nI'm also interested in how comfortable people are with people like me modifying JIRAs where it seems like there is a clear update or status change that needs to happen, like, it's clearly already resolved and needs to be marked as such?"], "derived": {"summary": "Hi, \n\n1. I am using single EC2 instance with pre-built mesos (ami-0fcb7966) (Same issue if I build mesos from source code in locall VM)\n\n2.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "spark integration issue with Cloudera hadoop - Hi, \n\n1. I am using single EC2 instance with pre-built mesos (ami-0fcb7966) (Same issue if I build mesos from source code in locall VM)\n\n2."}, {"q": "What updates or decisions were made in the discussion?", "a": "I'm testing whether contributors can in fact close others JIRAs. It seems clear this one is stale and possibly already resolved.\n\nI'm also interested in how comfortable people are with people like me modifying JIRAs where it seems like there is a clear update or status change that needs to happen, like, it's clearly already resolved and needs to be marked as such?"}]}}
{"project": "SPARK", "issue_id": "SPARK-523", "title": "Added a method to report slave memory status; force serialize accumulator update in local mode.", "status": "Resolved", "priority": null, "reporter": "Reynold Xin", "assignee": null, "labels": [], "created": "0012-10-15T20:40:00.000+0000", "updated": "2013-04-01T22:05:03.000+0000", "description": null, "comments": ["Imported from Github issue spark-281, originally reported by rxin", "This was an old pull request that was merged in https://github.com/mesos/spark/pull/281; closing."], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Added a method to report slave memory status; force serialize accumulator update in local mode."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was an old pull request that was merged in https://github.com/mesos/spark/pull/281; closing."}]}}
{"project": "SPARK", "issue_id": "SPARK-522", "title": "Updated Kryo to version 2.20", "status": "Resolved", "priority": null, "reporter": "Reynold Xin", "assignee": "Reynold Xin", "labels": [], "created": "0012-10-16T00:59:00.000+0000", "updated": "2013-01-20T12:22:49.000+0000", "description": "sbt/sbt test pass.\n\nTried it on Shark using a join query that produces 60 million rows and it worked fine. Saw a negligible performance increase in the join query (very hard to tell the difference - if anything, ~ 5% - could well be just system variability even though I tested 20 times).", "comments": ["Github comment from mateiz: Cool, that was fast! Let me figure out the branches a bit before merging this though. I want to keep branch 0.6 on Kryo 1.x, and then merge dev into master and have that take in this stuff.", "Github comment from rxin: BTW - I run Shark with Kryo 2.0 through all 700 tests, and only 4 of them failed because of the following reason:\n\nspark.SparkException: Job failed: ShuffleMapTask(2236, 0) failed: ExceptionFailure(com.esotericsoftware.kryo.KryoException: java.lang\n.IllegalArgumentException: Can not set java.sql.Timestamp field org.apache.hadoop.hive.serde2.io.TimestampWritable.timestamp to java.\nutil.Date\nSerialization trace:\ntimestamp (org.apache.hadoop.hive.serde2.io.TimestampWritable)\nvalue (org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantTimestampObjectInspector)\nfieldObjectInspector (org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector$MyField)\nfields (org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector)\narray (scala.collection.mutable.ArrayBuffer))\n\n\nI will look into this. Other than that, this looks pretty solid.", "Github comment from mateiz: Not bad. I assume the Spark tests pass too?\n\nMatei\n\nOn Oct 18, 2012, at 12:28 PM, Reynold Xin wrote:\n\n> BTW - I run Shark with Kryo 2.0 through all 700 tests, and only 4 of them failed because of the following reason:\n> \n> spark.SparkException: Job failed: ShuffleMapTask(2236, 0) failed: ExceptionFailure(com.esotericsoftware.kryo.KryoException: java.lang\n> .IllegalArgumentException: Can not set java.sql.Timestamp field org.apache.hadoop.hive.serde2.io.TimestampWritable.timestamp to java.\n> util.Date\n> Serialization trace:\n> timestamp (org.apache.hadoop.hive.serde2.io.TimestampWritable)\n> value (org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantTimestampObjectInspector)\n> fieldObjectInspector (org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector$MyField)\n> fields (org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector)\n> array (scala.collection.mutable.ArrayBuffer))\n> \n> I will look into this. Other than that, this looks pretty solid.\n> \n> \n> Reply to this email directly or view it on GitHub.\n> \n>", "Github comment from rxin: That was in the original comment: sbt/sbt test passes :)", "Github comment from rxin: I was able to fix the problem with TimestampWritable in Shark by creating a new specialized serializer for writables:\n\n```scala\n\n    kryo.register(classOf[org.apache.hadoop.hive.serde2.io.TimestampWritable],\n      new KryoWritableSerializer[org.apache.hadoop.hive.serde2.io.TimestampWritable])\n\n\n/** A Kryo serializer for Hadoop writables. */\nclass KryoWritableSerializer[T <: Writable] extends KSerializer[T] {\n  override def write(kryo: Kryo, output: KryoOutput, writable: T) {\n    val ouputStream = new DataOutputStream(output)\n    writable.write(ouputStream)\n  }\n\n  override def read(kryo: Kryo, input: KryoInput, cls: java.lang.Class[T]): T = {\n    val writable = cls.newInstance()\n    val inputStream = new DataInputStream(input)\n    writable.readFields(inputStream)\n    writable\n  }\n}\n```", "Imported from Github issue spark-282, originally reported by rxin", "Resolving since this pull request was merged."], "derived": {"summary": "sbt/sbt test pass. Tried it on Shark using a join query that produces 60 million rows and it worked fine.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "Updated Kryo to version 2.20 - sbt/sbt test pass. Tried it on Shark using a join query that produces 60 million rows and it worked fine."}, {"q": "What updates or decisions were made in the discussion?", "a": "Resolving since this pull request was merged."}]}}
{"project": "SPARK", "issue_id": "SPARK-520", "title": "Structure SBT build file into modules.", "status": "Resolved", "priority": null, "reporter": "RayRacine", "assignee": null, "labels": [], "created": "0012-10-18T08:30:00.000+0000", "updated": "2014-09-03T11:38:39.000+0000", "description": "*Build Changes*\n\nThis does a bit of housekeeping for the SparkBuild SBT file.  It should be considered a first cut and not an ideal.  Please feel free to modify to taste.\n\n- It is intended to be \"neutral\" in effect in the sense no build semantics were meant to be changed.\n- No dependency versions were changed.\n- It does publish-local a Spark build successfully on my machine.\n\n*Breakage*\n\n- The Maven stuff is certainly not working and I hope this is something that can be repaired quickly.  Sorry about that.\n\n*Modifications*\n\n- I added an additional file ShellPrompt.scala  This modifies the SBT prompt to show the prompt as \nproject:branch:project-version>\n- As a result of that, I changed the root projects name from \"root\" to \"spark\" as seeing a console prompt starting with the word \"root\" gives me palpitations.\n- I took the liberty of adding two additional scalac compiler settings -Xlint and -Xcheckinit.\n\nRay", "comments": ["Github comment from RayRacine: I added a comment about something I noted in the build file.\n  /* Workaround for issue #206 (fixed after SBT 0.11.0) */\n  // Is this still applicable?  Remove? RPR\n  watchTransitiveSources <<= Defaults.inDependencies[Task[Seq[File]]] (watchSources.task,\n                                                                              const (std.TaskExtra.constant (Nil)), \n                                                                              aggregate = true, \n                                                                              includeRoot = true) apply { _.join.map (_.flatten) },\n\nSince Spark is now on SBT 11.3 (maybe heading to 12.2) does anyone recall if this is still necessary?", "Imported from Github issue spark-284, originally reported by RayRacine", "Given subsequent reorganization of SBT and Maven build, I think this is obsolete."], "derived": {"summary": "*Build Changes*\n\nThis does a bit of housekeeping for the SparkBuild SBT file. It should be considered a first cut and not an ideal.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Structure SBT build file into modules. - *Build Changes*\n\nThis does a bit of housekeeping for the SparkBuild SBT file. It should be considered a first cut and not an ideal."}, {"q": "What updates or decisions were made in the discussion?", "a": "Given subsequent reorganization of SBT and Maven build, I think this is obsolete."}]}}
{"project": "SPARK", "issue_id": "SPARK-521", "title": "Create repository for /root/mesos-ec2 scripts", "status": "Resolved", "priority": null, "reporter": "Josh Rosen", "assignee": "Shivaram Venkataraman", "labels": [], "created": "0012-10-18T08:30:00.000+0000", "updated": "2013-04-05T19:47:03.000+0000", "description": "The Spark AMI contains scripts in `/root/mesos-ec2` which do not appear to be part of the standard Mesos distribution and are not in any public repositories.\n\nFor example, `/root/mesos-ec2/compute_cluster_url.py` and `/root/mesos-ec2/deploy_templates.py`.  It looks like there are also some modifications to the standard mesos-ec2 scripts to call these new scripts.\n\nThese should probably in a public repository and the EC2 script README should have a link to it.  The current EC2 scripts can be hard to understand since these files aren't included.  For example, the 0.5.1 release notes mention that the launch script automatically configures Spark's memory limit; this feature appears to be implemented in `/root/mesos-ec2/deploy_templates.py`, so I couldn't find a commit for it in Spark.", "comments": ["Github comment from mateiz: I like this idea, and it should be a accompanied by a guide to making your own AMIs.\n\n\nOn Oct 18, 2012, at 9:30 AM, Josh Rosen wrote:\n\n> The Spark AMI contains scripts in /root/mesos-ec2 which do not appear to be part of the standard Mesos distribution and are not in any public repositories.\n> \n> For example, /root/mesos-ec2/compute_cluster_url.py and /root/mesos-ec2/deploy_templates.py. It looks like there are also some modifications to the standard mesos-ec2 scripts to call these new scripts.\n> \n> These should probably in a public repository and the EC2 script README should have a link to it. The current EC2 scripts can be hard to understand since these files aren't included. For example, the 0.5.1 release notes mention that the launch script automatically configures Spark's memory limit; this feature appears to be implemented in /root/mesos-ec2/deploy_templates.py, so I couldn't find a commit for it in Spark.\n> \n> \n> Reply to this email directly or view it on GitHub.\n> \n>", "Imported from Github issue spark-283, originally reported by JoshRosen", "Should these scripts be copied from the AMI and placed into here instead of a new repo?\n\nhttps://github.com/mesos/spark/tree/master/ec2/deploy.generic/root/mesos-ec2\n\nI am happy to do this if that is all that is required. Please assign to me.", "No, I think it's best to keep them in a separate Git repo that is checked out on the AMI. The deploy.generic setup is there so that the spark-ec2 scripts can work with any version of the AMI, including possibly future versions. We don't want to require users to update their local Spark version in order to be able to launch a fixed AMI.\n\nWe'll move the stuff that is on the AMI into a separate repo, but I think that's something we have to do on this end, along with maybe a better way to package the AMIs in the first place.", "I am happy to help out with the AMI creation scripts. Have done plenty in the past, including using Ansible do make it easy to maintain and rerun.\n\n\n\n", "Great! As I discussed on the mailing list, an automatic script would be awesome. Just make sure it works with the way spark-ec2 deploys stuff, which is basically to copy a file containing some environment variables to ~/mesos-ec2 and then to run a script that's on the machine.", "A repository for these scripts was created when we released 0.7; it's at https://github.com/mesos/spark-ec2/"], "derived": {"summary": "The Spark AMI contains scripts in `/root/mesos-ec2` which do not appear to be part of the standard Mesos distribution and are not in any public repositories. For example, `/root/mesos-ec2/compute_cluster_url.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Create repository for /root/mesos-ec2 scripts - The Spark AMI contains scripts in `/root/mesos-ec2` which do not appear to be part of the standard Mesos distribution and are not in any public repositories. For example, `/root/mesos-ec2/compute_cluster_url."}, {"q": "What updates or decisions were made in the discussion?", "a": "A repository for these scripts was created when we released 0.7; it's at https://github.com/mesos/spark-ec2/"}]}}
{"project": "SPARK", "issue_id": "SPARK-519", "title": "Support for Hadoop 2 distributions such as cdh4", "status": "Resolved", "priority": null, "reporter": "Thomas Dudziak", "assignee": "Thomas Dudziak", "labels": [], "created": "0012-10-18T15:11:00.000+0000", "updated": "2013-05-04T19:12:39.000+0000", "description": "Hadoop 2 unfortunately introduced backwards-incompatible changes in classes used by Spark, `JobContext` and `TaskAttemptContext` in both the `mapred` and `mapreduce` packages. In the new codebase, these classes are now interfaces and thus cannot be instantiated directly anymore. Instead, you are now required to use the `JobContextImpl` and `TaskAttemptContextImpl` classes, respectively.\n\nThis pull requests extracts the code in Spark where it uses these, into separate traits that are located in Hadoop major version specific source folders that the sbt build then includes/excludes as necessary.\n\nIn addition, for Hadoop 2, Spark now also needs to depend on the `hadoop-client` artifact.\n\nThis pull request is a new version of #264 against the 0.6 dev branch.", "comments": ["Github comment from tomdz: This approach will require that there are two Spark artifacts published to the maven repo (one for Hadoop 1 and one for Hadoop 2). I have not made those changes to the `SparkBuild` file primarily because I'm not very familar with sbt (we use Maven to generate our Spark build).\n\nAlso worth noting that we played around with an alternate approach where Spark deals with the Hadoop incompatibilities at runtime by using reflection. The benefit of this would be a single Spark artifact. However this introduces a runtime overhead for every task/job that spark generates in HDFS, plus it removes type safety and overall feels less clean.", "Imported from Github issue spark-285, originally reported by tomdz", "[~tomdzk]'s pull request adding Hadoop 2 support was merged and was first released in v0.6.1 (https://github.com/mesos/spark/commit/15e95be2fd26161416736a323d114ff1fc98f941).  Resolving this as fixed."], "derived": {"summary": "Hadoop 2 unfortunately introduced backwards-incompatible changes in classes used by Spark, `JobContext` and `TaskAttemptContext` in both the `mapred` and `mapreduce` packages. In the new codebase, these classes are now interfaces and thus cannot be instantiated directly anymore.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support for Hadoop 2 distributions such as cdh4 - Hadoop 2 unfortunately introduced backwards-incompatible changes in classes used by Spark, `JobContext` and `TaskAttemptContext` in both the `mapred` and `mapreduce` packages. In the new codebase, these classes are now interfaces and thus cannot be instantiated directly anymore."}, {"q": "What updates or decisions were made in the discussion?", "a": "[~tomdzk]'s pull request adding Hadoop 2 support was merged and was first released in v0.6.1 (https://github.com/mesos/spark/commit/15e95be2fd26161416736a323d114ff1fc98f941).  Resolving this as fixed."}]}}
{"project": "SPARK", "issue_id": "SPARK-518", "title": "Allow EC2 script to stop/destroy cluster after master/slave failures", "status": "Closed", "priority": null, "reporter": "Josh Rosen", "assignee": null, "labels": [], "created": "0012-10-18T16:00:00.000+0000", "updated": "2012-10-24T00:00:22.000+0000", "description": "This commit solves a problem where `spark-ec2` could not destroy or stop a cluster if its master or all of its slaves had been destroyed.\n\nIt also prints the total count of master and slave nodes before printing error messages.", "comments": ["Imported from Github issue spark-286, originally reported by JoshRosen", "Closing, since this pull request was merged."], "derived": {"summary": "This commit solves a problem where `spark-ec2` could not destroy or stop a cluster if its master or all of its slaves had been destroyed. It also prints the total count of master and slave nodes before printing error messages.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Allow EC2 script to stop/destroy cluster after master/slave failures - This commit solves a problem where `spark-ec2` could not destroy or stop a cluster if its master or all of its slaves had been destroyed. It also prints the total count of master and slave nodes before printing error messages."}, {"q": "What updates or decisions were made in the discussion?", "a": "Closing, since this pull request was merged."}]}}
{"project": "SPARK", "issue_id": "SPARK-290", "title": "Use SPARK_MASTER_IP if it is set in start-slaves.sh.", "status": "Resolved", "priority": null, "reporter": "Reynold Xin", "assignee": null, "labels": [], "created": "0012-10-19T00:09:00.000+0000", "updated": "2012-10-19T22:50:34.000+0000", "description": "Also check to prompt user if it is not set and the script cannot figure out the master's ip.", "comments": ["Imported from Github issue spark-287, originally reported by rxin"], "derived": {"summary": "Also check to prompt user if it is not set and the script cannot figure out the master's ip.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Use SPARK_MASTER_IP if it is set in start-slaves.sh. - Also check to prompt user if it is not set and the script cannot figure out the master's ip."}, {"q": "What updates or decisions were made in the discussion?", "a": "Imported from Github issue spark-287, originally reported by rxin"}]}}
{"project": "SPARK", "issue_id": "SPARK-517", "title": "Exiting spark-ec2 with unfulfilled spot instance request does not cancel request", "status": "Closed", "priority": null, "reporter": "Josh Rosen", "assignee": "Josh Rosen", "labels": [], "created": "0012-10-19T08:37:00.000+0000", "updated": "2012-11-01T11:36:04.000+0000", "description": "Exiting the `spark-ec2` script after placing a spot instance request will not cancel that request, which may cause extra instances to be launched.  The script should probably register an `atexit` or `SIGINT` handler to perform cleanup.  There are some potential race conditions here, so canceling the spot instance request might not work if its instances are just beginning to launch; we may want to print a warning message about this.", "comments": ["Imported from Github issue spark-288, originally reported by JoshRosen", "Fixed in https://github.com/mesos/spark/pull/297"], "derived": {"summary": "Exiting the `spark-ec2` script after placing a spot instance request will not cancel that request, which may cause extra instances to be launched. The script should probably register an `atexit` or `SIGINT` handler to perform cleanup.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Exiting spark-ec2 with unfulfilled spot instance request does not cancel request - Exiting the `spark-ec2` script after placing a spot instance request will not cancel that request, which may cause extra instances to be launched. The script should probably register an `atexit` or `SIGINT` handler to perform cleanup."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed in https://github.com/mesos/spark/pull/297"}]}}
{"project": "SPARK", "issue_id": "SPARK-516", "title": "Improve error reporting when slaves fail to start", "status": "Closed", "priority": "Major", "reporter": "Reynold Xin", "assignee": null, "labels": [], "created": "0012-10-19T10:49:00.000+0000", "updated": "2015-11-08T20:24:16.000+0000", "description": "Currently Spark just hangs waiting for resources and slaves to respond. This behavior is very confusing to users, especially first time users.\n\nIf an error message is generated, it should be propagated back to the master so the user is aware of it.", "comments": ["Imported from Github issue spark-289, originally reported by rxin", "[~rxin] is this still relevant? ", "Marking as not a problem since it's been a really long time.\n"], "derived": {"summary": "Currently Spark just hangs waiting for resources and slaves to respond. This behavior is very confusing to users, especially first time users.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Improve error reporting when slaves fail to start - Currently Spark just hangs waiting for resources and slaves to respond. This behavior is very confusing to users, especially first time users."}, {"q": "What updates or decisions were made in the discussion?", "a": "Marking as not a problem since it's been a really long time."}]}}
{"project": "SPARK", "issue_id": "SPARK-579", "title": "An anonymous submission to Spark", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": "Patrick McFadin", "labels": [], "created": "2012-10-19T21:28:15.000+0000", "updated": "2012-10-19T21:28:41.000+0000", "description": "I found a bug! I am going to anonymously report it :)", "comments": ["This was a test, I am resolving."], "derived": {"summary": "I found a bug! I am going to anonymously report it :).", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "An anonymous submission to Spark - I found a bug! I am going to anonymously report it :)."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was a test, I am resolving."}]}}
{"project": "SPARK", "issue_id": "SPARK-580", "title": "Use Spark local directory as PySpark tmp directory", "status": "Resolved", "priority": "Minor", "reporter": "Josh Rosen", "assignee": "Josh Rosen", "labels": [], "created": "2012-10-19T23:01:52.000+0000", "updated": "2013-02-01T12:20:43.000+0000", "description": null, "comments": ["Fixed in https://github.com/mesos/spark/pull/439"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Use Spark local directory as PySpark tmp directory"}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed in https://github.com/mesos/spark/pull/439"}]}}
{"project": "SPARK", "issue_id": "SPARK-581", "title": "Try using a smart commit and seeing if it works", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": null, "labels": [], "created": "2012-10-19T23:12:33.000+0000", "updated": "2012-10-19T23:40:05.000+0000", "description": null, "comments": ["Worked! cool."], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Try using a smart commit and seeing if it works"}, {"q": "What updates or decisions were made in the discussion?", "a": "Worked! cool."}]}}
{"project": "SPARK", "issue_id": "SPARK-582", "title": "Add unit test for complex column (lazy column)", "status": "Closed", "priority": "Major", "reporter": "Reynold Xin", "assignee": null, "labels": [], "created": "2012-10-20T02:21:48.000+0000", "updated": "2012-10-20T11:09:39.000+0000", "description": "46e4baba281dc994666d21773d6151eea01c/src/test/scala/shark/memstore/ColumnSuite.scala", "comments": ["Should this JIRA be under SHARK?", "This is a Shark issue created in the wrong project."], "derived": {"summary": "46e4baba281dc994666d21773d6151eea01c/src/test/scala/shark/memstore/ColumnSuite. scala.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Add unit test for complex column (lazy column) - 46e4baba281dc994666d21773d6151eea01c/src/test/scala/shark/memstore/ColumnSuite. scala."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is a Shark issue created in the wrong project."}]}}
{"project": "SPARK", "issue_id": "SPARK-583", "title": "Failures in BlockStore may lead to infinite loops of task failures", "status": "Resolved", "priority": "Major", "reporter": "Josh Rosen", "assignee": "Charles Reiss", "labels": [], "created": "2012-10-20T15:54:00.000+0000", "updated": "2013-05-19T13:31:09.000+0000", "description": "Summary: failures in BlockStore may lead to infinite loops of task failures.\n\nI ran into a situation where a block manager operation failed:\n{code}\n12/10/20 21:25:13 ERROR storage.BlockManagerWorker: Exception handling buffer message\ncom.esotericsoftware.kryo.SerializationException: Buffer limit exceeded writing object of type: shark.ColumnarWritable\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:492)\n\tat spark.KryoSerializationStream.writeObject(KryoSerializer.scala:78)\n\tat spark.serializer.SerializationStream$class.writeAll(Serializer.scala:58)\n\tat spark.KryoSerializationStream.writeAll(KryoSerializer.scala:73)\n\tat spark.storage.BlockManager.dataSerialize(BlockManager.scala:834)\n\tat spark.storage.MemoryStore.getBytes(MemoryStore.scala:72)\n\tat spark.storage.BlockManager.getLocalBytes(BlockManager.scala:311)\n\tat spark.storage.BlockManagerWorker.getBlock(BlockManagerWorker.scala:79)\n\tat spark.storage.BlockManagerWorker.processBlockMessage(BlockManagerWorker.scala:58)\n\tat spark.storage.BlockManagerWorker$$anonfun$2.apply(BlockManagerWorker.scala:33)\n\tat spark.storage.BlockManagerWorker$$anonfun$2.apply(BlockManagerWorker.scala:33)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:772)\n\tat scala.collection.IndexedSeqLike$Elements.foreach(IndexedSeqLike.scala:54)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:73)\n\tat spark.storage.BlockMessageArray.foreach(BlockMessageArray.scala:12)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:233)\n\tat spark.storage.BlockMessageArray.map(BlockMessageArray.scala:12)\n\tat spark.storage.BlockManagerWorker.onBlockMessageReceive(BlockManagerWorker.scala:33)\n\tat spark.storage.BlockManagerWorker$$anonfun$1.apply(BlockManagerWorker.scala:23)\n\tat spark.storage.BlockManagerWorker$$anonfun$1.apply(BlockManagerWorker.scala:23)\n\tat spark.network.ConnectionManager.spark$network$ConnectionManager$$handleMessage(ConnectionManager.scala:276)\n\tat spark.network.ConnectionManager$$anon$4.run(ConnectionManager.scala:242)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\n\tat java.lang.Thread.run(Thread.java:679)\n{code}\n\nThis failure appears to have been detected via a fetch failure in the following stage:\n\n{code}\n12/10/20 21:25:12 INFO scheduler.DAGScheduler: Marking Stage 2 (mapPartitions at Operator.scala:197) for resubmision due to a fetch failure\n12/10/20 21:25:12 INFO scheduler.DAGScheduler: The failed fetch was from Stage 3 (mapPartitions at Operator.scala:197); marking it for resubmission\n{code}\n\nThe failed task was retried on the same machine, and it executed without exceptions.\n\nHowever, the job is unable to make forward progress; the scheduler gets stuck in an infinite loop of the form\n{code}\n12/10/20 22:23:08 INFO spark.CacheTrackerActor: Asked for current cache locations\n12/10/20 22:23:08 INFO scheduler.DAGScheduler: Resubmitting Stage 3 (mapPartitions at Operator.scala:197) because some of its tasks had failed: 220\n12/10/20 22:23:08 INFO scheduler.DAGScheduler: Submitting Stage 3 (mapPartitions at Operator.scala:197), which has no missing parents\n12/10/20 22:23:08 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from Stage 3\n12/10/20 22:23:08 INFO cluster.ClusterScheduler: Adding task set 3.4080 with 1 tasks\n12/10/20 22:23:08 INFO cluster.TaskSetManager: Starting task 3.4080:0 as TID 5484 on slave 201210202106-1093469194-5050-5222-43: domU-12-31-39-14-5E-5E.compute-1.internal (preferred)\n12/10/20 22:23:08 INFO cluster.TaskSetManager: Serialized task 3.4080:0 as 7605 bytes in 0 ms\n12/10/20 22:23:09 INFO cluster.TaskSetManager: Finished TID 5484 in 820 ms (progress: 1/1)\n12/10/20 22:23:09 INFO scheduler.DAGScheduler: Completed ShuffleMapTask(3, 220)\n12/10/20 22:23:09 INFO scheduler.DAGScheduler: ShuffleMapTask finished with host domU-12-31-39-14-5E-5E.compute-1.internal\n12/10/20 22:23:09 INFO scheduler.DAGScheduler: Stage 3 (mapPartitions at Operator.scala:197) finished; looking for newly runnable stages\n12/10/20 22:23:09 INFO scheduler.DAGScheduler: running: Set()\n12/10/20 22:23:09 INFO scheduler.DAGScheduler: waiting: Set(Stage 2, Stage 1)\n12/10/20 22:23:09 INFO scheduler.DAGScheduler: failed: Set()\n12/10/20 22:23:09 INFO spark.CacheTrackerActor: Asked for current cache locations\n{code}\n\nIf I look at the worker that is running these tasks, I see infinite loop of the form\n{code}\n12/10/20 21:29:29 INFO exec.GroupByPreShuffleOperator: #hash table=24918 #rows=100000 reduction=0.24918 minReduction=0.5\n12/10/20 21:29:29 WARN storage.BlockManager: Block shuffle_0_220_0 already exists on this machine; not re-adding it\n12/10/20 21:29:29 WARN storage.BlockManager: Block shuffle_0_220_1 already exists on this machine; not re-adding it\n12/10/20 21:29:29 WARN storage.BlockManager: Block shuffle_0_220_2 already exists on this machine; not re-adding it\n[..]\n12/10/20 21:29:29 WARN storage.BlockManager: Block shuffle_0_220_199 already exists on this machine; not re-adding it\n12/10/20 21:29:29 INFO executor.Executor: Serialized size of result for 1677 is 350\n12/10/20 21:29:29 INFO executor.Executor: Finished task ID 1677\n12/10/20 21:29:29 INFO executor.Executor: Running task ID 1678\n12/10/20 21:29:29 INFO executor.Executor: Its generation is 3\n12/10/20 21:29:29 INFO spark.CacheTracker: Cache key is rdd_4_220\n12/10/20 21:29:29 INFO spark.CacheTracker: Found partition in cache!\n12/10/20 21:29:29 INFO exec.GroupByPreShuffleOperator: Running Pre-Shuffle Group-By\n12/10/20 21:29:29 INFO exec.GroupByPreShuffleOperator: Mapside hash aggregation enabled\n12/10/20 21:29:29 INFO exec.GroupByPreShuffleOperator: #hash table=24918 #rows=100000 reduction=0.24918 minReduction=0.5\n{code}\n\nI'm not sure of the exact cause of this behavior, but I have a guess:\n\nDuring the original failed execution, the task's output blocks were not stored but their block ids were added to the BlockManager's {{blockInfo}} map.  This prevents these blocks from being recomputed and causes the \"{{Block shuffle_*_*_* already exists on this machine; not re-adding it}}\" warnings.  As a result, the block is never stored and the master is never informed of its location.\n\nThis causes the DAG scheduler to repeatedly launch the same task in an infinite loop, saying that it is \"{{Resubmitting Stage * because some of its tasks had failed: *}}\".", "comments": ["I haven't been able to reproduce the infinite loop of task retrials, but I did try modifying the {{DiskStore}} to randomly fail with {{IOException}} while trying to write blocks.  On a local standalone cluster, these failures were detected but the job hung after trying to recompute the failed tasks.\n\nIf I catch the exception and remove the block's {{blockId}} from the {{BlockManager}}'s {{blockInfo}} map, then the job is eventually able to succeed.  This suggests that we should add exception handling inside the {{BlockManager}}'s {{put()}} and {{putBytes()}} calls.\n\nThere is one subtle race condition to watch for, though: between the time that we store the {{blockId}} and the time that the write fails, another task may have retrieved that block's {{BlockInfo}} object and began to wait by calling {{waitForReady()}}.  To avoid deadlock, this waiter needs to notified of the failure.", "I managed to run across this issue again today, using a branch based on Spark 0.6.1.  Even though no exceptions occurred, I saw the same infinite loop of task resubmissions and \"Block already exists\" messages.\n\nTwo machines exhibited loops, and both loops were preceded by a block being dropped from the memory store to free up memory.  This is a bit strange, since it looks like the machine had plenty of free memory:\n\n{code}\n12/12/14 19:43:36 INFO storage.MemoryStore: Block rdd_323_43 of size 199481138 dropped from memory (free 8579469803)\n12/12/14 19:43:36 INFO executor.StandaloneExecutorBackend: Got assigned task 76480\n12/12/14 19:43:36 INFO executor.Executor: Running task ID 76480\n12/12/14 19:43:36 INFO executor.Executor: Its generation is 325\n12/12/14 19:43:36 INFO spark.CacheTracker: Cache key is rdd_645_43\n12/12/14 19:43:36 INFO storage.BlockManager: Getting local block rdd_645_43\n12/12/14 19:43:36 INFO storage.BlockManager: Getting remote block rdd_645_43\n12/12/14 19:43:36 INFO spark.CacheTracker: Computing partition spark.ParallelCollectionSplit@6de0\n12/12/14 19:43:39 INFO storage.MemoryStore: ensureFreeSpace(199481138) called with curMem=0, maxMem=8579469803\n12/12/14 19:43:39 INFO storage.MemoryStore: Block rdd_645_43 stored as values to memory (estimated size 190.2 MB, free 7.8 GB)\n{code}\n\nMy job looked something like this:\n\n{code}\nval samples = sc.parallelize(1 to numMachines, numMachines).flatMap(_ => generate_pairs)\nsamples.cache()\nsamples.count() // Force evaluation\n\n// In a loop:\nsamples.groupByKey().count()\n{code}\n\nThe real code used preshuffle() and the CoalescedRDD class in my skew-handling branch instead of performing the regular groupByKey().\n\nIt looks like a partition of {{samples}} was lost and recomputed.  Following this, the resubmitted tasks were ShuffleMapTasks; the master log said that they correspond to the flatMap() line, so they are shuffling the recomputed {{samples}} partition.  However, it looks like these shuffle blocks are already in the BlockManager:\n\n{code}\n12/12/14 19:43:39 INFO executor.StandaloneExecutorBackend: Got assigned task 76530\n12/12/14 19:43:39 INFO executor.Executor: Running task ID 76530\n12/12/14 19:43:39 INFO executor.Executor: Its generation is 325\n12/12/14 19:43:39 INFO spark.CacheTracker: Cache key is rdd_645_43\n12/12/14 19:43:39 INFO storage.BlockManager: Getting local block rdd_645_43\n12/12/14 19:43:39 INFO spark.CacheTracker: Found partition in cache!\n12/12/14 19:43:43 INFO executor.Executor: Serialized size of result for 76530 is 184\n12/12/14 19:43:43 INFO executor.Executor: Finished task ID 76530\n12/12/14 19:43:43 INFO executor.StandaloneExecutorBackend: Got assigned task 76538\n12/12/14 19:43:43 INFO executor.Executor: Running task ID 76538\n12/12/14 19:43:43 INFO executor.Executor: Its generation is 325\n12/12/14 19:43:43 INFO spark.CacheTracker: Cache key is rdd_645_43\n12/12/14 19:43:43 INFO storage.BlockManager: Getting local block rdd_645_43\n12/12/14 19:43:43 INFO spark.CacheTracker: Found partition in cache!\n12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_0 already exists on this machine; not re-adding it\n12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_1 already exists on this machine; not re-adding it\n12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_2 already exists on this machine; not re-adding it\n12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_3 already exists on this machine; not re-adding it\n12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_4 already exists on this machine; not re-adding it\n12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_5 already exists on this machine; not re-adding it\n12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_6 already exists on this machine; not re-adding it\n12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_7 already exists on this machine; not re-adding it\n12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_8 already exists on this machine; not re-adding it\n...\n{code}\n\nMy theory:\n\n* A block being dropped somehow results in shuffle blocks on the same machine being marked as missing in the master's MapOutputTracker (or some other bookkeeping structure on the master).\n* This causes the master to resubmit tasks, because blocks required by the next phase seem to be missing.\n* The blocks were never actually lost, so they cannot be recomputed.\n* The master is not notified of the existence of these blocks, because put() returns before updating the master if the block already exists.\n* This leads to an infinite loop.", "On closer inspection of the master logs, it looks like the looping machines were marked as dead hosts, causing the master CacheTracker to mark their caches as lost.  It looks like the dead host comes back, causing tasks to be scheduled on it:\n\n{code}\n12/12/14 19:23:37 INFO cluster.TaskSetManager: Lost TID 38348 (task 484.0:37)\n12/12/14 19:23:37 INFO cluster.TaskSetManager: Loss was due to fetch failure from BlockManagerId(ip-10-12-129-38, 52252)\ntaskLaunchOverhead is 0 milliseconds\ntaskLaunchOverhead is 0 milliseconds\n12/12/14 19:23:37 INFO scheduler.DAGScheduler: Marking Stage 484 (CoalescedShuffleFetcherRDD at SkewBenchmark.scala:77) for resubmision due to a fetch failure\n12/12/14 19:23:37 INFO scheduler.DAGScheduler: The failed fetch was from Stage 483 (flatMap at SkewBenchmark.scala:43); marking it for resubmission\n12/12/14 19:23:37 INFO scheduler.DAGScheduler: Host lost: ip-10-12-129-38\n12/12/14 19:23:37 INFO storage.BlockManagerMasterActor: Trying to remove the host: ip-10-12-129-38 from BlockManagerMaster.\n12/12/14 19:23:37 INFO storage.BlockManagerMasterActor: Previous hosts: ArrayBuffer(BlockManagerId(ip-10-144-65-13, 57952), BlockManagerId(ip-10-12-130-28, 38283), BlockManagerId(ip-10-152-157-13, 57580), BlockManagerId(ip-10-60-97-229, 60271), BlockManagerId(ip-10-152-161-81, 51219), BlockManagerId(ip-10-12-130-82, 37488), BlockManagerId(ip-10-144-141-107, 51761), BlockManagerId(ip-10-152-155-24, 44270), BlockManagerId(ip-10-152-146-166, 44224), BlockManagerId(ip-10-152-150-78, 34278), BlockManagerId(ip-10-145-205-182, 50214), BlockManagerId(ip-10-152-151-66, 59577), BlockManagerId(ip-10-145-205-217, 47570), BlockManagerId(ip-10-145-179-146, 55921), BlockManagerId(ip-10-144-72-168, 37861), BlockManagerId(ip-10-152-166-209, 35915), BlockManagerId(ip-10-145-203-221, 60217), BlockManagerId(ip-10-144-140-91, 48968), BlockManagerId(ip-10-152-148-80, 57024), BlockManagerId(ip-10-144-132-152, 42273), BlockManagerId(ip-10-147-128-53, 44072), BlockManagerId(ip-10-152-146-72, 50587), BlockManagerId(ip-10-60-99-32, 53933), BlockManagerId(ip-10-145-212-30, 58755), BlockManagerId(ip-10-147-130-32, 34711), BlockManagerId(ip-10-152-150-104, 60430), BlockManagerId(ip-10-152-156-131, 40720), BlockManagerId(ip-10-152-148-92, 43706), BlockManagerId(ip-10-147-129-249, 45660), BlockManagerId(ip-10-12-129-38, 52252), BlockManagerId(ip-10-152-151-151, 51417), BlockManagerId(ip-10-145-203-198, 44413), BlockManagerId(ip-10-152-159-35, 41803), BlockManagerId(ip-10-145-203-17, 53531), BlockManagerId(ip-10-147-129-235, 40164), BlockManagerId(ip-10-60-99-168, 54819), BlockManagerId(ip-10-147-129-219, 38611), BlockManagerId(ip-10-152-165-130, 46365), BlockManagerId(ip-10-144-135-87, 50616), BlockManagerId(ip-10-152-138-241, 45156), BlockManagerId(ip-10-152-146-74, 45560), BlockManagerId(ip-10-145-230-152, 39402), BlockManagerId(ip-10-152-157-159, 58826), BlockManagerId(ip-10-10-189-14, 39648), BlockManagerId(ip-10-152-156-104, 42281), BlockManagerId(ip-10-144-65-37, 36194), BlockManagerId(ip-10-152-148-69, 48816), BlockManagerId(ip-10-62-94-62, 41136), BlockManagerId(ip-10-60-71-25, 42670), BlockManagerId(ip-10-144-65-33, 36063))\n12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38350 because its task set is gone\ntaskLaunchOverhead is 0 milliseconds\n12/12/14 19:23:37 INFO storage.BlockManagerMasterActor: Current hosts: ArrayBuffer(BlockManagerId(ip-10-144-65-13, 57952), BlockManagerId(ip-10-12-130-28, 38283), BlockManagerId(ip-10-152-157-13, 57580), BlockManagerId(ip-10-60-97-229, 60271), BlockManagerId(ip-10-152-161-81, 51219), BlockManagerId(ip-10-12-130-82, 37488), BlockManagerId(ip-10-144-141-107, 51761), BlockManagerId(ip-10-152-155-24, 44270), BlockManagerId(ip-10-152-146-166, 44224), BlockManagerId(ip-10-152-150-78, 34278), BlockManagerId(ip-10-145-205-182, 50214), BlockManagerId(ip-10-152-151-66, 59577), BlockManagerId(ip-10-145-205-217, 47570), BlockManagerId(ip-10-145-179-146, 55921), BlockManagerId(ip-10-144-72-168, 37861), BlockManagerId(ip-10-152-166-209, 35915), BlockManagerId(ip-10-145-203-221, 60217), BlockManagerId(ip-10-144-140-91, 48968), BlockManagerId(ip-10-152-148-80, 57024), BlockManagerId(ip-10-144-132-152, 42273), BlockManagerId(ip-10-147-128-53, 44072), BlockManagerId(ip-10-152-146-72, 50587), BlockManagerId(ip-10-60-99-32, 53933), BlockManagerId(ip-10-145-212-30, 58755), BlockManagerId(ip-10-147-130-32, 34711), BlockManagerId(ip-10-152-150-104, 60430), BlockManagerId(ip-10-152-156-131, 40720), BlockManagerId(ip-10-152-148-92, 43706), BlockManagerId(ip-10-147-129-249, 45660), BlockManagerId(ip-10-152-151-151, 51417), BlockManagerId(ip-10-145-203-198, 44413), BlockManagerId(ip-10-152-159-35, 41803), BlockManagerId(ip-10-145-203-17, 53531), BlockManagerId(ip-10-147-129-235, 40164), BlockManagerId(ip-10-60-99-168, 54819), BlockManagerId(ip-10-147-129-219, 38611), BlockManagerId(ip-10-152-165-130, 46365), BlockManagerId(ip-10-144-135-87, 50616), BlockManagerId(ip-10-152-138-241, 45156), BlockManagerId(ip-10-152-146-74, 45560), BlockManagerId(ip-10-145-230-152, 39402), BlockManagerId(ip-10-152-157-159, 58826), BlockManagerId(ip-10-10-189-14, 39648), BlockManagerId(ip-10-152-156-104, 42281), BlockManagerId(ip-10-144-65-37, 36194), BlockManagerId(ip-10-152-148-69, 48816), BlockManagerId(ip-10-62-94-62, 41136), BlockManagerId(ip-10-60-71-25, 42670), BlockManagerId(ip-10-144-65-33, 36063))\n12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38329 because its task set is gone\ntaskLaunchOverhead is 0 milliseconds\n12/12/14 19:23:37 INFO storage.BlockManagerMaster: Removed ip-10-12-129-38 successfully in notifyADeadHost\n12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38341 because its task set is gone\ntaskLaunchOverhead is 0 milliseconds\n12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38360 because its task set is gone\ntaskLaunchOverhead is 0 milliseconds\n12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38351 because its task set is gone\ntaskLaunchOverhead is 0 milliseconds\n12/12/14 19:23:37 INFO spark.CacheTrackerActor: Memory cache lost on ip-10-12-129-38\n12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38334 because its task set is gone\ntaskLaunchOverhead is 0 milliseconds\n12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38323 because its task set is gone\ntaskLaunchOverhead is 0 milliseconds\n12/12/14 19:23:37 INFO spark.CacheTracker: CacheTracker successfully removed entries on ip-10-12-129-38\n12/12/14 19:23:37 INFO spark.CacheTrackerActor: Asked for current cache locations\n12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38327 because its task set is gone\ntaskLaunchOverhead is 0 milliseconds\n12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38333 because its task set is gone\ntaskLaunchOverhead is 0 milliseconds\n12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38333 because its task set is gone\ntaskLaunchOverhead is 0 milliseconds\n12/12/14 19:23:37 INFO scheduler.DAGScheduler: Resubmitting failed stages\n12/12/14 19:23:37 INFO spark.CacheTrackerActor: Asked for current cache locations\n12/12/14 19:23:37 INFO scheduler.DAGScheduler: Submitting Stage 483 (flatMap at SkewBenchmark.scala:43), which has no missing parents\n12/12/14 19:23:37 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from Stage 483\n12/12/14 19:23:37 INFO cluster.ClusterScheduler: Adding task set 483.1 with 1 tasks\n12/12/14 19:23:37 INFO cluster.TaskSetManager: Starting task 483.1:0 as TID 38361 on slave worker-20121214185650-ip-10-144-65-37-40073: ip-10-144-65-37 (preferred)\n12/12/14 19:23:37 INFO cluster.TaskSetManager: Serialized task 483.1:0 as 1398 bytes in 0 ms\ntaskLaunchOverhead is 0 milliseconds\n12/12/14 19:23:38 INFO cluster.ClusterScheduler: Ignoring update from TID 38346 because its task set is gone\ntaskLaunchOverhead is 0 milliseconds\n12/12/14 19:23:38 INFO cluster.ClusterScheduler: Ignoring update from TID 38332 because its task set is gone\ntaskLaunchOverhead is 0 milliseconds\n12/12/14 19:23:38 INFO cluster.ClusterScheduler: Ignoring update from TID 38352 because its task set is gone\ntaskLaunchOverhead is 0 milliseconds\n12/12/14 19:23:38 INFO storage.BlockManagerMasterActor: Registering block manager ip-10-12-129-38:52252 with 8.0 GB RAM\n12/12/14 19:23:38 INFO storage.BlockManagerMasterActor: Added rdd_323_39 in memory on ip-10-12-129-38:52252 (size: 190.2 MB, free: 7.8 GB)\n{code}\n\nWhen the DAGScheduler handles task completion, it does not store records of output locations that are on dead hosts:\n{code}\n          case smt: ShuffleMapTask =>                                           \n            val stage = idToStage(smt.stageId)                                  \n            val status = event.result.asInstanceOf[MapStatus]                   \n            val host = status.address.ip                                        \n            logInfo(\"ShuffleMapTask finished with host \" + host)                \n            if (!deadHosts.contains(host)) {   // TODO: Make sure hostnames are consistent with Mesos\n              stage.addOutputLoc(smt.partition, status)                         \n            }  \n{code}\n\nHosts are permanently marked as dead, so this leads to an infinite loop as tasks scheduled on those machines can never register their output locations.\n\nIt looks like the solution is to properly handle the return of dead hosts.", "Charles may have recently fixed this in https://github.com/mesos/spark/pull/317. Talk with him to see whether it handles this case.", "My branch already includes Charles' pull request.  His code allows the dead node's block manager to reconnect (which actually takes place in the above log), but doesn't address my specific problem.\n\nThe problem is that the dead node's hostname is never removed from DAGScheduler's {{deadHosts}} set, even if that node returns.", "It looks like Charles fixed this in https://github.com/mesos/spark/pull/408, which made it into 0.7.0."], "derived": {"summary": "Summary: failures in BlockStore may lead to infinite loops of task failures. I ran into a situation where a block manager operation failed:\n{code}\n12/10/20 21:25:13 ERROR storage.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Failures in BlockStore may lead to infinite loops of task failures - Summary: failures in BlockStore may lead to infinite loops of task failures. I ran into a situation where a block manager operation failed:\n{code}\n12/10/20 21:25:13 ERROR storage."}, {"q": "What updates or decisions were made in the discussion?", "a": "It looks like Charles fixed this in https://github.com/mesos/spark/pull/408, which made it into 0.7.0."}]}}
{"project": "SPARK", "issue_id": "SPARK-584", "title": "Pass slave ip address when starting a cluster ", "status": "Resolved", "priority": "Minor", "reporter": null, "assignee": null, "labels": [], "created": "2012-10-22T14:39:08.000+0000", "updated": "2016-01-05T21:13:57.000+0000", "description": "Pass slave ip address from conf while starting a cluster:\n\nbin/start-slaves.sh is used to start all the slaves in the cluster. While the slave class takes a --ip argument, we don't pass the ip address from the conf/slaves. ", "comments": ["I've created a patch to fix this issue.\n\nA brief explanation:\nI modified \"slaves.sh\" to support a special placeholder string of \"\\_\\_SLAVE\\_IP\\_\\_\", which will be convert to the real slave ip when execute cmd on each slaves.\n\nWith such enhancement, the rest works should be trivial, just add \"--ip \\_\\_SLAVE\\_IP\\_\\_\" into the execution cmdline of \"start-salves.sh\".", "what's the use case for this?", "Resolving as \"incomplete.\" Please submit a PR if this is still an issue."], "derived": {"summary": "Pass slave ip address from conf while starting a cluster:\n\nbin/start-slaves. sh is used to start all the slaves in the cluster.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Pass slave ip address when starting a cluster  - Pass slave ip address from conf while starting a cluster:\n\nbin/start-slaves. sh is used to start all the slaves in the cluster."}, {"q": "What updates or decisions were made in the discussion?", "a": "Resolving as \"incomplete.\" Please submit a PR if this is still an issue."}]}}
{"project": "SPARK", "issue_id": "SPARK-585", "title": "Mesos may not work with mesos:// URLs", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "labels": [], "created": "2012-10-23T12:01:53.000+0000", "updated": "2012-11-21T11:43:07.000+0000", "description": "According to some users. I guess we should strip the mesos:// at the front.", "comments": [], "derived": {"summary": "According to some users. I guess we should strip the mesos:// at the front.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Mesos may not work with mesos:// URLs - According to some users. I guess we should strip the mesos:// at the front."}]}}
{"project": "SPARK", "issue_id": "SPARK-586", "title": "Update docs to say that you need to set MESOS_NATIVE_LIBRARY when running standalone apps on Mesos", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "2012-10-23T12:02:27.000+0000", "updated": "2012-10-24T10:01:52.000+0000", "description": null, "comments": ["Duplicates SPARK-589"], "derived": {"summary": "", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Update docs to say that you need to set MESOS_NATIVE_LIBRARY when running standalone apps on Mesos"}, {"q": "What updates or decisions were made in the discussion?", "a": "Duplicates SPARK-589"}]}}
{"project": "SPARK", "issue_id": "SPARK-587", "title": "Test issue", "status": "Closed", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "2012-10-23T21:53:27.000+0000", "updated": "2012-10-23T21:56:54.000+0000", "description": null, "comments": ["Test comment"], "derived": {"summary": "", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Test issue"}, {"q": "What updates or decisions were made in the discussion?", "a": "Test comment"}]}}
{"project": "SPARK", "issue_id": "SPARK-588", "title": "Have assembly example in quick start, or elsewhere in docs", "status": "Closed", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "labels": [], "created": "2012-10-24T09:47:53.000+0000", "updated": "2012-12-12T14:02:09.000+0000", "description": null, "comments": ["This already exists in \"linking with Spark\" I think that's probably the best place for this (might be too much for the quickstart). So I'm closing."], "derived": {"summary": "", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Have assembly example in quick start, or elsewhere in docs"}, {"q": "What updates or decisions were made in the discussion?", "a": "This already exists in \"linking with Spark\" I think that's probably the best place for this (might be too much for the quickstart). So I'm closing."}]}}
{"project": "SPARK", "issue_id": "SPARK-589", "title": "MESOS_NATIVE_LIBRARY env var needs to be set when running on Mesos", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": null, "labels": [], "created": "2012-10-24T09:57:13.000+0000", "updated": "2013-08-10T16:31:04.000+0000", "description": null, "comments": ["This was fixed a while ago."], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "MESOS_NATIVE_LIBRARY env var needs to be set when running on Mesos"}, {"q": "What updates or decisions were made in the discussion?", "a": "This was fixed a while ago."}]}}
{"project": "SPARK", "issue_id": "SPARK-590", "title": "Log task size when it's too large on master", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": null, "labels": ["Starter"], "created": "2012-10-24T09:58:27.000+0000", "updated": "2013-12-07T13:04:53.000+0000", "description": "For now, log once per job if the closure is > 100Kb. ", "comments": ["Fixed in https://github.com/apache/incubator-spark/pull/207"], "derived": {"summary": "For now, log once per job if the closure is > 100Kb.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Log task size when it's too large on master - For now, log once per job if the closure is > 100Kb."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed in https://github.com/apache/incubator-spark/pull/207"}]}}
{"project": "SPARK", "issue_id": "SPARK-591", "title": "Configuration System", "status": "Closed", "priority": "Blocker", "reporter": "Denny Britz", "assignee": "Denny Britz", "labels": [], "created": "2012-10-24T15:57:16.000+0000", "updated": "2012-10-24T16:20:29.000+0000", "description": "Use a SparkConf object instead of system properties. Also, dont rely on SPARK_MEM environment variable.\n", "comments": ["Duplicate of https://spark-project.atlassian.net/browse/SPARK-544."], "derived": {"summary": "Use a SparkConf object instead of system properties. Also, dont rely on SPARK_MEM environment variable.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Configuration System - Use a SparkConf object instead of system properties. Also, dont rely on SPARK_MEM environment variable."}, {"q": "What updates or decisions were made in the discussion?", "a": "Duplicate of https://spark-project.atlassian.net/browse/SPARK-544."}]}}
{"project": "SPARK", "issue_id": "SPARK-592", "title": "Total memory size on per-RDD basis", "status": "Resolved", "priority": "Minor", "reporter": "Denny Britz", "assignee": null, "labels": [], "created": "2012-10-24T15:58:10.000+0000", "updated": "2014-09-21T18:18:53.000+0000", "description": "Expose total memory size on a per-RDD basis.", "comments": [], "derived": {"summary": "Expose total memory size on a per-RDD basis.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Total memory size on per-RDD basis - Expose total memory size on a per-RDD basis."}]}}
{"project": "SPARK", "issue_id": "SPARK-593", "title": "Send last few lines of failed standalone mode or Mesos task to master", "status": "Resolved", "priority": "Minor", "reporter": "Denny Britz", "assignee": "Denny Britz", "labels": [], "created": "2012-10-24T15:58:43.000+0000", "updated": "2016-01-16T13:00:13.000+0000", "description": "Send last few lines of failed standalone mode or Mesos task to master.", "comments": [], "derived": {"summary": "Send last few lines of failed standalone mode or Mesos task to master.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Send last few lines of failed standalone mode or Mesos task to master - Send last few lines of failed standalone mode or Mesos task to master."}]}}
{"project": "SPARK", "issue_id": "SPARK-594", "title": "Update examples to pass JARs when building a SparkContext in 0.6 and master", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "labels": [], "created": "2012-10-24T21:54:17.000+0000", "updated": "2013-02-25T20:30:12.000+0000", "description": "The \"right\" way to create a SparkContext is to pass a list of JARs and a sparkHome to the constructor, but our examples still use the older 2-parameter constructor that doesn't take these parameters, and rely on the spark.examples code being present on every worker node's classpath. For some reason a commit fixing this was in branch 0.5 but never made it into 0.6 and master.", "comments": ["Fixed in https://github.com/mesos/spark/commit/5d7b591cfe14177f083814fe3e81745c5d279810"], "derived": {"summary": "The \"right\" way to create a SparkContext is to pass a list of JARs and a sparkHome to the constructor, but our examples still use the older 2-parameter constructor that doesn't take these parameters, and rely on the spark. examples code being present on every worker node's classpath.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Update examples to pass JARs when building a SparkContext in 0.6 and master - The \"right\" way to create a SparkContext is to pass a list of JARs and a sparkHome to the constructor, but our examples still use the older 2-parameter constructor that doesn't take these parameters, and rely on the spark. examples code being present on every worker node's classpath."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed in https://github.com/mesos/spark/commit/5d7b591cfe14177f083814fe3e81745c5d279810"}]}}
{"project": "SPARK", "issue_id": "SPARK-595", "title": "Document \"local-cluster\" mode", "status": "Resolved", "priority": "Minor", "reporter": "Josh Rosen", "assignee": "Yuto Akutsu", "labels": [], "created": "2012-10-26T15:26:45.000+0000", "updated": "2021-08-06T14:29:22.000+0000", "description": "The 'Spark Standalone Mode' guide describes how to manually launch a standalone cluster, which can be done locally for testing, but it does not mention SparkContext's `local-cluster` option.\n\nWhat are the differences between these approaches?  Which one should I prefer for local testing?  Can I still use the standalone web interface if I use 'local-cluster' mode?\n\nIt would be useful to document this.", "comments": ["local-cluster is only for unit tests, to emulate a distributed cluster in a single JVM (it will still launch executors in separate ones, but at least you won't need one JVM per slave). For user applications there's no reason not to run the normal \"local\". So this is actually why I didn't include it in the doc, because the name can confuse people.", "Thanks for the explanation; I'm closing this as \"Won't Fix\".", "Maybe we should revisit this and actually document this; it could be useful for users who run Spark on a single machine with a large amount of memory (say 512 gigabytes, for example).  I think it's also been recommended on the mailing list a few times.", "I agree with Josh.  This is really useful for running tests and simulating conditions that cannot be simulated in the local[n] node, namely serialization issues and multi-JVM issues.", "+1 for reopen", "I've re-opened this issue.  Folks are using the API in the wild and we're not going to break compatibility for it, so we should document it.", "+1 We are using for internal testing to ensure that our kryo serialization works.", "Hey Justin, do you want to submit a PR for this?  I'll help review.", "Sure, I'll get to it after I finish some tasks for work =)", "User 'yutoacts' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/33537"], "derived": {"summary": "The 'Spark Standalone Mode' guide describes how to manually launch a standalone cluster, which can be done locally for testing, but it does not mention SparkContext's `local-cluster` option. What are the differences between these approaches?  Which one should I prefer for local testing?  Can I still use the standalone web interface if I use 'local-cluster' mode?\n\nIt would be useful to document this.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Document \"local-cluster\" mode - The 'Spark Standalone Mode' guide describes how to manually launch a standalone cluster, which can be done locally for testing, but it does not mention SparkContext's `local-cluster` option. What are the differences between these approaches?  Which one should I prefer for local testing?  Can I still use the standalone web interface if I use 'local-cluster' mode?\n\nIt would be useful to document this."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'yutoacts' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/33537"}]}}
{"project": "SPARK", "issue_id": "SPARK-596", "title": "Memory Dashboard", "status": "Resolved", "priority": "Major", "reporter": "Denny Britz", "assignee": "Denny Britz", "labels": [], "created": "2012-10-27T11:52:39.000+0000", "updated": "2013-05-04T22:59:40.000+0000", "description": "Provide a dashboard that gives a view into the BlockManager and RDD storage.", "comments": ["This was added in https://github.com/mesos/spark/pull/401 and released in 0.7.0."], "derived": {"summary": "Provide a dashboard that gives a view into the BlockManager and RDD storage.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Memory Dashboard - Provide a dashboard that gives a view into the BlockManager and RDD storage."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was added in https://github.com/mesos/spark/pull/401 and released in 0.7.0."}]}}
{"project": "SPARK", "issue_id": "SPARK-597", "title": "HashPartitioner incorrectly partitions RDD[Array[_]]", "status": "Resolved", "priority": "Major", "reporter": "Josh Rosen", "assignee": "Josh Rosen", "labels": [], "created": "2012-10-28T22:51:45.000+0000", "updated": "2014-10-08T22:27:50.000+0000", "description": "Java arrays have {{hashCodes}} that are based on the arrays' identities rather than their contents [1].  As a result, attempting to partition an {{RDD[Array[_]]}} using a {{HashPartitioner}} will produce an unexpected/incorrect result.\n\nThis was the cause of a bug in PySpark, where I hash partition PairRDDs with {{Array[Byte]}} keys.  In PySpark, I fixed this by using a custom {{Partitioner}} that calls {{Arrays.hashCode(byte[])}} when passed an {{Array[Byte]}} [2].\n\nI would like to address this issue more generally in Spark.\n\nWe could add logic to {{HashPartitioner}} to perform special handling for arrays, but I'm not sure whether the additional branching would add a significant performance overhead.  The logic could become messy because the {{Arrays}} module defines {{Arrays.hashCode()}} for primitive arrays and {{Arrays.deepHashCode()}} for Object arrays.  Perhaps Guava or Apache Commons has an implementation of this.\n\nAn alternative would be to keep the current {{HashPartitioner}} and add logic to print warnings (or to fail with an error) when shuffling an {{RDD[Array[_]]}} using the default {{HashPartitioner}}.\n\n\n[1] http://stackoverflow.com/questions/744735/java-array-hashcode-implementation\n[2] https://github.com/JoshRosen/spark/commit/2ccf3b665280bf5b0919e3801d028126cb070dbd", "comments": ["There might be similar problems if arrays are used as {{Map}} keys or are stored in a {{Set}}.", "This is a good catch, but it seems tough to do the right thing automatically in HashPartitioner, as you said. I think one interim fix is to add an ArrayPartitioner (or even ByteArrayPartitioner) and ask people to use that when their keys are byte arrays. I guess this is what you did in PySpark? We might be able to warn people of this condition in the PairRDDFunctions constructor, where we have a ClassManifest for the key K.\n\nAnother option would be to choose the partitioner automatically based on the type of K, but let's try the other approach first and see whether there are any implications.", "If we add ArrayPartitioner or ByteArrayPartitioner, then it might be useful to perform type checking to prevent mistakes like using a {{ByteArrayPartitioner}} on an RDD with integer keys.  We might do this by adding a type parameter {{K}} to {{Partitioner}}.\n\nThe {{partitioner}} field is defined for all RDDs, not just key-value RDDs, so its type would have to be {{Partitioner[Any]}}.  However, {{partitioner}} is a {{val}}, so it only matters that a partitioner of the right type is passed in the RDD's constructor.\n\nI would declare {{Partitioner[-K]}} to be contravariant in the key type, so that a {{Partitioner[Any]}} can be used with an {{RDD[Array[Byte]]}} but a {{Partitioner[Array[Byte]]}} cannot be used with an {{RDD[Int]}}.\n\nIf we take this approach, then we might want to split this into two commits: one that adds a warning when using {{HashPartitioner}} with arrays of any kind, and another that changes {{Partition}} and adds the type parameters.  This would retain backwards-compatibility for the existing releases, while allowing us to provide better type checking in future releases.\n\nShould I take a stab at this and submit a pull request?", "Also, array keys will cause problems with map-side combiners, which use RDD keys as {{HashMap}} keys.\n\nIs it worth supporting map-side combiners with array keys?  If not, an easy solution would be to fail with an error.  We could probably detect the error before the job ever runs.  This would require separate patches for the 0.5/0.6 and 0.7 branches, due to my shuffle refactoring changes in 0.7.\n", "On reflection, it's probably a bad idea to use contravariant types in user-facing APIs because they could cause issues in the Java API.\n\nI submitted a pull request that causes Spark to throw exceptions in cases where we know that hashing arrays will produce incorrect results: https://github.com/mesos/spark/pull/348", "Merged this into 0.6 as well."], "derived": {"summary": "Java arrays have {{hashCodes}} that are based on the arrays' identities rather than their contents [1]. As a result, attempting to partition an {{RDD[Array[_]]}} using a {{HashPartitioner}} will produce an unexpected/incorrect result.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "HashPartitioner incorrectly partitions RDD[Array[_]] - Java arrays have {{hashCodes}} that are based on the arrays' identities rather than their contents [1]. As a result, attempting to partition an {{RDD[Array[_]]}} using a {{HashPartitioner}} will produce an unexpected/incorrect result."}, {"q": "What updates or decisions were made in the discussion?", "a": "Merged this into 0.6 as well."}]}}
{"project": "SPARK", "issue_id": "SPARK-598", "title": "Test PySpark's Java-side pickling of arrays of key-value pairs", "status": "Resolved", "priority": "Major", "reporter": "Josh Rosen", "assignee": "Josh Rosen", "labels": [], "created": "2012-10-29T23:53:45.000+0000", "updated": "2013-02-04T10:45:47.000+0000", "description": "There may be a bug in the PySpark code that packs {{Array[(Array[Byte], Array[Byte]]}} into a pickled object.  This particular code path isn't exercised by the current PySpark implementation, but it may lead to strange results when collecting certain internal RDDs in Python (while debugging PySpark itself, for example).", "comments": ["PySpark's Java-side pickling code has changed significantly and the code affected by this issue was removed, so I'm closing this."], "derived": {"summary": "There may be a bug in the PySpark code that packs {{Array[(Array[Byte], Array[Byte]]}} into a pickled object. This particular code path isn't exercised by the current PySpark implementation, but it may lead to strange results when collecting certain internal RDDs in Python (while debugging PySpark itself, for example).", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Test PySpark's Java-side pickling of arrays of key-value pairs - There may be a bug in the PySpark code that packs {{Array[(Array[Byte], Array[Byte]]}} into a pickled object. This particular code path isn't exercised by the current PySpark implementation, but it may lead to strange results when collecting certain internal RDDs in Python (while debugging PySpark itself, for example)."}, {"q": "What updates or decisions were made in the discussion?", "a": "PySpark's Java-side pickling code has changed significantly and the code affected by this issue was removed, so I'm closing this."}]}}
{"project": "SPARK", "issue_id": "SPARK-599", "title": "OutOfMemoryErrors can cause workers to hang indefinitely", "status": "Resolved", "priority": "Major", "reporter": "Josh Rosen", "assignee": null, "labels": [], "created": "2012-10-30T22:06:44.000+0000", "updated": "2012-11-26T12:32:39.000+0000", "description": "While running Shark with an insufficient number of reduce tasks, an overloaded worker machine raised {{java.lang.OutOfMemoryError : GC overhead limit exceeded}}.  This caused that Java process to hang at 100% CPU, spending all of its time in the garbage collector.  This failure wasn't detected by the master, causing the entire job to hang.\n\nHandling and reporting failures due to {{OutOfMemoryError}} can be complicated because the {{OutOfMemoryError}} exception can be raised at many different locations, depending on which allocation caused the error.\n\nI'm not sure that it's safe to recover from {{OutOfMemoryError}}, so worker processes should probably die once they raise that error.  We might be able to do this in an uncaught exception handler.", "comments": ["I think we might want to deal with this using a timeout instead of trying to catch the error. Do you know whether any threads continue running at all when there's an OOM? Somehow I doubt it.", "Here's a forced stack trace from the hanging worker: https://gist.github.com/3984822.  Looks like all threads are blocked, but I don't know if this always happens.\n\nCould we take both approaches, killing the worker if we catch OOM while using a watchdog timeout in case that fails?", "Did https://github.com/mesos/spark/pull/305 fix this?", "Good point, I think it does. Going to close it."], "derived": {"summary": "While running Shark with an insufficient number of reduce tasks, an overloaded worker machine raised {{java. lang.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "OutOfMemoryErrors can cause workers to hang indefinitely - While running Shark with an insufficient number of reduce tasks, an overloaded worker machine raised {{java. lang."}, {"q": "What updates or decisions were made in the discussion?", "a": "Good point, I think it does. Going to close it."}]}}
{"project": "SPARK", "issue_id": "SPARK-600", "title": "SparkContext.stop and clearJars delete local JAR files", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "2012-10-31T14:43:25.000+0000", "updated": "2014-11-06T06:58:50.000+0000", "description": "If you happen to pass a JAR that's in your current working directory to SparkContext, clearJars() will delete it. I'm not exactly sure why it's deleting files to begin with (maybe it was meant to deal with JAR files that are somehow copied in local mode?) but it's certainly not something that should be done in SparkContext.", "comments": ["Should no longer be a problem since 1.0"], "derived": {"summary": "If you happen to pass a JAR that's in your current working directory to SparkContext, clearJars() will delete it. I'm not exactly sure why it's deleting files to begin with (maybe it was meant to deal with JAR files that are somehow copied in local mode?) but it's certainly not something that should be done in SparkContext.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "SparkContext.stop and clearJars delete local JAR files - If you happen to pass a JAR that's in your current working directory to SparkContext, clearJars() will delete it. I'm not exactly sure why it's deleting files to begin with (maybe it was meant to deal with JAR files that are somehow copied in local mode?) but it's certainly not something that should be done in SparkContext."}, {"q": "What updates or decisions were made in the discussion?", "a": "Should no longer be a problem since 1.0"}]}}
{"project": "SPARK", "issue_id": "SPARK-601", "title": "PairRDDFunctions.lookup fails unnecessarily when self.partitioner is None", "status": "Resolved", "priority": "Minor", "reporter": null, "assignee": "Mark Hamstra", "labels": [], "created": "2012-10-31T16:02:31.000+0000", "updated": "2013-08-06T23:20:00.000+0000", "description": "If a lookup(k) is attempted on an RDD[(K, V)] with no partitioner, an UnsupportedOperationException is thrown even when the operation should succeed:\n\n  scala> val rdd = sc.parallelize(List((1, 'a'), (1, 'b'), (2, 'c')))\n  rdd: spark.RDD[(Int, Char)] = spark.ParallelCollection@73f4117b\n\n  scala> rdd.lookup(1)\n  java.lang.UnsupportedOperationException: lookup() called on an RDD without a partitioner\n  \t  at spark.rdd.PairRDDFunctions.lookup(PairRDDFunctions.scala:315)\n\nAt a minimum, a filter.map.collect over the whole RDD works when the optimized path using a partitioner is not available:\n\n         case None =>\n-          throw new UnsupportedOperationException(\"lookup() called on an RDD without a partitioner\")\n+          self.filter(kv => kv._1 == key).map(kv => kv._2).collect", "comments": [], "derived": {"summary": "If a lookup(k) is attempted on an RDD[(K, V)] with no partitioner, an UnsupportedOperationException is thrown even when the operation should succeed:\n\n  scala> val rdd = sc. parallelize(List((1, 'a'), (1, 'b'), (2, 'c')))\n  rdd: spark.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "PairRDDFunctions.lookup fails unnecessarily when self.partitioner is None - If a lookup(k) is attempted on an RDD[(K, V)] with no partitioner, an UnsupportedOperationException is thrown even when the operation should succeed:\n\n  scala> val rdd = sc. parallelize(List((1, 'a'), (1, 'b'), (2, 'c')))\n  rdd: spark."}]}}
{"project": "SPARK", "issue_id": "SPARK-602", "title": "Pass integer hashcodes to Java during PySpark hash partitioning", "status": "Closed", "priority": "Major", "reporter": "Josh Rosen", "assignee": null, "labels": [], "created": "2012-11-01T09:50:27.000+0000", "updated": "2013-10-07T12:15:17.000+0000", "description": "PySpark's current hash partitioning works by returning a sequence of serialized keys and values.  The resulting {{JavaRDD[Array[Byte]]}} is grouped into pairs, treating the serialized Python keys as the pairs' keys.\n\nThis hashes each key twice (once in Python and once in Java), but we could return integers from Python and avoid extra hashing (using a custom partitioner to treat the integer key itself as the hash bucket number, since we know the proper number of buckets in Python).  This would avoid hashing byte arrays.\n\nThis could be implemented cleanly by restructuring the Python code around two execution methods that resemble {{ShuffleMapTask}} and {{ResultTask}}.", "comments": ["Should be solved by PR #33 so I am closing this."], "derived": {"summary": "PySpark's current hash partitioning works by returning a sequence of serialized keys and values. The resulting {{JavaRDD[Array[Byte]]}} is grouped into pairs, treating the serialized Python keys as the pairs' keys.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Pass integer hashcodes to Java during PySpark hash partitioning - PySpark's current hash partitioning works by returning a sequence of serialized keys and values. The resulting {{JavaRDD[Array[Byte]]}} is grouped into pairs, treating the serialized Python keys as the pairs' keys."}, {"q": "What updates or decisions were made in the discussion?", "a": "Should be solved by PR #33 so I am closing this."}]}}
{"project": "SPARK", "issue_id": "SPARK-603", "title": "add simple Counter API", "status": "Closed", "priority": "Minor", "reporter": "Imran Rashid", "assignee": null, "labels": [], "created": "2012-11-01T12:46:28.000+0000", "updated": "2015-09-09T04:06:27.000+0000", "description": "Users need a very simple way to create counters in their jobs.  Accumulators provide a way to do this, but are a little clunky, for two reasons:\n\n1) the setup is a nuisance\n2) w/ delayed evaluation, you don't know when it will actually run, so its hard to look at the values\n\nconsider this code:\n\n{code}\ndef filterBogus(rdd:RDD[MyCustomClass], sc: SparkContext) = {\n  val filterCount = sc.accumulator(0)\n  val filtered = rdd.filter{r =>\n    if (isOK(r)) true else {filterCount += 1; false}\n  }\n  println(\"removed \" + filterCount.value + \" records)\n  filtered\n}\n{code}\n\nThe println will always say 0 records were filtered, because its printed before anything has actually run.  I could print out the value later on, but note that it would destroy the modularity of the method -- kinda ugly to return the accumulator just so that it can get printed later on.  (and of course, the caller in turn might not know when the filter is going to get applied, and would have to pass the accumulator up even further ...)\n\nI'd like to have Counters which just automatically get printed out whenever a stage has been run, and also with some api to get them back.  I realize this is tricky b/c a stage can get re-computed, so maybe you should only increment the counters once.\n\nMaybe a more general way to do this is to provide some callback for whenever an RDD is computed -- by default, you would just print the counters, but the user could replace w/ a custom handler.", "comments": ["Hi Anonymous (not sure how you can read this but...),\n\nI'm not sure how a counters API could solve the delayed evaluation problem and be fundamentally different from the existing Accumulator feature.  You'd have to make accessing a counter force evaluation of every RDD it's accessed from, and you'd have issues with recomputing the RDD later on when it's actually needed, causing a duplicate access.\n\nI can see some kind of API that offers a hook into the RDD evaluation DAG, but that operates at a stage level rather than an operation level (for example multiple maps are pipelined together) so the mapping for available hook points wouldn't be 1:1 with RDD operations, which would be quite tricky.\n\nI don't see a way to implement what you propose with Counters without compromising major parts of the Spark API contract (RDD laziness) so propose to close.  Especially given that I haven no way to contact you given your information doesn't appear on the prior Atlassian Jira either: https://spark-project.atlassian.net/browse/SPARK-603\n\n[~rxin] are you ok with closing this?", "Closing this one as part of [~aash]'s cleanup. I think this problem is being fixed as we add accumulator / metrics values to the web ui.", "Hey, this was originally reported by me too (probably I messed up when creating it on the old Jira, not sure if there is a way to change the reporter now?)\n\nI think perhaps the original issue was a little unclear, I'll try to clarify a little bit:\n\nI do *not* think we need to support something at the \"operation\" level -- having it work at the stage level (or even job level) is fine.  I'm not even sure what it would mean to work at the operation level, since individual records are pushed through all the operations of a stage in one go.\n\nBut the operation level is still a useful abstraction for the *developer*.  Its nice for them to be able to write methods which are eg., just a {{filter}}.  For normal RDD operations, this works just fine of course -- you can have a bunch of util methods that take in an RDD and output an RDD, maybe some {{filter}}, some {{map}}, etc., they can get combined however you like, everything remains lazy until there is some action.  All wonderful.\n\nThings get messy as soon as you start to include accumulators, however -- you've got include them in your return values and then the outside logic has to know when they actual contain valid data.  Rather than trying to solve this problem in general, I'm proposing that we do something dead-simple for basic counters, which might even live outside of accumulators completely.\n\nPutting accumulator values in the web UI is not bad for just this purpose, but overall I don't think its the right solution:\n\n1. It limits what we can do with accumulators (see my comments on SPARK-664)\n2. The api is more complicated than it needs to be.  If the only point of accumulators is counters, then we can get away with something as simple as:\n\n{code}\nrdd.map{x =>\n  if (isFoobar(x)) {\n    Counters(\"foobar\") += 1\n  }\n  ...\n}\n{code}\n\n(eg., no need to even declare the counter up front.)\n\n3. Having the value in the UI is nice, but its not the same as programmatic access.  eg. it can be useful to have them in the job logs, the actual values might be used in other computation (eg., gives the size of a datastructure for a later step), etc.\nEven with the simpler counter api, this is tricky b/c of lazy evaluation.  But maybe that is a reason you create a call-back up front:\n\n{code}\nCounters.addCallback(\"foobar\"){counts => ...}\nrdd.map{x =>\n  if (isFoobar(x)) {\n    Counters(\"foobar\") += 1\n  }\n  ...\n}\n{code}\n\n4. If you have long-running tasks, it might be nice to get incremental feedback from counters *during* the task.  (There was a real need for long-running tasks before sort-based shuffle, when you couldn't have too many tasks in a shuffle ... perhaps its not anymore, I'm not sure.)\n\nWe can get a little further with accumulators, eg. a SparkListener could do something with accumulator values when the stages finish.  But I think we're stuck on the other points.  I feel like right now accumulators are trapped between just being counters, and being a more general method of computation, and not quite doing either one very well.", "Is this still live? I also kind of think this is accomplishable pretty easily with accumulators and adding another abstraction on top with different semantics might be more confusing than it's worth. FWIW.", "Hi [~srowen]\n\nI don't think anyone is actively working on this, and probably won't for a while -- I suppose that means it should be closed for now.\n\nI disagree that its easy to do this with accumulators.  Its certainly possible, but it makes it quite complicated to do something that is use very common and should be dead-simple.  (or at least, its harder than most people realize to use accumulators to do this *correctly*.)   i guess it will be confusing to have counters & accumulators in the api, but it might only serve to highlight some of the intricacies of the accumulator api which aren't obvious (and can't be fixed w/out breaking changes).", "we use counters a lot in scalding (to verify records counts mostly at different stages, for certain criteria). \n\ni do not think it is easy at all to recreate counters with accumulators. in fact with the current behavior of accumulators (they do not account for task failure, leading to double counting) i think its nearly impossible to implement counters."], "derived": {"summary": "Users need a very simple way to create counters in their jobs. Accumulators provide a way to do this, but are a little clunky, for two reasons:\n\n1) the setup is a nuisance\n2) w/ delayed evaluation, you don't know when it will actually run, so its hard to look at the values\n\nconsider this code:\n\n{code}\ndef filterBogus(rdd:RDD[MyCustomClass], sc: SparkContext) = {\n  val filterCount = sc.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "add simple Counter API - Users need a very simple way to create counters in their jobs. Accumulators provide a way to do this, but are a little clunky, for two reasons:\n\n1) the setup is a nuisance\n2) w/ delayed evaluation, you don't know when it will actually run, so its hard to look at the values\n\nconsider this code:\n\n{code}\ndef filterBogus(rdd:RDD[MyCustomClass], sc: SparkContext) = {\n  val filterCount = sc."}, {"q": "What updates or decisions were made in the discussion?", "a": "we use counters a lot in scalding (to verify records counts mostly at different stages, for certain criteria). \n\ni do not think it is easy at all to recreate counters with accumulators. in fact with the current behavior of accumulators (they do not account for task failure, leading to double counting) i think its nearly impossible to implement counters."}]}}
{"project": "SPARK", "issue_id": "SPARK-604", "title": "reconnect if mesos slaves dies", "status": "Resolved", "priority": "Major", "reporter": null, "assignee": null, "labels": [], "created": "2012-11-01T16:55:24.000+0000", "updated": "2015-05-15T13:50:33.000+0000", "description": "when running on mesos, if a slave goes down, spark doesn't try to reassign the work to another machine.  Even if the slave comes back up, the job is doomed.\nCurrently when this happens, we just see this in the driver logs:\n\n12/11/01 16:48:56 INFO mesos.MesosSchedulerBackend: Mesos slave lost: 201210312057-1560611338-5050-24091-52\nException in thread \"Thread-346\" java.util.NoSuchElementException: key not found: value: \"201210312057-1560611338-5050-24091-52\"\n\n    at scala.collection.MapLike$class.default(MapLike.scala:224)\n    at scala.collection.mutable.HashMap.default(HashMap.scala:43)\n    at scala.collection.MapLike$class.apply(MapLike.scala:135)\n    at scala.collection.mutable.HashMap.apply(HashMap.scala:43)\n    at spark.scheduler.cluster.ClusterScheduler.slaveLost(ClusterScheduler.scala:255)\n    at spark.scheduler.mesos.MesosSchedulerBackend.slaveLost(MesosSchedulerBackend.scala:275)\n12/11/01 16:48:56 INFO mesos.MesosSchedulerBackend: driver.run() returned with code DRIVER_ABORTED", "comments": ["Stale at this point, without similar findings recently."], "derived": {"summary": "when running on mesos, if a slave goes down, spark doesn't try to reassign the work to another machine. Even if the slave comes back up, the job is doomed.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "reconnect if mesos slaves dies - when running on mesos, if a slave goes down, spark doesn't try to reassign the work to another machine. Even if the slave comes back up, the job is doomed."}, {"q": "What updates or decisions were made in the discussion?", "a": "Stale at this point, without similar findings recently."}]}}
{"project": "SPARK", "issue_id": "SPARK-605", "title": "Add support for new m3.xlarge and m3.2xlarge EC2 instance types", "status": "Resolved", "priority": "Minor", "reporter": "Josh Rosen", "assignee": "Patrick McFadin", "labels": [], "created": "2012-11-02T09:31:24.000+0000", "updated": "2014-01-23T19:57:39.000+0000", "description": "Today, Amazon introduced two new EC2 instance types, {{m3.xlarge}} and {{m3.2xlarge}} (http://aws.typepad.com/aws/2012/10/new-ec2-second-generation-standard-instances-and-price-reductions-1.html).  These instances do not have local disks, which may require special handling in {{spark-ec2}}'s {{get_num_disks()}} method.", "comments": ["It looks like https://github.com/mesos/spark/pull/603 might fix this, but are there any other places in the spark-ec2 toolchain where not having a local physical disk will cause problems?", "SPARK-727", "It looks like this was fixed as of Spark 0.8.0."], "derived": {"summary": "Today, Amazon introduced two new EC2 instance types, {{m3. xlarge}} and {{m3.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add support for new m3.xlarge and m3.2xlarge EC2 instance types - Today, Amazon introduced two new EC2 instance types, {{m3. xlarge}} and {{m3."}, {"q": "What updates or decisions were made in the discussion?", "a": "It looks like this was fixed as of Spark 0.8.0."}]}}
{"project": "SPARK", "issue_id": "SPARK-606", "title": "Add mapSideCombine setting to Java API partitionBy() method.", "status": "Resolved", "priority": "Minor", "reporter": "Josh Rosen", "assignee": "Reynold Xin", "labels": ["Starter"], "created": "2012-11-05T15:09:56.000+0000", "updated": "2014-07-26T20:02:29.000+0000", "description": "Add mapSideCombine setting to Java API partitionBy() method.", "comments": ["I am interested in working on this issue. Please assign it to me.", "Actually I feel we should just deprecate mapSideCombine in partitionBy ... it provides almost no benefit in the case of partitionBy. ", "mapSideCombine was removed from partitionBy in https://github.com/apache/spark/commit/0e84fee76b529089fb52f15151202e9a7b847ed5"], "derived": {"summary": "Add mapSideCombine setting to Java API partitionBy() method.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add mapSideCombine setting to Java API partitionBy() method. - Add mapSideCombine setting to Java API partitionBy() method."}, {"q": "What updates or decisions were made in the discussion?", "a": "mapSideCombine was removed from partitionBy in https://github.com/apache/spark/commit/0e84fee76b529089fb52f15151202e9a7b847ed5"}]}}
{"project": "SPARK", "issue_id": "SPARK-607", "title": "Timeout while fetching map statuses may cause job to hang", "status": "Resolved", "priority": "Major", "reporter": "Josh Rosen", "assignee": "Josh Rosen", "labels": [], "created": "2012-11-06T10:32:47.000+0000", "updated": "2013-01-16T21:54:26.000+0000", "description": "Jobs may hang if workers time out while fetching map output locations from the MapOutputTracker.\n\nI ran into this issue while running under Mesos on EC2, but I was able to reproduce it on my own machine using a 1-node standalone cluster.\n\nAfter applying the attached patch to generate random timeout failures, my groupByKey job lost a task due to the timeout.  It looks like the master is notified of the failure, since it appears in its log:\n\n{code}\n12/11/06 10:19:39 INFO TaskSetManager: Serialized task 0.0:7 as 3095 bytes in 1 ms\n12/11/06 10:19:39 INFO TaskSetManager: Lost TID 10 (task 0.0:2)\n12/11/06 10:19:40 INFO TaskSetManager: Loss was due to spark.SparkException: Error communicating with MapOutputTracker\n\tat spark.MapOutputTracker.askTracker(MapOutputTracker.scala:78)\n\tat spark.MapOutputTracker.getServerStatuses(MapOutputTracker.scala:154)\n\tat spark.BlockStoreShuffleFetcher.fetch(BlockStoreShuffleFetcher.scala:14)\n\tat spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:38)\n\tat spark.RDD.iterator(RDD.scala:161)\n\tat spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:18)\n\tat spark.RDD.iterator(RDD.scala:161)\n\tat spark.scheduler.ResultTask.run(ResultTask.scala:18)\n\tat spark.executor.Executor$TaskRunner.run(Executor.scala:76)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n\tat java.lang.Thread.run(Thread.java:680)\n12/11/06 10:19:40 INFO TaskSetManager: Starting task 0.0:2 as TID 16 on slave worker-20121106101845-128.32.130.156-50931: 128.32.130.156 (preferred)\n12/11/06 10:19:40 INFO TaskSetManager: Serialized task 0.0:2 as 3095 bytes in 1 ms\n{code}\n\nThe job hangs here; perhaps the failure leaves some inconsistent state on the worker.", "comments": ["I've got the same problem. \n\nIs there any workaround? ", "I believe this was fixed in 0.6.1, when we increased the Akka message timeout. Can you try that? ", "My test case patch can still produce a hang under 0.6.1.  Increasing the Akka timeout addresses the cause of the TimeoutException but doesn't fix the hang itself.  The exception should cause the task to fail rather than hanging.", "I found a potential cause for the freeze:\n\nIf an exception is throwing while communicating with the MapOutputTracker, the requested shuffleId will never be removed from the {{fetching}} set, which tracks in-progress requests.  As a result, subsequent fetches for the block will block while waiting for the failed fetch to finish.\n\nWhat's the right fix here?\n\nWe could add a try-finally block around the call to the MapOutputTracker to ensure that the failed fetch removes the shuffleId from the {{fetching}} set, but this will just lead to a NullPointerException when the waiting thread tries to read the missing value.  I suppose that this is okay, since the chain of failures will eventually be caught when the RDD partitions that were being computed are discovered to be missing.  Does this sound reasonable?", "I tested that fix locally and it works, so I submitted a pull request: https://github.com/mesos/spark/pull/332", "Pull request merged into master and branch-0.6, so I'm marking this as resolved."], "derived": {"summary": "Jobs may hang if workers time out while fetching map output locations from the MapOutputTracker. I ran into this issue while running under Mesos on EC2, but I was able to reproduce it on my own machine using a 1-node standalone cluster.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Timeout while fetching map statuses may cause job to hang - Jobs may hang if workers time out while fetching map output locations from the MapOutputTracker. I ran into this issue while running under Mesos on EC2, but I was able to reproduce it on my own machine using a 1-node standalone cluster."}, {"q": "What updates or decisions were made in the discussion?", "a": "Pull request merged into master and branch-0.6, so I'm marking this as resolved."}]}}
{"project": "SPARK", "issue_id": "SPARK-608", "title": "Allow showing worker STDOUT/STDERR log tail (e.g. last 512KB)", "status": "Resolved", "priority": "Major", "reporter": "Reynold Xin", "assignee": null, "labels": [], "created": "2012-11-06T14:02:05.000+0000", "updated": "2013-07-12T20:31:46.000+0000", "description": "When a job is long running, log grows large and it becomes impossible to download the entire log. The standalone console should support showing the tail of the logs with a user specified tail size.", "comments": ["This is still an issue.  When I try to view large logs through the web UI, I receive the error \"The server could not handle the request in the appropriate time frame (async timeout)\".\n\nWhy don't we implement a general mechanism that allows an offset and length to be specified when requesting a log?  This would support more general use-cases than just {{tail}} and might also allow us to use long-polling to implement a live-updating log viewer."], "derived": {"summary": "When a job is long running, log grows large and it becomes impossible to download the entire log. The standalone console should support showing the tail of the logs with a user specified tail size.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Allow showing worker STDOUT/STDERR log tail (e.g. last 512KB) - When a job is long running, log grows large and it becomes impossible to download the entire log. The standalone console should support showing the tail of the logs with a user specified tail size."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is still an issue.  When I try to view large logs through the web UI, I receive the error \"The server could not handle the request in the appropriate time frame (async timeout)\".\n\nWhy don't we implement a general mechanism that allows an offset and length to be specified when requesting a log?  This would support more general use-cases than just {{tail}} and might also allow us to use long-polling to implement a live-updating log viewer."}]}}
{"project": "SPARK", "issue_id": "SPARK-609", "title": "Add instructions for enabling Akka debug logging", "status": "Resolved", "priority": "Minor", "reporter": "Josh Rosen", "assignee": null, "labels": [], "created": "2012-11-06T15:05:17.000+0000", "updated": "2015-01-03T22:43:59.000+0000", "description": "How can I enable Akka debug logging in Spark?  I tried setting {{akka.loglevel = \"DEBUG\"}} in the configuration in {{AkkaUtils}}, and I also tried setting properties in a {{log4j.conf}} file, but neither approach worked.  It might be helpful to have instructions for this in a \"Spark Internals Debugging\" guide.", "comments": ["I'm going to close out this issue for now, since I think it's no longer an issue in recent versions of Spark.  Please comment / reopen if you think there's still something that needs fixing."], "derived": {"summary": "How can I enable Akka debug logging in Spark?  I tried setting {{akka. loglevel = \"DEBUG\"}} in the configuration in {{AkkaUtils}}, and I also tried setting properties in a {{log4j.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Add instructions for enabling Akka debug logging - How can I enable Akka debug logging in Spark?  I tried setting {{akka. loglevel = \"DEBUG\"}} in the configuration in {{AkkaUtils}}, and I also tried setting properties in a {{log4j."}, {"q": "What updates or decisions were made in the discussion?", "a": "I'm going to close out this issue for now, since I think it's no longer an issue in recent versions of Spark.  Please comment / reopen if you think there's still something that needs fixing."}]}}
{"project": "SPARK", "issue_id": "SPARK-610", "title": "Support master failover in standalone mode", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Aaron Davidson", "labels": [], "created": "2012-11-06T16:04:19.000+0000", "updated": "2014-11-06T06:59:46.000+0000", "description": "The standalone deploy mode is quite simple, which shouldn't make it too bad to add support for master failover using ZooKeeper or something similar. This would really up its usefulness.", "comments": ["[~matei] given YARN and Mesos implementations, is this something the standalone mode should strive to do?"], "derived": {"summary": "The standalone deploy mode is quite simple, which shouldn't make it too bad to add support for master failover using ZooKeeper or something similar. This would really up its usefulness.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support master failover in standalone mode - The standalone deploy mode is quite simple, which shouldn't make it too bad to add support for master failover using ZooKeeper or something similar. This would really up its usefulness."}, {"q": "What updates or decisions were made in the discussion?", "a": "[~matei] given YARN and Mesos implementations, is this something the standalone mode should strive to do?"}]}}
{"project": "SPARK", "issue_id": "SPARK-611", "title": "Allow JStack to be run from web UI", "status": "Closed", "priority": "Major", "reporter": "Reynold Xin", "assignee": "Josh Rosen", "labels": [], "created": "2012-11-07T13:43:39.000+0000", "updated": "2014-11-04T02:19:21.000+0000", "description": "Huge debugging improvement if the standalone mode dashboard can run jstack and show it on the web page for a slave.", "comments": ["Note for my own reference:\nhttp://nadeausoftware.com/articles/2008/04/java_tip_how_list_and_find_threads_and_thread_groups#Gettingalistofallthreads", "Patrick and I discussed this yesterday and I'm going to take a shot at implementing it this afternoon.  There are two different design considerations here: _how_ do we obtain information on threads and _when_ do we do it?\n\nOne approach would be to literally invoke {{jstack}} on the executor JVM from the executor JVM by obtaining its own process ID and using a process builder to fork out to {{jstack}} (this approach is described [in this blog post|http://www.takipiblog.com/supercharged-jstack-how-to-debug-your-servers-at-100mph/]).  Unfortunately, this could be prohibitively expensive due to the high cost of forking from JVMs with large heaps.  Another approach, which I'm planning to use, is to call {{Thread.getAllStackTraces()}} and format the result into a string.  I think that this loses a bit of information compared to {{jstack}} but it's much lower overhead in the worst case.\n\nAs for the \"when\", one approach would be an \"on demand\" jstack button that triggers a thread dump on an executor.  This approach would require a large amount of new RPC plumbing / machinery in several places.  A lighter-weight, first-cut approach, which we plan to explore, is to piggyback periodic thread dumps on the Executor -> Driver heartbeat messages.\n\nI'm going to try to hack this together and submit a PR tomorrow. ", "User 'JoshRosen' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/2944"], "derived": {"summary": "Huge debugging improvement if the standalone mode dashboard can run jstack and show it on the web page for a slave.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Allow JStack to be run from web UI - Huge debugging improvement if the standalone mode dashboard can run jstack and show it on the web page for a slave."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'JoshRosen' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/2944"}]}}
{"project": "SPARK", "issue_id": "SPARK-612", "title": "Update examples to pass JAR file to SparkContext in master and 0.6 branches", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "2012-11-07T15:42:34.000+0000", "updated": "2013-01-20T12:50:20.000+0000", "description": "We did this in 0.5 but somehow lost it in master and 0.6. It's confusing to users.", "comments": ["Duplicates SPARK-594"], "derived": {"summary": "We did this in 0. 5 but somehow lost it in master and 0.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Update examples to pass JAR file to SparkContext in master and 0.6 branches - We did this in 0. 5 but somehow lost it in master and 0."}, {"q": "What updates or decisions were made in the discussion?", "a": "Duplicates SPARK-594"}]}}
{"project": "SPARK", "issue_id": "SPARK-613", "title": "Standalone web UI links to internal IPs when running on EC2", "status": "Resolved", "priority": "Major", "reporter": "Josh Rosen", "assignee": "Josh Rosen", "labels": [], "created": "2012-11-07T23:01:09.000+0000", "updated": "2013-05-25T23:42:28.000+0000", "description": "When I visit the standalone cluster web UI on EC2, the links to worker UIs are based on internal addresses (e.g. http://10.159.2.115:8081/) instead of externally-accessible addresses (e.g. http://ec2-*-*-*-*.compute-1.amazonaws.com/).  This makes it hard to view the worker logs.", "comments": ["Fixed by https://github.com/mesos/spark/pull/316", "It looks like there are hostnames like domU-12-31-39-09-F5-02.compute-1.internal and ip-10-226-87-193 which aren't recognized by the pattern used in that pull request.\n\nWe should probably add an environment variable in spark-env.sh to specify that the machines are running on EC2, and automatically set this variable in the EC2 scripts.", "Fixed for Spark AMI by https://github.com/mesos/spark/pull/419", "Can https://github.com/mesos/spark/pull/419 be backported to 0.6?", "This may still be broken.  I noticed a link to a worker with domu-*-*-*-*=*.compute-1.internal:8081, an internal address, when running the latest Spark master on the 0.7 AMI.", "Found what's hopefully the last issue here and opened a new PR: https://github.com/mesos/spark/pull/621", "Merged and cherry-picked that PR into branch-0.7, so this is resolved."], "derived": {"summary": "When I visit the standalone cluster web UI on EC2, the links to worker UIs are based on internal addresses (e. g.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Standalone web UI links to internal IPs when running on EC2 - When I visit the standalone cluster web UI on EC2, the links to worker UIs are based on internal addresses (e. g."}, {"q": "What updates or decisions were made in the discussion?", "a": "Merged and cherry-picked that PR into branch-0.7, so this is resolved."}]}}
{"project": "SPARK", "issue_id": "SPARK-614", "title": "Make last 4 digits of framework id in standalone mode logging monotonically increasing", "status": "Closed", "priority": "Major", "reporter": "Reynold Xin", "assignee": "Denny Britz", "labels": [], "created": "2012-11-08T11:41:33.000+0000", "updated": "2014-09-21T14:21:16.000+0000", "description": "In mesos mode, the work log directories are monotonically increasing, and makes it very easy to spot a folder and go into it (e.g. only need to type *[last4digit]).\n\nWe lost this in the standalone mode, as seen in this example. The last four digits would go up and down ....\n\n\ndrwxr-xr-x 3 root root 4096 Nov  8 08:03 job-20121108080355-0000\ndrwxr-xr-x 3 root root 4096 Nov  8 08:04 job-20121108080450-0001\ndrwxr-xr-x 3 root root 4096 Nov  8 08:07 job-20121108080757-0002\ndrwxr-xr-x 3 root root 4096 Nov  8 08:10 job-20121108081014-0003\ndrwxr-xr-x 3 root root 4096 Nov  8 08:23 job-20121108082316-0004\ndrwxr-xr-x 3 root root 4096 Nov  8 08:26 job-20121108082616-0005\ndrwxr-xr-x 3 root root 4096 Nov  8 08:30 job-20121108083034-0006\ndrwxr-xr-x 3 root root 4096 Nov  8 08:35 job-20121108083514-0007\ndrwxr-xr-x 3 root root 4096 Nov  8 08:38 job-20121108083807-0008\ndrwxr-xr-x 3 root root 4096 Nov  8 08:41 job-20121108084105-0009\ndrwxr-xr-x 3 root root 4096 Nov  8 08:42 job-20121108084242-0010\ndrwxr-xr-x 3 root root 4096 Nov  8 08:45 job-20121108084512-0011\ndrwxr-xr-x 3 root root 4096 Nov  8 09:01 job-20121108090113-0000\ndrwxr-xr-x 3 root root 4096 Nov  8 09:15 job-20121108091536-0001\ndrwxr-xr-x 3 root root 4096 Nov  8 09:24 job-20121108092341-0003\ndrwxr-xr-x 3 root root 4096 Nov  8 09:27 job-20121108092703-0000\ndrwxr-xr-x 3 root root 4096 Nov  8 09:46 job-20121108094629-0001\ndrwxr-xr-x 3 root root 4096 Nov  8 09:48 job-20121108094809-0002\ndrwxr-xr-x 3 root root 4096 Nov  8 10:04 job-20121108100418-0003\ndrwxr-xr-x 3 root root 4096 Nov  8 10:18 job-20121108101814-0004\ndrwxr-xr-x 3 root root 4096 Nov  8 10:22 job-20121108102207-0005\ndrwxr-xr-x 3 root root 4096 Nov  8 18:48 job-20121108184842-0006\ndrwxr-xr-x 3 root root 4096 Nov  8 18:49 job-20121108184932-0007\ndrwxr-xr-x 3 root root 4096 Nov  8 18:50 job-20121108185007-0008\ndrwxr-xr-x 3 root root 4096 Nov  8 18:50 job-20121108185040-0009\ndrwxr-xr-x 3 root root 4096 Nov  8 18:51 job-20121108185127-0010\ndrwxr-xr-x 3 root root 4096 Nov  8 18:54 job-20121108185428-0011\ndrwxr-xr-x 3 root root 4096 Nov  8 18:58 job-20121108185837-0012\ndrwxr-xr-x 3 root root 4096 Nov  8 18:58 job-20121108185854-0013\ndrwxr-xr-x 3 root root 4096 Nov  8 19:00 job-20121108190005-0014\ndrwxr-xr-x 3 root root 4096 Nov  8 19:00 job-20121108190059-0015\ndrwxr-xr-x 3 root root 4096 Nov  8 19:10 job-20121108191010-0016\ndrwxr-xr-x 3 root root 4096 Nov  8 19:15 job-20121108191508-0017\ndrwxr-xr-x 3 root root 4096 Nov  8 19:21 job-20121108192125-0018\ndrwxr-xr-x 3 root root 4096 Nov  8 19:23 job-20121108192329-0019\ndrwxr-xr-x 3 root root 4096 Nov  8 19:26 job-20121108192638-0020\ndrwxr-xr-x 3 root root 4096 Nov  8 19:35 job-20121108193554-0022\n", "comments": ["This only happens if you restart the standalone cluster between jobs. If you leave it running (the same as in Mesos), it will not happen.", "I've seen a few times the workers crashed, and that's why I had to restart the server. \n\nThat said, any particular reason we follow the current directory naming scheme? How about making the first 4 digits monotonically increasing?", "Yes, the reason was to avoid race conditions that would happen if we tried to figure out the last job ID. In particular, the job ID is assigned by the master, so the master would need to remember the last job ID it gave when it restarted, which is kind of ugly. You could restart a worker without restarting the master.. no need to restart the whole cluster.", "it looks like nothing has happened with this in the past 23 months. i'm going to close this, but feel free to re-open."], "derived": {"summary": "In mesos mode, the work log directories are monotonically increasing, and makes it very easy to spot a folder and go into it (e. g.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Make last 4 digits of framework id in standalone mode logging monotonically increasing - In mesos mode, the work log directories are monotonically increasing, and makes it very easy to spot a folder and go into it (e. g."}, {"q": "What updates or decisions were made in the discussion?", "a": "it looks like nothing has happened with this in the past 23 months. i'm going to close this, but feel free to re-open."}]}}
{"project": "SPARK", "issue_id": "SPARK-615", "title": "Add mapPartitionsWithIndex() to the Java API", "status": "Resolved", "priority": "Minor", "reporter": "Josh Rosen", "assignee": "Holden Karau", "labels": ["Starter"], "created": "2012-11-09T20:46:04.000+0000", "updated": "2014-04-30T00:40:19.000+0000", "description": "We should add {{mapPartitionsWithIndex()}} to the Java API.\n\nWhat should the interface for this look like?  We could require the user to pass in a {{FlatMapFunction[(Int, Iterator[T]))}}, but this requires them to unpack the tuple from Java.  It would be nice if the UDF had a signature like {{f(int partition, Iterator[T] iterator)}}, but this will require defining a new set of {{Function}} classes.", "comments": ["Sorry, thought I had replied to this earlier, but I think we should just create another subclass of Function.", "Holden Karau has a pull request for this at https://github.com/mesos/spark/pull/930/", "This still needs unit tests, since I don't think that it's actually callable from Java without passing ClassManifest; see my comment at https://github.com/mesos/spark/pull/930/files#r6640056"], "derived": {"summary": "We should add {{mapPartitionsWithIndex()}} to the Java API. What should the interface for this look like?  We could require the user to pass in a {{FlatMapFunction[(Int, Iterator[T]))}}, but this requires them to unpack the tuple from Java.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add mapPartitionsWithIndex() to the Java API - We should add {{mapPartitionsWithIndex()}} to the Java API. What should the interface for this look like?  We could require the user to pass in a {{FlatMapFunction[(Int, Iterator[T]))}}, but this requires them to unpack the tuple from Java."}, {"q": "What updates or decisions were made in the discussion?", "a": "This still needs unit tests, since I don't think that it's actually callable from Java without passing ClassManifest; see my comment at https://github.com/mesos/spark/pull/930/files#r6640056"}]}}
{"project": "SPARK", "issue_id": "SPARK-616", "title": "Show dead workers in the standalone web UI", "status": "Resolved", "priority": "Minor", "reporter": "Josh Rosen", "assignee": "Patrick McFadin", "labels": [], "created": "2012-11-10T21:30:22.000+0000", "updated": "2012-12-20T16:23:02.000+0000", "description": "It would be helpful if the standalone web UI listed dead workers; currently, dead workers disappear from the cluster summary.  Perhaps they could be listed in a separate section or be highlighted in red.", "comments": ["Pull request up for this."], "derived": {"summary": "It would be helpful if the standalone web UI listed dead workers; currently, dead workers disappear from the cluster summary. Perhaps they could be listed in a separate section or be highlighted in red.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Show dead workers in the standalone web UI - It would be helpful if the standalone web UI listed dead workers; currently, dead workers disappear from the cluster summary. Perhaps they could be listed in a separate section or be highlighted in red."}, {"q": "What updates or decisions were made in the discussion?", "a": "Pull request up for this."}]}}
{"project": "SPARK", "issue_id": "SPARK-617", "title": "Driver program can crash when a standalone worker is lost", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "labels": [], "created": "2012-11-10T22:46:56.000+0000", "updated": "2012-11-11T21:21:56.000+0000", "description": "Seems to be due to an uncaught communication timeout in Akka.", "comments": ["Fixed in https://github.com/mesos/spark/commit/173e0354c0fc95d63112c7ff7121d8ff39f961b7"], "derived": {"summary": "Seems to be due to an uncaught communication timeout in Akka.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Driver program can crash when a standalone worker is lost - Seems to be due to an uncaught communication timeout in Akka."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed in https://github.com/mesos/spark/commit/173e0354c0fc95d63112c7ff7121d8ff39f961b7"}]}}
{"project": "SPARK", "issue_id": "SPARK-618", "title": "start-mesos and stop-mesos scripts should check for running processes", "status": "Resolved", "priority": "Minor", "reporter": "Josh Rosen", "assignee": null, "labels": [], "created": "2012-11-11T02:16:16.000+0000", "updated": "2013-09-27T14:30:17.000+0000", "description": "The {{start-mesos}} script in the EC2 AMI should log warnings if Mesos processes are already running.  Currently, running the script multiple times will launch multiple Mesos workers on each slave.\n\nThis is blocked by [SPARK-521].", "comments": ["Our EC2 images no longer include Mesos, so I'm going to close this."], "derived": {"summary": "The {{start-mesos}} script in the EC2 AMI should log warnings if Mesos processes are already running. Currently, running the script multiple times will launch multiple Mesos workers on each slave.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "start-mesos and stop-mesos scripts should check for running processes - The {{start-mesos}} script in the EC2 AMI should log warnings if Mesos processes are already running. Currently, running the script multiple times will launch multiple Mesos workers on each slave."}, {"q": "What updates or decisions were made in the discussion?", "a": "Our EC2 images no longer include Mesos, so I'm going to close this."}]}}
{"project": "SPARK", "issue_id": "SPARK-619", "title": "Hadoop MapReduce should be configured to use all local disks for shuffle on AMI", "status": "Resolved", "priority": "Minor", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "2012-11-11T11:22:40.000+0000", "updated": "2014-11-06T07:00:29.000+0000", "description": "It used to be, but that got lost at some point.", "comments": [], "derived": {"summary": "It used to be, but that got lost at some point.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Hadoop MapReduce should be configured to use all local disks for shuffle on AMI - It used to be, but that got lost at some point."}]}}
{"project": "SPARK", "issue_id": "SPARK-620", "title": "Default SPARK_MEM on AMI too high", "status": "Resolved", "priority": "Minor", "reporter": "Denny Britz", "assignee": null, "labels": [], "created": "2012-11-11T12:05:57.000+0000", "updated": "2013-05-04T19:19:00.000+0000", "description": "When I use the 0.6 AMI, start the Spark shell and try to load a dataset from S3 I get an out of memory error. It worked after I reduced the SPARK_MEM setting in the config.", "comments": ["The spell occasionally forks some commands, and each fork would require starting a new heap that's the same size as the old heap. To get rid of the problem, the spark_mem on the master cannot be greater than half of the ram available on the master node.", "I only reduced the memory by about 1g and it got rid of the problem. It definitely was more than half of the memory available on the master (12g on m1.xlarge instance).", "There are two problems here really."], "derived": {"summary": "When I use the 0. 6 AMI, start the Spark shell and try to load a dataset from S3 I get an out of memory error.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Default SPARK_MEM on AMI too high - When I use the 0. 6 AMI, start the Spark shell and try to load a dataset from S3 I get an out of memory error."}, {"q": "What updates or decisions were made in the discussion?", "a": "There are two problems here really."}]}}
{"project": "SPARK", "issue_id": "SPARK-621", "title": "Provide an API to manually throw RDDs out of the cache", "status": "Resolved", "priority": "Minor", "reporter": "Denny Britz", "assignee": "Reynold Xin", "labels": [], "created": "2012-11-11T12:14:34.000+0000", "updated": "2013-05-06T16:10:18.000+0000", "description": "I want this.", "comments": ["If this is a useful feature, it shouldn't be that hard to implement now that 0.7 added support for dropping blocks in the block manager: https://github.com/mesos/spark/pull/327", "In pull request: https://github.com/mesos/spark/pull/591"], "derived": {"summary": "I want this.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Provide an API to manually throw RDDs out of the cache - I want this."}, {"q": "What updates or decisions were made in the discussion?", "a": "In pull request: https://github.com/mesos/spark/pull/591"}]}}
{"project": "SPARK", "issue_id": "SPARK-622", "title": "spark-ec2 launch command hangs if instances fail while starting up.", "status": "Resolved", "priority": "Minor", "reporter": "Josh Rosen", "assignee": null, "labels": [], "created": "2012-11-11T12:59:07.000+0000", "updated": "2012-11-11T13:03:46.000+0000", "description": "The {{spark-ec2}} script should detect if instances fail while launching, rather than waiting indefinitely for the instances to start up.", "comments": ["Actually, it looks there is already code to handle this; I observed both a failure and a node that became stuck in \"pending\", and the pending node lead to the hang.\n\nClosing this."], "derived": {"summary": "The {{spark-ec2}} script should detect if instances fail while launching, rather than waiting indefinitely for the instances to start up.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "spark-ec2 launch command hangs if instances fail while starting up. - The {{spark-ec2}} script should detect if instances fail while launching, rather than waiting indefinitely for the instances to start up."}, {"q": "What updates or decisions were made in the discussion?", "a": "Actually, it looks there is already code to handle this; I observed both a failure and a node that became stuck in \"pending\", and the pending node lead to the hang.\n\nClosing this."}]}}
{"project": "SPARK", "issue_id": "SPARK-623", "title": "Don't hardcode log location for standalone UI", "status": "Resolved", "priority": "Minor", "reporter": "Denny Britz", "assignee": "Christoph Grothaus", "labels": [], "created": "2012-11-14T15:41:13.000+0000", "updated": "2013-05-13T14:37:09.000+0000", "description": null, "comments": ["Does anyone know which log / directory this issue is talking about?", "In WorkerArguments.scala, I do see\n\n\n{code}\n    if (System.getenv(\"SPARK_WORKER_DIR\") != null) {\n      workDir = System.getenv(\"SPARK_WORKER_DIR\")\n    }\n{code}", "I think that this issue referred to the WorkerWebUI assuming that worker logs would be in $SPARK_HOME/work.  This was fixed by https://github.com/mesos/spark/pull/539"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Don't hardcode log location for standalone UI"}, {"q": "What updates or decisions were made in the discussion?", "a": "I think that this issue referred to the WorkerWebUI assuming that worker logs would be in $SPARK_HOME/work.  This was fixed by https://github.com/mesos/spark/pull/539"}]}}
{"project": "SPARK", "issue_id": "SPARK-624", "title": "The local IP address to bind to should be configurable", "status": "Resolved", "priority": "Minor", "reporter": null, "assignee": null, "labels": [], "created": "2012-11-15T13:40:45.000+0000", "updated": "2012-11-21T11:38:19.000+0000", "description": "Local IP address that Spark uses by default to bind server sockets should be configurable. This is essential on systems where InetAddress.getLocalHost.getHostAddress does not return the correct interface, e.g. the default configuration of Debian/Ubuntu, where /etc/hosts resolves host name as 127.0.1.1. Spark should not have to rely on hostnames, /etc/hosts and DNS, and the proposed solution is to add an environment variable, e.g. SPARK_DEFAULT_LOCAL_IP, that would take precedence over InetAddress.getLocalHost.getHostAddress in Utils.localIpAddress.", "comments": ["An environment variable is good, but let me see first if we can just ask Java for all the IP addresses of the local host and ignore ones that start with 127.*. Ideally users shouldn't need to configure anything to bind to the right IP address. It looks like it may be possible to enumerate all IPs for the local hostname: http://stackoverflow.com/questions/494465/how-to-enumerate-ip-addresses-of-all-enabled-nic-cards-from-java. Let me know if you'd like to look into this.", "Yes, I can try to implement the general-case NIC IP detection. This would help users that only have one real NIC per host to get up and running out of the box.\n\nHowever, this would not help in our case, because we have two legitimate interfaces per host (one for communication within cluster and one for accessing the internet) and we have to tell Spark which one to use explicitly, so the environment variable would still be very useful for us.", "I see, that makes sense. In that case I'll merge your pull request, but just beware that there will still be places where we use the hostname. For example, we found a bug where HDFS data locality didn't work on the standalone cluster because the workers reported IP addresses instead of hostnames, but HDFS uses hostnames to determine data locality. So we might need to make sure we use this variable whenever we bind a server socket in more places (but report the hostname to the master and hope it resolves to the right IP). If you try the latest master or 0.6 branches, they have this fix to the standalone mode."], "derived": {"summary": "Local IP address that Spark uses by default to bind server sockets should be configurable. This is essential on systems where InetAddress.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "The local IP address to bind to should be configurable - Local IP address that Spark uses by default to bind server sockets should be configurable. This is essential on systems where InetAddress."}, {"q": "What updates or decisions were made in the discussion?", "a": "I see, that makes sense. In that case I'll merge your pull request, but just beware that there will still be places where we use the hostname. For example, we found a bug where HDFS data locality didn't work on the standalone cluster because the workers reported IP addresses instead of hostnames, but HDFS uses hostnames to determine data locality. So we might need to make sure we use this variable whenever we bind a server socket in more places (but report the hostname to the master and hope it resolves to the right IP). If you try the latest master or 0.6 branches, they have this fix to the standalone mode."}]}}
{"project": "SPARK", "issue_id": "SPARK-625", "title": "Client hangs when connecting to standalone cluster using wrong address", "status": "Resolved", "priority": "Minor", "reporter": "Josh Rosen", "assignee": null, "labels": [], "created": "2012-11-27T14:13:26.000+0000", "updated": "2015-02-07T22:48:32.000+0000", "description": "I launched a standalone cluster on my laptop, connecting the workers to the master using my machine's public IP address (128.32.*.*:7077).  If I try to connect spark-shell to the master using \"spark://0.0.0.0:7077\", it successfully brings up a Scala prompt but hangs when I try to run a job.\n\nFrom the standalone master's log, it looks like the client's messages are being dropped without the client discovering that the connection has failed:\n\n{code}\n12/11/27 14:00:52 ERROR NettyRemoteTransport(null): dropping message RegisterJob(JobDescription(Spark shell)) for non-local recipient akka://spark@0.0.0.0:7077/user/Master at akka://spark@128.32.*.*:7077 local is akka://spark@128.32.*.*:7077\n12/11/27 14:00:52 ERROR NettyRemoteTransport(null): dropping message DaemonMsgWatch(Actor[akka://spark@128.32.*.*:57518/user/$a],Actor[akka://spark@0.0.0.0:7077/user/Master]) for non-local recipient akka://spark@0.0.0.0:7077/remote at akka://spark@128.32.*.*:7077 local is akka://spark@128.32.*.*:7077\n{code}", "comments": ["I have run into this as well. From what it appears is that akka is just very sensitive to hostnames.\n\nSince these are different, you got that error:\nakka://spark@0.0.0.0:7077\nakka://spark@128.32.*.*:7077\n\n\nI ran into this because my slaves file was using fqdn, while akka was expecting just hostname.  As soon as I switched my slaves file to just using hostnames, things started working great for me.", "Fixed by Matei in https://github.com/mesos/spark/commit/173e0354c0fc95d63112c7ff7121d8ff39f961b7, which also fixed SPARK-617", "Actually, I take that back:  If I run\n\n{code}\nMASTER=spark://0.0.0.0:7077 ./spark-shell\n{code}\n\non my laptop, I see the same hang.  This is with the current (0.8) master branch.", "What OS is your laptop Josh? \n\nsomething like http://stackoverflow.com/questions/11982562/socket-connect-to-0-0-0-0-windows-vs-mac suggests that java just won't like connecting to it.\n\nChrome on windows for me says 0.0.0.0 is an invalid address, it works on OS X however. so you could just be hitting that?", "Spark is very sensitive to hostnames in Spark URLs, and that comes from Akka being very sensitive.  I've personally been bitten by hostnames vs FQDNs vs external IP address vs loopback IP address, and it's really a pain.\n\nOn current master branch (1.2) with the Spark standalone master listening on {{spark://aash-mbp.local:7077}} as confirmed by the master web UI, and the spark shell attempting to connect to {{spark://127.0.01:7077}} with the {{--master}} parameter, the driver tries 3 attempts and then fails with this message:\n\n{noformat}\n14/11/14 01:37:56 INFO AppClient$ClientActor: Connecting to master spark://127.0.0.1:7077...\n14/11/14 01:37:56 WARN AppClient$ClientActor: Could not connect to akka.tcp://sparkMaster@127.0.0.1:7077: akka.remote.InvalidAssociation: Invalid address: akka.tcp://sparkMaster@127.0.0.1:7077\n14/11/14 01:37:56 WARN Remoting: Tried to associate with unreachable remote address [akka.tcp://sparkMaster@127.0.0.1:7077]. Address is now gated for 5000 ms, all messages to this address will be delivered to dead letters. Reason: Connection refused: /127.0.0.1:7077\n14/11/14 01:38:16 INFO AppClient$ClientActor: Connecting to master spark://127.0.0.1:7077...\n14/11/14 01:38:16 WARN Remoting: Tried to associate with unreachable remote address [akka.tcp://sparkMaster@127.0.0.1:7077]. Address is now gated for 5000 ms, all messages to this address will be delivered to dead letters. Reason: Connection refused: /127.0.0.1:7077\n14/11/14 01:38:16 WARN AppClient$ClientActor: Could not connect to akka.tcp://sparkMaster@127.0.0.1:7077: akka.remote.InvalidAssociation: Invalid address: akka.tcp://sparkMaster@127.0.0.1:7077\n14/11/14 01:38:36 INFO AppClient$ClientActor: Connecting to master spark://127.0.0.1:7077...\n14/11/14 01:38:36 WARN Remoting: Tried to associate with unreachable remote address [akka.tcp://sparkMaster@127.0.0.1:7077]. Address is now gated for 5000 ms, all messages to this address will be delivered to dead letters. Reason: Connection refused: /127.0.0.1:7077\n14/11/14 01:38:36 WARN AppClient$ClientActor: Could not connect to akka.tcp://sparkMaster@127.0.0.1:7077: akka.remote.InvalidAssociation: Invalid address: akka.tcp://sparkMaster@127.0.0.1:7077\n14/11/14 01:38:56 ERROR SparkDeploySchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\n14/11/14 01:38:56 WARN SparkDeploySchedulerBackend: Application ID is not initialized yet.\n14/11/14 01:38:56 ERROR TaskSchedulerImpl: Exiting due to error from cluster scheduler: All masters are unresponsive! Giving up.\n{noformat}\n\nSo the hang seems to be gone and replaced with a reasonable 3x attempts and fail.\n\n[~joshrosen], short of changing Akka ourselves to make it less strict on exact URL matches, is there anything else we can do for this ticket?  I think we can reasonably close as fixed.", "Let's resolve this as \"Fixed\" for now.  Reducing Akka's sensitivity to hostnames is a more general issue and we may have a fix for this in the future by either upgrading to a version of Akka that differentiates between bound and advertised addressed or by replacing Akka with a different communications layer.  I don't think we've observed the \"hang indefinitely\" behavior described in this ticket for many versions, so I think this should be safe to close."], "derived": {"summary": "I launched a standalone cluster on my laptop, connecting the workers to the master using my machine's public IP address (128. 32.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Client hangs when connecting to standalone cluster using wrong address - I launched a standalone cluster on my laptop, connecting the workers to the master using my machine's public IP address (128. 32."}, {"q": "What updates or decisions were made in the discussion?", "a": "Let's resolve this as \"Fixed\" for now.  Reducing Akka's sensitivity to hostnames is a more general issue and we may have a fix for this in the future by either upgrading to a version of Akka that differentiates between bound and advertised addressed or by replacing Akka with a different communications layer.  I don't think we've observed the \"hang indefinitely\" behavior described in this ticket for many versions, so I think this should be safe to close."}]}}
{"project": "SPARK", "issue_id": "SPARK-626", "title": "deleting security groups gives me a 400 error", "status": "Resolved", "priority": "Major", "reporter": "Reynold Xin", "assignee": "Peter Sankauskas", "labels": [], "created": "2012-11-29T21:40:22.000+0000", "updated": "2012-12-11T12:13:57.000+0000", "description": "Filing on behalf of Shivaram:\n\n\nDeleting security group tinytasks-test-zoo\nERROR:boto:400 Bad Request\nERROR:boto:<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Response><Errors><Error><Code>InvalidGroup.InUse</Code><Message>Group 905882038624:tinytasks-test-zoo is used by groups: 905882038624:tinytasks-test-master 905882038624:tinytasks-test-slaves</Message></Error></Errors><RequestID>5067d547-e88c-45d0-8ef0-15db51a444c0</RequestID></Response>\nTraceback (most recent call last):\n  File \"./spark_ec2.py\", line 637, in <module>\n    main()\n  File \"./spark_ec2.py\", line 571, in main\n    conn.delete_security_group(group.name)\n  File \"/home/shivaram/projects/tinytasks/tiny-tasks-spark/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/ec2/connection.py\", line 2039, in delete_security_group\n  File \"/home/shivaram/projects/tinytasks/tiny-tasks-spark/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/connection.py\", line 944, in get_status\nboto.exception.EC2ResponseError: EC2ResponseError: 400 Bad Request\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Response><Errors><Error><Code>InvalidGroup.InUse</Code><Message>Group 905882038624:tinytasks-test-zoo is used by groups: 905882038624:tinytasks-test-master 905882038624:tinytasks-test-slaves</Message></Error></Errors><RequestID>5067d547-e88c-45d0-8ef0-15db51a444c0</RequestID></Response>", "comments": ["I ran into the same issue and tried to investigate it:\n\nThere may be dependencies between security groups (e.g. allow traffic to/from another group), so these dependencies must be removed before the groups can be deleted.  The current script tries to do this, but it needs an additional for-loop: the current script removes an individual group's rules then deletes that group, but it should first remove all rules from all groups then delete all groups.\n\nI tried modifying the script to do this (https://gist.github.com/4187604), but it ran into the same error.  When I ran the destroy command a second time, it successfully deleted the groups, so there may be a race condition there.", "I think that Gist will work, but perhaps there needs to be a delay between the steps. I get the feeling that behind the scenes the AWS backend is using eventual consistency, which is why I didn't experience this issues but others are.", "Have you tried that, Peter? Would be nice to see this fixed.", "Here you go:\nhttps://github.com/mesos/spark/pull/323\n\nI tried lowering the sleep delay in between deleting the rules and deleting the groups, but anything below 30 seconds caused issues. 30 seconds seems to be as low as we can consistently go.", "It turns out this is far more error prone than I first though. There are multiple dependencies in play:\n - dependencies between rules in security groups\n - instances may not have been terminated completely when trying to delete a security group\n - AWS back-end eventual consistency\n - group.revoke() returns True even when it fails\n\nThe unfortunate result is the code is somewhat messy. This is as clean as I can make - there are 3 retries to delete the all groups, and exceptions are caught.\n\nBy default, groups will not be deleted, so most people won't experience this. You can delete groups when destroying a cluster by adding `--delete-groups`", "Alright, I've merged your commit. Thanks for looking into this and fixing it!"], "derived": {"summary": "Filing on behalf of Shivaram:\n\n\nDeleting security group tinytasks-test-zoo\nERROR:boto:400 Bad Request\nERROR:boto:<?xml version=\"1. 0\" encoding=\"UTF-8\"?>\n<Response><Errors><Error><Code>InvalidGroup.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "deleting security groups gives me a 400 error - Filing on behalf of Shivaram:\n\n\nDeleting security group tinytasks-test-zoo\nERROR:boto:400 Bad Request\nERROR:boto:<?xml version=\"1. 0\" encoding=\"UTF-8\"?>\n<Response><Errors><Error><Code>InvalidGroup."}, {"q": "What updates or decisions were made in the discussion?", "a": "Alright, I've merged your commit. Thanks for looking into this and fixing it!"}]}}
{"project": "SPARK", "issue_id": "SPARK-627", "title": "Unimplemented configuration options in spark-daemon[s].sh", "status": "Resolved", "priority": "Minor", "reporter": "Josh Rosen", "assignee": "Karthik G Tunga", "labels": ["Starter"], "created": "2012-12-01T16:50:00.000+0000", "updated": "2013-10-19T18:06:24.000+0000", "description": "The spark-daemon.sh and spark-daemons.sh scripts in the Spark bin directory have command-line arguments for features that aren't implemented, such as the ability to pass the configuration file and slave files as command-line arguments:\n\n{code}\nUsage: spark-daemons.sh [--config confdir] [--hosts hostlistfile] [start|stop] command args...\n{code}\n\nWe should either implement these features or remove the options.", "comments": ["Assigned by request", "I have a couple of questions \n\n1) In case the --config argument is not a valid directory should the execution abort or should it continue taking the default path ?\n2) Can I go ahead and remove the --hosts argument ? Do we have a use-case where it would be required ? It can still be achieved by creating a separate conf directory and passing it as an argument.", "The changes are checked in.", "https://github.com/apache/incubator-spark/pull/69"], "derived": {"summary": "The spark-daemon. sh and spark-daemons.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Unimplemented configuration options in spark-daemon[s].sh - The spark-daemon. sh and spark-daemons."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/incubator-spark/pull/69"}]}}
{"project": "SPARK", "issue_id": "SPARK-628", "title": "Make deletion of EC2 security groups optional", "status": "Resolved", "priority": "Major", "reporter": "Josh Rosen", "assignee": "Peter Sankauskas", "labels": [], "created": "2012-12-01T23:07:46.000+0000", "updated": "2012-12-12T13:13:01.000+0000", "description": "The [recent change|https://github.com/mesos/spark/pull/306] to automatically delete EC2 security groups breaks support for custom security groups.  For example, I use launch clusters using custom security groups that open ports used by the YourKit remote debugger.\n\nI think that deletion of security groups should be optional and disabled by default to avoid surprising users with unexpected behavior or breaking existing deployments when users upgrade.", "comments": ["Fixed in https://github.com/mesos/spark/pull/323"], "derived": {"summary": "The [recent change|https://github. com/mesos/spark/pull/306] to automatically delete EC2 security groups breaks support for custom security groups.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Make deletion of EC2 security groups optional - The [recent change|https://github. com/mesos/spark/pull/306] to automatically delete EC2 security groups breaks support for custom security groups."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed in https://github.com/mesos/spark/pull/323"}]}}
{"project": "SPARK", "issue_id": "SPARK-629", "title": "Standalone job details page has strange value for number of cores", "status": "Resolved", "priority": "Minor", "reporter": "Josh Rosen", "assignee": "Josh Rosen", "labels": [], "created": "2012-12-04T14:50:23.000+0000", "updated": "2013-05-04T22:52:32.000+0000", "description": "When I view the job details page of a job running on a standalone cluster, I see the following strange output:\n\n{code}\nCores: 2147483647 (400 Granted )\n{code}\n\nI'm not sure where 2147483647 is coming from (it's Integer.MAX_VALUE).\n\nLooking at the code for this job details page, this is generated by the following:\n\n{code}\n        <li><strong>Cores:</strong>                                             \n          @job.desc.cores                                                       \n          (@job.coresGranted Granted                                            \n          @if(job.desc.cores == Integer.MAX_VALUE) {                            \n                                                                                \n          } else {                                                              \n            , @job.coresLeft                                                    \n          }                                                                     \n          )                                                                     \n        </li>     \n{code}\n\nI'm not sure what this is supposed to do; is the idea to display something like \"Cores: totalCores (x granted, y pending)\"?  Does Integer.MAX_VALUE have any special significance when used as the number of cores in a JobDescription?", "comments": ["This is still a problem in 0.7.", "Fixed in https://github.com/mesos/spark/pull/597"], "derived": {"summary": "When I view the job details page of a job running on a standalone cluster, I see the following strange output:\n\n{code}\nCores: 2147483647 (400 Granted )\n{code}\n\nI'm not sure where 2147483647 is coming from (it's Integer. MAX_VALUE).", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Standalone job details page has strange value for number of cores - When I view the job details page of a job running on a standalone cluster, I see the following strange output:\n\n{code}\nCores: 2147483647 (400 Granted )\n{code}\n\nI'm not sure where 2147483647 is coming from (it's Integer. MAX_VALUE)."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed in https://github.com/mesos/spark/pull/597"}]}}
{"project": "SPARK", "issue_id": "SPARK-630", "title": "Master web UI shows some finished/killed executors as running", "status": "Resolved", "priority": "Minor", "reporter": "Josh Rosen", "assignee": "Josh Rosen", "labels": [], "created": "2012-12-04T18:15:10.000+0000", "updated": "2013-05-04T22:52:12.000+0000", "description": "When I view a finished job's Job Details page on the standalone master, it shows all executors as RUNNING.  However, when I view any of the workers' pages, the same executor appears under the \"Finished Executors\" list.", "comments": ["Fixed in https://github.com/mesos/spark/pull/597"], "derived": {"summary": "When I view a finished job's Job Details page on the standalone master, it shows all executors as RUNNING. However, when I view any of the workers' pages, the same executor appears under the \"Finished Executors\" list.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Master web UI shows some finished/killed executors as running - When I view a finished job's Job Details page on the standalone master, it shows all executors as RUNNING. However, when I view any of the workers' pages, the same executor appears under the \"Finished Executors\" list."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed in https://github.com/mesos/spark/pull/597"}]}}
{"project": "SPARK", "issue_id": "SPARK-631", "title": "SPARK_LOCAL_IP environment variable should also affect spark.master.host", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "labels": [], "created": "2012-12-06T18:10:09.000+0000", "updated": "2012-12-08T01:11:30.000+0000", "description": "So that we can have a single variable for configuring the IP address that Spark uses.", "comments": ["This was actually happening correctly by default. I also updated it so that Spark prefers to bind to a non-loopback address in case InetAddress.getLocalHost returns a loopback one."], "derived": {"summary": "So that we can have a single variable for configuring the IP address that Spark uses.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "SPARK_LOCAL_IP environment variable should also affect spark.master.host - So that we can have a single variable for configuring the IP address that Spark uses."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was actually happening correctly by default. I also updated it so that Spark prefers to bind to a non-loopback address in case InetAddress.getLocalHost returns a loopback one."}]}}
{"project": "SPARK", "issue_id": "SPARK-632", "title": "Akka system names need to be normalized (since they are case-sensitive)", "status": "Closed", "priority": "Major", "reporter": "Matt Massie", "assignee": null, "labels": [], "created": "2012-12-07T11:44:42.000+0000", "updated": "2014-11-11T09:13:29.000+0000", "description": "The \"system\" name of the Akka full path is case-sensitive (see http://akka.io/faq/#what_is_the_name_of_a_remote_actor).\n\nSince DNS names are case-insensitive and we're using them in the \"system\" name, we need to normalize them (e.g. make them all lowercase).  Otherwise, users will find the \"workers\" will not be able to connect with the \"master\" even though the URI appears to be correct.\n\nFor example, Berkeley DNS occasionally uses names e.g. foo.Berkley.EDU. If I used foo.berkeley.edu as the master adddress, the workers would write to their logs that they are connecting to foo.berkeley.edu but failed to. They never show up in the master UI.  If use the foo.Berkeley.EDU address, everything works as it should. ", "comments": ["// link moved to http://doc.akka.io/docs/akka/current/additional/faq.html#what-is-the-name-of-a-remote-actor\n\nI believe having the hostname change case will still break Spark.  But after a search of the dev and user mailing lists over the past year I haven't seen any other users with this issue.\n\nA potential fix could be to call .toLower on the hostname in the Akka string across the cluster, but it's a little dirty to make this assumption everywhere.\n\nTechnically [hostnames ARE case insensitive|http://serverfault.com/questions/261341/is-the-hostname-case-sensitive] so Spark's behavior is wrong, but the issue is in the underlying Akka library.  This is the same underlying behavior where Akka requires that hostnames exactly match as well -- you can't use an IP address to refer to a Akka listening on a hostname -- SPARK-625.\n\nUntil Akka handles differently-cased hostnames I think can only be done with an ugly workaround.\n\nPossibly relevant Akka issues:\n- https://github.com/akka/akka/issues/15990\n- https://github.com/akka/akka/issues/15007\n\nMy preference would be to close this as \"Won't Fix\" until it's raised again as a problem from the community.\n\ncc [~rxin]", "Sounds good. In the future we might roll our own RPC rather than using Actor for RPC. I think the current RPC library built for the shuffle service is already ok with case insensitive hostnames."], "derived": {"summary": "The \"system\" name of the Akka full path is case-sensitive (see http://akka. io/faq/#what_is_the_name_of_a_remote_actor).", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Akka system names need to be normalized (since they are case-sensitive) - The \"system\" name of the Akka full path is case-sensitive (see http://akka. io/faq/#what_is_the_name_of_a_remote_actor)."}, {"q": "What updates or decisions were made in the discussion?", "a": "Sounds good. In the future we might roll our own RPC rather than using Actor for RPC. I think the current RPC library built for the shuffle service is already ok with case insensitive hostnames."}]}}
{"project": "SPARK", "issue_id": "SPARK-633", "title": "Support dropping blocks and RDDs from block manager", "status": "Resolved", "priority": "Major", "reporter": "Reynold Xin", "assignee": "Reynold Xin", "labels": [], "created": "2012-12-10T13:56:45.000+0000", "updated": "2013-01-20T12:52:22.000+0000", "description": null, "comments": ["First pull request for this ticket https://github.com/mesos/spark/pull/327", "Resolving since the pull request was merged."], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support dropping blocks and RDDs from block manager"}, {"q": "What updates or decisions were made in the discussion?", "a": "Resolving since the pull request was merged."}]}}
{"project": "SPARK", "issue_id": "SPARK-634", "title": "Track and display a read count for each block replica in BlockManager", "status": "Resolved", "priority": "Major", "reporter": "Reynold Xin", "assignee": "Patrick McFadin", "labels": [], "created": "2012-12-11T11:38:55.000+0000", "updated": "2020-05-17T18:20:56.000+0000", "description": null, "comments": ["This should be part of the exposed metrics.", "Hey [~rxin] - what do you mean by number of uses? Did you mean the number of machines it's on? That seems redundant with the list of machines which we now display. ", "Number of times a block is used.", "Closing just because of extreme age"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Track and display a read count for each block replica in BlockManager"}, {"q": "What updates or decisions were made in the discussion?", "a": "Closing just because of extreme age"}]}}
{"project": "SPARK", "issue_id": "SPARK-635", "title": "Pass a TaskContext object to compute() interface", "status": "Resolved", "priority": "Major", "reporter": "Reynold Xin", "assignee": "Reynold Xin", "labels": [], "created": "2012-12-13T00:13:28.000+0000", "updated": "2012-12-30T14:54:56.000+0000", "description": "\nTaskContext should allow compute() to register a callback, executed when the task finishes executing. \n\nThe use case is for limit on s3. The HadoopRDD interface should register a call back to close the stream when the task ends.\n", "comments": ["Actually, TaskContext exists already on the master side. Matei - any good idea for naming? This will be on the slave side.", "Ignore my previous comment - I will have a pull request soon.", "pull request: https://github.com/mesos/spark/pull/328"], "derived": {"summary": "TaskContext should allow compute() to register a callback, executed when the task finishes executing. The use case is for limit on s3.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Pass a TaskContext object to compute() interface - TaskContext should allow compute() to register a callback, executed when the task finishes executing. The use case is for limit on s3."}, {"q": "What updates or decisions were made in the discussion?", "a": "pull request: https://github.com/mesos/spark/pull/328"}]}}
{"project": "SPARK", "issue_id": "SPARK-636", "title": "Add mechanism to run system management/configuration tasks on all workers", "status": "Resolved", "priority": "Major", "reporter": "Josh Rosen", "assignee": null, "labels": ["bulk-closed"], "created": "2012-12-13T09:45:59.000+0000", "updated": "2019-05-21T05:37:14.000+0000", "description": "It would be useful to have a mechanism to run a task on all workers in order to perform system management tasks, such as purging caches or changing system properties.  This is useful for automated experiments and benchmarking; I don't envision this being used for heavy computation.\n\nRight now, I can mimic this with something like\n\n{code}\nsc.parallelize(0 until numMachines, numMachines).foreach { } \n{code}\n\nbut this does not guarantee that every worker runs a task and requires my user code to know the number of workers.\n\nOne sample use case is setup and teardown for benchmark tests.  For example, I might want to drop cached RDDs, purge shuffle data, and call {{System.gc()}} between test runs.  It makes sense to incorporate some of this functionality, such as dropping cached RDDs, into Spark itself, but it might be helpful to have a general mechanism for running ad-hoc tasks like {{System.gc()}}.", "comments": ["For the streaming branch we had to add a stricter version of sc.makeRDD (parallelize equivalent) that takes placement constraints for each partition. This could be used for a poor-mans version of what you are looking for, or we could implement in a way that pushes down to the scheduler and does not require explicit knowledge of the nodes in the cluster.", "Does broadcasting get us close enough to handling this or is this something we are still considering for the API?", "I feel like the broadcasting mechanism doesn't get me \"close\" enough to solve my issue (initialization of a logging system). That's partly because my initialization would be deferred (meaning a loss of useful logs), and also it could enable us to have some 'init' code that is guaranteed to only be evaluated once as opposed to implementing that 'guarantee' yourself, which can currently lead to bad practices.\n\nEdit: For some context, I'm approaching this issue from SPARK-650", "I agree, that's why I also feel that these issues are no duplicates. ", "If you have a logging system you want to initialize wouldn't using an object with lazy initialization on call be sufficient?"], "derived": {"summary": "It would be useful to have a mechanism to run a task on all workers in order to perform system management tasks, such as purging caches or changing system properties. This is useful for automated experiments and benchmarking; I don't envision this being used for heavy computation.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Add mechanism to run system management/configuration tasks on all workers - It would be useful to have a mechanism to run a task on all workers in order to perform system management tasks, such as purging caches or changing system properties. This is useful for automated experiments and benchmarking; I don't envision this being used for heavy computation."}, {"q": "What updates or decisions were made in the discussion?", "a": "If you have a logging system you want to initialize wouldn't using an object with lazy initialization on call be sufficient?"}]}}
{"project": "SPARK", "issue_id": "SPARK-637", "title": "Create troubleshooting checklist", "status": "Closed", "priority": "Major", "reporter": "Josh Rosen", "assignee": null, "labels": [], "created": "2012-12-13T11:55:59.000+0000", "updated": "2014-09-21T14:34:21.000+0000", "description": "We should provide a checklist for troubleshooting common Spark problems.\n\nFor example, it could include steps like \"check that the Spark code was copied to all nodes\" and \"check that the workers successfully connect to the master.\"", "comments": ["Last old-issue update for the day: this can be subsumed by https://issues.apache.org/jira/browse/SPARK-719 ?", "this is a good idea, and it will take a significant amount of effort. it looks like nothing has happened for almost 2 years. i'm going to close this, but feel free to re-open and push forward with it."], "derived": {"summary": "We should provide a checklist for troubleshooting common Spark problems. For example, it could include steps like \"check that the Spark code was copied to all nodes\" and \"check that the workers successfully connect to the master.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Create troubleshooting checklist - We should provide a checklist for troubleshooting common Spark problems. For example, it could include steps like \"check that the Spark code was copied to all nodes\" and \"check that the workers successfully connect to the master."}, {"q": "What updates or decisions were made in the discussion?", "a": "this is a good idea, and it will take a significant amount of effort. it looks like nothing has happened for almost 2 years. i'm going to close this, but feel free to re-open and push forward with it."}]}}
{"project": "SPARK", "issue_id": "SPARK-638", "title": "Standalone --cluster-type option broken in spark-ec2 due to SPARK_MASTER_IP setting", "status": "Resolved", "priority": "Major", "reporter": "Josh Rosen", "assignee": "Josh Rosen", "labels": [], "created": "2012-12-13T17:04:22.000+0000", "updated": "2012-12-13T18:08:16.000+0000", "description": "spark-ec2 has a --cluster-type option to launch standalone clusters, but this is broken because SPARK_MASTER_IP is set in the start-slaves.sh script but not in start-master.sh.  This causes the workers to connect to the master on the wrong address, which fails.", "comments": ["Pull request: https://github.com/mesos/spark/pull/330"], "derived": {"summary": "spark-ec2 has a --cluster-type option to launch standalone clusters, but this is broken because SPARK_MASTER_IP is set in the start-slaves. sh script but not in start-master.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Standalone --cluster-type option broken in spark-ec2 due to SPARK_MASTER_IP setting - spark-ec2 has a --cluster-type option to launch standalone clusters, but this is broken because SPARK_MASTER_IP is set in the start-slaves. sh script but not in start-master."}, {"q": "What updates or decisions were made in the discussion?", "a": "Pull request: https://github.com/mesos/spark/pull/330"}]}}
{"project": "SPARK", "issue_id": "SPARK-639", "title": "Standalone cluster should report executor exit codes more nicely to clients", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "2012-12-13T20:15:44.000+0000", "updated": "2014-11-11T08:58:00.000+0000", "description": "Right now they are just part of a string message, requiring string parsing to make sense of them.", "comments": ["It looks reporting executor exit statuses to the user was done in commit fa9df4a45daf5fd8b19df20c1fb7466bde3b2054 by [~woggle] on 12 Dec 2012 (one day before this ticket was created).\n\nWhen Spark submit was added I think it may have dropped these exit statuses, but they were re-added in SPARK-3759.\n\nI think this can be closed with a fix version of 0.7.0\n\n{noformat}\naash@aash-mbp ~/git/spark$ git log v0.6.2 | grep fa9df4a45daf5fd8b19df20c1fb7466bde3b2054\naash@aash-mbp ~/git/spark$ git log v0.7.0 | grep fa9df4a45daf5fd8b19df20c1fb7466bde3b2054\ncommit fa9df4a45daf5fd8b19df20c1fb7466bde3b2054\naash@aash-mbp ~/git/spark$ git log v0.6.2 | grep 'Normalize executor exit statuses and report them to the user.'\naash@aash-mbp ~/git/spark$ git log v0.7.0 | grep 'Normalize executor exit statuses and report them to the user.'\n    Normalize executor exit statuses and report them to the user.\naash@aash-mbp ~/git/spark$ git log | grep 'Normalize executor exit statuses and report them to the user.'\n    Normalize executor exit statuses and report them to the user.\naash@aash-mbp ~/git/spark$\n{noformat}\n\ncc [~rxin]", "Fixed according to [~aash] :)"], "derived": {"summary": "Right now they are just part of a string message, requiring string parsing to make sense of them.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Standalone cluster should report executor exit codes more nicely to clients - Right now they are just part of a string message, requiring string parsing to make sense of them."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed according to [~aash] :)"}]}}
{"project": "SPARK", "issue_id": "SPARK-640", "title": "Update Hadoop 1 version to 1.1.0 (especially on AMIs)", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "2012-12-13T21:03:15.000+0000", "updated": "2014-09-04T19:54:50.000+0000", "description": "Hadoop 1.1.0 has a fix to the notorious \"trailing slash for directory objects in S3\" issue: https://issues.apache.org/jira/browse/HADOOP-5836, so would be good to support on the AMIs.", "comments": ["This looks stale right? Hadoop 1 version has been at 1.2.1 for some time.", "[~pwendell] what is our Hadoop 1 version on AMIs now?"], "derived": {"summary": "Hadoop 1. 1.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Update Hadoop 1 version to 1.1.0 (especially on AMIs) - Hadoop 1. 1."}, {"q": "What updates or decisions were made in the discussion?", "a": "[~pwendell] what is our Hadoop 1 version on AMIs now?"}]}}
{"project": "SPARK", "issue_id": "SPARK-641", "title": "spark-ec2 standalone launch should create ~/mesos-ec2/slaves", "status": "Resolved", "priority": "Minor", "reporter": "Josh Rosen", "assignee": "Shivaram Venkataraman", "labels": [], "created": "2012-12-14T10:05:13.000+0000", "updated": "2013-04-05T19:45:07.000+0000", "description": "When launching a standalone cluster using the --cluster-type option, the ~/mesos-ec2/slaves file is not populated (this is normally done by the ~/mesos-ec2/setup script).  We should fix this, since users may still wish to use scripts like copy-dir to copy files.", "comments": ["I actually would prefer not to support a standalone mode EC2 cluster, or else to switch the default cluster to be the standalone one. The reason is that it's annoying to maintain two types of AMIs and two ways of running on EC2, and it confuses users (they will wonder why go for one type over the other). So maybe we should just switch the AMIs to use the standalone mode at some point.", "It might be useful to support this feature for our own testing purposes.\n\nIt's possible to support both cluster modes using a single AMI by running a different EC2-side configuration script from spark-ec2 depending on the cluster mode.  After launching the cluster, spark-ec2 could just run something similar to ~/mesos-ec2/setup, but for standalone mode.  The two scripts could share most of their code.  This would block on SPARK-521.", "The EC2 scripts in 0.7 properly support both standalone and Mesos clusters, so I'm marking this as fixed."], "derived": {"summary": "When launching a standalone cluster using the --cluster-type option, the ~/mesos-ec2/slaves file is not populated (this is normally done by the ~/mesos-ec2/setup script). We should fix this, since users may still wish to use scripts like copy-dir to copy files.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "spark-ec2 standalone launch should create ~/mesos-ec2/slaves - When launching a standalone cluster using the --cluster-type option, the ~/mesos-ec2/slaves file is not populated (this is normally done by the ~/mesos-ec2/setup script). We should fix this, since users may still wish to use scripts like copy-dir to copy files."}, {"q": "What updates or decisions were made in the discussion?", "a": "The EC2 scripts in 0.7 properly support both standalone and Mesos clusters, so I'm marking this as fixed."}]}}
{"project": "SPARK", "issue_id": "SPARK-642", "title": "spark-ec2 standalone launch should set SPARK_MEM and SPARK_JAVA_OPTS", "status": "Resolved", "priority": "Minor", "reporter": "Josh Rosen", "assignee": "Shivaram Venkataraman", "labels": [], "created": "2012-12-14T10:47:56.000+0000", "updated": "2013-04-05T19:48:04.000+0000", "description": "spark-ec2's standalone cluster launch script does not configure SPARK_MEM and SPARK_JAVA_OPTS.  This is done automatically when running under Mesos mode by using the scripts included in the AMI.\n\nWe should either have feature parity between the --cluster-type modes or remove the feature if it's too difficult to support both modes.", "comments": ["This was fixed in 0.7, which fixed standalone mode support in spark-ec2."], "derived": {"summary": "spark-ec2's standalone cluster launch script does not configure SPARK_MEM and SPARK_JAVA_OPTS. This is done automatically when running under Mesos mode by using the scripts included in the AMI.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "spark-ec2 standalone launch should set SPARK_MEM and SPARK_JAVA_OPTS - spark-ec2's standalone cluster launch script does not configure SPARK_MEM and SPARK_JAVA_OPTS. This is done automatically when running under Mesos mode by using the scripts included in the AMI."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was fixed in 0.7, which fixed standalone mode support in spark-ec2."}]}}
{"project": "SPARK", "issue_id": "SPARK-643", "title": "Standalone master crashes during actor restart", "status": "Resolved", "priority": "Major", "reporter": "Josh Rosen", "assignee": "Josh Rosen", "labels": [], "created": "2012-12-14T19:19:26.000+0000", "updated": "2014-11-06T17:33:50.000+0000", "description": "The standalone master will crash if it restarts due to an exception:\n\n{code}\n12/12/15 03:10:47 ERROR master.Master: Job SkewBenchmark wth ID job-20121215031047-0000 failed 11 times.\nspark.SparkException: Job SkewBenchmark wth ID job-20121215031047-0000 failed 11 times.\n        at spark.deploy.master.Master$$anonfun$receive$1.apply(Master.scala:103)\n        at spark.deploy.master.Master$$anonfun$receive$1.apply(Master.scala:62)\n        at akka.actor.Actor$class.apply(Actor.scala:318)\n        at spark.deploy.master.Master.apply(Master.scala:17)\n        at akka.actor.ActorCell.invoke(ActorCell.scala:626)\n        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:197)\n        at akka.dispatch.Mailbox.run(Mailbox.scala:179)\n        at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:516)\n        at akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259)\n        at akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975)\n        at akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479)\n        at akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)\n12/12/15 03:10:47 INFO master.Master: Starting Spark master at spark://ip-10-226-87-193:7077\n12/12/15 03:10:47 INFO io.IoWorker: IoWorker thread 'spray-io-worker-1' started\n12/12/15 03:10:47 ERROR master.Master: Failed to create web UI\nakka.actor.InvalidActorNameException:actor name HttpServer is not unique!\n[05aed000-4665-11e2-b361-12313d316833]\n        at akka.actor.ActorCell.actorOf(ActorCell.scala:392)\n        at akka.actor.LocalActorRefProvider$Guardian$$anonfun$receive$1.liftedTree1$1(ActorRefProvider.scala:394)\n        at akka.actor.LocalActorRefProvider$Guardian$$anonfun$receive$1.apply(ActorRefProvider.scala:394)\n        at akka.actor.LocalActorRefProvider$Guardian$$anonfun$receive$1.apply(ActorRefProvider.scala:392)\n        at akka.actor.Actor$class.apply(Actor.scala:318)\n        at akka.actor.LocalActorRefProvider$Guardian.apply(ActorRefProvider.scala:388)\n        at akka.actor.ActorCell.invoke(ActorCell.scala:626)\n        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:197)\n        at akka.dispatch.Mailbox.run(Mailbox.scala:179)\n        at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:516)\n        at akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259)\n        at akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975)\n        at akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479)\n        at akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)\n{code}\n\nWhen the Master actor restarts, Akka calls the {{postRestart}} hook.  [By default|http://doc.akka.io/docs/akka/snapshot/general/supervision.html#supervision-restart], this calls {{preStart}}.  The standalone master's {{preStart}} method tries to start the webUI but crashes because it is already running.\n\nI ran into this after a job failed more than 11 times, which causes the Master to throw a SparkException from its {{receive}} method.\n\nThe solution is to implement a custom {{postRestart}} hook.", "comments": ["I'd like to add unit tests for executor failures, but I can't seem to find a way to reproduce them in tests.  Writing a UDF that throws an exception just results in task failures; I want to completely kill the executor processes.\n\nI tried adding a call to System.exit() in a UDF, hoping that it would only be executed in the Executor's JVM, but this caused test runner to exit.  I was using local-cluster mode, which looks like it spawns separate JVMs for Executors.  Any ideas?", "You could add extra testing in the code to allow Executors to crash. Or, you can use the \"local-cluster\" mode, where System.exit will indeed crash an executor without killing the whole test runner.", "You should add extra testing in the code to allow Executors to crash. Or, you can use the \"local-cluster\" mode, where System.exit will indeed crash an executor without killing the whole test runner.\n\nMatei\n\n\n\n\n", "Restarted Akka actors have fresh states (see https://groups.google.com/d/topic/akka-user/HN5zEsMd_PA/discussion and http://doc.akka.io/docs/akka/snapshot/general/supervision.html#What_Restarting_Means), so allowing actors to restart might lead to unexpected behavior.  I propose that we add {{postRestart}} methods so that actor restarts always lead to crashes:\n\n{code}\n  override def postRestart(reason: Throwable) {\n    logError(\"Spark worker actor failed: \" + reason)\n    // Allowing the actor to restart will cause problems, because the new actor will have a fresh\n    // state; see https://groups.google.com/d/topic/akka-user/HN5zEsMd_PA/discussion\n    // We could try copying the state, but it's probably safer to just exit:\n    logError(\"Exiting because we do not restart failed worker actors\")\n    System.exit(1)\n  }\n{code}\n\nThis should go in at least the standalone Worker and Master actors (where I've observed problems related to actor restarts), but we might want to add it elsewhere."], "derived": {"summary": "The standalone master will crash if it restarts due to an exception:\n\n{code}\n12/12/15 03:10:47 ERROR master. Master: Job SkewBenchmark wth ID job-20121215031047-0000 failed 11 times.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Standalone master crashes during actor restart - The standalone master will crash if it restarts due to an exception:\n\n{code}\n12/12/15 03:10:47 ERROR master. Master: Job SkewBenchmark wth ID job-20121215031047-0000 failed 11 times."}, {"q": "What updates or decisions were made in the discussion?", "a": "Restarted Akka actors have fresh states (see https://groups.google.com/d/topic/akka-user/HN5zEsMd_PA/discussion and http://doc.akka.io/docs/akka/snapshot/general/supervision.html#What_Restarting_Means), so allowing actors to restart might lead to unexpected behavior.  I propose that we add {{postRestart}} methods so that actor restarts always lead to crashes:\n\n{code}\n  override def postRestart(reason: Throwable) {\n    logError(\"Spark worker actor failed: \" + reason)\n    // Allowing the actor to restart will cause problems, because the new actor will have a fresh\n    // state; see https://groups.google.com/d/topic/akka-user/HN5zEsMd_PA/discussion\n    // We could try copying the state, but it's probably safer to just exit:\n    logError(\"Exiting because we do not restart failed worker actors\")\n    System.exit(1)\n  }\n{code}\n\nThis should go in at least the standalone Worker and Master actors (where I've observed problems related to actor restarts), but we might want to add it elsewhere."}]}}
{"project": "SPARK", "issue_id": "SPARK-644", "title": "Jobs canceled due to repeated executor failures may hang", "status": "Resolved", "priority": "Major", "reporter": "Josh Rosen", "assignee": "Josh Rosen", "labels": [], "created": "2012-12-20T23:55:35.000+0000", "updated": "2014-11-06T17:33:56.000+0000", "description": "In order to prevent an infinite loop, the standalone master aborts jobs that experience more than 10 executor failures (see https://github.com/mesos/spark/pull/210).  Currently, the master crashes when aborting jobs (this is the issue that uncovered SPARK-643).  If we fix the crash, which involves removing a {{throw}} from the actor's {{receive}} method, then these failures can lead to a hang because they cause the job to be removed from the master's scheduler, but the upstream scheduler components aren't notified of the failure and will wait for the job to finish.\n\nI've considered fixing this by adding additional callbacks to propagate the failure to the higher-level schedulers.  It might be cleaner to move the decision to abort the job into the higher-level layers of the scheduler, sending an {{AbortJob(jobId)}} method to the Master.  The Client is already notified of executor state changes, so it may be able to make the decision to abort (or defer that decision to a higher layer).", "comments": [], "derived": {"summary": "In order to prevent an infinite loop, the standalone master aborts jobs that experience more than 10 executor failures (see https://github. com/mesos/spark/pull/210).", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Jobs canceled due to repeated executor failures may hang - In order to prevent an infinite loop, the standalone master aborts jobs that experience more than 10 executor failures (see https://github. com/mesos/spark/pull/210)."}]}}
{"project": "SPARK", "issue_id": "SPARK-645", "title": "Calling distinct() without parentheses fails", "status": "Resolved", "priority": "Trivial", "reporter": "Mark Hamstra", "assignee": "Mark Hamstra", "labels": [], "created": "2012-12-24T01:47:31.000+0000", "updated": "2012-12-24T08:05:46.000+0000", "description": "While distinct now supports a number of splits parameter and has a default value for that parameter, it only supports default calls of a form similar to rdd.distinct().persist and not rdd.distinct.persist.  The without-parentheses form should be allowed -- as it is for persist().", "comments": ["Pull request submitted.", "Committed it. Thanks Mark!"], "derived": {"summary": "While distinct now supports a number of splits parameter and has a default value for that parameter, it only supports default calls of a form similar to rdd. distinct().", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Calling distinct() without parentheses fails - While distinct now supports a number of splits parameter and has a default value for that parameter, it only supports default calls of a form similar to rdd. distinct()."}, {"q": "What updates or decisions were made in the discussion?", "a": "Committed it. Thanks Mark!"}]}}
{"project": "SPARK", "issue_id": "SPARK-646", "title": "Floating point overflow/underflow in LR examples", "status": "Resolved", "priority": "Major", "reporter": "Josh Rosen", "assignee": "Matei Alexandru Zaharia", "labels": [], "created": "2012-12-29T17:59:52.000+0000", "updated": "2013-08-06T23:24:46.000+0000", "description": "The SparkLR examples call scala.math.exp() with very large or small exponents, causing its result to be rounded to 0 or Infinity.  Is this a bug?\n\nI discovered this while porting the LR example to Python, because Python's math.exp() function rounds very small results to 0 but raises OverflowError for large results.\n\nIn Scala:\n\n{code}\nscala> import math.exp\nimport math.exp\n\nscala> math.exp(10000)\nres4: Double = Infinity\n\nscala> math.exp(-10000)\nres5: Double = 0.0\n{code}\n\nPython:\n\n{code}\nfrom math import exp\nexp(10000)\nTraceback (most recent call last):\n File \"<stdin>\", line 1, in <module>\nOverflowError: math range error\nexp(-10000)\n0.0\n{code}\n\nI added a call to println(\"\" + (-p.y * (w dot p.x))) in the map UDF in SparkLR and SparkHdfsLR to log the exponents, and in both cases I saw small exponents like\n{code}\n-4.967736504527945\n-1.0153344192159428\n0.4639647012587064\n{code}\n\nin the first round and huge exponents like\n\n{code}\n-3731.0565020800145\n469.3852842964799\n-2838.8348220771445\n{code}\n\nin all later rounds.\n\nThe examples calculate the gradients using\n\n{code}\n(1 / (1 + exp(-p.y * (w dot p.x))) - 1) * p.y * p.x.\n{code}\n\nThe exponent (w dot p.x) grows rapidly because the magnitudes of w's components grow rapidly.\n\nI'm not familiar enough with logistic regression to know whether this is common or how to fix this.\n\nThis could be a problem because a model whose weights have large magnitudes would always make predictions with extremely high confidence (e.g. p(y = 1 | x) is always 0 or 1, due to rounding).", "comments": ["This is still a problem, particularly in PySpark: https://groups.google.com/d/msg/spark-users/ngHhosQN2MY/WZmjHzCtfU0J", "This is no longer a problem in the new numpy-based LR."], "derived": {"summary": "The SparkLR examples call scala. math.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Floating point overflow/underflow in LR examples - The SparkLR examples call scala. math."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is no longer a problem in the new numpy-based LR."}]}}
{"project": "SPARK", "issue_id": "SPARK-647", "title": "ConnectionManager.sendMessage may create too many unnecessary connections", "status": "Resolved", "priority": "Major", "reporter": "Shane Huang", "assignee": "Shane Huang", "labels": [], "created": "2013-01-07T22:59:19.000+0000", "updated": "2013-01-10T17:54:04.000+0000", "description": "In ConnectionManager, sendMessage creates a new SendingConnection when connection host key is not found in connectionsById. But there might be too many unnecessary connections created before connectionsById is updated in the connection-manager-thread.run. In out test ConnectionMangerTest fails on \"connection reset by peer\" or \"timeout\" when there're too many message sending threads. We should make connectionRequests a map to track the connections by key so that connections can be reused.  ", "comments": ["Submitted a pull request for this issue @ https://github.com/mesos/spark/pull/356", "Shane fixed this in https://github.com/mesos/spark/pull/356. Will also merge it to 0.6 branch."], "derived": {"summary": "In ConnectionManager, sendMessage creates a new SendingConnection when connection host key is not found in connectionsById. But there might be too many unnecessary connections created before connectionsById is updated in the connection-manager-thread.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "ConnectionManager.sendMessage may create too many unnecessary connections - In ConnectionManager, sendMessage creates a new SendingConnection when connection host key is not found in connectionsById. But there might be too many unnecessary connections created before connectionsById is updated in the connection-manager-thread."}, {"q": "What updates or decisions were made in the discussion?", "a": "Shane fixed this in https://github.com/mesos/spark/pull/356. Will also merge it to 0.6 branch."}]}}
{"project": "SPARK", "issue_id": "SPARK-648", "title": "takeSample() with repetitions should be able to return more items than an RDD contains", "status": "Resolved", "priority": "Minor", "reporter": "Josh Rosen", "assignee": "Matei Alexandru Zaharia", "labels": [], "created": "2013-01-08T15:27:55.000+0000", "updated": "2013-08-07T10:45:42.000+0000", "description": "If I use takeSample() _with repetition_ and attempt to take more items than the RDD contains, I may receive fewer than {{num}} items:\n\n{code}\nscala> sc.parallelize(0 to 1).takeSample(true, 10, 42)\nres17: Array[Int] = Array(1, 0)\n\nscala> sc.parallelize(0 to 4).takeSample(true, 10, 42)\nres33: Array[Int] = Array(3, 2, 0, 3, 0)\n{code}\n\nIf we fix this, we should add more tests for sample() and takeSample(), since right now they're only called in one test in the JavaAPISuite.", "comments": ["When / where was this fixed?  Is there a unit test for the fix?"], "derived": {"summary": "If I use takeSample() _with repetition_ and attempt to take more items than the RDD contains, I may receive fewer than {{num}} items:\n\n{code}\nscala> sc. parallelize(0 to 1).", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "takeSample() with repetitions should be able to return more items than an RDD contains - If I use takeSample() _with repetition_ and attempt to take more items than the RDD contains, I may receive fewer than {{num}} items:\n\n{code}\nscala> sc. parallelize(0 to 1)."}, {"q": "What updates or decisions were made in the discussion?", "a": "When / where was this fixed?  Is there a unit test for the fix?"}]}}
{"project": "SPARK", "issue_id": "SPARK-649", "title": "Windows support for PySpark", "status": "Resolved", "priority": "Major", "reporter": "Josh Rosen", "assignee": "Matei Alexandru Zaharia", "labels": ["Starter"], "created": "2013-01-10T22:45:49.000+0000", "updated": "2013-09-01T22:27:34.000+0000", "description": "Provide Windows support for PySpark:\n\n- Add a {{pyspark.cmd}} script.\n- Maybe use sockets instead of pipes for communication between Java and the Python worker processes (does Spark's regular {{RDD.pipe()}} method work under Windows?).", "comments": ["I'm pretty sure pipe() works on Windows, though I haven't tried it. There is still a notion of standard in and out on Windows.", "Fixed here: https://github.com/mesos/spark/pull/885."], "derived": {"summary": "Provide Windows support for PySpark:\n\n- Add a {{pyspark. cmd}} script.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Windows support for PySpark - Provide Windows support for PySpark:\n\n- Add a {{pyspark. cmd}} script."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed here: https://github.com/mesos/spark/pull/885."}]}}
{"project": "SPARK", "issue_id": "SPARK-650", "title": "Add a \"setup hook\" API for running initialization code on each executor", "status": "Resolved", "priority": "Minor", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": ["bulk-closed"], "created": "2013-01-11T16:46:50.000+0000", "updated": "2021-05-25T01:42:39.000+0000", "description": "Would be useful to configure things like reporting libraries", "comments": ["As mentioned in SPARK-572 static classes' initialization methods are being \"abused\" to perform this functionality.\n\n[~matei] do you still feel that a per-executor initialization function is a hook that Spark should expose in its public API?", "Not [~matei] but I think this would be a good idea to have. Abusing another undocumented concept doesn't seem like a nice way to treat a useful and common use-case.", "I would need this feature as well to perform some initialization of the logging system (which reads its configuration from an external source rather than just a file).", "I have similar requirements to Michael's  this would be a very useful feature to have.", "I think this is a duplicate of SPARK-636 yes?", "To me, the two seem related, but not exact duplicates. SPARK-636 seems to aim for a more generic mechanism.", "Would people feel ok if we marked this as a duplicate of 636 since it does seem like this a subset of 636.", "I disagree that those issues are duplicates. Spark-636 looks for a generic way to execute code on the Executors, but not for a reliable and easy mechanism to execute code during Executor initialization.", "In practice, these should probably all be WontFix as it hasn't mattered enough to implement in almost 4 years. It really doesn't matter.", "Then somebody should please explain to me, how this doesn't matter or rather how certain use-cases are supposed to be solved. We need to initialize each JVM and connect it to our logging system, set correlation IDs, initialize contexts and so on. I guess that most users just have implemented work-arounds as we did, but in an enterprise environment, this is really not the preferable long-term solution to me. Plus, I think that it would really not be hard to implement this feature for someone who has knowledge about the Spark executor setup.", "Sorry, I mean the _status_ doesn't matter. Most issues this old are obsolete or de facto won't-fix. Resolving it or not doesn't matter.\n\nI would even say this is 'not a problem', because a simple singleton provides once-per-executor execution of whatever you like. It's more complex to make a custom mechanism that makes you route this via Spark. That's probably way this hasn't proved necessary.", "Sean, a singleton is not the best option in our case. The Spark Streaming executors are writing to HBase, we need to initialize the HBase connection. The singleton seems (or seemed when we tested it for our customer a few months after this issue was raised) to be created when the first RDD is processed by the executor, and not when the driver starts. This imposes very high processing time for the first events.", "If you need init to happen ASAP when the driver starts, isn't any similar mechanism going to be about the same in this regard? This cost is paid just once, and I don't think in general startup is very low latency for any Spark app.", "I also have to disagree with this being a duplicate or obsolete.\n\n[~oarmand] and [~Skamandros] already mentioned reasons regarding the duplication.\n\nAbout it being obsolete: I have seen multiple clients facing this problem, finding this issue and hoping it'd get fixed some day. I would hesitate a guess and say that most _users_ of Spark have no JIRA account here and do not register or log in just to vote for this issue. That said: This issue is (with six votes) in the top 150 out of almost 17k total issues in the Spark project.\n\nAs it happens this is a non-trivial thing to implement in Spark (as far as I can tell from my limited knowledge of the inner workings) so it's pretty hard for a \"drive by\" contributor to help here.\n\nYou had the discussion about community perception on the mailing list (re: Spark Improvement Proposals) and this issue happens to be one of those that at least I see popping up every once in a while in discussions with clients.\n\nI would love to see this issue staying open as a feature request and have some hope that someone someday will implement it.", "As you wish, but, I disagree with this type of reasoning about JIRAs. I dont think anyone has addressed why a singleton isn't the answer. I can think of corner cases, but, that's why I suspect it isn't something that has needed implementing. ", "I can only come up with three reasons at the moment. I hope they all make sense.\n\n1) Singletons/Static Initialisers run once per Class Loader where this class is being loaded/used. I haven't actually seen this being a problem (and it might actually be desired behaviour in this case) but making the init step explicit would prevent this from ever becoming one.\n2) I'd like to fail fast for some things and not upon first access (which might be behind a conditional somewhere)\n3) It is hard enough to reason about where some piece of code is running as it is (Driver or Task/Executor), adding Singletons to the mix makes it even more confusing.\n\n\nThank you for reopening!", "Reopening doesn't do anything by itself, or cause anyone to consider this. If this just sits for another year, it will have been a tiny part of a larger problem. I would ask those asking to keep this open to advance the discussion, or else I think you'd agree it eventually should be closed. (Here, I'm really speaking about hundreds of issues like this here, not so much this one.)\n\nPart of the problem is that I don't think the details of this feature request were ever elaborated. I think that if you dig into what it would mean, you'd find that a) it's kind of tricky to define and then implement all the right semantics, and b) almost any use case along these lines in my experience is resolved as I suggest, with a simple per-JVM initialization. If the response lately here is, well, we're not quite sure how that works, then we need to get to the bottom of that, not just insisting an issue stay open.\n\nTo your points:\n\n- The executor is going to load user code into one classloader, so we do have that an executor = JVM = classloader. \n- You can fail things as fast as you like by invoking this init as soon as like in your app.\n- It's clear where things execute, or else, we must assume app developers understand this or else all bets are off. The driver program executes things in the driver unless they're part of a distributed map() etc operation, which clearly execute on the executor.\n\nThese IMHO aren't reasons to design a new, different, bespoke mechanism. That has a cost too, if you're positing that it's hard to understand when things run where. \n\nThe one catch I see is that, by design, we don't control which tasks run on what executors. We can't guarantee init code runs on all executors this way. But, is it meaningful to initialize an executor that never sees an app's tasks? it can't be. Lazy init is a good thing and compatible with the Spark model. If startup time is an issue (and I'm still not clear on the latency problem mentioned above), then it gets a little more complicated, but, that's also a little more niche: just run a dummy mapPartitions at the outset on the same data that the first job would touch, even asynchronously if you like with other driver activities. No need to wait; it just gives the init a head-start on the executors that will need it straight away.\n\nThat's just my opinion of course, but I think those are the questions that would need to be answered to argue something happens here.", "> \"just run a dummy mapPartitions at the outset on the same data that the first job would touch\"\n\nBut this wouldn't work for Spark Streaming? (our case).", "It would work in this case to immediately schedule initialization on the executors because it sounds like data arrives immediately in your case. The part I am missing is how it can occur faster than this with another mechanism. ", "Data doesn't arrives necessarily immediately, but we need to ensure that when it arrives, lazy initialization doesn't introduce latency.", "Yeah that's a decent use case, because latency is an issue (streaming) and you potentially have time to set up before latency matters. \n\nYou can still use this approach because empty RDDs arrive if no data has, and empty RDDs can still be repartitioned. Here's a way to, once, if the first RDD has no data, do something once per partition, which ought to amount to at least once per executor:\n\n{code}\nvar first = true\nlines.foreachRDD { rdd =>\n  if (first) {\n    if (rdd.isEmpty) {\n      rdd.repartition(sc.defaultParallelism).foreachPartition(_ => Thing.initOnce())\n    }\n    first = false\n  }\n}\n{code}\n\n\"ought\", because, there isn't actually a guarantee that it will put the empty partitions on different executors. In practice, it seems to, when I just tried it.\n\nThat's a partial solution, but it's an optimization anyway, and maybe it helps you right now. I am still not sure it means this needs a whole mechanism, if this is the only type of use case. Maybe there are others.", "Ok, let me explain the specific problems that we have encountered, which might help to understand the issue and possible solutions:\n\nWe need to run some code on the executors before anything gets processed, e.g. initialization of the log system or context setup. To do this, we need information that is present on the driver, but not on the executors. Our current solution is to provide a base class for Spark function implementations which contains the information from the driver and initializes everything in its readObject method. Since multiple narrow-dependent functions may be executed on the same executor JVM subsequently, this class needs to make sure that initialization doesn't run multiple times. Sure, that's not hard to do, but if you mix setup and cleanup logic for functions, partitions and/or the JVM itself, it can get quite confusing without explicit hooks.\n\nSo, our solution basically works, but with that approach, you can't use lambdas for Spark functions, which is quite inconvenient, especially for simple map operations. Even worse, if you use a lambda or otherwise forget to extend the required base class, the initialization doesn't occur and very weird exceptions follow, depending on which resource your function tries to access during its execution. Or if you have very bad luck, no exception will occur, but the log messages will get logged to an incorrect destination. It's very hard to prevent such cases without an explicit initialization mechanism and in a team with several developers, you can't expect everyone to know what is going on there.", "This is still easy to do with mapPartitions, which can call {{initWithTheseParamsIfNotAlreadyInitialized(...)}} once per partition, which should guarantee it happens once per JVM before anything else proceeds. I don't think you need to bury it in serialization logic. I can see there are hard ways to implement this, but I believe an easy way is still readily available within the existing API mechanisms.", "But I'll need to have an RDD to do this, I can't just do it during the SparkContext setup - right now, we have multiple sources of RDDs and every developer would still need to know that they have to run this code after creating an RDD, won't they? Or is there some way to use a \"pseudo-RDD\" right after creation of the SparkContext to execute the init code on the executors?", "But, why do you need to do it before you have an RDD? You can easily make this a library function. Or, just some static init that happens on demand whenever a certain class is loaded. The nice thing about that is that it's transparent, just like with any singleton / static init in the JVM.\n\nIf you really want, you can make an empty RDD and repartition it and use that as a dummy, but it only serves to do some initialization early that would happen transparently anyway.", "What if I have a Hadoop InputFormat? Then, certain things happen before the first RDD exists, don't they?\n\nI'll give the solution with the empty RDD a shot next week, this sounds a little bit better than what we have right now, but it still relies on certain internals of Spark which are most likely undocumented and might change in future? I've had the feeling that Spark basically has a functional approach with the RDDs and executing anything on an empty RDD could be optimized to just do nothing?", "BTW I am not suggesting an \"empty RDD\" for your case. That was specific to the streaming scenario.\n\nFor this, again, why not just access some initialization method during class init of some class that is referenced wherever you want, including a custom InputFormat? This can be made to happen once per JVM (class loader), from any code, at class init time before anything else can happen. It's just a standard Java mechanism.\n\nIf you mean it requires some configuration not available at class-loading time you can still make such an init take place wherever, as soon as, such configuration is available. Even in an InputFormat.\n\nAlthough I can imagine corner cases where this becomes hard, I think it's over-thinking this to imagine a whole new lifecycle method to accomplish what basic JVM mechanisms allow.", "I agree that static initialization would solve the problem for cases where everything is known or can be loaded at class-loading time, e.g. from property files in the artifact itself.\n\nFor situations like RecordReaders, it might also work, because they have an initialize method where they get contextual information that could have been enriched with the required values from the driver.\n\nHowever, we also have other cases, where information from the driver is needed. Imagine the following case: We have a temporary directory in HDFS which is determined by the Oozie workflow instance ID. The driver knows this information, because it is provided by Oozie via main method arguments. The executor needs this information as well, e.g. to load some data that is required to initialize a static context. Then, the question arises: How does the information get to the executor?\n\nEither with the function instance which would mean that the developer of the function needs to know that he has to call an initialization method in every function or at least in every first function on an RDD (which he probably doesn't know, because he received the RDD from a different part of the application). Or with an explicit mechanism which is executed before the developer functions run on any executor. Which would lead me again to the \"empty RDD\" workaround.", "Yep, if you must pass some configuration, it generally can't happen magically at class-loading time. You can provide a \"initIfNeeded(conf)\" method that must be explicitly called in key places, but, that's simple and canonical Java practice.\n\nIn your example, there's no need to do anything. Just use the info in the function the executor runs. It's passed in the closure. This is entirely normal Spark.", "I am supporting Olivier Armand. We need a way in our Streaming job to setup an HBase connection per executor (and not per partition). A Singleton is not something we are looking at for this purpose.", "Why would a singleton not work? This is really the essential question in this thread.", "Sean, I agree this is the essential question in this thread. If we get this sorted out, then we are good and can achieve consensus on what to do with this ticket.\nA singleton \"works\" indeed. However, from a software engineering point of view it is not nice. There exists a class of Spark Streaming jobs that requires \"setup -> do -> cleanup\" semantics. The framework (in this case Spark Streaming) should explicitly support these semantics through appropriate API hooks. A singleton instead would hide these semantics and you would need to implement some laxy code to check whether an HBase connection was already setup or not; the singelton would need to do this for every write operation to HBase.\nI do not think that application logic (the Singleton within the Spark Streaming job) is the right place to wire in the \"setup -> do -> cleanup\" pattern. It is a generic pattern and there exists a class of Spark Streaming jobs (not only one specific Streaming job) that are based on this pattern.", "[~lars_francke][~Skamandros][~rneumann] If you think that this is an important feature, then write a design doc and open a PR.", "OK. Will do.", "A singleton is not really feasible if additional information is required which is known (or determined) by the driver and thus needs to be sent to the executors for the initialization to happen. In this case, the options are 1) use some side-channel that is \"magically\" inferred by the executor, 2) use an empty RDD, repartition it to the number of executors and run mapPartitions on it, 3) piggy-back the JavaSerializer to run the initialization before any function is called or 4) require every function which may need the resource to initialize it on its own.\n\nEach of these options has significant drawbacks in my opinion. While 4 sounds good for most cases, it has some  cons which I've described earlier (my comment from Oct 16) and make it unfeasible for our use-case. Option 1 might be possible, but the data flow wouldn't be all that obvious. Right now, we go with a mix of option 2 and 3 (try to determine the number of executors and if you can't, hijack the serializer), but really, this is hacked and might break in future releases of Spark.", "Thanks [~robert.neumann]! I am ready to help, if I can.", "Why? info X can be included in the closure, and the executor can call \"single.getInstance(X)\" to pass this info. Init happens only once in any event.", "Sure it can be included in the closure and this was also our first solution to the problem. But if the application has many layers and you need the resource which requires info X to initialize often, it soon gets very inconvenient because you have to pass X around a lot and pollute your APIs.\n\nThus, our next solution was to create a base function class which takes X in its constructor and makes sure that the resource is initialized on the executor side if it wasn't before. The drawback of this solution is that the function developer can forget to extend the function base class and then he may or may not be able to access the resource depending on whether a function has run before on the executor which performed the initialization. This is really error-prone (actually led to errors) and even if done correctly, prevents lambdas from beeing used for functions.\n\nAs a result, we now use the \"empty RDD\" approach or piggy-back the Spark JavaSerializer. Both works fine and initializes the executor-side resource properly on all executors. So, from a function developer's point-of-view that's nice, but overall, the solution relies on Spark internals to work which is why I would rather have an explicit mechanism to perform such an initialization.", "If you only try to propagate information, then you can use SparkContext.localProperties and the TaskContext on the executor side. They provide the machinery to do this.", "A creatively applied broadcast variable might also do the trick BTW.", "No, it's not just about propagating information - some code actually needs to be run. We have some static utilities which need to be initialized, but they don't know anything about Spark but are rather provided by external libraries. Thus, we need to actually trigger the initialization on all executors. The only other way that I see is to wrap all access to those external utilities with something on our side that is Spark-aware and initializes them if needed. But I think compared to this, our current solution is better.", "Both suggested workarounds here are lacking or broken / actively harmful, afaict, and the use case is real and valid.\n\nThe ADAM project struggled for >2 years with this problem:\n\n- [a 3rd-party {{OutputFormat}} required this field to be set|https://github.com/HadoopGenomics/Hadoop-BAM/blob/eb688fb90c60e8c956f9d1e4793fea01e3164056/src/main/java/org/seqdoop/hadoop_bam/KeyIgnoringAnySAMOutputFormat.java#L93]\n- the value of the field is computed on the driver, and needs to somehow be sent to and set in each executor JVM.\n\nh3. {{mapPartitions}} hack\n\n[Some attempts to set the field via a dummy {{mapPartitions}} job|https://github.com/hammerlab/adam/blob/b87bfb72c7411b5ea088b12334aa1b548102eb4b/adam-core/src/main/scala/org/bdgenomics/adam/rdd/read/AlignmentRecordRDDFunctions.scala#L134-L146] actually added [pernicious, non-deterministic bugs|https://github.com/bigdatagenomics/adam/issues/676#issuecomment-219347677].\n\nIn general Spark seems to provide no guarantees that 1 tasks will get scheduled on each executor in such a situation:\n\n- in the above, node locality resulted in some executors being missed\n- dynamic-allocation also offers chances for executors to come online later and never be initialized\n\nh3. object/singleton initialization\n\nHow can one use singleton initialization to pass an object from the driver to each executor? Maybe I've missed this in the discussion above.\n\nIn the end, ADAM decided to write the object to a file and route that file's path to the {{OutputFormat}} via a hadoop configuration value, which is pretty inelegant.\n\nh4. Another use case\n\nI have another need for this atm where regular lazy-object-initialization is also insufficient: [due to a rough-edge in Scala programs' classloader configuration, {{FileSystemProvider}}'s in user JARs are not loaded properly|https://github.com/scala/bug/issues/10247].\n\n[A workaround discussed in the 1st post on that issue fixes the problem|https://github.com/hammerlab/spark-commands/blob/1.0.3/src/main/scala/org/hammerlab/commands/FileSystems.scala#L8-L20], but needs to be run before {{FileSystemProvider.installedProviders}} is first called on the JVM, which can be triggered by numerous {{java.nio.file}} operations.\n\nI don't see a clear way to work in code in that will always lazily call my {{FileSystems.load}} function on each executor, let alone ensure that it happens before any code in the JAR calls e.g.\n {{Paths.get}}.", "[~Skamandros] how did you manage to hook `JavaSerializer`? I tried doing so myself, by defining a new subclass, but then I need to make sure that new class is installed on all executors. Meaning I have to copy a .jar on all my nodes manually. For some reason Spark won't try looking for the serializer inside my application JAR.", "[~Skamandros] - I would also like to know about hooking 'JavaSerializer'. I have a similar use case where I need to initialize set of objects/resources on each executor. I would also like to know if anybody has a way to hook into some \"clean up\" on each executor when 1) the executor shutdown 2) when a batch finishes and before next batch starts", "In a nutshell, we have our own class \"MySerializer\" which is derived from {{org.apache.spark.serializer.JavaSerializer}} and performs our custom initialization in {{MySerializer#newInstance}} before calling the super method {{org.apache.spark.serializer.JavaSerializer#newInstance}}. Then, when building the SparkConf for initialization of the SparkContext, we add {{pSparkConf.set(\"spark.closure.serializer\", MySerializer.class.getCanonicalName());}}.\n\nWe package this with our application JAR and it works. So I think you have to look at your classpath configuration [~mboes]. In our case, the JAR which contains the closure serializer is listed in the following properties (we use Spark 1.5.0 on YARN in cluster mode):\n* driver.extraClassPath\n* executor.extraClassPath\n* yarn.secondary.jars\n* spark.yarn.secondary.jars\n* spark.driver.extraClassPath\n* spark.executor.extraClassPath\n\nIf I recall it correctly, the variants without the \"spark.\" prefix are produced by us because we prefix all of our properties with \"spark.\" to transfer them via Oozie and unmask them again later, so you should only need the properties with the \"spark.\" prefix.\n\nRegarding the questions of [~riteshtijoriwala]: 1) Please see the related issue SPARK-1107. 2) You can add a TaskCompletionListener with {{org.apache.spark.TaskContext#addTaskCompletionListener(org.apache.spark.util.TaskCompletionListener)}}. To get the current TaskContext on the executor, just use {{org.apache.spark.TaskContext#get}}. We have some functionality to log the progress of a function in fixed intervals (e.g. every 1,000 records). To do this, you can use mapPartitions with a custom iterator. ", "[~Skamandros] - Any similar tricks for spark 2.0.0? I see the config option to set the closure serializer has been removed - https://issues.apache.org/jira/browse/SPARK-12414. Currently we do \"set of different things\" to ensure our classes are loaded/instantiated before spark starts execution of its stages. It would be nice to consolidate this in one place/hook.", "[~riteshtijoriwala] - Sorry, but I am not familiar with Spark 2.0.0 yet. But what I can say is that we have raised a Cloudera support case to address this issue so maybe we can expect some help from this side.", "I can't understand how people are dismissing this as not an issue.  There are many cases where you need to initialize something on an executor, and many of them need input from the driver.  All of the given workarounds are terrible hacks and at best force bad design, and at worst introduce confusing and non-deterministic bugs.  Any time that the recommended solution to a common problem that many people are having is to abuse the Serializer in order to trick it into executing non-serialization code it seems obvious that there's a missing capability in the system. \n\nThe fact that executors can come on and offline at any time during the run makes it especially essential that we have a robust way of initializing them.  I just really don't understand the opposition to adding an initialization hook, it would solve so many problems in a clean way and doesn't seem like it would be particularly problematic on its own.", "I still don't see an argument against my primary suggestion: the singleton. The last comment on it just said, oh, how do you do it? it's quite possible. Nothing to do with the serializer.", "Please see my comment from 05/Dec/16 12:39 and the following discussion - we are kind of going in circles here. I tried to explain the (real) problems we were facing as good as I can and which solution we applied to them and why other solutions have been dismissed. The fact is: There are numerous people here who seem to have the same issues and are glad to apply the workaround because \"using the singleton\" doesn't seem to provide a solution to them either. Probably we all don't understand how to do this but then again there seems to be something missing - at least documentation, doesn't it? What I can tell you in addition is that we have concerned experienced developers with the topic who have used quite a few singletons.", "Are you looking for an example of how it works? something like this, for what I assume is the common case of something like initializing a connection to an external resource:\n\n{code}\nval config = ...\ndf.mapPartitions { it =>\n  MyResource.initIfNeeded(config)\n  it.map(...)\n}\n\n...\n\nobject MyResource {\n  private var initted = false\n  def initIfNeeded(config: Config): Unit = this.synchronized {\n    if (!initted) {\n      initializeResource(config)\n      initted = true\n  }\n}\n{code}\n\nIf config is big, or tricky to pass around, that too can be read directly from a location, or wrapped up in some object in your code. It can actually be:\n\n{code}\ndf.mapPartitions { it =>\n  MyResource.initIfNeeded()\n  it.map(...)\n}\n\n...\n\nobject MyResource {\n  private var initted = false\n  def initIfNeeded(): Unit = this.synchronized {\n    if (!initted) {\n      val config = getConf()\n      initializeResource(config)\n      initted = true\n  }\n}\n{code}\n\nYou get the idea. This is not a special technique, not even really singletons. Just making a method that executes the first time it's called and then does nothing after. \nIf you don't like having to call initResource -- call that in whatever code produces the resource connection or whatever.\n\nWe can imagine objections and answers like this all day I'm sure. I think it covers all use cases I can imagine that a setup hook does, so the question is just is it easy enough? You're saying it's unusably hard, and proposing some hack on the serializer that sounds much more error-prone. I just cannot agree with this. This is much simpler than other solutions people are arguing against here, which I also think are too complex. Was it just a misunderstanding of the proposal?\n\n[~louisb@broadinstitute.org] have you considered the implications of the semantics of a setup hook? for example, if setup fails on an executor, can you schedule a task that needed it? how do you track that? Here, the semantics are obvious.", "[~srowen]  Thanks for the reply and the example.  Unfortunately, I still believe that the singleton approach doesn't work well for our use case.  \n\nWe don't have a single resource which needs initialization and can always be wrapped in a singleton.  We have a sprawl of legacy dependencies that need to be initialized in certain ways before use, and then can be called into from literally hundreds of entry points.  One of the things that needs initializing is the set of FileSystemProviders that [~rdub] mentioned above.  This has to be done before potentially any file access in our dependencies.  It's implausible to wrap all of our library code into singleton objects and it's difficult to always call initResources() before every library call.  It requires a lot of discipline on the part of the developers.  Since we develop a framework for biologists to use to write tools, any thing that has to be enforced by convention isn't ideal and is likely to cause problems.  People will forget to start their work by calling initResources() or worse, they'll remember to call initResources(), but only at the start of the first stage.  Then they'll run into issues when executors die and are replaced during a later stage and the initialization doesn't run on the new executor.\n\nFor something that could be cleanly wrapped in a singleton I agree that the semantics are obvious, but for the case where you're calling init() before running your code, the semantics are confusing and error prone.  \n\nI'm sure there are complications from introducing a setup hook, but the one you mention seems simple enough to me.  If a setup fails, that executor is killed and can't schedule tasks.  There would probably have to be a mechanism for timing out after a certain number of failed executor starts, but I suspect that that exists already in some fashion for other sorts of failures.\n", "I can also imagine cases involving legacy code that make this approach hard to implement. Still, it's possible with enough 'discipline', but this is true of wrangling any legacy code. I don't think the question of semantics is fully appreciated here. Is killing the app's other tasks on the same executor reasonable behavior? how many failures are allowed by default by this new mechanism? what do you do if init never returns? for how long? Are you willing to reschedule the task on another executor? how does it interact with locality? I know, any change raises questions, but this one raises a lot.\n\nIt's a conceptual change in Spark and I'm just sure it's not going to happen 3 years in. Tasks have never had special status or lifecycle w.r.t. executors and that's a positive thing, really.", "I need a hook too. Some case, We need init something like spring initbean :(", "Hi,\r\nWe had an application run on spark cluster to a secured hdfs(kerberos)\r\nBecause spark had not supported for kerberos yet, it will be convenient for us that spark supports a setup hook to login as an user on each executor.\r\nCan u figure out another solution for us? (run on yarn mode isn't an option)\r\n", "I too have this problem. It seems that Apache Flink solves this quite nicely by having \"RichFunction\" variants for operations like map, filter, reduce etc. A RichFunction, such as RichMapFunction, provides open(Configuration parameters) and close() methods which can be used to run setup and teardown code once per worker and also initialise the worker from primitive key-value pairs.", "We encountered an issue with the combination of lazy static loading and speculation.\nBecause speculation kills tasks it might kill while loading lazy static classes which make them unusuable and later all application might fail for noclassdeferror", "Folks may be interested in SPARK-24918.  perhaps one should be closed a duplicate of the other, but for now there is some discussion on both, so I'll leave them open for the time being"], "derived": {"summary": "Would be useful to configure things like reporting libraries.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add a \"setup hook\" API for running initialization code on each executor - Would be useful to configure things like reporting libraries."}, {"q": "What updates or decisions were made in the discussion?", "a": "Folks may be interested in SPARK-24918.  perhaps one should be closed a duplicate of the other, but for now there is some discussion on both, so I'll leave them open for the time being"}]}}
{"project": "SPARK", "issue_id": "SPARK-651", "title": "Port sample()/takeSample() to PySpark", "status": "Resolved", "priority": "Major", "reporter": "Josh Rosen", "assignee": "Andre Schumacher", "labels": [], "created": "2013-01-13T00:05:52.000+0000", "updated": "2013-09-01T15:15:24.000+0000", "description": "Due to batching, PySpark can't directly call Spark's sample() and takeSample() methods.  Now that PySpark has mapPartitionsWithSplit(), we need to port the sample() / takeSample() code to Python.\n\nWe should probably resolve SPARK-648 before porting the code.\n\nSampling with repetition is implemented by sampling from a Poisson distribution.  NumPy has a fast implementation of this, but I don't want to rely on the NumPy library because we can't easily package it with PySpark (the compiled C extensions are platform / architecture specific).\n\nWe might be able to write code to use NumPy as a performance optimization if it's installed, falling back on a pure-Python implementation otherwise.  To ensure deterministic behavior, it's important that the NumPy and pure-Python implementations return the same results.", "comments": ["This issue is now addressed in this pull request:\n\nhttps://github.com/mesos/spark/pull/861"], "derived": {"summary": "Due to batching, PySpark can't directly call Spark's sample() and takeSample() methods. Now that PySpark has mapPartitionsWithSplit(), we need to port the sample() / takeSample() code to Python.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "Port sample()/takeSample() to PySpark - Due to batching, PySpark can't directly call Spark's sample() and takeSample() methods. Now that PySpark has mapPartitionsWithSplit(), we need to port the sample() / takeSample() code to Python."}, {"q": "What updates or decisions were made in the discussion?", "a": "This issue is now addressed in this pull request:\n\nhttps://github.com/mesos/spark/pull/861"}]}}
{"project": "SPARK", "issue_id": "SPARK-652", "title": "Propagate exceptions from PySpark workers to the driver", "status": "Resolved", "priority": "Major", "reporter": "Josh Rosen", "assignee": null, "labels": [], "created": "2013-01-13T00:08:08.000+0000", "updated": "2013-01-31T22:00:43.000+0000", "description": "PySpark should propagate exceptions thrown by Python workers so that they are visible to the Python driver.  This would greatly simplify debugging.", "comments": ["Duplicate of SPARK-673"], "derived": {"summary": "PySpark should propagate exceptions thrown by Python workers so that they are visible to the Python driver. This would greatly simplify debugging.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Propagate exceptions from PySpark workers to the driver - PySpark should propagate exceptions thrown by Python workers so that they are visible to the Python driver. This would greatly simplify debugging."}, {"q": "What updates or decisions were made in the discussion?", "a": "Duplicate of SPARK-673"}]}}
{"project": "SPARK", "issue_id": "SPARK-653", "title": "Add accumulators to PySpark", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "labels": [], "created": "2013-01-13T14:32:36.000+0000", "updated": "2013-01-20T11:01:34.000+0000", "description": "Doesn't seem like it would be *that* bad to implement.", "comments": ["Added in https://github.com/mesos/spark/pull/387"], "derived": {"summary": "Doesn't seem like it would be *that* bad to implement.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add accumulators to PySpark - Doesn't seem like it would be *that* bad to implement."}, {"q": "What updates or decisions were made in the discussion?", "a": "Added in https://github.com/mesos/spark/pull/387"}]}}
{"project": "SPARK", "issue_id": "SPARK-654", "title": "Use ID of hash function when comparing Python partitioner objects in equals()", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Josh Rosen", "labels": [], "created": "2013-01-13T20:24:51.000+0000", "updated": "2013-01-20T17:09:19.000+0000", "description": "Right now we might compare two Python partitioners as equal even if they have different hash functions.", "comments": ["We should fix this, although it isn't currently a correctness problem because joins / groups / cogroups are not implemented in terms of the Java / Scala implementations, so PythonPartitioners are never compared for equality.\n\nThis reminds me that I should implement a co-partitioning-aware joins in PySpark, for which I'll open a different issue."], "derived": {"summary": "Right now we might compare two Python partitioners as equal even if they have different hash functions.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Use ID of hash function when comparing Python partitioner objects in equals() - Right now we might compare two Python partitioners as equal even if they have different hash functions."}, {"q": "What updates or decisions were made in the discussion?", "a": "We should fix this, although it isn't currently a correctness problem because joins / groups / cogroups are not implemented in terms of the Java / Scala implementations, so PythonPartitioners are never compared for equality.\n\nThis reminds me that I should implement a co-partitioning-aware joins in PySpark, for which I'll open a different issue."}]}}
{"project": "SPARK", "issue_id": "SPARK-655", "title": "Implement co-partitioning aware joins in PySpark", "status": "Resolved", "priority": "Major", "reporter": "Josh Rosen", "assignee": null, "labels": [], "created": "2013-01-14T15:22:08.000+0000", "updated": "2015-02-17T00:09:36.000+0000", "description": "Implement co-partitioning aware joins in PySpark", "comments": [], "derived": {"summary": "Implement co-partitioning aware joins in PySpark.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Implement co-partitioning aware joins in PySpark - Implement co-partitioning aware joins in PySpark."}]}}
{"project": "SPARK", "issue_id": "SPARK-656", "title": "Let Amazon choose our EC2 clusters' availability zone if the user does not specify one", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "2013-01-15T18:26:57.000+0000", "updated": "2014-11-06T07:00:34.000+0000", "description": "Amazon will automatically assign the zone with the most free resources if you don't specify one in your request.", "comments": [], "derived": {"summary": "Amazon will automatically assign the zone with the most free resources if you don't specify one in your request.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Let Amazon choose our EC2 clusters' availability zone if the user does not specify one - Amazon will automatically assign the zone with the most free resources if you don't specify one in your request."}]}}
{"project": "SPARK", "issue_id": "SPARK-657", "title": "Don't use multiple loopback IP addresses in unit tests", "status": "Resolved", "priority": "Blocker", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "labels": [], "created": "2013-01-16T11:07:17.000+0000", "updated": "2013-01-27T23:18:21.000+0000", "description": "Right now we require users to manually alias their interface as 127.100.0.1, 2, etc, at least on some systems (Mac OS X).", "comments": ["By the way, the work-around for this is \n\nsudo ifconfig lo0 add 127.100.0.1\nsudo ifconfig lo0 add 127.100.0.2\nsudo ifconfig lo0 add 127.100.0.3\n\netc.", "Closed by commit https://github.com/mesos/spark/commit/44b4a0f88fcb31727347b755ae8ec14d69571b52"], "derived": {"summary": "Right now we require users to manually alias their interface as 127. 100.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Don't use multiple loopback IP addresses in unit tests - Right now we require users to manually alias their interface as 127. 100."}, {"q": "What updates or decisions were made in the discussion?", "a": "Closed by commit https://github.com/mesos/spark/commit/44b4a0f88fcb31727347b755ae8ec14d69571b52"}]}}
{"project": "SPARK", "issue_id": "SPARK-658", "title": "Make Spark execution time logging more obvious and easier to read", "status": "Resolved", "priority": "Major", "reporter": "Reynold Xin", "assignee": "Patrick McFadin", "labels": [], "created": "2013-01-16T11:40:54.000+0000", "updated": "2013-01-29T21:30:32.000+0000", "description": "I believe we log some execution time, but they are hard to read. Making those execution time (by stages) easier to read would be a great start to performance analysis.", "comments": [], "derived": {"summary": "I believe we log some execution time, but they are hard to read. Making those execution time (by stages) easier to read would be a great start to performance analysis.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "Make Spark execution time logging more obvious and easier to read - I believe we log some execution time, but they are hard to read. Making those execution time (by stages) easier to read would be a great start to performance analysis."}]}}
{"project": "SPARK", "issue_id": "SPARK-659", "title": "The master web interface is broken for Scala 2.10", "status": "Closed", "priority": "Major", "reporter": "Eric Christiansen", "assignee": null, "labels": [], "created": "2013-01-18T16:27:38.000+0000", "updated": "2013-05-14T15:30:16.000+0000", "description": "To reproduce:\n\nBuild branch scala-2.10.\n\nEdit the \"run\" script to use Scala 2.10 and comment out the REPL stuff.\n\nLaunch with \"./run spark.deploy.master.Master\".\n\nVisit localhost:8080 in a browser.\n\nThis generates a MatchError: None at MasterWebUI.scala:28\n\nThanks!\n\nAlso, it would be nice if you used a Github issue tracker.", "comments": ["This is fixed in the current 2.10 branch. Hrm, I don't think I have permissions to close this.", "Closing this issue, since [~emchristiansen]'s comment says that it's fixed.  If this is still broken, please re-open this issue."], "derived": {"summary": "To reproduce:\n\nBuild branch scala-2. 10.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "The master web interface is broken for Scala 2.10 - To reproduce:\n\nBuild branch scala-2. 10."}, {"q": "What updates or decisions were made in the discussion?", "a": "Closing this issue, since [~emchristiansen]'s comment says that it's fixed.  If this is still broken, please re-open this issue."}]}}
{"project": "SPARK", "issue_id": "SPARK-660", "title": "Add StorageLevel support in Python", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Aaron Davidson", "labels": ["Starter"], "created": "2013-01-19T22:53:13.000+0000", "updated": "2013-09-10T10:59:01.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add StorageLevel support in Python"}]}}
{"project": "SPARK", "issue_id": "SPARK-661", "title": "Java unit tests don't seem to run with Maven", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "2013-01-20T23:44:52.000+0000", "updated": "2014-07-26T20:04:52.000+0000", "description": "I don't see them in the test output. Definitely important to fix this, though they do run with SBT or if you launch them manually.", "comments": ["The Java API tests are written using JUnit.  In sbt, I had to add a plugin so that scalatest picked up those tests:\n\n{code}\n\"com.novocode\" % \"junit-interface\" % \"0.8\" % \"test\",\n{code}\n\nMaybe we have to do something similar with Maven.", "Is this still an issue?\n\nWhen I just ran,\n{code}\n$ mvn -Phadoop1 clean test\n{code}\nall the test were executed as expected.\n\nMaven finds tests by looking for any source that starts or ends with Test, e.g. MyTest.scala, TestMine.scala. You can also just drop the tests in the Maven test directory for a (sub-)project. For example, a typical project layout is.\n\n{code}\nproject/src\nproject/src/main/java\nproject/src/main/resources\nproject/src/main/scala\nproject/src/test/java\nproject/src/test/resources\nproject/src/test/scala\n{code}\n\nIdeally, we would have a test class for each source file, e.g. Foo.java FooTest.java, Bar.scala, BarTest.scala.\n\n\n\n\n\nThe output of all tests is in the 'surefire-reports' directory, e.g.\n{code}\n$ find . -name \"surefire-reports\"\n./core/target/surefire-reports\n./bagel/target/surefire-reports\n./streaming/target/surefire-reports\n{code}\nNote: these reports are what Jenkins will read to present information about the state of the build.\n\n\n", "Did you see JavaAPISuite run? There are two test suites called this (one in spark and one in spark.streaming) that are written in Java, as JUnit tests. I don't think the ScalaTest plugin runs them."], "derived": {"summary": "I don't see them in the test output. Definitely important to fix this, though they do run with SBT or if you launch them manually.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Java unit tests don't seem to run with Maven - I don't see them in the test output. Definitely important to fix this, though they do run with SBT or if you launch them manually."}, {"q": "What updates or decisions were made in the discussion?", "a": "Did you see JavaAPISuite run? There are two test suites called this (one in spark and one in spark.streaming) that are written in Java, as JUnit tests. I don't think the ScalaTest plugin runs them."}]}}
{"project": "SPARK", "issue_id": "SPARK-662", "title": "Executor should only download files & jars once", "status": "Resolved", "priority": "Minor", "reporter": "Imran Rashid", "assignee": "Josh Rosen", "labels": [], "created": "2013-01-21T15:34:29.000+0000", "updated": "2014-07-28T05:25:47.000+0000", "description": "Executor.updateDependencies is called by each task, without any synchronization.  This means that all threads try to simultaneously download jars & files from the client host, which make the fetch much slower if there are many threads per node.", "comments": ["Looks like I fixed this in LocalScheduler.updateDependencies (https://github.com/JoshRosen/spark/commit/bd237d4a9d7f08eb143b2a2b8636a6a8453225ea) but not for Executor.  I'm actually working on some fixes for addFile() / addJar(), so I'll add synchronization.", "This should be fixed in master, but we should backport the fix to branch-0.6.", "I was planning to backport this to branch-0.6, but I never got around to it.  Do we still want to backport performance improvements like this, or are we only backporting bugfixes?  Can I mark this as resolved?", "Marking as resolved, since it looks like we're no longer backporting bugfixes to branch-0.6 (?)."], "derived": {"summary": "Executor. updateDependencies is called by each task, without any synchronization.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "Executor should only download files & jars once - Executor. updateDependencies is called by each task, without any synchronization."}, {"q": "What updates or decisions were made in the discussion?", "a": "Marking as resolved, since it looks like we're no longer backporting bugfixes to branch-0.6 (?)."}]}}
{"project": "SPARK", "issue_id": "SPARK-663", "title": "Implement Fair scheduler within ClusterScheduler", "status": "Resolved", "priority": "Major", "reporter": "Bradley Freeman", "assignee": "xiajunluan", "labels": [], "created": "2013-01-23T14:41:16.000+0000", "updated": "2014-03-30T23:32:00.000+0000", "description": "Each SparkContext currently has a simple FIFO scheduler for task assignment.  Applications which use long-running jobs (as in Shark) on the cluster submit tasks which are processed in a strict FIFO fashion.\n\nIn the Shark example, the FIFO scheduler causes Spark to process one query at a time, which heavily impacts short running queries that are submitted after the long running tasks.  In other words, a large query will block completion of a smaller query until the big query has completed.", "comments": ["Moving some notes from my email to Harold. Quoting myself:\n\nIf the purpose is to prevent large queries from blocking small queries - how about implement the fair sharing policy in TaskScheduler level so we can avoid changing any of the APIs? It is not entirely \"fair\" because a query with multiple stages will gain more shares - but I think it would work for most purposes?\n", "From Patrick:\n\nTo be even more concrete, if you look at the following method in ClusterScheduler:\n\ndef resourceOffers(offers: Seq[WorkerOffer]): Seq[Seq[TaskDescription]]\n\nThis sorts the activeTaskSetQueue by priority. Instead, it could sort it based on a FairShare calculation. This wouldn't be able to add weights/pools to certain users without adding tracing data up and down the stack, but it would prevent head-of-line blocking.\n", "From Patrick:\n\nSorry for the delay - we had some internal discussion about this. To\nenable what you want (having tracking for each user and combined user\npools with shares) we need to add some instrumentation throughout the\nstack.\n\nI spoke with Matei about this a bit, he said the best way to do this\nis to have a ThreadLocal variable that tracks meta-data about current\nuser. My sense is that this would involve having some functions in\nSparkContext that lets you set information about the current running\nuser and then this gets passed down do the TaskScheduler so it can do\nthings like fair sharing. The you could provide various policies\nwithin the TaskScheduler implementation (e.g. ClusterScheduler) and\none of them could be fair sharing.\n", "Just to give even more details. If we want something similar to the Hadoop Fair Scheduler that assigns jobs to one of the pools, where each pool has their own share or weights, then we need to modify the Spark Stack to be able to propagate this information down to the ClusterScheduler. If we look at the current implementation (TaskSet, etc), there is no way to attach this information and for the scheduler to know this information.\n\nAs the previous comment from Reynold suggests, we can use ThreadLocal variable.\n\nIn scala, we can use DynamicVariable. My design thoughts are as follows:\n\nWe have to modify SparkContext, DagScheduler, DagSchedulerEvent, Stage, and TaskSet.\n\nIn the SparkContext class:\n1. we can have a DynamicVariable of type Properties (like Java properties) or something similar to that (like Configuration in Hadoop). This way, we can use this to add meta-data for future features too (e.g., if we need more meta information for some new future scheduler, we can use this). For example, private val localProperties = new DynamicVariable[Properties](null)\n2. We can expose a function that allows users (threads) to set values to this variable (e.g., def function addLocalProperties(String key, String value)). \n3. We modify the runJob and runApproximateJob method, so that when it calls the dagScheduler.runJob or dagScheduler.runApproximateJob, it includes the value of the DynamicVariable as another parameter. \nNote that with this, there will be no changes needed for users of Spark. \n\nIn the DagSchedulerEvent class:\n1. We modify the JobSubmitted case class to have Properties member val\n\nIn the Stage class:\n1. We add a Properties member val\n\nIn the TaskSet class\n1. We add a Properties member val\n\n\nIn the DagScheduler class:\n1. We modify runJob and runApproximateJob methods to have a new parameter of type Properties, with default value so that it doesn't break any other code (if exists) up the spark stack that didn't pass a properties when calling this method.\n2. We make the necessary changes so that when a Job, stage, or TaskSet is created, the corresponding properties (from 1) are included.\nI don't think we can also use a thread local variable in DagScheduler class, like the ones we propose in the SparkContext because jobs submitted (runJob) are simply put in an event queue. There is a separate long running thread that loops and reads from the event queue. This is why I think when the spark context submits a job to the DagScheduler (runJob), we just attach the properties to the job created and correspondingly use this attached properties and attach to the stages/TaskSets when they are created.\n\n\nThe above changes are just for passing information down the TaskScheduler.\n\nFor the Fair scheduler, we can extend the ClusterScheduler, with the following changes:\n1. Instead of just having a single ActiveTaskSetsQueue, we want a queue of pools, where each pool contains an ActiveTaskSetsQueue. We can create a separate class that represents this pool.\n2. In the constructor (like in the Hadoop fair scheduler), we can also read from an allocation xml to initialize the pools\n3. We override the submitTasks method. We first check the properties of the TaskSet to determine which pool it belongs to. Like in ClusterScheduler, we create a TaskSetManager for each TaskSet, but we add it to the corresponding pool's ActiveTaskSetQueue.\n4. We override the taskSetFinished method, to also remove TaskSetManager from the corresponding pool.\n5. We override the other methods, that use ActiveTaskSetsQueue, to use pool.ActiveTaskSetsQueue\n6. We create a comparator function that compares pools used for sorting the pools for fair sharing. I think the comparator function that compares pools need to know the number of tasks currently running for each pool. We can make the necessary changes in the code to easily extract this information (e.g., maybe in the pool class we can  keep track of this such that when a new task is launched it gets incremented and when a task ended, it can get decremented)\n7. We override resourceOffers. For each offer, we sort the pools using the comparator function in 6. We then start from the head of the sorted pool to find a task to assign, until we find a pool that can use the offer. This also handles delay scheduling, so if the current pool's managers didn't take the offer, we can simply go to the next pool.\n\n\n\n\n\n\n\n\n\n\n\n", "Take sharkserver for example, need we change the sharkserver codes(e.g. call addLocalProperties exposed by SparkContext) to support fair scheduler in spark cluster scheduler? if so, could sharkserver get the user info that submits queries to sharkserver?", "I haven't really looked at the Shark code, but I think there will not be any significant change. What I was thinking is users of Shark can probably set this information in the hive conf. The sharkserver can then parse the \"pool\" information and set it in the thread that process the client requests. One issue I think is that the SharkServer uses a Threadpool to handle client requests. So, we have to be careful in using the DynamicVariable across shared threads. We can probably expose a function in SparkContext to clear/unset the DynamicVariable and the SharkServer can use it whenever a new request comes in."], "derived": {"summary": "Each SparkContext currently has a simple FIFO scheduler for task assignment. Applications which use long-running jobs (as in Shark) on the cluster submit tasks which are processed in a strict FIFO fashion.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Implement Fair scheduler within ClusterScheduler - Each SparkContext currently has a simple FIFO scheduler for task assignment. Applications which use long-running jobs (as in Shark) on the cluster submit tasks which are processed in a strict FIFO fashion."}, {"q": "What updates or decisions were made in the discussion?", "a": "I haven't really looked at the Shark code, but I think there will not be any significant change. What I was thinking is users of Shark can probably set this information in the hive conf. The sharkserver can then parse the \"pool\" information and set it in the thread that process the client requests. One issue I think is that the SharkServer uses a Threadpool to handle client requests. So, we have to be careful in using the DynamicVariable across shared threads. We can probably expose a function in SparkContext to clear/unset the DynamicVariable and the SharkServer can use it whenever a new request comes in."}]}}
{"project": "SPARK", "issue_id": "SPARK-664", "title": "Accumulator updates should get locally merged before sent to the driver", "status": "Closed", "priority": "Minor", "reporter": "Imran Rashid", "assignee": null, "labels": [], "created": "2013-01-23T21:12:25.000+0000", "updated": "2015-05-06T16:45:58.000+0000", "description": "Whenever a task finishes, the accumulator updates from that task are immediately sent back to the driver.  When the accumulator updates are big, this is inefficient because (a) a lot more data has to be sent to the driver and (b) the driver has to do all the work of merging the updates together.\n\nProbably doesn't matter for small accumulators / low number of tasks, but if both are big, this could be a big bottleneck.", "comments": ["this also applies to reduce() as well -- all the merging is done on the driver, but the executors can do most of the merging", "[~irashid] it sounds like your proposal is to batch accumulator updates between tasks on the executor before sending them back to the driver?\n\nI agree this would reduce the amount of network traffic, but the batching would come at a cost of higher latency between task completion and accumulator update landing in the accumulator in the driver.  With the completion of SPARK-2380 these accumulators are now shown in the UI, so increasing latency would have an effect on end users.\n\nIf network bandwidth and UI update latency are fundamentally at odds, maybe this is a case for a user option to choose to optimize for network or UI, something like {{spark.accumulators.mergeUpdatesOnExecutor}} defaulted to false.\n\ncc [~pwendell] for thoughts", "Hi [~aash] ,\n\nthanks for taking another look at this -- sorry I have been aloof for a little while.  I didn't know about SPARK-2380 , obviously this was created long before that.  Honestly, I'm not a big fan of SPARK-2380, it seems to really limit what we can do with accumulators.  We could really use them to expose a completely different model of computation.\n\nLet me give an example use case.  Accumulators are in principle general enough that they let you compute lots of different things in one pass.  Eg., by using accumulators, you could:\n\n* create a bloom filter of records that meet some criteria\n* assign records to different buckets, and count how many are in each bucket, even up to 100K buckets (eg., by having accumulator of {{Array<Long>}})\n* use hyperloglog to count how many distinct ids you have\n* filter down to only those records with some parsing error, for a closer look  (just by using plain old {{rdd.filter()}}\n\nYou could do all that in one pass, if the first 3 were done w/ accumulators.  When I started using spark, I actually wrote a bunch of code to do exactly that kind of thing.  But it performed really poorly -- after some profiling & investigating how accumulators work, I saw why.  Those big accumulators I was creating just put a lot of work on the driver.  Accumulators provide the right API to do that kind of thing, but the implementation would have to change.\n\nI definitely agree that if the results get merged on the executor before getting sent to the executor, it increases the latency of the *per-task* results, but does that matter?  I would prefer that we have something that supports the more general computation model, and the important thing is only the latency of the *overall* result.  It feels like we're moving to accumulators being treated just like counters (but with an awkward api).", "I'm going to close this due to inactivity. Also unclear because based on PRs submitted in the past we are going towards the direction of lower latency rather than higher throughput."], "derived": {"summary": "Whenever a task finishes, the accumulator updates from that task are immediately sent back to the driver. When the accumulator updates are big, this is inefficient because (a) a lot more data has to be sent to the driver and (b) the driver has to do all the work of merging the updates together.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Accumulator updates should get locally merged before sent to the driver - Whenever a task finishes, the accumulator updates from that task are immediately sent back to the driver. When the accumulator updates are big, this is inefficient because (a) a lot more data has to be sent to the driver and (b) the driver has to do all the work of merging the updates together."}, {"q": "What updates or decisions were made in the discussion?", "a": "I'm going to close this due to inactivity. Also unclear because based on PRs submitted in the past we are going towards the direction of lower latency rather than higher throughput."}]}}
{"project": "SPARK", "issue_id": "SPARK-665", "title": "Create RPM packages for Spark", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "2013-01-24T19:00:29.000+0000", "updated": "2015-02-11T08:46:43.000+0000", "description": "This could be doable with the JRPM Maven plugin, similar to how we make Debian packages now, but I haven't looked into it. The plugin is described at http://jrpm.sourceforge.net.", "comments": ["Providing RPM packages will make it easier to install Spark via Puppet (see https://github.com/deric/puppet-spark for example).", "Probably we can give sbt-native-packager a shot. https://github.com/sbt/sbt-native-packager\n\nWill try it out when i get a chance.", "I would highly suggest to use \"fpm\" (https://github.com/jordansissel/fpm) for this task.\nHere is what we do in RocketFuel (http://rocketfuel.com), we create a directory hierarchy and put into all necessary jars, then call fpm to do the hard work.   \nPlease note, configurations are not part of rpm in our case since we always use puppet to manage them. \n==========Rpm Build Script with fpm ===============\nSCALA_VERSION=2.9.3\nSPARK_VERSION=0.8.1\nSPARK_DEPLOY_PATH=/usr/share/spark\n\nrm -rf build\nmkdir build\ncd build\n\nmkdir -p .$SPARK_DEPLOY_PATH .$SPARK_DEPLOY_PATH/bin\ncp ../bin/*.sh .$SPARK_DEPLOY_PATH/bin\ncp ../spark-class .$SPARK_DEPLOY_PATH/spark\ncp ../spark-shell ../spark-executor ../run-example .$SPARK_DEPLOY_PATH\ncp ../assembly/target/scala-$SCALA_VERSION/spark*.jar .$SPARK_DEPLOY_PATH/spark-$SPARK_VERSION.jar\ncp ../examples/target/scala-$SCALA_VERSION/spark*.jar .$SPARK_DEPLOY_PATH/examples-$SPARK_VERSION.jar\ncp ../tools/target/spark-tools*\\[0-9Tg\\].jar .$SPARK_DEPLOY_PATH/tools-$SPARK_VERSION.jar\n \nfpm -s dir -t rpm -n \"rfi-spark\" -v \"$SPARK_VERSION.$BUILD_NUMBER\" -a \"all\" --prefix \"/\" --url \"http://spark.incubator.apache.org\" --maintainer \"MAINTAINER_EMAIL\" --description \"An open source cluster computing system that aims to make data analytics fast\" --license \"Apache Software Foundation (ASF)\" --vendor \"Apache Software Foundation (ASF)\" --category \"grid-thirdparty\" --epoch 1  --verbose .\n", "This role of creating RPM packages seems to have been taken up by the various Hadoop distributors for their distributions (Cloudera, MapR, HortonWorks, etc).\n\nDoes the Apache Spark team still intend to create RPMs for Spark?  An obvious subsequent request would be to also release in the DEB format.\n\nI don't think this is a route we want to go down now but wanted to hear others' thoughts.", "There are already Debian packages. I agree that you can find distros and RPMs easily these days.\nMy feeling is that the build is very complex at this point and the real cost of attempting to maintain another packaging probably isn't justified.", "Sean are you suggesting dropping the .deb packages that Apache Spark releases as a simplification effort?  It feels inequitable to support one packaging format (deb) but not the other (rpm).", "Not suggesting that, no. I suppose it does depend on demand. Apparently there was enough of a need for a .deb package that it was contributed and maintained, but perhaps not for .rpm, because there are other sources? If there were demand it could be worth the cost.", "I've looked at the JRPM maven plugin but unlike the jdeb one, JRPM depends on native rpm libraries (e.g. it is platform dependent)\n\nMy somewhat pragmatic approach to build Spark RPM is to reuse the existing deb package and convert it into rpm using the 'alien' tool. \n\nI've wrapped the spark-rpm build pipeline into a parameterizable docker container: \nhttps://registry.hub.docker.com/u/tzolov/apache-spark-build-pipeline\n\n\n", "Given SPARK-5727, I suggest this will also be WontFix, in favor of delegating this to projects like Bigtop."], "derived": {"summary": "This could be doable with the JRPM Maven plugin, similar to how we make Debian packages now, but I haven't looked into it. The plugin is described at http://jrpm.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Create RPM packages for Spark - This could be doable with the JRPM Maven plugin, similar to how we make Debian packages now, but I haven't looked into it. The plugin is described at http://jrpm."}, {"q": "What updates or decisions were made in the discussion?", "a": "Given SPARK-5727, I suggest this will also be WontFix, in favor of delegating this to projects like Bigtop."}]}}
{"project": "SPARK", "issue_id": "SPARK-666", "title": "Make Spark's master debug level logging consumable", "status": "Resolved", "priority": "Major", "reporter": "Reynold Xin", "assignee": "Patrick McFadin", "labels": [], "created": "2013-01-24T19:03:16.000+0000", "updated": "2013-01-29T21:31:10.000+0000", "description": "Right now if you turn debug level logging on, the console gets flooded with debug messages that basically make it impossible to use the debug level.\n\n", "comments": [], "derived": {"summary": "Right now if you turn debug level logging on, the console gets flooded with debug messages that basically make it impossible to use the debug level.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Make Spark's master debug level logging consumable - Right now if you turn debug level logging on, the console gets flooded with debug messages that basically make it impossible to use the debug level."}]}}
{"project": "SPARK", "issue_id": "SPARK-667", "title": "IntelliJ may insert stubs for inherited methods in Java Function classes", "status": "Resolved", "priority": "Minor", "reporter": "Josh Rosen", "assignee": null, "labels": ["Starter"], "created": "2013-01-26T13:01:07.000+0000", "updated": "2013-10-22T14:17:43.000+0000", "description": "When implementing Java API Function classes (e.g. PairFlatMapFunction) in IntelliJ, it looks like IntelliJ may auto-complete empty stubs for compose() and andThen() (here's an example: https://groups.google.com/d/topic/spark-users/KrVIf-DHg60/discussion).  Default implementations of these are inherited from AbstractFunction1, so this shouldn't happen.\n\nHaven't investigated too much, but it looks like this may only happen when using the Java API in a pure-Java project that depends on the Spark JAR; this doesn't seem to happen in my Spark IntelliJ workspace, but that may be because it's a mixed Scala + Java project.\n\nThis is a minor annoyance; maybe we can fix this by explicitly implementing compose() and andThen() in the public Function classes (just calling super in each implementation).", "comments": ["Since I haven't been able to reproduce this, I'm going to close this issue.  I'll chalk the old behavior up to an IntelliJ bug."], "derived": {"summary": "When implementing Java API Function classes (e. g.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "IntelliJ may insert stubs for inherited methods in Java Function classes - When implementing Java API Function classes (e. g."}, {"q": "What updates or decisions were made in the discussion?", "a": "Since I haven't been able to reproduce this, I'm going to close this issue.  I'll chalk the old behavior up to an IntelliJ bug."}]}}
{"project": "SPARK", "issue_id": "SPARK-668", "title": "JavaRDDLike.flatMap(PairFlatMapFunction) may fail with typechecking errors", "status": "Resolved", "priority": "Major", "reporter": "Josh Rosen", "assignee": "Josh Rosen", "labels": [], "created": "2013-01-26T14:40:56.000+0000", "updated": "2014-11-18T20:12:56.000+0000", "description": "As described in https://groups.google.com/d/topic/spark-users/KrVIf-DHg60/discussion, calls to JavaRDdLike.flatMap(PairFlatMapFunction) may be falsely rejected by the compiler with \"cannot find symbol; method: flatMap\" errors.\n\nHere's a complete standalone example that reproduces the problem:\n\nhttps://gist.github.com/4640356\n\nI tried implementing a similar example in pure-Java (no Spark code) and was able to get the proper typechecking, so I suspect that this might be a Scala compiler bug.", "comments": ["Fixed in https://github.com/mesos/spark/pull/417", "For reference, I think that this was caused by https://issues.scala-lang.org/browse/SI-6057"], "derived": {"summary": "As described in https://groups. google.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "JavaRDDLike.flatMap(PairFlatMapFunction) may fail with typechecking errors - As described in https://groups. google."}, {"q": "What updates or decisions were made in the discussion?", "a": "For reference, I think that this was caused by https://issues.scala-lang.org/browse/SI-6057"}]}}
{"project": "SPARK", "issue_id": "SPARK-669", "title": "Send back task results through BlockManager instead of Akka messages", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Kay Ousterhout", "labels": [], "created": "2013-01-27T22:49:41.000+0000", "updated": "2013-10-04T15:11:54.000+0000", "description": "A common problem for users is that their task results are multiple MB in size, and Akka cannot send messages larger than its frameSize. It would be better to avoid this altogether by sending them through the BlockManager. The driver would then delete the result from the remote node after it fetched it.", "comments": ["Looks like this has affected some users: https://groups.google.com/d/msg/spark-users/WG87fG8rrKY/6VpJ1AFXS-cJ", "For small task results, it is better to piggyback the akka task status message. For large ones, better using our own layer.", "Josh has a mock-up of this in a branch (borrowed from comment in SPARK-747).\nhttps://github.com/mesos/spark/pull/610\n\nAlso see his comment here:\nhttps://github.com/mesos/spark/pull/610#issuecomment-18021295", "Fixed by https://github.com/apache/incubator-spark/pull/10"], "derived": {"summary": "A common problem for users is that their task results are multiple MB in size, and Akka cannot send messages larger than its frameSize. It would be better to avoid this altogether by sending them through the BlockManager.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Send back task results through BlockManager instead of Akka messages - A common problem for users is that their task results are multiple MB in size, and Akka cannot send messages larger than its frameSize. It would be better to avoid this altogether by sending them through the BlockManager."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed by https://github.com/apache/incubator-spark/pull/10"}]}}
{"project": "SPARK", "issue_id": "SPARK-670", "title": "spark-ec2 should warn if you use the \"start\" command without passing a SSH key file", "status": "Resolved", "priority": "Trivial", "reporter": "Matei Alexandru Zaharia", "assignee": "Josh Rosen", "labels": [], "created": "2013-01-30T11:07:14.000+0000", "updated": "2013-05-08T23:18:21.000+0000", "description": "Right now it doesn't, so you get an error saying \"Identity file None not accessible\" from SSH if you don't pass the -i option. The \"launch\" command should likewise check for this.", "comments": ["Fixed in https://github.com/mesos/spark/pull/599"], "derived": {"summary": "Right now it doesn't, so you get an error saying \"Identity file None not accessible\" from SSH if you don't pass the -i option. The \"launch\" command should likewise check for this.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "spark-ec2 should warn if you use the \"start\" command without passing a SSH key file - Right now it doesn't, so you get an error saying \"Identity file None not accessible\" from SSH if you don't pass the -i option. The \"launch\" command should likewise check for this."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed in https://github.com/mesos/spark/pull/599"}]}}
{"project": "SPARK", "issue_id": "SPARK-671", "title": "Spark runs out of memory on fork/exec (affects both pipes and python)", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Jey Kottalam", "labels": [], "created": "2013-01-31T12:52:26.000+0000", "updated": "2015-11-02T12:11:18.000+0000", "description": "Because the JVM uses fork/exec to launch child processes, any child process initially has the memory footprint of its parent. In the case of a large Spark JVM that spawns many child processes (for Pipe or Python support), this quickly leads to kernel memory exhaustion.\n\nThis problem is discussed here:\nhttps://gist.github.com/1970815\n\nIt results in errors like this:\n{code}\n13/01/31 20:18:48 INFO cluster.TaskSetManager: Loss was due to java.io.IOException: Cannot run program \"cat\": java.io.IOException: error=12, Cannot allocate memory\n       at java.lang.ProcessBuilder.start(ProcessBuilder.java:475)\n       at spark.rdd.PipedRDD.compute(PipedRDD.scala:38)\n       at spark.RDD.computeOrReadCheckpoint(RDD.scala:203)\n       at spark.RDD.iterator(RDD.scala:192)\n       at spark.scheduler.ResultTask.run(ResultTask.scala:76)\n{code}\n\nI was able to workaround by allowing for memory over-commitment by the kernel on all slaves,\n\n{code}\necho 1 > /proc/sys/vm/overcommit_memory\n{code}\n\nbut we should try to include a more robust solution, such as the one here:\nhttps://github.com/axiak/java_posix_spawn", "comments": ["You might want to make sure this still happens now that you fixed the bug which launches JVM's for every task.", "Actually, since we saw this even with rdd.pipe(), probably it's still an issue.", "It seems like this is a JVM problem that only affects some platforms.  It sounds like Jenkins and Hadoop don't work around this, so maybe a fix is out of scope for us.\n\nI propose that we add some configuration documentation on how to work around this issue (e.g. through overcommit_memory or adding extra virtual memory), then resolve this issue as \"Won't Fix.\"", "Don't put a fix version on this since it's not yet fixed. You're supposed to only assign that once you fix it.", "And yes I agree that we probably don't want to work around this if Hadoop and Jenkins don't. We might just leave it as an open issue and document it in various places.", "Matei - that's not how Fix versions are used in JIRA. Fix versions are for the version of the intended fix, regardless of whether the issue is completed, it says so clearly in the JIRA docs:\n\nhttps://confluence.atlassian.com/display/JIRA/What+is+an+Issue\n\nThis is necessary for using \"Roadmap\" features of JIRA - to answer questions like: \"How many outstanding issues are there for 0.7\". That way people will actually know what remains before a release comes out and can track projects. The other projects (STREAMING, SHARK) also use fix versions like this pervasively.", "Ah, I was actually going by what I saw happen in Hadoop. In this case though, let's not assign fix versions unless we actually make a roadmap where we agree we'll do this for a particular version. (At least for Spark.)", "This was fixed by Jey in https://github.com/mesos/spark/pull/563, which uses a separate process to fork PySpark's {{python}} processes.", "AFAIK, this has only been fixed for PySpark, not for general pipe() calls in Spark.  If we still need to fix that, please re-open this issue (or open a new linked issue).", "is it still a problem in latest version?\nI 'm using pipe() operation, and found that if I use pipe() before any shuffle task, the memory always grows very high.\nmaybe the memory usage of sub process is not a constant volume ? is it affected by the memory of the parent process?"], "derived": {"summary": "Because the JVM uses fork/exec to launch child processes, any child process initially has the memory footprint of its parent. In the case of a large Spark JVM that spawns many child processes (for Pipe or Python support), this quickly leads to kernel memory exhaustion.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Spark runs out of memory on fork/exec (affects both pipes and python) - Because the JVM uses fork/exec to launch child processes, any child process initially has the memory footprint of its parent. In the case of a large Spark JVM that spawns many child processes (for Pipe or Python support), this quickly leads to kernel memory exhaustion."}, {"q": "What updates or decisions were made in the discussion?", "a": "is it still a problem in latest version?\nI 'm using pipe() operation, and found that if I use pipe() before any shuffle task, the memory always grows very high.\nmaybe the memory usage of sub process is not a constant volume ? is it affected by the memory of the parent process?"}]}}
{"project": "SPARK", "issue_id": "SPARK-672", "title": "Executor gets stuck in a \"zombie\" state after running out of memory", "status": "Resolved", "priority": "Major", "reporter": "Mikhail Bautin", "assignee": null, "labels": [], "created": "2013-01-31T12:54:32.000+0000", "updated": "2015-02-08T13:04:48.000+0000", "description": "As a result of running a workload, an executor ran out of memory, but the executor process stayed up. Also (not sure this is related) the standalone worker process stayed up but disappeared from the master web UI.", "comments": ["[~mbautin] when an executor JVM is under extremely heavy GC load, it will often lock up, not even responding to normal kill commands and requiring a kill -9 to shut down.  I agree that Spark could behave better in these situations.\n\nWhat is your preference for handling these issues -- maybe give the executor a timeout and kill -9 it after a certain period of time of non-responsiveness?", "The right-er answer is to fail for lack of memory faster, per SPARK-1989."], "derived": {"summary": "As a result of running a workload, an executor ran out of memory, but the executor process stayed up. Also (not sure this is related) the standalone worker process stayed up but disappeared from the master web UI.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Executor gets stuck in a \"zombie\" state after running out of memory - As a result of running a workload, an executor ran out of memory, but the executor process stayed up. Also (not sure this is related) the standalone worker process stayed up but disappeared from the master web UI."}, {"q": "What updates or decisions were made in the discussion?", "a": "The right-er answer is to fail for lack of memory faster, per SPARK-1989."}]}}
{"project": "SPARK", "issue_id": "SPARK-673", "title": "PySpark should capture and re-throw Python exceptions", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "labels": [], "created": "2013-01-31T14:18:48.000+0000", "updated": "2013-02-01T00:34:37.000+0000", "description": "Right now if there is an exception inside of a PySpark worker, it causes the worker process to exit prematurely, triggering an EOF exception at the JVM worker. This means you have to go dig through worker logs to find the exception trace.\n\nIt would be more helpful if the Python worker instead caught the exception and passed the string representation of the exception to the JVM worker, which could then wrap it in a Java exception. e.g. \n\n{code}\nthrow new PythonException(exnString).\n{code}\n\nThis would make it much easier to debug python tasks, since that string would show up at the driver.", "comments": ["Added in https://github.com/mesos/spark/pull/434"], "derived": {"summary": "Right now if there is an exception inside of a PySpark worker, it causes the worker process to exit prematurely, triggering an EOF exception at the JVM worker. This means you have to go dig through worker logs to find the exception trace.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "PySpark should capture and re-throw Python exceptions - Right now if there is an exception inside of a PySpark worker, it causes the worker process to exit prematurely, triggering an EOF exception at the JVM worker. This means you have to go dig through worker logs to find the exception trace."}, {"q": "What updates or decisions were made in the discussion?", "a": "Added in https://github.com/mesos/spark/pull/434"}]}}
{"project": "SPARK", "issue_id": "SPARK-674", "title": "Gateway JVM's should not be launched on slave", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Josh Rosen", "labels": [], "created": "2013-01-31T14:22:52.000+0000", "updated": "2013-02-01T11:45:24.000+0000", "description": "The slaves seem to launch a new JVM for each task (to run the Gateway Java program). This is a bug since that program is only needed on the driver. It causes increased latency for tasks - due to JVM launch - and also memory pressure, since the gateway asks for SPARK_MEM memory on launch.", "comments": ["Fixed in https://github.com/mesos/spark/pull/438"], "derived": {"summary": "The slaves seem to launch a new JVM for each task (to run the Gateway Java program). This is a bug since that program is only needed on the driver.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Gateway JVM's should not be launched on slave - The slaves seem to launch a new JVM for each task (to run the Gateway Java program). This is a bug since that program is only needed on the driver."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed in https://github.com/mesos/spark/pull/438"}]}}
{"project": "SPARK", "issue_id": "SPARK-675", "title": "Gateway JVM should ask for less than SPARK_MEM memory", "status": "Resolved", "priority": "Minor", "reporter": "Patrick McFadin", "assignee": "Josh Rosen", "labels": [], "created": "2013-01-31T14:24:21.000+0000", "updated": "2014-09-08T16:45:10.000+0000", "description": "This is not so big of a deal assuming that we fix SPARK-674, but it would be nice if the gateway JVM asked for less than SPARK_MEM amount of memory. This might require decoupling the class-path component of \"run.sh\" so it can be used independently. ", "comments": ["This might be possible to do now that we have `compute-classpath.sh` in Spark 0.8.  \n\nhttps://groups.google.com/d/msg/spark-users/yPYEGk2g9Ug/ox1IHHCk2CMJ suggests that maybe we should have a separate setting for controlling the gateway's memory usage, e.g. PYSPARK_GATEWAY_MEM or something like that.", "[~joshrosen] it looks like SPARK-674 was resolved, do you think this is still an issue or can it be closed?", "Thanks for the reminder.  I'm going to close this since it only affected a very old version of Spark and most code related to this has been significantly changed (for example, I don't think we have SPARK_MEM anymore, and I think the gateway JVM's heap size is controlled through spark-submit settings (see PYSPARK_SUBMIT_ARGS))."], "derived": {"summary": "This is not so big of a deal assuming that we fix SPARK-674, but it would be nice if the gateway JVM asked for less than SPARK_MEM amount of memory. This might require decoupling the class-path component of \"run.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Gateway JVM should ask for less than SPARK_MEM memory - This is not so big of a deal assuming that we fix SPARK-674, but it would be nice if the gateway JVM asked for less than SPARK_MEM amount of memory. This might require decoupling the class-path component of \"run."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for the reminder.  I'm going to close this since it only affected a very old version of Spark and most code related to this has been significantly changed (for example, I don't think we have SPARK_MEM anymore, and I think the gateway JVM's heap size is controlled through spark-submit settings (see PYSPARK_SUBMIT_ARGS))."}]}}
{"project": "SPARK", "issue_id": "SPARK-676", "title": "Abbreviation in SPARK_MEM but not in SPARK_WORKER_MEMORY", "status": "Resolved", "priority": "Trivial", "reporter": "Josh Rosen", "assignee": "Mark Grover", "labels": [], "created": "2013-02-02T16:33:27.000+0000", "updated": "2013-10-20T14:26:21.000+0000", "description": "{{SPARK_MEM}} and {{SPARK_WORKER_MEMORY}} are inconsistent in how they abbreviate \"memory\".  This is a potential source of typos.", "comments": ["Is there a nomenclature preference?  Should we stick with MEM and change SPARK_WORKER_MEMORY -> SPARK_WORKER_MEM then?", "Is this still open (And is the above comment the desired solution)?", "I was working on this last weekend but forgot to assign it to myself, sorry! Anyways, I have created a pull request for doing this in a backwards compatible way:\nhttps://github.com/apache/incubator-spark/pull/48", "I've created SPARK-929 to track deprecating SPARK_MEM", "Thanks! This JIRA, in my opinion, should be marked as Won't Fix (see pull request for more details). Perhaps, I need to be given some JIRA karma because I can't figure out a way to resolve it."], "derived": {"summary": "{{SPARK_MEM}} and {{SPARK_WORKER_MEMORY}} are inconsistent in how they abbreviate \"memory\". This is a potential source of typos.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Abbreviation in SPARK_MEM but not in SPARK_WORKER_MEMORY - {{SPARK_MEM}} and {{SPARK_WORKER_MEMORY}} are inconsistent in how they abbreviate \"memory\". This is a potential source of typos."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks! This JIRA, in my opinion, should be marked as Won't Fix (see pull request for more details). Perhaps, I need to be given some JIRA karma because I can't figure out a way to resolve it."}]}}
{"project": "SPARK", "issue_id": "SPARK-677", "title": "PySpark should not collect results through local filesystem", "status": "Resolved", "priority": "Major", "reporter": "Josh Rosen", "assignee": "Davies Liu", "labels": [], "created": "2013-02-02T17:28:30.000+0000", "updated": "2015-05-22T20:40:07.000+0000", "description": "Py4J is slow when transferring large arrays, so PySpark currently dumps data to the disk and reads it back in order to collect() RDDs.  On large enough datasets, this data will spill from the buffer cache and write to the physical disk, resulting in terrible performance.\n\nInstead, we should stream the data from Java to Python over a local socket or a FIFO.", "comments": ["Perhaps a ZeroMQ IPC socket between python and JVM might be might be useful here? This is at the cost of adding a dependency to ZeroMQ of course, though.", "A local socket is probably fine, as long as we turn it into a file object by getting its file descriptor, as we did in the communication from workers to the Java process. The built-in socket.makefile in Python results in a very slow file object.", "this can also be used to address the fragile nature of py4j connection construction. the parent can create the fifo.", "[~joshrosen] is this fixed now?", "No, it's still an issue in 1.2.0:\n\n{code}\n def collect(self):\n        \"\"\"\n        Return a list that contains all of the elements in this RDD.\n        \"\"\"\n        with SCCallSiteSync(self.context) as css:\n            bytesInJava = self._jrdd.collect().iterator()\n        return list(self._collect_iterator_through_file(bytesInJava))\n\n    def _collect_iterator_through_file(self, iterator):\n        # Transferring lots of data through Py4J can be slow because\n        # socket.readline() is inefficient.  Instead, we'll dump the data to a\n        # file and read it back.\n        tempFile = NamedTemporaryFile(delete=False, dir=self.ctx._temp_dir)\n        tempFile.close()\n        self.ctx._writeToFile(iterator, tempFile.name)\n        # Read the data into Python and deserialize it:\n        with open(tempFile.name, 'rb') as tempFile:\n            for item in self._jrdd_deserializer.load_stream(tempFile):\n                yield item\n        os.unlink(tempFile.name)\n{code}", "User 'davies' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/4923", "This was fixed for 1.3.1, 1.2.2, and 1.4.0.  I don't think that we'l do a 1.1.x backport, so I'm going to mark this as resolved."], "derived": {"summary": "Py4J is slow when transferring large arrays, so PySpark currently dumps data to the disk and reads it back in order to collect() RDDs. On large enough datasets, this data will spill from the buffer cache and write to the physical disk, resulting in terrible performance.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "PySpark should not collect results through local filesystem - Py4J is slow when transferring large arrays, so PySpark currently dumps data to the disk and reads it back in order to collect() RDDs. On large enough datasets, this data will spill from the buffer cache and write to the physical disk, resulting in terrible performance."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was fixed for 1.3.1, 1.2.2, and 1.4.0.  I don't think that we'l do a 1.1.x backport, so I'm going to mark this as resolved."}]}}
{"project": "SPARK", "issue_id": "SPARK-678", "title": "Add an example that does a roll-up on log data", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "labels": [], "created": "2013-02-04T11:27:00.000+0000", "updated": "2013-02-12T14:00:11.000+0000", "description": "People don't always understand how you can aggregate multiple statistics in parallel with Spark. Since it's such a common thing, would be good to have an example in the examples directory.", "comments": ["I'm happy to work up an example if someone can point me to a small sample dataset for log data?", "Oh I see you already did it in 446"], "derived": {"summary": "People don't always understand how you can aggregate multiple statistics in parallel with Spark. Since it's such a common thing, would be good to have an example in the examples directory.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add an example that does a roll-up on log data - People don't always understand how you can aggregate multiple statistics in parallel with Spark. Since it's such a common thing, would be good to have an example in the examples directory."}, {"q": "What updates or decisions were made in the discussion?", "a": "Oh I see you already did it in 446"}]}}
{"project": "SPARK", "issue_id": "SPARK-679", "title": "Have a DSL or other language support for OLAP expressions", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "labels": [], "created": "2013-02-04T11:35:59.000+0000", "updated": "2014-03-25T15:09:55.000+0000", "description": "One common use of Spark is to do roll-ups that collect multiple statistics in parallel. Right now, this requires writing a custom aggregator do do things like mean/min/max/percentiles/total/etc. It wouldn't be hard to write some language support for this.\n\nThis needs some more thought, but something like this would be cool:\n{code}\nrdd.stats(average(.numPosts), percentile(95)(.size), )\n{code}\n\nSort of a scala version of MDX. This avoids people having to re-invent the wheel with custom aggregators all the time.", "comments": ["I like this idea!", "Fixed by SPARK-1251... many years later."], "derived": {"summary": "One common use of Spark is to do roll-ups that collect multiple statistics in parallel. Right now, this requires writing a custom aggregator do do things like mean/min/max/percentiles/total/etc.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Have a DSL or other language support for OLAP expressions - One common use of Spark is to do roll-ups that collect multiple statistics in parallel. Right now, this requires writing a custom aggregator do do things like mean/min/max/percentiles/total/etc."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed by SPARK-1251... many years later."}]}}
{"project": "SPARK", "issue_id": "SPARK-680", "title": "broadcast hangs spark cluster", "status": "Closed", "priority": "Major", "reporter": "Reynold Xin", "assignee": "Mosharaf Chowdhury", "labels": [], "created": "2013-02-04T13:39:59.000+0000", "updated": "2014-02-10T12:41:10.000+0000", "description": "Reported by Netease. Running logistics regression, in which the master repeatedly broadcast 30M of data to 100 slaves will hang the cluster.", "comments": ["Couldn't reproduce the bug while transferring the same amount of data hundreds of times. Contacted Netease; waiting for updates.", "Never heard back from Netease regarding this issue. \nCurrent broadcast implementations should be easy able to handle the scenarios they mentioned.\nClosing this issue."], "derived": {"summary": "Reported by Netease. Running logistics regression, in which the master repeatedly broadcast 30M of data to 100 slaves will hang the cluster.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "broadcast hangs spark cluster - Reported by Netease. Running logistics regression, in which the master repeatedly broadcast 30M of data to 100 slaves will hang the cluster."}, {"q": "What updates or decisions were made in the discussion?", "a": "Never heard back from Netease regarding this issue. \nCurrent broadcast implementations should be easy able to handle the scenarios they mentioned.\nClosing this issue."}]}}
{"project": "SPARK", "issue_id": "SPARK-681", "title": "Optimize hashtables used in Spark", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "2013-02-04T14:51:05.000+0000", "updated": "2014-11-06T17:35:01.000+0000", "description": "The hash tables used in cogroup, join, etc take up a lot more space than they need to because they're using linked data structures. It would be nice to write a custom open hashtable class to use instead, especially since these tables are \"append-only\". A custom one would likely run better than fastutil as well.", "comments": ["I see that fastutil's Object2LongOpenHashMap is used everywhere.  Is this the data structure you want to replace?\n\nThis is an open-addressing hash table, so it's not using linked data structures, but rather an array whose size is preset and grows 2x every time it gets kind of full.   Is it possible that the initial size chosen is too large (see SHARK-27)?\n\n", "I think Matei was referring to the hash map (java.util.HashMap) used in aggregation and cogroup.", "Ok.  How about we just replace java.util.HashMap with the one from fastutil?   I honestly don't think our own implementation would be much faster (what makes the append-case expensive for an open hash map is growing the size of the backing array, which is unavoidable) other than that fastutil is a huge unwieldy dependency.", "Actually there are multiple benefits.\n\n1. As you mentioned, fastutil is a huge dependency.\n2. Our hashmap allows us to find and update within a single hash lookup.\n3. It can be specialized using Scala specialization, whereas fastutil would require writing separate Scala/Java code to use.\n4. Some other perf gains from not having to deal with deletes.\n", "External sorting has gotten its grubby paws on our new AppendOnlyMap in order to efficiently track the size of the cogroup and aggregation tables as they grow and to sort without using extra memory. It would be difficult (and performance-impacting) to remove it now, at least for the case where external sorting is enabled."], "derived": {"summary": "The hash tables used in cogroup, join, etc take up a lot more space than they need to because they're using linked data structures. It would be nice to write a custom open hashtable class to use instead, especially since these tables are \"append-only\".", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "Optimize hashtables used in Spark - The hash tables used in cogroup, join, etc take up a lot more space than they need to because they're using linked data structures. It would be nice to write a custom open hashtable class to use instead, especially since these tables are \"append-only\"."}, {"q": "What updates or decisions were made in the discussion?", "a": "External sorting has gotten its grubby paws on our new AppendOnlyMap in order to efficiently track the size of the cogroup and aggregation tables as they grow and to sort without using extra memory. It would be difficult (and performance-impacting) to remove it now, at least for the case where external sorting is enabled."}]}}
{"project": "SPARK", "issue_id": "SPARK-682", "title": "Memoize results of getPreferredLocations", "status": "Resolved", "priority": "Minor", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "2013-02-04T14:52:17.000+0000", "updated": "2014-11-06T07:02:13.000+0000", "description": "In certain lineage graphs, DAGScheduler.getPreferredLocations might explore an RDD multiple times if there are multiple paths from it to the root of a job, causing potentially exponential blowup.", "comments": [], "derived": {"summary": "In certain lineage graphs, DAGScheduler. getPreferredLocations might explore an RDD multiple times if there are multiple paths from it to the root of a job, causing potentially exponential blowup.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Memoize results of getPreferredLocations - In certain lineage graphs, DAGScheduler. getPreferredLocations might explore an RDD multiple times if there are multiple paths from it to the root of a job, causing potentially exponential blowup."}]}}
{"project": "SPARK", "issue_id": "SPARK-683", "title": "Spark 0.7 with Hadoop 1.0 does not work with current AMI's HDFS installation", "status": "Resolved", "priority": "Major", "reporter": "Tathagata Das", "assignee": null, "labels": [], "created": "2013-02-04T17:13:37.000+0000", "updated": "2014-10-29T09:17:12.000+0000", "description": "A simple saveAsObjectFile() leads to the following error.\n\norg.apache.hadoop.ipc.RemoteException: java.io.IOException: java.lang.NoSuchMethodException: org.apache.hadoop.hdfs.protocol.ClientProtocol.create(java.lang.String, org.apache.hadoop.fs.permission.FsPermission, java.lang.String, boolean, boolean, short, long)\n\tat java.lang.Class.getMethod(Class.java:1622)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:557)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1388)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1384)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:416)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1382)\n", "comments": ["I think the default hadoop version might have changed in the build. Are you sure you compiled spark with the same hadoop version as is on the AMI?", "That was the problem. We changed the default hadoop version to 1.0 in 0.7.0 -- We should either change the AMI to run HDFS v1.0 or change Spark on the AMI to make sure users don't run into this.", "Ya I ran into this testing streaming code. Probably same as TD.", "Hopefully this will help whoever makes the AMI for 0.7. I tried setting up Hadoop 1.0.3 on the existing AMI and the configuration we have right now works fine out of the box. All I had to do was:\n\nwget http://archive.apache.org/dist/hadoop/core/hadoop-1.0.3/hadoop-1.0.3.tar.gz\ntar -xf hadoop-1.0.3.tar.gz\n# Copy conf files from existing hdfs setup\n", "I think this is likely long since obsolete or fixed, since Spark, Hadoop and AMI Hadoop versions have moved forward, and have not heard of this issue in recent memory.", "PS I think this also turns out to be the same as SPARK-4078"], "derived": {"summary": "A simple saveAsObjectFile() leads to the following error. org.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Spark 0.7 with Hadoop 1.0 does not work with current AMI's HDFS installation - A simple saveAsObjectFile() leads to the following error. org."}, {"q": "What updates or decisions were made in the discussion?", "a": "PS I think this also turns out to be the same as SPARK-4078"}]}}
{"project": "SPARK", "issue_id": "SPARK-684", "title": "Move downloads links on Spark website away from GitHub", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "2013-02-05T12:30:08.000+0000", "updated": "2013-04-01T22:12:55.000+0000", "description": "GitHub is disabling its Downloads feature at the end of February, so we need to move them to our own server.", "comments": ["Looks like this was fixed."], "derived": {"summary": "GitHub is disabling its Downloads feature at the end of February, so we need to move them to our own server.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Move downloads links on Spark website away from GitHub - GitHub is disabling its Downloads feature at the end of February, so we need to move them to our own server."}, {"q": "What updates or decisions were made in the discussion?", "a": "Looks like this was fixed."}]}}
{"project": "SPARK", "issue_id": "SPARK-685", "title": "Add an environment variable to launch PySpark with ipython", "status": "Resolved", "priority": "Minor", "reporter": "Matei Alexandru Zaharia", "assignee": "Nicholas Pentreath", "labels": [], "created": "2013-02-05T15:21:38.000+0000", "updated": "2013-02-07T20:43:30.000+0000", "description": "Right now users can do\n\nPYSPARK_PYTHON=\"ipython\" ./pyspark -i $SPARK_HOME/python/pyspark/shell.py\n\nBut it's annoying.", "comments": ["Opened PR for this: https://github.com/mesos/spark/pull/454\n\nUsage: \"IPYTHON=1 ./pyspark\""], "derived": {"summary": "Right now users can do\n\nPYSPARK_PYTHON=\"ipython\". /pyspark -i $SPARK_HOME/python/pyspark/shell.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add an environment variable to launch PySpark with ipython - Right now users can do\n\nPYSPARK_PYTHON=\"ipython\". /pyspark -i $SPARK_HOME/python/pyspark/shell."}, {"q": "What updates or decisions were made in the discussion?", "a": "Opened PR for this: https://github.com/mesos/spark/pull/454\n\nUsage: \"IPYTHON=1 ./pyspark\""}]}}
{"project": "SPARK", "issue_id": "SPARK-686", "title": "Port FT heartbeat and fixes from 0.6 branch to master", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "2013-02-06T14:02:48.000+0000", "updated": "2013-12-07T14:14:25.000+0000", "description": "Commits https://github.com/mesos/spark/commit/4b53f145b49fd8228129edef9a1f4f3f3488b865 through https://github.com/mesos/spark/commit/f886b42cecc8097ab33b2ba5ac39445c103a9ba7 on branch 0.6 fix a few issues with fault tolerance, including detecting \"hard crashes\" of nodes faster than a TCP timeout in the standalone cluster, and properly removing block locations for failed nodes. These need to be ported to master, which change the code from using slave IDs to executor IDs.", "comments": [], "derived": {"summary": "Commits https://github. com/mesos/spark/commit/4b53f145b49fd8228129edef9a1f4f3f3488b865 through https://github.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Port FT heartbeat and fixes from 0.6 branch to master - Commits https://github. com/mesos/spark/commit/4b53f145b49fd8228129edef9a1f4f3f3488b865 through https://github."}]}}
{"project": "SPARK", "issue_id": "SPARK-687", "title": "Use separate SPARK_DAEMON_MEMORY setting in Windows run script too", "status": "Resolved", "priority": "Trivial", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "labels": [], "created": "2013-02-06T14:40:23.000+0000", "updated": "2013-02-07T21:51:01.000+0000", "description": "Commit https://github.com/mesos/spark/commit/c0d2ea111c17d9dde579c1b3bd79e5a8098f011a switches the standalone worker and master to use a different environment variable for their memory, since allocating them a huge amount of memory is a common pitfall. The Windows run script needs to receive the corresponding changes.", "comments": [], "derived": {"summary": "Commit https://github. com/mesos/spark/commit/c0d2ea111c17d9dde579c1b3bd79e5a8098f011a switches the standalone worker and master to use a different environment variable for their memory, since allocating them a huge amount of memory is a common pitfall.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Use separate SPARK_DAEMON_MEMORY setting in Windows run script too - Commit https://github. com/mesos/spark/commit/c0d2ea111c17d9dde579c1b3bd79e5a8098f011a switches the standalone worker and master to use a different environment variable for their memory, since allocating them a huge amount of memory is a common pitfall."}]}}
{"project": "SPARK", "issue_id": "SPARK-688", "title": "Task crashed when I do spark stress test", "status": "Resolved", "priority": "Major", "reporter": "xiajunluan", "assignee": null, "labels": [], "created": "2013-02-06T19:04:42.000+0000", "updated": "2020-05-17T18:30:14.000+0000", "description": "My spark test codes shows as following\n\nval data = spark.textFile(\"hdfs://...\")\nval data_map = Data.groupby(3).map{line =>\n    Thread.sleep(20000)  //20s\n    ......\n    }\n    data_Map.saveAsTextFile(\"hdfs://....\")\n\nI run above application in standalone mode ,while task sleeps, I choose one worker node and kill StandaloneExecutorBackend daemon, then tasks will report crash stack as:\n\n13/02/06 10:03:35 ERROR Executor: Exception in task ID 7\njava.lang.NullPointerException\n\tat spark.MapOutputTracker$$anonfun$getServerStatuses$12.apply(MapOutputTracker.scala:179)\n\tat spark.MapOutputTracker$$anonfun$getServerStatuses$12.apply(MapOutputTracker.scala:178)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)\n\tat scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:38)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:233)\n\tat scala.collection.mutable.ArrayOps.map(ArrayOps.scala:38)\n\tat spark.MapOutputTracker.getServerStatuses(MapOutputTracker.scala:178)\n\tat spark.BlockStoreShuffleFetcher.fetch(BlockStoreShuffleFetcher.scala:20)\n\tat spark.rdd.ShuffledAggregatedRDD.compute(ShuffledRDD.scala:118)\n\tat spark.RDD.iterator(RDD.scala:164)\n\tat spark.rdd.MappedRDD.compute(MappedRDD.scala:15)\n\tat spark.RDD.iterator(RDD.scala:164)\n\tat spark.scheduler.ResultTask.run(ResultTask.scala:19)\n\tat spark.executor.Executor$TaskRunner.run(Executor.scala:87)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\n\tat java.lang.Thread.run(Thread.java:722)\n\n\n ", "comments": ["I believe we fixed this issue in 0.6.2. Can you try it out? We just released it today.", "sure, I will try it in latest spark. thanks."], "derived": {"summary": "My spark test codes shows as following\n\nval data = spark. textFile(\"hdfs://.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Task crashed when I do spark stress test - My spark test codes shows as following\n\nval data = spark. textFile(\"hdfs://."}, {"q": "What updates or decisions were made in the discussion?", "a": "sure, I will try it in latest spark. thanks."}]}}
{"project": "SPARK", "issue_id": "SPARK-689", "title": "Task will crash when setting SPARK_WORKER_CORES> 128", "status": "Closed", "priority": "Major", "reporter": "xiajunluan", "assignee": null, "labels": [], "created": "2013-02-06T19:47:57.000+0000", "updated": "2014-11-14T08:46:05.000+0000", "description": "when I set SPARK_WORKER_CORES > 128(for example 200), and run a job in standalone mode that will allocate 200 tasks in one worker node, then task will crash(it seems that worker cores has been hard-code)\n\n{noformat}\n13/02/07 11:25:02 ERROR StandaloneExecutorBackend: Task spark.executor.Executor$TaskRunner@5367839e rejected from java.util.concurrent.ThreadPoolExecutor@30f224d9[Running, pool size = 128, active threads = 128, queued tasks = 0, completed tasks = 0]\njava.util.concurrent.RejectedExecutionException: Task spark.executor.Executor$TaskRunner@5367839e rejected from java.util.concurrent.ThreadPoolExecutor@30f224d9[Running, pool size = 128, active threads = 128, queued tasks = 0, completed tasks = 0]\n\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2013)\n\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:816)\n\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1337)\n\tat spark.executor.Executor.launchTask(Executor.scala:59)\n\tat spark.executor.StandaloneExecutorBackend$$anonfun$receive$1.apply(StandaloneExecutorBackend.scala:57)\n\tat spark.executor.StandaloneExecutorBackend$$anonfun$receive$1.apply(StandaloneExecutorBackend.scala:46)\n\tat akka.actor.Actor$class.apply(Actor.scala:318)\n\tat spark.executor.StandaloneExecutorBackend.apply(StandaloneExecutorBackend.scala:17)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:626)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:197)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:179)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:516)\n\tat akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259)\n\tat akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975)\n\tat akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479)\n\tat akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)\n13/02/07 11:25:02 INFO StandaloneExecutorBackend: Connecting to master: akka://spark@10.0.2.19:60882/user/StandaloneScheduler\n13/02/07 11:25:02 INFO StandaloneExecutorBackend: Got assigned task 1929\n13/02/07 11:25:02 INFO Executor: launch taskId: 1929\n13/02/07 11:25:02 ERROR StandaloneExecutorBackend: \njava.lang.NullPointerException\n\tat spark.executor.Executor.launchTask(Executor.scala:59)\n\tat spark.executor.StandaloneExecutorBackend$$anonfun$receive$1.apply(StandaloneExecutorBackend.scala:57)\n\tat spark.executor.StandaloneExecutorBackend$$anonfun$receive$1.apply(StandaloneExecutorBackend.scala:46)\n\tat akka.actor.Actor$class.apply(Actor.scala:318)\n\tat spark.executor.StandaloneExecutorBackend.apply(StandaloneExecutorBackend.scala:17)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:626)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:197)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:179)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:516)\n\tat akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259)\n\tat akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975)\n\tat akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479)\n\tat akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)\n13/02/07 11:25:02 INFO StandaloneExecutorBackend: Connecting to master: akka://spark@10.0.2.19:60882/user/StandaloneScheduler\n13/02/07 11:25:02 INFO StandaloneExecutorBackend: Got assigned task 1930\n13/02/07 11:25:02 INFO Executor: launch taskId: 1930\n13/02/07 11:25:02 ERROR StandaloneExecutorBackend: \njava.lang.NullPointerException\n\tat spark.executor.Executor.launchTask(Executor.scala:59)\n\tat spark.executor.StandaloneExecutorBackend$$anonfun$receive$1.apply(StandaloneExecutorBackend.scala:57)\n\tat spark.executor.StandaloneExecutorBackend$$anonfun$receive$1.apply(StandaloneExecutorBackend.scala:46)\n\tat akka.actor.Actor$class.apply(Actor.scala:318)\n\tat spark.executor.StandaloneExecutorBackend.apply(StandaloneExecutorBackend.scala:17)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:626)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:197)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:179)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:516)\n\tat akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259)\n\tat akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975)\n\tat akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479)\n\tat akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)\n{noformat}", "comments": ["I attempted a repro on a one-node cluster (my laptop) and confirmed that this bug no longer exists on master.  A code inspection reveals that there is no thread limit of 128 limit anymore on the Executor's threadpool from this stacktrace line: {{at spark.executor.Executor.launchTask(Executor.scala:59)}}\n\nHere's the outline of my repro attempt:\n\n{noformat}\naash@aash-mbp ~/git/spark$ cat conf/spark-env.sh\nSPARK_WORKER_CORES=200\nSPARK_MASTER_IP=aash-mbp.local\nSPARK_PUBLIC_DNS=aash-mbp.local\naash@aash-mbp ~/git/spark$ cat conf/spark-defaults.sh\nspark.master                     spark://aash-mbp.local:7077\naash@aash-mbp ~/git/spark$ sbin/start-all.sh\n...\naash@aash-mbp ~/git/spark$ bin/spark-shell\nspark> sc.parallelize(1l to 100000000l,200).reduce(_+_)\nres0: Long = 5000000050000000\nspark>\n{noformat}\n\nI'm now closing this ticket, but please reopen [~xiajunluan] if you're still having issues."], "derived": {"summary": "when I set SPARK_WORKER_CORES > 128(for example 200), and run a job in standalone mode that will allocate 200 tasks in one worker node, then task will crash(it seems that worker cores has been hard-code)\n\n{noformat}\n13/02/07 11:25:02 ERROR StandaloneExecutorBackend: Task spark. executor.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Task will crash when setting SPARK_WORKER_CORES> 128 - when I set SPARK_WORKER_CORES > 128(for example 200), and run a job in standalone mode that will allocate 200 tasks in one worker node, then task will crash(it seems that worker cores has been hard-code)\n\n{noformat}\n13/02/07 11:25:02 ERROR StandaloneExecutorBackend: Task spark. executor."}, {"q": "What updates or decisions were made in the discussion?", "a": "I attempted a repro on a one-node cluster (my laptop) and confirmed that this bug no longer exists on master.  A code inspection reveals that there is no thread limit of 128 limit anymore on the Executor's threadpool from this stacktrace line: {{at spark.executor.Executor.launchTask(Executor.scala:59)}}\n\nHere's the outline of my repro attempt:\n\n{noformat}\naash@aash-mbp ~/git/spark$ cat conf/spark-env.sh\nSPARK_WORKER_CORES=200\nSPARK_MASTER_IP=aash-mbp.local\nSPARK_PUBLIC_DNS=aash-mbp.local\naash@aash-mbp ~/git/spark$ cat conf/spark-defaults.sh\nspark.master                     spark://aash-mbp.local:7077\naash@aash-mbp ~/git/spark$ sbin/start-all.sh\n...\naash@aash-mbp ~/git/spark$ bin/spark-shell\nspark> sc.parallelize(1l to 100000000l,200).reduce(_+_)\nres0: Long = 5000000050000000\nspark>\n{noformat}\n\nI'm now closing this ticket, but please reopen [~xiajunluan] if you're still having issues."}]}}
