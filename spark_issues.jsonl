{"project": "SPARK", "issue_id": "SPARK-836", "title": "ResultTask's serialization forget to handle generation", "status": "Resolved", "priority": "Major", "reporter": "Andy Huang", "assignee": null, "labels": [], "created": "2013-07-30T07:14:16.000+0000", "updated": "2013-12-07T14:37:30.000+0000", "description": null, "comments": ["Could you provide elaboration on this issue? If not, I think we'll have to close it."], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "ResultTask's serialization forget to handle generation"}, {"q": "What updates or decisions were made in the discussion?", "a": "Could you provide elaboration on this issue? If not, I think we'll have to close it."}]}}
{"project": "SPARK", "issue_id": "SPARK-837", "title": "ResultTask's serialization forget about handling \"generation\" field, while ShuffleMapTask does", "status": "Resolved", "priority": "Blocker", "reporter": "Andy Huang", "assignee": null, "labels": [], "created": "2013-07-30T07:29:52.000+0000", "updated": "2013-12-07T14:37:23.000+0000", "description": "In ResultTask's serialization relative method: writeExternal and readExternal, they didn't do anything to generation. \n\nBut in ShuffleMapTask's method, writeExternal and readExternal, they do something like \"partition = in.readInt()\" and \" out.writeLong(generation)\" to them. \n\nAs we know ResultTask will be used after ShuffleMapTask, if right after ShuffleMapTask finish and the work failed for some reason, It will be recomputed, with a \"generation\" bigger than -1. The ResultTask can't get the right data again with default generation, that it will ask DAGScheduler to recompter ShuffleMapTask again. This will last until the whole job crash.\n\n\n", "comments": ["This does seem like a bug -- thanks for reporting it! Going to bump it up in priority.", "This is now fixed as far as I can tell because the epoch is serialized with the ResultTask."], "derived": {"summary": "In ResultTask's serialization relative method: writeExternal and readExternal, they didn't do anything to generation. But in ShuffleMapTask's method, writeExternal and readExternal, they do something like \"partition = in.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "ResultTask's serialization forget about handling \"generation\" field, while ShuffleMapTask does - In ResultTask's serialization relative method: writeExternal and readExternal, they didn't do anything to generation. But in ShuffleMapTask's method, writeExternal and readExternal, they do something like \"partition = in."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is now fixed as far as I can tell because the epoch is serialized with the ResultTask."}]}}
{"project": "SPARK", "issue_id": "SPARK-838", "title": "Add DoubleRDDFunctions methods to PySpark", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Andre Schumacher", "labels": ["Starter"], "created": "2013-07-30T20:55:42.000+0000", "updated": "2013-08-22T16:24:55.000+0000", "description": "This is sum(), stats(), mean(), variance(), etc. Instead of trying to convert Python doubles to Scala, we'll probably have to reimplement the Scala/Java StatCounter in Python.", "comments": ["Resolved by merged pull request #853."], "derived": {"summary": "This is sum(), stats(), mean(), variance(), etc. Instead of trying to convert Python doubles to Scala, we'll probably have to reimplement the Scala/Java StatCounter in Python.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add DoubleRDDFunctions methods to PySpark - This is sum(), stats(), mean(), variance(), etc. Instead of trying to convert Python doubles to Scala, we'll probably have to reimplement the Scala/Java StatCounter in Python."}, {"q": "What updates or decisions were made in the discussion?", "a": "Resolved by merged pull request #853."}]}}
{"project": "SPARK", "issue_id": "SPARK-839", "title": "Bug in how failed executors are removed by ID from standalone cluster", "status": "Closed", "priority": "Critical", "reporter": "Mark Hamstra", "assignee": null, "labels": [], "created": "2013-07-31T09:54:38.000+0000", "updated": "2015-02-09T02:51:58.000+0000", "description": "ClearStory data reported the following issue, where some hashmaps are indexed by executorId and some by appId/executorId, and we use the wrong string to search for an executor: https://github.com/clearstorydata/spark/pull/9. This affects FT on the standalone mode.", "comments": ["Related user list report: https://groups.google.com/forum/?fromgroups=#!topic/spark-users/DXIOjT_DP0Y", "Fixed long ago."], "derived": {"summary": "ClearStory data reported the following issue, where some hashmaps are indexed by executorId and some by appId/executorId, and we use the wrong string to search for an executor: https://github. com/clearstorydata/spark/pull/9.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Bug in how failed executors are removed by ID from standalone cluster - ClearStory data reported the following issue, where some hashmaps are indexed by executorId and some by appId/executorId, and we use the wrong string to search for an executor: https://github. com/clearstorydata/spark/pull/9."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed long ago."}]}}
{"project": "SPARK", "issue_id": "SPARK-840", "title": "Exporting 'SPARK_LAUNCH_WITH_SCALA=1' by default in 'spark-shell' causes 'run' in distribution to fail.", "status": "Resolved", "priority": "Blocker", "reporter": "Benjamin Hindman", "assignee": "Benjamin Hindman", "labels": [], "created": "2013-07-31T10:45:41.000+0000", "updated": "2013-07-31T16:40:12.000+0000", "description": "Even though the distribution has everything it needs invoking `spark-shell` now errors with:\n\nSCALA_HOME is not set and scala is not in PATH\n\nIt looks like the offending commit is f4d514810e6fd9f42868ebb9a89390c62c3b42e1.", "comments": ["Add a fix for this into the pull request at https://github.com/mesos/spark/pull/749. ", "This can be marked as resolved, it was included in https://github.com/mesos/spark/pull/749."], "derived": {"summary": "Even though the distribution has everything it needs invoking `spark-shell` now errors with:\n\nSCALA_HOME is not set and scala is not in PATH\n\nIt looks like the offending commit is f4d514810e6fd9f42868ebb9a89390c62c3b42e1.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Exporting 'SPARK_LAUNCH_WITH_SCALA=1' by default in 'spark-shell' causes 'run' in distribution to fail. - Even though the distribution has everything it needs invoking `spark-shell` now errors with:\n\nSCALA_HOME is not set and scala is not in PATH\n\nIt looks like the offending commit is f4d514810e6fd9f42868ebb9a89390c62c3b42e1."}, {"q": "What updates or decisions were made in the discussion?", "a": "This can be marked as resolved, it was included in https://github.com/mesos/spark/pull/749."}]}}
{"project": "SPARK", "issue_id": "SPARK-841", "title": "Use a smaller job UI port than 33000 by default", "status": "Resolved", "priority": "Minor", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "labels": ["Starter"], "created": "2013-07-31T11:04:14.000+0000", "updated": "2013-09-01T15:33:14.000+0000", "description": "33000 is in the ephemeral port range so it makes it more likely that your job sometimes binds to a higher port than expected.\n\nNote that this change should also be reflected in the EC2 scripts, as they open port 33000-33010 currently to let users view the job UI.", "comments": ["Fixed in https://github.com/mesos/spark/commit/498a26189b197bdaf4be47e6a8baca7b97fe9064 and https://github.com/mesos/spark/commit/793a722f8e14552b8d36f46cca39d336dc2df9dd"], "derived": {"summary": "33000 is in the ephemeral port range so it makes it more likely that your job sometimes binds to a higher port than expected. Note that this change should also be reflected in the EC2 scripts, as they open port 33000-33010 currently to let users view the job UI.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Use a smaller job UI port than 33000 by default - 33000 is in the ephemeral port range so it makes it more likely that your job sometimes binds to a higher port than expected. Note that this change should also be reflected in the EC2 scripts, as they open port 33000-33010 currently to let users view the job UI."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed in https://github.com/mesos/spark/commit/498a26189b197bdaf4be47e6a8baca7b97fe9064 and https://github.com/mesos/spark/commit/793a722f8e14552b8d36f46cca39d336dc2df9dd"}]}}
{"project": "SPARK", "issue_id": "SPARK-842", "title": "Maven assembly is including examples libs and dependencies", "status": "Resolved", "priority": "Major", "reporter": "Konstantin I Boudnik", "assignee": "Matei Alexandru Zaharia", "labels": [], "created": "2013-07-31T14:32:51.000+0000", "updated": "2013-10-10T18:05:02.000+0000", "description": "According to this [email exchange|http://mail-archives.apache.org/mod_mbox/incubator-spark-dev/201307.mbox/%3C62DB7E7A-0547-4090-BB9A-0182829A0D19%40gmail.com%3E] \n\nfinal assembly has to include \"...libraries that users' client programs need to run. These are core, repl (needed if they use the shell), and likely bagel and streaming and mllib,\"\n\nHence, current Maven assembly needs to be fixed accordinly to exclude examples/.  This fix will also affect BIGTOP-715.", "comments": [], "derived": {"summary": "According to this [email exchange|http://mail-archives. apache.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Maven assembly is including examples libs and dependencies - According to this [email exchange|http://mail-archives. apache."}]}}
{"project": "SPARK", "issue_id": "SPARK-843", "title": "Show time the app has been running for in job UI", "status": "Resolved", "priority": "Minor", "reporter": "Matei Alexandru Zaharia", "assignee": "Karen Feng", "labels": [], "created": "2013-07-31T19:29:17.000+0000", "updated": "2013-08-05T10:15:21.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Show time the app has been running for in job UI"}]}}
{"project": "SPARK", "issue_id": "SPARK-844", "title": "Occasional hang on shuffle fetches in master branch", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Patrick McFadin", "labels": [], "created": "2013-07-31T22:59:38.000+0000", "updated": "2020-05-17T18:30:24.000+0000", "description": "In running some iterative jobs with lots of shuffles, I've occasionally seen a few tasks hung with this kind of stack trace:\n\n{noformat}\n\n\"pool-5-thread-1\" prio=10 tid=0x00007f6530049000 nid=0x1217 waiting on condition [0x00007f65d7cfa000]\n   java.lang.Thread.State: WAITING (parking)\n  at sun.misc.Unsafe.park(Native Method)\n  - parking to wait for  <0x00007f6d17a1e298> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n  at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)\n  at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)\n  at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)\n  at spark.storage.BlockFetcherIterator$BasicBlockFetcherIterator.next(BlockFetcherIterator.scala:246)\n  at spark.storage.BlockFetcherIterator$BasicBlockFetcherIterator.next(BlockFetcherIterator.scala:71)\n  at scala.collection.Iterator$$anon$21.hasNext(Iterator.scala:440)\n  at spark.util.CompletionIterator.hasNext(CompletionIterator.scala:26)\n  at scala.collection.Iterator$$anon$22.hasNext(Iterator.scala:457)\n  at scala.collection.Iterator$class.foreach(Iterator.scala:772)\n  at scala.collection.Iterator$$anon$22.foreach(Iterator.scala:451)\n  at spark.Aggregator.combineValuesByKey(Aggregator.scala:37)\n  at spark.PairRDDFunctions$$anonfun$combineByKey$3.apply(PairRDDFunctions.scala:98)\n  at spark.PairRDDFunctions$$anonfun$combineByKey$3.apply(PairRDDFunctions.scala:98)\n  at spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:36)\n  at spark.RDD.computeOrReadCheckpoint(RDD.scala:252)\n  at spark.RDD.iterator(RDD.scala:241)\n  at spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:141)\n  at spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:138)\n  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)\n  at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:38)\n  at spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:138)\n  at spark.RDD.computeOrReadCheckpoint(RDD.scala:252)\n  at spark.RDD.iterator(RDD.scala:241)\n  at spark.MappedValuesRDD.compute(PairRDDFunctions.scala:758)\n  at spark.RDD.computeOrReadCheckpoint(RDD.scala:252)\n  at spark.RDD.iterator(RDD.scala:241)\n  at spark.FlatMappedValuesRDD.compute(PairRDDFunctions.scala:768)\n  at spark.RDD.computeOrReadCheckpoint(RDD.scala:252)\n  at spark.RDD.iterator(RDD.scala:241)\n  at spark.MappedValuesRDD.compute(PairRDDFunctions.scala:758)\n  at spark.RDD.computeOrReadCheckpoint(RDD.scala:252)\n  at spark.RDD.iterator(RDD.scala:241)\n  at spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:141)\n  at spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:138)\n  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)\n  at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:38)\n  at spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:138)\n  at spark.RDD.computeOrReadCheckpoint(RDD.scala:252)\n  at spark.RDD.iterator(RDD.scala:241)\n  at spark.MappedValuesRDD.compute(PairRDDFunctions.scala:758)\n  at spark.RDD.computeOrReadCheckpoint(RDD.scala:252)\n  at spark.RDD.iterator(RDD.scala:241)\n  at spark.FlatMappedValuesRDD.compute(PairRDDFunctions.scala:768)\n  at spark.RDD.computeOrReadCheckpoint(RDD.scala:252)\n  at spark.RDD.iterator(RDD.scala:241)\n  at spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:32)\n  at spark.RDD.computeOrReadCheckpoint(RDD.scala:252)\n  at spark.RDD.iterator(RDD.scala:241)\n  at spark.scheduler.ShuffleMapTask.run(ShuffleMapTask.scala:161)\n  at spark.scheduler.ShuffleMapTask.run(ShuffleMapTask.scala:93)\n  at spark.executor.Executor$TaskRunner.run(Executor.scala:129)\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n  at java.lang.Thread.run(Thread.java:724)\n{noformat}", "comments": ["Hey [~matei] did you look at the logs at all for these nodes? It would be helpful to know whether there was any log output.", "Attach a hung machine's stderr and jstack."], "derived": {"summary": "In running some iterative jobs with lots of shuffles, I've occasionally seen a few tasks hung with this kind of stack trace:\n\n{noformat}\n\n\"pool-5-thread-1\" prio=10 tid=0x00007f6530049000 nid=0x1217 waiting on condition [0x00007f65d7cfa000]\n   java. lang.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Occasional hang on shuffle fetches in master branch - In running some iterative jobs with lots of shuffles, I've occasionally seen a few tasks hung with this kind of stack trace:\n\n{noformat}\n\n\"pool-5-thread-1\" prio=10 tid=0x00007f6530049000 nid=0x1217 waiting on condition [0x00007f65d7cfa000]\n   java. lang."}, {"q": "What updates or decisions were made in the discussion?", "a": "Attach a hung machine's stderr and jstack."}]}}
{"project": "SPARK", "issue_id": "SPARK-845", "title": "Removing an executor can result in a negative number of cores used ", "status": "Resolved", "priority": "Major", "reporter": "Mark Hamstra", "assignee": null, "labels": [], "created": "2013-08-01T11:11:49.000+0000", "updated": "2013-11-14T18:22:39.000+0000", "description": "Under some circumstances, an executor dying or being removed will result in a negative number of cores granted to and reported as used by an application.  Still working on isolating the code paths that produce this result, but one know way to reproduce the problem is to kill a running executor with SIGKILL (kill -9).  If the executor is killed with SIGTERM, then the correct number of cores are removed from running applications; but SIGKILL seems to result in spark.deploy.master.ApplicationInfo.removeExecutor being called multiple times.\n\nA proposed fix is available at https://github.com/clearstorydata/spark/commit/96f5e70fdbf21e188f4fc76a30957cc78302037f", "comments": ["Just marking as fixed -- thanks Mark!"], "derived": {"summary": "Under some circumstances, an executor dying or being removed will result in a negative number of cores granted to and reported as used by an application. Still working on isolating the code paths that produce this result, but one know way to reproduce the problem is to kill a running executor with SIGKILL (kill -9).", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Removing an executor can result in a negative number of cores used  - Under some circumstances, an executor dying or being removed will result in a negative number of cores granted to and reported as used by an application. Still working on isolating the code paths that produce this result, but one know way to reproduce the problem is to kill a running executor with SIGKILL (kill -9)."}, {"q": "What updates or decisions were made in the discussion?", "a": "Just marking as fixed -- thanks Mark!"}]}}
{"project": "SPARK", "issue_id": "SPARK-846", "title": "Set `spark.job.annotation` and display it in the web UI.", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "labels": [], "created": "2013-08-01T13:31:15.000+0000", "updated": "2013-08-10T16:17:40.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Set `spark.job.annotation` and display it in the web UI."}]}}
{"project": "SPARK", "issue_id": "SPARK-847", "title": "Zombie workers", "status": "Resolved", "priority": "Major", "reporter": "Mark Hamstra", "assignee": null, "labels": [], "created": "2013-08-01T14:04:06.000+0000", "updated": "2013-11-14T18:21:48.000+0000", "description": "spark.deploy.master.Worker contains 'workers', a HashSet[WorkerInfo].  The only place where a worker is ever removed from that set is within Worker.addWorker.  Within addWorker, DEAD workers are only removed from the set if they were using both the same host address and port as the new worker being added.  The result of this is that DEAD workers hang around forever if a new worker is never added or is added with a different host:port; and every WORKER_TIMEOUT interval, timeOutDeadWorkers again tries to remove them.\n\nIt looks like one or both of two things needs to happen at least some of the time (proper conditions?): 1) Worker.removeWorker(worker) should remove the worker from the workers HashSet in addition to removing its associated entries in idToWorker, actorToWorker and addressToWorker; 2) addWorker should remove a DEAD worker from the set of workers even when there isn't an exact host:port match.\n\nWorker.removeWorker being called for zombies each WORKER_TIMEOUT is at least one of the reasons for core counts going negative: SPARK-845\n", "comments": ["It looks like worker ports are random in practice, but in theory there could be more than one worker on the same node, which would probably make removing dead worker(s) based only on the hostname infeasible. However, a timeout-based approach to cleaning up dead workers sounds reasonable.", "https://github.com/markhamstra/spark/commit/8e86fa5a11102e113559b7e200e594a05c9d16ef\n", "Just marking as fixed -- thanks Mark!"], "derived": {"summary": "spark. deploy.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Zombie workers - spark. deploy."}, {"q": "What updates or decisions were made in the discussion?", "a": "Just marking as fixed -- thanks Mark!"}]}}
{"project": "SPARK", "issue_id": "SPARK-848", "title": "The StageTable should sort by submitted time by default", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Karen Feng", "labels": [], "created": "2013-08-02T15:12:11.000+0000", "updated": "2013-08-06T19:54:50.000+0000", "description": "I think you can set this using the Javascript sorting library. It would be good if it showed the little sort arrow when it's loaded so people know they can sort on other columns.", "comments": [], "derived": {"summary": "I think you can set this using the Javascript sorting library. It would be good if it showed the little sort arrow when it's loaded so people know they can sort on other columns.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "The StageTable should sort by submitted time by default - I think you can set this using the Javascript sorting library. It would be good if it showed the little sort arrow when it's loaded so people know they can sort on other columns."}]}}
{"project": "SPARK", "issue_id": "SPARK-849", "title": "Variety of small fixes in the Web UI", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Karen Feng", "labels": [], "created": "2013-08-02T16:02:09.000+0000", "updated": "2013-08-06T19:55:14.000+0000", "description": "After discussion with Reynold we came up with some to-do's:\n\nJobs Page:\n- Summary count at the top for active completed stages should link to corresponding sections in page\n- Remove `Stored RDD` Column\n- Should say \"Running/Succeeded\" instead of \"Running/Completed\"\n\nRDD Page\n- For some reason, header says \"Jobs\" in RDD storage page\n- Page should have two titles: \"Data Distribution Summary\" (top part), \"Partitions\" (bottom part)\n\nEnvironment page:\n- Sort all tables by the first column (A -> Z) by default\n\nHeader:\n- Move Jobs tab before storage\n- Right-align the \"Application name\" part if possible\n- Remove the executor count from the header\n\nWe also had some other things I want to note here, but probably aren't in scope for this because they are trickier:\n\n- Clicking Spark logo should go to default page\n- Task progress should be overlaid with progress bar\n\n", "comments": [], "derived": {"summary": "After discussion with Reynold we came up with some to-do's:\n\nJobs Page:\n- Summary count at the top for active completed stages should link to corresponding sections in page\n- Remove `Stored RDD` Column\n- Should say \"Running/Succeeded\" instead of \"Running/Completed\"\n\nRDD Page\n- For some reason, header says \"Jobs\" in RDD storage page\n- Page should have two titles: \"Data Distribution Summary\" (top part), \"Partitions\" (bottom part)\n\nEnvironment page:\n- Sort all tables by the first column (A -> Z) by default\n\nHeader:\n- Move Jobs tab before storage\n- Right-align the \"Application name\" part if possible\n- Remove the executor count from the header\n\nWe also had some other things I want to note here, but probably aren't in scope for this because they are trickier:\n\n- Clicking Spark logo should go to default page\n- Task progress should be overlaid with progress bar.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Variety of small fixes in the Web UI - After discussion with Reynold we came up with some to-do's:\n\nJobs Page:\n- Summary count at the top for active completed stages should link to corresponding sections in page\n- Remove `Stored RDD` Column\n- Should say \"Running/Succeeded\" instead of \"Running/Completed\"\n\nRDD Page\n- For some reason, header says \"Jobs\" in RDD storage page\n- Page should have two titles: \"Data Distribution Summary\" (top part), \"Partitions\" (bottom part)\n\nEnvironment page:\n- Sort all tables by the first column (A -> Z) by default\n\nHeader:\n- Move Jobs tab before storage\n- Right-align the \"Application name\" part if possible\n- Remove the executor count from the header\n\nWe also had some other things I want to note here, but probably aren't in scope for this because they are trickier:\n\n- Clicking Spark logo should go to default page\n- Task progress should be overlaid with progress bar."}]}}
{"project": "SPARK", "issue_id": "SPARK-850", "title": "WARN cluster.ClusterScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered", "status": "Resolved", "priority": "Major", "reporter": "William Zajac", "assignee": "William Zajac", "labels": [], "created": "2013-08-02T17:43:19.000+0000", "updated": "2013-08-05T12:10:51.000+0000", "description": "When you running the spark job with SPARK_MEM set too large, you will receive the following error  \"WARN cluster.ClusterScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered\"  Could you give a better error message in the console.\n\nStep to recreate the issue:\n\n1. set the SPARK_MEM in conf/spark-env.sh close to your memory limit on the box.\n2. run the interactive Spark shell against the local cluster; for me is \"MASTER=spark://billz-retina.local:7077 ./spark-shell\"\n3. run the spark job; i.e. \"val a = sc.parallelize(1 to 100)\", a.count();\n\nyou will see console error:\n13/08/02 16:45:01 WARN cluster.ClusterScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered\n13/08/02 16:45:16 WARN cluster.ClusterScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered\n\n\nIn logs/spark-bill-spark.deploy.master.Master-1-billz-retina.local.out:\n\n13/08/02 16:43:22 INFO master.Master: Registering app Spark shell\n13/08/02 16:43:22 WARN master.Master: Could not find any workers with enough memory for app-20130802163627-0000\n13/08/02 16:43:22 INFO master.Master: Registered app Spark shell with ID app-20130802164322-0002", "comments": ["Hey Bill,\n\nIt might be a good idea to append \"and have sufficient memory\" to the warning message at the driver. Would you mind submitting a pull request for this?\n\n- Patrick", "I will submit  a pull request."], "derived": {"summary": "When you running the spark job with SPARK_MEM set too large, you will receive the following error  \"WARN cluster. ClusterScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered\"  Could you give a better error message in the console.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "WARN cluster.ClusterScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered - When you running the spark job with SPARK_MEM set too large, you will receive the following error  \"WARN cluster. ClusterScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered\"  Could you give a better error message in the console."}, {"q": "What updates or decisions were made in the discussion?", "a": "I will submit  a pull request."}]}}
{"project": "SPARK", "issue_id": "SPARK-851", "title": "Summary count at the top for active completed stages should link to corresponding sections in page", "status": "Resolved", "priority": "Major", "reporter": "Karen Feng", "assignee": "Karen Feng", "labels": [], "created": "2013-08-05T13:10:01.000+0000", "updated": "2013-08-06T19:56:41.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Summary count at the top for active completed stages should link to corresponding sections in page"}]}}
{"project": "SPARK", "issue_id": "SPARK-852", "title": "Remove `Stored RDD` Column", "status": "Resolved", "priority": "Major", "reporter": "Karen Feng", "assignee": "Karen Feng", "labels": [], "created": "2013-08-05T13:10:46.000+0000", "updated": "2013-08-06T19:56:49.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Remove `Stored RDD` Column"}]}}
{"project": "SPARK", "issue_id": "SPARK-853", "title": "Should say \"Running/Succeeded\" instead of \"Running/Completed\"", "status": "Resolved", "priority": "Major", "reporter": "Karen Feng", "assignee": "Karen Feng", "labels": [], "created": "2013-08-05T13:11:01.000+0000", "updated": "2013-08-06T19:56:57.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Should say \"Running/Succeeded\" instead of \"Running/Completed\""}]}}
{"project": "SPARK", "issue_id": "SPARK-854", "title": "For some reason, header says \"Jobs\" in RDD storage page", "status": "Resolved", "priority": "Major", "reporter": "Karen Feng", "assignee": "Karen Feng", "labels": [], "created": "2013-08-05T13:11:12.000+0000", "updated": "2013-08-06T19:57:03.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "For some reason, header says \"Jobs\" in RDD storage page"}]}}
{"project": "SPARK", "issue_id": "SPARK-855", "title": "Page should have two titles: \"Data Distribution Summary\" (top part), \"Partitions\" (bottom part)", "status": "Resolved", "priority": "Major", "reporter": "Karen Feng", "assignee": "Karen Feng", "labels": [], "created": "2013-08-05T13:11:21.000+0000", "updated": "2013-08-06T19:57:09.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Page should have two titles: \"Data Distribution Summary\" (top part), \"Partitions\" (bottom part)"}]}}
{"project": "SPARK", "issue_id": "SPARK-856", "title": "Sort all tables by the first column (A -> Z) by default", "status": "Resolved", "priority": "Major", "reporter": "Karen Feng", "assignee": "Karen Feng", "labels": [], "created": "2013-08-05T13:11:33.000+0000", "updated": "2013-08-06T19:55:39.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Sort all tables by the first column (A -> Z) by default"}]}}
{"project": "SPARK", "issue_id": "SPARK-857", "title": "Move Jobs tab before storage", "status": "Resolved", "priority": "Major", "reporter": "Karen Feng", "assignee": "Karen Feng", "labels": [], "created": "2013-08-05T13:11:45.000+0000", "updated": "2013-08-06T19:57:15.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Move Jobs tab before storage"}]}}
{"project": "SPARK", "issue_id": "SPARK-858", "title": "Right-align the \"Application name\" part if possible", "status": "Resolved", "priority": "Major", "reporter": "Karen Feng", "assignee": "Karen Feng", "labels": [], "created": "2013-08-05T13:11:55.000+0000", "updated": "2013-08-06T19:57:21.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Right-align the \"Application name\" part if possible"}]}}
{"project": "SPARK", "issue_id": "SPARK-859", "title": "Remove the executor count from the header", "status": "Resolved", "priority": "Major", "reporter": "Karen Feng", "assignee": "Karen Feng", "labels": [], "created": "2013-08-05T13:12:06.000+0000", "updated": "2013-08-06T19:57:27.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Remove the executor count from the header"}]}}
{"project": "SPARK", "issue_id": "SPARK-860", "title": "Clicking Spark logo should go to default page", "status": "Resolved", "priority": "Major", "reporter": "Karen Feng", "assignee": "Karen Feng", "labels": [], "created": "2013-08-05T13:12:16.000+0000", "updated": "2013-08-06T19:55:49.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Clicking Spark logo should go to default page"}]}}
{"project": "SPARK", "issue_id": "SPARK-861", "title": "Task progress should be overlaid with progress bar", "status": "Resolved", "priority": "Major", "reporter": "Karen Feng", "assignee": "Karen Feng", "labels": [], "created": "2013-08-05T13:12:25.000+0000", "updated": "2013-08-06T19:55:29.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Task progress should be overlaid with progress bar"}]}}
{"project": "SPARK", "issue_id": "SPARK-862", "title": "Could not use spark-ec2 to launch clusters with instance type 'cc1.4xlarge'", "status": "Closed", "priority": "Major", "reporter": "Hai-Anh Trinh", "assignee": null, "labels": [], "created": "2013-08-05T22:43:09.000+0000", "updated": "2013-08-20T16:10:30.000+0000", "description": "Error message: Virtualization type 'hvm' is required for instances of type 'cc1.4xlarge'.\n\nI believe this apply for other Cluster Compute instances as well, which must use HVM-based AMI, see: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using_cluster_computing.html \n\n{code}\n$ spark-ec2 -k mykey -i mykey.pem -s 4 --instance-type=cc1.4xlarge -w 120 --zone=us-east-1e --cluster-type=mesos launch spark3\nSetting up security groups...\nSearching for existing cluster ada-spark3...\nLatest Spark AMI: ami-530f7a3a\nLaunching instances...\nERROR:boto:400 Bad Request\nERROR:boto:<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Response><Errors><Error><Code>InvalidParameterCombination</Code><Message>Virtualization type 'hvm' is required for instances of type 'cc1.4xlarge'.</Message></Error></Errors><RequestID>c7bceb36-42f0-4f5d-95b1-8f6b50930cfb</RequestID></Response>\nTraceback (most recent call last):\n  File \"./spark_ec2.py\", line 761, in <module>\n    main()\n  File \"./spark_ec2.py\", line 621, in main\n    conn, opts, cluster_name)\n  File \"./spark_ec2.py\", line 329, in launch_cluster\n    block_device_map = block_map)\n  File \"/Users/aht/src/adatao/BigR/Installer/spark-ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/ec2/image.py\", line 255, in run\n  File \"/Users/aht/src/adatao/BigR/Installer/spark-ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/ec2/connection.py\", line 678, in run_instances\n  File \"/Users/aht/src/adatao/BigR/Installer/spark-ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/connection.py\", line 925, in get_object\nboto.exception.EC2ResponseError: EC2ResponseError: 400 Bad Request\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Response><Errors><Error><Code>InvalidParameterCombination</Code><Message>Virtualization type 'hvm' is required for instances of type 'cc1.4xlarge'.</Message></Error></Errors><RequestID>c7bceb36-42f0-4f5d-95b1-8f6b50930cfb</RequestID></Response>\n{code}", "comments": ["This has been fixed in Patrick's latest EC2 scripts.", "The mentioned fix is at https://github.com/mesos/spark/pull/603, right ?\n\nAlso, do you want to mark this issue as dup of\nhttps://spark-project.atlassian.net/browse/SPARK-728 ?\n", "Yep - this duplicates SPARK-728"], "derived": {"summary": "Error message: Virtualization type 'hvm' is required for instances of type 'cc1. 4xlarge'.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Could not use spark-ec2 to launch clusters with instance type 'cc1.4xlarge' - Error message: Virtualization type 'hvm' is required for instances of type 'cc1. 4xlarge'."}, {"q": "What updates or decisions were made in the discussion?", "a": "Yep - this duplicates SPARK-728"}]}}
{"project": "SPARK", "issue_id": "SPARK-863", "title": "Have Synchronous Versions of `stop-all.sh` and `start-all.sh`", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "labels": [], "created": "2013-08-06T14:55:53.000+0000", "updated": "2015-02-26T01:26:55.000+0000", "description": "When doing things like restarting the cluster, it's sometimes necessary to know when stopping has completed (it can take a long time to delete temporary files). We should have a synchronous version of these scripts that won't return until the cluster is actually stopped.", "comments": ["Despite the title it seems like this is about stopping only, and {{stop-all.sh}} has a {{--wait}} flag now. I'm not as sure how you wait on startup since it would require knowing the services were up and healthy."], "derived": {"summary": "When doing things like restarting the cluster, it's sometimes necessary to know when stopping has completed (it can take a long time to delete temporary files). We should have a synchronous version of these scripts that won't return until the cluster is actually stopped.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Have Synchronous Versions of `stop-all.sh` and `start-all.sh` - When doing things like restarting the cluster, it's sometimes necessary to know when stopping has completed (it can take a long time to delete temporary files). We should have a synchronous version of these scripts that won't return until the cluster is actually stopped."}, {"q": "What updates or decisions were made in the discussion?", "a": "Despite the title it seems like this is about stopping only, and {{stop-all.sh}} has a {{--wait}} flag now. I'm not as sure how you wait on startup since it would require knowing the services were up and healthy."}]}}
{"project": "SPARK", "issue_id": "SPARK-864", "title": "DAGScheduler Exception if A Node is Added then Deleted", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "xiajunluan", "labels": [], "created": "2013-08-06T20:41:25.000+0000", "updated": "2020-05-17T17:47:27.000+0000", "description": "According to [~markhamstra], if you run the UI tester locally and remove a slave, then add another slave, everything freezes. UPDATE: This appears to be caused by the DAGScheduler:\n\n{code}\nException in thread \"DAGScheduler\" java.util.NoSuchElementException: key not found: 2\n\tat scala.collection.MapLike$class.default(MapLike.scala:225)\n\tat scala.collection.mutable.HashMap.default(HashMap.scala:45)\n\tat scala.collection.MapLike$class.apply(MapLike.scala:135)\n\tat scala.collection.mutable.HashMap.apply(HashMap.scala:45)\n\tat spark.scheduler.DAGScheduler.spark$scheduler$DAGScheduler$$submitMissingTasks(DAGScheduler.scala:515)\n\tat spark.scheduler.DAGScheduler.spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:481)\n\tat spark.scheduler.DAGScheduler$$anonfun$resubmitFailedStages$3.apply(DAGScheduler.scala:383)\n\tat spark.scheduler.DAGScheduler$$anonfun$resubmitFailedStages$3.apply(DAGScheduler.scala:382)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)\n\tat scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:38)\n\tat spark.scheduler.DAGScheduler.resubmitFailedStages(DAGScheduler.scala:382)\n\tat spark.scheduler.DAGScheduler.spark$scheduler$DAGScheduler$$run(DAGScheduler.scala:433)\n\tat spark.scheduler.DAGScheduler$$anon$1.run(DAGScheduler.scala:135)\n{code}\n\nThis code is related to the FairScheduler change. Hey [~andrew xia] - could you take a look at this?", "comments": ["Right, so the scenario is:\n\n./run spark.deploy.master.Master\n./run spark.deploy.worker.Worker spark://...\n./run spark.ui.UIWorkloadGenerator spark://...\n\n...everything looks lovely in the WebUI...\n\n...kill the worker\n\n...everything is dead in the WebUI (obviously, since there is no worker available)\n\n./run spark.deploy.worker.Worker spark://...\n\n...looks like UIWorkloadGenerator is trying to launch new jobs with the new Worker...\n...but the application details page of the WebUI doesn't change from when the first Worker was killed\n\nI haven't worked out just how serious a problem this really is (i.e. whether something similar happens with real jobs, or whether this is just an artifact of how the UIWorkloadGenerator works); but regardless, I don't think it really shows what you want it to right now.", "Sure, I will look into this issue.", "Closing as it looks exceptionally stale at this point and haven't seen similar reports"], "derived": {"summary": "According to [~markhamstra], if you run the UI tester locally and remove a slave, then add another slave, everything freezes. UPDATE: This appears to be caused by the DAGScheduler:\n\n{code}\nException in thread \"DAGScheduler\" java.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "DAGScheduler Exception if A Node is Added then Deleted - According to [~markhamstra], if you run the UI tester locally and remove a slave, then add another slave, everything freezes. UPDATE: This appears to be caused by the DAGScheduler:\n\n{code}\nException in thread \"DAGScheduler\" java."}, {"q": "What updates or decisions were made in the discussion?", "a": "Closing as it looks exceptionally stale at this point and haven't seen similar reports"}]}}
{"project": "SPARK", "issue_id": "SPARK-865", "title": "Add the equivalent of ADD_JARS to PySpark", "status": "Resolved", "priority": "Minor", "reporter": "Matei Alexandru Zaharia", "assignee": "Andre Schumacher", "labels": ["Starter"], "created": "2013-08-06T23:42:49.000+0000", "updated": "2013-08-12T21:46:19.000+0000", "description": "There should be an environment variable for the user to specify zip files / eggs to be passed to SparkContext and sent to workers.", "comments": ["PySpark's SparkContext has a pyFiles constructor parameter which seems to do exactly what is required. How about naming this environment variable then PY_FILES ? Or PYSPARK_FILES to be less generic?", "Created a pull request which uses PYSPAKR_FILES"], "derived": {"summary": "There should be an environment variable for the user to specify zip files / eggs to be passed to SparkContext and sent to workers.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add the equivalent of ADD_JARS to PySpark - There should be an environment variable for the user to specify zip files / eggs to be passed to SparkContext and sent to workers."}, {"q": "What updates or decisions were made in the discussion?", "a": "Created a pull request which uses PYSPAKR_FILES"}]}}
{"project": "SPARK", "issue_id": "SPARK-866", "title": "Add the equivalent of ADD_JARS to PySpark", "status": "Closed", "priority": "Minor", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "2013-08-06T23:43:01.000+0000", "updated": "2013-08-12T17:25:38.000+0000", "description": "There should be an environment variable for the user to specify zip files / eggs to be passed to SparkContext and sent to workers.", "comments": ["PySpark's SparkContext has a pyFiles constructor parameter which seems to do exactly what is required. How about naming this environment variable then PY_FILES ? Or PYSPARK_FILES to be less generic?", "I'd actually call it ADD_FILES to be similar to the Scala one. It's only a way of passing a parameter to the ./pyspark binary, so I don't want it to have a name that makes it sound like it would work for an arbitrary PySpark program."], "derived": {"summary": "There should be an environment variable for the user to specify zip files / eggs to be passed to SparkContext and sent to workers.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add the equivalent of ADD_JARS to PySpark - There should be an environment variable for the user to specify zip files / eggs to be passed to SparkContext and sent to workers."}, {"q": "What updates or decisions were made in the discussion?", "a": "I'd actually call it ADD_FILES to be similar to the Scala one. It's only a way of passing a parameter to the ./pyspark binary, so I don't want it to have a name that makes it sound like it would work for an arbitrary PySpark program."}]}}
{"project": "SPARK", "issue_id": "SPARK-867", "title": "Add a native Python way to create input RDDs in PySpark", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "2013-08-07T16:49:32.000+0000", "updated": "2015-05-31T20:59:08.000+0000", "description": "While PySpark can easily read RDDs of Strings from Java/Scala, it would be nice to create your own subclass that implements the partitions(), preferredLocations() and compute() methods as in Java, to plug in new input sources. It shouldn't be too hard to turn this into a special RDD on the Java side by, say, parallelizing an array of partition objects and piping them through Python.", "comments": ["I'm going to close this as \"Won't Fix\" in order to help clear out the Python issue backlog."], "derived": {"summary": "While PySpark can easily read RDDs of Strings from Java/Scala, it would be nice to create your own subclass that implements the partitions(), preferredLocations() and compute() methods as in Java, to plug in new input sources. It shouldn't be too hard to turn this into a special RDD on the Java side by, say, parallelizing an array of partition objects and piping them through Python.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add a native Python way to create input RDDs in PySpark - While PySpark can easily read RDDs of Strings from Java/Scala, it would be nice to create your own subclass that implements the partitions(), preferredLocations() and compute() methods as in Java, to plug in new input sources. It shouldn't be too hard to turn this into a special RDD on the Java side by, say, parallelizing an array of partition objects and piping them through Python."}, {"q": "What updates or decisions were made in the discussion?", "a": "I'm going to close this as \"Won't Fix\" in order to help clear out the Python issue backlog."}]}}
{"project": "SPARK", "issue_id": "SPARK-868", "title": "Document fair scheduler", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "2013-08-12T16:31:15.000+0000", "updated": "2013-09-09T16:18:13.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Document fair scheduler"}]}}
{"project": "SPARK", "issue_id": "SPARK-869", "title": "Retrofit rest of RDD api to use proper serializer type", "status": "Resolved", "priority": "Major", "reporter": "Dmitriy Lyubimov", "assignee": null, "labels": [], "created": "2013-08-12T18:31:28.000+0000", "updated": "2015-09-16T18:30:56.000+0000", "description": "SPARK-826 and SPARK-827 resolved proper serialization support for some RDD method parameters, but  not all. \n\nThis issue is to address the rest of RDD api and operations. Most of the time this is due to wrapping RDD parameters into a closure which can only use a closure serializer to communicate to the backend.", "comments": ["Sorry. the correct issues to reference are SPARK-826 and SPARK-835", "Is this still live -- what are other methods that need the treatment? I know there's an issue about cloning the zero value in aggregate with the right serializer.", "Going to resolve this as Done; please open a new JIRA if you find specific examples where we're using the wrong serializer."], "derived": {"summary": "SPARK-826 and SPARK-827 resolved proper serialization support for some RDD method parameters, but  not all. This issue is to address the rest of RDD api and operations.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Retrofit rest of RDD api to use proper serializer type - SPARK-826 and SPARK-827 resolved proper serialization support for some RDD method parameters, but  not all. This issue is to address the rest of RDD api and operations."}, {"q": "What updates or decisions were made in the discussion?", "a": "Going to resolve this as Done; please open a new JIRA if you find specific examples where we're using the wrong serializer."}]}}
{"project": "SPARK", "issue_id": "SPARK-870", "title": "Jobs UI shows incorrect task count if #tasks is not #partitions", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "labels": [], "created": "2013-08-12T22:15:46.000+0000", "updated": "2013-10-25T10:20:18.000+0000", "description": "{code}\nval rdd = sc.textFile(\"/tpch10g/lineitem\")\nsc.runJob(rdd, (it: Iterator[String]) => it.take(10).toArray, Array(0), false)\n{code}", "comments": ["I'm bumping this to 0.8.1. It only affects things in a few cases and it's not super simple to fix.", "Which PR fixed this?", "https://github.com/apache/incubator-spark/commit/fa9a0e40b2d76b85918958cf7d57ec95f766e785#diff-6ddec7f06d0cf5392943ecdb80fcea24R55"], "derived": {"summary": "{code}\nval rdd = sc. textFile(\"/tpch10g/lineitem\")\nsc.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Jobs UI shows incorrect task count if #tasks is not #partitions - {code}\nval rdd = sc. textFile(\"/tpch10g/lineitem\")\nsc."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/incubator-spark/commit/fa9a0e40b2d76b85918958cf7d57ec95f766e785#diff-6ddec7f06d0cf5392943ecdb80fcea24R55"}]}}
{"project": "SPARK", "issue_id": "SPARK-871", "title": "Add a link to the stdout/stderr log on the job level web ui", "status": "Resolved", "priority": "Major", "reporter": "Reynold Xin", "assignee": "Patrick McFadin", "labels": [], "created": "2013-08-12T22:49:16.000+0000", "updated": "2014-03-25T15:11:31.000+0000", "description": null, "comments": ["This was fixed a long time ago."], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add a link to the stdout/stderr log on the job level web ui"}, {"q": "What updates or decisions were made in the discussion?", "a": "This was fixed a long time ago."}]}}
{"project": "SPARK", "issue_id": "SPARK-872", "title": "Should revive offer after tasks finish in Mesos fine-grained mode ", "status": "Resolved", "priority": "Major", "reporter": "xiajunluan", "assignee": null, "labels": [], "created": "2013-08-13T05:33:03.000+0000", "updated": "2015-05-19T14:03:17.000+0000", "description": "when running spark on latest Mesos release, I notice that spark on mesos fine-grained could not schedule spark tasks effectively, for example, if slave has 4 cpu cores resource, mesos master will call resourceOffer function of spark until 4 cpu cores are all free. but In my points like standalone scheduler mode, if one task finished and one cpus core is free, Mesos master should call spark resourceOffer to allocate resource to tasks. ", "comments": ["I'm not quite understanding your statement where Mesos master will call resourceOffer until 4 cores are free? Can you elaborate what that means?", "I think we should close this due to inactivity. What is the policy w.r.t. to the status to use?"], "derived": {"summary": "when running spark on latest Mesos release, I notice that spark on mesos fine-grained could not schedule spark tasks effectively, for example, if slave has 4 cpu cores resource, mesos master will call resourceOffer function of spark until 4 cpu cores are all free. but In my points like standalone scheduler mode, if one task finished and one cpus core is free, Mesos master should call spark resourceOffer to allocate resource to tasks.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Should revive offer after tasks finish in Mesos fine-grained mode  - when running spark on latest Mesos release, I notice that spark on mesos fine-grained could not schedule spark tasks effectively, for example, if slave has 4 cpu cores resource, mesos master will call resourceOffer function of spark until 4 cpu cores are all free. but In my points like standalone scheduler mode, if one task finished and one cpus core is free, Mesos master should call spark resourceOffer to allocate resource to tasks."}, {"q": "What updates or decisions were made in the discussion?", "a": "I think we should close this due to inactivity. What is the policy w.r.t. to the status to use?"}]}}
{"project": "SPARK", "issue_id": "SPARK-873", "title": "Add a way to specify rack topology in Mesos and standalone modes", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "2013-08-13T10:53:42.000+0000", "updated": "2016-01-12T13:42:54.000+0000", "description": "Right now the YARN mode can look up rack information from YARN, but the standalone and Mesos modes don't have any way of specifying rack topology. We should have a pluggable script or config file that allows this. For the standalone mode, we'd probably want the rack info to be known by the Master rather than driver apps, and maybe the apps can get a cluster map when they register.", "comments": [], "derived": {"summary": "Right now the YARN mode can look up rack information from YARN, but the standalone and Mesos modes don't have any way of specifying rack topology. We should have a pluggable script or config file that allows this.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add a way to specify rack topology in Mesos and standalone modes - Right now the YARN mode can look up rack information from YARN, but the standalone and Mesos modes don't have any way of specifying rack topology. We should have a pluggable script or config file that allows this."}]}}
{"project": "SPARK", "issue_id": "SPARK-874", "title": "Have a --wait flag in ./sbin/stop-all.sh that polls until Worker's are finished", "status": "Resolved", "priority": "Minor", "reporter": "Patrick Wendell", "assignee": "Ben Cook", "labels": ["starter"], "created": "2013-08-13T15:39:52.000+0000", "updated": "2014-12-09T20:17:27.000+0000", "description": "When running benchmarking jobs, sometimes the cluster takes a long time to shut down. We should add a feature where it will ssh into all the workers every few seconds and check that the processes are dead, and won't return until they are all dead. This would help a lot with automating benchmarking scripts.\n\nThere is some equivalent logic here written in python, we just need to add it to the shell script:\nhttps://github.com/pwendell/spark-perf/blob/master/bin/run#L117", "comments": ["I am interested in taking it up, please do assign. Thanks :) .", "I am not sure if this is still being worked on. If it is not, could you please assign it to me.", "User 'jbencook' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/3567"], "derived": {"summary": "When running benchmarking jobs, sometimes the cluster takes a long time to shut down. We should add a feature where it will ssh into all the workers every few seconds and check that the processes are dead, and won't return until they are all dead.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Have a --wait flag in ./sbin/stop-all.sh that polls until Worker's are finished - When running benchmarking jobs, sometimes the cluster takes a long time to shut down. We should add a feature where it will ssh into all the workers every few seconds and check that the processes are dead, and won't return until they are all dead."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'jbencook' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/3567"}]}}
{"project": "SPARK", "issue_id": "SPARK-875", "title": "Disk mounts can be wonky on EC2", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "labels": [], "created": "2013-08-13T15:40:44.000+0000", "updated": "2013-08-26T10:08:13.000+0000", "description": null, "comments": ["Here's what I saw:\n\n{noformat}\nroot@ip-10-141-150-79 ~]$ mount\n/dev/xvda1 on / type ext4 (rw,noatime)\nproc on /proc type proc (rw)\nsysfs on /sys type sysfs (rw)\ndevpts on /dev/pts type devpts (rw,gid=5,mode=620)\ntmpfs on /dev/shm type tmpfs (rw)\n/dev/xvdb on /mnt type ext3 (rw,noatime)\n/dev/xvdc on /mnt2 type ext3 (rw,noatime)\n/dev/xvdd on /mnt3 type ext3 (rw,noatime)\nnone on /proc/sys/fs/binfmt_misc type binfmt_misc (rw)\n/dev/xvdf on /mnt2 type ext3 (rw,noatime)\n/dev/xvdc on /mnt3 type ext3 (rw,noatime)\n/dev/xvdd on /mnt4 type ext3 (rw,noatime)\nnone on /cgroup type cgroup (rw)\nroot@ip-10-141-150-79 ~]$ df -h\nFilesystem            Size  Used Avail Use% Mounted on\n/dev/xvda1            7.9G  3.0G  4.9G  39% /\ntmpfs                 7.4G     0  7.4G   0% /dev/shm\n/dev/xvdb             414G  3.2G  390G   1% /mnt\n/dev/xvdc             414G  199M  393G   1% /mnt2\n/dev/xvdd             414G  199M  393G   1% /mnt3\n/dev/xvdf             414G  199M  393G   1% /mnt2\n/dev/xvdc             414G  199M  393G   1% /mnt3\n/dev/xvdd             414G  199M  393G   1% /mnt4\n{noformat}", "This was on m1.4xlarge nodes with no other arguments."], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Disk mounts can be wonky on EC2"}, {"q": "What updates or decisions were made in the discussion?", "a": "This was on m1.4xlarge nodes with no other arguments."}]}}
{"project": "SPARK", "issue_id": "SPARK-876", "title": "Install Missing Features on AMI", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "labels": [], "created": "2013-08-13T17:50:03.000+0000", "updated": "2013-08-26T10:08:25.000+0000", "description": "We need to copy the ~/.vim directory from the old ami. We also need to install some python packages. Notes from [~matei] below:\n\n===\nyum update\n\nyum install python-matplotlib scipy python-tornado gcc-c++\n\neasy_install pyzmq\n\nNote that the latter takes a while to decide to build its own ZMQ, and prints an error at the end, but it does work. You should be able to run ipython notebook --ip=* --port=33000 and see it. Note that if you do this, you should delete the *.ipynb file it creates before you make a new AMI.", "comments": [], "derived": {"summary": "We need to copy the ~/. vim directory from the old ami.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Install Missing Features on AMI - We need to copy the ~/. vim directory from the old ami."}]}}
{"project": "SPARK", "issue_id": "SPARK-877", "title": "java.lang.UnsupportedOperationException: empty.reduceLeft in UI", "status": "Resolved", "priority": "Major", "reporter": "Reynold Xin", "assignee": "Kay Ousterhout", "labels": [], "created": "2013-08-13T23:00:17.000+0000", "updated": "2013-08-14T08:33:39.000+0000", "description": "I opened stage's job progress UI page which had no active tasks and saw the following exception:\n\n{code}\njava.lang.UnsupportedOperationException: empty.reduceLeft\n        at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:152)\n        at scala.collection.mutable.ArrayOps.scala$collection$IndexedSeqOptimized$$super$reduceLeft(ArrayOps.scala:38)\n        at scala.collection.IndexedSeqOptimized$class.reduceLeft(IndexedSeqOptimized.scala:69)\n        at scala.collection.mutable.ArrayOps.reduceLeft(ArrayOps.scala:38)\n        at scala.collection.TraversableOnce$class.reduce(TraversableOnce.scala:180)\n        at scala.collection.mutable.ArrayOps.reduce(ArrayOps.scala:38)\n        at spark.ui.exec.ExecutorsUI.render(ExecutorsUI.scala:41)\n        at spark.ui.exec.ExecutorsUI$$anonfun$getHandlers$1.apply(ExecutorsUI.scala:35)\n        at spark.ui.exec.ExecutorsUI$$anonfun$getHandlers$1.apply(ExecutorsUI.scala:35)\n        at spark.ui.JettyUtils$$anon$1.handle(JettyUtils.scala:61)\n        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1040)\n        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:976)\n        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)\n        at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52)\n        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)\n        at org.eclipse.jetty.server.Server.handle(Server.java:363)\n        at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:483)\n        at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:920)\n        at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:982\n)\n        at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:635)\n        at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)\n        at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)\n        at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:628)\n        at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)\n        at java.lang.Thread.run(Thread.java:722)\n{code}", "comments": ["Hey this stack trace suggests you were accessing the Executors page rather than the job progress page. Is that right? \n\n{code}\n  at spark.ui.exec.ExecutorsUI.render(ExecutorsUI.scala:41)\n        at spark.ui.exec.ExecutorsUI$$anonfun$getHandlers$1.apply(ExecutorsUI.scala:35)\n        at spark.ui.exec.ExecutorsUI$$anonfun$getHandlers$1.apply(ExecutorsUI.scala:35)\n{code}", "That was probably right (otherwise the stack wouldn't make sense). I had multiple pages open at the time.\n\n"], "derived": {"summary": "I opened stage's job progress UI page which had no active tasks and saw the following exception:\n\n{code}\njava. lang.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "java.lang.UnsupportedOperationException: empty.reduceLeft in UI - I opened stage's job progress UI page which had no active tasks and saw the following exception:\n\n{code}\njava. lang."}, {"q": "What updates or decisions were made in the discussion?", "a": "That was probably right (otherwise the stack wouldn't make sense). I had multiple pages open at the time."}]}}
{"project": "SPARK", "issue_id": "SPARK-878", "title": "PySpark does not add Python *.zip and *.egg files to PYTHONPATH", "status": "Resolved", "priority": "Minor", "reporter": "Andre Schumacher", "assignee": "Andre Schumacher", "labels": ["Starter"], "created": "2013-08-15T11:45:18.000+0000", "updated": "2013-08-19T15:31:51.000+0000", "description": "When a list of *.zip or *.egg files is passed to SparkContext via pyFiles these do not get added to PYTHONPATH on the worker. The situation is different for *.py files since for these it is sufficient to add the working directory to PYTHONPATH (via sys.path).\n\nFor the original discussion see:\n\nhttps://groups.google.com/forum/#!searchin/spark-users/pyfiles/spark-users/jG8VC17vTe4/z_DhBoRWuAMJ", "comments": ["Does Jey's patch fix this?  Want to test that and submit it as a pull request?", "It does but it does also add all other *.zip files and such to the path. I actually talked to him and he suggested that it could be a nice case for learning PySpark internals. I now have an alternative solution (adding an include list to PythonRDD which is serialized into the worker input stream) which I still need to test though. Then it would be great to hear your comments on that, Josh.", "Added pull request. Thanks Jey for discussing various approaches and helpful comments.", "Issue was fixed in pull request #840. I tried to mark it as \"resolved\" but possibly my access right don't allow me to(?!)"], "derived": {"summary": "When a list of *. zip or *.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "PySpark does not add Python *.zip and *.egg files to PYTHONPATH - When a list of *. zip or *."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue was fixed in pull request #840. I tried to mark it as \"resolved\" but possibly my access right don't allow me to(?!)"}]}}
{"project": "SPARK", "issue_id": "SPARK-879", "title": "Typo in slaves file: \"listes\" instead of \"listed\"", "status": "Resolved", "priority": "Trivial", "reporter": "William McNeill", "assignee": "SeanM", "labels": [], "created": "2013-08-15T14:54:21.000+0000", "updated": "2013-09-01T15:35:03.000+0000", "description": "The comment at the top of the conf/slaves file reads \"A Spark Worker will be started on each of the machines listes below.\" It should say \"listed\" instead of \"listes\".", "comments": ["resolved by: https://github.com/mesos/spark/pull/843"], "derived": {"summary": "The comment at the top of the conf/slaves file reads \"A Spark Worker will be started on each of the machines listes below. \" It should say \"listed\" instead of \"listes\".", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Typo in slaves file: \"listes\" instead of \"listed\" - The comment at the top of the conf/slaves file reads \"A Spark Worker will be started on each of the machines listes below. \" It should say \"listed\" instead of \"listes\"."}, {"q": "What updates or decisions were made in the discussion?", "a": "resolved by: https://github.com/mesos/spark/pull/843"}]}}
{"project": "SPARK", "issue_id": "SPARK-880", "title": "When built with Hadoop2, spark-shell and examples don't initialize log4j properly", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "2013-08-19T11:56:49.000+0000", "updated": "2014-11-06T07:06:14.000+0000", "description": "They print this:\n\n{code}\nlog4j:WARN No appenders could be found for logger (akka.event.slf4j.Slf4jEventHandler).\nlog4j:WARN Please initialize the log4j system properly.\nlog4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n{code}\n\nIt might have to do with not finding a log4j.properties file. I believe hadoop1 had one in its own JARs (or depended on an older log4j that came with a default)? but hadoop2 doesn't. We should probably have our own default one in conf.", "comments": ["This should be resolved/obsoleted by subsequent updates to SLF4J and log4j integration, and the props file."], "derived": {"summary": "They print this:\n\n{code}\nlog4j:WARN No appenders could be found for logger (akka. event.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "When built with Hadoop2, spark-shell and examples don't initialize log4j properly - They print this:\n\n{code}\nlog4j:WARN No appenders could be found for logger (akka. event."}, {"q": "What updates or decisions were made in the discussion?", "a": "This should be resolved/obsoleted by subsequent updates to SLF4J and log4j integration, and the props file."}]}}
{"project": "SPARK", "issue_id": "SPARK-881", "title": "Add documentation for new monitoring capabilities", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "labels": [], "created": "2013-08-20T16:46:03.000+0000", "updated": "2013-09-15T23:07:42.000+0000", "description": "Should include, e.g. the JSON protocol.", "comments": [], "derived": {"summary": "Should include, e. g.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Add documentation for new monitoring capabilities - Should include, e. g."}]}}
{"project": "SPARK", "issue_id": "SPARK-882", "title": "Have link for feedback/suggestions in docs", "status": "Resolved", "priority": "Minor", "reporter": "Patrick Wendell", "assignee": "Patrick McFadin", "labels": [], "created": "2013-08-21T00:40:50.000+0000", "updated": "2016-10-31T09:38:42.000+0000", "description": "It would be cool to have a link at the top of the docs for feedback/suggestions/errors. I bet we'd get a lot of interesting stuff from that and it could be a good way to crowdsource correctness checking, since a lot of us that write them never have to use them.\n\nSomething to the right of the main top nav might be good. [~andyk] [~matei] - what do you guys think?", "comments": ["Is the intended use here that users could submit corrections easily without having to open a JIRA/PR? I think that's a great idea; it lowers the barrier to providing feedback on a high visibility item like the docs.\n\nCouple of questions:\n\n1. Is integration with 3rd party tools like UserVoice or Disqus allowed? Actually, it might be really sweet if some simple, in-page feedback form automatically submitted a JIRA issue with the appropriate tags and info.\n\n2. I assume the docs proper are the priority, right? Do we want to do this for the main site as well?", "I think it should just prompt people to send to the user@ mailing list or if somehow possible, a link to open a minor doc JIRA. I would not want to add a third-party forum to monitor just for this, as it's likely to be a /dev/null. We already have an issue tracking and discussion system.", "I don't see any activity, so mind if I take a crack at this for the Spark documentation (link to open a pre-populated minor doc JIRA)?\n\ncc [~pwendell] [~srowen]\n", "On second thought I'd just link to the https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark wiki as I'd rather funnel any contributions through that first.", "It looks like the More menu in the docs (http://spark.apache.org/docs/2.0.1/) already contains a \"Contributing to Spark\" link which takes the user to https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark. Since the link already exists, perhaps this JIRA should be resolved and closed?", "You're right, I was thinking of the API docs rather than the general documentation. If there's an easy way to do that, OK, otherwise yeah this is effectively already done."], "derived": {"summary": "It would be cool to have a link at the top of the docs for feedback/suggestions/errors. I bet we'd get a lot of interesting stuff from that and it could be a good way to crowdsource correctness checking, since a lot of us that write them never have to use them.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Have link for feedback/suggestions in docs - It would be cool to have a link at the top of the docs for feedback/suggestions/errors. I bet we'd get a lot of interesting stuff from that and it could be a good way to crowdsource correctness checking, since a lot of us that write them never have to use them."}, {"q": "What updates or decisions were made in the discussion?", "a": "You're right, I was thinking of the API docs rather than the general documentation. If there's an easy way to do that, OK, otherwise yeah this is effectively already done."}]}}
{"project": "SPARK", "issue_id": "SPARK-883", "title": "Remove dependency on Scala json library", "status": "Resolved", "priority": "Minor", "reporter": "Reynold Xin", "assignee": "Reynold Xin", "labels": ["Starter"], "created": "2013-08-22T13:59:25.000+0000", "updated": "2013-10-17T18:41:24.000+0000", "description": "Scala is going to deprecate its own json library. We should remove the uses of scala.util.parsing.json in Spark and move to a different library.\n\nOne candidate choice is the lift-json library.", "comments": [], "derived": {"summary": "Scala is going to deprecate its own json library. We should remove the uses of scala.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Remove dependency on Scala json library - Scala is going to deprecate its own json library. We should remove the uses of scala."}]}}
{"project": "SPARK", "issue_id": "SPARK-1217", "title": "Add proximal gradient updater.", "status": "Resolved", "priority": "Major", "reporter": "Ameet Talwalkar", "assignee": null, "labels": [], "created": "2013-08-25T18:21:22.000+0000", "updated": "2014-04-07T17:46:33.000+0000", "description": "Add proximal gradient updater, in particular for L1 regularization.", "comments": ["The L1 updater is already proximal, as in the current code. Since it has no effect for L2, we could mark the issue as resolved for now."], "derived": {"summary": "Add proximal gradient updater, in particular for L1 regularization.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add proximal gradient updater. - Add proximal gradient updater, in particular for L1 regularization."}, {"q": "What updates or decisions were made in the discussion?", "a": "The L1 updater is already proximal, as in the current code. Since it has no effect for L2, we could mark the issue as resolved for now."}]}}
{"project": "SPARK", "issue_id": "SPARK-1214", "title": "0-1 labels ", "status": "Resolved", "priority": "Major", "reporter": "Ameet Talwalkar", "assignee": "Xiangrui Meng", "labels": [], "created": "2013-08-25T18:25:21.000+0000", "updated": "2014-04-07T17:45:23.000+0000", "description": "Use \\{0,1\\} labels for binary classification instead of {-1,1}. Advantages include:\n(+) Consistency across algorithms\n(+) Naturally extends to multi-class classification", "comments": ["Fixed in 0.9.0 or an earlier version."], "derived": {"summary": "Use \\{0,1\\} labels for binary classification instead of {-1,1}. Advantages include:\n(+) Consistency across algorithms\n(+) Naturally extends to multi-class classification.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "0-1 labels  - Use \\{0,1\\} labels for binary classification instead of {-1,1}. Advantages include:\n(+) Consistency across algorithms\n(+) Naturally extends to multi-class classification."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed in 0.9.0 or an earlier version."}]}}
{"project": "SPARK", "issue_id": "SPARK-884", "title": "unit test to validate various Spark json output", "status": "Resolved", "priority": "Major", "reporter": "Reynold Xin", "assignee": "Aaron Davidson", "labels": ["Starter"], "created": "2013-08-26T12:00:15.000+0000", "updated": "2013-09-06T15:30:35.000+0000", "description": "commit 5c7494d fixes this issue", "comments": ["See some notes here:\nhttps://github.com/mesos/spark/pull/864#issuecomment-23285662"], "derived": {"summary": "commit 5c7494d fixes this issue.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "unit test to validate various Spark json output - commit 5c7494d fixes this issue."}, {"q": "What updates or decisions were made in the discussion?", "a": "See some notes here:\nhttps://github.com/mesos/spark/pull/864#issuecomment-23285662"}]}}
{"project": "SPARK", "issue_id": "SPARK-885", "title": "PySpark shell should capture ctrl-c to prevent users from accidentally killing the Java gateway", "status": "Resolved", "priority": "Major", "reporter": "Josh Rosen", "assignee": "Josh Rosen", "labels": [], "created": "2013-08-27T10:33:37.000+0000", "updated": "2013-09-01T15:10:12.000+0000", "description": "If users hit 'ctrl-c' to discard a line of input (or interrupt a command) in the `pyspark` shell, the resulting KeyboardInterrupt kills the Py4J Java Gateway.  Maybe we could capture these interrupts to prevent this behavior.  We'd still want to allow users to interrupt regular Python commands and jump to new input lines, but we'd throw a warning when attempting to interrupt a running PySpark job.\n\nI think the interrupt may also be forwarded to the Py4J java gateway; we may need to do something else to suppress this behavior.", "comments": ["Fixed in https://github.com/mesos/spark/pull/870.  We may want to backport this to branch-0.7."], "derived": {"summary": "If users hit 'ctrl-c' to discard a line of input (or interrupt a command) in the `pyspark` shell, the resulting KeyboardInterrupt kills the Py4J Java Gateway. Maybe we could capture these interrupts to prevent this behavior.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "PySpark shell should capture ctrl-c to prevent users from accidentally killing the Java gateway - If users hit 'ctrl-c' to discard a line of input (or interrupt a command) in the `pyspark` shell, the resulting KeyboardInterrupt kills the Py4J Java Gateway. Maybe we could capture these interrupts to prevent this behavior."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed in https://github.com/mesos/spark/pull/870.  We may want to backport this to branch-0.7."}]}}
{"project": "SPARK", "issue_id": "SPARK-886", "title": "NullpointerExceptions in InputFormatInfo.computePreferredLocations/SparkHDFSLR ", "status": "Resolved", "priority": "Critical", "reporter": "Thomas Graves", "assignee": "Thomas Graves", "labels": [], "created": "2013-08-27T13:05:33.000+0000", "updated": "2013-11-04T09:52:10.000+0000", "description": "Since the changes from pr838 to change to hadoop agnostic builds, the SparkHdfsLR example throws a null pointer exception on Spark on Yarn (not sure if it fails on others). It also has the same issue if you call InputFormatInfo.computePreferredLocations before the SparkContext is created.  The reason is that both of those call SparkEnv.get.hadoop which hasn't been created yet if you haven't created the SparkContext.  \n\nThis is being used in the SparkHdfsLR example to pass the preferred locations into the SparkContext:\n\n   // This is used only by yarn for now, but should be relevant to other cluster types (mesos, etc) too.\n    // This is typically generated from InputFormatInfo.computePreferredLocations .. host, set of data-local splits on host\n    val preferredNodeLocationData: scala.collection.Map[String, scala.collection.Set[SplitInfo]] = scala.collection.immutable.Map()", "comments": ["Should we close this one now https://github.com/apache/incubator-spark/pull/124 is merged?", "Yes we can close it.  I don't seem to have permission to assign to myself or close.", "Thanks. I closed it. I will look into the permission issues ..."], "derived": {"summary": "Since the changes from pr838 to change to hadoop agnostic builds, the SparkHdfsLR example throws a null pointer exception on Spark on Yarn (not sure if it fails on others). It also has the same issue if you call InputFormatInfo.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "NullpointerExceptions in InputFormatInfo.computePreferredLocations/SparkHDFSLR  - Since the changes from pr838 to change to hadoop agnostic builds, the SparkHdfsLR example throws a null pointer exception on Spark on Yarn (not sure if it fails on others). It also has the same issue if you call InputFormatInfo."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks. I closed it. I will look into the permission issues ..."}]}}
{"project": "SPARK", "issue_id": "SPARK-887", "title": "mvn package doesn't include yarn in the repl-bin shaded jar", "status": "Resolved", "priority": "Major", "reporter": "Thomas Graves", "assignee": "Matei Alexandru Zaharia", "labels": [], "created": "2013-08-29T15:29:13.000+0000", "updated": "2013-09-03T12:33:10.000+0000", "description": "If you build with sbt (SPARK_WITH_YARN=true sbt/sbt package assembly) it will add the Yarn classes (YarnClientImpl) into the repl assembled jar ./repl/target/spark-repl-assembly-0.8.0-SNAPSHOT.jar , but if you build with mvn -Phadoop2-yarn package it doesn't add the Yarn class in the repl shaded jar (./repl-bin/target/spark-repl-bin-0.8.0-SNAPSHOT-shaded.jar)\n\nWe should keep these consistent.   This matters for https://github.com/mesos/spark/pull/868 as it is relying on the YarnClientImpl being in the jar.", "comments": ["I think this might be fixed by https://github.com/mesos/spark/pull/857", "Yup, it is.", "So it looks like the new assembly jar does not include the YarnClientImpl. The only way for me to run the example is to include the yarn jar in the export SPARK_CLASSPATH variable."], "derived": {"summary": "If you build with sbt (SPARK_WITH_YARN=true sbt/sbt package assembly) it will add the Yarn classes (YarnClientImpl) into the repl assembled jar. /repl/target/spark-repl-assembly-0.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "mvn package doesn't include yarn in the repl-bin shaded jar - If you build with sbt (SPARK_WITH_YARN=true sbt/sbt package assembly) it will add the Yarn classes (YarnClientImpl) into the repl assembled jar. /repl/target/spark-repl-assembly-0."}, {"q": "What updates or decisions were made in the discussion?", "a": "So it looks like the new assembly jar does not include the YarnClientImpl. The only way for me to run the example is to include the yarn jar in the export SPARK_CLASSPATH variable."}]}}
{"project": "SPARK", "issue_id": "SPARK-888", "title": "Killing jobs on standalone cluster", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "2013-08-29T15:47:01.000+0000", "updated": "2015-03-02T15:16:33.000+0000", "description": "Need an admin interface for this.", "comments": [], "derived": {"summary": "Need an admin interface for this.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Killing jobs on standalone cluster - Need an admin interface for this."}]}}
{"project": "SPARK", "issue_id": "SPARK-889", "title": "Bring back DFS broadcast", "status": "Resolved", "priority": "Minor", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "2013-08-30T10:48:29.000+0000", "updated": "2014-09-24T23:03:09.000+0000", "description": "DFS broadcast was a simple way to get better-than-single-master performance for broadcast, so we should add it back for people who have HDFS.", "comments": ["Hi Matei,\n\nSounds interesting.  I can take a look at this one.", "This is a really old JIRA and actually I wouldn't implement this now. HTTPBroadcast works as well as DFS did, and TorrentBroadcast is even better. We just need to make TorrentBroadcast the default -- would be great if you can spend some time testing that.", "[~matei] should we close ticket this as Won't Fix then, since effort is better spent making TorrentBroadcast better?", "In fact, I think [~rxin] has some JIRAs and PRs to make TorrentBroadcast _even_ better than it is now (it was greatly improved from 1.0.2 to 1.1.0), so it's probably safe to close this.", "Yea I think we should close this as won't fix for now."], "derived": {"summary": "DFS broadcast was a simple way to get better-than-single-master performance for broadcast, so we should add it back for people who have HDFS.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Bring back DFS broadcast - DFS broadcast was a simple way to get better-than-single-master performance for broadcast, so we should add it back for people who have HDFS."}, {"q": "What updates or decisions were made in the discussion?", "a": "Yea I think we should close this as won't fix for now."}]}}
{"project": "SPARK", "issue_id": "SPARK-890", "title": "Allow multiple parallel commands in spark-shell", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "2013-08-30T12:34:47.000+0000", "updated": "2014-10-21T07:42:05.000+0000", "description": "While a long command is running, it would be cool to still query the dataset from another window. This might be possible with a UI in front of spark-shell (there's no reason why the REPL environment can't be shared).", "comments": ["Spark context was made thread-safe a long time ago. "], "derived": {"summary": "While a long command is running, it would be cool to still query the dataset from another window. This might be possible with a UI in front of spark-shell (there's no reason why the REPL environment can't be shared).", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Allow multiple parallel commands in spark-shell - While a long command is running, it would be cool to still query the dataset from another window. This might be possible with a UI in front of spark-shell (there's no reason why the REPL environment can't be shared)."}, {"q": "What updates or decisions were made in the discussion?", "a": "Spark context was made thread-safe a long time ago."}]}}
{"project": "SPARK", "issue_id": "SPARK-891", "title": "Update Windows launch scripts to use assembly", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "2013-08-31T13:51:13.000+0000", "updated": "2014-05-19T00:24:04.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Update Windows launch scripts to use assembly"}]}}
{"project": "SPARK", "issue_id": "SPARK-892", "title": "Add docs page for fair scheduler", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "labels": [], "created": "2013-08-31T18:02:36.000+0000", "updated": "2013-09-09T16:18:13.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Add docs page for fair scheduler"}]}}
{"project": "SPARK", "issue_id": "SPARK-893", "title": "Create a docs page on monitoring and metrics", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Patrick McFadin", "labels": [], "created": "2013-08-31T23:12:12.000+0000", "updated": "2013-09-09T16:16:27.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Create a docs page on monitoring and metrics"}]}}
{"project": "SPARK", "issue_id": "SPARK-894", "title": "Not all WebUI fields delivered VIA JSON", "status": "Resolved", "priority": "Minor", "reporter": "David McCauley", "assignee": "David McCauley", "labels": [], "created": "2013-09-05T07:26:06.000+0000", "updated": "2013-09-16T08:10:50.000+0000", "description": "The following fields which are displayed in the Master WebUI for both the are not delivered via /json url.\n\n'state' within the Worker and Application list\n'duration' within the Application list", "comments": ["I added worker state in this PR: https://github.com/mesos/spark/pull/864/files", "Created pull request for this change:\nhttps://github.com/mesos/spark/pull/925"], "derived": {"summary": "The following fields which are displayed in the Master WebUI for both the are not delivered via /json url. 'state' within the Worker and Application list\n'duration' within the Application list.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Not all WebUI fields delivered VIA JSON - The following fields which are displayed in the Master WebUI for both the are not delivered via /json url. 'state' within the Worker and Application list\n'duration' within the Application list."}, {"q": "What updates or decisions were made in the discussion?", "a": "Created pull request for this change:\nhttps://github.com/mesos/spark/pull/925"}]}}
{"project": "SPARK", "issue_id": "SPARK-895", "title": "Add a Ganglia sink to Spark Metrics", "status": "Resolved", "priority": "Critical", "reporter": "Patrick Wendell", "assignee": "Patrick McFadin", "labels": [], "created": "2013-09-05T23:33:10.000+0000", "updated": "2014-03-30T04:14:04.000+0000", "description": null, "comments": ["Great idea. Here is the similar work for my test environment. see the comment in SPARK-788 and [github link| https://github.com/GraceH/spark/commit/c9717adfb97dacb97c34ad1fe94c8076843e5bf9]. \n\nMeanwhile, it would be better if we can improve the metric name to group the metrics according to their source or category. This really helps to present a better view in the new ganglia web page. ", "Hey [~Grace Huang], [~andrew xia],\n\nI'm so sorry that I missed this! I spent time today duplicating this effort and actually just merged our own Ganglia sink (it looks almost identical to the code that Grace wrote). I didn't see the comment on this JIRA until just now. [~Grace Huang] if you want to take a look at what I merged in today, it would be great to have a second person look at it and propose any changes necessary as a pull request. I noticed that you also had an example in the config file that I didn't have, it would be nice to get that merged as well.", "Hi Patrick, that is quite considerate to involve the app info which differentiates the metrics among different apps.  \n\nBTW, is there any further plan to improve the naming mechanism for those metrics, so that we can organize certain types of metrics under the same group-view? As you know, the Codahale-Metrics chooses those characters before the last dot from its whole metric name as the group name. If we can make use of this implementation, it will present those metrics in a better way for new ganglia web frontend. For example, it would be nice to gruop the metrics of executor.threadpool.{activeTask.count, completeTask.count, currentPool.size, maxPool.size} altogether, instead of letting them isolated into different groups.  Any comment?", "Oops. Since this issue has been closed already, we can discuss the naming mechanism in another place. Thanks.", "We can discuss here (we'll make a new JIRA if needed). Just so I understand, could you explain how the names would be different with the proposed change? For instance, how would the following be different:\n\n{code}\n<executor>.<appid>.threadpool.activeTasks.count\n{code}", "We can divide the metric name into two parts, one is before the last dot (which is the _GroupName_), and the other is after that last dot (which is the _CounterName_ without any dot mark ). \n\nFor example, we can name it as \n{noformat}\n<executor>.<appid>.threadpool.{activeTask_count | completeTask_count | currentPool_size | maxPool_size}\n{noformat}\nHere the _GroupName_ is \"*<executor>.<appid>.threadpool*\", and the rest parts are connected by \"\\_\" mark representing the _CounterName_, like \"*activeTasks_count*\". What do you think?", "Just wondering - is this exclusively for the benefit of Ganglia? Or are there other reasons for this as well...", "Yes. Ganglia has the concept of metrics group, which is not involved by other reporters. This feature helps to cluster several metrics under the same group view. Otherwise, it looks fragmented in the web page. \nAccording to my understanding, the current metrics name is formatted as\n{noformat}\nXXX.YYY.ZZZ.*.COUNTER.UNIT\n{noformat}\nI wonder if it is possible to concatenate the COUNTER and UNIT parts, since it is quite clear or readable for the user to identify the counter name at a glance, like\n{noformat}\nXXX.YYY.ZZZ.COUNTER_UNIT or XXX.YYY.ZZZ.CounterUnit\n{noformat}\nActually, the metrics of _FileSystemStat_ already demonstrates such kind of format to some extent. For example\n{noformat}\n*.filesystem.{hdfs|file}.{bytesRead| bytesWritten| readOps| largeReadOps| writeOps}\n{noformat}\nIt shows two groups for two different file systems respectively. And each group has 5 counters. Is it possible to unify the formats for all the metrics alike? Any comment?\n\n", "This is a good idea - I've created SPARK 900 and assigned it to you Clare, feel free to submit a PR when you have done some work.", "OK. I see. Thanks. Pls help to comment more after the PR is ready.\n\nHi Patrick, one more question on the JVM metrics source. Is it possible to differentiate the sources of JVM metrics according to their metric names? Currently, all the JVM metrics are started with the _sourceName_ of jvm. It is hard to distinguish which process the jvm metrics belong to. This is the common issue for all the Reporters. We may need the role or proc information here, just like what you have done in the ExecutorSource? For example \n{noformat}\n{master|worker}.jvm.* OR {driver|executor}.{executorID|app}.jvm.*. \n{noformat}\nIt is preferable to refine the _sourceName_ in the _JVMSource_ class. One proposal is to extract the information from the JVMs main ClassName and the applications arguments, or to simply obtain the current processID as the prefix name of _SourceName_? What do you think?\n", "BTW, there is one trivial point which is also ignored in my patch file. Since we have installed the latest Ganglia, I did not enable the Ganglia30 protocol. In case someone else need the previous Ganglia version to be compatible, we may choose \"*public GMetric(String group, int port, UDPAddressingMode mode, int ttl, {color:red}boolean ganglia311{color})*\"  to create the ganglia instance. And add one more option for either 311(as default value) or Ganglia30 protocol.  If it is unnecessary, just ignore it. Thanks."], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add a Ganglia sink to Spark Metrics"}, {"q": "What updates or decisions were made in the discussion?", "a": "BTW, there is one trivial point which is also ignored in my patch file. Since we have installed the latest Ganglia, I did not enable the Ganglia30 protocol. In case someone else need the previous Ganglia version to be compatible, we may choose \"*public GMetric(String group, int port, UDPAddressingMode mode, int ttl, {color:red}boolean ganglia311{color})*\"  to create the ganglia instance. And add one more option for either 311(as default value) or Ganglia30 protocol.  If it is unnecessary, just ignore it. Thanks."}]}}
{"project": "SPARK", "issue_id": "SPARK-896", "title": "ADD_JARS does not add all classes to classpath in the spark-shell for cluster on Mesos.", "status": "Resolved", "priority": "Major", "reporter": "Gary Malouf", "assignee": null, "labels": [], "created": "2013-09-09T11:18:27.000+0000", "updated": "2015-03-08T13:52:46.000+0000", "description": "I do not believe the issue is limited to scheduler/executors running on Mesos but added the information for debugging purposes.\n\nh3. Reproducing the issue:\n\n# Implement some custom functionalities and package them into a 'monster jar' with something like sbt assembly.\n\n# Drop this jar onto the Spark master box and specify the path to it in the ADD_JARS variable.\n\n# Start up the spark shell on same box as the master.  You should be able to import packages/classes specified in the jar without any compilation trouble.  \n\n# In a map function on an RDD, trying to call a class from within this jar (with fully qualified name) fails on a ClassNotFoundException.\n\nh3. Workaround\n\nMatei Zaharia suggested adding this jar to the SPARK_CLASSPATH environment variable - that resolved the issue.  My understanding however is that the functionality should work using solely the ADD_JARS variable - the documentation does not capture this.", "comments": ["I'm gonna call this WontFix as ADD_JARS has been deprecated for a while."], "derived": {"summary": "I do not believe the issue is limited to scheduler/executors running on Mesos but added the information for debugging purposes. h3.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "ADD_JARS does not add all classes to classpath in the spark-shell for cluster on Mesos. - I do not believe the issue is limited to scheduler/executors running on Mesos but added the information for debugging purposes. h3."}, {"q": "What updates or decisions were made in the discussion?", "a": "I'm gonna call this WontFix as ADD_JARS has been deprecated for a while."}]}}
{"project": "SPARK", "issue_id": "SPARK-897", "title": "Preemptively serialize closures to help users identify non-serializable errors early on", "status": "Resolved", "priority": "Major", "reporter": "Reynold Xin", "assignee": "William Benton", "labels": [], "created": "2013-09-09T20:34:24.000+0000", "updated": "2014-06-30T06:27:59.000+0000", "description": "We can do a preemptive serialization of the closures in map, mapParititons and all other RDD functions that take closures as arguments to help users catch non-serializable errors early on. Right now those exceptions are triggered when an action is triggered, which makes it harder to debug.\n\n", "comments": ["I'd be interested in taking this issue if no one is working on it yet.  Can someone assign it to me?"], "derived": {"summary": "We can do a preemptive serialization of the closures in map, mapParititons and all other RDD functions that take closures as arguments to help users catch non-serializable errors early on. Right now those exceptions are triggered when an action is triggered, which makes it harder to debug.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Preemptively serialize closures to help users identify non-serializable errors early on - We can do a preemptive serialization of the closures in map, mapParititons and all other RDD functions that take closures as arguments to help users catch non-serializable errors early on. Right now those exceptions are triggered when an action is triggered, which makes it harder to debug."}, {"q": "What updates or decisions were made in the discussion?", "a": "I'd be interested in taking this issue if no one is working on it yet.  Can someone assign it to me?"}]}}
{"project": "SPARK", "issue_id": "SPARK-898", "title": "Add jets3t dependency to Spark Build", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Matei Alexandru Zaharia", "labels": [], "created": "2013-09-09T23:10:06.000+0000", "updated": "2013-09-15T23:07:56.000+0000", "description": "This is necessary for s3 reads and writes to work correctly with some hadoop versions.", "comments": [], "derived": {"summary": "This is necessary for s3 reads and writes to work correctly with some hadoop versions.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add jets3t dependency to Spark Build - This is necessary for s3 reads and writes to work correctly with some hadoop versions."}]}}
{"project": "SPARK", "issue_id": "SPARK-899", "title": "Outdated Bagel documentation", "status": "Resolved", "priority": "Major", "reporter": "Matteo Ceccarello", "assignee": null, "labels": [], "created": "2013-09-10T09:04:37.000+0000", "updated": "2014-11-08T09:35:15.000+0000", "description": "The documentation for Bagel at http://spark.incubator.apache.org/docs/latest/bagel-programming-guide.html seems to be outdated.\n\nIn the code example it refers to an Edge class that does not exist in Bagel.", "comments": ["Trawling old issues again... I assume this is a \"WontFix\" because GraphX has superseded Bagel."], "derived": {"summary": "The documentation for Bagel at http://spark. incubator.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Outdated Bagel documentation - The documentation for Bagel at http://spark. incubator."}, {"q": "What updates or decisions were made in the discussion?", "a": "Trawling old issues again... I assume this is a \"WontFix\" because GraphX has superseded Bagel."}]}}
{"project": "SPARK", "issue_id": "SPARK-900", "title": "Use coarser grained naming for metrics", "status": "Resolved", "priority": "Minor", "reporter": "Patrick McFadin", "assignee": "Jie Huang", "labels": [], "created": "2013-09-10T10:11:23.000+0000", "updated": "2015-07-14T17:39:07.000+0000", "description": "See discussio in SPARK-895", "comments": ["see https://github.com/apache/incubator-spark/pull/22", "User 'rikima' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/7400"], "derived": {"summary": "See discussio in SPARK-895.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Use coarser grained naming for metrics - See discussio in SPARK-895."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'rikima' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/7400"}]}}
{"project": "SPARK", "issue_id": "SPARK-901", "title": "UISuite \"jetty port increases under contention\" fails if startPort is in use", "status": "Resolved", "priority": "Minor", "reporter": "Mark Hamstra", "assignee": null, "labels": [], "created": "2013-09-10T15:58:58.000+0000", "updated": "2014-09-17T22:17:07.000+0000", "description": "Recent change of startPort to 3030 conflicts with IANA assignment for arepa-cas.  If 3030 is already in use, the UISuite fails.", "comments": ["Ah ha!  It's actually Typesafe's Zinc server that is the other half of the conflict.  Zinc declares 3030 to be the default Nailgun port.", "the question that Mark raised on the JIRA is essentially about finding which number is greater out of the following two:\n - 'how many people out there are relying on well-known 3030 port'\nvs\n - 'how many Zinc server devs are using Spark'\n\n", "Doing some more googling, it's pretty hard to find out just what arepa-cas is, much less how many people are actually using it.  I think that it is far more likely that Spark developers trying to use Zinc will run into this conflict than do arepa-cas users.  Zinc is a pretty useful tool that more Scala developers should use, so we should at least warn them of the port conflict if we choose to leave the SparkUI.DEFAULT_PORT at 3030.\n\nIncidentally, shouldn't UISuite be using SparkUI.DEFAULT_PORT instead of 3030 so that it is obvious where this magic number is coming from and why it is important?", "bq. Incidentally, shouldn't UISuite be using SparkUI.DEFAULT_PORT instead of 3030 so that it is obvious where this magic number is coming from and why it is important?\n\nwon't it cause the port conflict when server and a worker run on the same node? ", "Leaving the current test as it is, still causes transient issues. Shouldn't we protect the test setup call server.start() ?\nEven though the chance of the port being used is low, it will be good to avoid the setup of a test being dependent on a fixed port.", "This is fixed by SPARK-3555 since we no longer chose a specific starting port."], "derived": {"summary": "Recent change of startPort to 3030 conflicts with IANA assignment for arepa-cas. If 3030 is already in use, the UISuite fails.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "UISuite \"jetty port increases under contention\" fails if startPort is in use - Recent change of startPort to 3030 conflicts with IANA assignment for arepa-cas. If 3030 is already in use, the UISuite fails."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is fixed by SPARK-3555 since we no longer chose a specific starting port."}]}}
{"project": "SPARK", "issue_id": "SPARK-902", "title": "java.lang.AbstractMethodError when using FlatMapFunction from Java", "status": "Resolved", "priority": "Major", "reporter": "Martin Weindel", "assignee": "Josh Rosen", "labels": [], "created": "2013-09-11T04:54:45.000+0000", "updated": "2013-10-22T16:33:12.000+0000", "description": "*Important Note*: a patch for _FlatMapFunction.scala_ fixing the problem is attached!\n\nDefining a Java function class based on org.apache.spark.api.java.function.FlatMapFunction compiles without problems, but on execution results in \n\n{quote}\njava.lang.AbstractMethodError: org.apache.spark.api.java.function.WrappedFunction1.call(Ljava/lang/Object;)Ljava/lang/Object;\n\tat org.apache.spark.api.java.function.WrappedFunction1.apply(WrappedFunction1.scala:31)\n{quote}\n\nI have investigated the problem in detail. Using the following small Java class, the issue can easily be reproduced.\n\n{code:java}\nimport java.lang.reflect.Method;\nimport java.util.Arrays;\n\nimport org.apache.spark.api.java.function.FlatMapFunction;\n\n\npublic class X {\n\tpublic static class MyFunction extends FlatMapFunction<String, Long> {\n\t\t@Override\n\t\tpublic Iterable<Long> call(String s) throws Exception {\n\t\t\treturn Arrays.asList(Long.parseLong(s));\n\t\t}\t\t\n\t}\n\t\n\tpublic static void main(String[] args) {\n\t\t//printMethods(WrappedFunction1.class);\n\t\tprintMethods(FlatMapFunction.class);\n\t\t\n\t\tMyFunction f = new MyFunction();\n\t\tfinal Iterable<Long> result = f.apply(\"1\");\n\t\tSystem.out.println(result);\n\t}\n\n\tprivate static void printMethods(Class<?> cls) {\n\t\tSystem.out.println(cls.getName() + \" --------------\");\n\t\tfor (Method m: cls.getDeclaredMethods()) {\n\t\t\tSystem.out.println(m);\n\t\t}\n\t\tfinal Class<?> superCls = cls.getSuperclass();\n\t\tif (superCls != null && superCls != Object.class) {\n\t\t\tprintMethods(cls.getSuperclass());\n\t\t}\t\t\n\t}\n\n}\n{code}\n\nWhen you run this code, you get following output:\n{quote}\norg.apache.spark.api.java.function.FlatMapFunction --------------\npublic abstract java.lang.Iterable org.apache.spark.api.java.function.FlatMapFunction.call(java.lang.Object) throws java.lang.Exception\npublic scala.reflect.ClassManifest org.apache.spark.api.java.function.FlatMapFunction.elementType()\norg.apache.spark.api.java.function.Function --------------\npublic scala.reflect.ClassManifest org.apache.spark.api.java.function.Function.returnType()\npublic abstract java.lang.Object org.apache.spark.api.java.function.Function.call(java.lang.Object) throws java.lang.Exception\norg.apache.spark.api.java.function.WrappedFunction1 --------------\npublic final java.lang.Object org.apache.spark.api.java.function.WrappedFunction1.apply(java.lang.Object)\npublic abstract java.lang.Object org.apache.spark.api.java.function.WrappedFunction1.call(java.lang.Object) throws java.lang.Exception\n...\nException in thread \"main\" java.lang.AbstractMethodError: org.apache.spark.api.java.function.WrappedFunction1.call(Ljava/lang/Object;)Ljava/lang/Object;\n\tat org.apache.spark.api.java.function.WrappedFunction1.apply(WrappedFunction1.scala:31)\n\tat X.main(X.java:21)\n{quote}\n\nSo the problem seems to be that the _call_ method is defined twice.\n# returning Object\n# returning Iterable\n\nSolution: just delete the definition of the _call_ method in _FlatMapFunction_ (see attached file)", "comments": ["I tried running your code example using both Maven and sbt, with both Spark 0.7.3 and 0.8.0-incubating, but I wasn't able to reproduce the AbstractMethodError.\n\nI'm running this Java version on OSX:\n\n{code}\njava version \"1.7.0_21\"\nJava(TM) SE Runtime Environment (build 1.7.0_21-b12)\nJava HotSpot(TM) 64-Bit Server VM (build 23.21-b01, mixed mode)\n{code}\n\nHow are you compiling and running this example code?", "The problem only occurs with the Eclipse compiler!\n\nIf I'm using javac, the sample runs without errors. I never expected that the Eclipse compiler is the cause.\nSo this problem happens if someone builds a JAR package in Eclipse and then deploys it to Spark. In fact, this is my use case.\n\nTo reproduce the problem, either compile the sample code in Eclipse or by running the following command line after downloading the Eclipse compiler from e.g. http://repo1.maven.org/maven2/org/eclipse/tycho/org.eclipse.jdt.core/3.9.0.v20130604-1421/org.eclipse.jdt.core-3.9.0.v20130604-1421.jar:\n\njava -jar org.eclipse.jdt.core-3.9.0.v20130604-1421.jar -cp spark-assembly_2.9.3-0.8.0-incubating-hadoop2.0.0-mr1-cdh4.2.0.jar -source 1.7 -target 1.7 Main.java\n\n\n", "Yep, it looks like the Eclipse compiler is the culprit.  I was able to reproduce this bug by compiling my code through Maven, using the Eclipse compiler plugin.  For my own future reference, here's the POM that I used:\n\n{code}\n<project>\n  <groupId>edu.berkeley</groupId>\n  <artifactId>simple-project</artifactId>\n  <modelVersion>4.0.0</modelVersion>\n  <name>Simple Project</name>\n  <packaging>jar</packaging>\n  <version>1.0</version>\n  <repositories> <!-- Repositories that Spark needs -->\n    <repository>\n      <id>Spray Repository</id>\n      <url>http://repo.spray.cc</url>\n    </repository>\n    <repository>\n      <id>Akka Repository</id>\n      <url>http://repo.akka.io/releases</url>\n    </repository>\n  </repositories>\n  <dependencies>\n    <dependency> <!-- Spark dependency -->\n      <groupId>org.apache.spark</groupId>\n      <artifactId>spark-core_2.9.3</artifactId>\n      <version>0.8.0-incubating</version>\n    </dependency>\n  </dependencies>\n  <build>\n    <plugins>\n      <plugin>\n     <groupId>org.apache.maven.plugins</groupId>\n     <artifactId>maven-compiler-plugin</artifactId>\n     <version>3.1</version>\n     <configuration>\n        <compilerId>eclipse</compilerId>\n        <source>1.7</source>\n        <target>1.7</target>\n     </configuration>\n     <dependencies>\n        <dependency>\n           <groupId>org.codehaus.plexus</groupId>\n           <artifactId>plexus-compiler-eclipse</artifactId>\n           <version>2.3</version>\n        </dependency>\n     </dependencies>\n      </plugin>\n    </plugins>\n  </build>\n</project>\n{code}\n\nI'm going to take a a closer look at the fix in your pull request (https://github.com/apache/incubator-spark/pull/30) to see whether we need to apply it anywhere else, or whether the change breaks the ability to throw exceptions from JavaFunctions. ", "Fixed by https://github.com/apache/incubator-spark/pull/100"], "derived": {"summary": "*Important Note*: a patch for _FlatMapFunction. scala_ fixing the problem is attached!\n\nDefining a Java function class based on org.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "java.lang.AbstractMethodError when using FlatMapFunction from Java - *Important Note*: a patch for _FlatMapFunction. scala_ fixing the problem is attached!\n\nDefining a Java function class based on org."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed by https://github.com/apache/incubator-spark/pull/100"}]}}
{"project": "SPARK", "issue_id": "SPARK-903", "title": "Inconsistent spark assembly", "status": "Resolved", "priority": "Minor", "reporter": "Martin Weindel", "assignee": "Patrick McFadin", "labels": [], "created": "2013-09-11T05:04:16.000+0000", "updated": "2013-09-15T14:39:31.000+0000", "description": "I have two problems when using the spark assembly (build via sbt\\sbt assembly)\n\n1. HDFS file system is not available\n{quote}\njava.io.IOException: No FileSystem for scheme: hdfs\n{quote}\nThis seems to be caused by overriding the file\n{{META-INF/services/org.apache.hadoop.fs.FileSystem}}\n\nMerging the original files manually resolves this problem:\n- hadoop-hdfs-2.0.0-cdh4.3.0.jar and \n- hadoop-common-2.0.0-cdh4.3.0.jar\n\n\n2. Illegal jar file on windows\nThe assembly jar contains \n- a directory META-INF/license\n- a file META-INF/LICENSE\nAs Windows file system is case-insensitive, this can cause trouble.", "comments": ["HDFS file system problem has already been fixed by Patrick Wendell (commit 0c1985b153a2dc2c891ae61c1ee67506926384ae)"], "derived": {"summary": "I have two problems when using the spark assembly (build via sbt\\sbt assembly)\n\n1. HDFS file system is not available\n{quote}\njava.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Inconsistent spark assembly - I have two problems when using the spark assembly (build via sbt\\sbt assembly)\n\n1. HDFS file system is not available\n{quote}\njava."}, {"q": "What updates or decisions were made in the discussion?", "a": "HDFS file system problem has already been fixed by Patrick Wendell (commit 0c1985b153a2dc2c891ae61c1ee67506926384ae)"}]}}
{"project": "SPARK", "issue_id": "SPARK-904", "title": "Not able to Start/Stop Spark Worker from Remote Machine", "status": "Closed", "priority": "Major", "reporter": "Ayush", "assignee": null, "labels": [], "created": "2013-09-11T05:06:44.000+0000", "updated": "2014-11-14T09:59:30.000+0000", "description": "I have two machines A and B. I am trying to run Spark Master on machine A and Spark Worker on machine B. \nI have set machine B'host name in conf/slaves in my Spark directory. \n\n\nWhen I am executing start-all.sh to start master and workers, I am getting below message on console:\n\nabc@abc-vostro:~/spark-scala-2.10$ sudo sh bin/start-all.sh \nsudo: /etc/sudoers.d is world writable\nstarting spark.deploy.master.Master, logging to /home/abc/spark-scala-2.10/bin/../logs/spark-root-spark.deploy.master.Master-1-abc-vostro.out\n13/09/11 14:54:29 WARN spark.Utils: Your hostname, abc-vostro resolves to a loopback address: 127.0.1.1; using 1XY.1XY.Y.Y instead (on interface wlan2)\n13/09/11 14:54:29 WARN spark.Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nMaster IP: abc-vostro\ncd /home/abc/spark-scala-2.10/bin/.. ; /home/abc/spark-scala-2.10/bin/start-slave.sh 1 spark://abc-vostro:7077\nxyz@1XX.1XX.X.X's password: \nxyz@1XX.1XX.X.X: bash: line 0: cd: /home/abc/spark-scala-2.10/bin/..: No such file or directory\nxyz@1XX.1XX.X.X: bash: /home/abc/spark-scala-2.10/bin/start-slave.sh: No such file or directory\n\n\nMaster is started but worker is failed to start. \n\nI have set xyz@1XX.1XX.X.X in conf/slaves in my Spark directory. \n\nCan anyone help me to resolve this? This is probably something I'm missing any configuration on my end. \n\nHowever When I create Spark Master and Worker on same machine, It is working fine. ", "comments": ["[~ayushmishra2005] I suspect you don't have Spark installed on the remote machine -- the {{start-all.sh}} script won't install it for you on remote machines.\n\nIf you're still having trouble, please reach out to the spark users list from http://spark.apache.org/community.html which is a better place for these kinds of requests anyway.  I'm closing this issue for now but let me know here if you aren't able to get a resolution on the mailing lists.\n\nThanks, and good luck with Spark!\nAndrew"], "derived": {"summary": "I have two machines A and B. I am trying to run Spark Master on machine A and Spark Worker on machine B.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Not able to Start/Stop Spark Worker from Remote Machine - I have two machines A and B. I am trying to run Spark Master on machine A and Spark Worker on machine B."}, {"q": "What updates or decisions were made in the discussion?", "a": "[~ayushmishra2005] I suspect you don't have Spark installed on the remote machine -- the {{start-all.sh}} script won't install it for you on remote machines.\n\nIf you're still having trouble, please reach out to the spark users list from http://spark.apache.org/community.html which is a better place for these kinds of requests anyway.  I'm closing this issue for now but let me know here if you aren't able to get a resolution on the mailing lists.\n\nThanks, and good luck with Spark!\nAndrew"}]}}
{"project": "SPARK", "issue_id": "SPARK-905", "title": "Not able to run Job on  remote machine", "status": "Resolved", "priority": "Major", "reporter": "Ayush", "assignee": null, "labels": [], "created": "2013-09-11T05:08:42.000+0000", "updated": "2014-11-08T09:54:58.000+0000", "description": "I have two machines A and B.  On machine A, I run\n ./run spark.deploy.master.Master \nto start Master.\n\nMaster URL is spark://abc-vostro.local:7077. \n\nNow on machine B, I run \n\n./run spark.deploy.worker.Worker spark://abc-vostro.local:7077\n\nNow worker has been registered to  master. \n\nNow I want to run a simple job on cluster. \n\nHere is SimpleJob.scala\n\npackage spark.examples\n \nimport spark.SparkContext\nimport SparkContext._\n \nobject SimpleJob {\n  def main(args: Array[String]) {\n    val logFile = \"s3n://<AWS_ACCESS_KEY_ID>:<AWS_SECRET_ACCESS_KEY>/<File Name>\"\n    val sc = new SparkContext(\"spark://abc-vostro.local:7077\", \"Simple Job\",\n      System.getenv(\"SPARK_HOME\"), Seq(\"/home/abc/spark-scala-2.10/examples/target/scala-2.10/spark-examples_2.10-0.8.0-SNAPSHOT.jar\"))\n    val logData = sc.textFile(logFile)\n    val numsa = logData.filter(line => line.contains(\"a\")).count\n    val numsb = logData.filter(line => line.contains(\"b\")).count\n    println(\"total a : %s, total b : %s\".format(numsa, numsb))\n  }\n}  \n\n\nThis file is located at \"/home/abc/spark-scala-2.10/examples/src/main/scala/spark/examples\" on machine A.\n\n\nNow on machine A, I run sbt/sbt package.\n\nWhen I run\n MASTER=spark://abc-vostro.local:7077 ./run spark.examples.SimpleJob\n\nto run my job, I am getting below exception on both machines A and B,\n\n(class java.io.IOException: Cannot run program \"/home/abc/spark-scala-2.10/bin/compute-classpath.sh\" (in directory \".\"): error=2, No such file or directory)\n\n\nCould you please help me to resolve this? This is probably something I'm missing any configuration on my end. \n\n\nThanks in advance.\n", "comments": ["Ayush, did you manage to figure out how to fix this?", "Not yet.", "This looks like something that was either long since fixed, or just a matter of not having the Spark installation set up on each machine. The compute-classpath.sh script does exist in bin/ in the tree and distro."], "derived": {"summary": "I have two machines A and B. On machine A, I run.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Not able to run Job on  remote machine - I have two machines A and B. On machine A, I run."}, {"q": "What updates or decisions were made in the discussion?", "a": "This looks like something that was either long since fixed, or just a matter of not having the Spark installation set up on each machine. The compute-classpath.sh script does exist in bin/ in the tree and distro."}]}}
{"project": "SPARK", "issue_id": "SPARK-906", "title": "How to recover Spark Master in case of machine failure, where Spark Master was running", "status": "Resolved", "priority": "Major", "reporter": "Ayush", "assignee": "Aaron Davidson", "labels": [], "created": "2013-09-11T05:10:29.000+0000", "updated": "2013-11-14T18:15:34.000+0000", "description": "I have one Spark master on machine A and two Spark workers on another machines B and C. \nIf machine A is failed for any reason, Spark master would die in this case.\n\nIs there any way to recover Spark Master or to create a new Spark Master on another machine automatically?\n\nCan anyone help me to resolve this?\n\nThanks in advance.\n", "comments": ["In Spark 0.8.1 and onward, there are two solutions for this (one uses ZooKeeper for full featured fault-tolerance, and the other uses the file system to be able to recover on the same machine). 0.8.1 is not yet released, but if you're interested in reading more about this, you can check out the unpublished docs at https://github.com/apache/incubator-spark/blob/master/docs/spark-standalone.md.\n\n(Even further down the pipeline, we hope to remove the Master entirely...)"], "derived": {"summary": "I have one Spark master on machine A and two Spark workers on another machines B and C. If machine A is failed for any reason, Spark master would die in this case.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "How to recover Spark Master in case of machine failure, where Spark Master was running - I have one Spark master on machine A and two Spark workers on another machines B and C. If machine A is failed for any reason, Spark master would die in this case."}, {"q": "What updates or decisions were made in the discussion?", "a": "In Spark 0.8.1 and onward, there are two solutions for this (one uses ZooKeeper for full featured fault-tolerance, and the other uses the file system to be able to recover on the same machine). 0.8.1 is not yet released, but if you're interested in reading more about this, you can check out the unpublished docs at https://github.com/apache/incubator-spark/blob/master/docs/spark-standalone.md.\n\n(Even further down the pipeline, we hope to remove the Master entirely...)"}]}}
{"project": "SPARK", "issue_id": "SPARK-907", "title": "Add JSON endpoints to SparkUI", "status": "Closed", "priority": "Minor", "reporter": "David McCauley", "assignee": "David McCauley", "labels": [], "created": "2013-09-11T07:58:16.000+0000", "updated": "2014-11-25T17:00:17.000+0000", "description": "At the moment there does not appear to be any JSON endpoints for the main  SparkUI like there is in place for the standalone Master and Worker web UIs.\n\nAdding these endpoints will allow for easier custom UI development.", "comments": ["[~dmccauley] this report looks like a duplicate of SPARK-3644, not just related.  Do you feel like this ticket is encapsulated in the design work for a Spark REST API happening there?\n\nIf so I think we can close this one as a duplicate for the other, more fleshed-out ticket.\n\nThanks!\nAndrew", "[~aash] yes, SPARK-3644 seems to be a much more comprehensive description of the work needed for the task. I'll close this ticket out.\n\nThanks,\nDave"], "derived": {"summary": "At the moment there does not appear to be any JSON endpoints for the main  SparkUI like there is in place for the standalone Master and Worker web UIs. Adding these endpoints will allow for easier custom UI development.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add JSON endpoints to SparkUI - At the moment there does not appear to be any JSON endpoints for the main  SparkUI like there is in place for the standalone Master and Worker web UIs. Adding these endpoints will allow for easier custom UI development."}, {"q": "What updates or decisions were made in the discussion?", "a": "[~aash] yes, SPARK-3644 seems to be a much more comprehensive description of the work needed for the task. I'll close this ticket out.\n\nThanks,\nDave"}]}}
{"project": "SPARK", "issue_id": "SPARK-908", "title": "local metrics test has race condition due to new SparkListener architecture", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Kay Ousterhout", "labels": [], "created": "2013-09-15T14:41:36.000+0000", "updated": "2013-11-25T14:44:04.000+0000", "description": "This test assumes that the listener will synchronously process an event once an action is called. Kay's changes made this asynchronous, so this test can now fail sometimes depending on the order in which events occur.\n\nhttps://github.com/mesos/spark/blob/master/core/src/test/scala/org/apache/spark/scheduler/SparkListenerSuite.scala", "comments": [], "derived": {"summary": "This test assumes that the listener will synchronously process an event once an action is called. Kay's changes made this asynchronous, so this test can now fail sometimes depending on the order in which events occur.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "local metrics test has race condition due to new SparkListener architecture - This test assumes that the listener will synchronously process an event once an action is called. Kay's changes made this asynchronous, so this test can now fail sometimes depending on the order in which events occur."}]}}
{"project": "SPARK", "issue_id": "SPARK-909", "title": "add task serialization footprint (time and size) into TaskMetrics", "status": "Resolved", "priority": "Minor", "reporter": "Colorsmart", "assignee": null, "labels": [], "created": "2013-09-16T06:36:24.000+0000", "updated": "2014-10-21T07:50:37.000+0000", "description": "currently, task serialization taken time and size are only saved as log. To better using these information to tune the spark programming. I would suggest put them in TaskMetrics so that it could be accessed by adding a custom spark listener.", "comments": ["Do you know what the overhead might be with this turned on? If we are going to add this, we need to make sure the overhead is low because this is on the critical path. ", "I could do some tests on that, but I think it won't be bigger than the deserialization part.", "Thanks that would be great. I think it would be fine as long as it brings minimal overhead.", "Hi , I read revlevant code , and I wonder how we put task serialization time and size into TaskMetrics\nTask serialization is done in TaskSetManager.resourceOffer before it is passed as a ByteBuffer to TaskDescription, and finally to launchTask as a message to actor of ExectorBackEnd. And this ByteBuffer is finally fetched and deserialized in Executor's  side ,in TaskRunner.\n One method I can think to put these footprint into TaskMetrics is that we do task serialization twice . First time we get a serializedTask ByteBuffer and serialization takenTime. And we put them into task.metrics , and then we serialize this task local var with task serizlization footprint in its merics again  and get a new serializedTask ByteBuffer, whers its metrics member have serializtion footprint. But it may bring overhead because we serialize the task object twice, and the footprint in task.metrics is not that percise because first time we serialize it , task.metrics is empty, so serialized size may be smaller.\nAnyone has a better idea, I'd like to imporve this and make a PR", "This has already been fixed."], "derived": {"summary": "currently, task serialization taken time and size are only saved as log. To better using these information to tune the spark programming.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "add task serialization footprint (time and size) into TaskMetrics - currently, task serialization taken time and size are only saved as log. To better using these information to tune the spark programming."}, {"q": "What updates or decisions were made in the discussion?", "a": "This has already been fixed."}]}}
{"project": "SPARK", "issue_id": "SPARK-910", "title": "hadoopFile creates RecordReader key and value at the wrong scope", "status": "Resolved", "priority": "Major", "reporter": "aaron babcock", "assignee": null, "labels": [], "created": "2013-09-16T21:36:26.000+0000", "updated": "2014-11-25T09:50:52.000+0000", "description": "I'm not a scala or hadoop expert so forgive me if I'm in the wrong but it seems to me that SparkContext.hadoopFile is broken.\n\nhf = sc.hadoopFile(\"hdfs://namenod.local/something.xml\", XmlInputFormat.class,  LongWritable.class, Text.class);\nhf.take(5);\n\nproduces the same record over and over instead of iterating.\n\n\nHere is a pull request for a proposed fix: \n\nhttps://github.com/mesos/spark/pull/934\n\n\n\n", "comments": ["I suspect this problem is similar related to SPARK-1018.  The problem isn't the input format (in my case I used TextInputFormat), which correctly reads the file, but with the take method, which isn't correctly handling the Pair(LongWritable,Text) being output.", "Given the PR discussion, it looks like this was resolved as NotAProblem. Either the InputFormat has to create new key/value objects, or the caller in Spark does."], "derived": {"summary": "I'm not a scala or hadoop expert so forgive me if I'm in the wrong but it seems to me that SparkContext. hadoopFile is broken.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "hadoopFile creates RecordReader key and value at the wrong scope - I'm not a scala or hadoop expert so forgive me if I'm in the wrong but it seems to me that SparkContext. hadoopFile is broken."}, {"q": "What updates or decisions were made in the discussion?", "a": "Given the PR discussion, it looks like this was resolved as NotAProblem. Either the InputFormat has to create new key/value objects, or the caller in Spark does."}]}}
{"project": "SPARK", "issue_id": "SPARK-911", "title": "Support map pruning on sorted (K, V) RDD's", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Aaron", "labels": [], "created": "2013-09-17T13:01:43.000+0000", "updated": "2015-04-25T21:58:57.000+0000", "description": "If someone has sorted a (K, V) rdd, we should offer them a way to filter a range of the partitions that employs map pruning. This would be simple using a small range index within the rdd itself. A good example is I sort my dataset by time and then I want to serve queries that are restricted to a certain time range.", "comments": ["I was just wondering about this when answering http://stackoverflow.com/questions/24677180/how-do-i-select-a-range-of-elements-in-spark-rdd, I noticed that RangePartitioner stores an array of upper bounds, but I was unsure from the docs if that meant that the each partition had no overlap in range. If they don't this seems like it would be a pretty easy feature to implement", "I decided to take a look at this myself (disclaimer haven't worked on sparks source code before). A couple things I noticed were:  \n1. since compute is called per split for all RDD's, the best I could figure out using that approach is to just return an empty iterator, not sure if that creates a weird partition situation\n2. you could more efficiently check the partitions that could contain correct numbers, but with only an iterator you have to linearly search", "pull request https://github.com/apache/spark/pull/1381/files", "User 'aaronjosephs' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1381", "Issue resolved by pull request 1381\n[https://github.com/apache/spark/pull/1381]"], "derived": {"summary": "If someone has sorted a (K, V) rdd, we should offer them a way to filter a range of the partitions that employs map pruning. This would be simple using a small range index within the rdd itself.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support map pruning on sorted (K, V) RDD's - If someone has sorted a (K, V) rdd, we should offer them a way to filter a range of the partitions that employs map pruning. This would be simple using a small range index within the rdd itself."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 1381\n[https://github.com/apache/spark/pull/1381]"}]}}
{"project": "SPARK", "issue_id": "SPARK-912", "title": "Take stage breakdown functionality and runLocally out of the main event loop in DAGScheduler", "status": "Resolved", "priority": "Major", "reporter": "Reynold Xin", "assignee": null, "labels": [], "created": "2013-09-18T04:09:23.000+0000", "updated": "2020-05-17T17:47:56.000+0000", "description": "This can reduce the complexity of the main event loop and improve performance (since the main event loop is single threaded).\n\nWe can also take the result task deserialization code out of the main loop (maybe Kay is already working on this?).", "comments": ["It looks like the task result deserialization code was moved out of the main loop in SPARK-7655.  It also looks like the {{runLocally}} was moved into a separate thread a long time ago (pre-1.0?).  Given this, I'm going to resolve this as fixed; I'm not quite sure what the \"stage breakdown\" in the title is referring to but I think that code has been significantly optimized since 0.8, so it's probably no longer relevant."], "derived": {"summary": "This can reduce the complexity of the main event loop and improve performance (since the main event loop is single threaded). We can also take the result task deserialization code out of the main loop (maybe Kay is already working on this?).", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Take stage breakdown functionality and runLocally out of the main event loop in DAGScheduler - This can reduce the complexity of the main event loop and improve performance (since the main event loop is single threaded). We can also take the result task deserialization code out of the main loop (maybe Kay is already working on this?)."}, {"q": "What updates or decisions were made in the discussion?", "a": "It looks like the task result deserialization code was moved out of the main loop in SPARK-7655.  It also looks like the {{runLocally}} was moved into a separate thread a long time ago (pre-1.0?).  Given this, I'm going to resolve this as fixed; I'm not quite sure what the \"stage breakdown\" in the title is referring to but I think that code has been significantly optimized since 0.8, so it's probably no longer relevant."}]}}
{"project": "SPARK", "issue_id": "SPARK-913", "title": "log the size of each shuffle block in block manager", "status": "Resolved", "priority": "Minor", "reporter": "Reynold Xin", "assignee": null, "labels": [], "created": "2013-09-20T14:52:33.000+0000", "updated": "2020-05-17T18:21:33.000+0000", "description": null, "comments": ["User 'devaraj-kavali' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/11819"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "log the size of each shuffle block in block manager"}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'devaraj-kavali' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/11819"}]}}
{"project": "SPARK", "issue_id": "SPARK-914", "title": "Make RDD implement Scala and Java Iterable interfaces", "status": "Resolved", "priority": "Minor", "reporter": "Paul Snively", "assignee": null, "labels": [], "created": "2013-09-21T12:10:40.000+0000", "updated": "2014-11-06T07:06:49.000+0000", "description": "It would benefit developers if RDD implemented the Java and Scala Iterable interfaces so RDDs could be used in contexts requiring those types. A good source of inspiration might be Kafka's KafkaStream, which implements both interfaces.", "comments": ["https://github.com/apache/spark/pull/156", "Please close this ticket -- it is a duplicate of SPARK-1259 which was completed for 1.0"], "derived": {"summary": "It would benefit developers if RDD implemented the Java and Scala Iterable interfaces so RDDs could be used in contexts requiring those types. A good source of inspiration might be Kafka's KafkaStream, which implements both interfaces.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Make RDD implement Scala and Java Iterable interfaces - It would benefit developers if RDD implemented the Java and Scala Iterable interfaces so RDDs could be used in contexts requiring those types. A good source of inspiration might be Kafka's KafkaStream, which implements both interfaces."}, {"q": "What updates or decisions were made in the discussion?", "a": "Please close this ticket -- it is a duplicate of SPARK-1259 which was completed for 1.0"}]}}
{"project": "SPARK", "issue_id": "SPARK-915", "title": "Tidy up the scripts", "status": "Resolved", "priority": "Major", "reporter": "Shane Huang", "assignee": null, "labels": [], "created": "2013-09-21T21:05:40.000+0000", "updated": "2014-07-25T23:27:09.000+0000", "description": "Having worked with a few customers, we found the current organization of scripts looked a bit confusing and inconvenient. \n\nFor most of our customer, the application developers/users and platform administrators belong to two teams. So it's more convenient to separate the scripts used by administrators and application users. \n\nSpecifically, we'd propose to employ a methodology similar to hadoop's. \n1, make an \"sbin\" folder containing all the scripts for administrators, specifically,\n1) all service administration scripts, i.e. start-*, stop-*, slaves.sh, *-daemons, *-daemon scripts\n2) low-level or internally used utility scripts, i.e. compute-classpath, spark-config, spark-class, spark-executor\n2. make a \"bin\" folder containing all the scripts for application developers/users, specifically,\n1) user level app  running scripts, i.e. pyspark, spark-shell, and we propose to add a script \"spark\" for users to run applications (very much like spark-class but may add some more control or convenient utilities)\nscripts for status checking, e.g. spark and hadoop version checking, \n2) running applications checking, etc. We can make this a separate script or add functionality to \"spark\" script.     \n3. No wandering scripts outside the sbin and bin folders\n", "comments": ["The link to the original discussion on mailing list is http://www.mail-archive.com/dev@spark.incubator.apache.org/msg00446.html", "A pull request was submitted for separating admin and user scripts. https://github.com/apache/incubator-spark/pull/21"], "derived": {"summary": "Having worked with a few customers, we found the current organization of scripts looked a bit confusing and inconvenient. For most of our customer, the application developers/users and platform administrators belong to two teams.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Tidy up the scripts - Having worked with a few customers, we found the current organization of scripts looked a bit confusing and inconvenient. For most of our customer, the application developers/users and platform administrators belong to two teams."}, {"q": "What updates or decisions were made in the discussion?", "a": "A pull request was submitted for separating admin and user scripts. https://github.com/apache/incubator-spark/pull/21"}]}}
{"project": "SPARK", "issue_id": "SPARK-916", "title": "Better Support for Flat/Tabular RDD's", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": null, "labels": [], "created": "2013-09-23T12:43:49.000+0000", "updated": "2014-10-21T07:45:54.000+0000", "description": "Many people use Spark to run analysis on flat datasets, where the RDD is composed records with a single set of non-nested fields. We could have better support for this use case in a variety of areas. Two of which are:\n\n1. Allowing people to name individual fields and access them by name, rather than using tuple indices (see Scalding[1]). This avoids the mess that is {x => (x._3(), x._4())}\n\n2. Support columnar in-memory storage.\n\nThis is just an umbrella/brainstorming JIRA to see if other people have thoughts about this. Curious to hear feedback.\n\n[1] https://dev.twitter.com/blog/scalding", "comments": ["Just kibitzing, but you can kinda/sorta solve this by have per-table case classes, e.g.\n\ncase class FooTableLine(line: String) {\n  def columnA() = ...\n  def columnB() = ...\n}\n\nSo, then with an RDD[FooTableLine], stuff like rdd.filter { _.columnA == \"whatever\" } is using the nice aliases.\n\nSo, fine, this can be solved with user convention.\n\nThe hard part is as soon as soon as you change the shape, e.g.:\n\nval rdd1: RDD[FooTableLine] = ...\nval rdd2 = rdd.map { l => (l.columnA, l.columnB) }\n\nNow you've lost your case class goodness and are stuck with a tuple.\n\nSo, right, I'm sure this is all obvious/review, but just making sure I'm on the same page.\n\nPersonally, I think the only way to get non-tuples is with C#-style anonymous types, where the compiler can basically synthesize (l.columnA, l.columnB) into a new unnamed type that has \"columnA\" and \"columnB\" fields.\n\nWith scala, this would require compiler/macro magic. Is that where you're thinking of heading, Patrick? I.e. is that the nut you're trying to crack?\n\nFWIW I think Scalding's approach of using strings to keys is a cop-out, as you lose type-safety, which is one of the points of using Scala in the first place.\n\n(Disclaimer: I'm a macro/compiler/etc. newbie, so will defer to others to correct any wrong assumptions I have. This is just my understanding.)", "Turned out [~marmbrus] did all of this and more in SparkSQL (which btw also works for nested types). So I'm gonna close this very old issue."], "derived": {"summary": "Many people use Spark to run analysis on flat datasets, where the RDD is composed records with a single set of non-nested fields. We could have better support for this use case in a variety of areas.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Better Support for Flat/Tabular RDD's - Many people use Spark to run analysis on flat datasets, where the RDD is composed records with a single set of non-nested fields. We could have better support for this use case in a variety of areas."}, {"q": "What updates or decisions were made in the discussion?", "a": "Turned out [~marmbrus] did all of this and more in SparkSQL (which btw also works for nested types). So I'm gonna close this very old issue."}]}}
{"project": "SPARK", "issue_id": "SPARK-917", "title": "API docs for Spark/MLLib/Streaming should point to package URL", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "labels": ["Starter"], "created": "2013-09-24T19:12:49.000+0000", "updated": "2014-03-17T10:22:27.000+0000", "description": "Now they point to a base page from which you need to click \"org->apache->spark\" etc.", "comments": ["This was fixed a while back"], "derived": {"summary": "Now they point to a base page from which you need to click \"org->apache->spark\" etc.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "API docs for Spark/MLLib/Streaming should point to package URL - Now they point to a base page from which you need to click \"org->apache->spark\" etc."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was fixed a while back"}]}}
{"project": "SPARK", "issue_id": "SPARK-918", "title": "hadoop-client dependency should be explained for Scala in addition to Java in quickstart", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": null, "labels": ["starter"], "created": "2013-09-24T20:50:41.000+0000", "updated": "2014-04-21T17:56:16.000+0000", "description": null, "comments": ["This was fixed as a result of a separate re-factoring of the docs."], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "hadoop-client dependency should be explained for Scala in addition to Java in quickstart"}, {"q": "What updates or decisions were made in the discussion?", "a": "This was fixed as a result of a separate re-factoring of the docs."}]}}
{"project": "SPARK", "issue_id": "SPARK-919", "title": "spark-ec2 launch --resume doesn't re-initialize all modules", "status": "Resolved", "priority": "Major", "reporter": "Josh Rosen", "assignee": "Josh Rosen", "labels": [], "created": "2013-09-26T13:51:54.000+0000", "updated": "2013-10-04T21:28:27.000+0000", "description": "I launched a Spark cluster using the new EC2 scripts, stopped it with {{stop}}, then restarted it with {{start}} and ran {{launch --resume}} to re-deploy the Spark configurations.\n\nIt looks like the script exits after initializing Spark if it's already installed:\n\n{code}\nInitializing spark\n~ ~/spark-ec2\nSpark seems to be installed. Exiting.\nConnection to ec2-*-.amazonaws.com closed.\nSpark standalone cluster started at http://ec2-*.compute-1.amazonaws.com:8080\nGanglia started at http://ec2-*-1.amazonaws.com:5080/ganglia\nDone!\n{code}\n\nIt looks like the problem is that the module init scripts are run by {{sourcing}} them into the top-level {{setup.sh}} script rather than running them in subshells, so the module script's own exit command kills the top-level setup script:\n\n{code}\n# Install / Init module\nfor module in $MODULES; do\n  echo \"Initializing $module\"\n  if [[ -e $module/init.sh ]]; then\n    source $module/init.sh\n  fi\ndone\n{code}", "comments": ["Pull request here: https://github.com/mesos/spark-ec2/pull/22"], "derived": {"summary": "I launched a Spark cluster using the new EC2 scripts, stopped it with {{stop}}, then restarted it with {{start}} and ran {{launch --resume}} to re-deploy the Spark configurations. It looks like the script exits after initializing Spark if it's already installed:\n\n{code}\nInitializing spark\n~ ~/spark-ec2\nSpark seems to be installed.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "spark-ec2 launch --resume doesn't re-initialize all modules - I launched a Spark cluster using the new EC2 scripts, stopped it with {{stop}}, then restarted it with {{start}} and ran {{launch --resume}} to re-deploy the Spark configurations. It looks like the script exits after initializing Spark if it's already installed:\n\n{code}\nInitializing spark\n~ ~/spark-ec2\nSpark seems to be installed."}, {"q": "What updates or decisions were made in the discussion?", "a": "Pull request here: https://github.com/mesos/spark-ec2/pull/22"}]}}
{"project": "SPARK", "issue_id": "SPARK-1226", "title": "Import breeze to Spark mllib", "status": "Resolved", "priority": "Major", "reporter": "Kun Yang", "assignee": "Xiangrui Meng", "labels": [], "created": "2013-09-27T11:38:42.000+0000", "updated": "2014-04-01T05:33:08.000+0000", "description": "Breeze' math module includes many common linear algebra and optimization routines. It is going to be very handy for developers if it is imported into mllib. \nI am implementing ADMM (which is mentioned by the speaker in the Spark meetup) in mllib, but there are few utility functions in current mllib and it is error-prone to code it from scratch.\nMany large scale machine learning algorithms need to solve several smaller scale algorithms and combine the results (paralleled SGD) or to convert a disk resident problem into a memory resident problem (linear regression). Breeze should be very useful in these cases.", "comments": ["if we are planning to expand the scope of ML algorithms, I think it is necessary. \nAny feedback? ", "We use breeze to support underlying linear algebra operations in version 1.0.0."], "derived": {"summary": "Breeze' math module includes many common linear algebra and optimization routines. It is going to be very handy for developers if it is imported into mllib.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Import breeze to Spark mllib - Breeze' math module includes many common linear algebra and optimization routines. It is going to be very handy for developers if it is imported into mllib."}, {"q": "What updates or decisions were made in the discussion?", "a": "We use breeze to support underlying linear algebra operations in version 1.0.0."}]}}
{"project": "SPARK", "issue_id": "SPARK-920", "title": "JSON endpoint URI scheme part (spark://) duplicated", "status": "Resolved", "priority": "Trivial", "reporter": "David McCauley", "assignee": "David McCauley", "labels": [], "created": "2013-10-02T05:00:22.000+0000", "updated": "2013-10-09T02:25:39.000+0000", "description": "Within JsonProtocol the MasterStateResponse URI field is prepended with \"spark://\" but the URI is always already prepended with this string within DeployMessages.\n\nExample output:\nspark://spark://host:port", "comments": ["Merged, PR: https://github.com/apache/incubator-spark/pull/27"], "derived": {"summary": "Within JsonProtocol the MasterStateResponse URI field is prepended with \"spark://\" but the URI is always already prepended with this string within DeployMessages. Example output:\nspark://spark://host:port.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "JSON endpoint URI scheme part (spark://) duplicated - Within JsonProtocol the MasterStateResponse URI field is prepended with \"spark://\" but the URI is always already prepended with this string within DeployMessages. Example output:\nspark://spark://host:port."}, {"q": "What updates or decisions were made in the discussion?", "a": "Merged, PR: https://github.com/apache/incubator-spark/pull/27"}]}}
{"project": "SPARK", "issue_id": "SPARK-921", "title": "Add Application UI URL to ApplicationInfo Json output", "status": "Resolved", "priority": "Minor", "reporter": "David McCauley", "assignee": "David McCauley", "labels": [], "created": "2013-10-02T06:41:20.000+0000", "updated": "2013-10-09T02:24:57.000+0000", "description": "Adding the URL of the Main Application UI will allow custom interfaces (that use the JSON output) to redirect from the standalone UI.\n\nThe idea is that eventually the Main Application UI will also have JSON output allowing for complete custom UIs (or many other types of app) to plug in and gather information (SPARK-907).", "comments": ["Merged, PR: https://github.com/apache/incubator-spark/pull/27"], "derived": {"summary": "Adding the URL of the Main Application UI will allow custom interfaces (that use the JSON output) to redirect from the standalone UI. The idea is that eventually the Main Application UI will also have JSON output allowing for complete custom UIs (or many other types of app) to plug in and gather information (SPARK-907).", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add Application UI URL to ApplicationInfo Json output - Adding the URL of the Main Application UI will allow custom interfaces (that use the JSON output) to redirect from the standalone UI. The idea is that eventually the Main Application UI will also have JSON output allowing for complete custom UIs (or many other types of app) to plug in and gather information (SPARK-907)."}, {"q": "What updates or decisions were made in the discussion?", "a": "Merged, PR: https://github.com/apache/incubator-spark/pull/27"}]}}
{"project": "SPARK", "issue_id": "SPARK-922", "title": "Update Spark AMI to Python 2.7", "status": "Closed", "priority": "Blocker", "reporter": "Josh Rosen", "assignee": null, "labels": [], "created": "2013-10-04T14:45:55.000+0000", "updated": "2016-04-22T16:42:48.000+0000", "description": "Many Python libraries only support Python 2.7+, so we should make Python 2.7 the default Python on the Spark AMIs.", "comments": ["As a short-term workaround, here's a quick technique to update an existing cluster to use Python 2.7:\n\n{code}\nyum install -y pssh\nyum install -y python27 python27-devel\npssh -h /root/spark-ec2/slaves yum install -y python27\nwget https://bitbucket.org/pypa/setuptools/raw/bootstrap/ez_setup.py -O - | python27\neasy_install-2.7 pip\npip-2.7 install ipython[all]\n{code}\n\nThen, edit {{spark-env.sh}} to {{export PYSPARK_PYTHON=python2.7}}.  If you're using IPython notebook, make sure sure to {{source spark-env.sh}} before launching the notebook server so that this environment variable is picked up (the {{pyspark}} script normally handles this).", "This little script is so cool! I didn't know there was such a thing as {{pssh}}.\n\nBy the way, I think there is a typo at the end of the code block. The command that worked for me was:\n\n{code}\npip2.7 install ipython[all]\n{code}\n\nThat is, minus the dash.\n\nAlso, you could script the rest of the required upgrade steps as follows:\n\n{code}\nprintf \"\\n# Set Spark Python version\\nexport PYSPARK_PYTHON=python2.7\\n\" >> /root/spark/conf/spark-env.sh\nsource /root/spark/conf/spark-env.sh\n{code}", "This is no longer a blocker now that we've downgraded the python dependency, but would still be nice to have.", "Updated script, which also updates numpy:\n\n{code}\nyum install -y pssh\nyum install -y python27 python27-devel\npssh -h /root/spark-ec2/slaves yum install -y python27 python27-devel\nwget https://bitbucket.org/pypa/setuptools/raw/bootstrap/ez_setup.py -O - | python27\npssh -h /root/spark-ec2/slaves \"wget https://bitbucket.org/pypa/setuptools/raw/bootstrap/ez_setup.py -O - | python27\"\neasy_install-2.7 pip\npssh -h /root/spark-ec2/slaves easy_install-2.7 pip\npip2.7 install numpy\npssh -h /root/spark-ec2/slaves pip2.7 -t0 install numpy\n{code}\n\nAnd to check that numpy is successfully installed:\n\n{code}\npssh -h /root/spark-ec2/slaves --inline-stdout 'python2.7 -c \"import numpy; print numpy\"'\n{code}", "Josh, at the end of your updated script do we still also need the step to edit {{spark-env.sh}}?", "Yeah, you still need to set PYSPARK_PYTHON since this doesn't overwrite the system Python.  I was updating this to brain-dump the script I'm using for a Python 2.6 vs Python 2.7 benchmark.", "FYI, I believe the line to install numpy on the slaves should read:\n\n{code}\npssh -t0 -h /root/spark-ec2/slaves pip2.7 install numpy\n{code}\n\ni.e. Change the position of the {{-t0}}.", "[~joshrosen] By the way, as part of this work to update the AMIs, can we also have them include the latest security patches and updates? It's a good practice, and we also suspect that it would [improve our EC2 startup time|https://github.com/apache/spark/pull/2339#issuecomment-55483793].", "[~nchammas] In the long run, it might be nice to automate the AMI creation / upgrade process so that changes like this can be done in build configuration files.  We might have some internal tooling for this; I'll ask around and find out.", "here is how I am launching iPython notebook. I am running as the ec2-user\nIPYTHON_OPTS=\"notebook --pylab inline --no-browser --port=7000\" $SPARK_HOME/bin/pyspark\n\nBellow are all the upgrade commands I ran \n\nI ran into a small problem the ipython magic %matplotlib inline\n\ncreates an error, you can work around this by commenting it out. \n\nAndy\n\nyum install -y pssh\nyum install -y python27 python27-devel\npssh -h /root/spark-ec2/slaves yum install -y python27 python27-devel\nwget https://bitbucket.org/pypa/setuptools/raw/bootstrap/ez_setup.py -O - | python27\npssh -h /root/spark-ec2/slaves \"wget https://bitbucket.org/pypa/setuptools/raw/bootstrap/ez_setup.py -O - | python27\"\neasy_install-2.7 pip\npssh -h /root/spark-ec2/slaves easy_install-2.7 pip\npip2.7 install numpy\npssh -t0 -h /root/spark-ec2/slaves pip2.7 install numpy\npip2.7 install ipython[all]\nprintf \"\\n# Set Spark Python version\\nexport PYSPARK_PYTHON=/usr/bin/python2.7\\n\" >> /root/spark/conf/spark-env.sh\nsource /root/spark/conf/spark-env.sh\n\n", "I chatted with someone who had a job that ran ~20x slower on Python 2.6 than on 2.7, likely due to changes to the json library (in 2.7+, json is implemented as a C extension rather than in pure-Python).  Maybe we should add a note on this to the docs.", "We did not use json heavily in pyspark, also user have several choice of json library in Python, this should not be a issue, i think.\n\nWe definitely need to upgrade to Python2.7 (as default), if some user need python2.6, it's easy to use it by PYSPARK_PYTHON.", "[~joshrosen] Are you open to having this resolved as part of [SPARK-3821]?", "Wow upgrading matplotlib was a bear. The following worked for me. The trick was getting the correct version of the source code. The recipe bellow is not 100% correct. I have not figured out how to use pssh with yum. yum prompts you y/n before downloading \n\npip2.7 install six\npssh -t0 -h /root/spark-ec2/slaves pip2.7 install six\n\npip2.7 install python-dateutil\npssh -t0 -h /root/spark-ec2/slaves pip2.7 install python-dateutil\n\npip2.7 install pyparsing\npssh -t0 -h /root/spark-ec2/slaves pip2.7 install pyparsing\n\n\nyum install yum-utils\n\nwget https://github.com/matplotlib/matplotlib/archive/master.tar.gz\ntar -zxvf master.tar.gz\ncd matplotlib-master/\nyum install freetype-devel\nyum install libpng-devel\npython2.7 setup.py build\npython2.7 setup.py install", "also forgot the mention there are a couple of steps on http://nbviewer.ipython.org/gist/JoshRosen/6856670\n\nthat are important in the upgrade process\n\n#\n# restart spark\n#\n/root/spark/sbin/stop-all.sh\n/root/spark/sbin/start-all.sh\n", "[~nchammas]: It would be great to include Python 2.7 in the next AMI; I think our current AMI shell script has it, though.\n\n[~aedwip]:\n\n{quote}\nI have not figured out how to use pssh with yum. yum prompts you y/n before downloading\n{quote}\n\nTry {{yum install -y}}.", "[~joshrosen] - Do you mean [this script|https://github.com/mesos/spark-ec2/blob/v4/create_image.sh]? It doesn't seem to have anything related to Python 2.7.\n\nAnyway, what I meant was if you were open to holding off on updating the Spark AMIs until we had also figured out how to automate that process per [SPARK-3821]. I should have something for that as soon as this week or next.", "[~nchammas] - I don't think that there's an urgent rush to update the AMIs before the next round of releases, so I'm fine with waiting to incorporate this into SPARK-3821.", "cc [~shivaram]", "This is now outside the scope of Spark."], "derived": {"summary": "Many Python libraries only support Python 2. 7+, so we should make Python 2.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Update Spark AMI to Python 2.7 - Many Python libraries only support Python 2. 7+, so we should make Python 2."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is now outside the scope of Spark."}]}}
{"project": "SPARK", "issue_id": "SPARK-923", "title": "Bytes columns in web UI tables don't sort properly", "status": "Resolved", "priority": "Minor", "reporter": "Josh Rosen", "assignee": "xiajunluan", "labels": ["Starter"], "created": "2013-10-04T23:22:05.000+0000", "updated": "2013-11-30T15:59:44.000+0000", "description": "In the Web UI, columns displaying sizes in bytes do not sort correctly; they are sorted by the string value instead of the number of bytes, megabytes, etc.  It's possible to add a sort key in the HTML that the table sorter plugin will recognize and sort by.\n\nI observed this issue on the \"Shuffle Write\" column in a detailed stage view in the application Web UI, but it might also affect other column like \"Shuffle Read\".  I've attached a screenshot illustrating the problem.\n\nSPARK-801 was a similar issue, except with Time columns.", "comments": ["Fixed by https://github.com/apache/incubator-spark/pull/160"], "derived": {"summary": "In the Web UI, columns displaying sizes in bytes do not sort correctly; they are sorted by the string value instead of the number of bytes, megabytes, etc. It's possible to add a sort key in the HTML that the table sorter plugin will recognize and sort by.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Bytes columns in web UI tables don't sort properly - In the Web UI, columns displaying sizes in bytes do not sort correctly; they are sorted by the string value instead of the number of bytes, megabytes, etc. It's possible to add a sort key in the HTML that the table sorter plugin will recognize and sort by."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed by https://github.com/apache/incubator-spark/pull/160"}]}}
{"project": "SPARK", "issue_id": "SPARK-924", "title": "Nested RDD", "status": "Resolved", "priority": "Major", "reporter": "Pavel Ajtkulov", "assignee": null, "labels": [], "created": "2013-10-06T23:24:21.000+0000", "updated": "2013-10-07T14:59:39.000+0000", "description": "Is it possible to work with nested RDD (ie RDD[RDD])?\n\nThis code is failed (scala)\n\nval q1 = sc.makeRDD(Array(1,2,3,4,5))\nval q2 = sc.makeRDD(Array(1,2,3,4,5))\nval qq = sc.makeRDD(Array(q1, q2))\nval ar = qq.collect\nval z = ar(0)\nz.collect\n\nwith\njava.lang.NullPointerException\n        at org.apache.spark.rdd.RDD.collect(RDD.scala:560)\n        at <init>(<console>:23)\n        at <init>(<console>:28)\n        at <init>(<console>:30)\n        at <init>(<console>:32)\n        at <init>(<console>:34)\n        at .<init>(<console>:38)\n        at .<clinit>(<console>)\n        at .<init>(<console>:11)\n        at .<clinit>(<console>)\n        at $export(<console>)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:629)\n        at org.apache.spark.repl.SparkIMain$Request$$anonfun$10.apply(SparkIMain.scala:890)\n        at scala.tools.nsc.interpreter.Line$$anonfun$1.apply$mcV$sp(Line.scala:43)\n        at scala.tools.nsc.io.package$$anon$2.run(package.scala:25)\n        at java.lang.Thread.run(Thread.java:724)\n", "comments": ["Spark doesn't support nested RDDs; see https://groups.google.com/d/msg/spark-users/_Efj40upvx4/DbHCixW7W7kJ\n\nMaybe you can express your program using RDDs of collections (such as {{RDD[Array[[T]]])}}, or with RDD.union()/SparkContext.union()."], "derived": {"summary": "Is it possible to work with nested RDD (ie RDD[RDD])?\n\nThis code is failed (scala)\n\nval q1 = sc. makeRDD(Array(1,2,3,4,5))\nval q2 = sc.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Nested RDD - Is it possible to work with nested RDD (ie RDD[RDD])?\n\nThis code is failed (scala)\n\nval q1 = sc. makeRDD(Array(1,2,3,4,5))\nval q2 = sc."}, {"q": "What updates or decisions were made in the discussion?", "a": "Spark doesn't support nested RDDs; see https://groups.google.com/d/msg/spark-users/_Efj40upvx4/DbHCixW7W7kJ\n\nMaybe you can express your program using RDDs of collections (such as {{RDD[Array[[T]]])}}, or with RDD.union()/SparkContext.union()."}]}}
{"project": "SPARK", "issue_id": "SPARK-925", "title": "Allow ec2 scripts to load default options from a json file", "status": "Resolved", "priority": "Minor", "reporter": "Shay Seng", "assignee": null, "labels": [], "created": "2013-10-08T10:58:31.000+0000", "updated": "2016-01-16T13:00:31.000+0000", "description": "The option list for ec2 script can be a little irritating to type in, especially things like path to identity-file, region , zone, ami etc.\nIt would be nice if ec2 script looks for an options.json file in the following order: (1) CWD, (2) ~/spark-ec2, (3) same dir as spark_ec2.py\n\nSomething like:\ndef get_defaults_from_options():\n  # Check to see if a options.json file exists, if so load it. \n  # However, values in the options.json file can only overide values in opts\n  # if the Opt values are None or \"\"\n  # i.e. commandline options take presidence \n  defaults = {'aws-access-key-id':'','aws-secret-access-key':'','key-pair':'', 'identity-file':'', 'region':'ap-southeast-1', 'zone':'', 'ami':'','slaves':1, 'instance-type':'m1.large'}\n\n  # Look for options.json in directory cluster was called from\n  # Had to modify the spark_ec2 wrapper script since it mangles the pwd\n  startwd = os.environ['STARTWD']\n  if os.path.exists(os.path.join(startwd,\"options.json\")):\n      optionspath = os.path.join(startwd,\"options.json\")\n  else:\n      optionspath = os.path.join(os.getcwd(),\"options.json\")\n      \n  try:\n    print \"Loading options file: \", optionspath  \n    with open (optionspath) as json_data:\n        jdata = json.load(json_data)\n        for k in jdata:\n          defaults[k]=jdata[k]\n  except IOError:\n    print 'Warning: options.json file not loaded'\n\n  # Check permissions on identity-file, if defined, otherwise launch will fail late and will be irritating\n  if defaults['identity-file']!='':\n    st = os.stat(defaults['identity-file'])\n    user_can_read = bool(st.st_mode & stat.S_IRUSR)\n    grp_perms = bool(st.st_mode & stat.S_IRWXG)\n    others_perm = bool(st.st_mode & stat.S_IRWXO)\n    if (not user_can_read):\n      print \"No read permission to read \", defaults['identify-file']\n      sys.exit(1)\n    if (grp_perms or others_perm):\n      print \"Permissions are too open, please chmod 600 file \", defaults['identify-file']\n      sys.exit(1)\n\n  # if defaults contain AWS access id or private key, set it to environment. \n  # required for use with boto to access the AWS console \n  if defaults['aws-access-key-id'] != '':\n    os.environ['AWS_ACCESS_KEY_ID']=defaults['aws-access-key-id'] \n  if defaults['aws-secret-access-key'] != '':   \n    os.environ['AWS_SECRET_ACCESS_KEY'] = defaults['aws-secret-access-key']\n\n  return defaults  \n    ", "comments": ["Loading config from a file seems like a good thing to have and matches what comparable tools like Ubuntu Juju and MIT StarCluster do.\n\nHowever, I would favor a format other than JSON since JSON doesn't allow you to comment stuff out. I've used tools with JSON-backed config files, and they are super annoying to deal with if you try to alternate invocations between version A with this line uncommented and version B with the same line commented out.\n\nYAML seems like a better choice for this task. What do you think [~shayping]/[~shay]?\n\ncc [~shivaram]", "JSON config files are indeed irritating to work with. Unfortunately the spark conf files are in JSON. AFAIK. I would rather stick to one form rather than have to bother with more than one format.\n\nWith JSON we deal with internally, we have started to nest definitions so that it is easy for some one to modify one small setting without having to specify all the other settings --  and as a work around to comments.\n\ne.g.\n{\n_default: {\n  settingA :\n  settingB :  \n  settingC :\n  },\nmainOption: {\n  default : _default,\n  settingB : xyz\n  },\nmyModifiedOption: {\n  default: mainOption,\n  settingC: abc\n}", "Formatting side comment: You can surround your code block with {{code}} tags and it'll look nice.\n\ne.g. \n\n{noformat}\n{code}\n{\n  \"ayyo\": \"wassup\"\n}\n{code}\n{noformat}", "I would prefer a format that is more human friendly and that supports comments directly. To me, JSON is better for data exchange, and YAML is better for config files and other things that humans are going to be dealing with directly.\n\nIt's true that there are other config formats used in Spark. The ones under [conf/|https://github.com/apache/spark/tree/master/conf], however, are not JSON. Which ones were you thinking of?\n\nAs long as the config format is consistent within a sub-project, I think it's OK. Since spark-ec2 doesn't have any config files yet, I don't think it's bad to go with YAML.\n\n{quote}\nWith JSON we deal with internally, we have started to nest definitions so that it is easy for some one to modify one small setting without having to specify all the other settings  and as a work around to comments.\n{quote}\n\nAs discussed before, YAML supports comments directly, which IMO is essential for a config format. With regards to modifying a setting without specifying everything, I'm not sure I understand the use case.\n\nIf we define some config file resolution order (first check /first/config, then check /second/config, etc.), is it that bad if people just copied the default config from /second/config to /first/config and modified what they wanted? I believe that's how it generally works in tools that check multiple places for configuration.\n\nA better way to do this would probably be to allow people to specify a sub-set of options in any given file, and option sets just get merged on top of the options specified in the preceding file. That seems more complexity than is worth it at this time, though.", "Here's an example of what a spark-ec2 {{config.yml}} file could look like:\n\n{code}\nregion: us-east-1\n\naws_auth:\n  key_pair: mykey\n  identity_file: /path/to/file.pem\n\n# spark_version: 1.2.1\n\nslaves: 5\ninstance_type: m3.large\n\nuse_existing_master: no\n{code}\n\nIt's dead simple and there's not much to learn, really.", "User 'marmbrus' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/7906"], "derived": {"summary": "The option list for ec2 script can be a little irritating to type in, especially things like path to identity-file, region , zone, ami etc. It would be nice if ec2 script looks for an options.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Allow ec2 scripts to load default options from a json file - The option list for ec2 script can be a little irritating to type in, especially things like path to identity-file, region , zone, ami etc. It would be nice if ec2 script looks for an options."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'marmbrus' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/7906"}]}}
{"project": "SPARK", "issue_id": "SPARK-926", "title": "spark_ec2 script when ssh/scp-ing should pipe UserknowHostFile to /dev/null", "status": "Resolved", "priority": "Trivial", "reporter": "Shay Seng", "assignee": null, "labels": [], "created": "2013-10-08T11:03:09.000+0000", "updated": "2015-02-06T07:34:18.000+0000", "description": "The know host file in the local machine gets all kinds of crap after a few cluster launches. When SSHing, or SCPing, please add \"-o UserKnowHostFile=/dev/null\"\n\nAlso remove the -t option from SSH, and only add in when necessary - to reduce chatter on console. \ne.g.\n# Copy a file to a given host through scp, throwing an exception if scp fails\ndef scp(host, opts, local_file, dest_file):\n  subprocess.check_call(\n      \"scp -q -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i %s '%s' '%s@%s:%s'\" %\n      (opts.identity_file, local_file, opts.user, host, dest_file), shell=True)\n\n\n# Run a command on a host through ssh, retrying up to two times\n# and then throwing an exception if ssh continues to fail.\ndef ssh(host, opts, command, sshopts=\"\"):\n  tries = 0\n  while True:\n    try:\n      # removed -t option from ssh command, not sure why it is required all the time.\n      return subprocess.check_call(\n        \"ssh %s -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i %s %s@%s '%s'\" %\n        (sshopts, opts.identity_file, opts.user, host, command), shell=True)\n    except subprocess.CalledProcessError as e:\n      if (tries > 2):\n        raise e\n      print \"Couldn't connect to host {0}, waiting 30 seconds\".format(e)\n      time.sleep(30)\n      tries = tries + 1", "comments": ["[~shay] / [~shayping] (pinging both names registered): Is this still an issue as of 1.2.0? I just stumbled across this issue and am curious.", "I think it is; there's now a PR to fix this, since it's also the cause of SPARK-5403: https://github.com/apache/spark/pull/4196", "Going to make this one the duplicate since SPARK-5403 has an active PR."], "derived": {"summary": "The know host file in the local machine gets all kinds of crap after a few cluster launches. When SSHing, or SCPing, please add \"-o UserKnowHostFile=/dev/null\"\n\nAlso remove the -t option from SSH, and only add in when necessary - to reduce chatter on console.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "spark_ec2 script when ssh/scp-ing should pipe UserknowHostFile to /dev/null - The know host file in the local machine gets all kinds of crap after a few cluster launches. When SSHing, or SCPing, please add \"-o UserKnowHostFile=/dev/null\"\n\nAlso remove the -t option from SSH, and only add in when necessary - to reduce chatter on console."}, {"q": "What updates or decisions were made in the discussion?", "a": "Going to make this one the duplicate since SPARK-5403 has an active PR."}]}}
{"project": "SPARK", "issue_id": "SPARK-927", "title": "PySpark sample() doesn't work if numpy is installed on master but not on workers", "status": "Resolved", "priority": "Minor", "reporter": "Josh Rosen", "assignee": "Matthew Farrellee", "labels": [], "created": "2013-10-08T11:03:56.000+0000", "updated": "2015-01-05T23:05:46.000+0000", "description": "PySpark's sample() method crashes with ImportErrors on the workers if numpy is installed on the driver machine but not on the workers.  I'm not sure what's the best way to fix this.  A general mechanism for automatically shipping libraries from the master to the workers would address this, but that could be complicated to implement.", "comments": ["it looks like the issue is rddsampler checks for numpy in its constructor instead of when initializing the random number generator", "PR #2313 was subsumed by PR #3351, which resolved SPARK-4477 and this issue\n\nthe resolution was to remove the use of numpy altogether"], "derived": {"summary": "PySpark's sample() method crashes with ImportErrors on the workers if numpy is installed on the driver machine but not on the workers. I'm not sure what's the best way to fix this.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "PySpark sample() doesn't work if numpy is installed on master but not on workers - PySpark's sample() method crashes with ImportErrors on the workers if numpy is installed on the driver machine but not on the workers. I'm not sure what's the best way to fix this."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR #2313 was subsumed by PR #3351, which resolved SPARK-4477 and this issue\n\nthe resolution was to remove the use of numpy altogether"}]}}
{"project": "SPARK", "issue_id": "SPARK-928", "title": "Add support for Unsafe-based serializer in Kryo 2.22", "status": "Resolved", "priority": "Minor", "reporter": "Matei Alexandru Zaharia", "assignee": "Sandeep Singh", "labels": ["starter"], "created": "2013-10-09T16:59:39.000+0000", "updated": "2016-10-22T19:03:12.000+0000", "description": "This can reportedly be quite a bit faster, but it also requires Chill to update its Kryo dependency. Once that happens we should add a spark.kryo.useUnsafe flag.", "comments": ["This probably can't be fixed in 1.0.0 because no Chill release uses Kryo 2.22 yet, and as far as I can tell we can't build the current Chill with Kryo 2.22 (I get some nasty Scala compiler errors when I try that). We can bump fix version to 1.1.0 once we do the final pass through 1.0.0 issues.", "Latest Chill (0.5.0) is still using Kryo 2.21 so this is still waiting on a Chill update\n\nhttps://github.com/twitter/chill/blob/0.5.0/project/Build.scala#L13", "It looks like we'll _finally_ be able to do this after SPARK-11416 goes in.", "We've now upgraded to Kryo 3.0.0 (in SPARK-11416), so it would be awesome if someone wants to try this out and report back with benchmark results.", "Since it is labeled as Starter, I would like to take a try on this one.", "[~joshrosen] I would like to work on this.\n\nI tried benchmarking the difference between unsafe kryo and our current impl. and then we can have a spark.kryo.useUnsafe flag as Matei has mentioned.\n\n{code:title=Benchmarking results|borderStyle=solid}\nJBenchmark Kryo Unsafe vs safe Serialization: Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n      ------------------------------------------------------------------------------------------------\n      basicTypes: Int unsafe:true                    160 /  178         98.5          10.1       1.0X\n      basicTypes: Long unsafe:true                   210 /  218         74.9          13.4       0.8X\n      basicTypes: Float unsafe:true                  203 /  213         77.5          12.9       0.8X\n      basicTypes: Double unsafe:true                 226 /  235         69.5          14.4       0.7X\n      Array: Int unsafe:true                        1087 / 1101         14.5          69.1       0.1X\n      Array: Long unsafe:true                       2758 / 2844          5.7         175.4       0.1X\n      Array: Float unsafe:true                      1511 / 1552         10.4          96.1       0.1X\n      Array: Double unsafe:true                     2942 / 2972          5.3         187.0       0.1X\n      Map of string->Double unsafe:true             2645 / 2739          5.9         168.2       0.1X\n      basicTypes: Int unsafe:false                   211 /  218         74.7          13.4       0.8X\n      basicTypes: Long unsafe:false                  247 /  253         63.6          15.7       0.6X\n      basicTypes: Float unsafe:false                 211 /  216         74.5          13.4       0.8X\n      basicTypes: Double unsafe:false                227 /  233         69.2          14.4       0.7X\n      Array: Int unsafe:false                       3012 / 3032          5.2         191.5       0.1X\n      Array: Long unsafe:false                      4463 / 4515          3.5         283.8       0.0X\n      Array: Float unsafe:false                     2788 / 2868          5.6         177.2       0.1X\n      Array: Double unsafe:false                    3558 / 3752          4.4         226.2       0.0X\n      Map of string->Double unsafe:false            2806 / 2933          5.6         178.4       0.1X\n{code}\n\nYou can find the code for benchmarking here (https://github.com/techaddict/spark/commit/46fa44141c849ca15bbd6136cea2fa52bd927da2), very ugly right now but will improve it(add more benchmarks) before creating a PR.\n", "User 'techaddict' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/12913"], "derived": {"summary": "This can reportedly be quite a bit faster, but it also requires Chill to update its Kryo dependency. Once that happens we should add a spark.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add support for Unsafe-based serializer in Kryo 2.22 - This can reportedly be quite a bit faster, but it also requires Chill to update its Kryo dependency. Once that happens we should add a spark."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'techaddict' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/12913"}]}}
{"project": "SPARK", "issue_id": "SPARK-929", "title": "Deprecate SPARK_MEM", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Aaron Davidson", "labels": [], "created": "2013-10-09T19:34:49.000+0000", "updated": "2014-03-17T10:21:54.000+0000", "description": "Now that executor memory can be set through the spark.executor.memory system property, there's no reason to have SPARK_MEM. In general as we update the configuration system we should reduce reliance on environment variables.", "comments": ["I will work on this. I should assign this to myself, can someone give me karma to do so, please?", "Alright, I've assigned it to you. I'll also add you to the developers group so you can do that.", "Thanks Matei! I posted this question on the original pull request (https://github.com/apache/incubator-spark/pull/48) but I am posting it here in case you missed the notification there:\nI see SPARK_MEM's usage in 2 places.\n1. In populating of the Spark context (see https://github.com/apache/incubator-spark/blob/master/core/src/main/scala/org/apache/spark/SparkContext.scala#L1011)\n2. In passing this info to the executors so they can set the memory size. (see https://github.com/apache/incubator-spark/blob/master/core/src/main/scala/org/apache/spark/SparkContext.scala#L145)\n\nAre you referring to deprecation at both 1 and 2 above? Or just 1?\n\nThanks again!", "I think doing it in just the first one is fine, but we may want to rename the parameter in the second one. For that second one, it's hard to set the memory without an environment var because it needs to be seen in a shell script *before* the JVM launches in order to set parameters on the `java` command.", "Thanks and agreed!", "Issued a pull request for feedback. I haven't done much testing but plan to do it soon.", "After this, what will be the preferred way for setting the driver memory?", "As far as I know, SPARK_MEM is still the way to do this, though it is no longer documented in Spark Configuration. Is this a problem, or am I missing something?", "I've confirmed with @[~patrick] that this is indeed a problem. Here are some thoughts from our discussion:\n\nThe current state of the code is not great -- SPARK_MEM is no longer documented, but it is still the only way to specify the memory to be used in spark-shell and run-examples. (Additionally, it is still the default for executors if spark.executor.mem is not specified.)\n\nOne proposal for going forward is to create a new environment variable SPARK_DRIVER_MEM which is intended to be used for both spark-shell and run-examples, and nowhere else. This allows us to keep SPARK_MEM deprecated and to avoid introducing another super-general option. It also means that users who intend to set the spark-shell memory do not accidentally set executor memory as well.\n\nSince environment variables are so pass, we can make spark-shell take an additional parameter for the driver memory (which uses SPARK_DRIVER_MEM to pass to spark-class). It's harder to add such an option to run-examples, since it's unprecedented to have parameters that run-examples parses before running the actual example (so it would complicate the interface), and since it's unlikely that changing the memory is needed. For this case, users can still use SPARK_DRIVER_MEM directly.\n\nComplications of this approach:\n(1) The name \"SPARK_DRIVER_MEM\" may confuse users into thinking it actually sets the driver memory, rather than just setting the memory for the host process for spark-shell and run-examples. If a user runs their own driver, the driver will still use the memory of the host JVM, totally independent of SPARK_DRIVER_MEM.\n\n(2) Configuring the spark-shell memory can be done through both SPARK_DRIVER_MEM and a spark-shell command-line option, which can be confusing to users. This is true of the other spark-shell configuration options as well. Perhaps we can a warning if the user attempts to specify both.", "On reflection, I think the following may be simpler: Two environment variables, SPARK_SHELL_MEM (private, not documented) and SPARK_EXAMPLES_MEM (documented somewhere, but probably not on the Configuration page). The main way to configure the spark-shell's memory will be through a command-line option like \"spark-shell --driver-mem 4g\", which simply sets SPARK_SHELL_MEM (which is only used by spark-class when executing the repl class, similar to SPARK_DAEMON_MEM for master and worker classes).\n\nSPARK_EXAMPLES_MEM can be documented in the same way as SPARK_EXAMPLES_JAR, since it is should be infrequently used, but does need to be configurable.\n\nThis avoids both complications listed above, and still keeps us from using SPARK_MEM internally so we don't accidentally set the executor memory. Any thoughts?", "[~ilikerps] This does seem better. Just to be clear though - we still have the issue of `spark-class` needing to check what class is being run in order to determine which XX_MEM to us - correct? Also, how about SPARK_EXAMPLE_MEM (no -S) since it will apply at any given time to one example (similar to SPARK_SHELL_MEM).", "Right, same issue of needing to check the class that's being run to decide which env variable to use. I'm fine with SPARK_EXAMPLE_MEM, but if someone types SPARK_EXAMPLES_MEM, following the SPARK_EXAMPLES_JAR style, they'll be in for a fun debugging session. (Could actually specifically check for the \"S\" as a little usability improvement.)", "How would one set the memory when running a job with spark-class?", "Great question, one I had just hit as I was writing up the commit message for my patch :)\n\nI think a good solution is to add a SPARK_DRIVER_MEMORY which is publicly documented. By also introducing an explicit SPARK_EXECUTOR_MEMORY which is set for the Mesos schedulers (standalone and YARN do not use spark-class) using spark.executor.memory, we can keep backwards compatibility of SPARK_MEM while ensuring SPARK_DRIVER_MEMORY does not affect executors.\n\nI should have a patch ready for review tomorrow in case that made not so much sense.", "That makes sense to me.  Thanks Aaron.\n\nOne more thing: how will this function in yarn-standalone mode, where the driver runs in the YARN application master?  Will SPARK_DRIVER_MEM get passed there?  Do we consider the client that is submitting the job, but not driving it in terms of scheduling, a DRIVER?"], "derived": {"summary": "Now that executor memory can be set through the spark. executor.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Deprecate SPARK_MEM - Now that executor memory can be set through the spark. executor."}, {"q": "What updates or decisions were made in the discussion?", "a": "That makes sense to me.  Thanks Aaron.\n\nOne more thing: how will this function in yarn-standalone mode, where the driver runs in the YARN application master?  Will SPARK_DRIVER_MEM get passed there?  Do we consider the client that is submitting the job, but not driving it in terms of scheduling, a DRIVER?"}]}}
{"project": "SPARK", "issue_id": "SPARK-930", "title": "Unicode failing in pyspark - UnicodeEncodeError: 'ascii' codec can't encode character u'\\ufffd' in position 440: ordinal not in range(128)", "status": "Resolved", "priority": "Major", "reporter": "Ben Duffield", "assignee": null, "labels": [], "created": "2013-10-10T00:13:29.000+0000", "updated": "2013-12-15T14:18:44.000+0000", "description": "Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.takePartition.\n: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/palantir/spark/spark-0.8.0/python/pyspark/worker.py\", line 82, in main\n    for obj in func(split_index, iterator):\n  File \"/opt/palantir/spark/spark-0.8.0/python/pyspark/serializers.py\", line 41, in batched\n    for item in iterator:\n  File \"/opt/palantir/spark/spark-0.8.0/python/pyspark/rdd.py\", line 521, in takeUpToNum\n    yield next(iterator)\n  File \"/usr/lib64/python2.6/csv.py\", line 104, in next\n    row = self.reader.next()\nUnicodeEncodeError: 'ascii' codec can't encode character u'\\ufffd' in position 440: ordinal not in range(128)\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:151)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:173)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:116)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:226)\n\tat org.apache.spark.scheduler.DAGScheduler.runLocallyWithinThread(DAGScheduler.scala:484)\n\tat org.apache.spark.scheduler.DAGScheduler$$anon$2.run(DAGScheduler.scala:470)", "comments": ["This is me being a total idiot. \n\nIt's happening outside of spark - it's me.\n\nPlease close."], "derived": {"summary": "Py4JJavaError: An error occurred while calling z:org. apache.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Unicode failing in pyspark - UnicodeEncodeError: 'ascii' codec can't encode character u'\\ufffd' in position 440: ordinal not in range(128) - Py4JJavaError: An error occurred while calling z:org. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is me being a total idiot. \n\nIt's happening outside of spark - it's me.\n\nPlease close."}]}}
{"project": "SPARK", "issue_id": "SPARK-931", "title": "Potential bug with Spark streaming example", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "labels": ["starter"], "created": "2013-10-10T20:03:58.000+0000", "updated": "2013-10-16T10:45:08.000+0000", "description": "See e-mail thread:\nhttps://groups.google.com/forum/#!topic/spark-users/GVPDZOCZMcw", "comments": ["Patch submitted: https://github.com/apache/incubator-spark/pull/63/files"], "derived": {"summary": "See e-mail thread:\nhttps://groups. google.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Potential bug with Spark streaming example - See e-mail thread:\nhttps://groups. google."}, {"q": "What updates or decisions were made in the discussion?", "a": "Patch submitted: https://github.com/apache/incubator-spark/pull/63/files"}]}}
{"project": "SPARK", "issue_id": "SPARK-932", "title": "Consolidate local scheduler and cluster scheduler", "status": "Resolved", "priority": "Major", "reporter": "Reynold Xin", "assignee": "Kay Ousterhout", "labels": ["starter"], "created": "2013-10-10T23:18:53.000+0000", "updated": "2014-01-05T23:16:28.000+0000", "description": "We should consolidate LocalScheduler and ClusterScheduler, given most of the functionalities are duplicated in both. \n\nThis can be done by removing the LocalScheduler, and create a LocalSchedulerBackend that connects directly to an Executor. ", "comments": [], "derived": {"summary": "We should consolidate LocalScheduler and ClusterScheduler, given most of the functionalities are duplicated in both. This can be done by removing the LocalScheduler, and create a LocalSchedulerBackend that connects directly to an Executor.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Consolidate local scheduler and cluster scheduler - We should consolidate LocalScheduler and ClusterScheduler, given most of the functionalities are duplicated in both. This can be done by removing the LocalScheduler, and create a LocalSchedulerBackend that connects directly to an Executor."}]}}
{"project": "SPARK", "issue_id": "SPARK-933", "title": "Doesn't compile", "status": "Resolved", "priority": "Major", "reporter": "Teodor Sigaev", "assignee": null, "labels": ["compile"], "created": "2013-10-14T18:56:55.000+0000", "updated": "2013-10-15T14:38:45.000+0000", "description": "Calling from RHEL5\nspark-0.8.0-incubating$ sbt/sbt assembly\n\ngives these warnings and errors\n[warn] /rhel5pdi/workplace/extprojects/spark-0.8.0-incubating/core/src/main/scala/org/apache/spark/SparkHadoopWriter.scala:129: method cleanupJob in class OutputCommitter is deprecated: see corresponding Javadoc for more information.\n[warn]     getOutputCommitter().cleanupJob(getJobContext())\n[warn]                          ^\n[warn] /rhel5pdi/workplace/extprojects/spark-0.8.0-incubating/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala:592: method cleanupJob in class OutputCommitter is deprecated: see corresponding Javadoc for more information.\n[warn]     jobCommitter.cleanupJob(jobTaskContext)\n[warn]                  ^\n[warn] two warnings found\n[error] ----------\n[error] 1. WARNING in /rhel5pdi/workplace/extprojects/spark-0.8.0-incubating/core/src/main/java/org/apache/spark/network/netty/FileClient.java (at line 22)\n[error]         import io.netty.channel.ChannelFuture;\n[error]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[error] The import io.netty.channel.ChannelFuture is never used\n[error] ----------\n[error] 2. WARNING in /rhel5pdi/workplace/extprojects/spark-0.8.0-incubating/core/src/main/java/org/apache/spark/network/netty/FileClient.java (at line 23)\n[error]         import io.netty.channel.ChannelFutureListener;\n[error]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[error] The import io.netty.channel.ChannelFutureListener is never used\n[error] ----------\n[error] ----------\n[error] 3. WARNING in /rhel5pdi/workplace/extprojects/spark-0.8.0-incubating/core/src/main/java/org/apache/spark/network/netty/FileServer.java (at line 23)\n[error]         import io.netty.channel.Channel;\n[error]                ^^^^^^^^^^^^^^^^^^^^^^^^\n[error] The import io.netty.channel.Channel is never used\n[error] ----------\n[error] ----------\n[error] 4. WARNING in /rhel5pdi/workplace/extprojects/spark-0.8.0-incubating/core/src/main/scala/org/apache/spark/api/java/JavaSparkContextVarargsWorkaround.java (at line 20)\n[error]         import java.util.Arrays;\n[error]                ^^^^^^^^^^^^^^^^\n[error] The import java.util.Arrays is never used\n[error] ----------\n[error] ----------\n[error] 5. ERROR in /rhel5pdi/workplace/extprojects/spark-0.8.0-incubating/core/src/main/scala/org/apache/spark/api/java/function/DoubleFlatMapFunction.java (at line 36)\n[error]         public final Iterable<Double> apply(T t) { return call(t); }\n[error]                                       ^^^^^^^^^^\n[error] The method apply(T) of type DoubleFlatMapFunction<T> must override a superclass method\n[error] ----------\n[error] 5 problems (1 error, 4 warnings)\n[error] (core/compile:compile) javac returned nonzero exit code", "comments": ["Which Java version do you have? You may need to use Java 6, or maybe your Java is somehow configured to treat those as errors instead of warnings.", "Currently compiling with java 1.6.0_13, javac Eclipse Java Compiler v_677_R32x, 3.2.1 release. Will check another version and let you know", "Yeah try the javac from OpenJDK or Oracle. My guess is that the Eclipse one is configured to treat those as errors.", "javac 1.7.0_25 compiled, thank you for the suggestion", "Marking this as \"won't fix\" since I don't think we want to support the Eclipse version of javac necessarily; maybe we'll revisit it later though so it's good to have this issue here for people to find."], "derived": {"summary": "Calling from RHEL5\nspark-0. 8.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Doesn't compile - Calling from RHEL5\nspark-0. 8."}, {"q": "What updates or decisions were made in the discussion?", "a": "Marking this as \"won't fix\" since I don't think we want to support the Eclipse version of javac necessarily; maybe we'll revisit it later though so it's good to have this issue here for people to find."}]}}
{"project": "SPARK", "issue_id": "SPARK-934", "title": "spark-mllib occasionally throw java.io.IOException (java.io.IOException: Corrupt data: overrun in decompress, input offset 51381, output offset 57509)", "status": "Resolved", "priority": "Major", "reporter": "Guoqiang Li", "assignee": null, "labels": [], "created": "2013-10-15T02:30:22.000+0000", "updated": "2014-08-20T08:22:06.000+0000", "description": "java.io.IOException (java.io.IOException: Corrupt data: overrun in decompress, input offset 51381, output offset 57509)\n\ncom.ning.compress.lzf.LZFDecoder.decompressChunk(LZFDecoder.java:346)\ncom.ning.compress.lzf.LZFDecoder.decompressChunk(LZFDecoder.java:192)\ncom.ning.compress.lzf.LZFInputStream.readyBuffer(LZFInputStream.java:254)\ncom.ning.compress.lzf.LZFInputStream.read(LZFInputStream.java:129)\njava.io.ObjectInputStream$PeekInputStream.read(ObjectInputStream.java:2309)\njava.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2322)\njava.io.ObjectInputStream$BlockDataInputStream.readDoubles(ObjectInputStream.java:3012)\njava.io.ObjectInputStream.readArray(ObjectInputStream.java:1691)\njava.io.ObjectInputStream.readObject0(ObjectInputStream.java:1342)\njava.io.ObjectInputStream.readArray(ObjectInputStream.java:1704)\njava.io.ObjectInputStream.readObject0(ObjectInputStream.java:1342)\njava.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1989)\njava.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1913)\njava.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796)\njava.io.ObjectInputStream.readObject0(ObjectInputStream.java:1348)\njava.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1989)\njava.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1913)\njava.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796)\njava.io.ObjectInputStream.readObject0(ObjectInputStream.java:1348)\njava.io.ObjectInputStream.readObject(ObjectInputStream.java:370)\norg.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:39)\norg.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:101)\norg.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)\nscala.collection.Iterator$$anon$21.hasNext(Iterator.scala:440)\norg.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:26)\norg.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:40)\norg.apache.spark.rdd.PairRDDFunctions$$anonfun$combineByKey$3.apply(PairRDDFunctions.scala:98)\norg.apache.spark.rdd.PairRDDFunctions$$anonfun$combineByKey$3.apply(PairRDDFunctions.scala:98)\norg.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:36)\norg.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237)\norg.apache.spark.rdd.RDD.iterator(RDD.scala:226)\norg.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:121)\norg.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:118)\nscala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)\nscala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:38)\norg.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:118)\norg.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237)\norg.apache.spark.rdd.RDD.iterator(RDD.scala:226)\norg.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:32)\norg.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237)\norg.apache.spark.rdd.RDD.iterator(RDD.scala:226)\norg.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:32)\norg.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237)\norg.apache.spark.rdd.RDD.iterator(RDD.scala:226)\norg.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:32)\norg.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237)\norg.apache.spark.rdd.RDD.iterator(RDD.scala:226)\norg.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:32)\norg.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237)\norg.apache.spark.rdd.RDD.iterator(RDD.scala:226)\norg.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:36)\norg.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237)\norg.apache.spark.rdd.RDD.iterator(RDD.scala:226)\norg.apache.spark.scheduler.ResultTask.run(ResultTask.scala:99)\norg.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:158)\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\njava.lang.Thread.run(Thread.java:724)", "comments": ["Could you provide some more details on what was the program being run and what was the input data etc. ?", "code:\nSystem.setProperty(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\nSystem.setProperty(\"spark.kryo.registrator\", classOfALSRegistrator.getName)\nSystem.setProperty(\"spark.kryo.referenceTracking\", \"false\")\nSystem.setProperty(\"spark.kryoserializer.buffer.mb\", \"8\")\nSystem.setProperty(\"spark.locality.wait\", \"10000\")\nALS.trainImplicit(ratings, rank, iterations)\nrank is 20\niterations is 20 \nratings.count is 3609314 \n\njava.lang.ArrayIndexOutOfBoundsException (java.lang.ArrayIndexOutOfBoundsException: 344)\n\norg.apache.spark.mllib.recommendation.ALS$$anonfun$updateBlock$1$$anonfun$apply$mcVI$sp$1.apply$mcVI$sp(ALS.scala:369)\nscala.collection.immutable.Range.foreach$mVc$sp(Range.scala:78)\norg.apache.spark.mllib.recommendation.ALS$$anonfun$updateBlock$1.apply$mcVI$sp(ALS.scala:366)\nscala.collection.immutable.Range.foreach$mVc$sp(Range.scala:78)\norg.apache.spark.mllib.recommendation.ALS.updateBlock(ALS.scala:365)\norg.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateFeatures$2.apply(ALS.scala:337)\norg.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateFeatures$2.apply(ALS.scala:336)\norg.apache.spark.rdd.MappedValuesRDD$$anonfun$compute$1.apply(MappedValuesRDD.scala:32)\norg.apache.spark.rdd.MappedValuesRDD$$anonfun$compute$1.apply(MappedValuesRDD.scala:32)\nscala.collection.Iterator$$anon$19.next(Iterator.scala:401)\nscala.collection.Iterator$$anon$21.hasNext(Iterator.scala:440)\nscala.collection.Iterator$class.foreach(Iterator.scala:772)\nscala.collection.Iterator$$anon$21.foreach(Iterator.scala:437)\norg.apache.spark.rdd.PairRDDFunctions.reducePartition$1(PairRDDFunctions.scala:176)\norg.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKeyLocally$1.apply(PairRDDFunctions.scala:191)\norg.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKeyLocally$1.apply(PairRDDFunctions.scala:191)\norg.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:36)\norg.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237)\norg.apache.spark.rdd.RDD.iterator(RDD.scala:226)\norg.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:107)\norg.apache.spark.scheduler.Task.run(Task.scala:53)\norg.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:210)\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\njava.lang.Thread.run(Thread.java:724)", "Seems org.apache.spark.serializer.KryoSerializer causing problems. Switch to org.apache.spark.serializer.JavaSerializer no problem."], "derived": {"summary": "java. io.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "spark-mllib occasionally throw java.io.IOException (java.io.IOException: Corrupt data: overrun in decompress, input offset 51381, output offset 57509) - java. io."}, {"q": "What updates or decisions were made in the discussion?", "a": "Seems org.apache.spark.serializer.KryoSerializer causing problems. Switch to org.apache.spark.serializer.JavaSerializer no problem."}]}}
{"project": "SPARK", "issue_id": "SPARK-935", "title": "Typo in documentation", "status": "Resolved", "priority": "Trivial", "reporter": "Rafal Kwasny", "assignee": null, "labels": [], "created": "2013-10-16T08:08:22.000+0000", "updated": "2013-12-07T14:40:16.000+0000", "description": "Small typo on:\nhttp://spark.incubator.apache.org/docs/latest/hadoop-third-party-distributions.html\n\ns/chd4/cdh4/\n\nfor cloudera artifacts\n", "comments": ["Fixed in https://github.com/apache/incubator-spark/pull/171"], "derived": {"summary": "Small typo on:\nhttp://spark. incubator.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Typo in documentation - Small typo on:\nhttp://spark. incubator."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed in https://github.com/apache/incubator-spark/pull/171"}]}}
{"project": "SPARK", "issue_id": "SPARK-936", "title": "Please publish jars for Scala 2.10", "status": "Resolved", "priority": "Major", "reporter": "Eric Christiansen", "assignee": null, "labels": [], "created": "2013-10-16T17:40:26.000+0000", "updated": "2013-12-11T09:26:31.000+0000", "description": "Please publish jars for the 2.10 branch. Currently, only jars for 2.9.3 are available: http://mvnrepository.com/artifact/org.apache.spark\n\n", "comments": ["We'll do this once we have official support for 2.10. For this, follow SPARK-994."], "derived": {"summary": "Please publish jars for the 2. 10 branch.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Please publish jars for Scala 2.10 - Please publish jars for the 2. 10 branch."}, {"q": "What updates or decisions were made in the discussion?", "a": "We'll do this once we have official support for 2.10. For this, follow SPARK-994."}]}}
{"project": "SPARK", "issue_id": "SPARK-937", "title": "Executors that exit cleanly should not have KILLED status", "status": "Resolved", "priority": "Critical", "reporter": "Aaron Davidson", "assignee": "Kan Zhang", "labels": [], "created": "2013-10-18T09:09:13.000+0000", "updated": "2014-06-15T21:57:50.000+0000", "description": "This is an unintuitive and overloaded status message when Executors are killed during normal termination of an application.", "comments": ["PR: https://github.com/apache/spark/pull/306"], "derived": {"summary": "This is an unintuitive and overloaded status message when Executors are killed during normal termination of an application.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Executors that exit cleanly should not have KILLED status - This is an unintuitive and overloaded status message when Executors are killed during normal termination of an application."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR: https://github.com/apache/spark/pull/306"}]}}
{"project": "SPARK", "issue_id": "SPARK-938", "title": "OpenStack Swift Storage Support", "status": "Resolved", "priority": "Minor", "reporter": "Murali Raju", "assignee": "Gil Vernik", "labels": [], "created": "2013-10-18T19:42:15.000+0000", "updated": "2014-09-08T03:58:05.000+0000", "description": "This issue is to track OpenStack Swift Storage support (development in progress) in addition to S3 for Spark.\n\n", "comments": ["hi, is there any follow-up on this issue?\n", "I am working on it. About to submit the patch. Almost done.", "Here we go: https://github.com/apache/spark/pull/1010\nThis is very  initial documentation, describing how to integrate Spark and Swift. ( uses stand alone mode of Spark )\nWill provide more patches with information how to integrate other cluster deployments of Spark with Swift and also how to use earlier versions of Hadoop.\n", "Issue resolved by pull request 2298\n[https://github.com/apache/spark/pull/2298]", "This was fixed by [~gvernik] with [~rxin] authoring a slightly revised version of the patch."], "derived": {"summary": "This issue is to track OpenStack Swift Storage support (development in progress) in addition to S3 for Spark.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "OpenStack Swift Storage Support - This issue is to track OpenStack Swift Storage support (development in progress) in addition to S3 for Spark."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was fixed by [~gvernik] with [~rxin] authoring a slightly revised version of the patch."}]}}
{"project": "SPARK", "issue_id": "SPARK-939", "title": "Allow user jars to take precedence over Spark jars, if desired", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Holden Karau", "labels": ["starter"], "created": "2013-10-19T14:54:38.000+0000", "updated": "2015-11-23T13:14:05.000+0000", "description": "Sometimes a user may want to include their own version of a jar that spark itself uses. For example, if their code requires a newer version of that jar than Spark offers. It would be good to have an option to give the users dependencies precedence over Spark. This options should be disabled by default, since it could lead to some odd behavior (e.g. parts of Spark not working). But I think we should have it.\n\nFrom an implementation perspective, this would require modifying the way we do class loading inside of an Executor. The default behavior of the  URLClassLoader is to delegate to it's parent first and, if that fails, to find a class locally. We want to have the opposite behavior. This is sometimes referred to as \"parent-last\" (as opposed to \"parent-first\") class loading precedence. There is an example of how to do this here:\n\nhttp://stackoverflow.com/questions/5445511/how-do-i-create-a-parent-last-child-first-classloader-in-java-or-how-to-overr\n\nWe should write a similar class which can encapsulate a URL classloader and change the delegation order. Or if possible, maybe we could find a more elegant way to do this. See relevant discussion on the user list here:\n\nhttps://groups.google.com/forum/#!topic/spark-users/b278DW3e38g\n\nAlso see the corresponding option in Hadoop:\nhttps://issues.apache.org/jira/browse/MAPREDUCE-4521\n\nSome other relevant Hadoop JIRA's:\nhttps://issues.apache.org/jira/browse/MAPREDUCE-1700\nhttps://issues.apache.org/jira/browse/MAPREDUCE-1938", "comments": ["I am interested in working on this issue. Please assign it to me.", "It's also worth considering something like the fancier classpath isolation stuff that was implemented for Hadoop later on: https://issues.apache.org/jira/browse/MAPREDUCE-1700\n", " Note that spark on yarn already supports putting user jars first - set config  spark.yarn.user.classpath.first to true.  The other parts mentioned are not supported. ", "Is this still relevant on YARN if someone uses sc.addJar()?", "No it probably doesn't work properly with addJars without the other changes Sandy mentioned.", "jars added through addJars on YARN are distributed to executors in the same way they are on standalone mode.  IIUC the changes proposed here are for how classes are loaded on the executor side, so they would still be relevant and likely work properly for Spark on YARN.", "https://github.com/apache/spark/pull/217", "Ok, but how does this actually work?  Any time we add Jackson 2.6.x to our classes then it crashes Spark when run on AWS EMR.  Obviously our JAR is not isolated from Spark.  ", "I see, this now reverses the problem.  We can crash Spark, but Spark can't crash us.   Maybe Spark can shade/rename common libraries it uses that are likely to conflict (Jackson is one example)."], "derived": {"summary": "Sometimes a user may want to include their own version of a jar that spark itself uses. For example, if their code requires a newer version of that jar than Spark offers.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Allow user jars to take precedence over Spark jars, if desired - Sometimes a user may want to include their own version of a jar that spark itself uses. For example, if their code requires a newer version of that jar than Spark offers."}, {"q": "What updates or decisions were made in the discussion?", "a": "I see, this now reverses the problem.  We can crash Spark, but Spark can't crash us.   Maybe Spark can shade/rename common libraries it uses that are likely to conflict (Jackson is one example)."}]}}
{"project": "SPARK", "issue_id": "SPARK-940", "title": "Do not directly pass Stage objects to SparkListener", "status": "Closed", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "labels": [], "created": "2013-10-20T13:05:39.000+0000", "updated": "2013-11-07T19:55:35.000+0000", "description": "Right now the SparkListener interface directly passes `Stage` objects to listeners. This is a problem because it exposes internal objects to Listeners, which is a semi-public interface. Consumers could, e.g. mutate the stages and break Spark.\n\nI recently found an even bigger reason this is a problem. It causes a bunch of extra pointers to RDD's and other objects to remain live when downstream consumers like the Web UI keep references to them. Even though the UI does its own cleaning, it will retain 1000 stages by default, which can reference a large number of RDD's. In a long running Spark streaming program I was running, these references caused the JVM to run out of memory and begin GC thrashing. A heap analysis later revealed that this was the cause.\n\nTo fix this, we should make `StageInfo` not just encapsulate a `Stage` (which it does now) and instead be similar to `TaskInfo` with its own distinct fields.", "comments": ["Hi Patrick\n\n   Have you started to involve in this issue? I would like manage to contribute this improvement.", "Hey [~andrew xia]. This issue is fixed (see the status is \"Resolved\"). I forgot to close it though."], "derived": {"summary": "Right now the SparkListener interface directly passes `Stage` objects to listeners. This is a problem because it exposes internal objects to Listeners, which is a semi-public interface.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Do not directly pass Stage objects to SparkListener - Right now the SparkListener interface directly passes `Stage` objects to listeners. This is a problem because it exposes internal objects to Listeners, which is a semi-public interface."}, {"q": "What updates or decisions were made in the discussion?", "a": "Hey [~andrew xia]. This issue is fixed (see the status is \"Resolved\"). I forgot to close it though."}]}}
{"project": "SPARK", "issue_id": "SPARK-941", "title": "Add javadoc and user docs for JobLogger", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "labels": [], "created": "2013-10-20T18:36:34.000+0000", "updated": "2014-03-21T14:59:00.000+0000", "description": null, "comments": ["Given the UI patch we're deprecating this. So adding more docs probably doesn't make sense."], "derived": {"summary": "", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Add javadoc and user docs for JobLogger"}, {"q": "What updates or decisions were made in the discussion?", "a": "Given the UI patch we're deprecating this. So adding more docs probably doesn't make sense."}]}}
{"project": "SPARK", "issue_id": "SPARK-942", "title": "Do not materialize partitions when DISK_ONLY storage level is used", "status": "Resolved", "priority": "Major", "reporter": "Kyle Ellrott", "assignee": "Kyle Ellrott", "labels": [], "created": "2013-10-20T21:36:09.000+0000", "updated": "2014-03-17T10:21:14.000+0000", "description": "If an operation returns a generating iterator (i.e. one that creates return values as the 'next' method is called), for example as the result of a 'flatMap' call on an RDD, the CacheManager first completely unrolls the iterator into an Array buffer before passing it to the blockManager (CacheManager.scala:74). Only after the entire iterator has been put into a buffer does it check if there is enough space in memory to store the data (BlockManager.scala:608). \nIn the attached test, the code can complete the operation of 'saveAsTextFile' of text strings if it is called directly on the result RDD of a flatMap operation, this is because it is given an iterator result, and works on the map-then-save operation as the results are generated. In the other branch, a 'persist' is called, and the cacheManger first tries to un-roll the entire iterator before deciding to store it too disk, this will cause a Memory Error (on systems with -Xmx512m)\n\nIn the cases where storing to disk is an option perhaps the CacheManager(or the BlockManager), can start to scan the iterator, calculating its size as the is pushed into a buffer as it goes (rather pushing everything into a buffer in a single operation), and if it determines that it will run out of memory, start pushing the already buffered portion of the iterator to disk, and then finish scanning the original iterator pushing that onto disk.\n\n\nExample Code (switch value of 'fail' variable to toggle behavior):\n\n{code:title=MemTest.scala|borderStyle=solid}\n\nimport org.apache.spark.SparkContext\nimport org.apache.spark.storage.StorageLevel\n\nclass Expander(base:String, count:Integer) extends Iterator[String] {\n  var i = 0;\n  def next() : String = {\n    i += 1;\n    return base + i.toString;\n  }\n  def hasNext() : Boolean = i < count;\n}\n\nobject MemTest {\n    def expand(s:String, i:Integer) : Iterator[String] = {\n      return new Expander(s,i)\n    }\n    def main(args:Array[String]) = {\n        val fail = false;\n        val sc = new SparkContext(\"local\", \"mem_test\");\n        val seeds = sc.parallelize( Array(\n          \"This is the first sentence that we will test:\",\n          \"This is the second sentence that we will test:\",\n          \"This is the third sentence that we will test:\"\n        ) ); \n        val out = seeds.flatMap(expand(_,10000000));\n        if (fail) {\n          out.map(_ + \"...\").persist(StorageLevel.MEMORY_AND_DISK).saveAsTextFile(\"test.out\")\n        } else {\n          out.map(_ + \"...\").saveAsTextFile(\"test.out\")\n        }\n    }\n} \n\n{code}", "comments": ["This happens with persist set to StorageLevel.DISK_ONLY as well.", "Fix submitted: https://github.com/apache/incubator-spark/pull/180", "I changed the title slightly here. This is a problem not only for generative iterators but even for normal `map` operations that happen to return large values. In all these cases we materialize the input partition even though, when writing to disk, we don't have to."], "derived": {"summary": "If an operation returns a generating iterator (i. e.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Do not materialize partitions when DISK_ONLY storage level is used - If an operation returns a generating iterator (i. e."}, {"q": "What updates or decisions were made in the discussion?", "a": "I changed the title slightly here. This is a problem not only for generative iterators but even for normal `map` operations that happen to return large values. In all these cases we materialize the input partition even though, when writing to disk, we don't have to."}]}}
{"project": "SPARK", "issue_id": "SPARK-943", "title": "Add `coalesce` and `repartition` to the streaming API", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "labels": [], "created": "2013-10-22T23:28:09.000+0000", "updated": "2013-10-25T10:08:20.000+0000", "description": "With streams, people often want to increase or decrease the level of parallelism. A good example is when the stream is ingested at a single node, but then fairly expensive operations happen at subsequent stages of the DAG. It would be good to add coalesce() and repartition() [this is just a new word I made up which is shorthand for coalesce with a larger # of partitions rather than smaller] directly to the DStream API.\n\nOnce we have this, we should also document it.", "comments": [], "derived": {"summary": "With streams, people often want to increase or decrease the level of parallelism. A good example is when the stream is ingested at a single node, but then fairly expensive operations happen at subsequent stages of the DAG.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add `coalesce` and `repartition` to the streaming API - With streams, people often want to increase or decrease the level of parallelism. A good example is when the stream is ingested at a single node, but then fairly expensive operations happen at subsequent stages of the DAG."}]}}
{"project": "SPARK", "issue_id": "SPARK-944", "title": "Give example of writing to HBase from Spark Streaming", "status": "Closed", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Tathagata Das", "labels": [], "created": "2013-10-23T22:35:38.000+0000", "updated": "2015-01-22T00:32:39.000+0000", "description": null, "comments": ["Hi Patrick. \n\nI do have working example of writing data to Spark Streaming but running into a into an issue with setting up checkpointing on the context as it is unable to serialize the org.apache.hadoop.mapred.JobConf object \n\nException in thread \"pool-6-thread-1\" java.io.NotSerializableException: org.apache.hadoop.mapred.JobConf \n\nAny suggestion on how we can fix this? I've tried couple of workarounds so far but no luck.\n\nThanks\nKanwal", "Hi Kanwaldeep, \n\nHave you been able to make any progress on this?\n", "Thanks for checking. Not so far. Any pointers to fix this would be very helpful.", "Can you explain the problem a little bit more. What is the spark streaming program doing? What is your cluster setup? Any logs and stacktraces, sample code for us reproduce the problem? Also, what version of Spark you are using?", "Currently using Spark 0.8 version but in the process of building using 0.8.1\n\nCluster Setup - Local for now\n\nCode \n\nobject KafkMetricStream {\n  def main(args: Array[String]) {\n    \n    if (args.length < 5) {\n      System.err.println(\"Usage: MetricStream <master> <zkQuorum> <group> <topics> <numThreads>\")\n      System.exit(1)\n    }\n\n    val Array(master, zkQuorum, group, topics, numThreads, hbaseCluster) = args\n\n    val ssc =  new StreamingContext(master, \"KafkaWordCount\", Seconds(2),\n      System.getenv(\"SPARK_HOME\"), Seq(System.getenv(\"SPARK_EXAMPLES_JAR\")))\n     ssc.checkpoint(\"checkpoint\")\n\n    val topicpMap = topics.split(\",\").map((_,numThreads.toInt)).toMap\n    val lines = ssc.kafkaStream(zkQuorum, group, topicpMap)\n    \n    val transformLines = lines.mapPartitions(ids => { ids.map(id => gsonconvert(id))\n    })\n  \n    \n    val appNodeMetrics = transformLines.flatMap(x => x.toList)\n  \n    // save data to HBase\n      \n    val conf = HBaseConfiguration.create()\n    conf.set(\"hbase.zookeeper.quorum\", hbaseCluster)\n    \n    val   jobConfig = new JobConf(conf)\n    \n    jobConfig.setOutputFormat(classOf[TableOutputFormat])\n    jobConfig.set(TableOutputFormat.OUTPUT_TABLE, \"sinet_test\")\n\n   appNodeMetrics.foreach(rdd =>  new PairRDDFunctions(rdd.map(convert)).saveAsHadoopDataset( jobConfig))\n   \n    \n    ssc.start()\n  }\n\n\nException Details\n\nException in thread \"Thread-35\" java.io.NotSerializableException: org.apache.hadoop.mapred.JobConf\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1165)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159)\n\tat java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1359)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1155)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535)\n\tat java.io.ObjectOutputStream.defaultWriteObject(ObjectOutputStream.java:422)\n\tat org.apache.spark.streaming.DStreamGraph.writeObject(DStreamGraph.scala:152)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:950)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1482)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:329)\n\tat org.apache.spark.streaming.CheckpointWriter.write(Checkpoint.scala:112)\n\tat org.apache.spark.streaming.Scheduler.doCheckpoint(Scheduler.scala:127)\n\tat org.apache.spark.streaming.Scheduler.generateJobs(Scheduler.scala:110)\n\tat org.apache.spark.streaming.Scheduler$$anonfun$1.apply$mcVJ$sp(Scheduler.scala:41)\n\tat org.apache.spark.streaming.util.RecurringTimer.org$apache$spark$streaming$util$RecurringTimer$$loop(RecurringTimer.scala:66)\n\tat org.apache.spark.streaming.util.RecurringTimer$$anon$1.run(RecurringTimer.scala:34)\nException in thread \"pool-5-thread-1\" java.io.NotSerializableException: org.apache.hadoop.mapred.JobConf\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1165)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159)\n\tat java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1359)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1155)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535)\n\tat java.io.ObjectOutputStream.defaultWriteObject(ObjectOutputStream.java:422)\n\tat org.apache.spark.streaming.DStreamGraph.writeObject(DStreamGraph.scala:152)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:950)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1482)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:329)\n\tat org.apache.spark.streaming.CheckpointWriter.write(Checkpoint.scala:112)\n\tat org.apache.spark.streaming.Scheduler.doCheckpoint(Scheduler.scala:127)\n\tat org.apache.spark.streaming.Scheduler.clearOldMetadata(Scheduler.scala:119)\n\tat org.apache.spark.streaming.JobManager.org$apache$spark$streaming$JobManager$$clearJob(JobManager.scala:79)\n\tat org.apache.spark.streaming.JobManager$JobHandler.run(JobManager.scala:41)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)\n\tat java.lang.Thread.run(Thread.java:695)\n", "Can you create the jobConfig object within the function in foreach(). So something like this.\n\n{code}\nappNodeMetrics.foreach(rdd => {\n    val jobConfig = new JobConf(conf)\n    jobConfig.setOutputFormat(classOfTableOutputFormat)\n    jobConfig.set(TableOutputFormat.OUTPUT_TABLE, \"sinet_test\")\n    new PairRDDFunctions(rdd.map(convert)).saveAsHadoopDataset( jobConfig))\n}\n{code}\n\n\nThat should most probably solve it.", "Thanks. I had to move the HBaseConfiguration in the foreach as well. \n\nI would like to understand if there is any performance impact in doing since for each rdd we create a new config which needs to establish connection with zookeeper and Hbase servers. Will this be done for each task that is created for a batch of streaming data.", "Well, if I understand correctly, just creating the jobConf object should not be creating any connection. The connection is made in the worker only when data needs to saved. And I believe it is done once per partition, per call to saveAsHadoopDataset(). So here the only cost of moving jobConfig into the foreach is that  a JobConf object every time the foreach function is called. I dont think that is too costly, probably takes a few extra milliseconds to create a JobConf object.\n", "Thanks a lot. I'd test it further. \n\nAny suggestions on how to best handle failure scenarios writing to HBase? I would like to stop reading messages from Kafka till the problem is resolved i..e make sure all the data read is aggregated and then persisted to HBase.", "That is tricky to do. Well, what spark streaming would guarantee that the Spark pushes the data into HBase wont complete until the pushing is over. And Spark jobs of next batch wont start until the previous Spark jobs is over. The system would keep receiving data from Kafka, storing them in memory, generating Spark jobs on them and queueing up the Spark jobs from processing. Which may be fine under your circumstances.\n\nIn future, we are thinking of adding flow control where the receiving will be slowed down if the queue of Spark jobs is too long.", "Thanks for the explanation this was my expectation as well. In my local cluster setup using 0.8 version, I don't see such a behavior. The tasks fail writing to HBase and when I bring the HBase cluster back online the failed jobs are not being retried. \n\nIs this a change in 0.8.1 version?", "Aah, if the job is marked as failed then nothing can be done. And job is marked as failed if any task has failed the maximum number of times. See the property spark.task.maxFailures\t in the Spark configuration guide. Setting that to a very high number maybe worth while. Then the task will keep failing while the Hbase is down, but the job will not failed until max task failures is reached.\n\nHowever, this in general may not be good idea if you expect your HBase to be down for a long time (long relative to the batch interval). Why does HBase have to be down for a long time?\n\nOn a related note, how are you writing to HBase, using saveAsHadoopFiles?\n", "Well HBase doesn't have to down for long time but I'm just trying to understand the worse case scenarios. I'd check out the spark task properties.\n\nWriting to HBase using saveAsHadoopDataset.", "[~kanwal] Do you have a working example now, that you would like to contribute?", "Sure. I'd share out an example using Spark Streaming and writing to HBase.\n\nKanwal\n\n\n\n", "Would be great if you can submit an example soon, would be great if we can make it to Spark 1.0 ;)", "Hi Tathagata\n\nI have the sample code ready and is attached.\n\nLet me know your feedback.\n\nThanks\nKanwal\n\n\n\n", "Hi Kanwal, the usual process of the contributing to Spark is through pull requests in Github. That way your name comes up in the change list as a contributor.\n\n", "Thank you for the assign.\n\nI will start working on this tomorrow.  We will review our approach to HBase and Spark and I will write the patch over the weekend.\n\nThanks again.", "I am closing this JIRA because this is not relevant any more. For examples, any reader take a look at \nhttps://github.com/cloudera-labs/SparkOnHBase/blob/cdh5-0.0.1/src/main/java/com/cloudera/spark/hbase/example/JavaHBaseStreamingBulkPutExample.java"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Give example of writing to HBase from Spark Streaming"}, {"q": "What updates or decisions were made in the discussion?", "a": "I am closing this JIRA because this is not relevant any more. For examples, any reader take a look at \nhttps://github.com/cloudera-labs/SparkOnHBase/blob/cdh5-0.0.1/src/main/java/com/cloudera/spark/hbase/example/JavaHBaseStreamingBulkPutExample.java"}]}}
{"project": "SPARK", "issue_id": "SPARK-945", "title": "Fix confusing behavior when assembly jars already exist", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "labels": [], "created": "2013-10-25T14:59:33.000+0000", "updated": "2014-03-30T04:15:01.000+0000", "description": "If you have multiple assembly jars (e.g. from doing builds with different hadoop versions) `spark-class` gives a very confusing message.\n\nWe should check for this explicitly. Also we should tell people do to `clean assembly` wherever we document this to avoid this happening.", "comments": [], "derived": {"summary": "If you have multiple assembly jars (e. g.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Fix confusing behavior when assembly jars already exist - If you have multiple assembly jars (e. g."}]}}
{"project": "SPARK", "issue_id": "SPARK-946", "title": "Reduce memory footprint of DiskBlockManager.blockToFileSegmentMap", "status": "Resolved", "priority": "Major", "reporter": "Aaron Davidson", "assignee": "Aaron Davidson", "labels": [], "created": "2013-10-26T13:53:24.000+0000", "updated": "2013-11-05T08:13:58.000+0000", "description": "blockToFileSegmentMap right now is taking up ~400 bytes per FileSegment. In large shuffles (e.g., >1000 mappers/executor and >1000 reducers), this can lead to several GB used just for this map, which is leading to OOMs.\n", "comments": ["Due to this issue, it'd also be useful if turning off shuffle file consolidation avoided this map entirely, rather than just further exploding the map size.", "Fixed by:\nhttps://github.com/apache/incubator-spark/pull/130\nhttps://github.com/apache/incubator-spark/pull/139"], "derived": {"summary": "blockToFileSegmentMap right now is taking up ~400 bytes per FileSegment. In large shuffles (e.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Reduce memory footprint of DiskBlockManager.blockToFileSegmentMap - blockToFileSegmentMap right now is taking up ~400 bytes per FileSegment. In large shuffles (e."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed by:\nhttps://github.com/apache/incubator-spark/pull/130\nhttps://github.com/apache/incubator-spark/pull/139"}]}}
{"project": "SPARK", "issue_id": "SPARK-947", "title": "Class path issue in case assembled with specific hadoop version", "status": "Resolved", "priority": "Minor", "reporter": "shekhar", "assignee": null, "labels": [], "created": "2013-10-28T02:33:11.000+0000", "updated": "2013-11-01T00:32:18.000+0000", "description": "assemble with hadoop version 1.2.1 using command SPARK_HADOOP_VERSION=1.2.1 sbt/sbt clean assembly\n\nnow we have two jars  $SPARK_HOME/assembly/target/scala-2.9.3/spark-assembly-0.8.0-incubating-hadoop1.2.1.jar and $SPARK_HOME/assembly/target/scala-2.9.3/spark-assembly-0.8.0-incubating-hadoop1.0.4.jar\n\ncompute class path script put both jars in classpath without adding \":\" between them and results in invalid jar name.\n\nclasspath computed by script::\n/data/installs/spark-0.8.0-incubating-bin-hadoop1/conf:/data/installs/spark-0.8.0-incubating-bin-hadoop1/assembly/target/scala-2.9.3/spark-assembly-0.8.0-incubating-hadoop1.0.4.jar/data/installs/spark-0.8.0-incubating-bin-hadoop1/assembly/target/scala-2.9.3/spark-assembly-0.8.0-incubating-hadoop1.2.1.jar:/data/installs/hadoop-1.2.1/conf", "comments": ["How did you generate multiple assembled jars? \n\nI just tried doing SPARK_HADOOP_VERSION=1.2.1 sbt/sbt clean assembly\n\nand only see one assembled jar:\n\nrxin @ rxin-air : ~/Downloads/spark-0.8.0-incubating-bin-hadoop1 \n> find . -name \"spark-assembly*\"\n./assembly/target/resolution-cache/org.apache.spark/spark-assembly_2.9.3\n./assembly/target/scala-2.9.3/spark-assembly-0.8.0-incubating-hadoop1.2.1.jar\n\n", "now I am not able to reproduce it with sbt clean assembly command but i see two jars when i run stb assembly command. steps to reproduce.\n\n\ndownload spark-0.8.0-incubating-bin-hadoop1.tgz\nextract it\n\nrun find command\n./assembly/target/scala-2.9.3/spark-assembly_2.9.3-0.8.0-incubating-hadoop1.0.4.jar\n./assembly/target/spark-assembly_2.9.3-0.8.0-incubating.jar\n./assembly/target/spark-assembly_2.9.3-0.8.0-incubating-sources.jar\n\n\nrun assembly command\n[admin@COL1 spark-0.8.0-incubating-bin-hadoop1]# SPARK_HADOOP_VERSION=1.2.1 sbt/sbt assembly\n\n\nrun find command again\n./assembly/target/scala-2.9.3/spark-assembly_2.9.3-0.8.0-incubating-hadoop1.0.4.jar\n./assembly/target/scala-2.9.3/spark-assembly-0.8.0-incubating-hadoop1.2.1.jar\n./assembly/target/spark-assembly_2.9.3-0.8.0-incubating.jar\n./assembly/target/spark-assembly_2.9.3-0.8.0-incubating-sources.jar\n./assembly/target/resolution-cache/org.apache.spark/spark-assembly_2.9.3\n", "Yea I think if you don't include \"clean\" and you have a different hadoop version, the assembly would create another jar resulting in two jars in the target directory.\n\nI am going to close this ticket because we already added to the spark-class script to throw a more meaningful error message when this situation happens. \n\nhttps://github.com/apache/incubator-spark/commit/4ba32678e04dc687a9f574eeeb1450e4d291ae1f"], "derived": {"summary": "assemble with hadoop version 1. 2.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Class path issue in case assembled with specific hadoop version - assemble with hadoop version 1. 2."}, {"q": "What updates or decisions were made in the discussion?", "a": "Yea I think if you don't include \"clean\" and you have a different hadoop version, the assembly would create another jar resulting in two jars in the target directory.\n\nI am going to close this ticket because we already added to the spark-class script to throw a more meaningful error message when this situation happens. \n\nhttps://github.com/apache/incubator-spark/commit/4ba32678e04dc687a9f574eeeb1450e4d291ae1f"}]}}
{"project": "SPARK", "issue_id": "SPARK-948", "title": "Move \"Classpath Entries\" in WebUI", "status": "Resolved", "priority": "Minor", "reporter": "Mark Hamstra", "assignee": null, "labels": [], "created": "2013-11-03T11:50:10.000+0000", "updated": "2014-11-21T15:11:34.000+0000", "description": "Currently the \"Classpath Entries\" section of the application environment page is way down at the bottom of the page, after the typically lengthy \"System Properties\" section.  That means that this useful information is usually not visible (or its existence apparent) without scrolling down a fair amount.  Since \"Classpath Entries\" is usually short, repositioning it ahead of \"System Properties\" would typically allow the classpaths to be seen along with the first few lines of \"System Properties\", and there would be no sections hidden off the bottom of the page.  Just a little better UX.", "comments": ["User 'srowen' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/3170", "I'll close as WontFix unless someone agrees with the move and commits the PR. It's quite minor and a question of taste."], "derived": {"summary": "Currently the \"Classpath Entries\" section of the application environment page is way down at the bottom of the page, after the typically lengthy \"System Properties\" section. That means that this useful information is usually not visible (or its existence apparent) without scrolling down a fair amount.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Move \"Classpath Entries\" in WebUI - Currently the \"Classpath Entries\" section of the application environment page is way down at the bottom of the page, after the typically lengthy \"System Properties\" section. That means that this useful information is usually not visible (or its existence apparent) without scrolling down a fair amount."}, {"q": "What updates or decisions were made in the discussion?", "a": "I'll close as WontFix unless someone agrees with the move and commits the PR. It's quite minor and a question of taste."}]}}
{"project": "SPARK", "issue_id": "SPARK-949", "title": "Turn DAGScheduler into an Actor", "status": "Resolved", "priority": "Major", "reporter": "Frank Dai", "assignee": "liancheng", "labels": [], "created": "2013-11-05T19:24:15.000+0000", "updated": "2013-11-14T18:03:11.000+0000", "description": "At present {{DAGScheduler}} creates a thread to process events in the {{eventQueue}}, this is a typical situation where Actor is suitable.\n\nSo how about turn {{DAGScheduler}} into an actor, and send events as messages to it?\n\nThe new {{DAGScheduler}} will be more efficient and the code will be shorter.  Maybe {{JobWaiter}} can also be replaced with {{Future}}.", "comments": ["Fixed in https://github.com/apache/incubator-spark/pull/159"], "derived": {"summary": "At present {{DAGScheduler}} creates a thread to process events in the {{eventQueue}}, this is a typical situation where Actor is suitable. So how about turn {{DAGScheduler}} into an actor, and send events as messages to it?\n\nThe new {{DAGScheduler}} will be more efficient and the code will be shorter.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Turn DAGScheduler into an Actor - At present {{DAGScheduler}} creates a thread to process events in the {{eventQueue}}, this is a typical situation where Actor is suitable. So how about turn {{DAGScheduler}} into an actor, and send events as messages to it?\n\nThe new {{DAGScheduler}} will be more efficient and the code will be shorter."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed in https://github.com/apache/incubator-spark/pull/159"}]}}
{"project": "SPARK", "issue_id": "SPARK-950", "title": "Use faster random number generator for sampling in K-means", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Marek Kolodziej", "labels": ["Starter"], "created": "2013-11-06T13:34:46.000+0000", "updated": "2013-12-07T14:34:22.000+0000", "description": "The k-means|| initialization algorithm in MLlib is fairly CPU-bound due to a lot of sampling across the datasets. It uses java.util.Random, but there exist faster RNGs that have equally good properties, like XORshift (http://www.javamex.com/tutorials/random_numbers/xorshift.shtml#.Unq1sZHnaMM). Implementing one of these instead might help.\n\nIf it's successful, we can also consider using it for RDD.sample().", "comments": ["I looked into it this morning and it turns out that XORShift is about 3.5x faster than java.util.Random - it generates 100 million random doubles in 1,254 ms instead of 3,928 ms for java.util.Random. This is based on subclassing java.util.Random and overriding next().\n\nHowever, this is only true in a naive implementation like this one: \n\nhttp://www.javamex.com/tutorials/random_numbers/java_util_random_subclassing.shtml\n\nThe above performance improvement disappears when trying to make XORShift as thread-safe as java.util.Random, e.g. by making it use an AtomicLong as its seed. When I did that, XORShift was actually a bit slower than java.util.Random - in this particular run, it ran for 4,303 ms as compared to 3,957 ms for java.util.Random.\n\nApparently the speed issue has to do with thread safety rather than java.util.Random's LCG algorithm being slow. Would lack of thread rsafety only affect Spark in local mode since in distributed mode we'd be dealing with separate JVMs? Also, it would seem to me that if each thread had its own instance of the Random object (LCG or XORShift), we wouldn't need AtomicLong with its compare-and-swap. \n\nLastly, even if we retain thread safety and the XORShift algorithm ends up not running faster, it could have substantially better statistical properties, which could be important for data science applications that heavily rely on random number generation (sampling, Monte Carlo simulations, bootstrapping, random weight matrix initialization of neural networks, more accurate draws from the normal distribution using nextGaussian(), etc.).", "Hi Marek,\n\nWe don't need it to be thread-safe -- no thread-safe implementation would scale well. If you look at our code, we're creating one Random per task. It's fine if this doesn't even extend Random and if it's is documented as not being thread-safe.", "Hi Matei,\n\nThat's what I thought, but I wanted to double-check. This would mean that the speed-up would simply have to do with turning off the thread safety while subclassing Random, rather than with an algorithmic change. However, the XORShift algorithm would give better statistical results beyond that, so I think it would be worth it.\n\nFeel free to officially assign this Jira ticket to me, and I'll go ahead and start integrating the code and testing it.\n\nThanks!\n\nMarek", "I'm in a bit of a predicament. Most of Spark's code is in Scala, and I would love to write the new random number generator in Scala. However, I wrote both a Scala and a Java version, and the Scala version performs half as fast. The Scala code is an exact port of the Java code (save for the uniquifier preincrement in Java and postincrement in Scala, which is immaterial) -  so it's imperative and mutable, rather than functional and immutable - therefore, one can't talk about the pentalty having to do with immutable constructs, or the JVM not inlining lambdas efficiently (http://www.azulsystems.com/blog/cliff/2011-04-04-fixing-the-inlining-problem). Here's the sample code. On my laptop, the Java code took 426 ms to run, while the Scala code took 873 ms. Also note that java.util.Random took 1,385 ms. Also note that these estimates were based on 100 million calls to nextInt() - the Scala version performs much worse if nextDouble() or nextGaussian() is called - which may not be important for the use of the new random number generator for sampling, but it may be important for other data science applications such as Monte Carlo simulations. \n\nMy recommendation is that for performance reasons, the random number generator be implemented in Java. Apparently there are cases in which Scala's compiler doesn't optimize as well as Java's. I would like to get feedback before proceeding. I'm pasting the code below. Note that the code formatting isn't necessarily the target formatting/style that would be committed - Jira is changing my original formating by introducing undesired inlining, etc.\n\n{code}\n// Java\n\npublic class XORShiftRandom extends Random {\n\t\n\tprivate static final long serialVersionUID = -6177051610615407537L;\n\t\n\t// same as in java.util.Random - including here since it can't be inherited\n\tprivate static volatile long seedUniquifier = 8682522807148012L;\t\n\tprivate long seed = System.nanoTime();\n\n\t  public XORShiftRandom() {\n\t\t  \n\t\t  this(++seedUniquifier + System.nanoTime());\n\t  }\n\t  \n\t  public XORShiftRandom(final long seed) {\n\t\t  \n\t\t  super();\n\t\t  this.seed = seed;\n\t  }\n\t  \n\t  @Override\n\t  protected int next(final int bits) {\n     \n\t\t  long nextSeed = seed ^ (seed << 21);\n\t\t  nextSeed ^= (nextSeed >>> 35);\n\t\t  nextSeed ^= (nextSeed << 4);\t  \n\t      seed = nextSeed;\n\t\t\n\t    return (int) (nextSeed & ((1L << bits) -1));\n\t\n\t  }\n}\n{code}\n\n{code}\n// Scala \nimport XORShiftRandom.seedUniquifier\n\nclass XORShiftRandom(init: Long) extends java.util.Random(init) {\n  \n  def this() = {\n    this(seedUniquifier + System.nanoTime())\n    seedUniquifier += 1\n  }\n  \n  var seed = init\n  \n  override protected def next(bits: Int): Int = {\n    \n    var nextSeed = seed ^ (seed << 21)\n    nextSeed ^= (nextSeed >>> 35)\n    nextSeed ^= (nextSeed << 4)  \n    seed = nextSeed\n    (nextSeed & ((1L << bits) -1)).asInstanceOf[Int]\n  }\n}\n\n\nobject XORShiftRandom {\n  \n  val serialVersionUID = -6177051610615407537L\n  var seedUniquifier = 8682522807148012L\n}\n\n{code}", "BTW, the substantially varying benchmark execution times have to do with my switching machines, but for any given comparison, the execution was obviously done on the same machine - e.g. the Java/Scala comparisons from the post above were done on one machine, while the previous java.util.Random and XORShift results were obtained on another - but the comparison done at a given time was done on the same hardware.", "What if you change the definition of seed from var to \n\n{code}\nprivate[this] var\n{code}", "Thanks Reynold! I tried it, and it doesn't confer an execution speed benefit. Of course it is nicer from an access control perspective, but the runtime bahavior remained the same. The testing was done with Scala 2.9.3 and OpenJDK/JRE 7 on Ubuntu, but I also tested with Sun JRE 7 on OS X.\n\nThe irony is that the bytecode for the next() method is exactly the same for Scala as it is for Java.\n\nJava:\n\n{code}\nprotected int next(int);\n  Code:\n   Stack=6, Locals=4, Args_size=2\n   0:\taload_0\n   1:\tgetfield\t#5; //Field seed:J\n   4:\taload_0\n   5:\tgetfield\t#5; //Field seed:J\n   8:\tbipush\t21\n   10:\tlshl\n   11:\tlxor\n   12:\tlstore_2\n   13:\tlload_2\n   14:\tlload_2\n   15:\tbipush\t35\n   17:\tlushr\n   18:\tlxor\n   19:\tlstore_2\n   20:\tlload_2\n   21:\tlload_2\n   22:\ticonst_4\n   23:\tlshl\n   24:\tlxor\n   25:\tlstore_2\n   26:\taload_0\n   27:\tlload_2\n   28:\tputfield\t#5; //Field seed:J\n   31:\tlload_2\n   32:\tlconst_1\n   33:\tiload_1\n   34:\tlshl\n   35:\tlconst_1\n   36:\tlsub\n   37:\tland\n   38:\tl2i\n   39:\tireturn\n{code}\n\nScala:\n\n{code}\npublic int next(int);\n  Code:\n   Stack=6, Locals=4, Args_size=2\n   0:\taload_0\n   1:\tgetfield\t#31; //Field seed:J\n   4:\taload_0\n   5:\tgetfield\t#31; //Field seed:J\n   8:\tbipush\t21\n   10:\tlshl\n   11:\tlxor\n   12:\tlstore_2\n   13:\tlload_2\n   14:\tlload_2\n   15:\tbipush\t35\n   17:\tlushr\n   18:\tlxor\n   19:\tlstore_2\n   20:\tlload_2\n   21:\tlload_2\n   22:\ticonst_4\n   23:\tlshl\n   24:\tlxor\n   25:\tlstore_2\n   26:\taload_0\n   27:\tlload_2\n   28:\tputfield\t#31; //Field seed:J\n   31:\tlload_2\n   32:\tlconst_1\n   33:\tiload_1\n   34:\tlshl\n   35:\tlconst_1\n   36:\tlsub\n   37:\tlando\n   38:\tl2i\n   39:\tireturn\n{code}\n\nIf the bytecode is identical, then why is Scala half as fast as Java?\n\nIs there a way to get a hold of a profiling tool like YourKit? I'm not yet an Apache committer but I don't have a profiler in Eclipse. JVisualVM didn't show anytthing beyond the main() method, it didn't show the call to next() of the XORShiftRandom object. I know that a real profiler would show where the bottleneck is.", "How are you testing the runtime? ", "So far I just ran the tests in Eclipse, using Scala and Java runtimes, respectively. Of course since the Scala runtime is actually just the Java runtime with scala-library.jar etc. on the classpath, the results should theoretically be the same.\n\n{code}\nobject SpeedTest extends App {\n  \n  val r = new XORShiftRandom()\n  \n  val start = System.currentTimeMillis\n  for (i <- 1 to 100000000) r.nextInt\n  val end = System.currentTimeMillis\n  \n  println(end - start) \n}\n{code}\n\n{code}\npublic class RandomTest {\n\n\tpublic static void main(final String[] args) {\n\t\t\n\t    final Random rand = new XORShiftRandom();\n\t\t\n\t    final long start = System.currentTimeMillis();\n\t    \n\t    for (int i = 0; i <= 100000000; i++) {\n\t    \t\n\t    \trand.nextInt();\n\t    }\n\t    \n\t    final long end = System.currentTimeMillis();\n\t    \n\t    System.out.println(end - start);\n\t\t\n\t}   \n\n}\n{code}\n\n\nAt first I thought that it was an Eclipse config/env. var. issue, but I compiled the classes and ran them from the command line, with the same results.\n\nLike I mentioned, I couldn't learn anything interesting using JVisualVM, because it doesn't seem to show nested calls the way YourKit, JProfiler or even the Netbeans profiler would.", "Ah I see. Try this with Scala (basically replace the for loop with the while loop to avoid closure in this case)\n\n{code}\nobject SpeedTest extends App {\n  \n  val r = new XORShiftRandom()\n  var sum = 0\n  val start = System.currentTimeMillis\n  var i = 0\n  while (i < 100000000) {\n    sum |= r.nextInt\n    i += 1\n  }\n  val end = System.currentTimeMillis\n  \n  println(end - start) \n  println(sum)\n}\n{code}\n\nYou should also change the Java implementation to do the same thing with a sum variable to make sure JIT doesn't eliminate the nextInt call as dead code ...", "Interesting. In the case of Scala, I forgot that for is really just syntactic sugar for map/flatMap/filter, so that could be part of the problem. Would the JIT really not optimize a regular Java for loop? I mean yes, we're not holding state, but the code is heavily iterative.", "The JIT and Scala compiler does indeed not fully optimize a Range.foreach().\n\nBy the way, are you running this multiple times in the same process? Usually the JIT takes time to compile code, and you don't want it to be warming up for different amounts of time based on other stuff in the program. The best way to run such benchmarks is to do them 3-4 times in the same process and see if the running times stabilize. You could also try a tool like this: https://code.google.com/p/caliper/.", "Hi Matei, very good point. I should have indeed let it stabilize. I will check out Caliper. By the way, are there good references for what the standard Hotspot optimizes, and which features of Scala's core are not optimized?", "Also, while the XORShift algorithm had already been peer-reviewed and had been showed to pass more statistical tests than the LCG algorithm used by java.util.Random, I was wondering if you would require one or more regression tests for the test suite? If so, I was thinking that a low-hanging fruit case would be a chi-square test. Including a Kolmogorov-Smirnov, serial correlation and other tests would be useful for proving that a particular algorithm has good statistical properties overall, but we already know that with XORShift. I gues s a chi-square test would be good to prevent fatal code regression that would negatively affect sampling. Of course, given a fixed seed, we don't even need a statistical test, we could just test the bins for exact values rather than for passing at some particular significance level. Any preferences with regard to regression tests? \n\nFinally, is it OK to just run some benchmarks using actual datasets, or microbenchmarks for the RNG and trying out the execution speed with java.util.Random and XORShift, or would you like benchmarking tests as part of the test suite? In the latter case it might change the code base since it would require one to provide the particular RNG implementations for testing, so it could use implicit vals that are overridden for testing purposes. The question is though whether that would be necessary in the long run, or just important for the sake of proving the concept of the new RNG being faster.", "As far as tests go, I think simpler is better, as ultimately the community has to be able to maintain them. As you said, we don't need to prove statistical soundness of the algorithm, just correctness of the implementation. Just a chi-squared test over a hundred million values would sound reasonable to me.\n\nI also think that just adding a test to compare the execution of generating N random numbers using XORShift versus Random would be good. Not only would this demonstrate in code that the new generator has a real benefit over Random and prevent regressions, but it would also allow us to be alerted if future Java implementations become faster.", "Thanks Aaron! In that case, I'll include a chi-squared test for regressions, and a microbenchmark for a comparison against java.util.Random's speed. In fact, it could all be done in one execution, but as two assertions. ", "After the JIT warm-up time (> 10k iterations for both Random instance), the benchmarks that I got for XORShiftRandom and java.util.Random were ~460 ms and 1,200 ms, respectively. This means that XORShift (without Random's thread safety) took only 38% of the time it took java.util.Random to generate 100 million random numbers. I wrapped the while loop in a class and an implicit def to ease the pain of writing loops that optimize well yet are convenient to use. In Scala 2.10+, I would have used an implicit class, but in 2.9.3 I used a class and an implicit def.\n\n{code}\n  class TimesInt(i: Int) {\n    def times(f: => Unit) = {\n      var x = 1\n      while (x <= i) {\n        f\n        x += 1 \n      }\n    }\n  }\n\nimplicit def intToTimesInt(i: Int) = new TimesInt(i)\n\ndef timeIt(f: => Unit, iters: Int): Long = {\n\n    val start = System.currentTimeMillis\n    iters.times(f)\n    System.currentTimeMillis - start\n\n  }\n{code}\n\n\nHere's a sample result from the run after the JIT had a chance to work.\n\n{code}\nscala> timeIt(javaRand.nextInt, 100000000)\nres10: Long = 1199\n\nscala> timeIt(xorShift.nextInt, 100000000)\nres11: Long = 379\n{code}\n\nRegarding applying unit tests to get specific performance improvements of XORShift over java.util.Random, it's clear that the exact numbers will not be deterministic, or even approximate if the build tool is doing a lot of things in a multithreaded way that can overwhelm the CPU. For instance, if we compare the above results to ones that show up by running the test in SBT, we will see that the SBT results are worse:\n\n{code}\n> test-only org.apache.spark.util.RandomNumberGeneratorSuite\n[info] RandomNumberGeneratorSuite:\njavaTime 2117.0\nxorTime 1275.0\n[info] - XORShift should be faster than java.util.Random\n[info] - XORShift generates valid random numbers\n[info] Passed: : Total 2, Failed 0, Errors 0, Passed 2, Skipped 0\n{code}\n\nThus, I think that for the simple timing test in the test suite, it would be enough to check that the XORShiftRandom RNG is faster than java.util.Random, but even that might not be enough in non-deterministic utilization of the JVM. So perhaps it would be enough to do the microbenchmarks and a real-world dataset timing with the old and new RNGs, and just keep the chi-squared test as a regression test. However, Aaron's point regarding the test that verifies that the new RNG is faster than java.util.Random would be helpful in case the JDK one improves in the future.\n\nI have the code and the unit tests, and will issue a pull request once I verify the execution performance against a reasonably-sized dataset. That is my plan for tomorrow.", "I wouldn't worry about putting performance tests in unit tests. If you want, you can make a short standalone program (i.e. a scala object with main) to evaluate performance, and check that into the codebase. But really no need to run that every time unit tests are run. ", "Great! I also thought it would be more reasonable, from both the unit test execution time perspective and the deterministic aspect.", "I am glad to report that the savings for even a modest-sized dataset are considerable.\n\nI ran K-means in stand-alone mode over a single 386 MiB file on a 4-core, 16 GiB machine, with 4 threads. The file contained 5 million records. Each line looked as follows:\n\n0002322919CBB308E9FCC984D438164CEE201F9D,0,19,0,0,0,0,0,0,0,0,0,0,0,0,0,0,27,0,0,0\n\nwhere the first element was a user ID and the next 20 elements represented the number of times that a user watched a video from a given IAB (Internet Advertising Bureau) category. This dataset is private to my company so I can't share it, but I used it for the benchmark to test a real-world case.\n\nI ran Spark with both the java.util.Random and the XORShift RNG implementation 10 times, and the average execution times were as follows:\n\njava.util.Random: 614.5 seconds\nXORShift:             600.6 seconds\n\nThus, for this particular test, the time savings were ~2.3%. Note that I didn't just change the RNG in K-means.scala for this, I applied it to RDD.scala as well.\n\nI was expecting the savings in the case of K-means to be relatively small. However, I would like to make the claim that changing the RNG in the RDD class might have a much larger benefit for certain cases that require more random numbers to be generated, e.g. for Monte Carlo simulations, bootstrap algorithms, etc. So, with K-means we probably see a lower bound of the benefit, and other algorithms could benefit even more from faster sampling.\n\nI will issue a pull request today.", "I just sent the pull request: https://github.com/apache/incubator-spark/pull/185\n\nNote that in addition to implementing the XORShift RNG and adding a unit test, I also updated KMeans.scala and RDD.scala to use the new RNG. I mentioned the \"real-world\" dataset time savings in my previous JIRA comment. For a microbenchmark, try the one described in the Git commit message."], "derived": {"summary": "The k-means|| initialization algorithm in MLlib is fairly CPU-bound due to a lot of sampling across the datasets. It uses java.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Use faster random number generator for sampling in K-means - The k-means|| initialization algorithm in MLlib is fairly CPU-bound due to a lot of sampling across the datasets. It uses java."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just sent the pull request: https://github.com/apache/incubator-spark/pull/185\n\nNote that in addition to implementing the XORShift RNG and adding a unit test, I also updated KMeans.scala and RDD.scala to use the new RNG. I mentioned the \"real-world\" dataset time savings in my previous JIRA comment. For a microbenchmark, try the one described in the Git commit message."}]}}
{"project": "SPARK", "issue_id": "SPARK-951", "title": "Gaussian Mixture Model", "status": "Resolved", "priority": "Critical", "reporter": "caizhua", "assignee": null, "labels": ["Learning", "Machine", "Model"], "created": "2013-11-09T21:30:13.000+0000", "updated": "2014-11-08T09:38:47.000+0000", "description": "This code includes the code for Gaussian Mixture Model. The input file named Gmm_spark.tbl is the input for this program.", "comments": ["caizhua Could you please elaborate a little more on the issue? right now 'This code' and 'input file named Gmm_spark.tbl' are unknown to me at the time of reading this", "Duplicate of I assume this is superseded, if anything, by SPARK-3588"], "derived": {"summary": "This code includes the code for Gaussian Mixture Model. The input file named Gmm_spark.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Gaussian Mixture Model - This code includes the code for Gaussian Mixture Model. The input file named Gmm_spark."}, {"q": "What updates or decisions were made in the discussion?", "a": "Duplicate of I assume this is superseded, if anything, by SPARK-3588"}]}}
{"project": "SPARK", "issue_id": "SPARK-952", "title": "Python version of Gaussian Mixture Model", "status": "Resolved", "priority": "Minor", "reporter": "caizhua", "assignee": null, "labels": ["Learning"], "created": "2013-11-09T21:37:26.000+0000", "updated": "2014-11-08T09:38:22.000+0000", "description": "This piece of code is written by Shangyu Luo at Rice University. The code is to learn the Gaussian Mixture Model.", "comments": ["I assume this is superseded, if anything, by SPARK-3588"], "derived": {"summary": "This piece of code is written by Shangyu Luo at Rice University. The code is to learn the Gaussian Mixture Model.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Python version of Gaussian Mixture Model - This piece of code is written by Shangyu Luo at Rice University. The code is to learn the Gaussian Mixture Model."}, {"q": "What updates or decisions were made in the discussion?", "a": "I assume this is superseded, if anything, by SPARK-3588"}]}}
{"project": "SPARK", "issue_id": "SPARK-953", "title": "Latent Dirichlet Association (LDA model)", "status": "Resolved", "priority": "Critical", "reporter": "caizhua", "assignee": null, "labels": [], "created": "2013-11-09T21:44:44.000+0000", "updated": "2014-08-27T17:37:10.000+0000", "description": "This code is for learning the LDA model. However, if our input is 2.5 M documents per machine, a dictionary with 10000 words, running in EC2 m2.4xlarge instance with 68 G memory each machine. The time is really really slow. For five iterations, the time cost is 8145, 24725, 51688, 58674, 56850 seconds. The time for shuffling is quite slow. The LDA.tbl is the simulated data set for the program, and it is quite fast.", "comments": ["Latent Dirichlet Allocation?", "parallel gibbs sampling for lda (plda) may be usable."], "derived": {"summary": "This code is for learning the LDA model. However, if our input is 2.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Latent Dirichlet Association (LDA model) - This code is for learning the LDA model. However, if our input is 2."}, {"q": "What updates or decisions were made in the discussion?", "a": "parallel gibbs sampling for lda (plda) may be usable."}]}}
{"project": "SPARK", "issue_id": "SPARK-954", "title": "One repeated sampling, and I am not sure if it is correct.", "status": "Resolved", "priority": "Major", "reporter": "caizhua", "assignee": null, "labels": [], "created": "2013-11-09T21:53:41.000+0000", "updated": "2014-11-08T09:42:12.000+0000", "description": "This piece of code reads the dataset, and then has two operations on the dataset. If I consider the RDD as a view definition, I think the result is correct. However, since the first iteration does result_sample.count(), then I was wondering whether we should repeat the computation in the initialize_doc_topic_word_count(.) function, when we run the the second result_sample.map(lambda (block_id, doc_prob): doc_prob).count(). Since people write Spark as a program not as a database view, sometimes it is confusing. For example, considering there  initialize_doc_topic_word_count(.)  is a statistical function with runtime seeds, I am not sure if this have impact on the result.", "comments": ["Spark's lineage-based fault-tolerance assumes that your RDD transformations are deterministic.  Your code is seeding a shared RNG based on the time that the task is executed on the worker:\n\n{code}\ndef initialize_doc_topic_word_count(doc_word_count_list, topic_size, dic_size):\n\t# initialize the seed\n\timport pygsl.rng\n\tr = pygsl.rng.rng()\n\tst = datetime.now()\n\tseed = st.hour*60*60+st.minute*60+st.second\n\tseed = seed*1000\n\tprint \"--------------------------step 0: initialization-------------------------------\"\t\n\talpha = np.zeros(topic_size, float)\n\talpha.fill(1.0)\n\t\n\tdoc_prob = {}\n\tfor (doc_id, word_count_list) in doc_word_count_list:\n\t\tr.set(seed+doc_id)\n\t\tprob = r.dirichlet(alpha)\n\t\tdoc_prob[doc_id] = prob\n\t\t\n\treturn \tdoc_prob\n{code}\n\nIf a task that uses this function is recomputed, it won't produce the same result because the RNG seed will have changed.  In your example, this is leading to incorrect results: \n\n{code}\n\tresult_sample = block_doc_word_count.map(lambda (block_id, doc_word_count_list): (block_id, initialize_doc_topic_word_count(doc_word_count_list, topic_size, dic_size)))\n\tprint \"count before projection = \", result_sample.count()\n\t\n\tprint \"count after projection = \",  result_sample.map(lambda (block_id, doc_prob): doc_prob).count()\n{code}\n\nHere, the {{result_sample.map()}} is evaluated twice with different RNG seeds, leading to different results.\n\nTo avoid these issues, you should ensure that your transformations are deterministic.  In this case, I recommend generating a list of RNG seeds in the driver program and writing your map function using {{mapPartitionsWithIndex}}; this will allow you to index into the list of RNG seeds based on the current partition index, ensuring determinism.", "That is correct! I agree with that, and it should be correct for keeping trace of the randomness. If RDD was consider as a view in database, it is correct as I said. The question I have is whether it should be considered as a view, since the data flow itself can be considered as a logical plan for database SQL queries. And like this case, block_doc_word_count is a vertex in a graph, and it has two leaves, whether the DAG should recognize it and save the computation. \n\nAnyway, I did not intend to report it as a bug, just as a story. :) I like Spark very much.", "The Spark scheduler doesn't get to see a logical plan up-front for your entire-program.  At the time that you call the first {{count()}} action, Spark doesn't know that you're going to subsequently call another {{map()}} and {{count()}} on the same RDD; it only knows about the portion of the DAG that's been defined up to that point in the driver program.\n\nWe can express fairly general DAGs, but the current scheduler doesn't allow us to run multiple actions/queries in parallel and share common parts of their query plans.  In many cases, you can work around this by using accumulators (so you could perform the pre-projection {{count()}} inside the projection's {{map()}} transformation).  (Aside: accumulators currently have slightly different semantics than running two actions in parallel; see SPARK-732)\n\nEven if we did have automatic caching (I think that it's still possible, even in the current model), it will be risky to have programs whose output changes depending on whether an RDD is cached or not.", "From the discussion, and later ones about guarantees of determinism in RDDs, sounds like this is working as intended."], "derived": {"summary": "This piece of code reads the dataset, and then has two operations on the dataset. If I consider the RDD as a view definition, I think the result is correct.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "One repeated sampling, and I am not sure if it is correct. - This piece of code reads the dataset, and then has two operations on the dataset. If I consider the RDD as a view definition, I think the result is correct."}, {"q": "What updates or decisions were made in the discussion?", "a": "From the discussion, and later ones about guarantees of determinism in RDDs, sounds like this is working as intended."}]}}
{"project": "SPARK", "issue_id": "SPARK-955", "title": "Paritions increase expotentially when doing cartisian product in loop program", "status": "Resolved", "priority": "Major", "reporter": "caizhua", "assignee": null, "labels": [], "created": "2013-11-09T22:01:07.000+0000", "updated": "2013-11-14T17:48:11.000+0000", "description": "Paritions increase expotentially when doing cartisian product in loop program. An example program is as follows:\n\nblock_doc_word_count = lines.map(parseVector).groupBy(lambda (doc_id, word_count): (doc_id % doc_block)) \n\nblock_dwc_dtp_twc = block_doc_word_count.map(lambda (block_id, doc_word_count_list): (block_id, initialize_doc_topic_word_count(doc_word_count_list, topic_size, dic_size))).persist(StorageLevel.MEMORY_ONLY)\n\nfor loop  in range(0, max_superstep):\n\n\t\ttopic_word_count_map = block_dwc_dtp_twc.flatMap(lambda (block_id, (doc_word_count_list, doc_prob, topic_word_count)): topic_word_count).reduceByKey(lambda para1, para2: addWordCount(para1, para2))\n\n\t\ttopic_word_prob_block = topic_word_count_map.mapValues(lambda wordcount_map: sample_topic_word_prob(wordcount_map, dic_size)).groupBy(lambda (topic_id, topic_word_prob): topic_id % 1)\n\n\t\tblock_dwc_dtp_twc = block_dwc_dtp_twc.cartesian(topic_word_prob_block).map(lambda (block_dwc_dtp_twc_temp, topic_word_prob_block_temp): sample_doc_word_topic(block_dwc_dtp_twc_temp, topic_word_prob_block_temp, topic_size, dic_size, loop))\n\nThe program and its input are as the affixed files.\n\t\n\t", "comments": ["This is not a bug -- cartesian(rdd1, rdd2) does return an rdd with (rdd1.numPartitions * rdd2.numPartitions) partitions. In order to avoid having too many partitions, you can use the coalesce() function to reduce the number of partitions after doing the cartesian product."], "derived": {"summary": "Paritions increase expotentially when doing cartisian product in loop program. An example program is as follows:\n\nblock_doc_word_count = lines.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Paritions increase expotentially when doing cartisian product in loop program - Paritions increase expotentially when doing cartisian product in loop program. An example program is as follows:\n\nblock_doc_word_count = lines."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is not a bug -- cartesian(rdd1, rdd2) does return an rdd with (rdd1.numPartitions * rdd2.numPartitions) partitions. In order to avoid having too many partitions, you can use the coalesce() function to reduce the number of partitions after doing the cartesian product."}]}}
{"project": "SPARK", "issue_id": "SPARK-956", "title": "The Spark python program for Lasso", "status": "Resolved", "priority": "Major", "reporter": "caizhua", "assignee": null, "labels": [], "created": "2013-11-09T22:03:42.000+0000", "updated": "2014-11-08T09:41:05.000+0000", "description": "The code describes the Spark python implementation of Lasso", "comments": ["Hi there,\n\nThis is definitely great to have, but if you'd like to contribute the patch to Spark, please create a pull request on GitHub as described here: https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark. It makes it easier for us to comment on the code.", "I assume this is WontFix as there was no followup. Spark has L1 regularization implemented already anyway."], "derived": {"summary": "The code describes the Spark python implementation of Lasso.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "The Spark python program for Lasso - The code describes the Spark python implementation of Lasso."}, {"q": "What updates or decisions were made in the discussion?", "a": "I assume this is WontFix as there was no followup. Spark has L1 regularization implemented already anyway."}]}}
{"project": "SPARK", "issue_id": "SPARK-957", "title": "The problem that repeated computation among iterations", "status": "Resolved", "priority": "Major", "reporter": "caizhua", "assignee": null, "labels": [], "created": "2013-11-09T22:14:23.000+0000", "updated": "2015-02-26T11:25:15.000+0000", "description": "For LDA model, if we make each document as a single record of RDD, it is quite slow, so we try making the RDD as a set of blocks, where each block has a subset of documents. However, when we run the program, we find that a lot of computation among iterations are repeated. Basically, when we comes to the ith iteration, all the jobs that happened in 0 to (i-1)th iteration are repeated. Certainly, the jobs in the ith iteration will be repeated in the (i+1) iteration. In total, if you have m iterations, then the jobs in the ith iteration will be repeated.\n\nHowever, the result is still correct. :)", "comments": ["The same problem happens to the Hidden Markov Model (HMM). See the affixed files. ", "It looks like {{doc_word_topic_sequence}} (on line 76) is producing a long lineage chain that's not cached, causing it to be recomputed on each iteration:\n\n{code}\ndoc_word_topic_sequence = doc_word_topic_sequence.map(lambda (doc_id, word_topic_sequence): (doc_id, sample_doc_topic(word_topic_sequence, topic_topic_prob, topic_word_prob, loop)))\n{code}\n\nI'd try adding cache() calls to {{doc_word_topic_sequence}}, which should improve performance.  In Spark 0.8+, you could also add uncache() calls to the previous {{doc_word_topic_sequence}} after you've cached the new sequence.\n\nA minor style note: since your map function doesn't use the pairs' keys, you can use {{mapValues}} instead:\n\n{code}\ndoc_word_topic_sequence = doc_word_topic_sequence.mapValues(lambda word_topic_sequence: sample_doc_topic(word_topic_sequence, topic_topic_prob, topic_word_prob, loop))).cache()\n{code}", "The code for repeated running.", "Josh Rosen, you are correct! \n\nFor HMM, if we add the cache(), it can address this problem, and we did our programming like that. I think it is my mistake to upload this code for HMM. \n\nHowever, the code for bloc_lda.py did have this problem. In the code, I use persist and unpersist together, however, the problems are stills there. See the affixed file \nblock_lda_repeated_running.py\n\n", "Hi [~caizhua], are you still having issues with your implementation of the LDA algorithm?  We try to only keep tickets open that have remaining work to be done.\n\nYou can also reference SPARK-1405 for the LDA implementation being worked on for future inclusion into MLlib."], "derived": {"summary": "For LDA model, if we make each document as a single record of RDD, it is quite slow, so we try making the RDD as a set of blocks, where each block has a subset of documents. However, when we run the program, we find that a lot of computation among iterations are repeated.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "The problem that repeated computation among iterations - For LDA model, if we make each document as a single record of RDD, it is quite slow, so we try making the RDD as a set of blocks, where each block has a subset of documents. However, when we run the program, we find that a lot of computation among iterations are repeated."}, {"q": "What updates or decisions were made in the discussion?", "a": "Hi [~caizhua], are you still having issues with your implementation of the LDA algorithm?  We try to only keep tickets open that have remaining work to be done.\n\nYou can also reference SPARK-1405 for the LDA implementation being worked on for future inclusion into MLlib."}]}}
{"project": "SPARK", "issue_id": "SPARK-958", "title": "When iteration in ALS increases to 10 running in local mode, spark throws out error of StackOverflowError", "status": "Resolved", "priority": "Major", "reporter": "Qiuzhuang Lian", "assignee": null, "labels": [], "created": "2013-11-15T01:25:56.000+0000", "updated": "2014-04-01T06:28:25.000+0000", "description": "I try to use ml-100k data to test ALS running in local mode in mllib project. If I specify iteration to be less than, it works well. However, when iteration is increased to more than 10 iterations,  spark throws out error of StackOverflowError.\n\nAttached is the log file.", "comments": ["Is this issue related to Scala bug at https://issues.scala-lang.org/browse/SI-6961?\n\nCan somebody advise?  Thanks.", "I tune JVM thread stack size to 512k via option -Xss512k and it works. This is deep level of object serialization issue known in JDK as here reported: http://bugs.sun.com/view_bug.do?bug_id=4152790, it is not related to Spark/Scala, closing this bug.\n\n\n ", "This is duplicate of bug 1066: https://spark-project.atlassian.net/browse/SPARK-1006"], "derived": {"summary": "I try to use ml-100k data to test ALS running in local mode in mllib project. If I specify iteration to be less than, it works well.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "When iteration in ALS increases to 10 running in local mode, spark throws out error of StackOverflowError - I try to use ml-100k data to test ALS running in local mode in mllib project. If I specify iteration to be less than, it works well."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is duplicate of bug 1066: https://spark-project.atlassian.net/browse/SPARK-1006"}]}}
{"project": "SPARK", "issue_id": "SPARK-959", "title": "Ivy fails to download javax.servlet.orbit dependency", "status": "Resolved", "priority": "Major", "reporter": "Aaron Davidson", "assignee": "Aaron Davidson", "labels": [], "created": "2013-11-17T22:36:01.000+0000", "updated": "2014-03-24T22:42:56.000+0000", "description": "Issue: Ivy attempts to download the \"javax.servlet.orbit\" dependency with an extension of \".orbit\" instead of \".jar\" and fails.\n\n(One) Solution: Can add the following to the libraryDependencies in SparkBuild.scala:\n\"org.eclipse.jetty.orbit\" % \"javax.servlet\" % \"2.5.0.v201103041518\" artifacts Artifact(\"javax.servlet\", \"jar\", \"jar\")\n\nCause: I don't know. This does not occur for everyone, and does not seem directly related to the version of Ant.\n\nSee the following for bug reports on the user lists:\nhttp://mail-archives.apache.org/mod_mbox/spark-user/201309.mbox/%3CCAJbo4neXyzQe6zGREQJTzZZ5ZrCoAvfEN+WmBYCed6N1EPftxA@mail.gmail.com%3E\nand\nhttp://mail-archives.apache.org/mod_mbox/spark-user/201311.mbox/%3CCANGvG8pXVhcKiGEpXnGHfQeAYTyUygA%3D1nxSe0%3D%2BfRfnKSq88w%40mail.gmail.com%3E", "comments": ["This is the ivy bug report related to the issue: https://issues.apache.org/jira/browse/IVY-899\nLooks like it was fixed in 2.3.x, but we're still using ivy 2.0, I believe.", "https://github.com/apache/incubator-spark/pull/183", "Seems that some transitive dependencies of javax.servlet also have the same issue. At least under my network environment, I have to add two more lines to get things done:\n\n\"org.eclipse.jetty.orbit\" % \"javax.activation\" % \"1.1.0.v201105071233\" artifacts Artifact(\"javax.activation\", \"jar\", \"jar\"),\n\"org.eclipse.jetty.orbit\" % \"javax.mail.glassfish\" % \"1.4.1.v201005082020\" artifacts Artifact(\"javax.mail.glassfish\", \"jar\", \"jar\"),\n\nBut I'm not sure whether it's sufficient to workaround the problem..."], "derived": {"summary": "Issue: Ivy attempts to download the \"javax. servlet.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Ivy fails to download javax.servlet.orbit dependency - Issue: Ivy attempts to download the \"javax. servlet."}, {"q": "What updates or decisions were made in the discussion?", "a": "Seems that some transitive dependencies of javax.servlet also have the same issue. At least under my network environment, I have to add two more lines to get things done:\n\n\"org.eclipse.jetty.orbit\" % \"javax.activation\" % \"1.1.0.v201105071233\" artifacts Artifact(\"javax.activation\", \"jar\", \"jar\"),\n\"org.eclipse.jetty.orbit\" % \"javax.mail.glassfish\" % \"1.4.1.v201005082020\" artifacts Artifact(\"javax.mail.glassfish\", \"jar\", \"jar\"),\n\nBut I'm not sure whether it's sufficient to workaround the problem..."}]}}
{"project": "SPARK", "issue_id": "SPARK-960", "title": "JobCancellationSuite \"two jobs sharing the same stage\" is broken", "status": "Resolved", "priority": "Major", "reporter": "Mark Hamstra", "assignee": "Sean R. Owen", "labels": [], "created": "2013-11-18T13:59:35.000+0000", "updated": "2015-01-26T22:32:56.000+0000", "description": "This test doesn't work as it appears to be intended since the map tasks can never acquire sem2.  The simplest way to demonstrate this is to comment out f1.cancel() in the future.  I believe the intention is that f1 and f2 would then complete normally; but they won't.  Instead, both jobs block, waiting on sem2.  It doesn't look like closing over Semaphores works even in a Local context, since sem2.hashCode() is different in each of f1, f2 and in the future containing f1.cancel, so the map jobs never see the sem2.release(10) in the future.\n\nInstead, the test only completes because all of the stages (the two final stages and the common dependent stage) get cancelled and aborted.  When job <--> stage dependencies are fully accounted for and job cancellation changed so that f1.cancel does not abort the common stage, then this test can never finish since it then becomes hung waiting on sem2.", "comments": ["User 'srowen' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/4180", "Issue resolved by pull request 4180\n[https://github.com/apache/spark/pull/4180]"], "derived": {"summary": "This test doesn't work as it appears to be intended since the map tasks can never acquire sem2. The simplest way to demonstrate this is to comment out f1.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "JobCancellationSuite \"two jobs sharing the same stage\" is broken - This test doesn't work as it appears to be intended since the map tasks can never acquire sem2. The simplest way to demonstrate this is to comment out f1."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 4180\n[https://github.com/apache/spark/pull/4180]"}]}}
{"project": "SPARK", "issue_id": "SPARK-961", "title": "Add a Vector.random() method or make our website examples show something else", "status": "Resolved", "priority": "Minor", "reporter": "Matei Alexandru Zaharia", "assignee": "Pillis", "labels": ["Starter"], "created": "2013-11-19T11:56:15.000+0000", "updated": "2014-01-10T18:56:09.000+0000", "description": "Pat pointed out this doesn't currently exist, although it's used in examples like the logistic regression on http://spark.incubator.apache.org/examples.html", "comments": ["I have submitted a pull request. Requesting assignment of issue to me.", "Fix pulled via https://github.com/apache/incubator-spark/pull/369. Do not have ability to resolve this issue.", "Closed it and also gave you dev access."], "derived": {"summary": "Pat pointed out this doesn't currently exist, although it's used in examples like the logistic regression on http://spark. incubator.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add a Vector.random() method or make our website examples show something else - Pat pointed out this doesn't currently exist, although it's used in examples like the logistic regression on http://spark. incubator."}, {"q": "What updates or decisions were made in the discussion?", "a": "Closed it and also gave you dev access."}]}}
{"project": "SPARK", "issue_id": "SPARK-962", "title": "debian package contains old version of executable scripts", "status": "Resolved", "priority": "Major", "reporter": "Diggory Hardy", "assignee": null, "labels": ["deb", "debian", "jdeb", "package", "script"], "created": "2013-11-21T07:40:58.000+0000", "updated": "2014-11-25T08:58:57.000+0000", "description": "When building debian package with maven\nThe spark-shell and spark-executor are not usable (packages are not org.apache.*) and seams outdated compare to ./spark-shell and ./spark-exector in repo.\n\ninstead of having outdated repl-bin/src/deb/bin/ use a common directory for run spark-shell and spark-executor scripts that jdeb can refer to.", "comments": ["https://github.com/clearstorydata/incubator-spark/pull/1 is a great way to do it I suppose.", "So we've got a \"make it work\" hack in for 0.8.1, but Debian packaging should be handled better in 0.9.  The most glaring issue in the current hack is that the packaged Spark doesn't use {{spark-class}} and the other new scripts that are described in all of the current how-to docs.  Instead, it uses the equivalent of the old {{run}} script.  The link above to the way I changed Debian packaging for ClearStory is adequate to build a package from the {{assembly}} sub-project and to deploy and use the standard scripts.  With perhaps some changes in the package name and metadata, that may be enough for Apache Spark 0.9.\n\nHowever, this would still be a fair remove from Debian packaging adequate for inclusion in a Debian or Ubuntu distribution.  It would still be just a minimalist wrapping of a fat Spark jar that can be used with a deployment tool such as Chef -- that's all that the Debian packaging of Spark has been intended for to date, and is how we use it at ClearStory.\n\nIf we're ready to go further with Spark packaging (i.e. not just proper Debian packaging, but also RPMs and installation packages for Mac and Windows), then we should produce more refined Debian packaging.  That would likely mean producing not just a spark-core package, but separate packages for examples, tools, Java API, Python API, MLlib, etc. as well as a proper source package.\n\nBuilding and maintaining packages for multiple OSes is a fair amount of work (and I'm not volunteering to do it, since what we've already got at ClearStory is adequate for my needs), so it is worth discussing whether that is something that Spark needs at this point or whether there are easier packaging/distribution targets to hit that are adequate for now.", "This appears fixed. The referenced PR was apparently merged into 0.9, and the outdated scripts referenced in the description are no longer present. Spark uses spark-class from spark-submit et al as desired."], "derived": {"summary": "When building debian package with maven\nThe spark-shell and spark-executor are not usable (packages are not org. apache.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "debian package contains old version of executable scripts - When building debian package with maven\nThe spark-shell and spark-executor are not usable (packages are not org. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "This appears fixed. The referenced PR was apparently merged into 0.9, and the outdated scripts referenced in the description are no longer present. Spark uses spark-class from spark-submit et al as desired."}]}}
{"project": "SPARK", "issue_id": "SPARK-1229", "title": "train on array (in addition to RDD)", "status": "Resolved", "priority": "Major", "reporter": "Arshak Navruzyan", "assignee": null, "labels": [], "created": "2013-11-21T11:43:58.000+0000", "updated": "2014-11-08T11:55:30.000+0000", "description": "since predict method accepts either RDD or Array for consistency so should train.  (particularly since RDD.takeSample() returns Array)", "comments": ["May I start working on this issue? Please assign it to me.", "Well... I'm new to Spark so please correct me if I'm wrong. _predict_ methods declared in ClassificationModel and in KMeansModel accept an RDD\\[Vector\\] or a single Vector (not an array!). Each item of the RDD\\[Vector\\] contains a single observation for which we want to calculate a prediction, and each item of the Vector contains a single feature of an observation.\n\nIf we need to take a sample of an RDD and feed this sample to any of the train methods, we can use the _sample_ method instead of _takeSample_ . Actually, _takeSample_ calls _sample_ internally. However, these methods accept different parameters. If this is a problem, then I think it's possible to implement another version of the _sample_ method:\n\n{code}\ndef sample(withReplacement: Boolean, num: Int, seed: Int): RDD[T]\n{code}\n\nLet me know if it makes sense.", "If I may be so bold: generally, you train on lots of data, so an RDD makes more sense than an Array as training input. That said, you can always parallelize an Array as an RDD and train on it, which is most of the use case here. If you really mean you need a sample method that returns an RDD, yes that exists as you see."], "derived": {"summary": "since predict method accepts either RDD or Array for consistency so should train. (particularly since RDD.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "train on array (in addition to RDD) - since predict method accepts either RDD or Array for consistency so should train. (particularly since RDD."}, {"q": "What updates or decisions were made in the discussion?", "a": "If I may be so bold: generally, you train on lots of data, so an RDD makes more sense than an Array as training input. That said, you can always parallelize an Array as an RDD and train on it, which is most of the use case here. If you really mean you need a sample method that returns an RDD, yes that exists as you see."}]}}
{"project": "SPARK", "issue_id": "SPARK-1228", "title": "confusion matrix", "status": "Closed", "priority": "Major", "reporter": "Arshak Navruzyan", "assignee": "Xiangrui Meng", "labels": ["classification"], "created": "2013-11-21T17:41:33.000+0000", "updated": "2014-06-05T08:53:20.000+0000", "description": "utility that print confusion matrix for multi-class classification including precision and recall ", "comments": ["Attach the implementation for ConfusionMatrix", "In the attachment, I implement the ConfusionMatrix, but haven't add comment and unit test. After review, if you think the design is OK. I would add comments and unit test. Let me know your comment on my first patch to MLlib. :)\n\nOverall, I design 2 traits for the ConfusionMatrix, both of these 2 trait has the ability to get the precision, recall, f-value and formatted confusion matrix.  The only difference is that trait ConfusionMatrix is based on double type label which correspond the label in LabeledPoint and ConfusionMatrixWithDict is based on the string type label.\n\nI also attach 2 screenshots of the confusion matrix copied to excel to allow you get an overview of what the confusion matrix looks like, one is double label, another is the string label.", "Confusion matrix was added in v1.0 as part of binary classification model evaluation."], "derived": {"summary": "utility that print confusion matrix for multi-class classification including precision and recall.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "confusion matrix - utility that print confusion matrix for multi-class classification including precision and recall."}, {"q": "What updates or decisions were made in the discussion?", "a": "Confusion matrix was added in v1.0 as part of binary classification model evaluation."}]}}
{"project": "SPARK", "issue_id": "SPARK-963", "title": "Races in JobLoggerSuite", "status": "Resolved", "priority": "Minor", "reporter": "Mark Hamstra", "assignee": "Patrick McFadin", "labels": [], "created": "2013-11-24T15:37:15.000+0000", "updated": "2013-12-08T18:24:29.000+0000", "description": "Since SparkListener events are now processed asynchronously from the SparkListenerBus eventQueue, the jobLogger conditions checked in the \"inner variables\" and \"interface functions\" tests of the JobLoggerSuite are not guaranteed to be satisfied just because rdd.reduceByKey(_+_).collect() has returned.  This leads to infrequent failures of these tests when we lose the race.", "comments": ["How come I never see typos until after I have clicked a 'submit' button?  This issue should be entitled \"Races in JobLoggerSuite\".", "I just changed it. Were you not able to edit the title yourself?", "Some combination of not being permitted to edit the title and not knowing how to edit it."], "derived": {"summary": "Since SparkListener events are now processed asynchronously from the SparkListenerBus eventQueue, the jobLogger conditions checked in the \"inner variables\" and \"interface functions\" tests of the JobLoggerSuite are not guaranteed to be satisfied just because rdd. reduceByKey(_+_).", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Races in JobLoggerSuite - Since SparkListener events are now processed asynchronously from the SparkListenerBus eventQueue, the jobLogger conditions checked in the \"inner variables\" and \"interface functions\" tests of the JobLoggerSuite are not guaranteed to be satisfied just because rdd. reduceByKey(_+_)."}, {"q": "What updates or decisions were made in the discussion?", "a": "Some combination of not being permitted to edit the title and not knowing how to edit it."}]}}
{"project": "SPARK", "issue_id": "SPARK-964", "title": "Investigate the potential for using JDK 8 lambda expressions for the Java/Scala APIs", "status": "Resolved", "priority": "Major", "reporter": "Marek Kolodziej", "assignee": "Marek Kolodziej", "labels": ["api-change"], "created": "2013-11-25T13:04:54.000+0000", "updated": "2015-12-10T15:06:41.000+0000", "description": "JDK 8 (to be released soon) will have lambda expressions. The question is whether they can be leveraged for Java to use Scala's Spark API (perhaps with some modifications), or whether a new functional API would need to be developed for Java 8+.", "comments": ["Here are some initial findings.\n\nJava's lambdas can be used with Scala, after a trivial conversion is applied. Scala's Function1<T, R> is essentially Java's Function<T, R>, but Java doesn't see scala.Function1 as honoring the JDK8 \"functional interface\" contract, with all non-apply() methods being [\"default\"|http://blog.sanaulla.info/2013/03/20/introduction-to-default-methods-defender-methods-in-java-8/]. It seems that Java's augmented \"interfaces with default methods\" aren't an exact match for Scala's traits. Therefore, one possibility is to do the following. To honor Scala's Function1 through Function22, one can create analogous Java interfaces, like so:\n\n{code}\n@FunctionalInterface\npublic interface JavaFunction1<T1, R> {\n    R apply(T1 v1);\n}\n\n@FunctionalInterface\npublic interface JavaFunction2<T1, T2, R> {\n    R apply(T1 v1, T2 v2);\n}\n\n// Function3-Function22 follow the same pattern\n{code}\n\nThe FunctionalInterface annotation is solely there to issue compiler errors if the interface doesn't honor the contract required by lambdas, but it's not strictly necessary. \n\nThen, let's create a regular Scala class that will accept lambdas as method inputs. Here, I just created a dummy class\n\n{code}\npackage foo\n\nclass FakeRDD[T](in: List[T]) {\n\n  def this(in: Array[T]) = this(in.toList)\n\n  def map(f: T => T) = new FakeRDD(in.map(f))\n\n  /* don't let the user know that the internal representation is a List\n    - hence I didn't use a case class with auto-generated toString() */\n  override def toString() = \"FakeRDD(\" + in.mkString(\",\") + \")\"\n\n}\n{code}\n\nWe will also need a Java8-Scala lambda converter, like so (just for Function1 here, could be expanded to Function2-Function22:\n\n{code}\npackage foo\n\nimport foo.{JavaFunction1 => Function1}\n\nobject Conv {\n  def f[T1, R](fun: Function1[T1, R]) = (v1: T1) => fun(v1)\n}\n\n{code}\n\nNow, let's go and create our Java client - instead of map(javaLambda), we'll have the wrapper that converts from a Java 8 to a Scala lambda, e.g. map(f(javaLambda)) where f(javaLambda) will resolve to scalaLambda\n\n{code}\npackage foo;\n\nimport static foo.Conv.f;\n\npublic class FooBar {\n    public static void main(final String[] args) {\n\n        final FakeRDD<Integer> rdd1 = new FakeRDD<Integer>(new Integer[] {1, 2, 3});\n        final FakeRDD<Integer> rdd2 = rdd1.map(f((Integer i) -> i * 2));\n        System.out.println(rdd1);\n        System.out.println(rdd2);\n    }\n}\n\n{code}\n\nSo now, the output from the execution of the Java client will be:\n\n{code}\nFakeRDD(1,2,3)\nFakeRDD(2,4,6)\n{code}\n\nBy the way, note that the f() wrapper is actually less boilerplate than using Java 8 collections with lambdas, since e.g. an ArrayList<T> would have to call a stream() method to get a Stream<T>, then apply the lambda, and then call collect() to get back a List, like so:\n\n{code}\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.Optional;\nimport java.util.Random;\n\npublic class Test {\n \n    public static void main(final String[] args) {\n \n        // seed = 1\n        final Random rand = new Random(1);\n        final List<Double> list = new ArrayList<>();\n \n        for (int i = 1; i < 100; i++) {\n \n            list.add(rand.nextGaussian());\n \n        }\n \n        final Optional<Double> sumOfDoubledPositives =\n            list.stream().filter(d -> d > 0).map(d -> d * 2).reduce((d1, d2) -> d1 + d2);\n \n        if (sumOfDoubledPositives.isPresent()) {\n \n            System.out.println(sumOfDoubledPositives.get());\n \n        }\n \n    }\n \n}\n{code}\n\nSo IMHO, that wrapper around the Scala Spark API would be more convenient for JDK 8 users than their own Java Collections Framework. However, note that the conversion from Java to Scala lambdas prevents the Java compiler from inferring types, because it can only do so for its own syntactic sugar, so instead of saying \"i -> i * 2\" you have to say \"(Integer i) -> i * 2.\" Nevertheless, I'm not sure if that inference is universal, and that's a small cost relative to the overall productivity boost stemming from a functional API. Maybe this inference issue will get resolved, after all JDK 8 is still in beta.\n\nNote also that I believe that for compatibility reasons, it may be necessary to keep the old Java Spark API for people with pre-Java 8 JDKs. However, it might be interesting to investigate whether moving from the existing Function interfaces in the Spark Java API to interfaces meeting the FunctionalInterface contract wouldn't allow JDK 8 users to benefit from lambdas directly, without the f() wrapper while using the Scala API. To the extent that the Scala API is more up to date though, it would be preferable for JDK 8 users to use the Scala API with that tiny f() wrapper in my opinion. Also, changing the Spark-specific Function interfaces to JDK 8 ones would break existing Java clientcode, so also for that reason it might be preferable to keep the pre-Java 8 API as-is while facilitating the use of the Scala API for JDK 8+ users.\n\nOf course, one could also get rid of the f() wrapper and have overloaded methods in the Spark classes, i.e. the original methods taking a Scala lambda, and analogous ones taking a Java lambda, that call the Java-to-Scala-lambda converter and redirect to the Scala lambda-based methods. Or one could accept Java lambdas in the existing methods instead of Scala ones, and import reusable implicit conversion defs for JavaFunction1-JavaFunction22 to Scala's Function1-Function22. The last option is way too invasive for the existing codebase and makes too many concessions for Java users, IMHO. The method overloading puts the burden on the Spark codebase too. On the other hand, a tiny client-side f() wrapper still permits the Java user to benefit from FP which is way better than creating anonymous classes everywhere, and lets the Java user access the more complete Scala API, but it doesn't affect all the existing Spark classes - one would merely need to include that wrapper and use it everywhere. ", "Note that in the above case, I was able to use Java's lambda in place of Scala's lambda when calling map() on FakeRDD<Integer>. However, using Scala's collections with this approach would be a bit different than what we're used to in Scala itself. For instance, we don't think twice about calling map() on a List. However, this wouldn't work with Java's lambdas. Instead, one would need to call mapConserve, like so:\n\n{code}\nfinal List<String> scalaList = List.fromArray(new String[]{\"foo\", \"bar\"});\nfinal List<String> scalaList2 = scalaList.mapConserve(f((String s) -> s.toUpperCase()));\n{code}\n\nThis works whether the transformation results in the same or a different type, for instance:\n\n{code}\nfinal List<String> scalaList3 = List.fromArray(new String[]{\"1\", \"2\"});\nfinal List<Integer> scalaList4 = scalaList3.mapConserve(f((String s) -> Integer.parseInt(s)*2));\n{code}\n\nThe reason that map() doesn't work but mapConserve() does is that map() is a curried method with an implicit argument:\n\n{code}\n map[B, That](f: (A)  B)(implicit bf: CanBuildFrom[List[A], B, That]): That \n{code}\n\nOn the other hand, mapConserve() doesn't have currying or implicit parameters:\n\n{code}\n mapConserve[B >: A <: AnyRef](f: (A)  B): List[B] \n{code}\n\nSince in case of FakeRDD the Java-exposed public map() method didn't have currying or implicits, Java \"understood\" it after the Java-to-Scala lambda conversion. In case of Scala's List, that was not the case (the above declarations of map() and mapConserve() come from [there|http://www.scala-lang.org/api/current/scala/collection/immutable/List.html]). So, as long as the Spark Scala API sticks to single parameter lists and no implicits, Java can use it with that thin f() wrapper.", "The {{f()}} wrapper approach would require still Java 8 users to manually wrap RDD\\[(K, V)] into PairRDDFunctions\\[K, V] since the implicit conversion won't happen in Java (see https://cwiki.apache.org/confluence/display/SPARK/Java+API+Internals).  We would probably want to continue to expose Java collections types instead of their Scala counterparts.\n\nTherefore, I think that we should continue maintain a separate Java wrapper.  It's a bit annoying to maintain, but the JavaAPICompletenessChecker script should make that easier; most of this code shouldn't have to ever change once it's been written since the user-facing APIs should remain stable and most of the code is quite simple.", "Hi Josh, yes, no question about maintaining a separate Java API. First of all, not everyone will upgrade to Java 8 (in fact most people are on JDK 6), so one has to support a separate API. I also get your point regarding not using the Scala API with the f() wrapper so as to return Java instead of Scala collections, and so on.\n\nWhat I'm thinking of investigating next is whether Java 8 lambdas could be used with the Java API. Looking at Spark's [function|http://spark.incubator.apache.org/docs/latest/api/core/index.html#org.apache.spark.api.java.function.package] package, the various Functions are handled by abstract classes as opposed to interfaces. Java 8 lambdas are based on an API having methods that take lambdas accept interface types that are single abstract method (SAM) interfaces - the \"abstract\" no longer being redundant, since Java 8 interfaces can have concrete (\"default\") methods, much like Scala's traits. While abstract classes are similar to interfaces with default methods, they are not the same syntactic construct, and the compiler complained when I tried using a lambda with Spark's methods that take Function instances for that reason. This is the case even though they are SAM abstract classes, but lambdas were only built around SAM interfaces.\n\nI have a new idea. The key goals with any solution, I think, are to support pre-Java 8 users while providing Java 8+ users with the convenience of lambdas. With that in mind, here's what I've been thinking about.\n\nSince pre-Java 8 interfaces can't have concrete methods, we can't provide the same exact functional API to Java 8 users as we would get by using the [java.util.function.Function|http://download.java.net/jdk8/docs/api/java/util/function/Function.html] interface, with default compose()/andThen() methods. In other words, we couldn't replace the existing Spark abstract Function classes. However, since the only abstract method in org.apache.spark.api.java.function.Function is call(), then perhaps one could expose just interfaces requiring the implementation of the call() method, and these abstract classes could be instantiated with that implementation within the Spark code itself. For example, say we have this simple SAM Function interface:\n\n{code}\npackage test;\n\ninterface Function<T, R> {\n    public R call(T t);\n}\n\n{code}\n\nThis would be what the Java 6/7 user would have to implement as an anonymous class, while a Java 8 user could in-line as a lambda. Now, let's suppose that internally, we need an abstract class with concrete methods like Spark's Function. Let's call it AbstractFun here to distinguish it from the Function interface above.\n\n{code}\npackage test;\n\npublic abstract class AbstractFun<T, R> implements Function<T, R> {\n\n    /* we're getting \"public R call(T t)\" from Function, but\n       we're not implementing it since the class is abstract */\n\n    /* second abstract method (in addition to Function's call())\n       breaks Java 8 lambda SAM contract, and Java 8 requires an\n       interface rather than an abstract class anyway for lambdas\n     */\n    public abstract void breakFunctionalInterfaceSAMContract();\n\n    /* a concrete method is OK in an abstract class, but Java 8\n       requires an interface with default methods instead for lambdas,\n       while pre-Java 8 there were no default methods\n     */\n    public int theMeaningOfLife() {\n        return 42;\n    }\n\n}\n{code}\n\nSo, how about having the API users only implement the Function interface, and having an internal conversion from the interface to the AbstractFun which has other concrete methods?\n\n{code}\npackage test;\n\npublic class Utils {\n\n    public static <T, R> AbstractFun<T, R> convert(final Function<T, R> fun) {\n        return new AbstractFun<T, R>() {\n            @Override\n            public R call(final T t) {\n                return fun.call(t);\n            }\n            \n            /* implement the other abstract method if need be\n               - if there are no other abstract method, we can just \n                 implement call() and we're done\n             */\n            @Override\n            public void breakFunctionalInterfaceSAMContract() {}\n        };\n    }\n}\n{code}\n\nNow in our API class (such as an RDD, we can just call the convert() method to instantiate the abstract class with the implementation of call() from the anonymous class passed by the user. Note the line\n\n{code}\nl2.add(convert(fun).call(t));\n{code}\n\nSee the code below\n\n{code}\npackage test;\n\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.List;\nimport static test.Utils.convert;\n\npublic class FakeRDD<T> {\n\n    final private List<T> l;\n\n    public FakeRDD(final List<T> l) {\n        this.l = l;\n    }\n\n    public <R> FakeRDD<R> map(final Function<T, R> fun) {\n        final List<R> l2 = new ArrayList<R>();\n        for (final T t : l) {\n            l2.add(convert(fun).call(t));\n        }\n        return new FakeRDD(l2);\n    }\n\n    @Override\n    public String toString() {\n        // don't show that we have a list inside - don't use List's toString()\n        return Arrays.toString(l.toArray());\n    }\n}\n{code}\n\nNow let's run it.\n\n{code}\npackage test;\n\nimport java.util.Arrays;\n\npublic class AbstractTest {\n\n    public static void main(final String[] args) {\n\n        final FakeRDD<Integer> foo1 = new FakeRDD<>(Arrays.asList(1, 2, 3, 4, 5));\n        final FakeRDD<Integer> foo2 = foo1.map(i -> i);\n\n        System.out.println(foo1);\n        System.out.println(foo2);\n    }\n}\n{code}\n\nThat way, if there are internal implementation dependencies on the abstract class, we can convert the interface to the existing class. This would allow pre-Java 8 to use the API as before (by implementing the call()) method, and would allow Java 8 users to use lambdas.\n\nOf course, switching from the abstract class to an interface would change the existing contract for current Java API users - so instead of calling the static convert() method in the implementation, the methods could be overloaded, with one taking the interface type (for Java 8 lambda users), and the other one taking the abstract class type. The interface type-taking overloaded method would then call convert() and would send the abstract class with implemented call() to the already existing API method.\n\nIt seems that this might add some extra code to the already time-consuming effort of maintaining a separate Java API, but I thought I would share some ideas as to how to accommodate Java 8 users.", "{quote}\nOf course, switching from the abstract class to an interface would change the existing contract for current Java API users - so instead of calling the static convert() method in the implementation, the methods could be overloaded, with one taking the interface type (for Java 8 lambda users), and the other one taking the abstract class type.\n{quote}\n\nInstead of overloading the methods, could we only implement methods that take the interface type and change the Java 6/7 abstract Function* classes to implement that interface instead of extending the Scala AbstractFunction classes?  This shouldn't break user code as long as users haven't overridden {{andThen()}} or {{compose()}}.\n\nMost of the Java API wrapper is implemented in Scala, so we could use use implicit conversions to avoid adding explicit {{convert()}} calls.  Hopefully we could make most of the changes via find-and-replace on the method signatures.\n\nIs there a clean way to handle the JavaDoubleRDD / JavaPairRDD conversions in Java 8?  Right now, we require users to implement DoubleFunction / PairFunction classes because we can't overload methods like map() based on the function's return type.  One approach would be to define constructors for {{DoubleFunction}} and {{PairFunction}} that accept a implementation of the new function interface; this would let Java 8 users write something like {{foo1.map(new PairFunction<Int, Int>(x -> new Tuple2<Int, Int>(x, x)))}} to get a JavaPairRDD.  This is a little verbose, but it gives us static type safety.", "Yes, I totally agree. As long as users haven't overridden andThen() and compose(), we should be fine replacing the abstract classes with the interfaces. This would certainly be easier to maintain in the long run anyway. I'm thinking back to the \"revolution\" in the API that Akka went through in version 2.2 - some changes broke existing code at a pretty basic level, but after a headache, I appreciated the new changes. I think that swapping the interfaces in Spark's case would hardly affect any of the existing users, so in relative terms, that should be a small transition. And yes, I love implicit conversions. I'll look more into how the wrappers are implemented, so far I was focused on the Scala API. :)\n\nOne thing that we have to consider to give Java 8 users lambdas while not breaking Java 6/7 users' access is that even if we replace the abstract Function classes with the Function interfaces, we would have to create a Spark package for those. There are two reasons for this:\n\n1) JDK 6/7 users don't have default methods, so we couldn't have andThen() and compose() in the interfaces the way JDK 8 has them implemented.\n2) JDK 6/7 users won't have the java.util.function package, which is why it would have to be a Spark package.\n\nThis should be easy and quick to do. And this way, given one code base and no doubling up with overloaded methods, we could keep JDK 6/7 and JDK 8 users happy at minimum effort, I think.\n\nI can definitely look into the DoubleRDD/PairRDD aspect too. In Java 8, there are both non-generically typed and generic function interfaces ([see here|http://download.java.net/jdk8/docs/api/java/util/function/package-summary.html]. The specific ones are mostly for primitives, but I guess they could be useful in some other cases, given type erasure. So, in addition to Function<T, R>, there's also [DoubleFunction|http://download.java.net/jdk8/docs/api/java/util/function/DoubleFunction.html] (which is essentially Function<Double, R>), [DoubleUnaryOperator|http://download.java.net/jdk8/docs/api/java/util/function/DoubleUnaryOperator.html] (which is Function<Double, Double> where the method applyAsDouble() (which could just be apply() for consistency) takes in a double primitive and returns a double primitive, after boxing/unboxing), etc. There's also [DoubleBinaryOperator|http://download.java.net/jdk8/docs/api/java/util/function/DoubleBinaryOperator.html] and so on. It could either be a PairFunction wrapping the lambda, or just something like the above. I think the PairFunction wrapping the lambda wouldn't be too verbose for JDK 8 users since the lambda would reduce the verbosity coming from the wrapper, but it would be more anonymous class verbosity for JDK 6/7 users who are already dealing with one anonymous class. \n\nThe nice thing for JDK 8 users is that they won't care if the interface is called DoubleBinaryOperator or FooBar, since all they have to do is insert the lambda, and the compiler will figure out whether the lambda honors the contract specified by the function interface in the map() or other method call. The JDK 6/7 users will of course have to know what the interface is.\n", "So are we all set regarding the approach for the implementation POC? Shall I try to get it working by replacing the Function abstract class with the Java 8-compatible functional interface for all classes, and once I get it working, demonstrate and see if it would be a good approach for all the other functional abstract classes?", "Sure, that sounds like a good plan.  Thanks for helping with this!\n\nOne comment:\n\n{quote}\nI think the PairFunction wrapping the lambda wouldn't be too verbose for JDK 8 users since the lambda would reduce the verbosity coming from the wrapper, but it would be more anonymous class verbosity for JDK 6/7 users who are already dealing with one anonymous class.\n{quote}\n\nI don't think that this has to introduce any additional verbosity for Java 6/7 users.  PairFunction could be an abstract base class that Java 7 users subclass like they do now, but with a static method for constructing a concrete PairFunction sublcass from a Java 8 lambda:\n\n{code}\npublic static PairFunction<T, K, V> of(Function<T, Tuple2<K, V>> function)\n{code}\n\nso Java 7 users would subclass this like they do now, but Java 8 users could write\n\n{code}\nPairFunction.of(x -> new Tuple2<Int, Int>(x, x))\n{code}\n\nI'm not sure if Java type inference would work here, so it might have to be something like \n\n{code}\nPairFunction.of<Int, Int, Int>(x -> new Tuple2<Int, Int>(x, x))\n{code}", "This is a great idea! I'll get to work on a PoC right away. This week is looking pretty busy, but I'll keep you posted once I am able to provide a working example to get your feedback before applying it throughout the API. Thanks!", "I have one more concern before I go ahead and start implementing. Since pre-Java 8 users may wish to build Spark from source using their older JDKs, wouldn't it make sense to ignore java.util.function interfaces and create our own? Java 8 doesn't care which ones one is using, as long as there is a single abstract method and the right types (generic or not). On the other hand, pre-Java 8 users would care both because of the absence of the package, and because older Java compilers don't know about default methods that exist in the interfaces of that package.", "I just wanted to share an example to see if this is a desirable implementation so far. Note that Java 8 lambdas require those interfaces, rather than the current abstract Function and related classes, otherwise lambdas will not work. Per Josh's suggestions above, I created of() static methods in the DubleFunction/DoubleFlatMapFunction/Function2/Function3/PairFlatMapFunction/PairFunction classes that take instance of IDoubleFunction/IDoubleFlatMapFunction/IFunction/IFunction2/IFunction3/IPairFlatMapFunction/IPairFunction, and instantiate the enclosing abstract classes using the call() method implemented in the anonymous class passed by the user. I wanted to avoid the \"I\" prefix but the Function names were taken for the abstract classes. Unlike the abstract classes, these interfaces are compatible with Java 8 lambdas, but the abstract classes are kept for backward compatibility with the existing Java 6/7 API. The of() static methods can be used by Java 6/7 but that's pointless since the verbosity is the same - however these static methods can accept Java 8 lambdas.\n\nI have a working example - the Java 7-compatible code can be found [here|https://github.com/mkolod/incubator-spark/tree/java8-lambda]. While this should be clear from the commit, all the changes pertain to [org.apache.spark.api.java.function|https://github.com/mkolod/incubator-spark/tree/java8-lambda/core/src/main/scala/org/apache/spark/api/java/function]. JDK 8 can be downloaded from [here|https://jdk8.java.net/download.html]. IntelliJ 12 has excellent Java 8 support, and IntelliJ 13 has some additional improvements. Eclipse support is still considerably lagging, but Java 8-compatible Eclipse can be obtained from [here|http://downloads.efxclipse.org/eclipse-java8/2013-09-13/].\n\nAfter building Spark itself (e.g. \"sbt assembly\"), it's possible to try the Java 8 lambda-based version of the Java example found [here|http://spark.incubator.apache.org/docs/latest/quick-start.html#a-standalone-app-in-java], by replacing the lines\n\n{code}\nlong numAs = logData.filter(new Function<String, Boolean>() {\n    public Boolean call(String s) { return s.contains(\"a\"); }\n}).count();\n\nlong numBs = logData.filter(new Function<String, Boolean>() {\n    public Boolean call(String s) { return s.contains(\"b\"); }\n}).count();\n{code}\n\nwith\n\n{code}\nlong numAs = logData.filter(Function.of(s -> s.contains(\"a\"))).count();\nlong numBs = logData.filter(Function.of(s -> s.contains(\"b\"))).count();\n{code}\n\nThen, one can run it normally in local mode, e.g.\n{code}\njava8/jdk/path/java -cp .:spark-java8.jar Test\n{code}\n\nI'm not sure what kinds of unit tests would be applicable here. Since most users will be pre-Java 8, the arguments passed to Function.of(), and to similar static methods on DoubleFunction/DoubleFlatMapFunction/Function2/Function3/PairFlatMapFunction/PairFunction, would have to be in terms of anonymous class implementations of the new IDoubleFunction/IDoubleFlatMapFunction/IFunction/IFunction2/IFunction3/IPairFlatMapFunction/IPairFunction interfaces rather than lambdas. This would become a bit circular, but would ensure that the tests would run in all cases. I believe that it would simply suffice to verify that the behavior of the function objects created using anonymous classes and direct use of DoubleFunction/DoubleFlatMapFunction/Function/Function2/Function3/PairFlatMapFunction/PairFunction would be equivalent to the ones using anonymous class implementations of the IDoubleFunction/IDoubleFlatMapFunction/IFunction/IFunction2/IFunction3/IPairFlatMapFunction/IPairFunction, which would be used with lambdas as well.\n\nPlease let me know what else should be done until it would be OK to issue a pull request.\n\nMy only concern is that static methods on Function/Function2 etc. require the Java 8 user to know which function type they require. With lambdas, one could have overloaded static \"of\" methods in one place (e.g. AllFun class) that would take in the kind of lambda that the user supplies, and convert it to the instances of the existing abstract classes that the user provides. The lambda would be inferred to be of a particular interface type, which would call the required method from the list of available overloaded method options. That might make for an easier API for Java 8 developers - the point of lambdas was not to remember interfaces but to simply provide a lambda that would fit one of the legal interfaces, and to have Java pick up the right one. This might be a possible improvement over the current state.\n", "Sorry for the late reply; I've been really busy since it's the end of the semester here.  This is a nice start.\n\nIf we replaced occurrences of Function with IFunction in the API's methods (so we have {{RDD<T>.map: IFunction<T, R> -> RDD<R>}}), then we might be able to omit the {{Function.of()}} calls and only require {{DoubleFunction.of()}} and {{PairFunction.of()}}.  On the other hand, that change would touch a lot more code and makes the API a little less uniform, since then we'd only _sometimes_ need to use the wrappers.\n\nAs far as unit tests are concerned, the existing test suite should cover Java 6/7.  It might be nice to include some examples and unit tests that use Java 8 lambdas, but I wouldn't want to break the ability for Java 6/7 users to run the tests.  Maybe there's a way to conditionally run those tests if a Java 8 compiler is available, but that might be painful to configure in Maven and SBT.  We should ensure that the interfaces are compatible with Java 8's lambda requirements, but this probably only requires simple test program to verify that everything compiles correctly.  To start, I think the tests that you proposed sound reasonable.\n\nWe should also update the [Java Programming Guide|https://spark.incubator.apache.org/docs/latest/java-programming-guide.html] to document this new feature.  \n\n{quote}\nWith lambdas, one could have overloaded static \"of\" methods in one place (e.g. AllFun class) that would take in the kind of lambda that the user supplies, and convert it to the instances of the existing abstract classes that the user provides. \n{quote}\n\nIs it possible to overload methods based on the lambda's type?  I don't think we can have a class with overloaded methods like {{of(IFunction<T, Pair<K, V>>), of(IFunction<T, Double>)}} where we're overloading based on some generic type's type parameter.  Even if those functions implemented different interfaces, wouldn't the resolution of the right method be ambiguous if I have methods like {{of(IFunction<T, R>)}} and {{of(IDoubleFunction<T>)}} that I call with a lambda {{of(x -> Double(2.0))}}?  I'd love to have a cleaner interface with fewer wrappers, but my hunch is that it won't be possible due to type erasure.", "Sorry for the late reply on my end, I was really ill for a few days. \n\nI agree that having the direct calls in some cases and static method calls in others would lead to confusion. However, I believe that we could make direct calls in all cases. For example, if DoubleFunction is only used with DoubleRDDs, then DoubleRDDs could take a special interface that deals with Doubles. Also, the key thing that would allow Java 8 to distinguish between non-reified generics and specific types such as Double would be to have interfaces that either take generics or specific types such as Double. When the Java 8 compiler sees a lambda that uses a Double, it will try to match against the closest interface while performing type inference. Therefore, if there's a function that is Double => Double, or Double => T, the IDoubleFunction would be picked instead of IFunction. I will test that to be sure, but that is exactly why the [java.util.function|http://download.java.net/jdk8/docs/api/java/util/function/package-summary.html] package has both generic interfaces such as [Function<T,R>|http://download.java.net/jdk8/docs/api/java/util/function/Function.html] and specific ones such as [DoubleFunction<R>|http://download.java.net/jdk8/docs/api/java/util/function/DoubleFunction.html] and [DoubleBinaryOperator|http://download.java.net/jdk8/docs/api/java/util/function/DoubleBinaryOperator.html], the latter being Double => Double. In case of IFunction<T, Pair<K,V>> maybe we could have IPairFunction<T, K, V> where the generic types would be unknown at runtime, but the fact that one of the types is Pair<?> would help with type inference for the lambda? Maybe the same could be the case for IFunction<T, Double> becoming IDoubleFunction<T>? I suppose it might be a problem that Double is the return type here, as opposed to a parameter type, but I would also assume that type inference has to be clever enough to determine that the returned type is consistent with whatever is consuming the returned value - so perhaps it would pick up the most specific case? I'll try to investigate that.\n\nI believe the above idea could resolve the issue of Java 8's inference of the special types that are not generic, such as Double. However, there are of course solutions that deal with the reification of generics. Most of the Java ones are hacky. The Scala ones, such as [scala.reflect.ClassTag|http://www.scala-lang.org/files/archive/nightly/docs/library/scala/reflect/ClassTag.html] are elegant but not suited for Java. Java 8 will have a similar solution using [type annotations|http://openjdk.java.net/projects/type-annotations/], but code using them wouldn't compile with earlier versions of Java, so the other above mentioned solutions (or maybe others not included here) would have to be considered.\n\nI would appreciate additional feedback given the above while I also investigate the possibility of using the above solution separating the interfaces for Double and generics, etc.", "I just did some further investigation and it seems that it would be possible to make certain distinctions while using the lambdas directly, without the static method conversions. I added the following in org.apache.spark.api.java.JavaRDDLike:\n\n{code}\ndef map[T1, R](f: IFunction[T1, R]): JavaRDD[R] =\n  new JavaRDD(rdd.map(x => f.apply(x)))\n\ndef map(f: IDoubleFunction[T]): JavaDoubleRDD =\n  new JavaDoubleRDD(rdd.map(x => f.apply(x)))\n{code}\n\nThen I updated the code not to use the Function.of() type wrappers, and just use the lambdas directly: \n\n{code}\npackage foo.bar;\n\nimport org.apache.spark.api.java.*;\n\npublic class Test {\n  public static void main(String[] args) {\n\n    JavaSparkContext sc = new JavaSparkContext(\"master\", \"appName\");\n    JavaRDD<String> lines = sc.textFile(\"hdfs://...\");\n\n    JavaDoubleRDD doubleRDD = lines.map(Double::parseDouble);\n    JavaRDD<String> genericDoubleRDD = lines.map(String::toLowerCase);\n\n  }\n}\n{code}\n\nThis seems to work, however Java 8 type inference can't distinguish between an IIFunction and an IPairFunction, the former returning generic R and the latter returning Tuple2<K, V>. Therefore, with overloaded map() methods, Java 8 would be confused as to whether create a JavaPairRDD<String, Integer> or a JavaRDD<Tuple2<String, Integer>>. So, perhaps the static wrappers we agreed on earlier are unavoidable. I would appreciate ideas to the contrary. Otherwise, if we agree on the static wrappers, shall I just add the unit tests and issue a pull request (I assume that updating the [Java Programming Guide|https://spark.incubator.apache.org/docs/latest/java-programming-guide.html] would only happen after the code is merged?).", "What happens when you call a method that's overloaded for IFunction and IPairFunction with a lambda that returns a Tuple2?  Does it result in an ambiguity error or does it consider the IFunction interface to be more specific than IPairFunction?\n\nI'm curious why Double works while Tuple2 doesn't.  Does the double example fail if you use valueOf instead of parseDouble?\n\nIf you dump your current draft on GitHub, I'm happy to play around to see if there's any way to eliminate the wrapper classes.  Otherwise, I guess we'll need to use the static wrappers.\n\nFeel free to submit a pull request, even if it's incomplete.  GitHub is nice for discussing code, and you can always update the PR to add unit tests and new documentation once you've gotten community feedback on the basic design (I think a lot more people follow Spark's development on GitHub than on JIRA).  I would like to make sure that the Java Programming Guide is updated before we merge this, but those changes don't need to be made before opening the PR.", "According to http://docs.oracle.com/javase/tutorial/java/javaOO/lambdaexpressions.html#target-types-and-method-arguments overloaded methods should resolve which method to call based on the return types, so it might be possible to make this work without wrappers even if you return a Tuple2 or Double. You guys should definitely try that.", "User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/17", "User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/71"], "derived": {"summary": "JDK 8 (to be released soon) will have lambda expressions. The question is whether they can be leveraged for Java to use Scala's Spark API (perhaps with some modifications), or whether a new functional API would need to be developed for Java 8+.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Investigate the potential for using JDK 8 lambda expressions for the Java/Scala APIs - JDK 8 (to be released soon) will have lambda expressions. The question is whether they can be leveraged for Java to use Scala's Spark API (perhaps with some modifications), or whether a new functional API would need to be developed for Java 8+."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/71"}]}}
{"project": "SPARK", "issue_id": "SPARK-965", "title": "Race in DAGSchedulerSuite", "status": "Resolved", "priority": "Minor", "reporter": "Mark Hamstra", "assignee": "liancheng", "labels": [], "created": "2013-11-25T14:19:29.000+0000", "updated": "2013-12-07T14:50:48.000+0000", "description": "After https://github.com/apache/incubator-spark/pull/159, resubmitFailedStages is scheduled in the eventProcessorActor preStart to run periodically in the actor's thread.  But DAGSchedulerSuite calls resubmitFailedStages directly from its own thread, which means that it is possible for both threads to be concurrently mutating shared data structures (e.g. the set of failed stages and cacheLocs), leading to failure of the test calling resubmitFailedStages.", "comments": ["Fixed in https://github.com/apache/incubator-spark/pull/216"], "derived": {"summary": "After https://github. com/apache/incubator-spark/pull/159, resubmitFailedStages is scheduled in the eventProcessorActor preStart to run periodically in the actor's thread.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Race in DAGSchedulerSuite - After https://github. com/apache/incubator-spark/pull/159, resubmitFailedStages is scheduled in the eventProcessorActor preStart to run periodically in the actor's thread."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed in https://github.com/apache/incubator-spark/pull/216"}]}}
{"project": "SPARK", "issue_id": "SPARK-966", "title": "Sometimes DAGScheduler throws NullPointerException ", "status": "Resolved", "priority": "Major", "reporter": "Qiuzhuang Lian", "assignee": "liancheng", "labels": [], "created": "2013-11-27T01:30:50.000+0000", "updated": "2013-12-07T14:46:03.000+0000", "description": "When running some spark examples, I run into NullPointerException  thrown by DAGScheduler. Log is as follows,\n\n13/11/27 17:26:40 ERROR dispatch.TaskInvocation: \njava.lang.NullPointerException\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$3$$anon$1$$anonfun$preStart$1.apply$mcV$sp(DAGScheduler.scala:116)\n\tat akka.actor.DefaultScheduler$$anon$1.run(Scheduler.scala:142)\n\tat akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:94)\n\tat akka.jsr166y.ForkJoinTask$AdaptedRunnableAction.exec(ForkJoinTask.java:1381)\n\tat akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259)\n\tat akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975)\n\tat akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479)\n\tat akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)\n13/11/27 17:26:42 WARN util.SizeEstimator: Failed to check whether UseCompressedOops is set; assuming yes", "comments": ["What version of Spark is this? Can you paste the code near line 116 in DAGScheduler.scala?", "I am using git trunk version. The last git version is e2a43b3 on 11/11/2013 by Lian,Cheng.\n\nline 116 in that class is \nif (failed.size > 0) { \nand here is the code near that line,\n\n  private val eventProcessActor: ActorRef = env.actorSystem.actorOf(Props(new Actor {\n    override def preStart() {\n      context.system.scheduler.schedule(RESUBMIT_TIMEOUT milliseconds, RESUBMIT_TIMEOUT milliseconds) {\n        if (failed.size > 0) {\n          resubmitFailedStages()\n        }\n      }\n    }\n\n", "[~liancheng] can you take a look at this? ", "Hi [~Qiuzhuang], I think I've found the cause, but not fully verified yet.  \nWould you please elaborate more on the Spark examples you were running?  I need the command line arguments and possible input data files to verify this bug.  Thanks!\n\nThe problematic commit is actually [2539c06|https://github.com/liancheng/incubator-spark/commit/2539c0674501432fb62073577db6da52a26db850], an ancestor of [e2a43b3|https://github.com/liancheng/incubator-spark/commit/e2a43b3dcce81fc99098510d09095e1be4bf3e29].\n\nIn that commit, I replaced the daemon thread in {{DAGScheduler}} with an Akka actor {{eventProcessActor}}, and moved the stage re-submission logic into a scheduled task, which references {{DAGScheduler.failed}}.\n\nFurthermore, since {{DAGScheduler}} is always started right after creation, I removed the {{DAGScheduler.start()}} method and started the actor within the {{DAGScheduler}} constructor.\n\nNow comes the problem: when {{eventProcessActor}} is started, {{DAGScheduler}} is not fully constructed yet, but the stage re-submission task is already scheduled to run.  Thus, sometimes, when the scheduled task is executed for the first time, {{DAGScheduler.failed}} may still be {{null}}, thus a {{NullPointerException}} is thrown.\n\nA possible solution would be:\n\n# Add back the {{DAGScheduler.start()}}\n# Create and start {{eventProcessActor}} within {{DAGScheduler.start()}} to ensure {{DAGScheduler}} is fully constructed when the scheduled task is executed.\n\nWill fix it ASAP.", "Code to simulate the bug.", "The example I write by myself to analyze my client's some log which contains all deployed tomcat's log files.  I am attaching the code file for you. As for data, since it's from client I couldn't attach them here, but I think you could just copy some log text files in some folder to simulate this.  Thanks.\n", "[~liancheng], see also SPARK-965 and [PR215|https://github.com/apache/incubator-spark/pull/215].", "Hi [~markhamstra], thanks for pointing this out.\n\nBy adding back {{DAGScheduler.start()}}, [SPARK-965|https://spark-project.atlassian.net/browse/SPARK-965] can also be solved.  Because {{DAGSchedulerSuite}} never calls {{DAGScheduler.start()}}, thus {{eventProcessActor}} won't be created, the stage re-submission task won't be scheduled, and the race won't exist.\n\nNevertheless, while reviewing {{DAGScheduler}}, I found that fields like {{failed}}, {{running}} & {{waiting}} are defined to be {{HashSet\\[T\\]}}, which is not thread safe, but are accessed by multiple threads.  This does introduce race conditions.  They should be defined as something like {{HashSet\\[T\\] with SynchronizedSet\\[T\\]}}.", "{quote}\nfields like failed, running & waiting are defined to be HashSet[T], which is not thread safe, but are accessed by multiple threads\n{quote}\nI believe that you are mistaken.  Other than in DAGSchedulerSource, those data structures should only be accessed within the DAGScheduler itself and in a way that is carefully maintained to be single-threaded.  In this way, we avoid synchronization overhead and other nasty possibilities such as dead locking.  From DAGSchedulerSource, the sizes of those data structures are read in a way that isn't sensitive to concurrent mutation.\n\nIt is only when scheduler package tests (like those in the DAGSchedulerSuite) manipulate the internals of the DAGScheduler directly that there should be any trouble with race conditions or other concurrency issues.\n\nIf these and other DAGScheduler data structures are accessed by multiple threads, then those are errors that we would be very much interested in understanding and correcting.", "bq. Other than in DAGSchedulerSource, those data structures should only be accessed within the DAGScheduler itself and in a way that is carefully maintained to be single-threaded. In this way, we avoid synchronization overhead and other nasty possibilities such as dead locking.\n\nMy fault... Just reviewed {{DAGScheduler}} in master HEAD and commit [bf4e613|https://github.com/liancheng/incubator-spark/blob/bf4e6131cceef4fe00fb5693117c0732f181dbd9/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala] (the commit before I replaced the daemon thread with Akka actor).  In the former case, your statement is true.  While in the latter case, I made a mistake that the scheduled stage re-submission task may run in another thread.  I should add a new {{DAGSchedulerEvent}} message {{ResubmitFailedStages}} and send this message to {{eventProcessActor}}, which can then call {{resubmitFailedStages()}} in the same thread that runs {{processEvent()}}.\n\nSo yes, embarrassingly, just spotted a bug introduced by myself :-(", "Hi, [~markhamstra] & [~rxin], please help review [PR-216|https://github.com/apache/incubator-spark/pull/216] that solves both [SPARK-966|https://spark-project.atlassian.net/browse/SPARK-966] and [SPARK-965|https://spark-project.atlassian.net/browse/SPARK-965].  Thanks!", "Fixed in https://github.com/apache/incubator-spark/pull/216"], "derived": {"summary": "When running some spark examples, I run into NullPointerException  thrown by DAGScheduler. Log is as follows,\n\n13/11/27 17:26:40 ERROR dispatch.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Sometimes DAGScheduler throws NullPointerException  - When running some spark examples, I run into NullPointerException  thrown by DAGScheduler. Log is as follows,\n\n13/11/27 17:26:40 ERROR dispatch."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed in https://github.com/apache/incubator-spark/pull/216"}]}}
{"project": "SPARK", "issue_id": "SPARK-967", "title": "start-slaves.sh uses local path from master on remote slave nodes", "status": "Resolved", "priority": "Trivial", "reporter": "Evgeniy Tsvigun", "assignee": null, "labels": ["script", "starter"], "created": "2013-11-27T06:49:17.000+0000", "updated": "2016-01-16T13:29:12.000+0000", "description": "If a slave node has home path other than master, start-slave.sh fails to start a worker instance, for other nodes behaves as expected, in my case: \n\n    $ ./bin/start-slaves.sh \n    node05.dev.vega.ru: bash: line 0: cd: /usr/home/etsvigun/spark/bin/..: No such file or directory\n    node04.dev.vega.ru: org.apache.spark.deploy.worker.Worker running as process 4796. Stop it first.\n    node03.dev.vega.ru: org.apache.spark.deploy.worker.Worker running as process 61348. Stop it first.\n\nI don't mention /usr/home anywhere, the only environment variable I set is $SPARK_HOME, relative to $HOME on every node, which makes me think some script takes `pwd` on master and tries to use it on slaves. \n\n\nSpark version: fb6875dd5c9334802580155464cef9ac4d4cc1f0\nOS:  FreeBSD 8.4", "comments": ["The problem is related to \n\nssh remote \"$SPARK_HOME\"\n\nI just tried this it will return the value from the local server and not the remote server.\n\nThe solution here does work http://unix.stackexchange.com/questions/79723/why-do-ssh-host-echo-path-and-printing-the-path-after-sshing-into-the-machi\n\nSo the implementation would be \n\nssh remote \". $HOME/.bash_profile; echo \\$SPARK_HOME\"\n\nLet me know if you like the solution.  I will make the patch.", "Initiated conversation on dev list.\n", "I won't create a pull request unless asked to, but I have a solution for this. I am running Spark in standalone mode within a Univa Grid Engine cluster. As such, configs and logs, etc should be specific to each UGE job, identified by an integer job ID. \n\nCurrently, any environment variables on the master are not passed along by the sbin/start-slaves.sh invocation of ssh. I put in a fix on my local version, which works.  However, this is still less than ideal in that UGE's job accounting cannot keep track of resource usage by jobs not under its process tree. Not sure, yet, what the correct solution is. I thought I saw a feature request to allow other remote shell programs besides ssh, but I can't find it now.\n\nPlease see my version of sbin/start-slaves.sh here, forked from current master: \nhttps://github.com/prehensilecode/spark/blob/master/sbin/start-slaves.sh", "I think this is obsolete, or no longer a problem; these scripts always respond to the local SPARK_HOME now."], "derived": {"summary": "If a slave node has home path other than master, start-slave. sh fails to start a worker instance, for other nodes behaves as expected, in my case: \n\n    $.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "start-slaves.sh uses local path from master on remote slave nodes - If a slave node has home path other than master, start-slave. sh fails to start a worker instance, for other nodes behaves as expected, in my case: \n\n    $."}, {"q": "What updates or decisions were made in the discussion?", "a": "I think this is obsolete, or no longer a problem; these scripts always respond to the local SPARK_HOME now."}]}}
{"project": "SPARK", "issue_id": "SPARK-968", "title": "In stage UI, add an overview section that shows task stats grouped by executor id", "status": "Resolved", "priority": "Major", "reporter": "Reynold Xin", "assignee": "Wangda Tan", "labels": ["starter"], "created": "2013-11-28T20:50:37.000+0000", "updated": "2014-03-02T17:49:51.000+0000", "description": "It would be very useful in the stage ui to show aggregated task information similar in semantics to the following SQL:\n\n{code}\nSELECT sum(duration), count(tasks) count(failed tasks), count(succeeded tasks), sum(shuffle read), sum(shuffle write) FROM tasks GROUP BY executorId\n{code}\n\n", "comments": ["To get started on this task, launch spark-shell and run some simple job such as\n\n{code}\nsc.parallelize(1 to 100, 2).map(x => (x, x)).reduceByKey(_ + _)\n{code}\n\nAnd then open localhost:4040 and go check out the stage web ui. ", "I'll take a try on this, but it seems I cannot change assignee. Is this feature blocked for new registered users?", "[~leftnoteasy] I just added you to the developer group. You should be able to assign yourself now. ", "Thanks :)", "Sent pull request in github for review. Thanks"], "derived": {"summary": "It would be very useful in the stage ui to show aggregated task information similar in semantics to the following SQL:\n\n{code}\nSELECT sum(duration), count(tasks) count(failed tasks), count(succeeded tasks), sum(shuffle read), sum(shuffle write) FROM tasks GROUP BY executorId\n{code}.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "In stage UI, add an overview section that shows task stats grouped by executor id - It would be very useful in the stage ui to show aggregated task information similar in semantics to the following SQL:\n\n{code}\nSELECT sum(duration), count(tasks) count(failed tasks), count(succeeded tasks), sum(shuffle read), sum(shuffle write) FROM tasks GROUP BY executorId\n{code}."}, {"q": "What updates or decisions were made in the discussion?", "a": "Sent pull request in github for review. Thanks"}]}}
{"project": "SPARK", "issue_id": "SPARK-969", "title": "Persistent web ui", "status": "Resolved", "priority": "Major", "reporter": "Reynold Xin", "assignee": null, "labels": [], "created": "2013-11-28T21:24:10.000+0000", "updated": "2016-10-01T08:23:09.000+0000", "description": "The Spark application web ui (at port 4040) is extremely helpful for debugging application correctness & performance. However, once the application completes (and thus SparkContext is stopped), the web ui is no longer accessible. It would be great to refactor the UI so stage informations (perhaps in JSON format or directly in HTML format) are stored persistently in the file system and can be viewed after the fact.", "comments": ["As a workaround, I add the following lines to my programs to save the web ui right now\n{code}\nimport scala.sys.process._\n\"wget -r -k localhost:4040\"!\nval mvCmd = \"mv localhost:4040 <dirname>\"\nmvCmd!\n{code}\n", "Hi Reynold\n\n   I have started to investigate into it, pls assign this improvement to me. Thanks.", "Thanks. I added you to the dev group so you should be able to assign tickets to yourself in the future.\n\nOnce you come up with a design, do you mind sharing that among the Spark dev list for discussion?\n", "Hey I actually un-marked this as \"starter\" since it's a fairly complicated issue. Also [~andrew xia] I think there are other people interested in this as well, including people on the core spark team, so for now I want to leave it unassigned.", "Any update on this as far as high level design?  The yarn history server jira is https://issues.apache.org/jira/browse/YARN-1530.  It would be nice to keep that in mind when designing this so spark on yarn could use that.", "Attached proposal design document for discussion.", "Thanks for posting this. I did a pass over the design. It's a pretty good first pass. \n\nHere are some questions: \n\n1. Some information are not append only. For example, executor information might change, and job/stage status change too (e.g. from running to error or success). Does your design intend to save all the snapshots of the inforrmation?\n\n2. Have you looked into possible integration with YARN's job history server, as mentioned by [~tgraves]?\n\n3. It seems to me the model (SparkContextData) and the view (SparkContextObserver) will have a lot in common (mostly about data representation and writing to external storage). It wouldn't be strictly MVC, but we should consider perhaps merging them. ", "1. SparkContextData is a simple key/value object stored into a single file (no snapshots spread into multiple files). If we want to store append only data (ex: values over time), then we have to make the value into an array of appended datum. If we want to store point-in-time data like job/status state, we have a value that is overwritten every time there is a change. The save() to disk saves the last set value/values.\n\n2. Yes. SparkContext can be launched in various environments (YARN/Mesos/etc.). The environments which create the SparkContext should be responsible for persisting the SparkContextData into their environments (History servers, etc.). This keeps the core agnostic of clusters it runs in. \nSo for YARN, the {{org.apache.spark.deploy.yarn.ApplicationMaster}} would be responsible for transforming SparkContextData.save() calls into YARN job history server REST calls. Maybe there we can override SparkContextData.save() to do YARN history server specific interactions.\n\n3. SparkContextObserver is a glorified SparkListener/Sink hooked into a live SparkContext that does not hold onto any data. The only reason (other than pure MVC) I separated it out into another class is the standalone SparkUI use case, where there is no need for a SparkContextObserver at all. Then SparkContextData is a simple JSON deserialize. The class would look obvious as to its purpose.\n\nHope that helps. \nThanks for going through the design document.", "Thanks for the design doc.\n\nSo your idea for allowing YARN integration is that YARN would extend and override the SparkContextData class?  That should work fine just make sure it can be installed/plugged in easily. One way might be how the scheduler backend is installed in the SparkContext.   If we get the config files working again it could just be a configurable class. Note, the applicationMaster can't do it as in yarn-client mode the application master does not contain the sparkContext.\n\nSo in this model you have to either have all the SparkContext's and SparkUI run on a single machine or use a NFS mounted directory and the SparkUI can be started on separate machine?\n\nSo just to be clear the save is saving the entire object with the current status to the file?  The file is overwritten on each call to save.  How long does this take for a long running applications with lots of executors and stages to write and load this file?  How often is it saving and sync'ing this to disk?  If this operation isn't atomic then the UI could end up in weird states between saves. It seems like this could potentially take a lot of time and thus have the user viewing the UI be out of sync with what is actually happening. This might be fine for historical data after the application has finished but if you are looking at a running application this doesn't seem ideal.  The current model also rules out saving to HDFS without doing other modifications.  \n\nIn the doc you say \"Any update to SparkContextData schedules a delayed save (say 500ms). Further updates keep postponing\nthat save\", what does this mean exactly?  if you postpone the saves isn't the UI going to be out of sync with what is really happening.\n\nCan the sparkUI still be started by a single application just as it is today?   This way if I didn't want to start a standalone sparkUI server I could still get the UI for my application.   \n\nPerhaps the SparkUI that exists now should still be started with the application and read the data from memory from the SparkContextData in order to keep from having delays.  The data can also be saved to disk to be read if the application crashes or finishes by a spark history UI which should just be the sparkUI with the added page to selecting which application to look at.\n\nIt seems like having it append new events that came in might be a better approach, this way you have the entire history of the application rather then just the final snapshot, although the downside there is the file may not getting very big and take a long time for it to parse.   \n\nHave you thought about security at all?  How do you protect one user from seeing another users history files?  This could be done by having the spark history ui run as a super user and making sure the file permissions are set restrictive enough.  The user would have to authenticate with the history UI.", ">> One way might be how the scheduler backend is installed in the SparkContext. \nYes, just like how cluster specific (local, YARN, Mesos) schedulers are determined for SparkContext, we will need cluster specific SparkContextDatas which should only differ in the save() mechanism. The local SparkContextData stores into local disk, whereas YARN SparkContextData would interact with YARN Application Timeline Server to record application data. \nOne approach is that each {{TaskSchedulerImpl}} itself sets the appropriate SparkContextData into the SparkContext, either during instance construction, or during its initialize(). {{TaskSchedulerImpl}} will by default set the local storing SparkContextData. {{YarnClusterScheduler}} will override to set YARN friendly SparkContextData. \nAnother approach is TaskSchedulers can implement a trait (say CustomSparkContextData) which has a createSparkContextData() method to create a custom SparkContextData. \n\n>> So in this model you have to either have all the SparkContext's and SparkUI run on a single machine or use a NFS mounted directory and the SparkUI can be started on separate machine? \nI have always thought of SparkUI as showing local node SparkContexts (got via disk or in-memory). Requiring SparkUI to show SparkContexts in the entire cluster by aggregating data by some means (NFS, HDFS, long running service, etc.) might be duplication of effort being put in by cluster managers to have their own history services (ex: YARN Application Timeline Server). Rather than building a Spark History service it might better to write UI leveraging the various history services directly.\n\n>> ...read the data from memory from the SparkContextData in order to keep from having delays\nI am in agreement over this. Delays in save() for constantly updating SparkContextDatas is a good argument.\n\n>> It seems like having it append new events that came in might be a better approach...\nMy concern was the unpredictability of file size, and the time it takes to reconstruct the data back - it was runtime dependent. When displaying UI it is better to have consistent response times. Also, as of now we do not have a usecase for showing historical changes within a SparkContext. Wanted to experiment first with periodic entire saves. If performance is an issue, maybe we can look into memory mapped IO implementation etc.\n\n>> Have you thought about security at all? ...  The user would have to authenticate with the history UI.\nThere is file-system permissions (depending on which OS user launched the Driver), and there is the SPARK_USER which could also vary independent of OS user.\nSparkUI will know SparkContextDatas it has file-permissions to read. Launch it as root - you will know all SparkContexts. Launch it as non-root user, you will know only a subset based on file-permissions. \nThe UI itself will show SparkContexts based on logged in user. Logged in user should match SPARK_USER for the SparkContext. Logging in will be a single textfield entering a username. We can default to the username of the user launching SparkUI. To prevent loading all SparkContextDatas just to match SPARK_USER, we could put it in filename (Ex: sparkcontext_\\{start-timestamp\\}_\\{username\\}.json).\n\nHope this helps.\n[~tgraves], thank you for your valuable feedback.", "I agree that archiving this page for post mortem analysis is helpful."], "derived": {"summary": "The Spark application web ui (at port 4040) is extremely helpful for debugging application correctness & performance. However, once the application completes (and thus SparkContext is stopped), the web ui is no longer accessible.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Persistent web ui - The Spark application web ui (at port 4040) is extremely helpful for debugging application correctness & performance. However, once the application completes (and thus SparkContext is stopped), the web ui is no longer accessible."}, {"q": "What updates or decisions were made in the discussion?", "a": "I agree that archiving this page for post mortem analysis is helpful."}]}}
{"project": "SPARK", "issue_id": "SPARK-970", "title": "PySpark's saveAsTextFile() throws UnicodeEncodeError when saving unicode strings", "status": "Resolved", "priority": "Major", "reporter": "Josh Rosen", "assignee": "Josh Rosen", "labels": [], "created": "2013-11-28T22:49:38.000+0000", "updated": "2013-12-04T11:10:25.000+0000", "description": "PySpark throws a UnicodeEncodeError when trying to save unicode objects to text files.  This is because saveAsTextFile() calls str() to get objects' string representations, when it should be calling unicode() instead.\n\nThis is probably a one-line fix.\n\nThis was originally reported on the mailing list at https://mail-archives.apache.org/mod_mbox/incubator-spark-user/201311.mbox/%3CCAPS2vjrorZGbxt7Nyqb1ZLZABk2MZy1O1p-KfF%3DxGJzSN0oq9g%40mail.gmail.com%3E", "comments": ["Fixed in https://github.com/apache/incubator-spark/pull/218"], "derived": {"summary": "PySpark throws a UnicodeEncodeError when trying to save unicode objects to text files. This is because saveAsTextFile() calls str() to get objects' string representations, when it should be calling unicode() instead.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "PySpark's saveAsTextFile() throws UnicodeEncodeError when saving unicode strings - PySpark throws a UnicodeEncodeError when trying to save unicode objects to text files. This is because saveAsTextFile() calls str() to get objects' string representations, when it should be calling unicode() instead."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed in https://github.com/apache/incubator-spark/pull/218"}]}}
{"project": "SPARK", "issue_id": "SPARK-971", "title": "Link to Confluence wiki from project website / documentation", "status": "Resolved", "priority": "Major", "reporter": "Josh Rosen", "assignee": "Sean R. Owen", "labels": [], "created": "2013-12-01T12:11:36.000+0000", "updated": "2014-11-10T01:41:22.000+0000", "description": "Spark's Confluence wiki (https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage) is really hard to find; try a Google search for \"apache spark wiki\", for example.\n\nWe should link to the wiki from the Spark project website and documentation.", "comments": ["User 'srowen' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/3169", "https://cwiki.apache.org/confluence/display/SPARK"], "derived": {"summary": "Spark's Confluence wiki (https://cwiki. apache.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Link to Confluence wiki from project website / documentation - Spark's Confluence wiki (https://cwiki. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://cwiki.apache.org/confluence/display/SPARK"}]}}
{"project": "SPARK", "issue_id": "SPARK-972", "title": "PySpark's \"cannot run multiple SparkContexts at once\" message should give source locations", "status": "Resolved", "priority": "Minor", "reporter": "Josh Rosen", "assignee": "Jyotiska NK", "labels": [], "created": "2013-12-01T17:37:28.000+0000", "updated": "2016-07-12T21:49:06.000+0000", "description": "It can be difficult to debug PySpark's \"Cannot run multiple SparkContexts at once\" error message if you're not sure where the first context is being created; it would be helpful if the SparkContext class remembered the linenumber/location where the  active context was created and printed it in the error message.", "comments": ["I have submitted PR #581 for this issue. Please see if it solves the problem.", "Your [pull request|https://github.com/apache/incubator-spark/pull/581] prints the master that the running SparkContext is connected to, but I was hoping to print the location of the call site where the running SparkContext was created in order to help debug the case in which a SparkContext is created deep within some piece of code and the user tries to create a new one elsewhere.", "Pushed the updated commit. Now it also prints the location of the call site( filename and linenumber). Please check if this is what you had in mind.", "Hi, I closed the old one and created a new pull request [https://github.com/apache/spark/pull/34], can you look into it?", "User 'jyotiska' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/34"], "derived": {"summary": "It can be difficult to debug PySpark's \"Cannot run multiple SparkContexts at once\" error message if you're not sure where the first context is being created; it would be helpful if the SparkContext class remembered the linenumber/location where the  active context was created and printed it in the error message.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "PySpark's \"cannot run multiple SparkContexts at once\" message should give source locations - It can be difficult to debug PySpark's \"Cannot run multiple SparkContexts at once\" error message if you're not sure where the first context is being created; it would be helpful if the SparkContext class remembered the linenumber/location where the  active context was created and printed it in the error message."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'jyotiska' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/34"}]}}
{"project": "SPARK", "issue_id": "SPARK-973", "title": "Mark fields in RDD class that are not used in workers as @transient to reduce task size", "status": "Resolved", "priority": "Major", "reporter": "Reynold Xin", "assignee": "Reynold Xin", "labels": [], "created": "2013-12-02T16:04:34.000+0000", "updated": "2013-12-07T14:10:22.000+0000", "description": "I can see a few candidates to mark as transient.\n\n{code}\n  /** Optionally overridden by subclasses to specify how they are partitioned. */\n  val partitioner: Option[Partitioner] = None\n\n  /** A friendly name for this RDD */\n  var name: String = null\n\n  /** User-defined generator of this RDD*/\n  var generator = Utils.getCallSiteInfo.firstUserClass\n\n{code}", "comments": ["Fixed in https://github.com/apache/incubator-spark/pull/223 and included in 0.8.1."], "derived": {"summary": "I can see a few candidates to mark as transient. {code}\n  /** Optionally overridden by subclasses to specify how they are partitioned.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Mark fields in RDD class that are not used in workers as @transient to reduce task size - I can see a few candidates to mark as transient. {code}\n  /** Optionally overridden by subclasses to specify how they are partitioned."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed in https://github.com/apache/incubator-spark/pull/223 and included in 0.8.1."}]}}
{"project": "SPARK", "issue_id": "SPARK-974", "title": "error in script spark-0.8.0-incubating/make-distribution.sh", "status": "Resolved", "priority": "Trivial", "reporter": "", "assignee": null, "labels": ["script"], "created": "2013-12-02T18:59:55.000+0000", "updated": "2013-12-02T21:51:01.000+0000", "description": "file spark-0.8.0-incubating/make-distribution.sh, line 98:\ncp \"$FWDIR/conf/*.template\" \"$DISTDIR\"/conf \nshould be \ncp \"$FWDIR\"/conf/*.template \"$DISTDIR\"/conf \n\nOtherwise report file not found error\n\np.s.:\nThe script file is included in this version:\nhttp://spark-project.org/download/spark-0.8.0-incubating.tgz", "comments": ["Thanks for reporting. Do you mind submitting a pull request against the Apache github mirror to fix this?\n\nhttps://github.com/apache/incubator-spark", "@Reynold Xin , never mind.\nI's already fixed with the #916 pull request.\n\nhttps://github.com/mesos/spark/pull/916", "Thanks. Closing this since it's been fixed."], "derived": {"summary": "file spark-0. 8.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "error in script spark-0.8.0-incubating/make-distribution.sh - file spark-0. 8."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks. Closing this since it's been fixed."}]}}
{"project": "SPARK", "issue_id": "SPARK-975", "title": "Spark Replay Debugger", "status": "Resolved", "priority": "Major", "reporter": "liancheng", "assignee": null, "labels": ["arthur", "debugger"], "created": "2013-12-03T05:28:36.000+0000", "updated": "2016-01-11T10:30:24.000+0000", "description": "The Spark debugger was first mentioned as {{rddbg}} in the [RDD technical report|http://www.cs.berkeley.edu/~matei/papers/2011/tr_spark.pdf].\n\n[Arthur|https://github.com/mesos/spark/tree/arthur], authored by [Ankur Dave|https://github.com/ankurdave], is an old implementation of the Spark debugger, which demonstrated both the elegance and power behind the RDD abstraction.  Unfortunately, the corresponding GitHub branch was not merged into the master branch and had stopped 2 years ago.  For more information about Arthur, please refer to [the Spark Debugger Wiki page|https://github.com/mesos/spark/wiki/Spark-Debugger] in the old GitHub repository.\n\nAs a useful tool for Spark application debugging and analysis, it would be nice to have a complete Spark debugger.  In [PR-224|https://github.com/apache/incubator-spark/pull/224], I propose a new implementation of the Spark debugger, the Spark Replay Debugger (SRD).\n\n[PR-224|https://github.com/apache/incubator-spark/pull/224] is only a preview for discussion.  In the current version, I only implemented features that can illustrate the basic mechanisms.  There are still features appeared in Arthur but missing in SRD, such as checksum based nondeterminsm detection and single task debugging with conventional debugger (like {{jdb}}).  However, these features can be easily built upon current SRD framework.  To minimize code review effort, I didn't include them into the current version intentionally.\n\nAttached is the visualization of the MLlib ALS application (with 1 iteration) generated by SRD.  For more information, please refer to [the SRD overview document|http://spark-replay-debugger-overview.readthedocs.org/en/latest/].", "comments": ["Hi [~lian cheng]. Are there any updates on this issue?", "Hi [~sarutak], thanks for caring about this. Sorry that this issue hasn't been updated for a while. At the time SRD was developed, related interfaces exposed by Spark and used in SRD were not well chosen and exposed some implementation details to API users, so SRD was not merged yet. We do have plan to improve Spark debugging facilities. Before we settle on a final design, I would like to rebase the SRD branch to the current master so that people can use it to debug and analyze their applications, though I can't promise anything for now.", "Thank you for your reply [~lian cheng].\nI understood the status.\nI think Debug tool like SRD is essentially needed, so may I help you?", "Drawback of GraphViz", "Thanks for being willing to help [~sarutak]! Actually I do need some help in seeking proper visualization tool. GraphViz is too primitive to draw complex RDD DAGs. Here is an over simplified case to illustrate the problem:\n\n!RDD DAG.png!\n\nDashed boxes are stages and circles are RDDs. When two stages share some RDD(s), the left version is what we need, while the one on the right is what GraphViz outputs. I investigated a bit but didn't find any visualization tools / libraries that can easily meets this requirement. Ideally, a JS library would be perfect since it's easy to be integrated in the web UI.", "I understood the problem and I also investigate another tool / library for the requirement.", "Cheng Lian, some JS libraries that can draw flow diagrams:\n\nhttp://www.graphdracula.net/\nhttp://www.daviddurman.com/automatic-graph-layout-with-jointjs-and-dagre.html\n\nYour diagram in 01/May/14 comment doesn't look like this one:\n\nhttp://spark-replay-debugger-overview.readthedocs.org/en/latest/_static/als-1-large.png\n\nHas the requirement changed?", "Hey [~phuocd], that image actually shows exactly the same issue as I commented. Take RDD #0 to #4 as an example: #0 to #3 form a stage, and #0, #1, #2 and #4 form another. These two stages share #0 to #2, and should overlap, and the generated dot file describes the topology correctly. But GraphVis gives wrong bounding boxes, just like the right part of the image I used in the comment.", "Cheng Lian, maybe something like this:\n\n!IMG_20140722_184149.jpg!", "[~phuocd] Yea, exactly :)", "Does the shape have any significance. I saw it was rectangle in the old screenshot and circle in new diagram.", "No it doesn't. I chose rectangles just because more text can be shown.", "In the old diagram, there are red links 5 - 3, 9 - 4. What's the meaning of red links?", "Red lines indicate wide dependencies (shuffles are introduced there).", "Cheng Lian, I saw that latest UI displays stack trace for each stage. Is there a way to filter out function calls that we don't display in debugger. There seems to be a lot of native code calls in there. See stack below.\n\nI did some work with d3 force layout. See here:\n\nhttps://github.com/dnprock/spark-debugger\n\nStack:\n\norg.apache.spark.rdd.RDD.count(RDD.scala:904)\n$line9.$read$$iwC$$iwC$$iwC$$iwC.&lt;init&gt;(&lt;console&gt;:15)\n$line9.$read$$iwC$$iwC$$iwC.&lt;init&gt;(&lt;console&gt;:20)\n$line9.$read$$iwC$$iwC.&lt;init&gt;(&lt;console&gt;:22)\n$line9.$read$$iwC.&lt;init&gt;(&lt;console&gt;:24)\n$line9.$read.&lt;init&gt;(&lt;console&gt;:26)\n$line9.$read$.&lt;init&gt;(&lt;console&gt;:30)\n$line9.$read$.&lt;clinit&gt;(&lt;console&gt;)\n$line9.$eval$.&lt;init&gt;(&lt;console&gt;:7)\n$line9.$eval$.&lt;clinit&gt;(&lt;console&gt;)\n$line9.$eval.$print(&lt;console&gt;)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:483)\norg.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:789)\norg.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1062)\norg.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:615)\norg.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:646)\norg.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:610)", "Usually we just filter them out by checking package/class names . Similar trick is used in {{org.apache.spark.util.Utils.getCallSite}}.", "To make it consistent with existing stack details, I leave the calls in place. I submit a Pull Request:\n\nhttps://github.com/apache/spark/pull/2077\n\nCan you review?", "User 'dnprock' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/2077"], "derived": {"summary": "The Spark debugger was first mentioned as {{rddbg}} in the [RDD technical report|http://www. cs.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Spark Replay Debugger - The Spark debugger was first mentioned as {{rddbg}} in the [RDD technical report|http://www. cs."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'dnprock' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/2077"}]}}
{"project": "SPARK", "issue_id": "SPARK-976", "title": "WikipediaPageRand doesn't work anymore", "status": "Resolved", "priority": "Major", "reporter": "Konstantin I Boudnik", "assignee": null, "labels": [], "created": "2013-12-03T09:49:41.000+0000", "updated": "2014-11-08T09:47:28.000+0000", "description": "Looks like wikipedia doesn't public the pages info in WEX format anymore, but instead is doing page dumps in XML format.\n\nBecause of that the example is failing with IOOBE as it expects tab-separated input strings.", "comments": ["I assume this is also WontFix as it is a Bagel example."], "derived": {"summary": "Looks like wikipedia doesn't public the pages info in WEX format anymore, but instead is doing page dumps in XML format. Because of that the example is failing with IOOBE as it expects tab-separated input strings.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "WikipediaPageRand doesn't work anymore - Looks like wikipedia doesn't public the pages info in WEX format anymore, but instead is doing page dumps in XML format. Because of that the example is failing with IOOBE as it expects tab-separated input strings."}, {"q": "What updates or decisions were made in the discussion?", "a": "I assume this is also WontFix as it is a Bagel example."}]}}
{"project": "SPARK", "issue_id": "SPARK-977", "title": "Add ZippedRDD / zip to PySpark", "status": "Resolved", "priority": "Major", "reporter": "Josh Rosen", "assignee": "Prabin Banka", "labels": [], "created": "2013-12-03T11:02:49.000+0000", "updated": "2014-03-16T22:30:08.000+0000", "description": "We should add an equivalent of ZippedRDD / zip() to PySpark.", "comments": [], "derived": {"summary": "We should add an equivalent of ZippedRDD / zip() to PySpark.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add ZippedRDD / zip to PySpark - We should add an equivalent of ZippedRDD / zip() to PySpark."}]}}
{"project": "SPARK", "issue_id": "SPARK-978", "title": "PySpark's cartesian method throws ClassCastException exception", "status": "Resolved", "priority": "Major", "reporter": "Reynold Xin", "assignee": "Josh Rosen", "labels": [], "created": "2013-12-03T13:54:08.000+0000", "updated": "2014-01-23T19:48:05.000+0000", "description": "Try the following in PySpark:\n\n{code}\na = sc.textFile(\"README.md\")\na.cartesian(a).collect()\n{code}\n\nexception thrown\n\n{code}\npy4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.writeToFile.\n: java.lang.ClassCastException: java.lang.String cannot be cast to [B\n\tat org.apache.spark.api.python.PythonRDD$.writeToStream(PythonRDD.scala:214)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeToFile$1.apply(PythonRDD.scala:233)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeToFile$1.apply(PythonRDD.scala:232)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:772)\n\tat scala.collection.JavaConversions$JIteratorWrapper.foreach(JavaConversions.scala:573)\n\tat org.apache.spark.api.python.PythonRDD$.writeToFile(PythonRDD.scala:232)\n\tat org.apache.spark.api.python.PythonRDD$.writeToFile(PythonRDD.scala:227)\n\tat org.apache.spark.api.python.PythonRDD.writeToFile(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:228)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:695)\n{code}\n\n\nBut if we convert the rdd to a list of strings it would work.\n\ni.e.\n\na = a.map(lambda line: str(line))", "comments": ["What's happening here is cartesian() produces a JavaPairRDD<String, String> when called on untransformed RDDs that were created with sc.textFile(), but writeToStream() isn't prepared to handle this case.  This fails with a ClassCastException rather than a MatchError because the type parameters of Tuple2 are eliminated by erasure; the compiler had actually warned about this:\n\n{code}\n[warn] /Users/joshrosen/Documents/spark/spark/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala:214: non-variable type argument Array[Byte] in type pattern (Array[Byte], Array[Byte]) is unchecked since it is eliminated by erasure\n[warn]       case pair: (Array[Byte], Array[Byte]) =>\n[warn]\n{code}\n\nBased on the underlying JavaRDD's ClassTag, we should be able to figure out which Java -> Python serialization method to use, rather than attempting to determine it on a per-element basis (this should be more efficient, too).  Unfortunately, this doesn't work since we need TypeTags rather than ClassTags to work around erasure of Tuple2's parameters.\n\nRather than breaking a bunch of existing APIs by introducing TypeTags, I changed the scope of the getClass()-based pattern matching to determine types based on the first item in the iterator.\n\nI've submitted my fix as part of this pull request: https://github.com/apache/incubator-spark/pull/501"], "derived": {"summary": "Try the following in PySpark:\n\n{code}\na = sc. textFile(\"README.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "PySpark's cartesian method throws ClassCastException exception - Try the following in PySpark:\n\n{code}\na = sc. textFile(\"README."}, {"q": "What updates or decisions were made in the discussion?", "a": "What's happening here is cartesian() produces a JavaPairRDD<String, String> when called on untransformed RDDs that were created with sc.textFile(), but writeToStream() isn't prepared to handle this case.  This fails with a ClassCastException rather than a MatchError because the type parameters of Tuple2 are eliminated by erasure; the compiler had actually warned about this:\n\n{code}\n[warn] /Users/joshrosen/Documents/spark/spark/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala:214: non-variable type argument Array[Byte] in type pattern (Array[Byte], Array[Byte]) is unchecked since it is eliminated by erasure\n[warn]       case pair: (Array[Byte], Array[Byte]) =>\n[warn]\n{code}\n\nBased on the underlying JavaRDD's ClassTag, we should be able to figure out which Java -> Python serialization method to use, rather than attempting to determine it on a per-element basis (this should be more efficient, too).  Unfortunately, this doesn't work since we need TypeTags rather than ClassTags to work around erasure of Tuple2's parameters.\n\nRather than breaking a bunch of existing APIs by introducing TypeTags, I changed the scope of the getClass()-based pattern matching to determine types based on the first item in the iterator.\n\nI've submitted my fix as part of this pull request: https://github.com/apache/incubator-spark/pull/501"}]}}
{"project": "SPARK", "issue_id": "SPARK-979", "title": "Add some randomization to scheduler to better balance in-memory partition distributions", "status": "Resolved", "priority": "Major", "reporter": "Reynold Xin", "assignee": "Kay Ousterhout", "labels": [], "created": "2013-12-03T15:13:58.000+0000", "updated": "2015-08-24T03:55:04.000+0000", "description": "The Spark scheduler is very deterministic, which causes problems for the following workload (in serial order on a cluster with a small number of nodes):\n\ncache rdd 1 with 1 partition\ncache rdd 2 with 1 partition\ncache rdd 3 with 1 partition\n....\n\nAfter a while, only executor 1 will have data in memory, and eventually leading to evicting in-memory blocks to disk while all other executors are empty. \n\nWe can solve this problem by adding some randomization to the cluster scheduling, or by adding memory aware scheduling (which is much harder to do). \n\n", "comments": ["For a production system, this is expected use case to have data coming in continuously. The problem will happen even with big cluster and big jobs.\n\nSee, if I have a 100 nodes cluster and my partition # is 50, only the first 50 nodes will get data. The remaining 50 will never get a chance to get anything.\nIt would be the same thing if number of partitions over than 100, (consider a case of 101) the memory usage will be most distributed onto the leading nodes (the workers with smaller ID).\n\nTo use shark as a reporting database, we'll need to utilize as many memory as possible and the RDD will be staying in memory for quite long time.\n\nHope this makes it clearer.", "Would picking free executors in an LRU order be better than simple randomization?", "Simple randomization won't work well if you consider some nodes can be added in later. Unless there is some kind of memory rebalancer, the harder way sounds like the right choice.", "Hi Reynold\n\n    Could you assign this improvement to me? I would like to try to fix it.", "made a PR : https://github.com/apache/incubator-spark/pull/548", "Using LRU adds significant complexity to the scheduler and I'm not sure there's much benefit.  For example, when a new node is added later, LRU will ensure that the next task is placed on the new node.  But subsequent tasks will be placed in round-robin order, so memory use still won't be evenly balanced across nodes.\n\nI submitted a new PR that just uses randomization: https://github.com/apache/spark/pull/27", "Hi, Kay, I think here, the round-robin subsequent tasks should be the right thing to happen? because the following nodes are placed in a LRU order", "LRU will not always solve the problem.  Consider the case when all nodes have N partitions on them, and then a new node is added:\n\nnode 0: N partitions\nnode 1: N partitions\nnode 2: N partitions\nnode 3: 0 partitions\n\nnode 3 will, for the rest of time, always have N fewer partitions than the rest of the nodes.\n\nLRU is also imperfect because nodes may be selected out of order due to locality preferences, or when the cluster is busy.  These problems don't make LRU a bad idea necessarily...I'm just not sure that the complexity of adding LRU is worth it.  In general the scheduler code has gotten to be quite complicated, so I think we should be careful when adding more complexity there.  Curious to hear what others think!", "Yeah, more feedbacks from others are better\n\nGenerally, I think randomization provides better worst-case performance than LRU, for overall, LRU > random > existing (if we don't consider the OS buffer, and even more, new HDFS has the cache system now https://issues.apache.org/jira/browse/HDFS-4949 )?\n\nJust my idea, also expecting others' feedbacks\n", "Given that this part of the code is getting really complex, I'd strongly favor a simple solution initially which we can refine based on feeedback over time. The randomized solution is simple and well tested so it seems preferable to the other patch.", "I agree, I don't mind closing my PR if you have decided to adopt Kay's solution", "User 'tdas' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/8387"], "derived": {"summary": "The Spark scheduler is very deterministic, which causes problems for the following workload (in serial order on a cluster with a small number of nodes):\n\ncache rdd 1 with 1 partition\ncache rdd 2 with 1 partition\ncache rdd 3 with 1 partition. After a while, only executor 1 will have data in memory, and eventually leading to evicting in-memory blocks to disk while all other executors are empty.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add some randomization to scheduler to better balance in-memory partition distributions - The Spark scheduler is very deterministic, which causes problems for the following workload (in serial order on a cluster with a small number of nodes):\n\ncache rdd 1 with 1 partition\ncache rdd 2 with 1 partition\ncache rdd 3 with 1 partition. After a while, only executor 1 will have data in memory, and eventually leading to evicting in-memory blocks to disk while all other executors are empty."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'tdas' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/8387"}]}}
{"project": "SPARK", "issue_id": "SPARK-980", "title": "NullPointerException for single-host setup with S3 URLs", "status": "Resolved", "priority": "Major", "reporter": "Paul R. Brown", "assignee": null, "labels": [], "created": "2013-12-03T22:10:57.000+0000", "updated": "2015-01-23T12:06:43.000+0000", "description": "Short version:\n\n* The use of {{execSparkHome_}} in [Worker.scala|https://github.com/apache/incubator-spark/blob/v0.8.0-incubating/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala#L135] should be checked for {{null}} or that value should be defaulted or plumbed through.\n* If the {{sparkHome}} argument to {{new SparkContext(...)}} is non-optional, then it should not be marked as optional.\n\nLong version:\n\nStarting up with {{bin/start-all.sh}} and then connecting from a Scala program and attempting to read two S3 URLs results in the following trace in the worker log:\n\n{code}\n13/12/03 21:50:23 ERROR worker.Worker:\njava.lang.NullPointerException\n\tat java.io.File.<init>(File.java:277)\n\tat org.apache.spark.deploy.worker.Worker$$anonfun$receive$1.apply(Worker.scala:135)\n\tat org.apache.spark.deploy.worker.Worker$$anonfun$receive$1.apply(Worker.scala:120)\n\tat akka.actor.Actor$class.apply(Actor.scala:318)\n\tat org.apache.spark.deploy.worker.Worker.apply(Worker.scala:39)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:626)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:197)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:179)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:516)\n\tat akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259)\n\tat akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975)\n\tat akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479)\n\tat akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)\n{code}\n\nThis is on Mac OS X 10.9, Oracle Java 7u45, and the Hadoop 1 download from the incubator.\n\nReading the code, this occurs because {{execSparkHome_}} is {{null}}; see [Worker.scala#L135|https://github.com/apache/incubator-spark/blob/v0.8.0-incubating/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala#L135], and setting a value explicitly in the Scala driver allows the computation to complete.", "comments": ["Thanks for reporting. What do you mean by S3 URLs? ", "I think this has been resolved by PR 442 https://github.com/apache/incubator-spark/pull/442 and PR 447 https://github.com/apache/incubator-spark/pull/447\n\nthis is not related to S3, but just due to an empty SPARK_HOME in driver end", "Those two PRs look like they would resolve the issue.  Correct that it is not related to S3  URLs, per se, but it does list the line with the attempt to read the S3 objects as the root of the trace.  (This is common for lazy invocations and stack traces, so c'est la vie.)", "I believe Nan is correct that this was subsequently resolved. Although the references to source and PRs aren't live anymore, I see this in Worker.scala:\n\n{code}\nnew File(sys.env.get(\"SPARK_HOME\").getOrElse(\".\"))\n{code}\n\nand this is the only reference to {{SPARK_HOME}}, so it seems to handle the case where this is missing."], "derived": {"summary": "Short version:\n\n* The use of {{execSparkHome_}} in [Worker. scala|https://github.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "NullPointerException for single-host setup with S3 URLs - Short version:\n\n* The use of {{execSparkHome_}} in [Worker. scala|https://github."}, {"q": "What updates or decisions were made in the discussion?", "a": "I believe Nan is correct that this was subsequently resolved. Although the references to source and PRs aren't live anymore, I see this in Worker.scala:\n\n{code}\nnew File(sys.env.get(\"SPARK_HOME\").getOrElse(\".\"))\n{code}\n\nand this is the only reference to {{SPARK_HOME}}, so it seems to handle the case where this is missing."}]}}
{"project": "SPARK", "issue_id": "SPARK-981", "title": "Seemingly spurious \"Duplicate worker ID\" error messages", "status": "Resolved", "priority": "Major", "reporter": "Paul R. Brown", "assignee": null, "labels": [], "created": "2013-12-03T22:17:15.000+0000", "updated": "2014-11-25T09:54:14.000+0000", "description": "I'm seeing {{Duplicate worker ID}} error messages after workers have crashed, and I'm presuming it comes from [Master.scala#L165|https://github.com/apache/incubator-spark/blob/c71499b7795564e1d16495c59273ecc027070fc5/core/src/main/scala/org/apache/spark/deploy/master/Master.scala#L165].\n\nIncomplete cleanup or ID collisions?\n\nThis is on Mac OS X 10.9, Oracle Java 7u45, 0.8.0 download from the incubator.", "comments": ["I'm interested in this bug...\n\nwhat's the reason of the crash? if the worker actor is restarted, it should certainly have a new ID \n\nas \n\n  def generateWorkerId(): String = {\n    \"worker-%s-%s-%d\".format(DATE_FORMAT.format(new Date), host, port)\n  }\n\nthis function will be called", "with the same port?", "even the port is the same, the DATE_FORMAT.formate(new Date) would make the ID distinct\n\nI once met register in the same address but not duplicate ID, (actually I'm thinking about what can trigger that if block(https://github.com/apache/incubator-spark/blob/c71499b7795564e1d16495c59273ecc027070fc5/core/src/main/scala/org/apache/spark/deploy/master/Master.scala#L165))", "It sounds like it may be the same issue reported, and being worked on, in https://issues.apache.org/jira/browse/SPARK-4592 . At least, both concern \"why does I see Duplicate worker ID\" messages? since the latter is active with a PR and this isn't, I suggest making this a duplicate."], "derived": {"summary": "I'm seeing {{Duplicate worker ID}} error messages after workers have crashed, and I'm presuming it comes from [Master. scala#L165|https://github.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Seemingly spurious \"Duplicate worker ID\" error messages - I'm seeing {{Duplicate worker ID}} error messages after workers have crashed, and I'm presuming it comes from [Master. scala#L165|https://github."}, {"q": "What updates or decisions were made in the discussion?", "a": "It sounds like it may be the same issue reported, and being worked on, in https://issues.apache.org/jira/browse/SPARK-4592 . At least, both concern \"why does I see Duplicate worker ID\" messages? since the latter is active with a PR and this isn't, I suggest making this a duplicate."}]}}
{"project": "SPARK", "issue_id": "SPARK-982", "title": "Typo on Hadoop third-party page", "status": "Resolved", "priority": "Minor", "reporter": "Matt Massie", "assignee": null, "labels": [], "created": "2013-12-04T19:13:51.000+0000", "updated": "2013-12-04T20:31:50.000+0000", "description": "http://spark.incubator.apache.org/docs/latest/hadoop-third-party-distributions.html\n\nThere's a subtle typo which prevents Spark users from compiling against CDH. It has \"chd4\" instead of \"cdh4\".\n\nRelease\tVersion code\nCDH 4.X.X (YARN mode)\t2.0.0-chd4.X.X\nCDH 4.X.X\t2.0.0-mr1-chd4.X.X\nCDH 3u6\t0.20.2-cdh3u6\nCDH 3u5\t0.20.2-cdh3u5\nCDH 3u4\t0.20.2-cdh3u4\n\nshould be\n\nRelease\tVersion code\nCDH 4.X.X (YARN mode)\t2.0.0-cdh4.X.X\nCDH 4.X.X\t2.0.0-mr1-cdh4.X.X\nCDH 3u6\t0.20.2-cdh3u6\nCDH 3u5\t0.20.2-cdh3u5\nCDH 3u4\t0.20.2-cdh3u4\n\n", "comments": ["This was fixed in bef398e5... closing!"], "derived": {"summary": "http://spark. incubator.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Typo on Hadoop third-party page - http://spark. incubator."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was fixed in bef398e5... closing!"}]}}
{"project": "SPARK", "issue_id": "SPARK-983", "title": "Support external sorting for RDD#sortByKey()", "status": "Resolved", "priority": "Critical", "reporter": "Reynold Xin", "assignee": "Matei Alexandru Zaharia", "labels": [], "created": "2013-12-04T23:45:12.000+0000", "updated": "2014-08-01T07:17:11.000+0000", "description": "Currently, RDD#sortByKey() is implemented by a mapPartitions which creates a buffer to hold the entire partition, then sorts it. This will cause an OOM if an entire partition cannot fit in memory, which is especially problematic for skewed data. Rather than OOMing, the behavior should be similar to the [ExternalAppendOnlyMap|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala], where we fallback to disk if we detect memory pressure.", "comments": ["Can someone comment on the scope of this feature?\nMy specific interest is to sort each partition in an RDD given a comparison function.\nThis can be done now if all partitions fit in memory, but I don't see an easy way to do it if partitions are larger than available memory.", "I think the main intention is to make the .sortByKey() call not require all data to fit into memory.  If you just want to sort data within each partition, but not across partitions, that should already be possible using .mapPartitions()", "This JIRA is pretty vague. We've already implemented an external hashing based solution for groupBys. We do not have a solution in the works for sorting, either for groupBys or RDD.sort(). We have deprioritized the former, since we have the hashing solution, but we don't have any way to do sort(), so that would be an excellent feature to have.\n\nGiven what's been marked as a duplicate of this JIRA, let me update this one to just refer to supporting external RDD.sort(). I will also unassign it, as I am not currently working on this -- let me know if you'd like me to assign it to you.", "[~aash] Note that the current implementation actually just sorts within each partition, using an initial pass from a RangePartitioner.", "Looking at [OrderedRDDFunctions|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala], there's a shuffle step using RangePartitioner, then an in-memory sort of each partition by key. If we separate the partition sort and make that available as an independent API call, it could serve two purposes: sortByKey() and sortPartitions(). Then we could improve sortPartitions() to fall back to disk like [ExternalAppendOnlyMap|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala].\n\nThe above approach would address this JIRA feature and support the equivalent of Hadoop secondary sort in a scalable way. There are plenty of time series-like use cases that could benefit from it. There's a lot more to it, but I'll code something up locally and see how it goes...", "I have the beginnings of a SortedIterator working for data that will fit in memory. It does more or less the same thing as partition sort in OrderedRDDFunctions, but it's an iterator. If we know that a partition cannot fit in memory, it's possible to split it up into chunks, sort each chunk, write to disk, and merge the chunks on disk.\n\nTo determine when to split, is it reasonable to use Runtime.freeMemory() / maxMemory() along with configuration limits? I could just keep adding to an in-memory sortable list until some memory threshold is reached, then sort/spill to disk, repeat until all data has been sorted and spilled. Then it's a basic merge operation.\n\nAny comments?", "Because ExternalAppendOnlyMap uses Runtime.getRuntime.maxMemory I think\nthat's what you should use for this feature work also.  See the\nmaxMemoryThreshold value here:\n\nhttps://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala#L79\n\n\nOn Fri, May 23, 2014 at 2:07 PM, Madhu Siddalingaiah (JIRA) <jira@apache.org\n\n", "Historically, we have not used Runtime.freeMemory(), instead favoring the use of \"memoryFractions\", such as spark.shuffle.memoryFraction and spark.storage.memoryFraction, which are fractions of Runtime.maxMemory(). One problem with Runtime.freeMemory() is that the JVM could very happily sit near 0 freeMemory, just waiting for significant new allocation to occur before freeing up some memory. If you just asked for some memory, though, it would be made available.\n\nThe downside to using the memoryFractions, though, is twofold:\n(1) It can lead to OOMs if misconfigured, even if the sum of memoryFractions is < 1, because Java + Spark need some amount of working memory to keep running (and to garbage collect).\n(2) It can lead to underutilized memory, if, for instance, the user is not caching any RDDs or currently doing a shuffle, as all that memory will remain reserved nevertheless.\n\nIn my opinion, the \"right\" solution is to have a memory manager for Spark, which allocates memory to various components (e.g., storage, shuffle, and sorting), which is willing to give one component more memory if other ones are not using it. However, this hasn't even been designed or agreed upon as a correct course, let alone implemented.\n", "[~pwendell] or [~matei], any opinions on memory management best practices? Adding a new memoryFraction for sorting will only exacerbate the problems we see with them, but I'm not sure we can rely on Runtime.freeMemory() as even an intermediary solution. \n\nPerhaps this feature could draw from the same pool as shuffle.memoryFraction, as it's used for a similar purpose, and that pool already implements some notion of memory sharing.", "I had similar concerns that RunTime.freeMemory() would not be reliable.\n\nWhat about using weak references? We could spill to disk in the finalize() method of a weak reference.\nFrom my reading of [finalize()|http://docs.oracle.com/javase/7/docs/api/java/lang/Object.html#finalize%28%29], it is acceptable to perform I/O operations (with sufficient rigor). The code might be a bit tricky as there is no guarantee on the thread that calls finalize(), but I think it could be done.", "We are actually looking at this problem in a few different places in the code base (we did this already for the external aggregations, and we also have SPARK-1777).\n\nRelying on GC's to decide when to spill is an interesting approach, but I'd rather have control of the heuristics ourselves. I think you'd get this thrashing behavior where a GC occurred and suddenly a million threads start writing to disk. In the past we've used a different mechanism (the size estimator) which approximates memory usage.\n\nIt might make sense to introduce a simple memory allocation mechanism that is shared between the external aggregation maps, partition unrolling, etc. This is something where a design doc would be helpful.\n", "I'm hoping these can be kept orthogonal, but I think that it is worth noting the existence of SPARK-1021 and the fact that sortByKey as it currently exists breaks Spark's \"transformations of RDDs are lazy\" contract.  I'm currently working on that issue, which is undoubtedly going to require at least some merge work to be compatible with the resolution of this issue.", "Understood. Here are my thoughts: I can implement this feature using SizeEstimator, even though it's not ideal. Once there is greater consensus on how to manage memory more efficiently, I think it should not be hard to adapt the code. At the minimum, the spill/merge code will be in place. I'm watching [SPARK-1021|https://issues.apache.org/jira/browse/SPARK-1021], so if that's done first, I'll merge it in.\n\nDoes that sound reasonable? [Aaron Davidson|https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ilikerps], feel free to assign this feature to me.", "Does sound reasonable. For some reason it does not allow me to assign the issue to you, though.\n\nEdit: Figured it out, thanks [~pwendell]!", "I tested some additions locally that seem to work well so far. I created a SortedPartitionsRDD and a sortPartitions(...) method in [RDD|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala]:\n\n{code}\n  /**\n   * Return a new RDD containing sorted partitions in this RDD.\n   */\n  def sortPartitions(lt: (T, T) => Boolean): RDD[T] = new SortedPartitionsRDD(this, sc.clean(lt))\n{code}\n\nI haven't added the spill/merge code to SortedPartitionsRDD yet. I wanted to get some buy in on this method as it's an addition to the API. It fits nicely with [OrderedRDDFunctions|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala] and passes all tests in [SortingSuite|https://github.com/apache/spark/blob/master/core/src/test/scala/org/apache/spark/rdd/SortingSuite.scala].\n\nI think this method can be used to address [SPARK-1021|https://issues.apache.org/jira/browse/SPARK-1021] as well as many use cases outside of sortByKey(). Does everyone agree? If so, I'll move forward with external sort in SortedPartitionsRDD and necessary tests.", "Is that code visible someplace?  In broad outline, it seems similar to the approach I'm anticipating taking to address SPARK-1021 -- and maybe I'll get a chance to actually do some work on that later this week.", "Yes, the code is checked in here:\n\n[https://github.com/msiddalingaiah/spark]\n\nIf you're happy with it, I can fill in the guts without affecting your work on [SPARK-1021|https://issues.apache.org/jira/browse/SPARK-1021].\n\nBTW, I was able to get spark-core to build and run in Eclipse (Spark-IDE + Scala Test).\nThere was a bit of fiddling, but it works quite well.\n", "I made more progress over the weekend. I have the merge sort working reliably and passing all sorting tests. The only things left are to read/write blocks to disk (I'm developing all in-memory so far) and add spill conditions. I hope to get it finalized in the next few days and submit the PR.", "[Aaron Davidson|https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ilikerps], can you make a recommendation on how to fill in this [fitsInMemory|https://github.com/msiddalingaiah/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/SortedParitionsRDD.scala#L78] method?\n\nI have the disk spill/merge all working, I just need to complete the spill condition. I looked at [SizeTrackingAppendOnlyMap|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/collection/SizeTrackingAppendOnlyMap.scala], but it's not completely clear to me how it's working.\n\nThanks!", "The idea for SizeTrackingAppendOnlyMap is that we can estimate the size of an object with SizeEstimator, but doing so can be relatively costly. Rather than running this estimation on every element added to the map, we amortize the cost by sampling exponentially less often (similar to amortization of adding to an ArrayList).\n\nIf you expect your sublists to be relatively large, it may be perfectly fine to just call SizeEstimator on each one.", "Thanks Aaron.\nPR submitted: https://github.com/apache/spark/pull/1090", "Now that an ExternalSorter class from SPARK-2045 is in, I've submitted a much smaller PR that reuses that: https://github.com/apache/spark/pull/1677. Thanks both [~msiddalingaiah] and [~andrew xia] for your previous patches on this."], "derived": {"summary": "Currently, RDD#sortByKey() is implemented by a mapPartitions which creates a buffer to hold the entire partition, then sorts it. This will cause an OOM if an entire partition cannot fit in memory, which is especially problematic for skewed data.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support external sorting for RDD#sortByKey() - Currently, RDD#sortByKey() is implemented by a mapPartitions which creates a buffer to hold the entire partition, then sorts it. This will cause an OOM if an entire partition cannot fit in memory, which is especially problematic for skewed data."}, {"q": "What updates or decisions were made in the discussion?", "a": "Now that an ExternalSorter class from SPARK-2045 is in, I've submitted a much smaller PR that reuses that: https://github.com/apache/spark/pull/1677. Thanks both [~msiddalingaiah] and [~andrew xia] for your previous patches on this."}]}}
{"project": "SPARK", "issue_id": "SPARK-984", "title": "SPARK_TOOLS_JAR not set if multiple tools jars exists", "status": "Resolved", "priority": "Minor", "reporter": "Aaron Davidson", "assignee": "Marcelo Masiero Vanzin", "labels": [], "created": "2013-12-07T01:33:07.000+0000", "updated": "2015-04-30T15:55:37.000+0000", "description": "If you have multiple tools assemblies (e.g., if you assembled on 0.8.1 and 0.9.0 before, for instance), then this error is thrown in spark-class:\n{noformat}./spark-class: line 115: [: /home/aaron/spark/tools/target/scala-2.9.3/spark-tools-assembly-0.8.1-incubating-SNAPSHOT.jar: binary operator expected{noformat}\n\nThis is because of a flaw in the bash script:\n{noformat}if [ -e \"$TOOLS_DIR\"/target/scala-$SCALA_VERSION/*assembly*[0-9Tg].jar ]; then{noformat}\nwhich does not parse correctly if the path resolves to multiple files.\n\nThe error is non-fatal, but a nuisance and presumably breaks whatever SPARK_TOOLS_JAR is used for.\n\nCurrently, we error if multiple Spark assemblies are found, so we could do something similar for tools assemblies. The only issue is that means that the user will always have to go through both errors (clean the assembly/ jars then tools/ jar) when it appears that the tools/ jar is not actually important for normal operation. The second possibility is to infer the correct tools jar using the single available assembly jar, but this is slightly complicated by the code path if $FWDIR/RELEASE exists.\n\nSince I'm not 100% on what SPARK_TOOLS_JAR is even for, I'm assigning this to Josh who wrote the code initially.", "comments": ["SPARK_TOOLS_JAR is the assembly for the spark-tools subproject, which contains developer tools like JavaAPICompletenessChecker; it was added as an alternative to placing those tools in the examples subproject.", "Since the Spark Tools project contains tools for use by Spark developers and not ordinary users, we could probably just require developers to access those tools through sbt.  In the sbt shell:\n\n{code}\nproject tools\nrun\n{code}\n\nI actually prefer to use JavaAPICompletenessChecker this was since it doesn't require me to go through a whole assembly cycle when I make changes.  If nobody has any objections, I'll submit a PR to remove the assemblies for spark-tools and update the wiki to describe the sbt run method.", "User 'srowen' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/4181", "It looks like this setting was removed as part of SPARK-4924, Marcelo's Spark Launcher PR, so I'm going to mark this as resolved with 1.4.0 as the fix version."], "derived": {"summary": "If you have multiple tools assemblies (e. g.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "SPARK_TOOLS_JAR not set if multiple tools jars exists - If you have multiple tools assemblies (e. g."}, {"q": "What updates or decisions were made in the discussion?", "a": "It looks like this setting was removed as part of SPARK-4924, Marcelo's Spark Launcher PR, so I'm going to mark this as resolved with 1.4.0 as the fix version."}]}}
{"project": "SPARK", "issue_id": "SPARK-985", "title": "Support Job Cancellation on Mesos Scheduler", "status": "Resolved", "priority": "Major", "reporter": "Josh Rosen", "assignee": null, "labels": [], "created": "2013-12-07T13:46:06.000+0000", "updated": "2020-05-17T17:48:52.000+0000", "description": "https://github.com/apache/incubator-spark/pull/29 added job cancellation but may still need support for Mesos scheduler backends:\n\nQuote: \n{quote}\nThis looks good except that MesosSchedulerBackend isn't yet calling Mesos's killTask. Do you want to add that too or are you planning to push it till later? I don't think it's a huge change.\n{quote}", "comments": ["Some more notes on this from a related thread:\n\nTask killing is not supported in the fine-grained mode on mesos because, in that mode, we use Mesos's built in support for all of the control plane messages relating to tasks. So we'll have to figure out how to support killing tasks in that model. There are two questions, one is who actually sends the \"kill\" message to the executor and the other is how we tell Mesos that the cores are freed which were in use by the task. In the course of normal operation that's handled by using the Mesos launchTask and sendStatusUpdate interfaces.", "I'm pretty sure that this was resolved by SPARK-3597 in 1.1.1 and 1.2.0: now that MesosSchedulerBackend implements killTask, I think we now have support for job cancellation on Mesos.  I'm going to mark this as \"Resolved\", but feel free to re-open if there's still work to be done."], "derived": {"summary": "https://github. com/apache/incubator-spark/pull/29 added job cancellation but may still need support for Mesos scheduler backends:\n\nQuote: \n{quote}\nThis looks good except that MesosSchedulerBackend isn't yet calling Mesos's killTask.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support Job Cancellation on Mesos Scheduler - https://github. com/apache/incubator-spark/pull/29 added job cancellation but may still need support for Mesos scheduler backends:\n\nQuote: \n{quote}\nThis looks good except that MesosSchedulerBackend isn't yet calling Mesos's killTask."}, {"q": "What updates or decisions were made in the discussion?", "a": "I'm pretty sure that this was resolved by SPARK-3597 in 1.1.1 and 1.2.0: now that MesosSchedulerBackend implements killTask, I think we now have support for job cancellation on Mesos.  I'm going to mark this as \"Resolved\", but feel free to re-open if there's still work to be done."}]}}
{"project": "SPARK", "issue_id": "SPARK-986", "title": "Add job cancellation to PySpark", "status": "Resolved", "priority": "Major", "reporter": "Josh Rosen", "assignee": "Ahir Reddy", "labels": [], "created": "2013-12-07T13:47:13.000+0000", "updated": "2014-04-25T03:21:56.000+0000", "description": "We should add support for job cancellation to PySpark.  It would also be nice to be able to cancel jobs via ctrl-c in the PySpark shell.", "comments": [], "derived": {"summary": "We should add support for job cancellation to PySpark. It would also be nice to be able to cancel jobs via ctrl-c in the PySpark shell.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add job cancellation to PySpark - We should add support for job cancellation to PySpark. It would also be nice to be able to cancel jobs via ctrl-c in the PySpark shell."}]}}
{"project": "SPARK", "issue_id": "SPARK-987", "title": "Cannot start workers successfully with hadoop 2.2.0", "status": "Resolved", "priority": "Minor", "reporter": "", "assignee": null, "labels": ["2.2.0", "hadoop"], "created": "2013-12-07T18:51:22.000+0000", "updated": "2014-11-08T09:51:12.000+0000", "description": "\nCannot start workers successfully with hadoop 2.2.0.\nI build with:\n$make-distribution.sh -hadoop 2.2.0\n\nP.S.\nCan work well with hadoop 2.0.5-alpha.\n\nBut cannot connect the hadoop 2.2.0 successfully with this exception :\n\nscala> var lines  = sc.textFile(\"hdfs://localhost:9000/user/hadoop/hadoop/hadoop-hadoop-jobtracker-master.log\")\nlines: org.apache.spark.rdd.RDD[String] = MappedRDD[3] at textFile at <console>:12\n\nscala> lines.count\njava.io.IOException: Failed on local exception: com.google.protobuf.InvalidProtocolBufferException: Message missing required fields: callId, status; Host Details : local host is: \"master/192.168.3.103\"; destination host is: \"localhost\":9000; \n\n\n\n\n", "comments": ["This just looks like a classic version mismatch between client and server. The app perhaps has embedded Hadoop libs instead of using 'provided' libs from the server installation."], "derived": {"summary": "Cannot start workers successfully with hadoop 2. 2.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Cannot start workers successfully with hadoop 2.2.0 - Cannot start workers successfully with hadoop 2. 2."}, {"q": "What updates or decisions were made in the discussion?", "a": "This just looks like a classic version mismatch between client and server. The app perhaps has embedded Hadoop libs instead of using 'provided' libs from the server installation."}]}}
{"project": "SPARK", "issue_id": "SPARK-988", "title": "Write PySpark profiling guide", "status": "Resolved", "priority": "Major", "reporter": "Josh Rosen", "assignee": null, "labels": [], "created": "2013-12-08T13:58:56.000+0000", "updated": "2015-01-03T22:45:29.000+0000", "description": "Write a guide on profiling PySpark applications.  I've done this in the past by modifying the workers to make cProfile dumps, then using various tools to collect and merge those dumps into an overall performance profile.", "comments": ["We added distributed Python profiling support in 1.2 (see SPARK-3478)."], "derived": {"summary": "Write a guide on profiling PySpark applications. I've done this in the past by modifying the workers to make cProfile dumps, then using various tools to collect and merge those dumps into an overall performance profile.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Write PySpark profiling guide - Write a guide on profiling PySpark applications. I've done this in the past by modifying the workers to make cProfile dumps, then using various tools to collect and merge those dumps into an overall performance profile."}, {"q": "What updates or decisions were made in the discussion?", "a": "We added distributed Python profiling support in 1.2 (see SPARK-3478)."}]}}
{"project": "SPARK", "issue_id": "SPARK-989", "title": "Executors table in application web ui is wrong", "status": "Closed", "priority": "Major", "reporter": "Aaron Davidson", "assignee": null, "labels": [], "created": "2013-12-08T17:21:23.000+0000", "updated": "2013-12-08T17:51:11.000+0000", "description": "See attached picture. The (executorId, address) pairs are not associated with the right state data. The most obvious sign of this is that the <driver> executor has run tasks while another executor has not. This happens for other executors as well.", "comments": ["I think this one was fixed in https://github.com/apache/incubator-spark/pull/181 ?", "My apologies, I did not realize that was what that patch was fixing. Also I checked the wrong place for recent updates to the file. Double fail on my part!\n\nAnyway, I've confirmed that that patch works, at least. Sorry about the trouble."], "derived": {"summary": "See attached picture. The (executorId, address) pairs are not associated with the right state data.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Executors table in application web ui is wrong - See attached picture. The (executorId, address) pairs are not associated with the right state data."}, {"q": "What updates or decisions were made in the discussion?", "a": "My apologies, I did not realize that was what that patch was fixing. Also I checked the wrong place for recent updates to the file. Double fail on my part!\n\nAnyway, I've confirmed that that patch works, at least. Sorry about the trouble."}]}}
{"project": "SPARK", "issue_id": "SPARK-990", "title": "JavaPairRDD.top produces ClassNotFoundException", "status": "Resolved", "priority": "Major", "reporter": "Albert Kwong", "assignee": null, "labels": [], "created": "2013-12-09T19:58:12.000+0000", "updated": "2014-01-04T15:04:46.000+0000", "description": "For a class the extends a PairFlatMapFunction and implements the Comparator interface, the flatMap operation is able to load and execute this class, while the top operation produces a ClassNotFound exception.\n\ni.e.\n{code}\npublic class TopTester {\n\n\t// declare an inner class as both a function and comparator.\n\tprivate static final class ScoreFunction extends\n\t\t\tPairFlatMapFunction<Tuple2<String,Tuple2<UUID,Double>> UUID, Double>\n\t\t\timplements Comparator<Tuple2<UUID, Double>> {\n\n\t\tpublic Iterable<Tuple2<UUID, Double>> call(Tuple2<String, Tuple2<UUID, Double>> match) throws Exception {\n\t\t\treturn null;\n\t\t}\n\n\t\tpublic int compare(Tuple2<UUID, Double> o1, Tuple2<UUID, Double> o2) {\n\t\t\treturn o1._2.compareTo(o2._2);\n\t\t}\n\t}\n\n\tpublic static void main(String[] argv) {\n\t\t// some setup ...\n\n\t\t// this operations passes, to prove that the env is setup correctly.\n\t\tJavaPairRDD<UUID,Double> scoreRdd = matchRdd.flatMap<new ScoreFunction());\n\n\t\t// ClassNotFoundException !\n\t\tscoreRdd.top(20, new ScoreFunction());\n\t}\n}\n{code}\n\n{code}\nCaused by: java.lang.ClassNotFoundException: controllers.KeywordIndex$ScoreFunction\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:366) ~[na:1.7.0_45]\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:355) ~[na:1.7.0_45]\n\tat java.security.AccessController.doPrivileged(Native Method) ~[na:1.7.0_45]\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:354) ~[na:1.7.0_45]\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:425) ~[na:1.7.0_45]\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:358) ~[na:1.7.0_45]\n{code}\n", "comments": ["Due to Spark's lazy evaluation, no distributed computations take place until you call the top() action (which triggers the execution of the flatMap() transformation).  You can call scoreRDD.foreach{} to force evaluation of the flatMap() without calling top(), but I think you'd run into the same error.\n\nThe error suggests that the Spark worker can't load your custom class, so my bet is that you need to add a JAR containing your code to your JavaSparkContext (see https://spark.incubator.apache.org/docs/0.8.0/scala-programming-guide.html#deploying-code-on-a-cluster for the relevant section of the Scala programming guide, which describes the same concept).  There's an example of this in the JavaWordCount example, where we add the Spark examples JAR to SparkContext: https://github.com/apache/incubator-spark/blob/v0.8.0-incubating/examples/src/main/java/org/apache/spark/examples/JavaWordCount.java#L39", "Hi Josh,\n\nThe jar is loaded successfully into the JavaSparkContext.  This is proven by the fact that the first call to flatMap is successful (which loads the same inner class that implements both PairFlatMapFunction and Comparator).  Yes, I have been using count() / collect() to force execution.  And yes, the code runs fine if I comment out the top() call and replace it with a collect() + local sort routine.  \n\n\t\t// List<Tuple2<UUID, Double>> collect = resultRdd.top(20, new ScoreFunction());\n\n\t\tList<Tuple2<UUID, Double>> collect = resultRdd.collect();\n\t\tCollections.sort(collect, new Comparator<Tuple2<UUID, Double>>() {\n\n\t\t\t@Override\n\t\t\tpublic int compare(Tuple2<UUID, Double> o1, Tuple2<UUID, Double> o2) {\n\t\t\t\treturn\n\t\t\t\to2._2.compareTo(o1._2); // descending\n\t\t\t}\n\t\t});", "I wonder if the top() call is somehow trying to load that class using a different class loader that doesn't include JARs added through addJars.", "I am wondering whether this sc.clean(f) is doing the trick, which is kinda missing from def top.\n\n{code:title=RDD.scala|borderStyle=solid}\n\n  /**\n   *  Return a new RDD by first applying a function to all elements of this\n   *  RDD, and then flattening the results.\n   */\n  def flatMap[U: ClassManifest](f: T => TraversableOnce[U]): RDD[U] =\n    new FlatMappedRDD(this, sc.clean(f))\n\n  /**\n   * Returns the top K elements from this RDD as defined by\n   * the specified implicit Ordering[T].\n   * @param num the number of top elements to return\n   * @param ord the implicit ordering for T\n   * @return an array of top elements\n   */\n  def top(num: Int)(implicit ord: Ordering[T]): Array[T] = {\n    mapPartitions { items =>\n      val queue = new BoundedPriorityQueue[T](num)\n      queue ++= items\n      Iterator.single(queue)\n    }.reduce { (queue1, queue2) =>\n      queue1 ++= queue2\n      queue1\n    }.toArray.sorted(ord.reverse)\n  }\n{code}\n\n{code:title=SparkContext.scala|borderStyle=solid}\n\n  /**\n   * Clean a closure to make it ready to serialized and send to tasks\n   * (removes unreferenced variables in $outer's, updates REPL variables)\n   */\n  private[spark] def clean[F <: AnyRef](f: F): F = {\n    ClosureCleaner.clean(f)\n    return f\n  }\n{code}", "Seems the problem may not be related to spark after all.  I have modified the JavaWordCount example with a top() step and it runs well. ", "Confirmed that the classpath problem is on the driver side, not the worker side.  In particular, the driver side was running in a web framework (Play 2.1.1) that is interfering with spark.  Moving away from Play to pure embedded Jetty solved the problem."], "derived": {"summary": "For a class the extends a PairFlatMapFunction and implements the Comparator interface, the flatMap operation is able to load and execute this class, while the top operation produces a ClassNotFound exception. i.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "JavaPairRDD.top produces ClassNotFoundException - For a class the extends a PairFlatMapFunction and implements the Comparator interface, the flatMap operation is able to load and execute this class, while the top operation produces a ClassNotFound exception. i."}, {"q": "What updates or decisions were made in the discussion?", "a": "Confirmed that the classpath problem is on the driver side, not the worker side.  In particular, the driver side was running in a web framework (Play 2.1.1) that is interfering with spark.  Moving away from Play to pure embedded Jetty solved the problem."}]}}
{"project": "SPARK", "issue_id": "SPARK-991", "title": "Report call sites of operators in Python", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": ["Starter"], "created": "2013-12-10T19:16:28.000+0000", "updated": "2018-03-29T14:39:17.000+0000", "description": "Similar to our call site reporting in Java, we can get a stack trace in Python with traceback.extract_stack. It would be nice to set that stack trace as the name on the corresponding RDDs so it can appear in the UI.", "comments": ["This was implemented by Tor Myklebust in https://github.com/apache/incubator-spark/pull/311", "Maybe I'm missing something here, but I'm not seeing a Python stack trace captured when I look at the RDD call site information."], "derived": {"summary": "Similar to our call site reporting in Java, we can get a stack trace in Python with traceback. extract_stack.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Report call sites of operators in Python - Similar to our call site reporting in Java, we can get a stack trace in Python with traceback. extract_stack."}, {"q": "What updates or decisions were made in the discussion?", "a": "Maybe I'm missing something here, but I'm not seeing a Python stack trace captured when I look at the RDD call site information."}]}}
{"project": "SPARK", "issue_id": "SPARK-992", "title": "Implement toString for Python and Java RDDs", "status": "Resolved", "priority": "Trivial", "reporter": "Matei Alexandru Zaharia", "assignee": "Nicholas Pentreath", "labels": ["Starter"], "created": "2013-12-10T19:20:00.000+0000", "updated": "2013-12-19T10:39:59.000+0000", "description": "They should return the toString() of the underlying Scala RDD, which includes the RDD and the place it was created.", "comments": ["See: https://github.com/apache/incubator-spark/pull/278"], "derived": {"summary": "They should return the toString() of the underlying Scala RDD, which includes the RDD and the place it was created.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Implement toString for Python and Java RDDs - They should return the toString() of the underlying Scala RDD, which includes the RDD and the place it was created."}, {"q": "What updates or decisions were made in the discussion?", "a": "See: https://github.com/apache/incubator-spark/pull/278"}]}}
{"project": "SPARK", "issue_id": "SPARK-993", "title": "Don't reuse Writable objects in HadoopRDDs by default", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "2013-12-10T20:25:22.000+0000", "updated": "2014-11-06T17:40:12.000+0000", "description": "Right now we reuse them as an optimization, which leads to weird results when you call collect() on a file with distinct items. We should instead make that behavior optional through a flag.", "comments": ["How does one reproduce this issue ?\n\nI tried a few things on the spark shell locally\n\n{noformat}\n    import java.io.File\n    import com.google.common.io.Files\n    import org.apache.hadoop.io._\n    val tempDir = Files.createTempDir()\n    val outputDir = new File(tempDir, \"output\").getAbsolutePath\n    val num = 100\n    val nums = sc.makeRDD(1 to num).map(x => (\"a\" * x, x)) \n    nums.saveAsSequenceFile(outputDir)\n\n    val output = sc.sequenceFile[String,Int](outputDir)\n    assert(output.collect().toSet.size == num)\n\n    val t = sc.sequenceFile(outputDir, classOf[Text], classOf[IntWritable])\n    assert( t.map { case (k,v) => (k.toString, v.get) }.collect().toSet.size == num )\n{noformat}\n\nBut, asserts seem to be fine. ", "We investigated this for 1.0 but found that many InputFormats behave wrongly if you try to clone the object, so we won't fix it.", "Arun, you'd see this issue if you do collect() or take() and then println. The problem is that the same Text object (for example) is referenced for all records in the dataset. The counts will be okay."], "derived": {"summary": "Right now we reuse them as an optimization, which leads to weird results when you call collect() on a file with distinct items. We should instead make that behavior optional through a flag.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "Don't reuse Writable objects in HadoopRDDs by default - Right now we reuse them as an optimization, which leads to weird results when you call collect() on a file with distinct items. We should instead make that behavior optional through a flag."}, {"q": "What updates or decisions were made in the discussion?", "a": "Arun, you'd see this issue if you do collect() or take() and then println. The problem is that the same Text object (for example) is referenced for all records in the dataset. The counts will be okay."}]}}
{"project": "SPARK", "issue_id": "SPARK-994", "title": "Upgrade Spark to Scala 2.10", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Prashant Sharma", "labels": [], "created": "2013-12-11T09:25:07.000+0000", "updated": "2020-02-07T17:26:38.000+0000", "description": "Tracking for the ongoing work upgrading Spark to Scala 2.10.", "comments": [], "derived": {"summary": "Tracking for the ongoing work upgrading Spark to Scala 2. 10.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Upgrade Spark to Scala 2.10 - Tracking for the ongoing work upgrading Spark to Scala 2. 10."}]}}
{"project": "SPARK", "issue_id": "SPARK-995", "title": "Improve Support for YARN 2.2", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "labels": [], "created": "2013-12-11T09:29:04.000+0000", "updated": "2013-12-30T16:17:53.000+0000", "description": "Due to a nasty conflict relating to the respective protocol buffer libraries in YARN 2.2 and Akka, we had to build a workaround in 0.8.1 for people wanting to build with YARN 2.2 involving different build profiles in Maven and SBT.\n\nThe problem stems from the fact that Hadoop 2.2+ versions pull in a dependency on protobuf 2.5. The akka version (2.0.5) we are currently using depends on protobuf 2.4.1 and they are not binary compatible. This is difficult to solve with classloader isolation because some parts of the code use both akka and the Hadoop libraries simultaneously.\n\nThe solution in the 0.8 branch was to publish our own port of akka 2.0.5 which depends on protobuf 2.5. Then we added a special case in the build where it relies on this special akka version.\n\nFor 0.9 we are pursuing a different solution. We are going to publish or own shaded version of protobuf 2.4 and a version of akka 2.2.4 which relies on that shaded protobof. This will completely isolate the protobuf used in akka vs in other pats of the code. There will no longer be a special case in the build... every build will rely on this special version of akka.\n", "comments": ["Hi Patrick\n\nYou mean , we will change protobuf 2.4 to some other name, and a home made akka 2.2.4 to depends on this renamed protobuf thus it will not conflict upon assemble and can coexist on runtime with other part who use protobuf 2.5, is that right? Great. \n\nHowever, I don't understand how the upgrade of akka can help in the future to deprecate this solution. when akka upgrades protobuf version in formal 2.3 release, if without this isolation solution, then, say we use the public 2.3 release, it work with yarn 2.2 and protobuf 2.5, We still need another version to work with yarn 2.0 for protobuf2.4.\n\nSo as far as what I understand, either depends on two version akka or a homemade akka with shaded protobuf to isolate from the other part. we will always need one of these two solution?", "Hey Raymond,\n\nAh you are right - that last line was a mistake. This doesn't totally get fixed by the newer akka because then we'll have the \"opposite\" problem for older hadoop versions.\n\nI think you understand correctly the proposal. Ya so we are going to publish protobuf 2.4 under a new package name and make a home-made akka that uses our specially named protobuf library. Basically this means multiple versions of protobuf will exist in the JVM under different package names.\n\nThis is a pretty hacky solution from the compile time perspective. But it makes the runtime situation and the user's experience much simpler.", "Thanks Patrick. I am fully understand now. So, it seems to that this solution do not impact our yarn src dir reorganize works. Can we not remove the yarn 2.2 support in source code and just disable it from build scripts or leave it as it is? So that I can continue the src dir reorganize works upon 2.10 merge. And then for those who need to run upon yarn 2.2. they will just need a very small patch to change several line of code to meet akka api change, and enable the build. it works. ;)", "Hey Raymond,\n\nIf you look at the current scala-2.10 branch it doesn't delete the YARN stuff it just disables it for 2.2 in the build. So we can merge your re-factoring change in on top of that after it goes in.\n\nThen later we can completely remove new-yarn.\n\n- Patrick", "Yep, I saw that. Just to double confirm we won't remove it further. Cool, That sounds a good plan. This multiple dimension of code developing on yarn 2.0/2.2; scala 2.9/2.10 and protobuf this and that version situation really need a clean end. ;)", "This was fixed using shaded protobuf library."], "derived": {"summary": "Due to a nasty conflict relating to the respective protocol buffer libraries in YARN 2. 2 and Akka, we had to build a workaround in 0.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Improve Support for YARN 2.2 - Due to a nasty conflict relating to the respective protocol buffer libraries in YARN 2. 2 and Akka, we had to build a workaround in 0."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was fixed using shaded protobuf library."}]}}
{"project": "SPARK", "issue_id": "SPARK-996", "title": "Delete Underlying Blocks When Cleaning Shuffle Meta-Data", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Aaron Davidson", "labels": [], "created": "2013-12-11T09:30:44.000+0000", "updated": "2014-01-05T23:16:56.000+0000", "description": "The proposal here is very basic. When we delete shuffle blocks during the existing cleaning process, we should clear the shuffle files as well.", "comments": ["Where and when are you proposing to do this cleanup?\n\nMy concern is with regard to {{DAGScheduler#newOrUsedStage}}, which wants to find a {{ShuffleDependency}} for a {{Stage}} computed in a job that already finished and whose {{DAGScheduler}} data structures have already been cleaned up -- e.g. an action was run on an RDD, and only after that job completed and was cleaned up after, the driver decided to launch a job joining that RDD with another.  If the {{ShuffleDependency}} is no longer present, then it will need to be re-computed.\n\nBut maybe you're not talking about {{MapOutputTracker}} cleanup... ", "Hey Mark the proposal here is very modest. It's not to implement proper reference tracking or clean-up of shuffles. It's just that if people have enabled shuffle meta data clean-up, in `ShuffleBlockManager` when it cleans up meta-data for a particular shuffle, it might as well delete the files also. At this point the blocks relating to that shuffle are inaccessible anyways so you might as well clean them from disk.", "Got it.  Sounds like that shouldn't cause the {{DAGScheduler}} to do extra re-computation.", "I propose that we organize the directory structure slightly differently to better link the files with the metadata - so we don't need to traverse a large number of inodes to get to the right shuffle files.\n\n(If we are already doing that - great!)"], "derived": {"summary": "The proposal here is very basic. When we delete shuffle blocks during the existing cleaning process, we should clear the shuffle files as well.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Delete Underlying Blocks When Cleaning Shuffle Meta-Data - The proposal here is very basic. When we delete shuffle blocks during the existing cleaning process, we should clear the shuffle files as well."}, {"q": "What updates or decisions were made in the discussion?", "a": "I propose that we organize the directory structure slightly differently to better link the files with the metadata - so we don't need to traverse a large number of inodes to get to the right shuffle files.\n\n(If we are already doing that - great!)"}]}}
{"project": "SPARK", "issue_id": "SPARK-997", "title": "Write integration tests for HDFS-based recovery", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Tathagata Das", "labels": [], "created": "2013-12-11T09:33:28.000+0000", "updated": "2014-01-24T14:00:52.000+0000", "description": "Task here is to write thorough tests for master recovery when using HDFS-based input. ", "comments": ["This has been done and will be added to spark-perf shortly"], "derived": {"summary": "Task here is to write thorough tests for master recovery when using HDFS-based input.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Write integration tests for HDFS-based recovery - Task here is to write thorough tests for master recovery when using HDFS-based input."}, {"q": "What updates or decisions were made in the discussion?", "a": "This has been done and will be added to spark-perf shortly"}]}}
{"project": "SPARK", "issue_id": "SPARK-998", "title": "Support Launching Driver Inside of Standalone Mode", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "labels": [], "created": "2013-12-11T09:35:13.000+0000", "updated": "2014-01-17T05:07:47.000+0000", "description": "We should support launching a driver inside of the cluster when using Standalone mode. We should also support a mode which restarts the driver program on failures. Users would use a similar client to the YARN client to submit their jobs.", "comments": ["How about making worker create two kinds of executor: TaskExecutor (original Executor) and DriverExecutor (the within-cluster driver)?\n", "This was actually fixed in 0.9.0. ", "oh, great, I checked the code, \n\nbut I think the current implementation does not handle the recovery of the dead driver, right?\n\nI'm interested in adding this feature"], "derived": {"summary": "We should support launching a driver inside of the cluster when using Standalone mode. We should also support a mode which restarts the driver program on failures.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Support Launching Driver Inside of Standalone Mode - We should support launching a driver inside of the cluster when using Standalone mode. We should also support a mode which restarts the driver program on failures."}, {"q": "What updates or decisions were made in the discussion?", "a": "oh, great, I checked the code, \n\nbut I think the current implementation does not handle the recovery of the dead driver, right?\n\nI'm interested in adding this feature"}]}}
{"project": "SPARK", "issue_id": "SPARK-999", "title": "Report More Instrumentation for Task Execution Time in UI", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": null, "labels": [], "created": "2013-12-12T12:07:53.000+0000", "updated": "2014-03-30T04:15:06.000+0000", "description": "We should report finer-grained information about task execution time inside of the Spark UI. Here is a proposal of exactly what we should report:\n\nTask execution goes through a few stages on the Executor.\n1. Deserializing the task\n2. Executing the task. This pipelines a few things:\n--> Reading shuffle input\n--> Running whatever function on the RDD\n--> Writing shuffle output\n3. Serializing the result\n\nI'd propose we should report the following five timing metrics. Man of these are already tracked in TaskMetrics.\n- Time spent deserializing the task on the executor (executorDeserializeTime)\n- Total execution time for the task (executorRunTime)\n-- Time spent blocking on shuffle reads during the task (fetchWaitTime)\n-- Time spent blocking on shuffle writes during the task (shuffleWriteTime)\n- Time spent serializing the result (not currently tracked)\n\nReporting all of these in the Stage UI table would be great. Bonus points if you can find some better way to visualize them.\n\nNote that the time spent serializing the result is currently not tracked. We should figure out if we can do this in a simple way - it seems like you could modify TaskResult to contain an already serialized buffer instead of the result itself. Then you could first serialize that result, update the TaskMetrics and then serialize them (we wouldn't track the time to serialize the metrics themselves). If this is too much performance overhead we could also write a custom serializer for the broader result struct (containing the accumulators, metrics, and result).\n\nOne other missing thing here is the ability to track various metrics if the task is reading or writing from HDFS or doing some other expensive thing within it's own execution. It would be nice to add support for counters and such in there, but we can keep that outside of the scope of this JIRA.\n\n\n", "comments": ["Hi Patrick \n   I would like to finish this improvement, could you assign this one to me?", "Hey - someone else actually wanted to do this and asked me to write up an outline so they could get started. Let me double check with them though, if they aren't still planning to do this I can assign it to you."], "derived": {"summary": "We should report finer-grained information about task execution time inside of the Spark UI. Here is a proposal of exactly what we should report:\n\nTask execution goes through a few stages on the Executor.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Report More Instrumentation for Task Execution Time in UI - We should report finer-grained information about task execution time inside of the Spark UI. Here is a proposal of exactly what we should report:\n\nTask execution goes through a few stages on the Executor."}, {"q": "What updates or decisions were made in the discussion?", "a": "Hey - someone else actually wanted to do this and asked me to write up an outline so they could get started. Let me double check with them though, if they aren't still planning to do this I can assign it to you."}]}}
{"project": "SPARK", "issue_id": "SPARK-1000", "title": "Crash when running SparkPi example with local-cluster", "status": "Closed", "priority": "Major", "reporter": "xiajunluan", "assignee": "Andrew Or", "labels": [], "created": "2013-12-16T05:19:02.000+0000", "updated": "2014-11-06T17:41:13.000+0000", "description": "when I run SparkPi with local-cluster[2,2,512], it will throw following exception at the end of job.\nWARNING: An exception was thrown by an exception handler.\njava.util.concurrent.RejectedExecutionException\n\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:1768)\n\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)\n\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)\n\tat org.jboss.netty.channel.socket.nio.AbstractNioWorker.start(AbstractNioWorker.java:184)\n\tat org.jboss.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:330)\n\tat org.jboss.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:35)\n\tat org.jboss.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:313)\n\tat org.jboss.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:35)\n\tat org.jboss.netty.channel.socket.nio.AbstractNioChannelSink.execute(AbstractNioChannelSink.java:34)\n\tat org.jboss.netty.channel.Channels.fireExceptionCaughtLater(Channels.java:504)\n\tat org.jboss.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:47)\n\tat org.jboss.netty.channel.Channels.fireChannelOpen(Channels.java:170)\n\tat org.jboss.netty.channel.socket.nio.NioClientSocketChannel.<init>(NioClientSocketChannel.java:79)\n\tat org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory.newChannel(NioClientSocketChannelFactory.java:176)\n\tat org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory.newChannel(NioClientSocketChannelFactory.java:82)\n\tat org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:213)\n\tat org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:183)\n\tat akka.remote.netty.ActiveRemoteClient$$anonfun$connect$1.apply$mcV$sp(Client.scala:173)\n\tat akka.util.Switch.liftedTree1$1(LockUtil.scala:33)\n\tat akka.util.Switch.transcend(LockUtil.scala:32)\n\tat akka.util.Switch.switchOn(LockUtil.scala:55)\n\tat akka.remote.netty.ActiveRemoteClient.connect(Client.scala:158)\n\tat akka.remote.netty.NettyRemoteTransport.send(NettyRemoteSupport.scala:153)\n\tat akka.remote.RemoteActorRef.$bang(RemoteActorRefProvider.scala:247)\n\tat akka.actor.LocalDeathWatch$$anonfun$publish$1.apply(ActorRefProvider.scala:559)\n\tat akka.actor.LocalDeathWatch$$anonfun$publish$1.apply(ActorRefProvider.scala:559)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:772)\n\tat scala.collection.immutable.VectorIterator.foreach(Vector.scala:648)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:73)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:63)\n\tat akka.actor.LocalDeathWatch.publish(ActorRefProvider.scala:559)\n\tat akka.remote.RemoteDeathWatch.publish(RemoteActorRefProvider.scala:280)\n\tat akka.remote.RemoteDeathWatch.publish(RemoteActorRefProvider.scala:262)\n\tat akka.actor.ActorCell.doTerminate(ActorCell.scala:701)\n\tat akka.actor.ActorCell.handleChildTerminated(ActorCell.scala:747)\n\tat akka.actor.ActorCell.systemInvoke(ActorCell.scala:608)\n\tat akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:209)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:178)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:516)\n\tat akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259)\n\tat akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975)\n\tat akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479)\n\tat akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)", "comments": ["I am not able to repro this with master branch. When you see in http://localhost:8080 do you see if worker has ALIVE status?"], "derived": {"summary": "when I run SparkPi with local-cluster[2,2,512], it will throw following exception at the end of job. WARNING: An exception was thrown by an exception handler.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Crash when running SparkPi example with local-cluster - when I run SparkPi with local-cluster[2,2,512], it will throw following exception at the end of job. WARNING: An exception was thrown by an exception handler."}, {"q": "What updates or decisions were made in the discussion?", "a": "I am not able to repro this with master branch. When you see in http://localhost:8080 do you see if worker has ALIVE status?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1001", "title": "Memory leak when reading sequence file and then sorting", "status": "Resolved", "priority": "Major", "reporter": "Matthew Cheah", "assignee": null, "labels": ["Hadoop", "Memory"], "created": "2013-12-16T13:27:17.000+0000", "updated": "2015-04-01T11:09:17.000+0000", "description": "Spark appears to build up a backlog of unreachable byte arrays when an RDD is constructed from a sequence file, and then that RDD is sorted.\n\nI have a class that wraps a Java ArrayList, that can be serialized and written to a Hadoop SequenceFile (I.e. Implements the Writable interface). Let's call it WritableDataRow. It can take a Java List as its argument to wrap around, and also has a copy constructor.\n\nSetup: 10 slaves, launched via EC2, 65.9GB RAM each, dataset is 100GB of text, 120GB when in sequence file format (not using compression to compact the bytes). CDH4.2.0-backed hadoop cluster.\n\nFirst, building the RDD from a CSV and then sorting on index 1 works fine:\n\n{code}\nscala> import scala.collection.JavaConversions._ // Other imports here as well\nimport scala.collection.JavaConversions._\n\nscala> val rddAsTextFile = sc.textFile(\"s3n://some-bucket/events-*.csv\")\nrddAsTextFile: org.apache.spark.rdd.RDD[String] = MappedRDD[1] at textFile at <console>:14\n\nscala> val rddAsWritableDataRows = rddAsTextFile.map(x => new WritableDataRow(x.split(\"\\\\|\").toList))\nrddAsWritableDataRows: org.apache.spark.rdd.RDD[com.palantir.finance.datatable.server.spark.WritableDataRow] = MappedRDD[2] at map at <console>:19\n\nscala> val rddAsKeyedWritableDataRows = rddAsWritableDataRows.map(x => (x.getContents().get(1).toString(), x));\nrddAsKeyedWritableDataRows: org.apache.spark.rdd.RDD[(String, com.palantir.finance.datatable.server.spark.WritableDataRow)] = MappedRDD[4] at map at <console>:22\n\nscala> val orderedFunct = new org.apache.spark.rdd.OrderedRDDFunctions[String, WritableDataRow, (String, WritableDataRow)](rddAsKeyedWritableDataRows)\norderedFunct: org.apache.spark.rdd.OrderedRDDFunctions[String,com.palantir.finance.datatable.server.spark.WritableDataRow,(String, com.palantir.finance.datatable.server.spark.WritableDataRow)] = org.apache.spark.rdd.OrderedRDDFunctions@587acb54\n\nscala> orderedFunct.sortByKey(true).count(); // Actually triggers the computation, as stated in a different e-mail thread\nres0: org.apache.spark.rdd.RDD[(String, com.palantir.finance.datatable.server.spark.WritableDataRow)] = MapPartitionsRDD[8] at sortByKey at <console>:27\n{code}\n\nThe above works without too many surprises. I then save it as a Sequence File (using JavaPairRDD as a way to more easily call saveAsHadoopFile(), and this is how it's done in our Java-based application):\n\n{code}\nscala> val pairRDD = new JavaPairRDD(rddAsWritableDataRows.map(x => (NullWritable.get(), x)));\npairRDD: org.apache.spark.api.java.JavaPairRDD[org.apache.hadoop.io.NullWritable,com.palantir.finance.datatable.server.spark.WritableDataRow] = org.apache.spark.api.java.JavaPairRDD@8d2e9d9\n\nscala> pairRDD.saveAsHadoopFile(\"hdfs://<hdfs-master-url>:9010/blah\", classOf[NullWritable], classOf[WritableDataRow], classOf[org.apache.hadoop.mapred.SequenceFileOutputFormat[NullWritable, WritableDataRow]]);\n\n2013-12-11 20:09:14,444 [main] INFO  org.apache.spark.SparkContext - Job finished: saveAsHadoopFile at <console>:26, took 1052.116712748 s\n{code}\n\nAnd now I want to get the RDD from the sequence file and sort THAT, and this is when I monitor Ganglia and \"ps aux\" and notice the memory usage climbing ridiculously:\n\n{code}\nscala> val rddAsSequenceFile = sc.sequenceFile(\"hdfs://<hdfs-master-url>:9010/blah\", classOf[NullWritable], classOf[WritableDataRow]).map(x => new WritableDataRow(x._2)); // Invokes copy constructor to get around re-use of writable objects\nrddAsSequenceFile: org.apache.spark.rdd.RDD[com.palantir.finance.datatable.server.spark.WritableDataRow] = MappedRDD[19] at map at <console>:19\n\nscala> val orderedFunct = new org.apache.spark.rdd.OrderedRDDFunctions[String, WritableDataRow, (String, WritableDataRow)](rddAsSequenceFile.map(x => (x.getContents().get(1).toString(), x)))\norderedFunct: org.apache.spark.rdd.OrderedRDDFunctions[String,com.palantir.finance.datatable.server.spark.WritableDataRow,(String, com.palantir.finance.datatable.server.spark.WritableDataRow)] = org.apache.spark.rdd.OrderedRDDFunctions@6262a9a6\n\nscala>orderedFunct.sortByKey().count();\n{code}\n\n(On the necessity to copy writables from hadoop RDDs, see: https://mail-archives.apache.org/mod_mbox/spark-user/201308.mbox/%3CCAF_KkPzrq4OTyQVwcOC6pLAz9X9_SFo33u4ySatki5PTqoYEDA@mail.gmail.com%3E )\n\nI got a memory dump from one worker node but can't share it. I've attached a screenshot from YourKit. At the point where around 5GB of RAM is being used on the worker, 3GB of unreachable byte arrays have accumulated. Furthermore, they're all exactly the same size, and seem to be the same size as most of the byte arrays that are strong reachable. The strong reachable byte arrays are referenced from output streams in the block output writers.\n\nLet me know if you require any more information. Thanks.", "comments": ["Yikes, I wasn't aware that square brackets would linkify all the things. If someone wants to go in and hack the description to change that, that would be fantastic.", "Hi @Matthew Cheah. I wonder the moment you got that memory dump on one worker node which is  analyzed by Yourkit in the pictures above? Is it at job runtime or after all job is finished? If you are using HPROF dump file,  can you provide which jvm process in that worker node you are dumping, the executor process or worker process if you are using standalone?(likewise, the pid you provide to jmap is from which jvm process). Becuase if it's at runtime, unreachable byte arrays will be deleted by GC.\nOtherwise ,if memory leak really happens , we should search it amony strong reachable objects , because they will not be deleted by GC. So I wonder it's the unreachable byte arrays you are worried about for memory leak or the reachable ones? Thanks!", "It's happening as the job is running. At job runtime, the memory keeps continuously climbing. It appears that the unreachable byte arrays are what keeps increasing in size, so that's what I believed was causing the leak. It IS possible for unreachable objects to NOT be cleaned up by GC for various reasons.\n\nI took a dump of the executor process, I believe.\n\n(I've since moved on from the project that was dealing with this issue though. Went back to school. It's been a little while since I looked at this, so I might not remember the specifics.)", "It's probably tough given how long ago this was reported, but can anyone confirm whether this is still an issue on the latest release (1.2.1)?"], "derived": {"summary": "Spark appears to build up a backlog of unreachable byte arrays when an RDD is constructed from a sequence file, and then that RDD is sorted. I have a class that wraps a Java ArrayList, that can be serialized and written to a Hadoop SequenceFile (I.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Memory leak when reading sequence file and then sorting - Spark appears to build up a backlog of unreachable byte arrays when an RDD is constructed from a sequence file, and then that RDD is sorted. I have a class that wraps a Java ArrayList, that can be serialized and written to a Hadoop SequenceFile (I."}, {"q": "What updates or decisions were made in the discussion?", "a": "It's probably tough given how long ago this was reported, but can anyone confirm whether this is still an issue on the latest release (1.2.1)?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1002", "title": "Remove binary artifacts from build", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": null, "labels": [], "created": "2013-12-17T10:11:19.000+0000", "updated": "2014-03-30T04:13:50.000+0000", "description": "This is a requirement from Apache and will block our 0.9 release unless dealt with. We should publish any jars we need through sonatype and remove sbt.\n\n- Patrick", "comments": [], "derived": {"summary": "This is a requirement from Apache and will block our 0. 9 release unless dealt with.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Remove binary artifacts from build - This is a requirement from Apache and will block our 0. 9 release unless dealt with."}]}}
{"project": "SPARK", "issue_id": "SPARK-1003", "title": "Spark application is blocked when running on yarn", "status": "Resolved", "priority": "Major", "reporter": "Xicheng Dong", "assignee": "Thomas Graves", "labels": [], "created": "2013-12-18T00:18:54.000+0000", "updated": "2014-01-23T23:41:16.000+0000", "description": "When  running Spark Application on yarn, it is blocked on client side:\nSPARK_JAR=./assembly/target/scala-2.9.3/spark-assembly-0.8.1-incubating-hadoop2.2.0.jar \\\n    ./spark-class org.apache.spark.deploy.yarn.Client \\\n      --jar ./assembly/target/scala-2.9.3/spark-examples-assembly-0.8.1-incubating.jar \\\n      --class org.apache.spark.examples.SparkPi \\\n      --args yarn-standalone \\\n      --name pi \\\n      --num-workers 3 \\\n      --master-memory 2g \\\n      --worker-memory 2g \\\n      --worker-cores 1\nI check the source and find a bug:\n// position: new-yarn/src/main/scala/org/apache/spark/deploy/yarn/ClientArguments.scala\n// line 91\n        case (\"--name\") :: value :: tail =>\n          appName = value\n          args = tail\nthe last args = tail is missed.", "comments": ["This was fixed in https://github.com/apache/incubator-spark/pull/257"], "derived": {"summary": "When  running Spark Application on yarn, it is blocked on client side:\nSPARK_JAR=. /assembly/target/scala-2.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Spark application is blocked when running on yarn - When  running Spark Application on yarn, it is blocked on client side:\nSPARK_JAR=. /assembly/target/scala-2."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was fixed in https://github.com/apache/incubator-spark/pull/257"}]}}
{"project": "SPARK", "issue_id": "SPARK-1004", "title": "PySpark on YARN", "status": "Resolved", "priority": "Blocker", "reporter": "Josh Rosen", "assignee": "Sandy Ryza", "labels": [], "created": "2013-12-20T12:02:04.000+0000", "updated": "2016-06-15T07:46:04.000+0000", "description": "This is for tracking progress on supporting YARN in PySpark.\n\nWe might be able to use {{yarn-client}} mode (https://spark.incubator.apache.org/docs/latest/running-on-yarn.html#launch-spark-application-with-yarn-client-mode).", "comments": ["I couldn't find anywhere it was documented how to do this.  Eventually I figured it out through trial-and-error.  It would be good if this could make it into the docs.\n\nI was able to run a simple pyspark program on YARN (CDH5.0b1 installed in pseudo-distributed mode) using the following command:\n\n{code}\nSPARK_JAR=$SPARK_HOME/assembly/target/scala-2.9.3/spark-assembly_2.9.3-0.8.1-incubating-hadoop2.2.0.jar \\\nSPARK_YARN_APP_JAR=~/testdata.txt pyspark \\\ntest1.py\n{code}\n\nin the program I call\n{code}\nsc = SparkContext(\"yarn-client\", \"Simple App\")\n{code}\nIt works.  I filed a bug regarding the requirement for SPARK_YARN_APP_JAR\nhttps://spark-project.atlassian.net/browse/SPARK-1030", "Thanks for reporting this.\n\nI think that PySpark should be able to automatically set SPARK_JAR and SPARK_YARN_APP_JAR when running under yarn-client mode, so I added some code to do that here: https://github.com/JoshRosen/incubator-spark/compare/pyspark-yarn\n\nThis works fine with a local Hadoop 2.0.5-alpha psuedo-distributed cluster, but I don't think it work correctly on an actual YARN cluster since the PySpark python libraries may not be installed.  One approach might be to package the PySpark files into a .egg and distribute it using SPARK_YARN_APP_JAR, then modify the worker launch code to use that PYTHONPATH.", "I just pushed my work-in-progress commit into my pyspark-yarn branch: \n\nhttps://github.com/JoshRosen/incubator-spark/compare/pyspark-yarn\n\nThe core changes are:\n\n- Remove reliance on SPARK_HOME on the workers.  Only the driver should know about SPARK_HOME.  On the workers, we ensure that the PySpark Python libraries are added to the PYTHONPATH.\n\n- Add a Makefile for generating a \"fat zip\" that contains PySpark's Python dependencies.  This is a bit of a hack and I'd be open to better packaging tools, but this doesn't require any extra Python libraries.  This use case doesn't seem to be well-addressed by the existing Python packaging tools: there are plenty of tools to package complete Python environments (such as pyinstaller and virtualenv) or to bundle *individual* libraries (e.g. distutils), but few to generate portable fat zips or eggs.\n\nMy last WIP commit might not actually compile and I haven't tested it on an actual YARN cluster yet.  I'm a  bit busy right now and may not have time to work on this in the next couple of weeks, so anyone who's interested should feel free to pick this up and finish it.", "Thanks Josh.  I'm going to try to take a stab at this.  The WIP commit seems to be missing the setup.py referenced in the Makefile. Did you possibly forget to git add it?", "Whoops, looks like I forgot to include the setup.py file; I pushed a new commit to add it: https://github.com/JoshRosen/incubator-spark/commit/c70f92ba79041f11f93097a3b9a9da20fc7bf316", "Thanks. I was able to get your patch to work with a few changes.  I took pyspark-assembly.zip out of SPARK_YARN_USER_JAR (this won't be required after SPARK-1053) and added it to SPARK_YARN_DIST_FILES.\n\nA couple changes that need to happen after this:\n* We shouldn't override PYTHONPATH. We should just add to it.\n* We shouldn't override SPARK_YARN_USER_ENV. We should just add to it.\n* yarn-standalone support\n** SPARK_YARN_DIST_FILES isn't used in yarn-standalone mode so we'll need to have some other way of getting pyspark-assembly.zip there\n** user should be able to give python script that gets distributed and run as well\n** Right now yarn-standalone runs a java/scala class inside the application master process, so not sure what the best way to use a python process as the driver is\n", "https://github.com/apache/incubator-spark/pull/640", "Issue resolved by pull request 30\n[https://github.com/apache/spark/pull/30]", "User 'sryza' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/30"], "derived": {"summary": "This is for tracking progress on supporting YARN in PySpark. We might be able to use {{yarn-client}} mode (https://spark.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "PySpark on YARN - This is for tracking progress on supporting YARN in PySpark. We might be able to use {{yarn-client}} mode (https://spark."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'sryza' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/30"}]}}
{"project": "SPARK", "issue_id": "SPARK-1005", "title": "Update Ning compress package to 1.0.0", "status": "Resolved", "priority": "Minor", "reporter": "Matei Alexandru Zaharia", "assignee": "Prashant Sharma", "labels": [], "created": "2013-12-21T09:24:52.000+0000", "updated": "2020-02-07T17:26:38.000+0000", "description": "I saw 1.0.0 is now out, it might have some improvements", "comments": [], "derived": {"summary": "I saw 1. 0.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Update Ning compress package to 1.0.0 - I saw 1. 0."}]}}
{"project": "SPARK", "issue_id": "SPARK-1006", "title": "MLlib ALS gets stack overflow with too many iterations", "status": "Closed", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "2013-12-21T11:02:48.000+0000", "updated": "2016-01-06T17:19:07.000+0000", "description": "The tipping point seems to be around 50. We should fix this by checkpointing the RDDs every 10-20 iterations to break the lineage chain, but checkpointing currently requires HDFS installed, which not all users will have.\n\nWe might also be able to fix DAGScheduler to not be recursive.", "comments": ["Same issue will affect Bagel at around 100 iterations.", "Hi Matei\n\n    ALS will be StackOverflow as too long linage chain(deserialization cause stackoverflow in executor), but I am not sure if we will fix this issue by changing DAGScheduler to not be recursive, because we don't break the linage chain. but it will avoid potential stackoverflow in driver(I have encountered this exception when recursively building stages) ", "Any plans to work on this or any pointers how one would go about making the needed modification?  I'm working with a dataset that doesn't appear to be converging before it runs into this limitation...", "This is fixed as part of SPARK-5955, where we use checkpointing to cut off lineage.", "Is there away to find out which build fix this problem: I am on 1.3.1 and error \n\n5/08/11 14:08:20 ERROR TaskSetManager: Task 1 in stage 4204.0 failed 4 times; aborting job\nException in thread \"main\" org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 4204.0 failed 4 times, most recent failure: Lost task 1.3 in stage 4204.0 (COMPANYDOMAIN-TAKENOUT): java.lang.StackOverflowError\n        at java.io.SerialCallbackContext.<init>(SerialCallbackContext.java:48)\n        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1890)\n        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)\n        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)\n        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)\n        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)\n        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)\n        at scala.collection.immutable.$colon$colon.readObject(List.scala:362)\n        at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\nI am doing 50 iterations \nval numIterations = 50\nALS.trainImplicit(trainData, rank, numIterations, 0.01, 0.01)", "First of all, I would like to thank you guys for developing spark and putting it open source that we can use. I'm new to Spark and Scala, and working in a project involving matrix factorizations in Spark. I have a problem regarding running ALS in Spark. It has a stackoverflow due to long linage chain as per comments on the internet. One of their suggestion is to use the setCheckpointInterval so that for every 10-20 iterations, we can checkpoint the RDDs and it prevents the error. Just want to ask details on how to do checkpointing with ALS. I am using spark-kernel developed by IBM: https://github.com/ibm-et/spark-kernel instead of spark-shell.\n\nHere are some of my specific questions regarding details on checkpoint:\n\n1. In setting checkpoint directory through SparkContext.setCheckPointDir(), it needs to be a hadoop compatible directory. Can we use any available hdfs-compatible directory?\n2. What do you mean by this comment on the code in ALS checkpointing:\nIf the checkpoint directory is not set in [[org.apache.spark.SparkContext]],\n  * this setting is ignored.\n3. Is the use of setCheckPointInterval the only code I needed to add to have checkpointing for ALS work?\n4. I am getting this error: Name: java.lang.IllegalArgumentException, Message: Wrong FS: expected file :///. How can I solve this? What is the proper way of using checkpointing.\n\nThanks a lot!\n"], "derived": {"summary": "The tipping point seems to be around 50. We should fix this by checkpointing the RDDs every 10-20 iterations to break the lineage chain, but checkpointing currently requires HDFS installed, which not all users will have.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "MLlib ALS gets stack overflow with too many iterations - The tipping point seems to be around 50. We should fix this by checkpointing the RDDs every 10-20 iterations to break the lineage chain, but checkpointing currently requires HDFS installed, which not all users will have."}, {"q": "What updates or decisions were made in the discussion?", "a": "First of all, I would like to thank you guys for developing spark and putting it open source that we can use. I'm new to Spark and Scala, and working in a project involving matrix factorizations in Spark. I have a problem regarding running ALS in Spark. It has a stackoverflow due to long linage chain as per comments on the internet. One of their suggestion is to use the setCheckpointInterval so that for every 10-20 iterations, we can checkpoint the RDDs and it prevents the error. Just want to ask details on how to do checkpointing with ALS. I am using spark-kernel developed by IBM: https://github.com/ibm-et/spark-kernel instead of spark-shell.\n\nHere are some of my specific questions regarding details on checkpoint:\n\n1. In setting checkpoint directory through SparkContext.setCheckPointDir(), it needs to be a hadoop compatible directory. Can we use any available hdfs-compatible directory?\n2. What do you mean by this comment on the code in ALS checkpointing:\nIf the checkpoint directory is not set in [[org.apache.spark.SparkContext]],\n  * this setting is ignored.\n3. Is the use of setCheckPointInterval the only code I needed to add to have checkpointing for ALS work?\n4. I am getting this error: Name: java.lang.IllegalArgumentException, Message: Wrong FS: expected file :///. How can I solve this? What is the proper way of using checkpointing.\n\nThanks a lot!"}]}}
{"project": "SPARK", "issue_id": "SPARK-1007", "title": "spark-class2.cmd should change SCALA_VERSION to be 2.10", "status": "Resolved", "priority": "Trivial", "reporter": "Qiuzhuang Lian", "assignee": "Patrick McFadin", "labels": [], "created": "2013-12-25T22:38:15.000+0000", "updated": "2013-12-29T19:01:13.000+0000", "description": "Most likely a mistake, spark-class2.cm still set scala version to be 2.9.3, it should be 2.10 instead. This is from spark git trunk.", "comments": ["Thanks for reporting this."], "derived": {"summary": "Most likely a mistake, spark-class2. cm still set scala version to be 2.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "spark-class2.cmd should change SCALA_VERSION to be 2.10 - Most likely a mistake, spark-class2. cm still set scala version to be 2."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for reporting this."}]}}
{"project": "SPARK", "issue_id": "SPARK-1008", "title": "Provide good default logging if log4j properties is not present", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "labels": [], "created": "2013-12-29T19:08:31.000+0000", "updated": "2014-03-09T17:54:53.000+0000", "description": "This is an issue now that we've excluded log4j files from our assembly jar.", "comments": [], "derived": {"summary": "This is an issue now that we've excluded log4j files from our assembly jar.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Provide good default logging if log4j properties is not present - This is an issue now that we've excluded log4j files from our assembly jar."}]}}
{"project": "SPARK", "issue_id": "SPARK-1009", "title": "Updated MLlib docs to show how to use it in Python", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Hossein Falaki", "labels": [], "created": "2013-12-30T19:29:31.000+0000", "updated": "2014-01-07T23:24:37.000+0000", "description": null, "comments": ["PR #322"], "derived": {"summary": "", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Updated MLlib docs to show how to use it in Python"}]}}
{"project": "SPARK", "issue_id": "SPARK-1010", "title": "Update all unit tests to use SparkConf instead of system properties", "status": "Resolved", "priority": "Minor", "reporter": "Patrick Wendell", "assignee": "Josh Rosen", "labels": [], "created": "2013-12-31T15:13:22.000+0000", "updated": "2015-01-31T05:54:28.000+0000", "description": null, "comments": ["Assign me this issue. I can fix this.", "Done!", "Hey Patrick\n\nBy executing the following command i could find only 2 results.\n\n*find -iname '*suite.scala' -type f -exec grep -nHI -e 'System.getProperty' {} +*\n\nresults:\n{quote}\n*./core/src/test/scala/org/apache/spark/scheduler/JobLoggerSuite.scala:98*:    val user = System.getProperty(\"user.name\", SparkContext.SPARK_UNKNOWN_USER)\n*./repl/src/test/scala/org/apache/spark/repl/ReplSuite.scala:30*:    \nval separator = System.getProperty(\"path.separator\")\n{quote}", "Is this issue still unresolved?", "Yes, lots of usage in tests still. A lot looks intentional.\n\n{code}\nfind . -name \"*Suite.scala\" -type f -exec grep -E \"System\\.[gs]etProperty\" {} \\;\n    ...\n    \"\"\".format(System.getProperty(\"user.name\", \"<unknown>\"),\n    \"\"\".format(System.getProperty(\"user.name\", \"<unknown>\")).stripMargin\n    System.setProperty(\"spark.testing\", \"true\")\n    System.setProperty(\"spark.reducer.maxMbInFlight\", \"1\")\n    System.setProperty(\"spark.storage.memoryFraction\", \"0.0001\")\n    System.setProperty(\"spark.storage.memoryFraction\", \"0.01\")\n    System.setProperty(\"spark.authenticate\", \"false\")\n    System.setProperty(\"spark.authenticate\", \"false\")\n    System.setProperty(\"spark.shuffle.manager\", \"hash\")\n    System.setProperty(\"spark.scheduler.mode\", \"FIFO\")\n    System.setProperty(\"spark.scheduler.mode\", \"FAIR\")\n    ...\n{code}\n", "please assign to me, I will fix it.", "[~joshrosen] Haven't you been doing some work towards this goal?", "Yeah, didn't realize there was already a JIRA for this.  I'll link this from my PR.", "User 'JoshRosen' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/3739", "I've merged my PR, which fixes this, into master and I'll followup later today or tomorrow with backports to the maintenance branches.\n\n(Whoops, didn't mean to edit the issue description)", "I've finished backporting this to all of the maintenance branches.", "The pull request for SPARK-5425 fixes a bug in the ResetSystemProperties trait added here.  My original implementation of that trait didn't properly reset the system properties because it didn't perform a proper clone: https://github.com/apache/spark/pull/4220#issuecomment-71992373."], "derived": {"summary": "", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Update all unit tests to use SparkConf instead of system properties"}, {"q": "What updates or decisions were made in the discussion?", "a": "The pull request for SPARK-5425 fixes a bug in the ResetSystemProperties trait added here.  My original implementation of that trait didn't properly reset the system properties because it didn't perform a proper clone: https://github.com/apache/spark/pull/4220#issuecomment-71992373."}]}}
{"project": "SPARK", "issue_id": "SPARK-1011", "title": "MatrixFactorizationModel in pyspark throws serialization error", "status": "Resolved", "priority": "Major", "reporter": "Hossein Falaki", "assignee": "Hossein Falaki", "labels": [], "created": "2014-01-02T12:42:56.000+0000", "updated": "2014-07-25T22:53:26.000+0000", "description": "When running the following sample code in pyspark, \n{code}\nfrom pyspark.mllib.recommendation import ALS\nfrom numpy import array\n\n# Load and parse the data\ndata = sc.textFile(\"mllib/data/als/test.data\")\nratings = data.map(lambda line: array([float(x) for x in line.split(',')]))\n\n# Build the recommendation model using Alternating Least Squares\nmodel = ALS.train(sc, ratings, 1, 20)\n\n# Evaluate the model on training data\nratesAndPreds = ratings.map(lambda p: (p[2], model.predict(int(p[0]), int(p[1]))))\nratesAndPreds.take(1)\n{code}\n\nI get:\n{code}\n>>> ratesAndPreds.take(1)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/Users/hossein/Projects/incubator-spark/python/pyspark/rdd.py\", line 585, in take\n    for partition in range(mapped._jrdd.splits().size()):\n  File \"/Users/hossein/Projects/incubator-spark/python/pyspark/rdd.py\", line 984, in _jrdd\n    pickled_command = CloudPickleSerializer().dumps(command)\n  File \"/Users/hossein/Projects/incubator-spark/python/pyspark/serializers.py\", line 248, in dumps\n    def dumps(self, obj): return cloudpickle.dumps(obj, 2)\n  File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 801, in dumps\n    cp.dump(obj)\n  File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 140, in dump\n    return pickle.Pickler.dump(self, obj)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 224, in dump\n    self.save(obj)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 548, in save_tuple\n    save(element)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 259, in save_function\n    self.save_function_tuple(obj, [themodule])\n  File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 316, in save_function_tuple\n    save(closure)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 600, in save_list\n    self._batch_appends(iter(obj))\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 633, in _batch_appends\n    save(x)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 259, in save_function\n    self.save_function_tuple(obj, [themodule])\n  File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 316, in save_function_tuple\n    save(closure)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 600, in save_list\n    self._batch_appends(iter(obj))\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 633, in _batch_appends\n    save(x)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 259, in save_function\n    self.save_function_tuple(obj, [themodule])\n  File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 316, in save_function_tuple\n    save(closure)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 600, in save_list\n    self._batch_appends(iter(obj))\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 636, in _batch_appends\n    save(tmp[0])\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 254, in save_function\n    self.save_function_tuple(obj, modList)\n  File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 314, in save_function_tuple\n    save(f_globals)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 181, in save_dict\n    pickle.Pickler.save_dict(self, obj)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 649, in save_dict\n    self._batch_setitems(obj.iteritems())\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 686, in _batch_setitems\n    save(v)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 331, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 631, in save_reduce\n    save(state)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 286, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 181, in save_dict\n    pickle.Pickler.save_dict(self, obj)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 649, in save_dict\n    self._batch_setitems(obj.iteritems())\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 681, in _batch_setitems\n    save(v)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 306, in save\n    rv = reduce(self.proto)\n  File \"build/bdist.macosx-10.8-intel/egg/py4j/java_gateway.py\", line 500, in __call__\n  File \"build/bdist.macosx-10.8-intel/egg/py4j/protocol.py\", line 304, in get_return_value\npy4j.protocol.Py4JError: An error occurred while calling o37.__getnewargs__. Trace:\npy4j.Py4JException: Method __getnewargs__([]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:333)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:342)\n\tat py4j.Gateway.invoke(Gateway.java:251)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:695)\n{code}\n", "comments": ["This error suggests that a Py4J object is being referenced inside of a function closure, because pickle is attempting to call __getnewargs__ on a Java object.  I haven't tried running your example, but my hunch is that the private _java_model field in MatrixFactorizationModel is being serialized.\n\nSince we're not using Py4J on the workers, I think we'd need to find a different way to pass model objects to PySpark UDFs for further processing.  This could be tricky and I don't know of any easy solution off the top of my head.", "I was working on examples for MLLib in python that I encountered this issue. One solution might be to add a predict() function to ALS that takes an RDD and returns predicted ratings. If we had such a method, I could avoid passing the model in the closure and void this issue for now (until we fix it properly).", "Yeah, actually MatrixFactorizationModel.predict() cannot be called on a cluster even in Scala, as far as I know. This is because it uses RDD operations inside it, and you can't call those within a closure on the cluster. We'd have to add a predict on RDDs if that's what you'd like to do.", "Yes, I added that. After thorough testing I will send the pull request. Hopefully someone can add the python binding for it later.", "It would be great to have python binding for the method added with <https://github.com/apache/incubator-spark/pull/328>", "This was fixed in Spark 0.9.1 with the addition of a new {{predictAll}} method for performing bulk predictions.\n\nThis was added in commit https://github.com/apache/spark/commit/b2e690f839e7ee47f405135d35170173386c5d13.", "Oh, and if you want to combine the actual vs. predicted ratings:\n\n{code}\nfrom pyspark.mllib.recommendation import ALS\nfrom numpy import array\n\n# Load and parse the data\ndata = sc.textFile(\"mllib/data/als/test.data\")\nratings = data.map(lambda line: array([float(x) for x in line.split(',')]))\n\n# Build the recommendation model using Alternating Least Squares\nmodel = ALS.train(ratings, 1, 20)\n\n# Evaluate the model on training data\npredictedRatings = model.predictAll(ratings.map(lambda x: (x[0], x[1])))\ntrueVsPredicted = ratings.map(lambda x: ((x[0], x[1]), x[2])).join(predictedRatings.map(lambda x: ((x[0], x[1]), x[2])))\ntrueVsPredicted.take(1)\n{code}"], "derived": {"summary": "When running the following sample code in pyspark, \n{code}\nfrom pyspark. mllib.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "MatrixFactorizationModel in pyspark throws serialization error - When running the following sample code in pyspark, \n{code}\nfrom pyspark. mllib."}, {"q": "What updates or decisions were made in the discussion?", "a": "Oh, and if you want to combine the actual vs. predicted ratings:\n\n{code}\nfrom pyspark.mllib.recommendation import ALS\nfrom numpy import array\n\n# Load and parse the data\ndata = sc.textFile(\"mllib/data/als/test.data\")\nratings = data.map(lambda line: array([float(x) for x in line.split(',')]))\n\n# Build the recommendation model using Alternating Least Squares\nmodel = ALS.train(ratings, 1, 20)\n\n# Evaluate the model on training data\npredictedRatings = model.predictAll(ratings.map(lambda x: (x[0], x[1])))\ntrueVsPredicted = ratings.map(lambda x: ((x[0], x[1]), x[2])).join(predictedRatings.map(lambda x: ((x[0], x[1]), x[2])))\ntrueVsPredicted.take(1)\n{code}"}]}}
{"project": "SPARK", "issue_id": "SPARK-1012", "title": "DAGScheduler Exception", "status": "Resolved", "priority": "Major", "reporter": "Hossein Falaki", "assignee": "Hossein Falaki", "labels": ["DAGScheduler", "MLLib,"], "created": "2014-01-02T16:19:27.000+0000", "updated": "2014-01-07T22:02:31.000+0000", "description": "I get \nWhen running the following code:\n{code:java}\nimport org.apache.spark.mllib.recommendation.ALS\nimport org.apache.spark.mllib.recommendation.Rating\n\n// Load and parse the data\nval data = sc.textFile(\"mllib/data/als/test.data\")\nval ratings = data.map(_.split(',') match {\n    case Array(user, item, rate) =>  Rating(user.toInt, item.toInt, rate.toDouble)\n})\n\n// Build the recommendation model using ALS\nval numIterations = 20\nval model = ALS.train(ratings, 1, 20, 0.01)\n\n// Evaluate the model on rating data\nval ratesAndPreds = ratings.map{ case Rating(user, item, rate) => (rate, model.predict(user, item))}\nval MSE = ratesAndPreds.map{ case(v, p) => math.pow((v - p), 2)}.reduce(_ + _)/ratesAndPreds.count\n{code}\n\nI get:\n{code:java}\norg.apache.spark.SparkException: Job aborted: Task 2.0:0 failed 1 times (most recent failure: Exception failure: scala.MatchError: null)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1026)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1024)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1024)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:617)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:617)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:617)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:205)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:456)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:219)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n{code}\n\nThe problem is deterministic. In addition ratesAndPreds has exactly 16 elements:\n{code:none}\nscala> ratesAndPreds.take(16)\nres1: Array[(Double, Double)] = Array((5.0,2.9995813468435633), (1.0,2.999581386744982), (5.0,2.9995813468435633), (1.0,2.999581386744982), (5.0,2.9995813468435633), (1.0,2.999581386744982), (5.0,2.9995813468435633), (1.0,2.999581386744982), (1.0,2.999581373444509), (5.0,2.999581413345928), (1.0,2.999581373444509), (5.0,2.999581413345928), (1.0,2.999581373444509), (5.0,2.999581413345928), (1.0,2.999581373444509), (5.0,2.999581413345928))\n\nscala> ratesAndPreds.count()\n{code}\n\nThrows the exception again.", "comments": ["The problem is that MatrixFactorizationModel has a reference to two RDDs (userFeatures, and productFeatures). As a result, when passed inside a closure all the bad things happen. \n\nThe solution is augmenting MatrixFactorizaitonModel with a bulk prediction method that takes an RDD of test data and returns a prediction RDD.", "Submitted this pull request to offer a viable method for bulk prediction.", "Fixed with PR #328"], "derived": {"summary": "I get \nWhen running the following code:\n{code:java}\nimport org. apache.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "DAGScheduler Exception - I get \nWhen running the following code:\n{code:java}\nimport org. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed with PR #328"}]}}
{"project": "SPARK", "issue_id": "SPARK-1213", "title": "loss function error of logistic loss", "status": "Resolved", "priority": "Major", "reporter": "Xusen Yin", "assignee": null, "labels": ["MLLib,"], "created": "2014-01-05T03:02:00.000+0000", "updated": "2014-04-01T06:34:13.000+0000", "description": "There might be a error in incubator-spark/mllib/src/main/scala/org/apache/spark/mllib/optimizatio/Gradient.scala\n\nThe loss function of class LogisticGradient might be wrong.\n\nThe original one is :\nval loss =\n  if (margin > 0) {\n      math.log(1 + math.exp(0 - margin))\n   } else {\n      math.log(1 + math.exp(margin)) - margin\n   }\n\nBut when we use this kind of loss function, we will find that the loss is increasing when optimizing, such as LogisticRegressionWithSGD.\n\nI think it should be something like this:\n\nval loss =\n      if (label > 0) {\n        math.log(1 + math.exp(margin))\n      } else {\n        math.log(1 + math.exp(margin)) - margin\n      }\n\nI tested the loss function. It works well.\n", "comments": [], "derived": {"summary": "There might be a error in incubator-spark/mllib/src/main/scala/org/apache/spark/mllib/optimizatio/Gradient. scala\n\nThe loss function of class LogisticGradient might be wrong.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "loss function error of logistic loss - There might be a error in incubator-spark/mllib/src/main/scala/org/apache/spark/mllib/optimizatio/Gradient. scala\n\nThe loss function of class LogisticGradient might be wrong."}]}}
{"project": "SPARK", "issue_id": "SPARK-1013", "title": "Have DEVELOPERS.txt file with documentation for developers", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "labels": [], "created": "2014-01-06T11:51:32.000+0000", "updated": "2014-03-25T15:12:25.000+0000", "description": "Some things to include would be:\n\n- How to run tests\n- How to run a single test in sbt or maven\n- How to build spark with assemble-deps \n", "comments": [], "derived": {"summary": "Some things to include would be:\n\n- How to run tests\n- How to run a single test in sbt or maven\n- How to build spark with assemble-deps.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Have DEVELOPERS.txt file with documentation for developers - Some things to include would be:\n\n- How to run tests\n- How to run a single test in sbt or maven\n- How to build spark with assemble-deps."}]}}
{"project": "SPARK", "issue_id": "SPARK-1014", "title": "MultilogisticRegressionWithSGD", "status": "Closed", "priority": "Major", "reporter": "Kun Yang", "assignee": null, "labels": [], "created": "2014-01-06T21:01:56.000+0000", "updated": "2015-02-20T23:50:52.000+0000", "description": "Multilogistic Regression With SGD based on mllib packages\n\nUse labeledpoint, gradientDescent to train the model", "comments": ["pr sent", "source code", "There is a typo in the comments, should be: \n\n/**\n  * Classification model trained using Multinomial Logistic Regression with category {0, 1, ..., k - 1}.\n  *\n  * @param weights Weights [w_00, w_01, ..., w_0m; w_10, w_11, ..., w_1m; ...; w_{k-2, 0}, ..., w_{k-2, m}]\n  *                where w_i0 is the intercept, i = 0, 1, ..., k - 2\n  */", "I'm curious if this is still active -- where was the PR? was this just one-vs-all LR ?", "I am not sure if you can find the pr on the repository. Please find it on\nmy github:\nhttps://github.com/kunyang1987/incubator-spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/classification/MultilogisticRegression.scala\n\n\n", "We support multinomial logistic regression with LBFGS in 1.3. I marked this JIRA as duplicated."], "derived": {"summary": "Multilogistic Regression With SGD based on mllib packages\n\nUse labeledpoint, gradientDescent to train the model.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "MultilogisticRegressionWithSGD - Multilogistic Regression With SGD based on mllib packages\n\nUse labeledpoint, gradientDescent to train the model."}, {"q": "What updates or decisions were made in the discussion?", "a": "We support multinomial logistic regression with LBFGS in 1.3. I marked this JIRA as duplicated."}]}}
{"project": "SPARK", "issue_id": "SPARK-1015", "title": "Visualize the DAG of RDD ", "status": "Resolved", "priority": "Major", "reporter": "Jeff Zhang", "assignee": null, "labels": [], "created": "2014-01-07T06:13:56.000+0000", "updated": "2015-02-27T09:52:14.000+0000", "description": "The DAG of RDD can help user understand the data flow and how spark get the final RDD executed.  It could help user to find chances to optimize the execution of some complex RDD.  I will leverage graphviz to visualize the DAG. \n\nFor this task, I plan to split it into 2 steps.\n\nStep 1.  Just visualize the simple DAG graph.  Each RDD is one node, and there will be one edge between the parent RDD and child RDD. ( I attach one simple graph in the attachments )\n\nStep 2.  Put RDD in the same stage into one sub graph. This may need to extract the splitting staging related code in DAGSchduler. \n", "comments": ["Hi, you may want to have a look at the [Spark Replay Debugger|http://spark-replay-debugger-overview.readthedocs.org/en/latest/], which has already implemented RDD DAG visualization and more :-) See [the overview document|http://spark-replay-debugger-overview.readthedocs.org/en/latest/], [JIRA-975|https://spark-project.atlassian.net/browse/SPARK-975], [PR-224|https://github.com/apache/incubator-spark/pull/224] & [PR-284|https://github.com/apache/incubator-spark/pull/284] for more details.\n\nA word about GraphViz.  GraphVis is excellent for drawing RDD DAGs, but it can't handle RDD DAG together with stage information.  Take this DAG of MLlib ALS as an example: RDD #0, #1, #2 & #3 form a stage, #0, #1, #2 & #4 form another, but in the figure, there is only #3 in the first stage.  These two stages share 3 RDDs, GraphViz can't handle this kind of overlapped subgraphs.  I am considering other visualization techniques, maybe some JS library that can be easily integrated into Spark Web UI.", "[~zjffdu] are you planning on working on this? We also have {{toDebugString}} which prints some of this info. How would the visualization work with spark-shell? Is this just a utility you can host outside Spark?", "[~sowen] I may not have time for this recently. \nbq. How would the visualization work with spark-shell? Is this just a utility you can host outside Spark?\nI would prefer to use graphviz for visualize the RDD. And spark just build the dot file for graphviz and let the graphviz to visualize it. Besides, I think integrating the DAG view to spark ui may be helpful for users to debug the RDD (especially on performance perspective ) ", "That sounds great, but also sounds like something non-core enough that it can be a separate package that users could bring in and which could be advertised at http://spark-packages.org/"], "derived": {"summary": "The DAG of RDD can help user understand the data flow and how spark get the final RDD executed. It could help user to find chances to optimize the execution of some complex RDD.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "Visualize the DAG of RDD  - The DAG of RDD can help user understand the data flow and how spark get the final RDD executed. It could help user to find chances to optimize the execution of some complex RDD."}, {"q": "What updates or decisions were made in the discussion?", "a": "That sounds great, but also sounds like something non-core enough that it can be a separate package that users could bring in and which could be advertised at http://spark-packages.org/"}]}}
{"project": "SPARK", "issue_id": "SPARK-1016", "title": "When running examples jar (compiled with maven) logs don't initialize properly", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": null, "labels": [], "created": "2014-01-07T22:20:09.000+0000", "updated": "2015-03-02T15:19:07.000+0000", "description": null, "comments": ["Is this resolved now? examples should be logging via log4j just fine at this point.", "At this point, logging seems just fine when I'm running examples, using Maven-built artifacts."], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "When running examples jar (compiled with maven) logs don't initialize properly"}, {"q": "What updates or decisions were made in the discussion?", "a": "At this point, logging seems just fine when I'm running examples, using Maven-built artifacts."}]}}
{"project": "SPARK", "issue_id": "SPARK-1017", "title": "Set the permgen even if we are calling the users sbt (via SBT_OPTS)", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "labels": [], "created": "2014-01-08T14:22:28.000+0000", "updated": "2014-10-13T18:14:39.000+0000", "description": "Now we will call the users sbt installation if they have one. But users might run into the permgen issues... so we should force the permgen unless the user explicitly overrides it.", "comments": ["As I understand, only {{sbt/sbt}} is supported for building Spark with SBT, rather than a local {{sbt}}. Maven is the primary build, and it sets {{MaxPermSize}} and {{PermGen}} for scalac and scalatest. I think this is obsolete and/or already covered then?"], "derived": {"summary": "Now we will call the users sbt installation if they have one. But users might run into the permgen issues.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Set the permgen even if we are calling the users sbt (via SBT_OPTS) - Now we will call the users sbt installation if they have one. But users might run into the permgen issues."}, {"q": "What updates or decisions were made in the discussion?", "a": "As I understand, only {{sbt/sbt}} is supported for building Spark with SBT, rather than a local {{sbt}}. Maven is the primary build, and it sets {{MaxPermSize}} and {{PermGen}} for scalac and scalatest. I think this is obsolete and/or already covered then?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1018", "title": "take and collect don't work on HadoopRDD", "status": "Resolved", "priority": "Major", "reporter": "Diana Carroll", "assignee": null, "labels": ["hadoop"], "created": "2014-01-09T10:43:29.000+0000", "updated": "2016-09-19T14:16:56.000+0000", "description": "I am reading a simple text file using hadoopFile as follows:\nvar hrdd1 = sc.hadoopFile(\"/home/training/testdata.txt\",classOf[TextInputFormat], classOf[LongWritable], classOf[Text])\n\nTesting using this simple text file:\n001 this is line 1\n002 this is line two\n003 yet another line\n\nthe data read is correct, as I can tell using println \nscala> hrdd1.foreach(println):\n(0,001 this is line 1)\n(19,002 this is line two)\n(40,003 yet another line)\n\nBut neither collect nor take work properly.  Take prints out the key (byte offset) of the last (non-existent) line repeatedly:\nscala> hrdd1.take(4):\nres146: Array[(org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text)] = Array((61,), (61,), (61,))\n\nCollect is even worse: it complains:\njava.io.NotSerializableException: org.apache.hadoop.io.LongWritable at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1183)\n\nThe problem appears to be the LongWritable in both cases, because if I map to a new RDD, converting the values from Text objects to strings, it works:\nscala> hrdd1.map(pair => (pair._1.toString,pair._2.toString)).take(4)\nres148: Array[(java.lang.String, java.lang.String)] = Array((0,001 this is line 1), (19,002 this is line two), (40,003 yet another line))\n\nSeems to me either rdd.collect and rdd.take ought to handle non-serializable types gracefully, or hadoopFile should return a mapped RDD that converts the hadoop types into the appropriate serializable Java objects.  (Or at very least the docs for the API should indicate that the usual RDD methods don't work on HadoopRDDs).\n\nBTW, this behavior is the same for both the old and new API versions of hadoopFile.  It also is the same whether the file is from HDFS or a plain old text file.\n\n", "comments": ["Hey [~dcarroll@cloudera.com] just wanted to clarify. When you create a Hadoop file and pass it specific types, it is assuming that the file is already encoded with those types (e.g. someone else wrote out as a LongWritable). I'm not sure you can just write your own text file and expect it to deserialize correctly as, e.g. a LongWritable. \n\nUsually people do this type of de-serialization in Spark itself if the data is just in textual format. E.g. they to sc.textFile(XX).map(line => line.split(\"\\t\")).map(arr => (arr(0), arr(1).toInt))", "I've been encountering a similar issue.  I've attached a custom Hadoop InputFormat (trivial example, basically a LineReader) along with the input data and the output logs.  \n\nThis is the behavior I'm seeing whether I use my custom InputFormat or Hadoop's TextInputFormat:\nscala> val data = sc.hadoopFile[LongWritable, Text, XmlInputFormat](\"test.txt\")\ndata: org.apache.spark.rdd.RDD[(org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text)] = HadoopRDD[0] at hadoopFile at <console>:13\n\nscala> data.take(1)\nres0: Array[(org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text)] = Array((311,This is line 1))\n\nscala> data.take(2)\nres1: Array[(org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text)] = Array((311,This is line 2), (311,This is line 2))\n\nAs to Patrick's comment, the [K,V] returned by the InputFormats are wrapped inside [LongWritable, Text] already if that's what you mean.\n\n\n", "I'm not sure what you mean.  The usual way of reading a text file is, as you say, sc.textFile.   The code for that method is\n{code:title=SparkContext.scala}\ndef textFile(path: String, minSplits: Int = defaultMinSplits): RDD[String] = {\n    hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text],\n      minSplits).map(pair => pair._2.toString)\n  }\n{code}\n\nAs you can see, it's simply calling hadoopFile() and telling it to use the Hadoop class TextInputFormat to read the file.  TextInputFormat does the job of reading in text input (using LineRecordReader), and outputting a key-value pair....the key being a long writable (the byte offset of the line in the file) and the value being Text (the line itself.)  textFile then ignores the key (the byte offset), and returns an RDD with just the lines from the file.\n\nIn other words, textFile() is simply a more specialized form of hadoopFile() that assumes that your input is line-oriented text.  So yeah, sc.textFile() works great if your input is line-oriented.\n\nI believe you are not correct about hadoopFile().  It doesn't expect the input file to be serialized java objects.  to the contrary, it doesn't have any expectations about the format at all...which is why you have to pass a pointer to a Hadoop InputFormat class.  That's the class that actually reads the file.  (Actually, technically, that's the class that calculates the input splits, and then creates a record RecordReader object to read the the data from the input split.)\n\nMy goal in testing out sc.hadoopFile was actually because I wanted to explore using a different input format than the default TextInputFormat.  (Hadoop includes several out of the box, including NLineInputFormat, CSVInputFormat, etc.  I also have one I've written myself called ColumnInputFormat). TextInputFormat is pretty limited...each record is assumed to begin where the last one ended, and end with a new-line.\n\nMy first test was the simplest...to call hadoopFile() with the same InputFormat class textFile() does: TextInputFormat.  That's where I encountered this bug.  \n\nNote that the LongWritable isn't what's supposed to be in the text file...it's the type of the key for the key-value pair generated by LineRecordReader.  It's the byte offset in the file.  So your code example doesn't work.  The long int value I want isn't *in* the text, it's meta-data *about* the text, specifically the byte offset of the text in the file.  textFile() throws that data away but what if I actually cared about it and wanted to use it?  That's the test case I was exploring.\n", "Hey [~dcarroll@cloudera.com] sorry for misunderstanding. You are running into two distinct issues here. The first is that Hadoop writables re-use objects in a way that makes iterating over sets of them (like Spark does) difficult. We have a fairly explicit warning in the docs for hadoopFile to make this clear. Copying from the scaladoc it says:\n\n{quote}\nNote: Because Hadoop's RecordReader class re-uses the same Writable object for each record, directly caching the returned RDD will create many references to the same object. If you plan to directly cache Hadoop writable objects, you should first copy them using a map function.\n{quote}\n\nMaybe we should improve this to say \"cache, collect, or take\" rather than just \"cache\". We actually write a whole feature to automatically clone these objects in a way that fixed this behavior, it used a cloning utility that Hadoop provides:\n\nhttp://hadoop.apache.org/docs/stable/api/org/apache/hadoop/io/WritableUtils.html#clone(T, org.apache.hadoop.conf.Configuration)\n\nUnfortunately we found in practice that there are several Writable implementations that break when you clone them, so this didn't work either. So we had to rip this out of Spark and leave the confusing semantics:\n\nhttps://github.com/apache/incubator-spark/pull/502\n\n[~sengtang] - what happens if you map your rdd using the WritableUtils clone() and then call take() does it work then? Maybe we should add a utility function to do this.\n\nA second issue is that Hadoop's types aren't themselves java serializeable. This is means Spark can't send them between nodes. For this one it's possible we could add automatic serializers but I'd have to think about this a bit. In general people tend to use the Hadoop classes when reading the data, but then they convert their data to other types before it has to be serialized. collect() and take() are an exception though (if people are exploring the data) so that might warrant trying to have more elegant behavior.", "I was able to do a take() after using clone() (or doing any sort of map transformation like you mentioned) but when I try to do a collect() I get a classNotFound exception for the InputFormat class specified.  I've attached the stack trace, and the same thing happens when I specify the Hadoop TextInputFormat class also.", "---\nsent from my phone\nOn Mar 5, 2014 7:56 AM, \"Seng Tang (JIRA)\" <jira@spark-project.atlassian.net>\n\n", "Hi Patrick, \nWe spent some time to understand why data is \"corrupted\" while working with avro objects that wasn't copied at our layer, and yes I've seen the note above newHadoopApiFile, but note doesn't describe the consequences of not copying objects, at least not for people who came to spark without deep knowledge in hadoop format(I think there are plenty of those)\n\nDo you think that even using read-avro - transform - write-avro chain can be corrupted due to not copying avro objects in the beginning? \ne.g. we've seen that several objects has same data when they shouldn't, this was solved by deep-copy of whole avro-object\n\nIf you permit me to suggest, it would be nice to have section about working with haddopRDD or newHadoopRDD which will advice on best practices and \"do-s\" and \"don't-s\" when working with hadoop files\n", "Hi,\n\nI've spent few hours trying to understand why I had *only duplicates of my last RDD item* after calling the {{collect}} API.\n\nI'm using Apache Spark 1.6.0 with Avro files stored in HDFS. Here is my workaround, hope it helps ...\n\n{code:java}\npublic JavaRDD<GenericRecord> readJavaRDD(final JavaSparkContext sparkContext,\n                                          final Schema schema,\n                                          final String path) throws IOException {\n    \n    final Configuration configuration = new Configuration();\n    configuration.set(\"avro.schema.input.key\", schema.toString());\n\n    final JavaPairRDD<AvroKey<GenericRecord>, NullWritable> rdd = sparkContext.newAPIHadoopFile(\n        path,\n        classHelper.classOf(new AvroKeyInputFormat<GenericRecord>()),\n        classHelper.classOf(new AvroKey<GenericRecord>()),\n        NullWritable.class,\n        configuration\n    );\n\n    return rdd.map(tuple -> tuple._1().datum())\n              // see the trick below - a deep copy ain't required\n              .map(record -> new GenericData.Record((GenericData.Record) record, false));\n\n}\n{code}"], "derived": {"summary": "I am reading a simple text file using hadoopFile as follows:\nvar hrdd1 = sc. hadoopFile(\"/home/training/testdata.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "take and collect don't work on HadoopRDD - I am reading a simple text file using hadoopFile as follows:\nvar hrdd1 = sc. hadoopFile(\"/home/training/testdata."}, {"q": "What updates or decisions were made in the discussion?", "a": "Hi,\n\nI've spent few hours trying to understand why I had *only duplicates of my last RDD item* after calling the {{collect}} API.\n\nI'm using Apache Spark 1.6.0 with Avro files stored in HDFS. Here is my workaround, hope it helps ...\n\n{code:java}\npublic JavaRDD<GenericRecord> readJavaRDD(final JavaSparkContext sparkContext,\n                                          final Schema schema,\n                                          final String path) throws IOException {\n    \n    final Configuration configuration = new Configuration();\n    configuration.set(\"avro.schema.input.key\", schema.toString());\n\n    final JavaPairRDD<AvroKey<GenericRecord>, NullWritable> rdd = sparkContext.newAPIHadoopFile(\n        path,\n        classHelper.classOf(new AvroKeyInputFormat<GenericRecord>()),\n        classHelper.classOf(new AvroKey<GenericRecord>()),\n        NullWritable.class,\n        configuration\n    );\n\n    return rdd.map(tuple -> tuple._1().datum())\n              // see the trick below - a deep copy ain't required\n              .map(record -> new GenericData.Record((GenericData.Record) record, false));\n\n}\n{code}"}]}}
{"project": "SPARK", "issue_id": "SPARK-1019", "title": "pyspark RDD take() throws NPE", "status": "Resolved", "priority": "Major", "reporter": "Diana Carroll", "assignee": "Josh Rosen", "labels": [], "created": "2014-01-09T14:20:36.000+0000", "updated": "2014-09-12T20:26:06.000+0000", "description": "I'm getting sporadic NPEs from pyspark that I can't narrow down, but I'm able to reproduce it consistently from the attached data file.  If I delete any single line from the file, it works; but the file as is or larger (it's a snippet of a much larger log file) causes the problem.\n\nPrinting the lines read works fine...but afterwards, I get the exception.\n\nProblem occurs with vanilla pyspark and IPython, but NOT with the scala spark shell.\n\nsc.textFile(\"testlog13\").take(5) (does not seem to matter how many lines I take).\n\nIn [32]: sc.textFile(\"testlog16\").take(5)\n14/01/09 14:16:45 INFO MemoryStore: ensureFreeSpace(69808) called with curMem=1185875, maxMem=342526525\n14/01/09 14:16:45 INFO MemoryStore: Block broadcast_16 stored as values to memory (estimated size 68.2 KB, free 325.5 MB)\n14/01/09 14:16:45 INFO FileInputFormat: Total input paths to process : 1\n14/01/09 14:16:45 INFO SparkContext: Starting job: runJob at PythonRDD.scala:288\n14/01/09 14:16:45 INFO DAGScheduler: Got job 23 (runJob at PythonRDD.scala:288) with 1 output partitions (allowLocal=true)\n14/01/09 14:16:45 INFO DAGScheduler: Final stage: Stage 23 (runJob at PythonRDD.scala:288)\n14/01/09 14:16:45 INFO DAGScheduler: Parents of final stage: List()\n14/01/09 14:16:45 INFO DAGScheduler: Missing parents: List()\n14/01/09 14:16:45 INFO DAGScheduler: Computing the requested partition locally\n14/01/09 14:16:45 INFO HadoopRDD: Input split: file:/home/training/testlog16:0+61124\n14/01/09 14:16:45 INFO PythonRDD: Times: total = 14, boot = 1, init = 3, finish = 10\n14/01/09 14:16:45 INFO SparkContext: Job finished: runJob at PythonRDD.scala:288, took 0.021146108 s\nOut[32]: \n[u'100.219.90.44 - 102 [15/Sep/2013:23:58:51 +0100] \"GET KBDOC-00087.html HTTP/1.0\" 200 8681 \"http://www.loudacre.com\"  \"Loudacre CSR Browser\" ',\n u'100.219.90.44 - 102 [15/Sep/2013:23:58:51 +0100] \"GET KBDOC-00087.html HTTP/1.0\" 200 8681 \"http://www.loudacre.com\"  \"Loudacre CSR Browser\" ',\n u'100.219.90.44 - 102 [15/Sep/2013:23:58:51 +0100] \"GET theme.css HTTP/1.0\" 200 8681 \"http://www.loudacre.com\"  \"Loudacre CSR Browser\" ',\n u'182.4.148.56 - 173 [15/Sep/2013:23:58:30 +0100] \"GET KBDOC-00076.html HTTP/1.0\" 200 17546 \"http://www.loudacre.com\"  \"Loudacre CSR Browser\" ',\n u'182.4.148.56 - 173 [15/Sep/2013:23:58:30 +0100] \"GET theme.css HTTP/1.0\" 200 17546 \"http://www.loudacre.com\"  \"Loudacre CSR Browser\" ']\n\nIn [33]: Exception in thread \"stdin writer for python\" java.lang.NullPointerException\n\tat org.apache.hadoop.fs.BufferedFSInputStream.getPos(BufferedFSInputStream.java:54)\n\tat org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:60)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:246)\n\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:275)\n\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:227)\n\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:195)\n\tat java.io.DataInputStream.read(DataInputStream.java:100)\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:211)\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)\n\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:206)\n\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:45)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:167)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:150)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:27)\n\tat scala.collection.Iterator$$anon$19.hasNext(Iterator.scala:400)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:772)\n\tat scala.collection.Iterator$$anon$19.foreach(Iterator.scala:399)\n\tat org.apache.spark.api.python.PythonRDD$$anon$2.run(PythonRDD.scala:98)\n\n\n", "comments": ["I'm getting this reliably again using the .9 rc build, using a different dataset. Will attach for ease of reproduction. (zip codes with latitude and longitude)", "Hey Diana,\n\nI just tried to reproduce this from a fresh spark build and couldn't. If you do a clean checkout of spark like I did below do you get the error?\n\n{code}\ncd /tmp\ngit clone https://git-wip-us.apache.org/repos/asf/incubator-spark.git -b branch-0.9\ncd incubator-spark\nsbt/sbt assembly\nwget https://spark-project.atlassian.net/secure/attachment/12402/testlog13\n./bin/pyspark\n>>> sc.textFile(\"testlog13\").take(5)\n[u'165.32.101.206 - 8 [15/Sep/2013:23:59:50 +0100] \"GET theme.css HTTP/1.0\" 200 17446 \"http://www.loudacre.com\"  \"Loudacre CSR Browser\" ', u'100.219.90.44 - 102 [15/Sep/2013:23:58:51 +0100] \"GET KBDOC-00087.html HTTP/1.0\" 200 8681 \"http://www.loudacre.com\"  \"Loudacre CSR Browser\" ', u'100.219.90.44 - 102 [15/Sep/2013:23:58:51 +0100] \"GET theme.css HTTP/1.0\" 200 8681 \"http://www.loudacre.com\"  \"Loudacre CSR Browser\" ', u'182.4.148.56 - 173 [15/Sep/2013:23:58:30 +0100] \"GET KBDOC-00076.html HTTP/1.0\" 200 17546 \"http://www.loudacre.com\"  \"Loudacre CSR Browser\" ', u'182.4.148.56 - 173 [15/Sep/2013:23:58:30 +0100] \"GET theme.css HTTP/1.0\" 200 17546 \"http://www.loudacre.com\"  \"Loudacre CSR Browser\" ']\n{code}\n\n", "I'm not getting the problem from that sample file -- it was sporadic when I was testing.  But I'm now getting it consistently from the latlon.tsv file I just uploaded (a tab-delimited list of zip codes with latitude and longitude).  \n\nI just did a fresh build from your instructions and am still getting the problem  In case it is helpful I will attach a log file with debugging enabled.  (Doesn't seem to give much more info I'm afraid.)\n\nI notice that the actual exception is occurring in Hadoop LineRecordReader.  Maybe my issue has something to do with the version of Hadoop I'm using?  I've tested this in CDH5b1 and the (yet to be released) CDH5b2.\n\n\n", "debug log", "I just thought to test the latlon.tsv file using HDFS instead of the local file system.  That works fine.  It is only loading it from the local file system that is giving me problems.\n\nI also note that the take() is successful...it displays the requested records and *then* throws NPE for whatever that's worth.", "Thanks for reporting this and for including sample data.  I was able to reproduce this bug using the latest master branch, Hadoop 1.0.4, and a local pyspark shell:\n\n{code}\n>>> sc.textFile(\"latlon.tsv\").take(1)\n14/01/30 18:35:00 INFO MemoryStore: ensureFreeSpace(34262) called with curMem=34238, maxMem=311387750\n14/01/30 18:35:00 INFO MemoryStore: Block broadcast_1 stored as values to memory (estimated size 33.5 KB, free 296.9 MB)\n14/01/30 18:35:00 INFO FileInputFormat: Total input paths to process : 1\n14/01/30 18:35:00 INFO SparkContext: Starting job: take at <stdin>:1\n14/01/30 18:35:00 INFO DAGScheduler: Got job 1 (take at <stdin>:1) with 1 output partitions (allowLocal=true)\n14/01/30 18:35:00 INFO DAGScheduler: Final stage: Stage 1 (take at <stdin>:1)\n14/01/30 18:35:00 INFO DAGScheduler: Parents of final stage: List()\n14/01/30 18:35:00 INFO DAGScheduler: Missing parents: List()\n14/01/30 18:35:00 INFO DAGScheduler: Computing the requested partition locally\n14/01/30 18:35:00 INFO HadoopRDD: Input split: file:/Users/joshrosen/Documents/spark/spark/latlon.tsv:0+1127316\n14/01/30 18:35:00 INFO PythonRDD: Times: total = 39, boot = 2, init = 37, finish = 0\n14/01/30 18:35:00 INFO SparkContext: Job finished: take at <stdin>:1, took 0.043407 s\n[u'00210\\t43.005895\\t-71.013202']\n>>> Exception in thread \"stdin writer for python\" java.lang.NullPointerException\n\tat org.apache.hadoop.fs.BufferedFSInputStream.getPos(BufferedFSInputStream.java:48)\n\tat org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:41)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:214)\n\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:237)\n\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:189)\n\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:158)\n\tat java.io.DataInputStream.read(DataInputStream.java:100)\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:134)\n\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:133)\n\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:38)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:164)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:149)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:27)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:350)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:232)\n\tat org.apache.spark.api.python.PythonRDD$$anon$2.run(PythonRDD.scala:85)\n{code}\n\nI used this stacktrace with [BrainLeg|http://brainleg.com/], a structural stacktrace search engine, and found a few other projects that have run into similar NPEs:\n\n- https://issues.apache.org/jira/browse/HCATALOG-626\n- https://github.com/facebook/presto/pull/950\n\nThe exception seems to be dependent on the size of the input file, since the job runs without any exceptions if I run on a small sample of lines from the original input.  I'll check out those links to see how those other projects may have fixed this issue.  I'll also try running under a debugger to see if I can track down where the null is being introduced.", "Interestingly, I can only reproduce the NPE when calling an action that only reads a portion of the input file, like first() or take().\n\nIf I call\n\n{code}\nsc.textFile(\"latlon.tsv\").collect()[0]\n{code}\n\nwhich forces the entire file to be read, then I don't see any exception.\n\nMy current theory is that there's a race condition between the Python daemon's stdin writer consuming its iterator and the job completion callbacks closing that iterator.\n\nImagine that I run {{sc.textFile().take(n )}} in Scala Spark.  The data flow here is entirely pull-based and we should only read as much of the file as is necessary to return the first {{n}} records.  When the job completes, the Hadoop iterator is closed using a job completion callback.\n\nNow, consider what happens when I run this locally using PySpark.  PySpark's data flow is a mixture of push and pull with backpressure: the stdin writer thread pulls records from Hadoop or cached RDDs and pushes them into the Python worker process (where network buffer sizes provide backpressure), which pushes its output records back to Java.  If the job finishes before the Python worker has consumed all of its input, the job completion callbacks might close the Hadoop iterator while the PySpark stdin writer thread is still attempting to consume it, leading to a NullPointerException because it calls a closed data stream.\n\nThis theory explains why the above {{collect()}} call didn't throw a NPE: since the entire input needed to be consumed before the job could complete, the stdin writer thread would never attempt to read from a closed stream.  It also explains why you didn't see NPEs for small inputs: if the input is small enough, the Python worker will have enough time and network buffer space to receive the entire input before the job completes, even if it only iterates over its first few input records.\n\nI'll try tracing through the execution in more detail to verify if this is what's causing the exception.\n\nWhen fixing this, we'll have to be careful to avoid introducing race conditions.  If we simply added job completion callbacks to stop the PythonRDD stdin writer thread, we'd have to worry about a race between that callback and the Hadoop iterator callback.  Maybe we could guarantee that completion callbacks are called in reverse order of registration.\n\nIt looks like the underlying bug might have been introduced in https://github.com/apache/incubator-spark/commit/b9d6783f36d527f5082bf13a4ee6fd108e97795c, which optimized PySpark's {{take()}} to push the limit into {{mapPartitions()}} calls to avoid computing the entire first partition.  That commit added a try-catch block that seems to imply that IOExceptions could occur if the Python worker didn't consume its entire input:\n\n{code}\n} catch {\ncase e: IOException =>\n// This can happen for legitimate reasons if the Python code stops returning data before we are done\n// passing elements through, e.g., for take(). Just log a message to say it happened.\nlogInfo(\"stdin writer to Python finished early\")\nlogDebug(\"stdin writer to Python finished early\", e)\n}\n{code}\n\nThis seems a little fishy, since it's implicitly detecting the Python worker process being finished rather than using an explicit signal from the worker to PythonRDD.  Also, IOException is too broad of an exception, which was the root cause of SPARK-1025, so we should remove this catch.", "Okay I spent some time today playing with this. Josh I confirmed that the issue is what you suspected - the callback for the HadoopRDD is getting called and invalidating the input stream. I've proposed a patch here that I found fixed the issue for me locally. I think my patch still has a potential race, but it least decreases the odds of this exception substantially so I'd say it's strictly better than what's there now.", "[~dcarroll@cloudera.com] If you could test this patch that would be great.", "Will do.  (Just back from vacation so probably not today!)\n\n\nOn Mon, Mar 10, 2014 at 12:09 AM, Patrick Wendell (JIRA) <\n\n", "Forgot to link the associated pull request:\nhttps://github.com/apache/spark/pull/112", "I tested the patch and it works!  I tested it side by side with unpatched 0.9.  Unpatched = NPE in the latlon.tsv test file; patched = no NPE.\n\nThanks.", "I think the fixed proposed here is the best we can do given the current cancellation support in Spark. [~joshrosen] please feel free to re-open this if you see a nicer solution. There may very well be one.\n\nhttps://github.com/apache/spark/pull/112/files", "I'm still skeptical of the {{catch IOException}} code, since it's been involved in two bugs now, so we should probably revisit that at some point, but this seems like an okay fix for now for the NPE issues."], "derived": {"summary": "I'm getting sporadic NPEs from pyspark that I can't narrow down, but I'm able to reproduce it consistently from the attached data file. If I delete any single line from the file, it works; but the file as is or larger (it's a snippet of a much larger log file) causes the problem.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "pyspark RDD take() throws NPE - I'm getting sporadic NPEs from pyspark that I can't narrow down, but I'm able to reproduce it consistently from the attached data file. If I delete any single line from the file, it works; but the file as is or larger (it's a snippet of a much larger log file) causes the problem."}, {"q": "What updates or decisions were made in the discussion?", "a": "I'm still skeptical of the {{catch IOException}} code, since it's been involved in two bugs now, so we should probably revisit that at some point, but this seems like an okay fix for now for the NPE issues."}]}}
{"project": "SPARK", "issue_id": "SPARK-1020", "title": "SparkListener interfaces should not expose internal types/objects", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": null, "labels": [], "created": "2014-01-09T22:31:40.000+0000", "updated": "2014-03-17T10:20:37.000+0000", "description": null, "comments": ["Hi Patrick,\nCould you please describe this issue: \n1. Which Spark version is affected?\n2. What types/objects are exposed?\n\nThanks in advance.", "1. In the latest versions of Spark, including 0.8, 0.9, and the latest master branch.\n2. ActiveJob, StageInfo", "I think initially the XXXInfo classes were intended to be for downstream consumption . But now they point to internal data types such as Stage, RDD, etc. We should either make XXInfo not have pointers to the internal objects, or create a new class that stores data for the stage, and have this one not contain any pointers to internal objects.", "I think that it could be done by extending constructors of *Info classes with required fields instead of creating new classes which in terms would lead to more complexity and types. I think keeping things small is better. I also noticed that event class names could be refactored\n\nTrait SparkListenerEvents should be named SparkListenerEvent or just SparkEvent.\nEvent classes for SparkListener should end with \"Event\" and not start with trait name, for example:\nJobEndEvent instead of SparkListenerJobEnd (it is too verbose)."], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "SparkListener interfaces should not expose internal types/objects"}, {"q": "What updates or decisions were made in the discussion?", "a": "I think that it could be done by extending constructors of *Info classes with required fields instead of creating new classes which in terms would lead to more complexity and types. I think keeping things small is better. I also noticed that event class names could be refactored\n\nTrait SparkListenerEvents should be named SparkListenerEvent or just SparkEvent.\nEvent classes for SparkListener should end with \"Event\" and not start with trait name, for example:\nJobEndEvent instead of SparkListenerJobEnd (it is too verbose)."}]}}
{"project": "SPARK", "issue_id": "SPARK-1021", "title": "sortByKey() launches a cluster job when it shouldn't", "status": "Resolved", "priority": "Major", "reporter": "Andrew Ash", "assignee": "Erik Erlandson", "labels": [], "created": "2014-01-09T22:47:47.000+0000", "updated": "2015-11-12T05:19:43.000+0000", "description": "The sortByKey() method is listed as a transformation, not an action, in the documentation.  But it launches a cluster job regardless.\n\nhttp://spark.incubator.apache.org/docs/latest/scala-programming-guide.html\n\nSome discussion on the mailing list suggested that this is a problem with the rdd.count() call inside Partitioner.scala's rangeBounds method.\n\nhttps://github.com/apache/incubator-spark/blob/master/core/src/main/scala/org/apache/spark/Partitioner.scala#L102\n\nJosh Rosen suggests that rangeBounds should be made into a lazy variable:\n\n{quote}\nI wonder whether making RangePartitoner .rangeBounds into a lazy val would fix this (https://github.com/apache/incubator-spark/blob/6169fe14a140146602fb07cfcd13eee6efad98f9/core/src/main/scala/org/apache/spark/Partitioner.scala#L95).  We'd need to make sure that rangeBounds() is never called before an action is performed.  This could be tricky because it's called in the RangePartitioner.equals() method.  Maybe it's sufficient to just compare the number of partitions, the ids of the RDDs used to create the RangePartitioner, and the sort ordering.  This still supports the case where I range-partition one RDD and pass the same partitioner to a different RDD.  It breaks support for the case where two range partitioners created on different RDDs happened to have the same rangeBounds(), but it seems unlikely that this would really harm performance since it's probably unlikely that the range partitioners are equal by chance.\n{quote}\n\nCan we please make this happen?  I'll send a PR on GitHub to start the discussion and testing.", "comments": ["I'm observing the same behavior in 0.9.0 and would add to the affects version/s field but I can't edit this task with my permissions.  You'd think the reporter of a bug would be able to edit it!", "I just granted you permission, [~ash211].", "Note that if we do this, we'll need a similar fix in Python, which may be trickier.", "I tried making the change of making that val lazy and a cluster job was still launched, so it'll need to be a more involved fix than just adding lazy.\n\nThe PR for that is probably somewhere on an old github repo.", "https://github.com/ash211/spark/commit/a62e828234d5b69585495593730032f2877932ae\n\n\n\n", "I actually played with the idea and just turning the {{rangeBounds}} variable into a lazy one doesn't work. That makes the variable be evaluated only when the transformation is executed on the worker nodes; at that point, you can't execute actions (which are needed to compute {{rangeBounds}}).\n\nOne way to work around this would be to have something be evaluated on the RDDs when the scheduler walks the graph before submitting jobs to the workers. I'm not aware of such functionality in the code, though. Or maybe there's something cleaner that can be done here?", "I deferred the compute of the partition bounds this way, and seems to work properly in my testing and the unit tests:\nhttps://github.com/erikerlandson/spark/compare/erikerlandson:rdd_drop_master...spark-1021\n", "User 'erikerlandson' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1689", "This was reverted due to the commit breaking correlationoptimizer14\n\nhttps://github.com/apache/spark/pull/1689#issuecomment-57106886\n\n", "@Erik Erlandson any update on this?", "User 'erikerlandson' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/3079", "This problem also occurred on Hive on Spark (HIVE-9370. Could we take this forward?", "I'd be happy to look into this and https://issues.apache.org/jira/plugins/servlet/mobile#issue/SPARK-4514 ."], "derived": {"summary": "The sortByKey() method is listed as a transformation, not an action, in the documentation. But it launches a cluster job regardless.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "sortByKey() launches a cluster job when it shouldn't - The sortByKey() method is listed as a transformation, not an action, in the documentation. But it launches a cluster job regardless."}, {"q": "What updates or decisions were made in the discussion?", "a": "I'd be happy to look into this and https://issues.apache.org/jira/plugins/servlet/mobile#issue/SPARK-4514 ."}]}}
{"project": "SPARK", "issue_id": "SPARK-1022", "title": "Add unit tests for kafka streaming", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Saisai Shao", "labels": [], "created": "2014-01-11T09:43:02.000+0000", "updated": "2015-08-06T00:21:53.000+0000", "description": "It would be nice if we could add unit tests to verify elements of kafka's stream. Right now we do integration tests only which makes it hard to upgrade versions of kafka. The place to start here would be to look at how kafka tests itself and see if the functionality can be exposed to third party users.", "comments": ["User 'tdas' has created a pull request for this issue:\n[https://github.com/apache/spark/pull/557|https://github.com/apache/spark/pull/557]", "User 'jerryshao' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1751", "User 'tdas' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1797", "There was a follow up to this issue:\nhttps://github.com/apache/spark/pull/1797", "User 'srowen' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1804", "[~tdas] and [~jerryshao2015] I was working on writing Integration test for my application, which reads data from kafka, performs transformation on the data and then writes the Dstream back to other kafka topic. To do this I had to get KafkaTestUtils and some other dependent classes locally as I needed embedded zookeeper and kafa queue. I also had to modify sendMessage to accept my specific type of ket and value. I was wondering if it is possible to make the KafkaTestUtils generic? so that others can use it for in-memory  unit/integration testing. I have created generic sendMessage method hopefully it's of some use. \n{code}\nclass KafkaTestUtils [T,U]{\n.......\n  // Kafka producer\n  private var producer: Producer[T, U] = _\n\n..........\n\n  def sendMessages(topic: String, messageToFreq: JMap[T, U]): Unit = {\n    import scala.collection.JavaConversions._\n    producer = new Producer[T, U](new ProducerConfig(producerConfiguration))\n    for ((k,v) <- messageToFreq) {\n      var message = new KeyedMessage(topic, k,v)\n      producer.send(message)\n    }\n    producer.close()\n    producer = null\n  }\n\n.........\n}\n{code}\n\nI also had to change producer properties to provide key and value serialization class. It might be worth making possible to set these values from user test. I did not create JIRA for the request as I wanted to see if this is even a acceptable request. Please let me know.", "Hi [~anujojha], {{KafkaTestUtils}} is used for Kafka and Spark integration test, it is not public to users to use it, so the API design is not so generic for everyone use. I think Spark itself will not provide such functionality for user to do integration test, if you want to do such thing, from my point it would be better for you to do it yourself, not replying on Spark.\n\n"], "derived": {"summary": "It would be nice if we could add unit tests to verify elements of kafka's stream. Right now we do integration tests only which makes it hard to upgrade versions of kafka.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Add unit tests for kafka streaming - It would be nice if we could add unit tests to verify elements of kafka's stream. Right now we do integration tests only which makes it hard to upgrade versions of kafka."}, {"q": "What updates or decisions were made in the discussion?", "a": "Hi [~anujojha], {{KafkaTestUtils}} is used for Kafka and Spark integration test, it is not public to users to use it, so the API design is not so generic for everyone use. I think Spark itself will not provide such functionality for user to do integration test, if you want to do such thing, from my point it would be better for you to do it yourself, not replying on Spark."}]}}
{"project": "SPARK", "issue_id": "SPARK-1023", "title": "Remove Thread.sleep(5000) from TaskSchedulerImpl", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": null, "labels": [], "created": "2014-01-11T16:02:27.000+0000", "updated": "2014-11-06T17:42:16.000+0000", "description": "This causes the unit tests to take super long. We should figure out why this exists and see if we can lower it or do something smarter.", "comments": ["https://github.com/apache/incubator-spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala", "I think @Mridul Muralidharan added this. Perhaps he can comment on this.", "From Mridul:\n\nHi,\n\n\n  Cant seem to login to that site, but the rationale is to ensure that all pending messages (on the fly) are actually sent out before we shutdown : as in, not just write to output streams/buffers, but to wire and reasonably ensure received on other side.\n\nIt is not a graceful fix - just to alleviate immediate concerns : not relevant for local mode, more relevant in cluster mode - particularly helps in congested clusters.\n\n\nRegards,\nMridul\n", "I found that we have actually waited for a response to StopDriver from the driver\n\nif (driverActor != null) {\n        val future = driverActor.ask(StopDriver)(timeout)\n        Await.ready(future, timeout)\n      }\n\nBy intuitive, we should not need to wait for the other 1000 ms. However, when I removed Thread.sleep() and tested on my laptop, it failed...\n\nthen I checked the driverActor side code\n\ncase StopDriver =>\n        sender ! true\n        context.stop(self)\n  \nit sends the response first and then stop itself...so I guess the failed testcase is caused by the scheduling order of threads in my laptop\n\nThread 1 -> sender!true\nThread 2-> TaskSchedulerImpl.scala, goes forward and jumps out from override def stop() {} -> test case\nThread 1-> context.stop(itself)      \n\nso the test case failed because context.stop(itself) hasn't been executed\n\nwe cannot simply change the order of \n\nsender ! true\ncontext.stop(self)\n\nI'm thinking about how to do this...\n", " confused... I cannot reproduce the bug in both local mode and cluster mode"], "derived": {"summary": "This causes the unit tests to take super long. We should figure out why this exists and see if we can lower it or do something smarter.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Remove Thread.sleep(5000) from TaskSchedulerImpl - This causes the unit tests to take super long. We should figure out why this exists and see if we can lower it or do something smarter."}, {"q": "What updates or decisions were made in the discussion?", "a": "confused... I cannot reproduce the bug in both local mode and cluster mode"}]}}
{"project": "SPARK", "issue_id": "SPARK-1024", "title": "-XX:+UseCompressedStrings is actually dropped in jdk7", "status": "Resolved", "priority": "Trivial", "reporter": "Chen Chao", "assignee": "Reynold Xin", "labels": ["JVM", "UseCompressedStrings"], "created": "2014-01-14T01:18:26.000+0000", "updated": "2014-02-09T22:32:53.000+0000", "description": "hi, guys,\n     -XX:+UseCompressedStrings is actually dropped in jdk7, but the Tuning Guide(http://spark.incubator.apache.org/docs/latest/tuning.html) is still recommend users to use this. JVM will warn us \"Java HotSpot(TM) 64-Bit Server VM warning: ignoring option UseCompressedStrings; support was removed in 7.0\" if we use this option. so,maybe we can remove the point from the guide now. \n     If the option is added back in the future, we also can add it back to the guide.\n      more details can access this url : http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=7129417", "comments": ["Thanks for reporting. Do you mind submitting a pull request to fix the doc?", "ok, i will fix it.", "pull request SPARK-1024 : https://github.com/apache/incubator-spark/pull/439", "Merged by Reynold on Jan 15"], "derived": {"summary": "hi, guys,\n     -XX:+UseCompressedStrings is actually dropped in jdk7, but the Tuning Guide(http://spark. incubator.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "-XX:+UseCompressedStrings is actually dropped in jdk7 - hi, guys,\n     -XX:+UseCompressedStrings is actually dropped in jdk7, but the Tuning Guide(http://spark. incubator."}, {"q": "What updates or decisions were made in the discussion?", "a": "Merged by Reynold on Jan 15"}]}}
{"project": "SPARK", "issue_id": "SPARK-1025", "title": "pyspark hangs when parent base file is unavailable", "status": "Resolved", "priority": "Major", "reporter": "Diana Carroll", "assignee": "Josh Rosen", "labels": [], "created": "2014-01-14T08:55:29.000+0000", "updated": "2014-01-26T00:23:49.000+0000", "description": "Working with a file in pyspark.  These steps work fine:\n\n{code}\nmydata = sc.textFile(\"somefile\")\nmyfiltereddata = mydata.filter(some filter)\nmyfiltereddata.count()\n{code}\n\nBut then I delete \"somefile\" from the file system and attempt to run\n{code}\nmyfiltereddata.count()\n{code}\n\nThis hangs indefinitely.  I eventually hit Ctrl-C and it displayed a stack trace.  (I will attach the output as a file.)  \n\nIt works in scala though.  If I do the same thing, the last line produces an expected error message:\n{code}\n14/01/14 08:41:43 ERROR Executor: Exception in task ID 4\njava.io.FileNotFoundException: File file:somefile does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:520)\n{code}", "comments": ["I turned on debug-level logging with log4j.properties and found the problem: PythonRDD's stdin writer thread was catching, logging, and ignoring the FileNotFoundException rather than allowing it to propagate and fail the job:\n\n{code}\n14/01/23 17:42:54 INFO HadoopRDD: Input split: file:/tmp/test.txt:0+0\n14/01/23 17:42:54 INFO PythonRDD: stdin writer to Python finished early\n14/01/23 17:42:54 DEBUG PythonRDD: stdin writer to Python finished early\njava.io.FileNotFoundException: File file:/tmp/test.txt does not exist.\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:397)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:251)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:125)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:283)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:427)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:78)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:51)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:168)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:161)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:73)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:232)\n\tat org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:232)\n\tat org.apache.spark.api.python.PythonRDD$$anon$2.run(PythonRDD.scala:81)\n{code}\n\nI submitted a pull request to fix this: https://github.com/apache/incubator-spark/pull/504"], "derived": {"summary": "Working with a file in pyspark. These steps work fine:\n\n{code}\nmydata = sc.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "pyspark hangs when parent base file is unavailable - Working with a file in pyspark. These steps work fine:\n\n{code}\nmydata = sc."}, {"q": "What updates or decisions were made in the discussion?", "a": "I turned on debug-level logging with log4j.properties and found the problem: PythonRDD's stdin writer thread was catching, logging, and ignoring the FileNotFoundException rather than allowing it to propagate and fail the job:\n\n{code}\n14/01/23 17:42:54 INFO HadoopRDD: Input split: file:/tmp/test.txt:0+0\n14/01/23 17:42:54 INFO PythonRDD: stdin writer to Python finished early\n14/01/23 17:42:54 DEBUG PythonRDD: stdin writer to Python finished early\njava.io.FileNotFoundException: File file:/tmp/test.txt does not exist.\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:397)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:251)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:125)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:283)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:427)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:78)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:51)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:168)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:161)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:73)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:232)\n\tat org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:232)\n\tat org.apache.spark.api.python.PythonRDD$$anon$2.run(PythonRDD.scala:81)\n{code}\n\nI submitted a pull request to fix this: https://github.com/apache/incubator-spark/pull/504"}]}}
{"project": "SPARK", "issue_id": "SPARK-1026", "title": "PySpark using deprecated mapPartitionsWithSplit", "status": "Resolved", "priority": "Minor", "reporter": "Mark Hamstra", "assignee": "Josh Rosen", "labels": [], "created": "2014-01-14T08:56:44.000+0000", "updated": "2014-01-23T21:08:22.000+0000", "description": "In the Scala API, mapPartitionsWithSplit has been deprecated for a long time and really should be removed.  However, PySpark uses mapPartitionsWithSplit instead of mapPartitionsWithIndex, so mapPartitionsWithSplit now can't be removed until it has first been deprecated in PySpark for at least one release.", "comments": ["I've submitted a pull request for this: https://github.com/apache/incubator-spark/pull/505"], "derived": {"summary": "In the Scala API, mapPartitionsWithSplit has been deprecated for a long time and really should be removed. However, PySpark uses mapPartitionsWithSplit instead of mapPartitionsWithIndex, so mapPartitionsWithSplit now can't be removed until it has first been deprecated in PySpark for at least one release.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "PySpark using deprecated mapPartitionsWithSplit - In the Scala API, mapPartitionsWithSplit has been deprecated for a long time and really should be removed. However, PySpark uses mapPartitionsWithSplit instead of mapPartitionsWithIndex, so mapPartitionsWithSplit now can't be removed until it has first been deprecated in PySpark for at least one release."}, {"q": "What updates or decisions were made in the discussion?", "a": "I've submitted a pull request for this: https://github.com/apache/incubator-spark/pull/505"}]}}
{"project": "SPARK", "issue_id": "SPARK-1027", "title": "Standalone cluster should use default spark home if not specified by user", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Nan Zhu", "labels": ["starter"], "created": "2014-01-15T11:03:41.000+0000", "updated": "2014-01-22T18:58:54.000+0000", "description": "I added a quick fix to this in PR #442 (only in 0.9.0). But a nicer fix would be to have SparkHome be an Option[String] in ApplicationDescription and then to percolate that option all the way to the worker.\n\nWhatever fix we get should merge into both 0.8 and 0.9 branches.", "comments": ["made a PR in https://github.com/apache/incubator-spark/pull/447"], "derived": {"summary": "I added a quick fix to this in PR #442 (only in 0. 9.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Standalone cluster should use default spark home if not specified by user - I added a quick fix to this in PR #442 (only in 0. 9."}, {"q": "What updates or decisions were made in the discussion?", "a": "made a PR in https://github.com/apache/incubator-spark/pull/447"}]}}
{"project": "SPARK", "issue_id": "SPARK-1028", "title": "spark-shell automatically set MASTER  fails", "status": "Resolved", "priority": "Major", "reporter": "Chen Chao", "assignee": null, "labels": ["spark-shell"], "created": "2014-01-15T19:05:26.000+0000", "updated": "2014-03-02T18:56:01.000+0000", "description": "spark-shell intends to set MASTER automatically if we do not provide the option when we start the shell , but there's a problem. \nThe condition is \"if [[ \"x\" != \"x$SPARK_MASTER_IP\" && \"y\" != \"y$SPARK_MASTER_PORT\" ]];\"     we sure will set SPARK_MASTER_IP explicitly, the SPARK_MASTER_PORT option, however, we probably do not set just using spark default port 7077. So if we do not set  SPARK_MASTER_PORT, the condition will never be true. We should just use default port if users do not set port explicitly I think.", "comments": ["if needed , i'll provide a patch. ", "Sure. Please do. Thanks!", "OK, i'll pull request today.", "SPARK-1028 pull request at https://github.com/apache/incubator-spark/pull/449/files", "already merged to master. please close.", "@Reynold, it seems that 0.9.0 did not fix this ?", "I think it was merged into master for 1.0.0 and 0.9.1."], "derived": {"summary": "spark-shell intends to set MASTER automatically if we do not provide the option when we start the shell , but there's a problem. The condition is \"if [[ \"x\" != \"x$SPARK_MASTER_IP\" && \"y\" != \"y$SPARK_MASTER_PORT\" ]];\"     we sure will set SPARK_MASTER_IP explicitly, the SPARK_MASTER_PORT option, however, we probably do not set just using spark default port 7077.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "spark-shell automatically set MASTER  fails - spark-shell intends to set MASTER automatically if we do not provide the option when we start the shell , but there's a problem. The condition is \"if [[ \"x\" != \"x$SPARK_MASTER_IP\" && \"y\" != \"y$SPARK_MASTER_PORT\" ]];\"     we sure will set SPARK_MASTER_IP explicitly, the SPARK_MASTER_PORT option, however, we probably do not set just using spark default port 7077."}, {"q": "What updates or decisions were made in the discussion?", "a": "I think it was merged into master for 1.0.0 and 0.9.1."}]}}
{"project": "SPARK", "issue_id": "SPARK-1029", "title": "spark Window shell script errors regarding shell script location reference", "status": "Resolved", "priority": "Minor", "reporter": "Qiuzhuang Lian", "assignee": null, "labels": [], "created": "2014-01-15T23:10:46.000+0000", "updated": "2015-01-24T22:17:24.000+0000", "description": "When launch spark-shell.cmd in Window 7, I got following errors\n\nE:\\projects\\amplab\\incubator-spark>bin\\spark-shell.cmd\n'E:\\projects\\amplab\\incubator-spark\\bin\\..\\sbin\\spark-class2.cmd' is not recognized as an internal or external command,\noperable program or batch file.\n\nE:\\projects\\amplab\\incubator-spark>bin\\spark-shell.cmd\n'\"E:\\projects\\amplab\\incubator-spark\\bin\\..\\sbin\\compute-classpath.cmd\"' is not recognized as an internal or external co\nmmand,\noperable program or batch file.\nException in thread \"main\" java.lang.NoClassDefFoundError: org/apache/spark/repl/Main\n\nI am attaching my patches,\n\n", "comments": ["Thanks for submitting a patch. Do you mind submitting a github pull request against https://github.com/apache/incubator-spark ?", "PR #: https://github.com/apache/incubator-spark/pull/451\n", "Looks like this was fixed in https://github.com/apache/spark/commit/4e510b0b0c8a69cfe0ee037b37661caf9bf1d057 for 1.0.0"], "derived": {"summary": "When launch spark-shell. cmd in Window 7, I got following errors\n\nE:\\projects\\amplab\\incubator-spark>bin\\spark-shell.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "spark Window shell script errors regarding shell script location reference - When launch spark-shell. cmd in Window 7, I got following errors\n\nE:\\projects\\amplab\\incubator-spark>bin\\spark-shell."}, {"q": "What updates or decisions were made in the discussion?", "a": "Looks like this was fixed in https://github.com/apache/spark/commit/4e510b0b0c8a69cfe0ee037b37661caf9bf1d057 for 1.0.0"}]}}
{"project": "SPARK", "issue_id": "SPARK-1215", "title": "Clustering: Index out of bounds error", "status": "Resolved", "priority": "Major", "reporter": "dewshick", "assignee": "Joseph K. Bradley", "labels": [], "created": "2014-01-17T05:24:22.000+0000", "updated": "2014-08-27T08:57:00.000+0000", "description": "code:\nimport org.apache.spark.mllib.clustering._\n\nval test = sc.makeRDD(Array(4,4,4,4,4).map(e => Array(e.toDouble)))\nval kmeans = new KMeans().setK(4)\nkmeans.run(test) evals with java.lang.ArrayIndexOutOfBoundsException\n\nerror:\n14/01/17 12:35:54 INFO scheduler.DAGScheduler: Stage 25 (collectAsMap at KMeans.scala:243) finished in 0.047 s\n14/01/17 12:35:54 INFO spark.SparkContext: Job finished: collectAsMap at KMeans.scala:243, took 16.389537116 s\nException in thread \"main\" java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat com.simontuffs.onejar.Boot.run(Boot.java:340)\n\tat com.simontuffs.onejar.Boot.main(Boot.java:166)\nCaused by: java.lang.ArrayIndexOutOfBoundsException: -1\n\tat org.apache.spark.mllib.clustering.LocalKMeans$.kMeansPlusPlus(LocalKMeans.scala:47)\n\tat org.apache.spark.mllib.clustering.KMeans$$anonfun$19.apply(KMeans.scala:247)\n\tat org.apache.spark.mllib.clustering.KMeans$$anonfun$19.apply(KMeans.scala:244)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)\n\tat scala.collection.immutable.Range.foreach(Range.scala:81)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:233)\n\tat scala.collection.immutable.Range.map(Range.scala:46)\n\tat org.apache.spark.mllib.clustering.KMeans.initKMeansParallel(KMeans.scala:244)\n\tat org.apache.spark.mllib.clustering.KMeans.run(KMeans.scala:124)\n\tat Clustering$$anonfun$1.apply$mcDI$sp(Clustering.scala:21)\n\tat Clustering$$anonfun$1.apply(Clustering.scala:19)\n\tat Clustering$$anonfun$1.apply(Clustering.scala:19)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)\n\tat scala.collection.immutable.Range.foreach(Range.scala:78)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:233)\n\tat scala.collection.immutable.Range.map(Range.scala:46)\n\tat Clustering$.main(Clustering.scala:19)\n\tat Clustering.main(Clustering.scala)\n\t... 6 more", "comments": ["The error was due to small number of points and large k. The k-means|| initialization doesn't collect more than k candidates. This is very unlikely to appear in practice because k is much smaller than number of points. I will re-visit this issue once we implement better weighted sampling algorithms.", "I don't think that the problem is about size of dataset. I've faced with similar issue on dataset  with about 900 items. As a workaround we've decided to fallback with random init mode.\n\n", "attach test dataset\nMLLib failed to find 4 centers with k-means|| init mode on this data\n", "Submitted fix as PR 1407: https://github.com/apache/spark/pull/1407\n\nMade default behavior to return k clusters still, with some duplicated", "Issue resolved by pull request 1468\n[https://github.com/apache/spark/pull/1468]"], "derived": {"summary": "code:\nimport org. apache.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Clustering: Index out of bounds error - code:\nimport org. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 1468\n[https://github.com/apache/spark/pull/1468]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1030", "title": "unneeded file required when running pyspark program using yarn-client", "status": "Resolved", "priority": "Major", "reporter": "Diana Carroll", "assignee": "Josh Rosen", "labels": [], "created": "2014-01-17T11:19:38.000+0000", "updated": "2014-07-25T01:42:23.000+0000", "description": "I can successfully run a pyspark program using the yarn-client master using the following command:\n{code}\nSPARK_JAR=$SPARK_HOME/assembly/target/scala-2.9.3/spark-assembly_2.9.3-0.8.1-incubating-hadoop2.2.0.jar \\\nSPARK_YARN_APP_JAR=~/testdata.txt pyspark \\\ntest1.py\n{code}\n\nHowever, the SPARK_YARN_APP_JAR doesn't make any sense; it's a Python program, and therefore there's no JAR.  If I don't set the value, or if I set the value to a non-existent files, Spark gives me an error message.  \n{code}\npy4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: env SPARK_YARN_APP_JAR is not set\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:46)\n{code}\n\nor\n\n{code}\npy4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.io.FileNotFoundException: File file:dummy.txt does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:520)\n{code}\n\nMy program is very simple:\n\n{code}\nfrom pyspark import SparkContext\ndef main():\n    sc = SparkContext(\"yarn-client\", \"Simple App\")\n    logData = sc.textFile(\"hdfs://localhost/user/training/weblogs/2013-09-15.log\")\n    numjpgs = logData.filter(lambda s: '.jpg' in s).count()\n    print \"Number of JPG requests: \" + str(numjpgs)\n{code}\n\nAlthough it reads the SPARK_YARN_APP_JAR file, it doesn't use the file at all; I can point it at anything, as long as it's a valid, accessible file, and it works the same.\n\nAlthough there's an obvious workaround for this bug, it's high priority from my perspective because I'm working on a course to teach people how to do this, and it's really hard to explain why this variable is needed!", "comments": ["using pyspark to submit is deprecated in spark 1.0 in favor of spark-submit. i think this should be closed as resolved/workfix. /cc: [~pwendell] [~joshrosen]", "Closing this now, since it was addressed as part of Spark 1.0's PySpark on YARN patches (including SPARK-1004)."], "derived": {"summary": "I can successfully run a pyspark program using the yarn-client master using the following command:\n{code}\nSPARK_JAR=$SPARK_HOME/assembly/target/scala-2. 9.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "unneeded file required when running pyspark program using yarn-client - I can successfully run a pyspark program using the yarn-client master using the following command:\n{code}\nSPARK_JAR=$SPARK_HOME/assembly/target/scala-2. 9."}, {"q": "What updates or decisions were made in the discussion?", "a": "Closing this now, since it was addressed as part of Spark 1.0's PySpark on YARN patches (including SPARK-1004)."}]}}
{"project": "SPARK", "issue_id": "SPARK-1031", "title": "FileNotFoundException when running simple Spark app on Yarn", "status": "Resolved", "priority": "Major", "reporter": "Sanford Ryza", "assignee": null, "labels": [], "created": "2014-01-19T01:20:50.000+0000", "updated": "2014-01-20T15:05:33.000+0000", "description": "I hit a FileNotFoundException in the application master when running the SparkPi example as described in the docs against Yarn 2.2.\n\nThe problem appears to be that the app master requires the app jar to be in its working directory with the same name as returned by SparkContext.jarOfClass.  A symlink called app.jar to the app jar exists in the working directory, but jar itself lives in the NodeManager local cache dir with the name it was submitted with (e.g. spark-examples-assembly-0.9.0-incubating-SNAPSHOT.jar).  When adding the jar, the SparkContext only uses the last component in its path:\n{code}\n            if (SparkHadoopUtil.get.isYarnMode() && master == \"yarn-standalone\") {\n              // In order for this to work in yarn standalone mode the user must specify the \n              // --addjars option to the client to upload the file into the distributed cache \n              // of the AM to make it show up in the current working directory.\n              val fileName = new Path(uri.getPath).getName()\n              try {\n                env.httpFileServer.addJar(new File(fileName))\n{code}\n\nIf SparkContext.jarOfClass returns the jar in its linked-to location, the file will not be found.\n\nWhen I modified things to set fileName to use the jar's full path, my app completed successfully.", "comments": ["You can work around this by specifying the --addJars option.\n\nTo make this more user friendly and not have to specify the --addJars option we made a small change:\nhttps://github.com/apache/incubator-spark/pull/470/", "With that change, an error will still be logged in normal operation, right?  If I am giving a jar through --jar, should I also need to --addJar it?", "yeah it will still log the error and your application will most likely fail later on when it tries to use it.  \n\nTo clarify this is non spark examples case.  The spark examples don't really need the jar distributed again since its the app jar.  In a \"real\" application that uses sc.addJar it will log an error if it doesn't exist and then later on will most likely fail since the jar won't have been distributed.", "not sure what you mean by --jar option.. If you sc.addJar something then for yarn you have to specify --addJars to distributed it to the application master where it can be distributed through the normal spark addJar HttpFileServer mechanism.", "\nSpoke to Patrick offline and I think I understand the fundamental issue behind the strangeness.  Let me know if this is incorrect:\nThe app jar is distributed through a different mechanism than additional added jars.  The app jar gets to every worker node as a Yarn local resource.  Additional jars only get to the app master, and the app master serves them to workers with the HTTP file server.  The strangeness comes when an application addJar's the app jar, which is a natural thing to do in mesos or standalone mode, but in Yarn mode, will try to distribute the same jar through a different mechanism.\n\nWould it make sense to use the same mechanism for both of these?  I.e. to ship the app jar to workers through the HTTP file server or to ship additional jars to workers as Yarn localized resources?\n", "An additional benefit of using the same mechanism is that it would greatly simplify debugging ClassNotFoundExceptions in workers. This is a painful exercise on any framework, but gets harder with each different way that bits can get sent over.", "This is something Patrick and I spoke about and I plan to look into.", "I should add feel free to take a look at it if you want.  I was planning on looking more to distributed the app.jar via the spark mechanism rather then the yarn mechamism", "Cool, I will try to take a look.  Filed SPARK-1035 for this.  Is there a reason you think the app.jar mechanism is preferable to the Yarn mechanism?  With the Yarn mechanism we could avoid sending the jars if another app already has them cached. It also requires fewer copies even when they aren't already cached.", "The main reason I was going to start there was to keep it consistent with the way spark does it. Should be easier to maintain, less changes for the on yarn side, easier for people to understand what is going on if they have used standalone/mesos mode, etc.\n\nBut as you say the performance needs to be evaluated.  If its slower or if we thinking people will use the public distributed cache then we should leave it using yarn.   I generally don't see many people using the public distributed cache for their application jars. Either on Spark or MR. That might not be true with other people though.  ", "bq. I generally don't see many people using the public distributed cache for their application jars.\nYARN-1492 should make the shared distributed cache a more attractive (and hopefully the default) option.  I've spoken to a few people who say that, on MapReduce, loading jars which many jobs have in common is a significant part of their job time.  It would be nice for Spark not to be behind MR and Tez in this regard. ", "I agree. Even without YARN-1492 the yarn distributed cache should be better, especially if the incase of a large # of workers.  With the spark HttpFileServer they are fetching from one server vs being distributed to multiple nodes from HDFS. \n\nThe downside again is that its different, even between yarn-client and yarn-standalone mode.  I guess we could add the functionality to yarn-client mode and if they specify the --addJars type parameter then it uses the distributed cache instead of the HttpFileServer."], "derived": {"summary": "I hit a FileNotFoundException in the application master when running the SparkPi example as described in the docs against Yarn 2. 2.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "FileNotFoundException when running simple Spark app on Yarn - I hit a FileNotFoundException in the application master when running the SparkPi example as described in the docs against Yarn 2. 2."}, {"q": "What updates or decisions were made in the discussion?", "a": "I agree. Even without YARN-1492 the yarn distributed cache should be better, especially if the incase of a large # of workers.  With the spark HttpFileServer they are fetching from one server vs being distributed to multiple nodes from HDFS. \n\nThe downside again is that its different, even between yarn-client and yarn-standalone mode.  I guess we could add the functionality to yarn-client mode and if they specify the --addJars type parameter then it uses the distributed cache instead of the HttpFileServer."}]}}
{"project": "SPARK", "issue_id": "SPARK-1032", "title": "If Yarn app fails before registering, app master stays around long after", "status": "Resolved", "priority": "Major", "reporter": "Sanford Ryza", "assignee": "Sandy Ryza", "labels": [], "created": "2014-01-19T01:37:20.000+0000", "updated": "2014-04-04T20:51:08.000+0000", "description": "My Spark/YARN app hit an error while initializing its SparkContext (YARN-1031).   The app stayed around for another 5 minutes continually logging \"14/01/18 23:58:59 INFO impl.AMRMClientImpl: Waiting for application to be successfully unregistered.\" until YARN finally killed it.  This is probably because it died before registering in the first place, but still tried to unregister.", "comments": ["https://github.com/apache/incubator-spark/pull/648", "pr https://github.com/apache/spark/pull/28 committed."], "derived": {"summary": "My Spark/YARN app hit an error while initializing its SparkContext (YARN-1031). The app stayed around for another 5 minutes continually logging \"14/01/18 23:58:59 INFO impl.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "If Yarn app fails before registering, app master stays around long after - My Spark/YARN app hit an error while initializing its SparkContext (YARN-1031). The app stayed around for another 5 minutes continually logging \"14/01/18 23:58:59 INFO impl."}, {"q": "What updates or decisions were made in the discussion?", "a": "pr https://github.com/apache/spark/pull/28 committed."}]}}
{"project": "SPARK", "issue_id": "SPARK-1033", "title": "Ask for cores in Yarn container requests ", "status": "Resolved", "priority": "Major", "reporter": "Sanford Ryza", "assignee": "Sandy Ryza", "labels": [], "created": "2014-01-19T01:39:17.000+0000", "updated": "2014-04-04T20:51:05.000+0000", "description": "Yarn 2.2 has support for requesting cores in addition to memory.  Spark against Yarn 2.2 should include cores in its resource requests in the same way it includes memory.", "comments": [], "derived": {"summary": "Yarn 2. 2 has support for requesting cores in addition to memory.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Ask for cores in Yarn container requests  - Yarn 2. 2 has support for requesting cores in addition to memory."}]}}
{"project": "SPARK", "issue_id": "SPARK-1034", "title": "Py4JException on PySpark Cartesian Result", "status": "Resolved", "priority": "Major", "reporter": "Taka Shinagawa", "assignee": "Josh Rosen", "labels": [], "created": "2014-01-19T12:27:51.000+0000", "updated": "2014-07-26T22:26:26.000+0000", "description": "RDD operations on results of the Pyspark Cartesian method return Py4JException.\n\n\n\nHere's a few examples\n{code:title=$ bin/pyspark|borderStyle=solid}\n>>> rdd1=sc.parallelize([1,2,3,4,5,1])\n>>> rdd2=sc.parallelize([11,12,13,14,15,11])\n>>> rdd1.cartesian(rdd2).map(lambda x: x[0] + x[1]).collect()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/pyspark/rdd.py\", line 446, in collect\n    bytesInJava = self._jrdd.collect().iterator()\n  File \"/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/pyspark/rdd.py\", line 1041, in _jrdd\n    class_tag)\n  File \"/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/lib/py4j-0.8.1-src.zip/py4j/java_gateway.py\", line 669, in __call__\n  File \"/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/lib/py4j-0.8.1-src.zip/py4j/protocol.py\", line 304, in get_return_value\npy4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.python.PythonRDD. Trace:\npy4j.Py4JException: Constructor org.apache.spark.api.python.PythonRDD([class org.apache.spark.rdd.CartesianRDD, class [B, class java.util.HashMap, class java.util.ArrayList, class java.lang.Boolean, class java.lang.String, class java.util.ArrayList, class org.apache.spark.Accumulator, class scala.reflect.ManifestFactory$$anon$2]) does not exist\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:184)\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:202)\n\tat py4j.Gateway.invoke(Gateway.java:213)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:744)\n\n\n>>> \n>>> rdd1.cartesian(rdd2).count()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/pyspark/rdd.py\", line 525, in count\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/pyspark/rdd.py\", line 516, in sum\n    return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)\n  File \"/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/pyspark/rdd.py\", line 482, in reduce\n    vals = self.mapPartitions(func).collect()\n  File \"/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/pyspark/rdd.py\", line 446, in collect\n    bytesInJava = self._jrdd.collect().iterator()\n  File \"/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/pyspark/rdd.py\", line 1041, in _jrdd\n    class_tag)\n  File \"/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/lib/py4j-0.8.1-src.zip/py4j/java_gateway.py\", line 669, in __call__\n  File \"/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/lib/py4j-0.8.1-src.zip/py4j/protocol.py\", line 304, in get_return_value\npy4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.python.PythonRDD. Trace:\npy4j.Py4JException: Constructor org.apache.spark.api.python.PythonRDD([class org.apache.spark.rdd.CartesianRDD, class [B, class java.util.HashMap, class java.util.ArrayList, class java.lang.Boolean, class java.lang.String, class java.util.ArrayList, class org.apache.spark.Accumulator, class scala.reflect.ManifestFactory$$anon$2]) does not exist\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:184)\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:202)\n\tat py4j.Gateway.invoke(Gateway.java:213)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:744)\n{code}\n\nI see this issue after the custom serializer change.\nhttps://github.com/apache/incubator-spark/commit/cbb7f04aef2220ece93dea9f3fa98b5db5f270d6", "comments": ["I can confirm that I see this bug as well using Spark 0.9.1.  It appears that collect() works correctly:\n\n>>> test_rdd1 = sc.parallelize(range(5), 2)\n>>> test_rdd2 = test_rdd1.cartesian(test_rdd1)\n>>> print test_rdd2.collect()\n[(0, 0), (0, 1), (1, 0), (1, 1), (0, 2), (0, 3), (1, 2), (1, 3), (0, 4), (1, 4), (2, 0), (2, 1), (3, 0), (3, 1), (4, 0), (4, 1), (2, 2), (2, 3), (3, 2), (3, 3), (2, 4), (3, 4), (4, 2), (4, 3), (4, 4)]\n\nHowever, other operations like map(), count(), and reduce() produce the same exception that Taka has reported above.", "After spending a little bit of time delving into the code, I can say that this problem appears to occur at line 1038 of rdd.py when the PythonRDD object is constructed.\n\n        python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(),\n            bytearray(pickled_command), env, includes, self.preservesPartitioning,\n            self.ctx.pythonExec, broadcast_vars, self.ctx._javaAccumulator,\n            class_tag)\n\nIt appears that Py4J can't find the corresponding Java constructor when this function is called on a Cartesian RDD.  I added the following debugging code immediately before line 1038:\n\n        print '---'\n        print self._prev_jrdd.rdd()\n        print len(bytearray(pickled_command))\n        print env\n        print includes\n        print self.preservesPartitioning\n        print self.ctx.pythonExec\n        print broadcast_vars\n        print self.ctx._javaAccumulator\n        print class_tag\n        print '---'\n\nThis prints out each argument that is passed to the PythonRDD constructor.   Having made this change, I ran this in Spark:\n\n  test_rdd = sc.parallelize(range(5), 2)  \n  print test_rdd.count()                            # THIS WORKS!!\n\nwhich produced this debugging output on the console:\n\n------\nParallelCollectionRDD[0] at parallelize at PythonRDD.scala:206\n1431\n{}\n[]\nFalse\npython\n[]\n[]\nArray[byte]\n---\n\nHowever, when I run this:\n\n  test_rdd1 = sc.parallelize(range(5), 2)  \n  test_rdd2= test_rdd1.cartesian(test_rdd1)\n  print test_rdd2.count()                          # FAILS WITH Py4J EXCEPTION\n\nI get this debugging output:\n\n---\nCartesianRDD[1] at cartesian at NativeMethodAccessorImpl.java:-2\n1506\n{}\n[]\nFalse\npython\n[]\n[]\nObject\n---\n\nSince both ParallelCollectionRDD and CartesianRDD are subclasses of the same RDD superclass, I suspect that the meaningful difference between the \"working\" and \"not working\" version is the last argument to the constructor: the class_tag.  It appears that the class_tag of a cartesian RDD is a Scala object of some sort, whereas the class_tag of the first RDD was Array[Byte].  \n\nI spent a little bit of time poking around PythonRDD.scala to see if I could understand why the constructor only worked with the first set of arguments, but I'm afraid I cannot see what is causing this bug.  But, hopefully you will find this info useful when you fix this bug.  Thanks!", "Michael, thanks for the info. I'm also looking into the same place.", "Thanks for reporting this bug, and for the detailed investigation so far.\n\nMichael, in both cases in your example, {{class_tag}} is actually a Scala ClassTag object and that {{Array\\[Byte\\]}} and {{Object}} are the results of calling {{toString()}} on those ClassTags.\n\nIn cartesian, we're trying to wrap a {{JavaPairRDD<Array<Byte>, Array<Byte>>}} into a PythonRDD.  The ClassTag for this {{JavaRDD<Tuple2<Array<Byte>, Array<Byte>>>}} should be a {{ClassTag<Tuple2<Array<Byte>, Array<Byte>>}} (or a {{ClassTag<Tuple2<?, ?>>}}).  The root problem here may actually be in JavaPairRDD's classTag:\n\n{code}\nscala> val x = sc.parallelize(Seq(Array[Byte](0))).map(x => (x, x))\nx: org.apache.spark.rdd.RDD[(Array[Byte], Array[Byte])] = MappedRDD[7] at map at <console>:18\n\nscala> JavaPairRDD.fromJavaRDD(x).classTag\nres15: scala.reflect.ClassTag[(Array[Byte], Array[Byte])] = Object\n\nscala> JavaRDD.fromRDD(x).classTag\nres17: scala.reflect.ClassTag[(Array[Byte], Array[Byte])] = scala.Tuple2\n{code}\n\nThis turns out to be caused by a line in JavaPairRDD that constructed its ClassTag by casting an AnyRef ClassTag, when it should have just used the underlying ScalaRDD's {{elementClassTag}} (since it's already of the right type):\n\n{code}\noverride val classTag: ClassTag[(K, V)] =\n    implicitly[ClassTag[AnyRef]].asInstanceOf[ClassTag[Tuple2[K, V]]]\n{code}\n\nshould be \n\n{code}\n  override val classTag: ClassTag[(K, V)] = rdd.elementClassTag\n{code}\n\nI've submitted this fix as part of a pull request: https://github.com/apache/incubator-spark/pull/501", "Josh, \nthanks for the quick fix! I've confirmed this change fixes the problem.", "Hi Josh,\n\nYes, I can confirm that this fixes it for me as well.  Tremendous thanks for taking a look at this, and for taking the time to explain the fix.  \n\nNow back to data analysis!\n\nBest,\n\n  -Michael"], "derived": {"summary": "RDD operations on results of the Pyspark Cartesian method return Py4JException. Here's a few examples\n{code:title=$ bin/pyspark|borderStyle=solid}\n>>> rdd1=sc.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Py4JException on PySpark Cartesian Result - RDD operations on results of the Pyspark Cartesian method return Py4JException. Here's a few examples\n{code:title=$ bin/pyspark|borderStyle=solid}\n>>> rdd1=sc."}, {"q": "What updates or decisions were made in the discussion?", "a": "Hi Josh,\n\nYes, I can confirm that this fixes it for me as well.  Tremendous thanks for taking a look at this, and for taking the time to explain the fix.  \n\nNow back to data analysis!\n\nBest,\n\n  -Michael"}]}}
{"project": "SPARK", "issue_id": "SPARK-1035", "title": "Use a single mechanism for distributing jars on Yarn", "status": "Resolved", "priority": "Major", "reporter": "Sanford Ryza", "assignee": null, "labels": [], "created": "2014-01-20T00:16:38.000+0000", "updated": "2014-04-07T20:06:24.000+0000", "description": "When running Spark on Yarn, the app jar is distributed through a different mechanism than additional added jars. The app jar gets to every worker node as a Yarn local resource. Additional jars only get to the app master, and the app master serves them to workers with the HTTP file server.   Strangeness comes when an application addJar's the app jar, which is a natural thing to do in mesos or standalone mode, but in Yarn mode, will try to distribute the same jar through a different mechanism.  Using the same mechanism for both would eliminate this issue, as well as greatly simplify debugging ClassNotFoundExceptions in workers.", "comments": ["When I originally filed this, I didn't realize that jars could be added at runtime.  In light of this, I don't think we can do much better than the current state of things."], "derived": {"summary": "When running Spark on Yarn, the app jar is distributed through a different mechanism than additional added jars. The app jar gets to every worker node as a Yarn local resource.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Use a single mechanism for distributing jars on Yarn - When running Spark on Yarn, the app jar is distributed through a different mechanism than additional added jars. The app jar gets to every worker node as a Yarn local resource."}, {"q": "What updates or decisions were made in the discussion?", "a": "When I originally filed this, I didn't realize that jars could be added at runtime.  In light of this, I don't think we can do much better than the current state of things."}]}}
{"project": "SPARK", "issue_id": "SPARK-1036", "title": ".gitignore is overly aggressive", "status": "Resolved", "priority": "Major", "reporter": "Sean Mackrory", "assignee": "Patrick Wendell", "labels": [], "created": "2014-01-20T14:02:20.000+0000", "updated": "2020-02-07T17:19:49.000+0000", "description": "The .gitignore file ignores \"lib/\" but this includes directories named lib anywhere in the tree, including the following:\n\n{code}\n./python/lib\n./graphx/src/main/scala/org/apache/spark/graphx/lib\n./graphx/src/test/scala/org/apache/spark/graphx/lib\n{code}\n\nIt would be confusing and incorrect if any patches accidentally omitted changes to the contents of these directories.", "comments": ["Ooops ....", "Here's one suggestion. I don't know if there are other lib/ directories that show up after some actions that should also be ignored, and there are probably other entries that are intended to be ignored in the root directory only, like /logs/, etc...", "I'm not even sure why we want to exclude the lib folder. Unlike lib_managed, jars in lib are meant to be included. Looks like [~prashant] included it originally. Can you comment on this, [~prashant]?", "[~mackrorysd] Could you submit this patch as a PR?", "Hey Reynold,\n\nI added it initially thinking of the ability to \"provide\" jars in sbt, as in sbt puts everything kept in lib folder on the classpath for run. Since at that point we were not having anything in lib/ so I added it in ignore so that it does not get accidentally committed. But now since we want it we can surely remove the entry from .gitignore or I even liked [~mackrorysd]'s suggestion too. ", "Thanks, everyone. I've submitted Pull Request #488 to remove this line entirely, given Prashant's comments.", "Fixed by Patrick in https://github.com/apache/spark/commit/e437069dcecea5b89c2a37f0b7b1f8dd5796b8df"], "derived": {"summary": "The. gitignore file ignores \"lib/\" but this includes directories named lib anywhere in the tree, including the following:\n\n{code}.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": ".gitignore is overly aggressive - The. gitignore file ignores \"lib/\" but this includes directories named lib anywhere in the tree, including the following:\n\n{code}."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed by Patrick in https://github.com/apache/spark/commit/e437069dcecea5b89c2a37f0b7b1f8dd5796b8df"}]}}
{"project": "SPARK", "issue_id": "SPARK-1037", "title": "the name of findTaskFromList & findTask in TaskSetManager.scala is confusing", "status": "Resolved", "priority": "Minor", "reporter": "Nan Zhu", "assignee": "Ilya Ganelin", "labels": ["starter"], "created": "2014-01-21T11:01:05.000+0000", "updated": "2014-12-15T22:51:56.000+0000", "description": "the name of these two functions is confusing \n\nthough in the comments the author said that the method does \"dequeue\" tasks from the list but from the name, it is not explicitly indicating that the method will mutate the parameter\n\nin \n\n{code}\nprivate def findTaskFromList(list: ArrayBuffer[Int]): Option[Int] = {\n    while (!list.isEmpty) {\n      val index = list.last\n      list.trimEnd(1)\n      if (copiesRunning(index) == 0 && !successful(index)) {\n        return Some(index)\n      }\n    }\n    None\n  }\n{code}", "comments": ["at least findAndDequeueTasks is better?\n\nIt just confused me when I read the source code... \"why I didn't see any method removing those launched tasks?\"..finally I found them in find* methods\n\nHi, administrators,  any comments? \n\nIf you think it worth changing, please assign it to me, I'm glad to make a quick fix\n", "also, I personally think it is better to make findTaskFromList an inner function of findTask?", "the same problem on findSpeculativeTask..", "User 'ilganeli' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/3665", "Issue resolved by pull request 3665\n[https://github.com/apache/spark/pull/3665]"], "derived": {"summary": "the name of these two functions is confusing \n\nthough in the comments the author said that the method does \"dequeue\" tasks from the list but from the name, it is not explicitly indicating that the method will mutate the parameter\n\nin \n\n{code}\nprivate def findTaskFromList(list: ArrayBuffer[Int]): Option[Int] = {\n    while (!list. isEmpty) {\n      val index = list.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "the name of findTaskFromList & findTask in TaskSetManager.scala is confusing - the name of these two functions is confusing \n\nthough in the comments the author said that the method does \"dequeue\" tasks from the list but from the name, it is not explicitly indicating that the method will mutate the parameter\n\nin \n\n{code}\nprivate def findTaskFromList(list: ArrayBuffer[Int]): Option[Int] = {\n    while (!list. isEmpty) {\n      val index = list."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 3665\n[https://github.com/apache/spark/pull/3665]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1038", "title": "Add more fields in JsonProtocol and add tests that verify the JSON itself", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "OuyangJin", "labels": ["starter"], "created": "2014-01-22T15:29:32.000+0000", "updated": "2014-02-21T19:38:52.000+0000", "description": "Over time we've added fields to various types that aren't exposed in the protocol, we should do an inventory and add important ones.\n\nAlso, we should have tests that actually verify the parsing. For instance, say someone changes a field type from T to Option[T]... these will render differently but right now there is no test to verify the stringified output.", "comments": ["I'll try on this. Can someone put me as Assignee. Thanks!", "PR is here:\nhttps://github.com/apache/incubator-spark/pull/551", "This issue should be closed"], "derived": {"summary": "Over time we've added fields to various types that aren't exposed in the protocol, we should do an inventory and add important ones. Also, we should have tests that actually verify the parsing.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add more fields in JsonProtocol and add tests that verify the JSON itself - Over time we've added fields to various types that aren't exposed in the protocol, we should do an inventory and add important ones. Also, we should have tests that actually verify the parsing."}, {"q": "What updates or decisions were made in the discussion?", "a": "This issue should be closed"}]}}
{"project": "SPARK", "issue_id": "SPARK-1039", "title": "In-cluster driver will retry infinitely when failed to start unless the user kill it manually", "status": "Resolved", "priority": "Major", "reporter": "Nan Zhu", "assignee": "Nan Zhu", "labels": [], "created": "2014-01-22T20:11:25.000+0000", "updated": "2014-03-09T17:41:04.000+0000", "description": "in the current implementation, the in-cluster driver will fall into a dead loop when it fails to start unless the user kill it manually\n\nin DriverRunner.scala: \n\nLine 209: keepTrying = supervise && exitCode != 0 && !killed\n\n", "comments": ["made a PR https://github.com/apache/incubator-spark/pull/472\n\nthere I set an upper bound for the retry times of the in-cluster driver\n\nusers specify that with spark.driver.maxretrynum", "The intended design is that if you select \"supervise\" you want it to retry indefinitely. So I don't think this is a bug."], "derived": {"summary": "in the current implementation, the in-cluster driver will fall into a dead loop when it fails to start unless the user kill it manually\n\nin DriverRunner. scala: \n\nLine 209: keepTrying = supervise && exitCode != 0 && !killed.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "In-cluster driver will retry infinitely when failed to start unless the user kill it manually - in the current implementation, the in-cluster driver will fall into a dead loop when it fails to start unless the user kill it manually\n\nin DriverRunner. scala: \n\nLine 209: keepTrying = supervise && exitCode != 0 && !killed."}, {"q": "What updates or decisions were made in the discussion?", "a": "The intended design is that if you select \"supervise\" you want it to retry indefinitely. So I don't think this is a bug."}]}}
{"project": "SPARK", "issue_id": "SPARK-1040", "title": "Collect as Map throws a casting exception when run on a JavaPairRDD object", "status": "Resolved", "priority": "Minor", "reporter": "Kevin Mader", "assignee": "Josh Rosen", "labels": [], "created": "2014-01-23T02:03:47.000+0000", "updated": "2015-09-23T14:29:16.000+0000", "description": "The error that arises\n{code}\nException in thread \"main\" java.lang.ClassCastException: [Ljava.lang.Object; cannot be cast to [Lscala.Tuple2;\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:427)\n\tat org.apache.spark.api.java.JavaPairRDD.collectAsMap(JavaPairRDD.scala:409)\n{code}\n\nThe code being executed\n{code:java}\npublic static String ImageSummary(final JavaPairRDD<Integer,int[]> inImg) {\n\t\tfinal Set<Integer> keyList=inImg.collectAsMap().keySet();\n\t\tfor(Integer cVal: keyList) outString+=cVal+\",\";\n\t\treturn outString;\n\t}\n{code}\n\nThe line 426-427 from PairRDDFunctions.scala\n{code:java}\n def collectAsMap(): Map[K, V] = {\n    val data = self.toArray()\n{code}", "comments": ["I wonder where that cast is being performed, since there's no explicit cast at that line.  Does the traceback have any more detail?\n\nJavaPairRDD.collectAsMap() is tested at least once in JavaAPISuite: https://github.com/apache/incubator-spark/blob/master/core/src/test/scala/org/apache/spark/JavaAPISuite.java#L290, so I wonder why that test works but your code fails.", "I tried it again and it's definitely throws the same error. Could it be a type issue since I am using an array of primitives (int[]) in the JavaPairRDD, which Java doesn't always so eagerly support particularly as a generic?", "I was able to reproduce this:\n\n{code}\n@Test\n  public void collectAsMapWithIntArrayValues() {\n    // Regression test for SPARK-1040\n    JavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(new Integer[] { 1 }));\n    JavaPairRDD<Integer, int[]> pairRDD = rdd.map(new PairFunction<Integer, Integer, int[]>() {\n      @Override\n      public Tuple2<Integer, int[]> call(Integer x) throws Exception {\n        return new Tuple2<Integer, int[]>(x, new int[] { x });\n      }\n    });\n    pairRDD.collect();  // works fine\n    Map<Integer, int[]> map = pairRDD.collectAsMap();  // crashed originally\n  }\n{code}\n\nI see the same traceback that you reported above.\n\nIf I create a JavaPairRDD using SparkContext.parallelizePairs(), collectAsMap() works fine; the problem seems to only affect JavaPairRDDs that have been derived by transforming non-JavaPairRDDs.\n\nAlso, it only seems to affect collectAsMap(); calling collect() on the same RDD works fine.  From the error, I suspect that the failing cast has to do with an implicit conversion to PairRDDFunctions.", "I found a fix and opened a pull request here: https://github.com/apache/incubator-spark/pull/511", "The github link above is not functional anymore.", "I am getting a similar error in Spark 1.3.0... see a new ticket I created:  https://issues.apache.org/jira/browse/SPARK-10762", "My ticket SPARK-10762 may have just been a user error, but was interesting none-the-less... evidently Scala or Spark is not correctly reporting the type of an ArrayBuffer as ArrayBuffer[Any], but claimed it had correctly been cast to ArrayBuffer[(Int,String)]."], "derived": {"summary": "The error that arises\n{code}\nException in thread \"main\" java. lang.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Collect as Map throws a casting exception when run on a JavaPairRDD object - The error that arises\n{code}\nException in thread \"main\" java. lang."}, {"q": "What updates or decisions were made in the discussion?", "a": "My ticket SPARK-10762 may have just been a user error, but was interesting none-the-less... evidently Scala or Spark is not correctly reporting the type of an ArrayBuffer as ArrayBuffer[Any], but claimed it had correctly been cast to ArrayBuffer[(Int,String)]."}]}}
{"project": "SPARK", "issue_id": "SPARK-1041", "title": "ec2-related lines in start-*.sh no longer work ", "status": "Closed", "priority": "Major", "reporter": "Nan Zhu", "assignee": "Nan Zhu", "labels": [], "created": "2014-01-23T08:27:11.000+0000", "updated": "2014-02-22T20:37:41.000+0000", "description": "Due to the change of hostname of EC2 instances, the following lines in the start-* scripts no longer work \n\n# Set SPARK_PUBLIC_DNS so slaves can be linked in master web UI\nif [ \"$SPARK_PUBLIC_DNS\" = \"\" ]; then\n    # If we appear to be running on EC2, use the public address by default:\n    # NOTE: ec2-metadata is installed on Amazon Linux AMI. Check based on that and hostname\n    if command -v ec2-metadata > /dev/null || [[ `hostname` == *ec2.internal ]]; then\n        export SPARK_PUBLIC_DNS=`wget -q -O - http://instance-data.ec2.internal/latest/meta-data/public-hostname`\n    fi\nfi\n\nsince we have spark-ec2, suggest removing these lines\n\n", "comments": ["PR: https://github.com/apache/incubator-spark/pull/588"], "derived": {"summary": "Due to the change of hostname of EC2 instances, the following lines in the start-* scripts no longer work \n\n# Set SPARK_PUBLIC_DNS so slaves can be linked in master web UI\nif [ \"$SPARK_PUBLIC_DNS\" = \"\" ]; then\n    # If we appear to be running on EC2, use the public address by default:\n    # NOTE: ec2-metadata is installed on Amazon Linux AMI. Check based on that and hostname\n    if command -v ec2-metadata > /dev/null || [[ `hostname` == *ec2.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "ec2-related lines in start-*.sh no longer work  - Due to the change of hostname of EC2 instances, the following lines in the start-* scripts no longer work \n\n# Set SPARK_PUBLIC_DNS so slaves can be linked in master web UI\nif [ \"$SPARK_PUBLIC_DNS\" = \"\" ]; then\n    # If we appear to be running on EC2, use the public address by default:\n    # NOTE: ec2-metadata is installed on Amazon Linux AMI. Check based on that and hostname\n    if command -v ec2-metadata > /dev/null || [[ `hostname` == *ec2."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR: https://github.com/apache/incubator-spark/pull/588"}]}}
{"project": "SPARK", "issue_id": "SPARK-1042", "title": "spark cleans all java broadcast variables when it hits the spark.cleaner.ttl ", "status": "Resolved", "priority": "Critical", "reporter": "Tal Sliwowicz", "assignee": "OuyangJin", "labels": ["memory_leak"], "created": "2014-01-23T09:29:17.000+0000", "updated": "2014-10-21T06:53:25.000+0000", "description": "When setting spark.cleaner.ttl, spark performs the cleanup on time - but it cleans all broadcast variables, not just the ones that are older than the ttl. This creates an exception when the next mapPartitions runs because it cannot find the broadcast variable, even when it was created immediately before running the task.\n\nOur temp workaround - not set the ttl and suffer from an ongoing memory leak (forces a restart).\n\nWe are using JavaSparkContext and our broadcast variables are Java HashMaps.", "comments": ["I'll try to solve this . But I can't put myself as Assignee now.", "Thanks. I assigned you the ticket.", "If I'm right, you can disable cleaning broadcast variables by setting the property spark.cleaner.ttl.BROADCAST_VARS=-1, when you set spark.cleaner.ttl. Take a look at MetadataCleaner.scala and BlockManager.scala.", "Just realized the fix I mentioned above is only in 0.9. So this still needs to be fixed in 0.8.0 and 0.8.1.", "Thanks. I'd rather not disable broadcast vars entirely but have some ttl for them too. The issue is that with 0.8.1, once the ttl comes, all the broadcast variables get cleaned, not just the old ones. Was this fixed too in 0.9?", "Sorry for latet comment. @Tal Sliwowicz I wonder which mode are you running spark? local or cluster. for local mode, broadcast variables will only be wriiten in BlockManager as a MEMORY_AND_DISK storage level. Otherwise, they will be written both in BlockManager and local files under control of HttpBroadCast (if you didn't change default broadcast), and both ways have their own clean up function callbacks period done by MetadataCleaner. Could u plz show your detail config for spark mode and BroadCast type, so I can do a further lookup of the src", "We are running spark 0.8.1 in a mesos 0.13 cluster (with zookeeper - so mesos runs in multi master). We did not change any of the cleaner defaults and are using the fault storage level (memory).  Thanks!\n\n", " @Tal Sliwowicz I wonder the value you set for spark.cleaner.ttl (in seconds) and the interval between each time you create broadcast valuse. Becuase I found that the way MetaCleaner works is a little tricky:\n1 there is a delay which equal to value set to ttl \n2 there is a period = max(10, delay / 10), so this period is at least 10 seconds\n3 there is a Timer schedule for clean tasks (for clean up broadcast vars under ttl.  Cleanup tasks first execute  after (period) secnods and then do every (period) seconds. \n(code is timer.schedule(task, periodSeconds * 1000, periodSeconds * 1000)\nWhat I found is that there is no bugs in cleanup function implementation because each time this task check the formula: ( ( currTime - varCreateTime) > ttl )  == true ), if this happens ,all this var entry will be cleaned.\nBut the period may have some blocks. For examples , if period = 10 seconds, and your create 3 broadcast vars just in 10 seconds, and your ttl is 5 seconds. So the first time the clean up task is executed after (period  = 10 ) seconds, all broadcast variables just hit the ttl , and will be cleaned.\nSo maybe your setting for ttl and the resulting period value will affect the clean up result. So check whether you ttl is smaller than the resulting period , and whether it is the reason which cause your issue description. And if you find out that it 's not the reason, I'll just look for other reasons and solutions.Thanks! ", "Thanks for the detailed reply!\n\nWe tried with ttl = 300 . We have a loop were we periodically, each iteration, create new RDDs and run calculations over them. Each loop iteration we also create new broadcast variables. These variables and RDDs are only used inside the specific loop iteration, and never again. When we use the ttl it kept giving exceptions on broadcast variables that were not available to the worker nodes. Is this some thing that does not reproduce for you?", "Hi,@Tal, I'm working on this, it would be wonderful if you give me the detail print stack for what you describle of 'exceptions on broadcast variables that were not available to the worker nodes', at least the class name of the Exception,so I can do a detail debug . Thanks!", "Hi @OuyangJin \nWe tried to reproduce again (on a different driver jar, but it runs the same way as described in the ticket) - this time received the error below. We tried several times, and it is very consistent. Notice - the \"No space left on device\" is bogus. There is 100G+ free space.\nWe are using Spark 0.8.1.\n\njava.io.FileNotFoundException (java.io.FileNotFoundException: /var/lib/spark/data/spark-local-20140218081428-ae01/2e/merged_shuffle_17562_361_0 (No space left on device))\n\njava.io.FileOutputStream.openAppend(Native Method)\njava.io.FileOutputStream.<init>(FileOutputStream.java:192)\norg.apache.spark.storage.DiskBlockObjectWriter.open(BlockObjectWriter.scala:114)\norg.apache.spark.storage.DiskBlockObjectWriter.write(BlockObjectWriter.scala:173)\norg.apache.spark.scheduler.ShuffleMapTask$$anonfun$runTask$1.apply(ShuffleMapTask.scala:162)\norg.apache.spark.scheduler.ShuffleMapTask$$anonfun$runTask$1.apply(ShuffleMapTask.scala:159)\nscala.collection.Iterator$class.foreach(Iterator.scala:772)\nscala.collection.Iterator$$anon$19.foreach(Iterator.scala:399)\norg.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:159)\norg.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:100)\norg.apache.spark.scheduler.Task.run(Task.scala:53)\norg.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:215)\norg.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:50)\norg.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:182)\njava.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)\njava.lang.Thread.run(Thread.java:662)", "We are planning to test this week with 0.9. I will update.", "Hi @Tal, sorry for late reply . What I can see from your exception which was  posted days ago happened during shuffle phase. \nAnd this code segment (exception report in ShuffleMapTask.runTask) is  a logic which write map output to its associate buckets. This is a pre-phase for standard shuffle which in latter phase ResultTask can read this output using socket or netty depending on your config. I wonder your broadcast variable are for shuffle use? You say\"Each loop iteration we also create new broadcast variables\" You use the broadcast to store a result of RDD?(like sc.broadcast(rdd.collect().toMap()) and use it inside of a closure of another RDD? Maybe you can show me the code logic inside of your loop(you can use simplified pseudocode if you think it's not suitable to show your real code relating some copyright)\nThanks!", "You are correct - there is a mismatch. It seems that there was a corruption of the filesystem which caused the error above. It is now fixed. We will try to reproduce again the issue with the broadcast using the 0.9 we now have running (the original exception was specific to broadcast variables, not a general one)", "@Tal. Thanks for your progress. When you mean fixed, you mean you fix a bug in your user code or in your local filesystem? ", "Local Filesystem (reformatted)", "Great. So any progress in your testing for 0.9.0, please let me know so that I can help you track this bug. If this ttl feature really don't work as it declare, I'll fix it with you test support\nThanks", "Tested on 0.9\nThis is an exception stack with ttl=120 (the job takes under 60 seconds and then gets rid of all the broadcast variables. It creates new ones each iteration)\n\n2014-03-05 09:01:59,165 ERROR [Executor task launch worker-17] Executor  - Exception in task ID 225722\njava.io.FileNotFoundException: http://192.168.10.85:54174/broadcast_312\n\tat sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1457)\n\tat org.apache.spark.broadcast.HttpBroadcast$.read(HttpBroadcast.scala:156)\n\tat org.apache.spark.broadcast.HttpBroadcast.readObject(HttpBroadcast.scala:56)\n\tat sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:348)\n\tat scala.collection.immutable.$colon$colon.readObject(List.scala:362)\n\tat sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:348)\n\tat scala.collection.immutable.$colon$colon.readObject(List.scala:362)\n\tat sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:348)\n\tat scala.collection.immutable.$colon$colon.readObject(List.scala:362)\n\tat sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:348)\n\tat scala.collection.immutable.$colon$colon.readObject(List.scala:362)\n\tat sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:348)\n\tat scala.collection.immutable.$colon$colon.readObject(List.scala:362)\n\tat sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:348)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:40)\n\tat org.apache.spark.scheduler.ShuffleMapTask$.deserializeInfo(ShuffleMapTask.scala:69)\n\tat org.apache.spark.scheduler.ShuffleMapTask.readExternal(ShuffleMapTask.scala:138)\n\tat java.io.ObjectInputStream.readExternalData(ObjectInputStream.java:1795)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1754)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:348)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:40)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:62)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:195)\n\tat org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)\n\tat java.lang.Thread.run(Thread.java:662)\n2014-03-05 09:01:59,168 ERROR [Executor task launch worker-11] Executor  - Exception in task ID 225734\njava.io.FileNotFoundException: http://192.168.10.85:54174/broadcast_312\n\tat sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1457)\n\tat org.apache.spark.broadcast.HttpBroadcast$.read(HttpBroadcast.scala:156)\n\tat org.apache.spark.broadcast.HttpBroadcast.readObject(HttpBroadcast.scala:56)\n\tat sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:348)\n\tat scala.collection.immutable.$colon$colon.readObject(List.scala:362)\n\tat sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:348)\n\tat scala.collection.immutable.$colon$colon.readObject(List.scala:362)\n\tat sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:348)\n\tat scala.collection.immutable.$colon$colon.readObject(List.scala:362)\n\tat sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:348)\n\tat scala.collection.immutable.$colon$colon.readObject(List.scala:362)\n\tat sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:348)\n\tat scala.collection.immutable.$colon$colon.readObject(List.scala:362)\n\tat sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:348)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:40)\n\tat org.apache.spark.scheduler.ShuffleMapTask$.deserializeInfo(ShuffleMapTask.scala:69)\n\tat org.apache.spark.scheduler.ShuffleMapTask.readExternal(ShuffleMapTask.scala:138)\n\tat java.io.ObjectInputStream.readExternalData(ObjectInputStream.java:1795)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1754)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:348)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:40)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:62)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:195)\n\tat org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)\n\tat java.lang.Thread.run(Thread.java:662)", "Great, I'll look into it. Any progress will be reported ASAP when I found the reason for it", "@Tal, according to your execption reported, I found out that it happens during task deserialization.So it looks like you put a broadcast variable inside a rdd closure ,and when excutor what to run a task, it has to deserialize a rdd closure together with broadcast variable, and the broadcast varialbe can't be found in the HttpServer\nI'm still not sure the reason exactly causes this problem. I'm sorry for that. There are 3 problems I want to be sure for your user code. 1 You say that you create new broadcast variables each time your enter the loop, and each boradcast variable has a ttl for 120 seconds. Are you sure that your rdd closre just use the newly created broadcast variables, not old ones in previous loop . 2 You say your job runs for 60 seconds and all broadcast varialbe cleaned up. In this 60 seconds, how many iterations of your loop happens? Just 1 round of your loop or serveral rounds. 3 When broadcast variables are cleaned up, there will be logs like \"Deleted broadcast file\" , And broadcast variables will be stored on machines which run your driver program. Have your ever see these log in your console or driver machine\nThanks. Sorry for still not finding the exact reason. But previously community also reported several user cases that program surprisingly clean up their broadcast varialbe which are still in use, due to httpserver down which storing the broadcast varialbes, or time not correctly caculated. So now some people are developing a new clean up way which will replace MetadataCleaner and TTL config, in this  PR:https://github.com/apache/spark/pull/126\nBut I still want to figure out the exact reason because many people are still using 0.9 version. It's best if you can provide some detail information", "1. Yes, I'm sure it creates and uses a news instance\n2. Just 1 round. I can increase the ttl, but it it still happens.\n3. I turned logs to WARN. Can you be more specific about the classes I should increase logging on? I will move them to info or even debug ", "About 3 - the package org.apache.spark.broadcast , right?", "yes is in org.apache.spark.broadcast, \"Deleted broadcast file\" ,this is a info log", "[~qqsun8819] I think the issue was resolved in 0.9.2. We are not experiencing it any more. Thanks!", "I think this was fixed back in 0.9.2"], "derived": {"summary": "When setting spark. cleaner.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "spark cleans all java broadcast variables when it hits the spark.cleaner.ttl  - When setting spark. cleaner."}, {"q": "What updates or decisions were made in the discussion?", "a": "I think this was fixed back in 0.9.2"}]}}
{"project": "SPARK", "issue_id": "SPARK-1043", "title": "Pyspark RDD's cannot deal with strings greater than 64K bytes.", "status": "Resolved", "priority": "Major", "reporter": "Erik Selin", "assignee": "Josh Rosen", "labels": [], "created": "2014-01-24T13:18:33.000+0000", "updated": "2014-01-28T21:32:55.000+0000", "description": "Pyspark uses java.io.DataOutputStream.writeUTF to send data to the python world which causes a problem since java.io.DataOutputStream.writeUTF fails if you pass it strings above 64K bytes. Furthermore a fix to this issue is not straight forward since the java to python protocol actually relies on this and uses it as the separator between items. The offending write happens in \n\n   core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala:220\n\nand the reliance on this to separate items can be found in the  MUTF8Deserializer class in\n\n   python/pyspark/serializers.py:264\n\nThe only solution I currently have in mind would be to change the protocol to either extend the number of bytes used to specify the length of the item or to add a boolean flag to every \"packet\" to indicate wether the item is split into multiple parts (although the second option might result in bad data if multiple things are writing to these steams)\n\n", "comments": ["Thanks for catching this.  I'll look into alternative encodings that support longer strings and add a test case for large strings to make sure this doesn't break again in the future.", "I've submitted a quick fix to this https://github.com/apache/incubator-spark/pull/512 not sure if you guys usually discuss things here or on github so I'm linking this to the pr and the pr to this :)", "Note that the current encoding will also break if any of the modified characters described here: http://docs.oracle.com/javase/7/docs/api/java/io/DataInput.html#modified-utf-8 are used. The python side doesn't implement anything to mitigate this, it's probably safest to switch to vanilla UTF-8", "There's no reason that PySpark has to use MUTF-8; this was a bad choice that I made when implementing custom serializer support (https://github.com/apache/incubator-spark/pull/146), so I'd switch to vanilla UTF-8 instead.", "Fixed in https://github.com/apache/incubator-spark/pull/523"], "derived": {"summary": "Pyspark uses java. io.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Pyspark RDD's cannot deal with strings greater than 64K bytes. - Pyspark uses java. io."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed in https://github.com/apache/incubator-spark/pull/523"}]}}
{"project": "SPARK", "issue_id": "SPARK-1044", "title": "Default spark logs location in EC2 AMI leads to out-of-disk space pretty soon", "status": "Resolved", "priority": "Minor", "reporter": "Tathagata Das", "assignee": null, "labels": [], "created": "2014-01-24T13:29:23.000+0000", "updated": "2016-01-12T13:45:13.000+0000", "description": "The default log location is SPARK_HOME/work/ and this leads to disk space running out pretty quickly. The spark-ec2 scripts should configure the cluster to automatically set the logging directory to /mnt/spark-work/ or something like that on the mounted disks. The SPARK_HOME/work may also be symlinked to that directory to maintain the existing setup.", "comments": ["Filling up the work dir could be alleviated by fixing https://issues.apache.org/jira/browse/SPARK-1860 so we could enable worker dir cleanup automatically.  If we had automatic worker dir cleanup, would you still want to move the work directory to somewhere else?", "I think it is still a good idea even if the automatic cleanup is implemented. One large job or many small jobs can fill many gigabytes before the cleanup can kick in.", "I'm assuming lots of smaller EC2 issues are obsolete as it's moving out of Spark"], "derived": {"summary": "The default log location is SPARK_HOME/work/ and this leads to disk space running out pretty quickly. The spark-ec2 scripts should configure the cluster to automatically set the logging directory to /mnt/spark-work/ or something like that on the mounted disks.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Default spark logs location in EC2 AMI leads to out-of-disk space pretty soon - The default log location is SPARK_HOME/work/ and this leads to disk space running out pretty quickly. The spark-ec2 scripts should configure the cluster to automatically set the logging directory to /mnt/spark-work/ or something like that on the mounted disks."}, {"q": "What updates or decisions were made in the discussion?", "a": "I'm assuming lots of smaller EC2 issues are obsolete as it's moving out of Spark"}]}}
{"project": "SPARK", "issue_id": "SPARK-1045", "title": "ExternalAppendOnlyMap Iterator throw no such element on joining two large rdd", "status": "Resolved", "priority": "Major", "reporter": "Jiacheng Guo", "assignee": null, "labels": [], "created": "2014-01-26T22:28:49.000+0000", "updated": "2020-05-17T18:30:42.000+0000", "description": "On latest master branch 05be7047744c88e64e7e6bd973f9bcfacd00da5f,  I keep getting no such element from a single shuffle task when performance join on two large rdd (memory spill 10G, disk spill 800m for a single task)\n\nthe code is here:\nhttps://gist.github.com/guojc/8643741\n\nthe exception is\n\njava.util.NoSuchElementException (java.util.NoSuchElementException)\norg.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:304)\norg.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:239)\norg.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:29)\nscala.collection.Iterator$$anon$11.next(Iterator.scala:328)\nscala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\nscala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\norg.apache.spark.rdd.PairRDDFunctions.org$apache$spark$rdd$PairRDDFunctions$$writeToFile$1(PairRDDFunctions.scala:724)\norg.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$2.apply(PairRDDFunctions.scala:734)\norg.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$2.apply(PairRDDFunctions.scala:734)\norg.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)\norg.apache.spark.scheduler.Task.run(Task.scala:53)\norg.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)\norg.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)\norg.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)\njava.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)\njava.lang.Thread.run(Thread.java:662)\n\n\n\n\n", "comments": ["  I was able to finally identify the bug as StreamBuffer.compareTo method's ill defined behavior when key's hashCode equals to Int.MaxValue. Though this only occur in aboue 1/2^32 chance, it can happen a lot when your key size approach 2^32. I have create a pull request for the bug fix https://github.com/apache/incubator-spark/pull/612"], "derived": {"summary": "On latest master branch 05be7047744c88e64e7e6bd973f9bcfacd00da5f,  I keep getting no such element from a single shuffle task when performance join on two large rdd (memory spill 10G, disk spill 800m for a single task)\n\nthe code is here:\nhttps://gist. github.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "ExternalAppendOnlyMap Iterator throw no such element on joining two large rdd - On latest master branch 05be7047744c88e64e7e6bd973f9bcfacd00da5f,  I keep getting no such element from a single shuffle task when performance join on two large rdd (memory spill 10G, disk spill 800m for a single task)\n\nthe code is here:\nhttps://gist. github."}, {"q": "What updates or decisions were made in the discussion?", "a": "I was able to finally identify the bug as StreamBuffer.compareTo method's ill defined behavior when key's hashCode equals to Int.MaxValue. Though this only occur in aboue 1/2^32 chance, it can happen a lot when your key size approach 2^32. I have create a pull request for the bug fix https://github.com/apache/incubator-spark/pull/612"}]}}
{"project": "SPARK", "issue_id": "SPARK-1046", "title": "Enable to build behind a proxy.", "status": "Resolved", "priority": "Minor", "reporter": "Kousuke Saruta", "assignee": null, "labels": [], "created": "2014-01-26T22:37:43.000+0000", "updated": "2014-09-27T19:07:03.000+0000", "description": "I tried to build spark-0.8.1 behind proxy and failed although I set http/https.proxyHost, proxyPort, proxyUser, proxyPassword.\nI found it's caused by accessing  github using git protocol (git://).\nThe URL is hard-corded in SparkPluginBuild.scala as follows.\n\n{code}\nlazy val junitXmlListener = uri(\"git://github.com/ijuma/junit_xml_listener.git#fe434773255b451a38e8d889536ebc260f4225ce\")\n{code}\n\nAfter I rewrite the URL as follows, I could build successfully.\n\n{code}\n\nlazy val junitXmlListener = uri(\"https://github.com/ijuma/junit_xml_listener.git#fe434773255b451a38e8d889536ebc260f4225ce\")\n{code}\n\nI think we should be able to build whether we are behind a proxy or not.", "comments": ["Thanks for reporting this. Do you mind submitting a pull request for this?", "Sure! I'll submit a pull request later.", "Is this stale / resolved? I don't see this in the code at this point."], "derived": {"summary": "I tried to build spark-0. 8.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Enable to build behind a proxy. - I tried to build spark-0. 8."}, {"q": "What updates or decisions were made in the discussion?", "a": "Is this stale / resolved? I don't see this in the code at this point."}]}}
{"project": "SPARK", "issue_id": "SPARK-1047", "title": "Ability to disable the spark ui server (unit tests)", "status": "Closed", "priority": "Minor", "reporter": "Heiko Braun", "assignee": "Andrew Or", "labels": [], "created": "2014-01-27T07:12:12.000+0000", "updated": "2015-01-08T23:10:12.000+0000", "description": "Provide the ability to disable the internal jetty server that serves the web ui. It's not needed when running unit tests and often conflicts may other ports on the system.\n", "comments": ["The PR is here: https://github.com/apache/incubator-spark/pull/518/", "Here's the more updated PR: https://github.com/apache/spark/pull/2363"], "derived": {"summary": "Provide the ability to disable the internal jetty server that serves the web ui. It's not needed when running unit tests and often conflicts may other ports on the system.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Ability to disable the spark ui server (unit tests) - Provide the ability to disable the internal jetty server that serves the web ui. It's not needed when running unit tests and often conflicts may other ports on the system."}, {"q": "What updates or decisions were made in the discussion?", "a": "Here's the more updated PR: https://github.com/apache/spark/pull/2363"}]}}
{"project": "SPARK", "issue_id": "SPARK-1048", "title": "Create spark-site.xml or spark-site.yaml for configuration", "status": "Resolved", "priority": "Critical", "reporter": "Shengzhe Yao", "assignee": null, "labels": [], "created": "2014-01-27T16:49:57.000+0000", "updated": "2014-09-21T09:25:52.000+0000", "description": "Currently, spark doesn't provide the ability to pass configuration as a file, people have to explicitly specify them on the command line. This becomes a bigger issue when deploying spark on a cluster and people want to specify parameters other than default value. \n\nAs of now, spark is being a top apache project and we need to pay more attention to the configuration. Most apache projects provide a xml configuration file (like hdfs, hadoop, hbase, etc.) and probably we want to do the same thing for spark. The advantages are obvious, it helps developers to specify their own spark configuration for the cluster and add/remove configuration parameters will be much easier via file than via system property. ", "comments": ["I can work on it if community like this idea and there is no work have been done. ", "Take a look at http://apache-spark-developers-list.1001551.n3.nabble.com/Moving-to-Typesafe-Config-td381.html", "As you might see on the previous discussion on this, we actually had some issues with config files. The main one is how to pass the file to the application -- should it just be on the classpath for example? A lot of Spark's properties are very specific to each application (e.g. spark.serializer) and do not make sense to configure globally. But if you suggest a design that 1) does not break the existing forms of configuration and 2) is easy to understand regarding what's application-specific and global, we can add it in.", "The application can always slurp in a config file itself and pass it in programmatically.  It would be super nice to have a  fromFile method in SparkConf however.", "As Evan mentioned, I would prefer to \n 1) Add an environment variable SPARK_CONFIG_FILE to point the configuration file path.\n 2) In SparkConf class, add logic to read SPARK_CONFIG_FILE variable and parse the configuration file in the beginning of the if statement.\nSparkConf.scala\n Line 50    if (loadDefaults) {\n                        // Add logic to read SPARK_CONFIG_FILE variable and parse the configuration file \n   \n                       // Load any spark.* system properties\n                      for ((k, v) <- System.getProperties.asScala if k.startsWith(\"spark.\")) {\n                           settings(k) = v\n                      }\n                 }\n\nIn this way, spark will first read the given configuration file and then apply the system property; therefore, properties specified in command line always have the highest priority.\n\nNote: this design doesn't solve the problem Matei mentioned: is easy to understand regarding what's application-specific and global.\nBut again, I think the point is to free programmer's hands by providing a way to pass configurations via a config file. For application specific ones, developer could still specify them in command line or provide their own mechanisms to pass into program. \n\nBTW: I use following command to find all configurations we use in spark, it seems we have 129 configurations and it would be better to allow people pass them via a configuration file (if they want to use a different value other than default).\n\nfind . -name \"\\*.scala\" | xargs -I{} grep \"get.\\*(\\\"&#92;\\|set.*(\\\"\" {} | perl -pe 's|^.\\*?(get&#92;|set).\\*?&#92;(\\\"(.\\*?)\\\".\\*$|\\2|' | grep \"^spark\" | sort | uniq", "The original discussion about using the type safe configs was to pick up configs like: java system proprties -> application specific file -> cluster config file (spark.conf).   Along the way it seems it changed to just java system properties -> cluster file (spark.conf).  Was there a reason it dropped the application file? ", "Any update on this issue?\nIs someone trying to address?", "[~sarutak] I am supposed to do the job but didn't figure out all use cases. This could be as simple as add a fromFile method in SparkConf as [~velvia] mentioned, but config file format should be addressed cleanly (like Hadoop/HBase's xml config file or something else ?). I might to add a xml based config file in next few weeks, but please feel free if you have ideas and want to do it now. I am happy to see people like this idea: config file for Spark. ", "Patrick submit a patch which introduces spark-defaults.conf to set system properties. This seems to achieve our original purpose: allow user to set configuration values in a well-known config file and Spark will automatically pick it up.\n\ncommit fb98488fc8e68cc84f6e0750fd4e9e29029879d2\nAuthor: Patrick Wendell <pwendell@gmail.com>\nDate:   Mon Apr 21 10:26:33 2014 -0700\n\n    Clean up and simplify Spark configuration\n\n    Over time as we've added more deployment modes, this have gotten a bit unwieldy with user-facing configuration options in Spark. Going forward we'll advise all users to run `spark-submit` to launch applications. This is a WIP patch but it makes the following improvements:\n\n    1. Improved `spark-env.sh.template` which was missing a lot of things users now set in that file.\n    2. Removes the shipping of SPARK_CLASSPATH, SPARK_JAVA_OPTS, and SPARK_LIBRARY_PATH to the executors on the cluster. This was an ugly hack. Instead it introduces config variables spark.executor.extraJavaOpts, spark.executor.extraLibraryPath, and spark.executor.extraClassPath.\n    3. Adds ability to set these same variables for the driver using `spark-submit`.\n    4. Allows you to load system properties from a `spark-defaults.conf` file when running `spark-submit`. This will allow setting both SparkConf options and other system properties utilized by `spark-submit`.\n    5. Made `SPARK_LOCAL_IP` an environment variable rather than a SparkConf property. This is more consistent with it being set on each node.\n\n    Author: Patrick Wendell <pwendell@gmail.com>"], "derived": {"summary": "Currently, spark doesn't provide the ability to pass configuration as a file, people have to explicitly specify them on the command line. This becomes a bigger issue when deploying spark on a cluster and people want to specify parameters other than default value.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Create spark-site.xml or spark-site.yaml for configuration - Currently, spark doesn't provide the ability to pass configuration as a file, people have to explicitly specify them on the command line. This becomes a bigger issue when deploying spark on a cluster and people want to specify parameters other than default value."}, {"q": "What updates or decisions were made in the discussion?", "a": "Patrick submit a patch which introduces spark-defaults.conf to set system properties. This seems to achieve our original purpose: allow user to set configuration values in a well-known config file and Spark will automatically pick it up.\n\ncommit fb98488fc8e68cc84f6e0750fd4e9e29029879d2\nAuthor: Patrick Wendell <pwendell@gmail.com>\nDate:   Mon Apr 21 10:26:33 2014 -0700\n\n    Clean up and simplify Spark configuration\n\n    Over time as we've added more deployment modes, this have gotten a bit unwieldy with user-facing configuration options in Spark. Going forward we'll advise all users to run `spark-submit` to launch applications. This is a WIP patch but it makes the following improvements:\n\n    1. Improved `spark-env.sh.template` which was missing a lot of things users now set in that file.\n    2. Removes the shipping of SPARK_CLASSPATH, SPARK_JAVA_OPTS, and SPARK_LIBRARY_PATH to the executors on the cluster. This was an ugly hack. Instead it introduces config variables spark.executor.extraJavaOpts, spark.executor.extraLibraryPath, and spark.executor.extraClassPath.\n    3. Adds ability to set these same variables for the driver using `spark-submit`.\n    4. Allows you to load system properties from a `spark-defaults.conf` file when running `spark-submit`. This will allow setting both SparkConf options and other system properties utilized by `spark-submit`.\n    5. Made `SPARK_LOCAL_IP` an environment variable rather than a SparkConf property. This is more consistent with it being set on each node.\n\n    Author: Patrick Wendell <pwendell@gmail.com>"}]}}
{"project": "SPARK", "issue_id": "SPARK-1049", "title": "spark on yarn - yarn-client mode doesn't always exit properly", "status": "Resolved", "priority": "Major", "reporter": "Thomas Graves", "assignee": "Thomas Graves", "labels": [], "created": "2014-01-30T08:27:04.000+0000", "updated": "2014-04-08T20:40:08.000+0000", "description": "When running in spark on yarn in yarn-client mode, the application master doesn't always exit in a timely manner if its still waiting on containers from the yarn resource manager.  \n\n", "comments": ["https://github.com/apache/incubator-spark/pull/526", "[~tgraves] Wasnt this resolved, in master as well as branch 0.9?\n\nThis commit https://github.com/apache/spark/commit/b044b0b\n\n If so, shall we mark this resolved?", "Yes it was, go ahead and resolve it."], "derived": {"summary": "When running in spark on yarn in yarn-client mode, the application master doesn't always exit in a timely manner if its still waiting on containers from the yarn resource manager.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "spark on yarn - yarn-client mode doesn't always exit properly - When running in spark on yarn in yarn-client mode, the application master doesn't always exit in a timely manner if its still waiting on containers from the yarn resource manager."}, {"q": "What updates or decisions were made in the discussion?", "a": "Yes it was, go ahead and resolve it."}]}}
{"project": "SPARK", "issue_id": "SPARK-1050", "title": "Investigate AnyRefMap", "status": "Closed", "priority": "Minor", "reporter": "Mark Hamstra", "assignee": null, "labels": [], "created": "2014-01-30T10:23:46.000+0000", "updated": "2015-02-23T03:07:11.000+0000", "description": "Once Spark is built using Scala 2.11, we should investigate (particularly on heavily-used, performance-critical code paths) replacing usage of HashMap with the new, higher-performance AnyRefMap.", "comments": ["I suspect the openhashset and hashmap we use in GraphX would be faster than this. They are both open addressing hash maps, and mine actually does specialization. ", "Perhaps so, but it is worth reaching a conclusion more solid than a guess.  Also, there are a lot of instances of HashMap in Spark that haven't been (and probably can't be) replaced with your newer alternatives.\n\nAnyway, there's nothing much to be done on this issue until we're using Scala 2.11, so this is mostly just a placeholder and a reminder for down the road. ", "Since Spark can now be built with Scala 2.11, I believe this issue can now be looked into. Tagging this with a 1.4.0 target so it can be reviewed by then.", "I'm going to close this for now. If somebody has time to review the new Scala hash map, we can reopen it. In general the performance characteristics of the Scala collection library is not very satisfying.\n\nI have some thoughts on performance optimizations in general and I will post something soon."], "derived": {"summary": "Once Spark is built using Scala 2. 11, we should investigate (particularly on heavily-used, performance-critical code paths) replacing usage of HashMap with the new, higher-performance AnyRefMap.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "Investigate AnyRefMap - Once Spark is built using Scala 2. 11, we should investigate (particularly on heavily-used, performance-critical code paths) replacing usage of HashMap with the new, higher-performance AnyRefMap."}, {"q": "What updates or decisions were made in the discussion?", "a": "I'm going to close this for now. If somebody has time to review the new Scala hash map, we can reopen it. In general the performance characteristics of the Scala collection library is not very satisfying.\n\nI have some thoughts on performance optimizations in general and I will post something soon."}]}}
{"project": "SPARK", "issue_id": "SPARK-1051", "title": "On Yarn, executors don't doAs as submitting user", "status": "Resolved", "priority": "Major", "reporter": "Sanford Ryza", "assignee": "Sandy Ryza", "labels": [], "created": "2014-02-03T13:33:38.000+0000", "updated": "2020-04-24T06:36:20.000+0000", "description": "This means that they can't write/read from files that the yarn user doesn't have permissions to but the submitting user does.  I don't think this isn't a problem when running with Hadoop security, because the executor processes will be run as the submitting user.", "comments": ["I'm not yet a Spark contributor, primarily a Spark user on Yarn.  This sounds very much like an issue we've had with Spark 0.8.1 on Yarn 2.2.0.   It is described on the Spark user mailing list in a posting 2/1/2014 entitled \"File write ownership conflicts for Spark 0.8.1 in YARN modes.\"  I can post more details here if desired.  We've replicated the problem in several environments:\n\n- CDH5.0.0-beta-1 or plain vanilla Apache Hadoop 2.2.0\n- A small CDH5-beta-1 cluster or a single-CPU Yarn/HDFS pseudocluster\n- In yarn-standalone or yarn-client mode, although there are subtle differences in ownership of directories, temporary files written to the output directories, and (when not halted by permission errors) final output result files.\n\nIn our tests, any file or directory created in RDD.saveAsTextFile(path:String) or FSDataOutputStream by a YARN process (the driver in yarn-standalone, or the worker nodes in either yarn mode) is owned by the UID of the user reported by System.properties(\"user.name\") and any other file or directory -- staging files or directories apparently created by the driver in yarn-client mode as destination containers for saveAsTextFile temporary and part* files are owned by the UID corresponding to \"appUser\" in logs reported by ...spark.deploy.yarn.Client using Yarn's ...hadoop.yarn.api.records.ApplicationReport.  \n\nDepending on UMASK and other settings, this can result in a ...hadoop.security.AccessControlException because of permission denied.  I've found simple workarounds for yarn-standalone mode because the same user creates both files and destination directories, but there are rarely good workarounds (that don't violate good security practices) for yarn-client, because files and destination directories are usually owned by different users.  This is particularly true for the typical CDH5 installation.", "https://github.com/apache/incubator-spark/pull/538", "This solves the issue for yarn-client, but not for yarn-standalone.\n\nIn all the following cases, Spark is started by UID \"kmarkey\".  This is reflected by the appUser as reported in the log.\n\nIf starting a job in yarn-standalone as any user, the Master (driver) process is owned by \"yarn\", container directories are owned by \"yarn\" and Spark worker process write to files with the SPARK_USER or appUser UID.  An ownership exception occurs if this is any user other than the \"yarn\" UID.  Ownership of destination directories is immaterial.  \n\nyarn-standalone\n    JIRA-1051 patch\n    appUser UID: kmarkey\n    yarn UID: hduser\n    destination: /user/hduser/output/Spark-Jira1051\n        drwxr-xr-x   - hduser  supergroup          0 2014-02-09 16:39 /user/hduser/output/Spark-Jira1051\n        drwxr-xr-x   - hduser  supergroup          0 2014-02-09 16:39 /user/hduser/output/Spark-Jira1051/values\n        drwxr-xr-x   - hduser  supergroup          0 2014-02-09 16:39 /user/hduser/output/Spark-Jira1051/values/column-0000\n        drwxr-xr-x   - hduser  supergroup          0 2014-02-09 16:39 /user/hduser/output/Spark-Jira1051/values/column-0000/_temporary\n        drwxr-xr-x   - hduser  supergroup          0 2014-02-09 16:39 /user/hduser/output/Spark-Jira1051/values/column-0000/_temporary/0\n     ... exception stops process when kmarkey tries to write to this directory ...\n\n    destination: /user/kmarkey/output/Spark-Jira1051\n        drwxr-xr-x   - hduser  supergroup          0 2014-02-09 16:45 /user/kmarkey/output/Spark-Jira1051\n        drwxr-xr-x   - hduser  supergroup          0 2014-02-09 16:45 /user/kmarkey/output/Spark-Jira1051/values\n        drwxr-xr-x   - hduser  supergroup          0 2014-02-09 16:45 /user/kmarkey/output/Spark-Jira1051/values/column-0000\n        drwxr-xr-x   - hduser  supergroup          0 2014-02-09 16:45 /user/kmarkey/output/Spark-Jira1051/values/column-0000/_temporary\n        drwxr-xr-x   - hduser  supergroup          0 2014-02-09 16:45 /user/kmarkey/output/Spark-Jira1051/values/column-0000/_temporary/0\n     ... exception stops process when kmarkey tries to write to this directory ...\n\nIn yarn-client mode, things are as they should be. SPARK_USER, appUser, and the worker process UIDs now match.  \n\nyarn-client\n    JIRA-1051 patch\n    appUser UID: kmarkey\n    yarn UID: hduser\n    destination: /user/hduser/output/Spark-Jira1051-Client\n        drwxr-xr-x   - kmarkey supergroup          0 2014-02-09 16:59 /user/kmarkey/output/Spark-Jira1051-Client\n        drwxr-xr-x   - kmarkey supergroup          0 2014-02-09 16:59 /user/kmarkey/output/Spark-Jira1051-Client/summary\n        -rw-r--r--   1 kmarkey supergroup      41220 2014-02-09 16:59 /user/kmarkey/output/Spark-Jira1051-Client/summary/summary.json\n        -rw-r--r--   1 kmarkey supergroup      25528 2014-02-09 16:59 /user/kmarkey/output/Spark-Jira1051-Client/summary/summary.txt\n        drwxr-xr-x   - kmarkey supergroup          0 2014-02-09 16:59 /user/kmarkey/output/Spark-Jira1051-Client/types\n        drwxr-xr-x   - kmarkey supergroup          0 2014-02-09 16:56 /user/kmarkey/output/Spark-Jira1051-Client/types/column-0000\n        -rw-r--r--   1 kmarkey supergroup          0 2014-02-09 16:56 /user/kmarkey/output/Spark-Jira1051-Client/types/column-0000/_SUCCESS\n        -rw-r--r--   1 kmarkey supergroup          0 2014-02-09 16:56 /user/kmarkey/output/Spark-Jira1051-Client/types/column-0000/part-00000\n        -rw-r--r--   1 kmarkey supergroup         15 2014-02-09 16:56 /user/kmarkey/output/Spark-Jira1051-Client/types/column-0000/part-00001\n\nSo, it seems that in yarn-standalone, only the \"yarn\" user can start or own the Master process, and this proposed fix should not apply.  Or the fix should be applied to the master (driver) process.  But in retrospect, that seems impossible to do, because it is a child of the ResourceMaster.\n\nTherefore, this fix should only apply to yarn-client mode.", "Updated the pull request to fix this issue", "Thank you.  That is an improvement.  Both yarn-client and yarn-standalone work similarly -- at least the portions controlled by Spark.  The only remaining files with the wrong ownership are written by Java using FSDataOutputStream, and I need to modify how I initialize FileSystem to fix those.  That is not your responsibility.\n\n{noformat}\nyarn-client\n    JIRA-1051 patch #2\n    appUser UID: kmarkey\n    yarn UID: hduser\n    destination: /user/hduser/output/Spark-Jira1051-Client\n        drwxr-xr-x   - kmarkey supergroup          0 2014-02-10 22:08 /user/kmarkey/output/Spark-Jira1051-Client2\n        drwxr-xr-x   - kmarkey supergroup          0 2014-02-10 22:08 /user/kmarkey/output/Spark-Jira1051-Client2/summary\n        -rw-r--r--   1 kmarkey supergroup      41220 2014-02-10 22:08 /user/kmarkey/output/Spark-Jira1051-Client2/summary/summary.json\n        -rw-r--r--   1 kmarkey supergroup      25528 2014-02-10 22:08 /user/kmarkey/output/Spark-Jira1051-Client2/summary/summary.txt\n        drwxr-xr-x   - kmarkey supergroup          0 2014-02-10 22:08 /user/kmarkey/output/Spark-Jira1051-Client2/types\n        drwxr-xr-x   - kmarkey supergroup          0 2014-02-10 22:05 /user/kmarkey/output/Spark-Jira1051-Client2/types/column-0000\n        -rw-r--r--   1 kmarkey supergroup          0 2014-02-10 22:05 /user/kmarkey/output/Spark-Jira1051-Client2/types/column-0000/_SUCCESS\n        -rw-r--r--   1 kmarkey supergroup          0 2014-02-10 22:05 /user/kmarkey/output/Spark-Jira1051-Client2/types/column-0000/part-00000\n        -rw-r--r--   1 kmarkey supergroup         15 2014-02-10 22:05 /user/kmarkey/output/Spark-Jira1051-Client2/types/column-0000/part-00001\n\nyarn-standalone\n    JIRA-1051 patch #2\n    appUser UID: kmarkey\n    yarn UID: hduser\n    destination: /user/hduser/output/Spark-Jira1051-YarnStandAlone2\n    * = files written using FSDataOutputStream in Java code unrelated to Spark.  All other files created by Spark.\n            (I will need to handle the FileSystem initializer differently.)\n        drwxr-xr-x   - kmarkey supergroup          0 2014-02-10 22:33 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone2\n    *   drwxr-xr-x   - hduser  supergroup          0 2014-02-10 22:33 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone2/summary\n    *   -rw-r--r--   1 hduser  supergroup      41220 2014-02-10 22:33 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone2/summary/summary.json\n    *   -rw-r--r--   1 hduser  supergroup      25528 2014-02-10 22:33 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone2/summary/summary.txt\n        drwxr-xr-x   - kmarkey supergroup          0 2014-02-10 22:33 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone2/types\n        drwxr-xr-x   - kmarkey supergroup          0 2014-02-10 22:31 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone2/types/column-0000\n        -rw-r--r--   1 kmarkey supergroup          0 2014-02-10 22:31 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone2/types/column-0000/_SUCCESS\n        -rw-r--r--   1 kmarkey supergroup          0 2014-02-10 22:31 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone2/types/column-0000/part-00000\n        -rw-r--r--   1 kmarkey supergroup         15 2014-02-10 22:31 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone2/types/column-0000/part-00001\n{noformat}\n", "I was able to fix the remaining ownership issues.  In the \"summary.*\" files above and below, I was using a small Java utility I authored to write a String or List of Strings to an HDFS file.  When instantiating the FileSystem object, I was using get(URI,configuration,user), naively passing the value of System.getProperty(\"user.name\") as user.  But this property is unaffected by the runAsUser(sparkuser) code added by the patch.  Using instead the get(URI,configuration) factory method, the default authority for the specified URI is determined from the correct context, and the file is saved with sparkuser (appUser) ownership instead of yarn user ownership.  The runAsUser(sparkuser) has the desired effect here, too, not just in the saveAsTextFile() instances on the worker nodes.  \n\nHooray!  Tested on pseudocluster, now running on full cluster.\n\n{noformat}\nyarn-standalone\n    JIRA-1051 patch #2 with fix in CuHdfsUtil\n    appUser UID: kmarkey\n    yarn UID: hduser\n    destination: /user/hduser/output/Spark-Jira1051-YarnStandAlone3\n        drwxr-xr-x   - kmarkey supergroup          0 2014-02-11 14:35 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone3\n        drwxr-xr-x   - kmarkey supergroup          0 2014-02-11 14:36 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone3/summary\n        -rw-r--r--   1 kmarkey supergroup      41220 2014-02-11 14:36 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone3/summary/summary.json\n        -rw-r--r--   1 kmarkey supergroup      25528 2014-02-11 14:35 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone3/summary/summary.txt\n        drwxr-xr-x   - kmarkey supergroup          0 2014-02-11 14:35 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone3/types\n        drwxr-xr-x   - kmarkey supergroup          0 2014-02-11 14:31 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone3/types/column-0000\n        -rw-r--r--   1 kmarkey supergroup          0 2014-02-11 14:31 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone3/types/column-0000/_SUCCESS\n        -rw-r--r--   1 kmarkey supergroup          0 2014-02-11 14:31 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone3/types/column-0000/part-00000\n        -rw-r--r--   1 kmarkey supergroup         15 2014-02-11 14:31 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone3/types/column-0000/part-00001\n{noformat}", "When testing on real cluster, running *yarn-standalone* with user *klmarkey*, two cases observed:\n\nWhen explicitly defining \"export SPARK_USER=klmarkey\", files all saved with \"klmarkey\" as owner.\nWhen *NOT* explicitly defining SPARK_USER, files all saved with \"*yarn*\" as owner.\n\n", "I'm not sure why the 2nd patch works for yarn-standalone in a pseudocluster but not on the real cluster.  The pseudocluster is plain vanilla Apache Hadoop 2.2.0.  The real cluster is CDH5 Hadoop 2.2.0 Beta-1.  I checked the cluster configuration, and there's nothing that stands out.  \n\nI also don't quite understand why \"appUser\" is consistently reported by ApplicationReport as \"kmarkey\" whether SPARK_USER is defined or not.  \n\nIs this a documentation issue?  Or should the *spark-class* shell initialize it?", "Very strange.  I'll look into this.", "Indeed, I was surprised, too.  At first I thought I had run in a shell I sudoed into as \"yarn\" to clean up some of the debris left behind from previous tests!  But I hadn't.  I repeated each of the results twice.  \n\nThe good news is that the user -- whichever it was -- is now *consistent* over all files, directories, and temporary files written to the output destination.  Before patch 2, jobs were failing because files written by worker node processes were conflicting with those written by the appMaster processes.  ", "User error!  After reconfiguring my cluster with up-to-date runtime scripts and other configuration data, the patched code worked as advertised, even *without* explicitly defining SPARK_USER.\n\nThanks, Sandy, for your work on this issue.", "pr https://github.com/apache/spark/pull/29 committed.", "sorry pr needs upmerge first.", "I committed this to branch-0.9 also. commit 748f002b3a58af118f7de0b6ea9170895b571c78", "User 'sryza' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/29"], "derived": {"summary": "This means that they can't write/read from files that the yarn user doesn't have permissions to but the submitting user does. I don't think this isn't a problem when running with Hadoop security, because the executor processes will be run as the submitting user.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "On Yarn, executors don't doAs as submitting user - This means that they can't write/read from files that the yarn user doesn't have permissions to but the submitting user does. I don't think this isn't a problem when running with Hadoop security, because the executor processes will be run as the submitting user."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'sryza' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/29"}]}}
{"project": "SPARK", "issue_id": "SPARK-1052", "title": "Spark on Mesos with CDH4.5.0 cannot start the Tasks properly", "status": "Resolved", "priority": "Major", "reporter": "Alberto Miorin", "assignee": "Bijay Singh Bisht", "labels": [], "created": "2014-02-04T13:48:02.000+0000", "updated": "2014-02-16T16:55:31.000+0000", "description": "Steps :\nStart the mesos cluster with the deb 0.15 from mesosphere.\nInstall cdh 4.5.0.\nStart the spark-shell with this code :\n\nval data = 1 to 10000\nval distData = sc.parallelize(data)\ndistData.filter(_< 10).collect()\n\nThe tasks die because of an exception.\n\nIt used to work with Spark 0.8.0", "comments": ["I have exactly the same problem using Hadoop 1.2.1, Spark 0.9 and Mesos 0.15.", "I'm able to repro too. I suspect it's networking binding issue.", "Fix is available in the pull request #568\n\nBug caused by using the new Conf object, for setting spark.dirver.host etc. But the user of those properties still using the System.properties to access it. ", "PR #568 merged."], "derived": {"summary": "Steps :\nStart the mesos cluster with the deb 0. 15 from mesosphere.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Spark on Mesos with CDH4.5.0 cannot start the Tasks properly - Steps :\nStart the mesos cluster with the deb 0. 15 from mesosphere."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR #568 merged."}]}}
{"project": "SPARK", "issue_id": "SPARK-1053", "title": "Should not require SPARK_YARN_APP_JAR when running on YARN", "status": "Resolved", "priority": "Major", "reporter": "Sanford Ryza", "assignee": "Sandy Ryza", "labels": [], "created": "2014-02-04T23:22:31.000+0000", "updated": "2014-04-04T20:49:15.000+0000", "description": "{code}\norg.apache.spark.SparkException: env SPARK_YARN_APP_JAR is not set\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:49)\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:126)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:200)\n\tat org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:954)\n{code}", "comments": ["This also causes problems for PySpark on YARN (SPARK-1030).  This isn't the only issue blocking Pyspark on YARN; see SPARK-1004 for more discussion.", "Hadn't seen that.  Are you already working on this?  If not, I was planning to give it a shot.", "Go ahead; I'm not working on the SPARK_YARN_APP_JAR issue (I have a work in progress branch for fixing PySpark on YARN, but it's a more extensive set of changes that are unrelated to this issue).", "https://github.com/apache/incubator-spark/pull/553"], "derived": {"summary": "{code}\norg. apache.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Should not require SPARK_YARN_APP_JAR when running on YARN - {code}\norg. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/incubator-spark/pull/553"}]}}
{"project": "SPARK", "issue_id": "SPARK-1054", "title": "Get Cassandra support in Spark Core/Spark Cassandra Module", "status": "Resolved", "priority": "Major", "reporter": "Rohit Rai", "assignee": null, "labels": ["calliope", "cassandra"], "created": "2014-02-05T00:31:02.000+0000", "updated": "2015-03-01T11:53:23.000+0000", "description": "Calliope is a library providing an interface to consume data from Cassandra to spark and store RDDs from Spark to Cassandra. \n\nBuilding as wrapper over Cassandra's Hadoop I/O it provides a simplified and very generic API to consume and produces data from and to Cassandra. It allows you to consume data from Legacy as well as CQL3 Cassandra Storage.  It can also harness C* to speed up your process by fetching only the relevant data from C* harnessing CQL3 and C*'s secondary indexes. Though it currently uses only the Hadoop I/O formats for Cassandra in near future we see the same API harnessing other means of consuming Cassandra data like using the StorageProxy or even reading from SSTables directly.\n\nOver the basic data fetch functionality, the Calliope API harnesses Scala and it's implicit parameters and conversions for you to work on a higher abstraction dealing with tuples/objects instead of Cassandra's Row/Columns in your MapRed jobs.\n\nOver past few months we have seen the combination of Spark+Cassandra gaining a lot of traction. And we feel Calliope provides the path of least friction for developers to start working with this combination.\n\nWe have been using this ins production for over a year now and the Calliope early access repository has 30+ users.  I am putting this issue to start a discussion around whether we would want Calliope to be a part of Spark and if yes, what will be involved in doing so.\n\nYou can read more about Calliope here -\nhttp://tuplejump.github.io/calliope\n\n", "comments": ["With the https://github.com/datastax/cassandra-driver-spark from Datastax, we should work on getting a united standard API in Spark, getting good things from both worlds.", "Like HBase, there are already examples of using Cassandra from Spark via standard Hadoop APIs.\nThis sounds like a great stand-alone project. I believe this is another one that should remain external and be linked at http://spark-packages.org/ ?\nAt least I don't see activity here and it seems like this is perfectly usable without requiring Spark to package it."], "derived": {"summary": "Calliope is a library providing an interface to consume data from Cassandra to spark and store RDDs from Spark to Cassandra. Building as wrapper over Cassandra's Hadoop I/O it provides a simplified and very generic API to consume and produces data from and to Cassandra.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Get Cassandra support in Spark Core/Spark Cassandra Module - Calliope is a library providing an interface to consume data from Cassandra to spark and store RDDs from Spark to Cassandra. Building as wrapper over Cassandra's Hadoop I/O it provides a simplified and very generic API to consume and produces data from and to Cassandra."}, {"q": "What updates or decisions were made in the discussion?", "a": "Like HBase, there are already examples of using Cassandra from Spark via standard Hadoop APIs.\nThis sounds like a great stand-alone project. I believe this is another one that should remain external and be linked at http://spark-packages.org/ ?\nAt least I don't see activity here and it seems like this is perfectly usable without requiring Spark to package it."}]}}
{"project": "SPARK", "issue_id": "SPARK-1055", "title": "Spark version on Dockerfile does not match release", "status": "Resolved", "priority": "Major", "reporter": "Gerard Maas", "assignee": "Nan Zhu", "labels": [], "created": "2014-02-05T03:12:36.000+0000", "updated": "2014-02-22T16:59:56.000+0000", "description": "The used Spark version in the .../base/Dockerfile is stale on 0.8.1 and should be updated to 0.9.x to match the release.\nSee:\nhttps://github.com/apache/incubator-spark/blob/branch-0.9/docker/spark-test/base/Dockerfile \n\nPS: Should we add a component for 'Docker'? - not sure under which component I should file this bug.", "comments": ["Scala version is also incorrect"], "derived": {"summary": "The used Spark version in the. /base/Dockerfile is stale on 0.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Spark version on Dockerfile does not match release - The used Spark version in the. /base/Dockerfile is stale on 0."}, {"q": "What updates or decisions were made in the discussion?", "a": "Scala version is also incorrect"}]}}
{"project": "SPARK", "issue_id": "SPARK-1212", "title": "Support sparse data in MLlib", "status": "Resolved", "priority": "Blocker", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-02-05T15:30:41.000+0000", "updated": "2014-04-03T07:22:23.000+0000", "description": "MLlib's NaiveBayes, SGD, and KMeans accept RDD[LabeledPoint] for training and RDD[Array[Double]] for prediction, where LabeledPoint is a wrapper of (Double, Array[Double]). Using Array[Double] could have good performance, but sparse data appears quite often in practice. So I created this JIRA to discuss the plan of adding sparse data support to MLlib and track its progress.\n\nThe goal is to support sparse data for training and prediction in all existing algorithms in MLlib:\n* Gradient Descent\n* K-Means\n* Naive Bayes\n\nPrevious discussions and pull requests:\n* https://github.com/mesos/spark/pull/736", "comments": ["The previous pull request was not closed. Dmitry's mahout math project has scala bindings...code is available in mahout apache git beginning 0.9...kmeans-sparse, sgd sparse and nb sparse can be developed through adding mahout math dependency...although cloudera mentioned that they are not going to support whole mahout in cdh5...but I guess mahout math is clean enough for apache, cdh and hdp ? If you can create a pull request with the right dependency (dmitry's project or mahout math) as Matei mentioned we can build upon it ?", "Which pull request did you mean?\n\nThe sparse vector in mahout is backed by a primitive-typed map, which is not efficient for sequential access used in kmeans, sgd, and nb. I understand that mahout-math is not optimized for performance, but its performance downgrade is not negligible. ", "I think there are multiple versions of sparse vector in mahout math.  This one is better for sequential access:\n\nhttp://svn.apache.org/repos/asf/mahout/trunk/math/src/main/java/org/apache/mahout/math/SequentialAccessSparseVector.java\n\nthe actual data is stored in this:\n\nhttp://svn.apache.org/repos/asf/mahout/trunk/math/src/main/java/org/apache/mahout/math/OrderedIntDoubleMapping.java\n\nthere are still some layers of indirection in there, so its not ideal, but maybe good enough.  Might be worth profiling to see the difference vs a more \"bare-metal\" array based approach.", "Thanks for pointing out there is one sequential access vector in mahout. I will put some benchmark results later. In the meantime, I would suggest separating the user interface from implementation. We don't use types defined outside scala/spark/mllib in public interfaces and utilize either breeze-math or mahout-math in the implementation. That means we need data models that can be easily converted into breeze or mahout vectors/matrices. The overhead is small if we don't need to copy the data, and the benefit is that we still have the freedom to choose which package to use.", "I just sent out a PR (https://github.com/apache/incubator-spark/pull/575, comments attached) for discussion. This PR added sparse data support to KMeans as a pilot. For KMeans, the centers are stored as dense vectors while the input data could be either sparse vectors or dense vectors. The operation needed are Euclidean distance between two vectors and adding a sparse/dense vector to a dense vector (center).\n\nI tested the performance of adding a sparse vector to a dense vector using mahout-math-0.9, breeze-0.5.2, and a naive implementation without any fancy stuff. The sparse vector is casted to a base Vector class (because we want to support both dense and sparse input). The results are the following:\n\nAdding a sparse vector of size 1,000,000 and sparsity 0.01 to a dense vector for 100,000 times.\n~~~\nmahout: 4.292476s\nbreeze: 56.132823s\nnaive : 3.504810s\n~~~\n\nAdding a sparse vector of size 1,000,000 and sparsity 0.05 to a dense vector for 100,000 times.\n~~~\nmahout: 33.308203s\nbreeze: 364.451941s\nnaive : 27.890856s\n~~~\n\nThe issue with breeze is that it uses a generic implementation the input is a base Vector class. If the input is a SparseVector instance, the performance is between mahout and the bare-bone implementation. This issue was fixed (https://github.com/scalanlp/breeze/issues/151) but we need a release from breeze to use in MLlib.\n\n====================\nThis is a proposal for sparse data support in MLlib (https://spark-project.atlassian.net/browse/MLLIB-18). So please ignore minor stuff in this PR. If most people agree with the plan, I will go and add sparse data support to other MLlib algorithms.\n\nThe idea of the proposal is that we define simple data models and factory methods for user to provide sparse input. Then instead of writing a linear algebra library for mllib, we take leverage on an existing linear algebra package in implementing algorithms. So we can change the underlying implementation without breaking the interfaces in the future. We need the following:\n\n\ndata models for sparse vectors. We need data models for dense vector, sequential access sparse vector (backed by two parallel arrays), and random access sparse vector (backed by a primitive-typed hash map, not in this pull request.). Those are defined in the Vec class in this PR.\na linear algebra package. We are considering either breeze or mahout-math. Both have pros and cons, and we can discuss more in the JIRA. This PR uses mahout-math. Mahout vectors do not implement serializable, so we need a serializable wrapper class (MahoutVectorWrapper) to use in spark. As a result, we added not only mahout-math but also mathout-core into dependencies because we need VectorWritable defined mahout-core for the wrapper class. But we can certainly remove most transitive dependencies of mahout-core.\nlightweight converters. The conversion between our data models and Mahout vectors shouldn't involve data copying. However, Mahout vectors hide their members. In this PR, Java reflection is used to get the private fields out. This doesn't seem to be a good solution, but I didn't figure out a better one.\n===================", "Part II adds sparse data support to GLMs and Naive Bayes.\n\nPR: https://github.com/apache/spark/pull/245"], "derived": {"summary": "MLlib's NaiveBayes, SGD, and KMeans accept RDD[LabeledPoint] for training and RDD[Array[Double]] for prediction, where LabeledPoint is a wrapper of (Double, Array[Double]). Using Array[Double] could have good performance, but sparse data appears quite often in practice.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support sparse data in MLlib - MLlib's NaiveBayes, SGD, and KMeans accept RDD[LabeledPoint] for training and RDD[Array[Double]] for prediction, where LabeledPoint is a wrapper of (Double, Array[Double]). Using Array[Double] could have good performance, but sparse data appears quite often in practice."}, {"q": "What updates or decisions were made in the discussion?", "a": "Part II adds sparse data support to GLMs and Naive Bayes.\n\nPR: https://github.com/apache/spark/pull/245"}]}}
{"project": "SPARK", "issue_id": "SPARK-1224", "title": "Improving Documentation for MLlib", "status": "Resolved", "priority": "Major", "reporter": "Martin Jaggi", "assignee": "Martin Jaggi", "labels": [], "created": "2014-02-06T03:11:12.000+0000", "updated": "2014-03-19T05:51:38.000+0000", "description": "Users would need some better documentation the ML models that we support. (For usability and e.g. for comparing different algorithms for the same models). This ticket could be used to track some progress on this.\n\nI have something prepared already for the classification & regression parts, but we definitely need some math formulas in the .md files for that.\nsth like this PR could do it?\nhttps://github.com/shivaram/spark/compare/mathjax\n\nAs for the user guide, we could split the current file\nhttp://spark.incubator.apache.org/docs/latest/mllib-guide.html\ninto separate ones for\na) classification and regression, b) clustering, c) collaborative filtering and d) optimization methods.\n\nAny thoughts on this?", "comments": ["here is a pull request:\nhttps://github.com/apache/incubator-spark/pull/552", "and here is another one, making use of these fancy new tex capabilities :-)\nhttps://github.com/apache/incubator-spark/pull/563", "had to rebase it, the active PR now is\nhttps://github.com/apache/incubator-spark/pull/566", "Martin, shall I mark this JIRA as fixed?"], "derived": {"summary": "Users would need some better documentation the ML models that we support. (For usability and e.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Improving Documentation for MLlib - Users would need some better documentation the ML models that we support. (For usability and e."}, {"q": "What updates or decisions were made in the discussion?", "a": "Martin, shall I mark this JIRA as fixed?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1227", "title": "Diagnostics for Classification&Regression", "status": "Resolved", "priority": "Major", "reporter": "Martin Jaggi", "assignee": "Martin Jaggi", "labels": [], "created": "2014-02-06T03:20:45.000+0000", "updated": "2016-11-07T18:03:27.000+0000", "description": "Currently, the attained objective function is not computed (for efficiency reasons, as one evaluation requires one full pass through the data).\n\nFor diagnostics and comparing different algorithms, we should however provide this as a separate function (one MR).\n\nDoing this requires the loss and regularizer functions themselves, not only their gradients (which are currently in the Gradient class). How about adding the new function directly on the corresponding models in classification/* and regression/* ? Any thoughts?", "comments": ["here is a related discussion for\nROC AUC and Average precision metrics for Classification\nhttps://github.com/apache/incubator-spark/pull/550\n\nthough these are for diagnosis of the testing (of any model candidate), while we also diagnosis for the training (optimization of the model)", "Is this still relevant now that conventional classifier and regressor metrics are implemented in MLlib? You wouldn't be able to compare models by their loss function in general anyway.", "actually this is still relevant, as looking at the training objective value is the only way to tell if a chosen model has been properly trained or not. without this, the user will not know if a resulting bad test error should be blamed on poor training of a good model, or on choosing a wrong model (e.g. wrong regularization parameter). both happens often. or imagine a deep net, where the same question is even harder to tell apart.\n\nnow that MLlib starts offering different training algorithms for the same models (e.g. SGD and L-BFGS), and also different ways of distributing training, the training objective would definitely be useful compare algorithms. (or also when comparing different step-size regimes, such as here:  https://issues.apache.org/jira/browse/SPARK-3942  )\n\nmaybe the nicest way around this would be to provide a nice *benchmarking suite* for regression and classification, which could be used to judge different algorithms in this respect, also if new contributed algorithms need to be compared for efficiency. a benchmarking suite should include a small selection of standard datasets for both sparse and dense data.\n\nthis is also related to the currently not so nice way of passing around the regularizer values (which are part of the training optimization objective) through the updater function, which is currently quite different in SGD as compared to L-BFGS, see the issue here:\nhttps://issues.apache.org/jira/browse/SPARK-2505\nand the spark L-BFBS implementation:\nhttps://github.com/apache/spark/pull/353\n\nirrespective of training error, the classifier methods would also benefit from adding the test accuracy percentage as a function (see current code examples, where this still has to be calculated manually, as it's not implemented yet in {{BinaryClassificationMetrics}}.", "OK you're interested in detecting overfitting, for one? You can't compare algorithms with a different objective function this way though, right? a different regularization param alone means a different objective. So I don't see it can be a general benchmark, and other classifier metrics are more appropriate. But yeah you could compare two runs that share the exact same objective, and that's a way of comparing training algorithms. \n\nAccuracy is in MulticlassMetrics which can be used with binary classification but it would be a nice to have for BinaryClassificationMetrics.", "yes, this can sometimes help to detect overfitting, if combined with test error. and tells us if the model is over-trained or under-trained (such as when using early stopping).\n\nand yes, comparison of methods is only possible for the same objective function.\n\nit's not a replacement for test error, but a benchmark suite would benefit if it could keep track of both test and train objectives.", "I agree it will be nice to provide loss classes.  Even though *Metrics classes exist already, loss classes might be nice as we provide more functionality for diagnosis during learning (e.g., for early stopping, model selection, etc.).  Added link to related JIRA on optimization APIs."], "derived": {"summary": "Currently, the attained objective function is not computed (for efficiency reasons, as one evaluation requires one full pass through the data). For diagnostics and comparing different algorithms, we should however provide this as a separate function (one MR).", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Diagnostics for Classification&Regression - Currently, the attained objective function is not computed (for efficiency reasons, as one evaluation requires one full pass through the data). For diagnostics and comparing different algorithms, we should however provide this as a separate function (one MR)."}, {"q": "What updates or decisions were made in the discussion?", "a": "I agree it will be nice to provide loss classes.  Even though *Metrics classes exist already, loss classes might be nice as we provide more functionality for diagnosis during learning (e.g., for early stopping, model selection, etc.).  Added link to related JIRA on optimization APIs."}]}}
{"project": "SPARK", "issue_id": "SPARK-1056", "title": "Header comment in Executor incorrectly implies it's not used for YARN", "status": "Resolved", "priority": "Trivial", "reporter": "Sanford Ryza", "assignee": "Sandy Ryza", "labels": [], "created": "2014-02-06T15:02:39.000+0000", "updated": "2014-04-04T20:51:01.000+0000", "description": "{code}\n/**\n * Spark executor used with Mesos and the standalone scheduler.\n */\n{code}\n", "comments": [], "derived": {"summary": "{code}\n/**\n * Spark executor used with Mesos and the standalone scheduler. */\n{code}.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Header comment in Executor incorrectly implies it's not used for YARN - {code}\n/**\n * Spark executor used with Mesos and the standalone scheduler. */\n{code}."}]}}
{"project": "SPARK", "issue_id": "SPARK-1057", "title": "Remove Fastutil", "status": "Resolved", "priority": "Major", "reporter": "Evan Chan", "assignee": "Sean R. Owen", "labels": ["Starter"], "created": "2014-02-06T15:30:32.000+0000", "updated": "2014-04-12T05:49:42.000+0000", "description": "Really simple request, to upgrade fastutil to 6.5.x.\nThe current version, 6.4.x, has some minor API's in the Object2xxOpenHashMap structures which is used in many places in Spark, and has been marked deprecated.\nPlus there is a conflict with another library we are using (saddle -- http://saddle.github.io/) which uses a newer version of fastutil.\n\nI'd be happy to send a PR.\n\nI guess a bigger question is do you want to keep using fastutil (SPARK-681) but Spark uses more than just hashmaps, so that probably requires another discussion.", "comments": ["How about upgrading fastutil to 6.5.x till any decision regarding SPARK-681 is made?", "This sounds good.  I'll open a PR soon, guess I can assign this to myself.", "Help, can't figure out how to assign this to myself.", "[~velvia] How did you end up assigning this?", "Usman, I didn't assign it, a committer did.\n\nOn Sat, Mar 22, 2014 at 4:42 PM, Usman Ghani (JIRA)\n\n\n\n-- \n--\nEvan Chan\nStaff Engineer\nev@ooyala.com  |\n", "PR created:\nhttps://github.com/apache/spark/pull/215\n", "I changed the title of the JIRA to reflect the ultimate decision to remove fastutil. It was removed here:\n\nhttps://github.com/apache/spark/pull/266"], "derived": {"summary": "Really simple request, to upgrade fastutil to 6. 5.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Remove Fastutil - Really simple request, to upgrade fastutil to 6. 5."}, {"q": "What updates or decisions were made in the discussion?", "a": "I changed the title of the JIRA to reflect the ultimate decision to remove fastutil. It was removed here:\n\nhttps://github.com/apache/spark/pull/266"}]}}
{"project": "SPARK", "issue_id": "SPARK-1058", "title": "Fix Style Errors and Add Scala Style to Spark Build", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Prashant Sharma", "labels": [], "created": "2014-02-06T17:10:39.000+0000", "updated": "2020-02-07T17:26:44.000+0000", "description": "Style errors eat up a huge amount of our review time. We should add scalastye to the build and fail builds if people make style violations.\n\nA corollary of this is that we need to bring style up to par in cases where it isn't.\n\nScalastyle is planning to make a 4.0.0 release soon, which will allow us to exclude imports from the line limit. I set up an example style file here:\n\nhttps://github.com/pwendell/incubator-spark/compare/style?expand=1\n\nhttps://github.com/scalastyle/scalastyle-plugin\n\nIt would be nice to fix up our existing style errors and integrate scala style into the build.\n", "comments": ["Thanks Prashant for this!"], "derived": {"summary": "Style errors eat up a huge amount of our review time. We should add scalastye to the build and fail builds if people make style violations.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Fix Style Errors and Add Scala Style to Spark Build - Style errors eat up a huge amount of our review time. We should add scalastye to the build and fail builds if people make style violations."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks Prashant for this!"}]}}
{"project": "SPARK", "issue_id": "SPARK-1059", "title": "Now that we submit core requests to YARN, fix usage message in ClientArguments", "status": "Resolved", "priority": "Minor", "reporter": "Sanford Ryza", "assignee": null, "labels": [], "created": "2014-02-06T17:57:08.000+0000", "updated": "2014-04-07T20:04:25.000+0000", "description": "\"Number of cores for the workers (Default: 1). This is unsused right now.\"", "comments": ["This got fixed in Tom's security patch."], "derived": {"summary": "\"Number of cores for the workers (Default: 1). This is unsused right now.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Now that we submit core requests to YARN, fix usage message in ClientArguments - \"Number of cores for the workers (Default: 1). This is unsused right now."}, {"q": "What updates or decisions were made in the discussion?", "a": "This got fixed in Tom's security patch."}]}}
{"project": "SPARK", "issue_id": "SPARK-1060", "title": "JettyUtil is not using host information to start server", "status": "Resolved", "priority": "Major", "reporter": "Nan Zhu", "assignee": "Nan Zhu", "labels": [], "created": "2014-02-06T18:55:42.000+0000", "updated": "2014-02-08T23:39:47.000+0000", "description": "In the current implementation, the webserver in Master/Worker is started with (MasterUI/WorkerUI)\n\nval (srv, bPort) = JettyUtils.startJettyServer(\"0.0.0.0\", port, handlers)\n\ninside startJettyServer\n\nval server = new Server(currentPort) //the Server will take \"0.0.0.0\" as the hostname, i.e. will always bind to the IP address of the first NIC\n\nwhich is actually not using host information got by val host = Utils.localHostName()\n\nthis can cause wrong IP binding, e.g. if the host has two NICs, N1 and N2, the user specify the SPARK_LOCAL_IP as the N2's IP address, however, when starting the web server, for the reason stated above, it will always bind to the N1's address", "comments": ["made a PR https://github.com/apache/incubator-spark/pull/556"], "derived": {"summary": "In the current implementation, the webserver in Master/Worker is started with (MasterUI/WorkerUI)\n\nval (srv, bPort) = JettyUtils. startJettyServer(\"0.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "JettyUtil is not using host information to start server - In the current implementation, the webserver in Master/Worker is started with (MasterUI/WorkerUI)\n\nval (srv, bPort) = JettyUtils. startJettyServer(\"0."}, {"q": "What updates or decisions were made in the discussion?", "a": "made a PR https://github.com/apache/incubator-spark/pull/556"}]}}
{"project": "SPARK", "issue_id": "SPARK-1061", "title": "allow Hadoop RDDs to be read w/ a partitioner", "status": "Resolved", "priority": "Major", "reporter": "Imran Rashid", "assignee": "Imran Rashid", "labels": [], "created": "2014-02-06T19:45:02.000+0000", "updated": "2016-01-05T11:50:42.000+0000", "description": "Using partitioners to get narrow dependencies can save tons of time on a shuffle.  However, after saving an RDD to hdfs, and then reloading it, all partitioner information is lost.  This means that you can never get a narrow dependency when loading data from hadoop.\n\nI think we could get around this by:\n1) having a modified version of hadoop rdd that kept track of original part file (or maybe just prevent splits altogether ...)\n2) add a \"assumePartition(partitioner:Partitioner, verify: Boolean)\" function to RDD.  It would create a new RDD, which had the exact same data but just pretended that the RDD had the given partitioner applied to it.  And if verify=true, it could add a mapPartitionsWithIndex to check that each record was in the right partition.\n\nhttp://apache-spark-user-list.1001560.n3.nabble.com/setting-partitioners-with-hadoop-rdds-td976.html", "comments": ["It might be necessary to modify HadoopRDD here due to the HDFS block size. The problem is that if you save the RDD and one partition is written as part-00000, but that file is more than one HDFS block long, you'll get multiple map tasks reading it when you re-create the RDD from it. So the ideal solution would take into account that any input split from part-00000 falls into partition 1, and would probably need one mapper for that whole file in the current version of Spark.", "yeah, I think this really only makes if the FileInputFormat has isSplittable set to false.  It would be really hard to keep the partitioner accurate if the part files get split.  (I don't think the api gives you enough flexibility even if you wanted to.)\n\nI think as long as HadoopPartitioner.assumePartitioner has a check that the input isn't splittlable, do you think this shoudl work?", "User 'squito' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/4449", "Is this still live?"], "derived": {"summary": "Using partitioners to get narrow dependencies can save tons of time on a shuffle. However, after saving an RDD to hdfs, and then reloading it, all partitioner information is lost.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "allow Hadoop RDDs to be read w/ a partitioner - Using partitioners to get narrow dependencies can save tons of time on a shuffle. However, after saving an RDD to hdfs, and then reloading it, all partitioner information is lost."}, {"q": "What updates or decisions were made in the discussion?", "a": "Is this still live?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1062", "title": "Add rdd.intersection(otherRdd) method", "status": "Resolved", "priority": "Major", "reporter": "Andrew Ash", "assignee": "Andrew Ash", "labels": [], "created": "2014-02-06T22:31:25.000+0000", "updated": "2014-02-06T22:50:13.000+0000", "description": "Based on this GitHub PR: https://github.com/apache/incubator-spark/pull/506", "comments": ["Ready to merge after several rounds of CR", "Hey [~ash211] usually the person working on it remains the asignee even if in the short term other people help review and commit it.", "Ah ok -- I had it assigned to you because I don't think I have permissions to merge into master.  Thanks for the merge!", "ya at least in other projects i've been on. The assignee is consistently the person who contributed the code. This way after the fact people can see that you worked on it, even if I happened to commit it.", "I'm used to having it land on the person with the \"next action\" but I'm happy to do the code contributor instead.  Cheers!", "Ya that's definitely how JIRA is used in many contexts... anyways I guess we can use either approach."], "derived": {"summary": "Based on this GitHub PR: https://github. com/apache/incubator-spark/pull/506.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add rdd.intersection(otherRdd) method - Based on this GitHub PR: https://github. com/apache/incubator-spark/pull/506."}, {"q": "What updates or decisions were made in the discussion?", "a": "Ya that's definitely how JIRA is used in many contexts... anyways I guess we can use either approach."}]}}
{"project": "SPARK", "issue_id": "SPARK-1063", "title": "Add .sortBy(f) method on RDD", "status": "Resolved", "priority": "Major", "reporter": "Andrew Ash", "assignee": "Andrew Ash", "labels": [], "created": "2014-02-06T22:34:36.000+0000", "updated": "2014-11-06T07:09:39.000+0000", "description": "Based on this GitHub PR: https://github.com/apache/incubator-spark/pull/508\n\nI've written the below so many times that I think it'd be broadly useful to have a .sortBy(f) method on RDD:\n\n{code}\n      .keyBy{l => <my function> }\n      .sortByKey()\n      .map(_._2)\n{code}", "comments": ["The old PR is gone now (and was never merged) so I'm re-aiming at the apache/spark repo here:\n\nhttps://github.com/apache/spark/pull/369"], "derived": {"summary": "Based on this GitHub PR: https://github. com/apache/incubator-spark/pull/508\n\nI've written the below so many times that I think it'd be broadly useful to have a.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add .sortBy(f) method on RDD - Based on this GitHub PR: https://github. com/apache/incubator-spark/pull/508\n\nI've written the below so many times that I think it'd be broadly useful to have a."}, {"q": "What updates or decisions were made in the discussion?", "a": "The old PR is gone now (and was never merged) so I'm re-aiming at the apache/spark repo here:\n\nhttps://github.com/apache/spark/pull/369"}]}}
{"project": "SPARK", "issue_id": "SPARK-1064", "title": "Make it possible to use cluster's Hadoop jars when running against YARN", "status": "Resolved", "priority": "Major", "reporter": "Sanford Ryza", "assignee": "Sandy Ryza", "labels": [], "created": "2014-02-07T10:25:03.000+0000", "updated": "2014-04-04T20:50:58.000+0000", "description": "YARN applications like MapReduce and Tez rely on the cluster's Hadoop jars instead of distributing their own.\n\nThis has a couple advantages\n* Avoids sending a bunch of bits to every node for each app\n* Only a single version of Hadoop can be running on a cluster at one time, simplifying debugging\n* Easier to upgrade and apply patched versions of Hadoop", "comments": ["https://github.com/apache/incubator-spark/pull/649", "https://github.com/apache/spark/pull/102"], "derived": {"summary": "YARN applications like MapReduce and Tez rely on the cluster's Hadoop jars instead of distributing their own. This has a couple advantages\n* Avoids sending a bunch of bits to every node for each app\n* Only a single version of Hadoop can be running on a cluster at one time, simplifying debugging\n* Easier to upgrade and apply patched versions of Hadoop.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Make it possible to use cluster's Hadoop jars when running against YARN - YARN applications like MapReduce and Tez rely on the cluster's Hadoop jars instead of distributing their own. This has a couple advantages\n* Avoids sending a bunch of bits to every node for each app\n* Only a single version of Hadoop can be running on a cluster at one time, simplifying debugging\n* Easier to upgrade and apply patched versions of Hadoop."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/102"}]}}
{"project": "SPARK", "issue_id": "SPARK-1065", "title": "PySpark runs out of memory with large broadcast variables", "status": "Resolved", "priority": "Major", "reporter": "Josh Rosen", "assignee": "Davies Liu", "labels": [], "created": "2014-02-07T11:41:54.000+0000", "updated": "2014-08-17T00:00:30.000+0000", "description": "PySpark's driver components may run out of memory when broadcasting large variables (say 1 gigabyte).\n\nBecause PySpark's broadcast is implemented on top of Java Spark's broadcast by broadcasting a pickled Python as a byte array, we may be retaining multiple copies of the large object: a pickled copy in the JVM and a deserialized copy in the Python driver.\n\nThe problem could also be due to memory requirements during pickling.\n\nPySpark is also affected by broadcast variables not being garbage collected.  Adding an unpersist() method to broadcast variables may fix this: https://github.com/apache/incubator-spark/pull/543.\n\nAs a first step to fixing this, we should write a failing test to reproduce the error.\n\nThis was discovered by [~sandy]: [\"trouble with broadcast variables on pyspark\"|http://apache-spark-user-list.1001560.n3.nabble.com/trouble-with-broadcast-variables-on-pyspark-tp1301.html].", "comments": ["Here's some code that reproduces it.\n\n{code}\ntheconf = SparkConf().set(\"spark.executor.memory\", \"5g\").setAppName(\"broadcastfail\").setMaster(cluster_url)\nsc = SparkContext(conf=theconf)\n\nbroadcast_vals = []\nfor i in range(5):\n  datas = [[float(i) for i in range(200)] for i in range(100000)]\n  val = sc.broadcast(datas).value\n  broadcast_vals.append(val)\n\nsc.parallelize([i for i in range(80)]).map(lambda x: sum([len(val) for val in broadcast_vals])).collect()\n{code}\n\nIt generates a few arrays of floats, each of which should take about 160 MB.  The executors never end up using much memory, but the driver uses an enormous amount.  Both the python and java processes ramp up to multiple GB until I start seeing a bunch of \"OutOfMemoryError: java heap space\".\n\nWith a single 160MB array, the job completes fine, but the driver still uses about 9 GB.\n", "I am facing the same issue in my project, where I use PySpark. As a proof of that the big objects I have could easily fit into nodes' memory, I am going to use dummy solution of saving my big objects into HDFS and load them on Python nodes.\n\nDoes anybody have an idea how to fix the issue in a better way? I don't have enough either Scala nor Java knowledge to fix this in Spark core. However, I feel like broadcast variables could be reimplemented on Python side though it seems a bit dangerous idea because we don't want to have separate implementations of one thing in both languages. That will also save memory, because while we use broadcasts through Scala we have 1 copy in JVM, 1 pickled copy in Python and 1 constructed object copy in Python.", "I have finished my experiment of using HDFS as a temp storage for my big objects. It showed that my mappers do not leak memory and work pretty stable. However, it takes long time to load these objects on each node.\n\nIs there straightforward way to detect memory leaks in Spark and PySpark?", "The broadcast was not used correctly in the above code, it should be used like this:\n\n{code}\nbroadcast_vals = []\nfor i in range(5):\n  datas = [[float(i) for i in range(200)] for i in range(100000)]\n  val = sc.broadcast(datas)\n  broadcast_vals.append(val)\n\nsc.parallelize([i for i in range(80)]).map(lambda x: sum([len(val.value) for val in broadcast_vals])).collect()\n{code}\n\nThe reference of object in Python driver in not necessary in most cases, we will make it optional (no reference by default), then it can reduce the memory used in Python driver.", "User 'davies' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1912", "[~davies] Will your PR take into account this fix: [SPARK-2521] Broadcast RDD object (instead of sending it along with every task) https://github.com/apache/spark/commit/7b8cd175254d42c8e82f0aa8eb4b7f3508d8fde2 ?\n\"The patch uses broadcast to send RDD objects and the closures to executors\"", "[~davies] I have not noticed that there was that mistake in the example, but I have not used that code. I run into the issue in my own code, where I use broadcasts correctly.\n\nI'm building your branch now and will try it right away. Thank you for your fix!", "[~frol], I think broadcast the RDD object is already done by that PR.\n\nBut the serialized closure will still be sent to JVM by py4j. After using broadcast for large datasets, the serialized closure should not be too huge, so I guess it will not be a big issue.", "After this patch, the above test can run successfully with about 700M memory in Python driver, 5xxMB memory in JVM driver, and 3G memory in python worker.\n\nIt may triggle another problem when run it with Mesos or YARN, because Spark does not reserve memory for Python worker, this may be fixed in 1.2 release.", "[~davies] I understand that if you use broadcast explicitly the closure won't be huge, but the point of that PR was also \"1. Users won't need to decide what to broadcast anymore, unless they would want to use a large object multiple times in different operations\".", "[~davies] I use YARN setup so I will see how it goes.", "[~davies] I have compiled and run your broadcast branch against my cluster on YARN. It does not leak memory any more! And it is at least 25% faster than my dummy implementation on top of HDFS. Implementation with broadcasts takes 4.5 minutes to finish a task, where my implementation took 6 minutes. More heavy tests are still working.", "Heavy tasks completed in 18 minutes each instead of 22 minutes, which is 20% speed up. That is nice!\nI don't see any problems on my YARN cluster. Java nodes eat up to 1.5GB RAM (which is my JVM limit) each and Python daemons eat around 650MB each. Though those numbers are still a bit weird, it is obvious to me that workers don't leak now.\n\nThank you a lot!", "Cool, thanks for the tests. If we can compress the data, it will be faster. I will do it in another separate PR.\n\nThe closure is serialized by cloudpickle, so it will be much slower if you do not use broadcast explicitly. We can show an warning if the serialized closure is too big."], "derived": {"summary": "PySpark's driver components may run out of memory when broadcasting large variables (say 1 gigabyte). Because PySpark's broadcast is implemented on top of Java Spark's broadcast by broadcasting a pickled Python as a byte array, we may be retaining multiple copies of the large object: a pickled copy in the JVM and a deserialized copy in the Python driver.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "PySpark runs out of memory with large broadcast variables - PySpark's driver components may run out of memory when broadcasting large variables (say 1 gigabyte). Because PySpark's broadcast is implemented on top of Java Spark's broadcast by broadcasting a pickled Python as a byte array, we may be retaining multiple copies of the large object: a pickled copy in the JVM and a deserialized copy in the Python driver."}, {"q": "What updates or decisions were made in the discussion?", "a": "Cool, thanks for the tests. If we can compress the data, it will be faster. I will do it in another separate PR.\n\nThe closure is serialized by cloudpickle, so it will be much slower if you do not use broadcast explicitly. We can show an warning if the serialized closure is too big."}]}}
{"project": "SPARK", "issue_id": "SPARK-1066", "title": "Improve Developer Documentation", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Patrick McFadin", "labels": [], "created": "2014-02-07T11:45:23.000+0000", "updated": "2014-03-30T04:14:19.000+0000", "description": "- Port release documentation to the wiki: \nhttps://docs.google.com/document/d/15YGRQjnL6IVsULgPd4xsxqbJQiU438mU6TxR-5uYvLs/edit#heading=h.y4mkppazvqs9\n\n- Contribute developer scripts to spark repo\n- Describe on wiki how to use local builds", "comments": ["Looks like this is fixed?"], "derived": {"summary": "- Port release documentation to the wiki: \nhttps://docs. google.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Improve Developer Documentation - - Port release documentation to the wiki: \nhttps://docs. google."}, {"q": "What updates or decisions were made in the discussion?", "a": "Looks like this is fixed?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1067", "title": "Default log4j initialization causes errors for those not using log4j", "status": "Closed", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "labels": [], "created": "2014-02-07T18:06:38.000+0000", "updated": "2014-03-08T16:12:49.000+0000", "description": "Some users do not use slf4j-log4j... by assuming thy are using log4j this breaks their use of spark.", "comments": [], "derived": {"summary": "Some users do not use slf4j-log4j. by assuming thy are using log4j this breaks their use of spark.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Default log4j initialization causes errors for those not using log4j - Some users do not use slf4j-log4j. by assuming thy are using log4j this breaks their use of spark."}]}}
{"project": "SPARK", "issue_id": "SPARK-1068", "title": "Worker.scala should kill drivers in method postStop()", "status": "Resolved", "priority": "Major", "reporter": "Qiuzhuang Lian", "assignee": "Qiuzhuang Lian", "labels": [], "created": "2014-02-08T00:05:50.000+0000", "updated": "2014-02-08T13:00:58.000+0000", "description": null, "comments": ["PR: https://github.com/apache/incubator-spark/pull/561"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Worker.scala should kill drivers in method postStop()"}, {"q": "What updates or decisions were made in the discussion?", "a": "PR: https://github.com/apache/incubator-spark/pull/561"}]}}
{"project": "SPARK", "issue_id": "SPARK-1069", "title": "Provide binary compatibility in Spark 1.X releases", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": null, "labels": [], "created": "2014-02-08T12:50:19.000+0000", "updated": "2014-09-16T16:12:21.000+0000", "description": "It would be good if Spark offered binary compatibility between versions, meaning users do not have to recompile. This JIRA marks the intention of codifying this as a policy once we do a 1.1 release which is link-level compatible with 1.0.\n\nExamples of binary incompatibilities:\nhttps://github.com/jsuereth/binary-resilience\n\nSome good talks on this:\nhttps://skillsmatter.com/skillscasts/3269-binary-resilience\nhttp://www.slideshare.net/mircodotta/managing-binary-compatibility-in-scala", "comments": ["We've implemented MIMA and a bunch of other things to help us do this, so I can close this now."], "derived": {"summary": "It would be good if Spark offered binary compatibility between versions, meaning users do not have to recompile. This JIRA marks the intention of codifying this as a policy once we do a 1.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Provide binary compatibility in Spark 1.X releases - It would be good if Spark offered binary compatibility between versions, meaning users do not have to recompile. This JIRA marks the intention of codifying this as a policy once we do a 1."}, {"q": "What updates or decisions were made in the discussion?", "a": "We've implemented MIMA and a bunch of other things to help us do this, so I can close this now."}]}}
{"project": "SPARK", "issue_id": "SPARK-1220", "title": "Principal Component Analysis", "status": "Resolved", "priority": "Major", "reporter": "Reza Zadeh", "assignee": "Reza Zadeh", "labels": [], "created": "2014-02-08T18:12:35.000+0000", "updated": "2014-03-20T21:58:35.000+0000", "description": "Build Principal Component Analysis (PCA) for Spark. Use previous SVD implementation.\nCurrent Pull Request for this:\n * github.com/apache/spark/pull/88", "comments": [], "derived": {"summary": "Build Principal Component Analysis (PCA) for Spark. Use previous SVD implementation.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Principal Component Analysis - Build Principal Component Analysis (PCA) for Spark. Use previous SVD implementation."}]}}
{"project": "SPARK", "issue_id": "SPARK-1070", "title": "Add check for JIRA ticket in the Github pull request title/summary with CI", "status": "Resolved", "priority": "Minor", "reporter": "Henry Saputra", "assignee": "Mark Hamstra", "labels": [], "created": "2014-02-08T23:43:12.000+0000", "updated": "2016-01-05T19:43:21.000+0000", "description": "As part of discussion in the dev@ list to add audit trail of Spark's Github pull requests (PR) to JIRA, need to add check maybe in the Jenkins CI to verify that the PRs contain JIRA ticket number in the title/ summary.\n \nThere are maybe some PRs that may not need ticket so probably add support for some \"magic\" keyword to bypass the check. But this should be done in rare cases.", "comments": ["[~hsaputra] - The [Spark PR Board | https://spark-prs.appspot.com/] automatically parses JIRA ticket IDs in the PR titles. Does that address the need behind this request?\n\ncc [~joshrosen]", "[~nchammas], way back when Patrick propose the right way to send PR there was a discussion to \"force\" PR to have JIRA rocket prefix in the summary.\nThis ticket is filed to address that issue/ idea.", "Resolving as \"Won't Fix.\" Most contributors now submit PRs with JIRAs, so the cost of infrequently reminding new folks to file them isn't high enough to justify automation here."], "derived": {"summary": "As part of discussion in the dev@ list to add audit trail of Spark's Github pull requests (PR) to JIRA, need to add check maybe in the Jenkins CI to verify that the PRs contain JIRA ticket number in the title/ summary. There are maybe some PRs that may not need ticket so probably add support for some \"magic\" keyword to bypass the check.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add check for JIRA ticket in the Github pull request title/summary with CI - As part of discussion in the dev@ list to add audit trail of Spark's Github pull requests (PR) to JIRA, need to add check maybe in the Jenkins CI to verify that the PRs contain JIRA ticket number in the title/ summary. There are maybe some PRs that may not need ticket so probably add support for some \"magic\" keyword to bypass the check."}, {"q": "What updates or decisions were made in the discussion?", "a": "Resolving as \"Won't Fix.\" Most contributors now submit PRs with JIRAs, so the cost of infrequently reminding new folks to file them isn't high enough to justify automation here."}]}}
{"project": "SPARK", "issue_id": "SPARK-1071", "title": "Tidy logging strategy and use of log4j", "status": "Resolved", "priority": "Minor", "reporter": "Sean R. Owen", "assignee": "Sean R. Owen", "labels": [], "created": "2014-02-09T14:49:59.000+0000", "updated": "2015-01-15T09:08:43.000+0000", "description": "Prompted by a recent thread on the mailing list, I tried and failed to see if Spark can be made independent of log4j. There are a few cases where control of the underlying logging is pretty useful, and to do that, you have to bind to a specific logger. \n\nInstead I propose some tidying that leaves Spark's use of log4j, but gets rid of warnings and should still enable downstream users to switch. The idea is to pipe everything (except log4j) through SLF4J, and have Spark use SLF4J directly when logging, and where Spark needs to output info (REPL and tests), bind from SLF4J to log4j.\n\nThis leaves the same behavior in Spark. It means that downstream users who want to use something except log4j should:\n\n- Exclude dependencies on log4j, slf4j-log4j12 from Spark\n- Include dependency on log4j-over-slf4j\n- Include dependency on another logger X, and another slf4j-X\n- Recreate any log config that Spark does, that is needed, in the other logger's config\n\nThat sounds about right.\n\nHere are the key changes: \n\n- Include the jcl-over-slf4j shim everywhere by depending on it in core.\n- Exclude dependencies on commons-logging from third-party libraries.\n- Include the jul-to-slf4j shim everywhere by depending on it in core.\n- Exclude slf4j-* dependencies from third-party libraries to prevent collision or warnings\n- Added missing slf4j-log4j12 binding to GraphX, Bagel module tests\n\nAnd minor/incidental changes:\n\n- Update to SLF4J 1.7.5, which happily matches Hadoop 2s version and is a recommended update over 1.7.2\n- (Remove a duplicate HBase dependency declaration in SparkBuild.scala)\n- (Remove a duplicate mockito dependency declaration that was causing warnings and bugging me)\n\nPull request coming.", "comments": ["If a user attempts to use log4j-over-slf4j without first excluding Spark's dependency on log4j and slf4j-log4j12, will they see the standard stack overflow exception between those two? If so, we  should perhaps add a \"Logging with Spark\" section somewhere in the docs to be very clear how to interface with Spark's logger.", "slf4j detects that situation and provides a good warning. (This isn't quite the StackOverflowError situation referred to in the conversation on the mailing list last week.) It's all par for the course in slf4j, but docs can't hurt. If the project proceeds with a change like this I'll write up a blurb on the wiki, sure."], "derived": {"summary": "Prompted by a recent thread on the mailing list, I tried and failed to see if Spark can be made independent of log4j. There are a few cases where control of the underlying logging is pretty useful, and to do that, you have to bind to a specific logger.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Tidy logging strategy and use of log4j - Prompted by a recent thread on the mailing list, I tried and failed to see if Spark can be made independent of log4j. There are a few cases where control of the underlying logging is pretty useful, and to do that, you have to bind to a specific logger."}, {"q": "What updates or decisions were made in the discussion?", "a": "slf4j detects that situation and provides a good warning. (This isn't quite the StackOverflowError situation referred to in the conversation on the mailing list last week.) It's all par for the course in slf4j, but docs can't hurt. If the project proceeds with a change like this I'll write up a blurb on the wiki, sure."}]}}
{"project": "SPARK", "issue_id": "SPARK-1072", "title": "Use binary search for RangePartitioner when there is more than 1000 partitions", "status": "Resolved", "priority": "Minor", "reporter": "Holden Karau", "assignee": "Holden Karau", "labels": [], "created": "2014-02-09T23:25:12.000+0000", "updated": "2014-06-18T07:57:22.000+0000", "description": null, "comments": ["This is fixed with \n\nhttps://github.com/apache/incubator-spark/pull/571\n\n?"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Use binary search for RangePartitioner when there is more than 1000 partitions"}, {"q": "What updates or decisions were made in the discussion?", "a": "This is fixed with \n\nhttps://github.com/apache/incubator-spark/pull/571\n\n?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1073", "title": "GitHub PR squasher has bad titles", "status": "Resolved", "priority": "Major", "reporter": "Andrew Ash", "assignee": "Andrew Ash", "labels": [], "created": "2014-02-10T00:27:10.000+0000", "updated": "2014-02-12T23:29:21.000+0000", "description": "https://github.com/apache/incubator-spark/pull/574", "comments": ["Thanks for this Andrew! I consider this a \"Major\" issue - good project infrastructure is important :)"], "derived": {"summary": "https://github. com/apache/incubator-spark/pull/574.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "GitHub PR squasher has bad titles - https://github. com/apache/incubator-spark/pull/574."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for this Andrew! I consider this a \"Major\" issue - good project infrastructure is important :)"}]}}
{"project": "SPARK", "issue_id": "SPARK-1074", "title": "JavaPairRDD as Object File", "status": "Resolved", "priority": "Minor", "reporter": "Kevin Mader", "assignee": null, "labels": [], "created": "2014-02-10T07:37:47.000+0000", "updated": "2014-11-08T10:00:50.000+0000", "description": "So I can perform a save command on a JavaPairRDD\n{code:java}\nstatic public void HSave(JavaPairRDD<D3int, int[]> baseImg,String path) {\n\t\tfinal String outpath=(new File(path)).getAbsolutePath();\n\t\tbaseImg.saveAsObjectFile(outpath);\n}\n{code}\n\nWhen I use the objectFile command from the JavaSparkContext \n\n{code:java}\nstatic public  ReadObjectFile(JavaSparkContext jsc, final String path) {\n\tJavaPairRDD<D3int, int[]> newImage=(JavaPairRDD<D3int,int[]>) jsc.objectFile(path);\n}\n{code}\n\nI get an error cannot cast from JavaRDD to JavaPairRDD. Is there a way to get back to JavaPairRDD or will I need to map my data to a JavaRDD, save, load, then remap the JavaRDD back to the JavaPairRDD\n", "comments": ["Attempting to open up the saved objectFile using the spark shell / scala initially appears to work fine\n\n{code:none}\nval inObj=sc.objectFile(objPath)\norg.apache.spark.rdd.RDD[Nothing] = FlatMappedRDD[3] at objectFile at <console>:12\n{code}\nBut when you take the first element it produces an Array Store Exception\n{code:none}\ninObj.take(1)\njava.lang.ArrayStoreException: [Ljava.lang.Object;\n\tat scala.runtime.ScalaRunTime$.array_update(ScalaRunTime.scala:88)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:835)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:835)\n\tat org.apache.spark.scheduler.JobWaiter.taskSucceeded(JobWaiter.scala:56)\n\tat org.apache.spark.scheduler.DAGScheduler.runLocallyWithinThread(DAGScheduler.scala:695)\n\tat org.apache.spark.scheduler.DAGScheduler$$anon$1.run(DAGScheduler.scala:679)\n{code}", "Hi Kevin,\n\nIn Java, try doing a map() on the JavaRDD to get a JavaPairRDD. You can use the identity function in there and it will just mark it as having pairs.\n\nAlternatively you can do new JavaPairRDD(javaRDD.rdd()).", "Am I right in thinking that if you want to save a JavaPairRDD to HDFS, you have key-value pairs, and so you want to use JavaPairRDD.saveAsNewAPIHadoopFile, and SparkContext.sequenceFile to read it? This works. objectFile doesn't seem like the right approach anyway."], "derived": {"summary": "So I can perform a save command on a JavaPairRDD\n{code:java}\nstatic public void HSave(JavaPairRDD<D3int, int[]> baseImg,String path) {\n\t\tfinal String outpath=(new File(path)). getAbsolutePath();\n\t\tbaseImg.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "JavaPairRDD as Object File - So I can perform a save command on a JavaPairRDD\n{code:java}\nstatic public void HSave(JavaPairRDD<D3int, int[]> baseImg,String path) {\n\t\tfinal String outpath=(new File(path)). getAbsolutePath();\n\t\tbaseImg."}, {"q": "What updates or decisions were made in the discussion?", "a": "Am I right in thinking that if you want to save a JavaPairRDD to HDFS, you have key-value pairs, and so you want to use JavaPairRDD.saveAsNewAPIHadoopFile, and SparkContext.sequenceFile to read it? This works. objectFile doesn't seem like the right approach anyway."}]}}
{"project": "SPARK", "issue_id": "SPARK-1075", "title": "Fix simple doc typo in Spark Streaming Custom Receiver", "status": "Resolved", "priority": "Minor", "reporter": "Henry Saputra", "assignee": "Henry Saputra", "labels": ["doc"], "created": "2014-02-10T11:56:47.000+0000", "updated": "2014-02-11T15:30:40.000+0000", "description": "The closing parentheses in the constructor in the first code block example is reversed:\n\ndiff --git a/docs/streaming-custom-receivers.md b/docs/streaming-custom-receivers.md\nindex 4e27d65..3fb540c 100644\n--- a/docs/streaming-custom-receivers.md\n+++ b/docs/streaming-custom-receivers.md\n@@ -14,7 +14,7 @@ This starts with implementing [NetworkReceiver](api/streaming/index.html#org.apa\n The following is a simple socket text-stream receiver.\n \n {% highlight scala %}\n-       class SocketTextStreamReceiver(host: String, port: Int(\n+       class SocketTextStreamReceiver(host: String, port: Int)\n          extends NetworkReceiver[String]\n        {\n          protected lazy val blocksGenerator: BlockGenerator =", "comments": ["PR: \n\nhttps://github.com/apache/incubator-spark/pull/577", "PR from https://github.com/apache/incubator-spark/pull/577 merged to ASF git repo."], "derived": {"summary": "The closing parentheses in the constructor in the first code block example is reversed:\n\ndiff --git a/docs/streaming-custom-receivers. md b/docs/streaming-custom-receivers.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Fix simple doc typo in Spark Streaming Custom Receiver - The closing parentheses in the constructor in the first code block example is reversed:\n\ndiff --git a/docs/streaming-custom-receivers. md b/docs/streaming-custom-receivers."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR from https://github.com/apache/incubator-spark/pull/577 merged to ASF git repo."}]}}
{"project": "SPARK", "issue_id": "SPARK-1076", "title": "Adding zipWithIndex and zipWithUniqueId to RDD", "status": "Resolved", "priority": "Major", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-02-10T15:28:36.000+0000", "updated": "2014-02-21T11:02:21.000+0000", "description": "Assign ranks to an ordered or unordered data set is a common operation. This could be done by first counting records in each partition and then assign ranks in parallel. \n\nThe purpose of assigning ranks to an unordered set is usually to get a unique id for each item, e.g., to map feature names to feature indices. In such cases, the assignment could be done without counting records, saving one spark job.", "comments": ["PR: https://github.com/apache/incubator-spark/pull/578"], "derived": {"summary": "Assign ranks to an ordered or unordered data set is a common operation. This could be done by first counting records in each partition and then assign ranks in parallel.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Adding zipWithIndex and zipWithUniqueId to RDD - Assign ranks to an ordered or unordered data set is a common operation. This could be done by first counting records in each partition and then assign ranks in parallel."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR: https://github.com/apache/incubator-spark/pull/578"}]}}
{"project": "SPARK", "issue_id": "SPARK-1225", "title": "ROC AUC and Average Precision for Binary classification models", "status": "Resolved", "priority": "Minor", "reporter": "Sven Schmit", "assignee": "Sven Schmit", "labels": [], "created": "2014-02-10T18:18:32.000+0000", "updated": "2014-04-11T19:08:13.000+0000", "description": "Implementation of Receiver Operator Characteristic area under the curve and Average Precision performance metrics for binary classification tasks. \n\nCurrently works for \n- Logistic regression\n- SVM classification\n\nPull request:\nhttps://github.com/apache/spark/pull/160", "comments": ["Included in https://github.com/apache/spark/pull/364"], "derived": {"summary": "Implementation of Receiver Operator Characteristic area under the curve and Average Precision performance metrics for binary classification tasks. Currently works for \n- Logistic regression\n- SVM classification\n\nPull request:\nhttps://github.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "ROC AUC and Average Precision for Binary classification models - Implementation of Receiver Operator Characteristic area under the curve and Average Precision performance metrics for binary classification tasks. Currently works for \n- Logistic regression\n- SVM classification\n\nPull request:\nhttps://github."}, {"q": "What updates or decisions were made in the discussion?", "a": "Included in https://github.com/apache/spark/pull/364"}]}}
{"project": "SPARK", "issue_id": "SPARK-1077", "title": "Highlighted codes are not displayed properly in docs", "status": "Resolved", "priority": "Minor", "reporter": "Jyotiska NK", "assignee": null, "labels": [], "created": "2014-02-10T22:46:36.000+0000", "updated": "2014-02-10T23:29:39.000+0000", "description": "In the documentation and programming guides, the highlighted codes are not displayed properly. Something wrong with Jekyll's syntax highlighter?\n\nSee here:\nhttps://github.com/jyotiska/incubator-spark/blob/master/docs/scala-programming-guide.md#initializing-spark", "comments": ["[The docs use Pygments for syntax highlighting|https://github.com/apache/incubator-spark/tree/master/docs#pygments], not Github-flavored markdown, so they don't display properly in GitHub.\n\nYou can follow the instructions in the {{docs}} folder to compile the docs yourself, or you can view the compiled docs on the Spark website: https://spark.incubator.apache.org/docs/latest/scala-programming-guide.html#initializing-spark", "Cool! I will be contributing to the existing Python programming guide, which is not much detailed compared to Scala guide. If I submit PR for the Github's docs and programming guides, will they be automatically merged with official docs at (https://spark.incubator.apache.org/docs/latest/python-programming-guide.html)? ", "No, they won't be automatically merged into the Spark website.  The official docs reflect released versions of Spark, not snapshot builds.  We might be able to modify the Jenkins master branch build to automatically publish snapshot version of the docs and Spark JARs.", "Actually that's a pretty good idea (to publish snapshot documentation). I am not sure how easy it is to setup though. ", "This is expected."], "derived": {"summary": "In the documentation and programming guides, the highlighted codes are not displayed properly. Something wrong with Jekyll's syntax highlighter?\n\nSee here:\nhttps://github.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Highlighted codes are not displayed properly in docs - In the documentation and programming guides, the highlighted codes are not displayed properly. Something wrong with Jekyll's syntax highlighter?\n\nSee here:\nhttps://github."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is expected."}]}}
{"project": "SPARK", "issue_id": "SPARK-1078", "title": "Replace lift-json with json4s-jackson", "status": "Resolved", "priority": "Minor", "reporter": "William Benton", "assignee": null, "labels": [], "created": "2014-02-11T12:03:45.000+0000", "updated": "2014-09-04T17:54:05.000+0000", "description": "json4s-jackson is a Jackson-backed implementation of the Json4s common JSON API for Scala JSON libraries.  (Evan Chan has a nice comparison of Scala JSON libraries here:  http://engineering.ooyala.com/blog/comparing-scala-json-libraries)  It is Apache-licensed, mostly API-compatible with lift-json, and easier for downstream operating system distributions to consume than lift-json.\n\nIn terms of performance, json4s-jackson is slightly slower but comparable to lift-json on my machine when parsing very small JSON files (< 2kb and < ~30 objects), around 40% faster than lift-json on medium-sized files (~50kb), and significantly (~10x) faster on multi-megabyte files.", "comments": ["(Here's the PR:  https://github.com/apache/incubator-spark/pull/582 )", "It looks like this was fixed in SPARK-1132 / Spark 1.0.0, where we migrated to json4s.jackson. "], "derived": {"summary": "json4s-jackson is a Jackson-backed implementation of the Json4s common JSON API for Scala JSON libraries. (Evan Chan has a nice comparison of Scala JSON libraries here:  http://engineering.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "Replace lift-json with json4s-jackson - json4s-jackson is a Jackson-backed implementation of the Json4s common JSON API for Scala JSON libraries. (Evan Chan has a nice comparison of Scala JSON libraries here:  http://engineering."}, {"q": "What updates or decisions were made in the discussion?", "a": "It looks like this was fixed in SPARK-1132 / Spark 1.0.0, where we migrated to json4s.jackson."}]}}
{"project": "SPARK", "issue_id": "SPARK-1079", "title": "EC2 scripts should allow mounting as XFS or EXT4", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": null, "labels": [], "created": "2014-02-11T20:05:10.000+0000", "updated": "2016-01-27T10:10:39.000+0000", "description": "These offer much better performance when running benchmarks: I've done a hacked together implementation here, but it would be better if you could officially give a filesystem as an argument in the ec2 scripts:\n\nhttps://github.com/pwendell/spark-ec2/blob/c63995ce014df61ec1c61276687767e789eb79f7/setup-slave.sh#L21", "comments": [], "derived": {"summary": "These offer much better performance when running benchmarks: I've done a hacked together implementation here, but it would be better if you could officially give a filesystem as an argument in the ec2 scripts:\n\nhttps://github. com/pwendell/spark-ec2/blob/c63995ce014df61ec1c61276687767e789eb79f7/setup-slave.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "EC2 scripts should allow mounting as XFS or EXT4 - These offer much better performance when running benchmarks: I've done a hacked together implementation here, but it would be better if you could officially give a filesystem as an argument in the ec2 scripts:\n\nhttps://github. com/pwendell/spark-ec2/blob/c63995ce014df61ec1c61276687767e789eb79f7/setup-slave."}]}}
{"project": "SPARK", "issue_id": "SPARK-1080", "title": "ZK PersistenceEngine does not respect zookeeper dir", "status": "Closed", "priority": "Major", "reporter": "Aaron Davidson", "assignee": "Aaron Davidson", "labels": [], "created": "2014-02-11T22:25:04.000+0000", "updated": "2014-02-11T23:25:54.000+0000", "description": "ZooKeeperPersistenceEngine is hard-coded to deserialize files from /spark/master_status, so the PersistenceEngine would fail to recover if the user specifies a custom spark.deploy.zookeeper.dir.", "comments": ["Not strictly limited to this issue and related pull request, but something that I've been wondering about wrt ZooKeeperPersistenceEngine that maybe someone can clarify for me: Why is this code using the notoriously difficult ZooKeeper API directly instead of taking advantage of Curator?", "https://github.com/apache/incubator-spark/pull/583\n\nThanks Raymond Liu!", "@[~markhamstra] Mostly because I didn't know about Curator until 5 minutes ago! A rewrite would not be unwelcome if it's simpler and more likely to work correctly. Such a rewrite could almost certainly allow us to remove [SparkZooKeeperSession|https://github.com/apache/incubator-spark/blob/master/core/src/main/scala/org/apache/spark/deploy/master/SparkZooKeeperSession.scala].\n\nA new JIRA should probably be opened for that, though.", "Okay, that answers that.\n\nYes, a rewrite to use Curator does belong in a separate JIRA.", "Created SPARK-1082 to track this."], "derived": {"summary": "ZooKeeperPersistenceEngine is hard-coded to deserialize files from /spark/master_status, so the PersistenceEngine would fail to recover if the user specifies a custom spark. deploy.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "ZK PersistenceEngine does not respect zookeeper dir - ZooKeeperPersistenceEngine is hard-coded to deserialize files from /spark/master_status, so the PersistenceEngine would fail to recover if the user specifies a custom spark. deploy."}, {"q": "What updates or decisions were made in the discussion?", "a": "Created SPARK-1082 to track this."}]}}
{"project": "SPARK", "issue_id": "SPARK-1081", "title": "Annotate developer and experimental API's [Core]", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "labels": [], "created": "2014-02-11T23:21:45.000+0000", "updated": "2014-04-09T08:15:45.000+0000", "description": "We should annotate API's that are internal despite being java/scala private. An example is the internal listener interface.\n\nThe main issue is figuring out the nicest way we can do this in scala and decide how we document it in the scala docs.", "comments": ["For denoting experimental APIs in the  doc, I suggest we follow the convention set by TypeSafe.  They just added the following HTML to the top of the scala doc comment. This fragment inserts a small red \"Experimental\" badge at the top of the page. [Example|http://www.scala-lang.org/api/2.10.2/index.html#scala.reflect.api.Universe]. We can easily make this \"Unstable\" instead of \"Experimental\" based on how we decide to name things.\n\n{{<span class=\"badge badge-red\" style=\"float: right;\">EXPERIMENTAL</span>}}\n\nI think there are three ways we can attempt to add this information to the source code.\n\nh2. Annotation\nOne option is to create an annotation that marks an API as unstable.\n\nPros: Simple, other projects have done this.\nCons: As far as I can tell there is no way to get the compiler to enforce this.  We could maybe enforce it with scala macros in the future, but even I am not sure if that is a good idea.\n\nh2. Package Visibility\nAnother option is to mark all internal components as {{protected[spark]}}.  We would then specify that people who wish to extend spark's core functionality must place their code in {{org.apache.spark.contrib}}.  This is close to what projects like Shark do already to modify internal APIs.\n\nPros: Clearly denotes close coupling with spark.\nCons: As far as I know there is no java equivalent of package visibility.  We are also forcing people to put things in the spark namespace.\n\nh2. Implicits\nWe could create an implicit object that when in scope denotes close coupling with Spark's internal APIs.  Even nicer, when not present, we can have the compiler provide nice messages to the user explaining the problem.  See the following REPL session for an example:\n\n{code:java}\nscala> import scala.annotation.implicitNotFound\nimport scala.annotation.implicitNotFound\n\nscala> @implicitNotFound(\"You are attemping to use a SparkInternal API, which may be subject to change in future versions. If you are sure you want to couple your code closely with this internal API then add 'import org.apache.spark.SparkUnstableApis' to this file.\") trait SparkUnstableApi\ndefined trait SparkUnstableApi\n\nscala> class SparkInternalClass(p1: Int)(implicit u: SparkUnstableApi)\ndefined class SparkInternalClass\n\nscala> new SparkInternalClass(1)\n<console>:11: error: You are attemping to use a SparkInternal API, which may be subject to change in future versions. If you are sure you want to couple your code closely with this internal API then add 'import org.apache.spark.SparkUnstableApis' to this file.\n              new SparkInternalClass(1)\n              ^\n\nscala> implicit object SparkUnstableApis extends SparkUnstableApi\ndefined module SparkUnstableApis\n\nscala> new SparkInternalClass(1)\nres1: SparkInternalClass = SparkInternalClass@7aae3ed7\n{code}\n\nThis is pretty close to what scala itself is already doing with imports that are now required for using advanced language features.\n\nPros: Compiler enforced, nice error messages. Scala magic.\nCons: Scala magic.", "I don't think 2 is an option (package visibility) because it forces user level code to be in a specific package.\n\nFor 3, how does it work with Java?\n\n", "Well, this isn't really \"user level code\", it is code that modifying the spark runtime by touching internal APIs.  Requiring people to put such code in a package called contrib seems reasonable to me. If these are APIs that we intend user code to touch then I would argue we should provide stability and thus this is a non-issue.\n\nThe second parameter list is just sugar on top of a combined parameter list, so java programmers would explicitly have to pass in a value here.  Depending on what we do inside this could just be null, or could be an object of the correct type.  So I think the answer is it would be usable but ugly (kinda like java :P).", "What about just adding a new RDD implementation? That doesn't modify Spark core at all. \n", "There is now [an example of marking APIs as experimental|https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/SchemaRDD.scala] in the Spark SQL source code.\n\nThis is based on the way that the scala compiler marks their APIs as experimental."], "derived": {"summary": "We should annotate API's that are internal despite being java/scala private. An example is the internal listener interface.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Annotate developer and experimental API's [Core] - We should annotate API's that are internal despite being java/scala private. An example is the internal listener interface."}, {"q": "What updates or decisions were made in the discussion?", "a": "There is now [an example of marking APIs as experimental|https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/SchemaRDD.scala] in the Spark SQL source code.\n\nThis is based on the way that the scala compiler marks their APIs as experimental."}]}}
{"project": "SPARK", "issue_id": "SPARK-1082", "title": "Use Curator for ZK interaction in standalone cluster", "status": "Resolved", "priority": "Major", "reporter": "Aaron Davidson", "assignee": null, "labels": [], "created": "2014-02-11T23:22:46.000+0000", "updated": "2015-07-20T12:54:24.000+0000", "description": "ZooKeeper's API is a veritable minefield of pitfalls. Apache Curator (http://curator.apache.org/) provides a cleaner and higher level API that helps avoid these problems.\n\nWe should be able to replace almost all of [ZooKeeperLeaderElectionAgent|https://github.com/apache/incubator-spark/blob/e2c68642c64345434e2034082cf9b299491e9e9f/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperLeaderElectionAgent.scala] and [SparkZooKeeperSession|https://github.com/apache/incubator-spark/blob/e2c68642c64345434e2034082cf9b299491e9e9f/core/src/main/scala/org/apache/spark/deploy/master/SparkZooKeeperSession.scala] with Curator equivalents.", "comments": ["@Aaron Are you going to take this? Or, if not in hurry, I can take this one", "Feel free to take it on!", "@Aaron : https://github.com/apache/incubator-spark/pull/611\n", "I'm guessing this long since timed out"], "derived": {"summary": "ZooKeeper's API is a veritable minefield of pitfalls. Apache Curator (http://curator.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Use Curator for ZK interaction in standalone cluster - ZooKeeper's API is a veritable minefield of pitfalls. Apache Curator (http://curator."}, {"q": "What updates or decisions were made in the discussion?", "a": "I'm guessing this long since timed out"}]}}
{"project": "SPARK", "issue_id": "SPARK-1083", "title": "Build fail", "status": "Resolved", "priority": "Major", "reporter": "Jan Paw", "assignee": null, "labels": [], "created": "2014-02-12T03:18:09.000+0000", "updated": "2014-10-13T18:08:13.000+0000", "description": "Problem with building the latest version from github.\n\n{code:none}[info] Loading project definition from C:\\Users\\Jan\\Documents\\GitHub\\incubator-s\npark\\project\\project\n[debug]\n[debug] Initial source changes:\n[debug]         removed:Set()\n[debug]         added: Set()\n[debug]         modified: Set()\n[debug] Removed products: Set()\n[debug] Modified external sources: Set()\n[debug] Modified binary dependencies: Set()\n[debug] Initial directly invalidated sources: Set()\n[debug]\n[debug] Sources indirectly invalidated by:\n[debug]         product: Set()\n[debug]         binary dep: Set()\n[debug]         external source: Set()\n[debug] All initially invalidated sources: Set()\n[debug] Copy resource mappings:\n[debug]\njava.lang.RuntimeException: Nonzero exit code (128): git clone https://github.co\nm/chenkelmann/junit_xml_listener.git C:\\Users\\Jan\\.sbt\\0.13\\staging\\5f76b43a3aca\n87b5c013\\junit_xml_listener\n        at scala.sys.package$.error(package.scala:27)\n        at sbt.Resolvers$.run(Resolvers.scala:134)\n        at sbt.Resolvers$.run(Resolvers.scala:123)\n        at sbt.Resolvers$$anon$2.clone(Resolvers.scala:78)\n        at sbt.Resolvers$DistributedVCS$$anonfun$toResolver$1$$anonfun$apply$11$\n$anonfun$apply$5.apply$mcV$sp(Resolvers.scala:104)\n        at sbt.Resolvers$.creates(Resolvers.scala:141)\n        at sbt.Resolvers$DistributedVCS$$anonfun$toResolver$1$$anonfun$apply$11.\napply(Resolvers.scala:103)\n        at sbt.Resolvers$DistributedVCS$$anonfun$toResolver$1$$anonfun$apply$11.\napply(Resolvers.scala:103)\n        at sbt.BuildLoader$$anonfun$componentLoader$1$$anonfun$apply$3.apply(Bui\nldLoader.scala:90)\n        at sbt.BuildLoader$$anonfun$componentLoader$1$$anonfun$apply$3.apply(Bui\nldLoader.scala:89)\n        at scala.Option.map(Option.scala:145)\n        at sbt.BuildLoader$$anonfun$componentLoader$1.apply(BuildLoader.scala:89\n)\n        at sbt.BuildLoader$$anonfun$componentLoader$1.apply(BuildLoader.scala:85\n)\n        at sbt.MultiHandler.apply(BuildLoader.scala:16)\n        at sbt.BuildLoader.apply(BuildLoader.scala:142)\n        at sbt.Load$.loadAll(Load.scala:314)\n        at sbt.Load$.loadURI(Load.scala:266)\n        at sbt.Load$.load(Load.scala:262)\n        at sbt.Load$.load(Load.scala:253)\n        at sbt.Load$.apply(Load.scala:137)\n        at sbt.Load$.buildPluginDefinition(Load.scala:597)\n        at sbt.Load$.buildPlugins(Load.scala:563)\n        at sbt.Load$.plugins(Load.scala:551)\n        at sbt.Load$.loadUnit(Load.scala:412)\n        at sbt.Load$$anonfun$15$$anonfun$apply$11.apply(Load.scala:258)\n        at sbt.Load$$anonfun$15$$anonfun$apply$11.apply(Load.scala:258)\n        at sbt.BuildLoader$$anonfun$componentLoader$1$$anonfun$apply$4$$anonfun$\napply$5$$anonfun$apply$6.apply(BuildLoader.scala:93)\n        at sbt.BuildLoader$$anonfun$componentLoader$1$$anonfun$apply$4$$anonfun$\napply$5$$anonfun$apply$6.apply(BuildLoader.scala:92)\n        at sbt.BuildLoader.apply(BuildLoader.scala:143)\n        at sbt.Load$.loadAll(Load.scala:314)\n        at sbt.Load$.loadURI(Load.scala:266)\n        at sbt.Load$.load(Load.scala:262)\n        at sbt.Load$.load(Load.scala:253)\n        at sbt.Load$.apply(Load.scala:137)\n        at sbt.Load$.defaultLoad(Load.scala:40)\n        at sbt.BuiltinCommands$.doLoadProject(Main.scala:451)\n        at sbt.BuiltinCommands$$anonfun$loadProjectImpl$2.apply(Main.scala:445)\n        at sbt.BuiltinCommands$$anonfun$loadProjectImpl$2.apply(Main.scala:445)\n        at sbt.Command$$anonfun$applyEffect$1$$anonfun$apply$2.apply(Command.sca\nla:60)\n        at sbt.Command$$anonfun$applyEffect$1$$anonfun$apply$2.apply(Command.sca\nla:60)\n        at sbt.Command$$anonfun$applyEffect$2$$anonfun$apply$3.apply(Command.sca\nla:62)\n        at sbt.Command$$anonfun$applyEffect$2$$anonfun$apply$3.apply(Command.sca\nla:62)\n        at sbt.Command$.process(Command.scala:95)\n        at sbt.MainLoop$$anonfun$1$$anonfun$apply$1.apply(MainLoop.scala:100)\n        at sbt.MainLoop$$anonfun$1$$anonfun$apply$1.apply(MainLoop.scala:100)\n        at sbt.State$$anon$1.process(State.scala:179)\n        at sbt.MainLoop$$anonfun$1.apply(MainLoop.scala:100)\n        at sbt.MainLoop$$anonfun$1.apply(MainLoop.scala:100)\n        at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:18)\n        at sbt.MainLoop$.next(MainLoop.scala:100)\n        at sbt.MainLoop$.run(MainLoop.scala:93)\n        at sbt.MainLoop$$anonfun$runWithNewLog$1.apply(MainLoop.scala:71)\n        at sbt.MainLoop$$anonfun$runWithNewLog$1.apply(MainLoop.scala:66)\n        at sbt.Using.apply(Using.scala:25)\n        at sbt.MainLoop$.runWithNewLog(MainLoop.scala:66)\n        at sbt.MainLoop$.runAndClearLast(MainLoop.scala:49)\n        at sbt.MainLoop$.runLoggedLoop(MainLoop.scala:33)\n        at sbt.MainLoop$.runLogged(MainLoop.scala:25)\n        at sbt.StandardMain$.runManaged(Main.scala:57)\n        at sbt.xMain.run(Main.scala:29)\n        at xsbt.boot.Launch$$anonfun$run$1.apply(Launch.scala:57)\n        at xsbt.boot.Launch$.withContextLoader(Launch.scala:77)\n        at xsbt.boot.Launch$.run(Launch.scala:57)\n        at xsbt.boot.Launch$$anonfun$explicit$1.apply(Launch.scala:45)\n        at xsbt.boot.Launch$.launch(Launch.scala:65)\n        at xsbt.boot.Launch$.apply(Launch.scala:16)\n        at xsbt.boot.Boot$.runImpl(Boot.scala:32)\n        at xsbt.boot.Boot$.main(Boot.scala:21)\n        at xsbt.boot.Boot.main(Boot.scala)\n[error] Nonzero exit code (128): git clone https://github.com/chenkelmann/junit_\nxml_listener.git C:\\Users\\Jan\\.sbt\\0.13\\staging\\5f76b43a3aca87b5c013\\junit_xml_l\nistener\n[error] Use 'last' for the full log.\n[debug] > load-failed\n[debug] > last\n{code}\n\nsbt version\n{code:none}\n[info] This is sbt 0.13.1\n[info] No project is currently loaded\n[info] sbt, sbt plugins, and build definitions are using Scala 2.10.3\n{code}\n\nscala version\n{code:none}\nC:\\Users\\Jan\\Documents\\GitHub\\incubator-spark>scala -version\nScala code runner version 2.10.2 -- Copyright 2002-2013, LAMP/EPFL\n{code}\njava version\n{code:none}\nC:\\Users\\Jan\\Documents\\GitHub\\incubator-spark>java -version\njava version \"1.7.0_07\"\nJava(TM) SE Runtime Environment (build 1.7.0_07-b11)\nJava HotSpot(TM) 64-Bit Server VM (build 23.3-b01, mixed mode)\n{code}", "comments": ["This looks like a git error, and is ancient at this point. I presume that since we have evidence that Windows builds subsequently worked, this was either a local problem or fixed by something else."], "derived": {"summary": "Problem with building the latest version from github. {code:none}[info] Loading project definition from C:\\Users\\Jan\\Documents\\GitHub\\incubator-s\npark\\project\\project\n[debug]\n[debug] Initial source changes:\n[debug]         removed:Set()\n[debug]         added: Set()\n[debug]         modified: Set()\n[debug] Removed products: Set()\n[debug] Modified external sources: Set()\n[debug] Modified binary dependencies: Set()\n[debug] Initial directly invalidated sources: Set()\n[debug]\n[debug] Sources indirectly invalidated by:\n[debug]         product: Set()\n[debug]         binary dep: Set()\n[debug]         external source: Set()\n[debug] All initially invalidated sources: Set()\n[debug] Copy resource mappings:\n[debug]\njava.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Build fail - Problem with building the latest version from github. {code:none}[info] Loading project definition from C:\\Users\\Jan\\Documents\\GitHub\\incubator-s\npark\\project\\project\n[debug]\n[debug] Initial source changes:\n[debug]         removed:Set()\n[debug]         added: Set()\n[debug]         modified: Set()\n[debug] Removed products: Set()\n[debug] Modified external sources: Set()\n[debug] Modified binary dependencies: Set()\n[debug] Initial directly invalidated sources: Set()\n[debug]\n[debug] Sources indirectly invalidated by:\n[debug]         product: Set()\n[debug]         binary dep: Set()\n[debug]         external source: Set()\n[debug] All initially invalidated sources: Set()\n[debug] Copy resource mappings:\n[debug]\njava."}, {"q": "What updates or decisions were made in the discussion?", "a": "This looks like a git error, and is ancient at this point. I presume that since we have evidence that Windows builds subsequently worked, this was either a local problem or fixed by something else."}]}}
{"project": "SPARK", "issue_id": "SPARK-1084", "title": "Fix most build warnings", "status": "Resolved", "priority": "Minor", "reporter": "Sean R. Owen", "assignee": "Sean R. Owen", "labels": ["mvn", "sbt", "warning"], "created": "2014-02-12T05:25:54.000+0000", "updated": "2015-01-15T09:08:40.000+0000", "description": "I hope another boring tidy-up JIRA might be welcome. I'd like to fix most of the warnings that appear during build, so that developers don't become accustomed to them. The accompanying pull request contains a number of commits to quash most warnings observed through the mvn and sbt builds, although not all of them.\n\n\nFIXED!\n\n[WARNING] Parameter tasks is deprecated, use target instead\n\nJust a matter of updating <tasks> -> <target> in inline Ant scripts.\n\n\nWARNING: -p has been deprecated and will be reused for a different (but still very cool) purpose in ScalaTest 2.0. Please change all uses of -p to -R.\n\nGoes away with updating scalatest plugin -> 1.0-RC2\n\n\n[WARNING] Note: /Users/srowen/Documents/incubator-spark/core/src/test/scala/org/apache/spark/JavaAPISuite.java uses unchecked or unsafe operations.\n[WARNING] Note: Recompile with -Xlint:unchecked for details.\n\nMostly @SuppressWarnings(\"unchecked\") but needed a few more things to reveal the warning source: <fork>true</fork> (also needd for <maxmem>) and version 3.1 of the plugin. In a few cases some declaration changes were appropriate to avoid warnings.\n\n\n/Users/srowen/Documents/incubator-spark/core/src/main/scala/org/apache/spark/util/IndestructibleActorSystem.scala:25: warning: Could not find any member to link for \"akka.actor.ActorSystem\".\n/**\n^\n\nGetting several scaladoc errors like this and I'm not clear why it can't find the type -- outside its module? Remove the links as they're evidently not linking anyway?\n\n\n/Users/srowen/Documents/incubator-spark/repl/src/main/scala/org/apache/spark/repl/SparkIMain.scala:86: warning: Variable eval undefined in comment for class SparkIMain in class SparkIMain\n\n$ has to be escaped as \\$ in scaladoc, apparently\n\n\n[WARNING] 'dependencyManagement.dependencies.dependency.exclusions.exclusion.artifactId' for org.apache.hadoop:hadoop-yarn-client:jar with value '*' does not match a valid id pattern. @ org.apache.spark:spark-parent:1.0.0-incubating-SNAPSHOT, /Users/srowen/Documents/incubator-spark/pom.xml, line 494, column 25\n\nThis one might need review.\n\nThis is valid Maven syntax, but, Maven still warns on it. I wanted to see if we can do without it. \n\nThese are trying to exclude:\n- org.codehaus.jackson\n- org.sonatype.sisu.inject\n- org.xerial.snappy\n\norg.sonatype.sisu.inject doesn't actually seem to be a dependency anyway. org.xerial.snappy is used by dependencies but the version seems to match anyway (1.0.5).\n\norg.codehaus.jackson was intended to exclude 1.8.8, since Spark streaming wants 1.9.11 directly. But the exclusion is in the wrong place if so, since Spark depends straight on Avro, which is what brings in 1.8.8, still. (hadoop-client 1.0.4 includes Jackson 1.0.1, so that needs an exclusion, but the other Hadoop modules don't.)\n\nHBase depends on 1.8.8 but figured it was intentional to leave that as it would not collide with Spark streaming. (?)\n\n(I understand this varies by Hadoop version but confirmed this is all the same for 1.0.4, 0.23.7, 2.2.0.)\n\n\n\nNOT FIXED.\n\n[warn] /Users/srowen/Documents/incubator-spark/streaming/src/test/scala/org/apache/spark/streaming/InputStreamsSuite.scala:305: method connect in class IOManager is deprecated: use the new implementation in package akka.io instead\n[warn]   override def preStart = IOManager(context.system).connect(new InetSocketAddress(port))\n\nNot confident enough to fix this.\n\n\n[WARNING] there were 6 feature warning(s); re-run with -feature for details\n\nDon't know enough Scala to address these, yet.\n\n\n[WARNING] We have a duplicate org/yaml/snakeyaml/scanner/ScannerImpl$Chomping.class in /Users/srowen/.m2/repository/org/yaml/snakeyaml/1.6/snakeyaml-1.6.jar\n\nProbably addressable by being more careful about how binaries are packed though this appear to be ignorable; two identical copies of the class are colliding.\n\n\n[WARNING] Zinc server is not available at port 3030 - reverting to normal incremental compile\nand\n[WARNING] JAR will be empty - no content was marked for inclusion!\n\nApparently harmless warnings, but I don't know how to disable them.", "comments": ["There are two PRs associated with this -- per Aaron, first there's everything but dependency changes:\n\nhttps://github.com/apache/incubator-spark/pull/637\n\nand then dependency-related changes\n\nhttps://github.com/apache/incubator-spark/pull/650"], "derived": {"summary": "I hope another boring tidy-up JIRA might be welcome. I'd like to fix most of the warnings that appear during build, so that developers don't become accustomed to them.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Fix most build warnings - I hope another boring tidy-up JIRA might be welcome. I'd like to fix most of the warnings that appear during build, so that developers don't become accustomed to them."}, {"q": "What updates or decisions were made in the discussion?", "a": "There are two PRs associated with this -- per Aaron, first there's everything but dependency changes:\n\nhttps://github.com/apache/incubator-spark/pull/637\n\nand then dependency-related changes\n\nhttps://github.com/apache/incubator-spark/pull/650"}]}}
{"project": "SPARK", "issue_id": "SPARK-1085", "title": "Fix Jenkins pull request builder for branch-0.9 (scalastyle command not found)", "status": "Resolved", "priority": "Major", "reporter": "Reynold Xin", "assignee": "Reynold Xin", "labels": [], "created": "2014-02-12T13:55:11.000+0000", "updated": "2014-02-12T20:50:52.000+0000", "description": null, "comments": ["Any idea what needs to be done to make this work? E.g. will `yum install\nscalastyle` on all Jenkins worker nodes fix it (or even work)?\nOn Feb 12, 2014 1:57 PM, \"Reynold Xin (JIRA)\" <\n\n", "The problem is the older versions of Spark doesn't have the scalastyle configuration in SBT. I submitted a PR to fix that. \n\nhttps://github.com/apache/incubator-spark/pull/590"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Fix Jenkins pull request builder for branch-0.9 (scalastyle command not found)"}, {"q": "What updates or decisions were made in the discussion?", "a": "The problem is the older versions of Spark doesn't have the scalastyle configuration in SBT. I submitted a PR to fix that. \n\nhttps://github.com/apache/incubator-spark/pull/590"}]}}
{"project": "SPARK", "issue_id": "SPARK-1086", "title": "Some corner case during HA master switching?", "status": "Resolved", "priority": "Major", "reporter": "Raymond Valencia", "assignee": null, "labels": [], "created": "2014-02-12T18:18:54.000+0000", "updated": "2015-10-19T23:07:03.000+0000", "description": "In spark standalone HA mode.\n\nIt seems to me that when current master is down, and the new master is not elected yet, at this time if the worker is sending executor update or driver update message. this message will be lost. Though executor status is rebuild after new master is recovered, thus the status might be alright. While the driver might be relaunched even it actually finished successfully, this might be a problem? And then the message send to AppClientListener might be lost and never been regenerated thus lead to some problem?", "comments": ["I'm trying to figure out if this is still a problem, is a question, is something with more actionable info? it's over a year old so I am tempted to close this otherwise.", "User 'JihongMA' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/9172"], "derived": {"summary": "In spark standalone HA mode. It seems to me that when current master is down, and the new master is not elected yet, at this time if the worker is sending executor update or driver update message.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Some corner case during HA master switching? - In spark standalone HA mode. It seems to me that when current master is down, and the new master is not elected yet, at this time if the worker is sending executor update or driver update message."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'JihongMA' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/9172"}]}}
{"project": "SPARK", "issue_id": "SPARK-1087", "title": "Separate file for traceback and callsite related functions", "status": "Resolved", "priority": "Major", "reporter": "Jyotiska NK", "assignee": null, "labels": [], "created": "2014-02-12T19:53:28.000+0000", "updated": "2014-09-16T02:28:30.000+0000", "description": "Right now, _extract_concise_traceback() is written inside rdd.py which provides the callsite information. But for [SPARK-972](https://spark-project.atlassian.net/browse/SPARK-972) in PR #581, we used the function from context.py. Also some issues were faced regarding the return string format. \n\nIt would be a good idea to move the the traceback function from rdd and create a separate file for future developments. ", "comments": ["Can one of the admins assign me to this issue and change the status to \"In Progress\"? I will submit a PR after PR #581 gets merged.", "[~jyotiska] PR 581 was merged (though it looked fairly trivial). is this still relevant?", "We initially thought this would be a good feature to add going forward. But after the PR was merged, it was abandoned. Also, PR 581 was created in the incubator github repo. In the new one, it was [PR #34|https://github.com/apache/spark/pull/34]. If it is a relevant feature, I can submit a PR for this.", "[~jyotiska] please do!", "User 'staple' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/2385", "Issue resolved by pull request 2385\n[https://github.com/apache/spark/pull/2385]"], "derived": {"summary": "Right now, _extract_concise_traceback() is written inside rdd. py which provides the callsite information.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Separate file for traceback and callsite related functions - Right now, _extract_concise_traceback() is written inside rdd. py which provides the callsite information."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 2385\n[https://github.com/apache/spark/pull/2385]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1088", "title": "Create a script for running tests", "status": "Resolved", "priority": "Major", "reporter": "Reynold Xin", "assignee": "Reynold Xin", "labels": [], "created": "2014-02-12T20:51:40.000+0000", "updated": "2014-03-08T16:13:17.000+0000", "description": "So we can run that in Jenkins and have version specific testing. ", "comments": ["PR submitted: https://github.com/apache/incubator-spark/pull/592"], "derived": {"summary": "So we can run that in Jenkins and have version specific testing.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Create a script for running tests - So we can run that in Jenkins and have version specific testing."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR submitted: https://github.com/apache/incubator-spark/pull/592"}]}}
{"project": "SPARK", "issue_id": "SPARK-1089", "title": "ADD_JARS regression in Spark 0.9.0", "status": "Resolved", "priority": "Blocker", "reporter": "Andrew Ash", "assignee": "Nan Zhu", "labels": [], "created": "2014-02-13T14:25:55.000+0000", "updated": "2014-07-09T18:12:36.000+0000", "description": "Using the ADD_JARS environment variable with spark-shell used to add the jar to both the shell and the various workers.  Now it only adds to the workers and importing a custom class in the shell is broken.\n\nThe workaround is to add custom jars to both ADD_JARS and SPARK_CLASSPATH.\n\nWe should fix ADD_JARS so it works properly again.\n\nSee various threads on the user list:\nhttps://mail-archives.apache.org/mod_mbox/incubator-spark-user/201402.mbox/%3CCAJbo4neMLiTrnm1XbyqomWmp0m+EUcg4yE-txuRGSVKOb5KLeA@mail.gmail.com%3E\n(another one that doesn't appear in the archives yet titled \"ADD_JARS not working on 0.9\")", "comments": ["I'm interested in fixing this\n\nCan anyone assign it to me?\n\nThank you!", "made a PR https://github.com/apache/incubator-spark/pull/614, the analysis on the reason of this bug is over there", "as a temporary work around, you don't need to set ADD_JARS and SPARK_CLASSPATH at the same time\n\njust SPARK_CLASSPATH is enough ", "So going forward, is the correct procedure for adding external jars to set {{SPARK_CLASSPATH}}? Is there a doc somewhere that details this process? And what is the difference between setting these environment variables and calling {{sc.addJar()}} from within the shell?"], "derived": {"summary": "Using the ADD_JARS environment variable with spark-shell used to add the jar to both the shell and the various workers. Now it only adds to the workers and importing a custom class in the shell is broken.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "ADD_JARS regression in Spark 0.9.0 - Using the ADD_JARS environment variable with spark-shell used to add the jar to both the shell and the various workers. Now it only adds to the workers and importing a custom class in the shell is broken."}, {"q": "What updates or decisions were made in the discussion?", "a": "So going forward, is the correct procedure for adding external jars to set {{SPARK_CLASSPATH}}? Is there a doc somewhere that details this process? And what is the difference between setting these environment variables and calling {{sc.addJar()}} from within the shell?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1090", "title": "spark-shell should print help information about parameters and should allow user to configure exe memory", "status": "Resolved", "priority": "Major", "reporter": "Nan Zhu", "assignee": "Nan Zhu", "labels": [], "created": "2014-02-13T18:03:03.000+0000", "updated": "2014-02-18T13:16:51.000+0000", "description": "spark-shell should print help information about parameters and should allow user to configure exe memory\n\nthere is no document about hot to set --cores/-c in spark-shell\n\nand also \n\nusers should be able to set executor memory through command line options\n\n", "comments": ["made a PR https://github.com/apache/incubator-spark/pull/599", "solved, can any administrator help to close this issue?\n\nThank you"], "derived": {"summary": "spark-shell should print help information about parameters and should allow user to configure exe memory\n\nthere is no document about hot to set --cores/-c in spark-shell\n\nand also \n\nusers should be able to set executor memory through command line options.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "spark-shell should print help information about parameters and should allow user to configure exe memory - spark-shell should print help information about parameters and should allow user to configure exe memory\n\nthere is no document about hot to set --cores/-c in spark-shell\n\nand also \n\nusers should be able to set executor memory through command line options."}, {"q": "What updates or decisions were made in the discussion?", "a": "solved, can any administrator help to close this issue?\n\nThank you"}]}}
{"project": "SPARK", "issue_id": "SPARK-1091", "title": "Cloudpickle does not work correctly for some methods that use a splat", "status": "Resolved", "priority": "Major", "reporter": "Bouke van der Bijl", "assignee": null, "labels": ["cloudpickle", "pyspark"], "created": "2014-02-14T10:49:50.000+0000", "updated": "2014-02-14T14:07:39.000+0000", "description": "Cloudpickle seems to have some issues with some methods that use splats\n\n{code}\nfrom operator import itemgetter\n\ntest = {\"one\": 1, \"two\": 2}\n\ntest_func = itemgetter(\"one\", \"two\")\n\nprint test_func(test) #=> (1, 2)\n\nfrom pyspark import cloudpickle\nimport pickle\n\nserialized = cloudpickle.dumps(test_func)\ndeserialized = pickle.loads(serialized)\nprint deserialized(test) #=> KeyError: (\"one\", \"two\")\n{code}\n\nAs you can see, instead of getting \"one\" and \"two\" seperately and creating one tuple, it instead looks for the key (\"one\", \"two\") in the dict", "comments": ["This looks like a duplicate of SPARK-791; I've got some notes in that issue, so let's continue any discussions over there."], "derived": {"summary": "Cloudpickle seems to have some issues with some methods that use splats\n\n{code}\nfrom operator import itemgetter\n\ntest = {\"one\": 1, \"two\": 2}\n\ntest_func = itemgetter(\"one\", \"two\")\n\nprint test_func(test) #=> (1, 2)\n\nfrom pyspark import cloudpickle\nimport pickle\n\nserialized = cloudpickle. dumps(test_func)\ndeserialized = pickle.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Cloudpickle does not work correctly for some methods that use a splat - Cloudpickle seems to have some issues with some methods that use splats\n\n{code}\nfrom operator import itemgetter\n\ntest = {\"one\": 1, \"two\": 2}\n\ntest_func = itemgetter(\"one\", \"two\")\n\nprint test_func(test) #=> (1, 2)\n\nfrom pyspark import cloudpickle\nimport pickle\n\nserialized = cloudpickle. dumps(test_func)\ndeserialized = pickle."}, {"q": "What updates or decisions were made in the discussion?", "a": "This looks like a duplicate of SPARK-791; I've got some notes in that issue, so let's continue any discussions over there."}]}}
{"project": "SPARK", "issue_id": "SPARK-1092", "title": "SparkContext should not read SPARK_MEM to set memory usage of executors", "status": "Resolved", "priority": "Major", "reporter": "Nan Zhu", "assignee": null, "labels": [], "created": "2014-02-14T15:32:37.000+0000", "updated": "2014-02-14T15:57:50.000+0000", "description": "Currently, users will usually set SPARK_MEM to control the memory usage of driver programs, (in spark-class)\n\n 91 JAVA_OPTS=\"$OUR_JAVA_OPTS\"\n 92 JAVA_OPTS=\"$JAVA_OPTS -Djava.library.path=$SPARK_LIBRARY_PATH\"\n 93 JAVA_OPTS=\"$JAVA_OPTS -Xms$SPARK_MEM -Xmx$SPARK_MEM\"\n\nif they didn't set spark.executor.memory, the value in this environment variable will also affect the memory usage of executors, because the following lines in SparkContext\n\n  private[spark] val executorMemory = conf.getOption(\"spark.executor.memory\")\n    .orElse(Option(System.getenv(\"SPARK_MEM\")))\n    .map(Utils.memoryStringToMb)\n    .getOrElse(512)\n\nalso \n\nsince SPARK_MEM has been (proposed to) deprecated in SPARK-929 (https://spark-project.atlassian.net/browse/SPARK-929)  and the corresponding PR (https://github.com/apache/incubator-spark/pull/104)\n\nwe should remove this line\n", "comments": ["Would regress existing behavior."], "derived": {"summary": "Currently, users will usually set SPARK_MEM to control the memory usage of driver programs, (in spark-class)\n\n 91 JAVA_OPTS=\"$OUR_JAVA_OPTS\"\n 92 JAVA_OPTS=\"$JAVA_OPTS -Djava. library.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "SparkContext should not read SPARK_MEM to set memory usage of executors - Currently, users will usually set SPARK_MEM to control the memory usage of driver programs, (in spark-class)\n\n 91 JAVA_OPTS=\"$OUR_JAVA_OPTS\"\n 92 JAVA_OPTS=\"$JAVA_OPTS -Djava. library."}, {"q": "What updates or decisions were made in the discussion?", "a": "Would regress existing behavior."}]}}
{"project": "SPARK", "issue_id": "SPARK-1093", "title": "API Stability in Spark 1.X (Umbrella)", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": null, "labels": [], "created": "2014-02-14T15:44:46.000+0000", "updated": "2014-05-19T19:08:12.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "API Stability in Spark 1.X (Umbrella)"}]}}
{"project": "SPARK", "issue_id": "SPARK-1094", "title": "Enforce Binary Compatibility in Spark Build", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Prashant Sharma", "labels": [], "created": "2014-02-14T15:46:00.000+0000", "updated": "2020-02-07T17:26:43.000+0000", "description": null, "comments": ["I think we can only enforce it once we have released 1.0 ? ", "We should start enforcing it now and we can add exceptions if necessary for 1.0. For instance, we may want to re-factor some code for Spark 1.0 based on what we learn from enforcing this with new pull requests.", "Then I guess we would need to publish SNAPSHOT artifacts somewhere ? As it is required by MiMa. \n\nThere should also be a way to ignore Mima for a particular PR ?", "I mean this: From now on, we should compare new pull requests against Spark 0.9.0 release. If they break binary compatibility then they should have to add an exception to Mima in the pull request so that the build is happy. This way we can practice adding exceptions.\n\nBefore we do this we need to (a) filter out false positives and add relevant exceptions in the Mima configuration. (b) if we have any true positives, e.g. we have actually broken this in master already vs 0.9.0, we should figure out what it was and decide to either add an exception or revert the change.", "Hmm... one thing that might come up is that we might add exceptions we want to later remove... or see if somehow the exceptions can be scoped only for specific releases. We can also just document once we add the exception that it should be removed once 1.0 is released. I'm not sure there are going to be many of these.", "But there are a lot \"b)\" reported at the moment. Will it be okay to add all of them as exceptions ?", "Hm - could you show what they are? Are they legitimate API changes or just things that aren't considered public API's. I think many of the things are actually API's we will eventually mark semi-private and mark as exceptions in the build. Could you give examples of changes you think are legitimate API changes we've made in the last few weeks?", "Here is a quick attempt https://gist.github.com/ScrapCodes/fde092bd2234d75efc37. There are things listed in (b) which may not actually belong there in a very general case. But they may, incase someone writes a library on top of spark.", "From whatever I have understood, I guess they all belong to the excludes ! \n\nHaven't verified mllib though.", "The main issue is that this gives a lot of false positives due to classes which are private\\[X\\]. I dug into MIMA a bit more and I because of the way it parses class files it may be diffficult to implement a rule in MIMA that ignores these. The issue is that these package private declaration end up inside of a special bytecode annotation called ScalaSig that is not easy to parse given the current architecture of MIMA which relies on a forked, fairly old, version of the Scala compiler's class parser.\n\nThere is a workaround though - we can create a standalone program that goes through all of the Spark classes and uses runtime reflection to determine their visibility. The program can output a flat file with a list of excludes, and then we can set-up the MIMA build to read these excludes from the file.\n\nHere is an example from the Spark Shell:\n\n{code}\nscala> import scala.reflect.runtime.universe.runtimeMirror\nimport scala.reflect.runtime.universe.runtimeMirror\n\nscala> val mirror = runtimeMirror(getClass.getClassLoader)\nmirror: reflect.runtime.universe.Mirror = JavaMirror with org.apache.spark.repl.SparkIMain$TranslatingClassLoader@3c6cf99b of type class org.apache.spark.repl.SparkIMain$TranslatingClassLoader with classpath [/tmp/spark-1d5fd4d3-97b5-4dde-915e-450ec53a6ee0] and parent being scala.tools.nsc.util.ScalaClassLoader$URLClassLoader@16774e1b of type class scala.tools.nsc.util.ScalaClassLoader$URLClassLoader with classpath [file:/usr/lib/jvm/jdk1.7.0_25/jre/lib/resources.jar,file:/usr/lib/jvm/jdk1....\nscala> mirror.staticClass(\"org.apache.spark.rdd.RDD\").privateWithin\nres15: reflect.runtime.universe.Symbol = <none>\nscala> mirror.staticClass(\"org.apache.spark.storage.BlockInfo\").privateWithin\nres16: reflect.runtime.universe.Symbol = package storage\n{code}\n\n", "User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/20"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Enforce Binary Compatibility in Spark Build"}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/20"}]}}
{"project": "SPARK", "issue_id": "SPARK-1095", "title": "Ensure all public methods return explicit types", "status": "Closed", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Reynold Xin", "labels": [], "created": "2014-02-14T15:53:10.000+0000", "updated": "2015-04-03T06:27:02.000+0000", "description": "This talk explains some of the challenges around typing for binary compatibility:\n\nhttp://www.slideshare.net/mircodotta/managing-binary-compatibility-in-scala\n\nFor public methods we should always declare the type. We've had this as a guideline in the past but we need to make sure we obey it in all public interfaces. Also, we should return the most general type possible.", "comments": ["I accidentally closed this but it's not merged yet."], "derived": {"summary": "This talk explains some of the challenges around typing for binary compatibility:\n\nhttp://www. slideshare.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Ensure all public methods return explicit types - This talk explains some of the challenges around typing for binary compatibility:\n\nhttp://www. slideshare."}, {"q": "What updates or decisions were made in the discussion?", "a": "I accidentally closed this but it's not merged yet."}]}}
{"project": "SPARK", "issue_id": "SPARK-1096", "title": "Add Scalastyle Plug-in For Spaces after Comments", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Prashant Sharma", "labels": [], "created": "2014-02-14T17:51:40.000+0000", "updated": "2020-02-07T17:26:39.000+0000", "description": "After line length (which is now checked) this is the number one thing people forget in pull requests. We should see whether a custom scalastyle rule can be used to check this. In general I'm guessing we'll want to add several custom rules, so this is a good time to look into how it works. The scala AST seems to have comments as recognized token so it might work, but not sure if they are gobbled before the plug-in gets to it. This is something to look into.\n\nI find this particular style issue so pervasive and annoying that I'd even settle for a bash command inside of our test script that greps throughout the codebase. Scalastyle is a much better way to do this though - if we can make it work!\n\n", "comments": ["I hope you mean this\n{noformat}\n// correct\n{noformat}\nand \n{noformat}\n//wrong \n{noformat}", "Yes - that's the one!", "I was adding a few more test cases.\n\n{noformat}\npackage foobar\n\nobject Foobar {\n  //Incorrect\n  // correct comment//not wrong//check\n  val a = 10//Incorrect\n  val b = 100 //Incorrect\n  val c = 1// Correct\n  val d = 2 // Correct\n  val e = 3\n}\"\"\"\n\n{noformat}\nDo you think they make sense ? Should we have similar thing for /* and /** ?", "Absolutely - let's make this cover as many cases as possible.", "Looks like they don't have this feature of letting us plugin checks yet, scalastyle/scalastyle#25", "Hey [~prashant] if you have the time or inclination, it would be great to see if we could add that feature to scalastyle, then we can ask them to make another release. That would open the doors for us to write a bunch of our own style rules, which would be awesome and allow us to review patches more easily.", "User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/124"], "derived": {"summary": "After line length (which is now checked) this is the number one thing people forget in pull requests. We should see whether a custom scalastyle rule can be used to check this.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add Scalastyle Plug-in For Spaces after Comments - After line length (which is now checked) this is the number one thing people forget in pull requests. We should see whether a custom scalastyle rule can be used to check this."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/124"}]}}
{"project": "SPARK", "issue_id": "SPARK-1097", "title": "ConcurrentModificationException", "status": "Resolved", "priority": "Major", "reporter": "Fabrizio Milo", "assignee": "Raymond Liu", "labels": [], "created": "2014-02-16T14:31:49.000+0000", "updated": "2014-10-06T23:06:02.000+0000", "description": "{noformat}\n14/02/16 08:18:45 WARN TaskSetManager: Loss was due to java.util.ConcurrentModificationException\njava.util.ConcurrentModificationException\n\tat java.util.HashMap$HashIterator.nextEntry(HashMap.java:926)\n\tat java.util.HashMap$KeyIterator.next(HashMap.java:960)\n\tat java.util.AbstractCollection.addAll(AbstractCollection.java:341)\n\tat java.util.HashSet.<init>(HashSet.java:117)\n\tat org.apache.hadoop.conf.Configuration.<init>(Configuration.java:554)\n\tat org.apache.hadoop.mapred.JobConf.<init>(JobConf.java:439)\n\tat org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:110)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:154)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:149)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:64)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:232)\n\tat org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:232)\n\tat org.apache.spark.rdd.UnionPartition.iterator(UnionRDD.scala:32)\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:72)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:232)\n\tat org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:232)\n\tat org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:232)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:161)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:53)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)\n\tat org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\n{noformat}", "comments": ["What version of Hadoop is this? ", "Attached is a patch for this issue. Verified with mvn test/compile/install. The fix is to move HashSet initialization to the synchronized block right above it.", "Standard procedure is to provide a pull request. But, you're suggesting a fix to Hadoop code, which belong in your Hadoop JIRA, yes. This can't fix the problem from the Spark end.\n\n(Eyeballing the Hadoop 2.2.0 code, I tend to agree with your patch. Mutation of finalParameters appears consistently synchronized, which means the constructor reading it to copy has to lock on the other Configuration or else exactly this can happen.)\n\nIs a workaround in Spark to synchronize on the Configuration object when calling this constructor? (I smell a deadlock risk.)\nOr something crazy like trying the constructor until it doesn't fail this way?", "The problem should be solved at the root. This issue can be exposed by other systems as well, in addition to Spark. The fix is straightforward and harmless. I can initiate a pull request as well.", "I agree, but, we can't patch Hadoop from here. I'm just saying that for purposes of a SPARK-* issue, in anything like the short-term, one would have to propose a workaround within Spark code, if anything. While also trying to fix it at the root, separately, in a HADOOP-* issue. (Spark does not have a copy of Hadoop, yesterday's April Fools joke aside.)", "We can consider putting a workaround in Spark as well (for non-CDH users that may be running an older version of Hadoop and not updating it periodically). For now, this fix needs to go upstream, so we can backport it to CDH. The CDH-Spark bundle would then inherit this fix. The same issue has been noted in Hadoop-10456 as well. ", "Patch submitted against Hadoop-10456.", "A patch by Nishkam on HADOOP-10456 has been already reviewed and will be committed in a few days against hadoop's trunk.", "FYI still seeing this on spark 1.0, Hadoop 2.4\n\n{code:java}\njava.util.ConcurrentModificationException (java.util.ConcurrentModificationException)\njava.util.HashMap$HashIterator.nextEntry(HashMap.java:922)\njava.util.HashMap$KeyIterator.next(HashMap.java:956)\njava.util.AbstractCollection.addAll(AbstractCollection.java:341)\njava.util.HashSet.<init>(HashSet.java:117)\norg.apache.hadoop.conf.Configuration.<init>(Configuration.java:671)\ncom.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:98)\norg.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2402)\norg.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)\norg.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2436)\norg.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2418)\norg.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\norg.apache.hadoop.fs.Path.getFileSystem(Path.java:296)\norg.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:107)\norg.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\norg.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:190)\norg.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:181)\norg.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:93)\norg.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)\norg.apache.spark.rdd.RDD.iterator(RDD.scala:229)\norg.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)\norg.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)\norg.apache.spark.rdd.RDD.iterator(RDD.scala:229)\norg.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply$mcV$sp(PythonRDD.scala:200)\norg.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:175)\norg.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:175)\norg.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1160)\norg.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:174)\n{code}", "[~jblomo], thank you for reporting. This issue is fixed in next minor Hadoop release - 2.4.1. Note that 2.4.0 doesn't include the fix.", "Have initiated a PR for a workaround in Spark as well (for developers using < 2.4.1):\nhttps://github.com/apache/spark/pull/1000", "Summit another PR to actually(hopes) workaround this problem\nhttps://github.com/apache/spark/pull/1273\n", "A follow up to this fix is in Spark 1.0.2:\nhttps://github.com/apache/spark/pull/1409/files"], "derived": {"summary": "{noformat}\n14/02/16 08:18:45 WARN TaskSetManager: Loss was due to java. util.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "ConcurrentModificationException - {noformat}\n14/02/16 08:18:45 WARN TaskSetManager: Loss was due to java. util."}, {"q": "What updates or decisions were made in the discussion?", "a": "A follow up to this fix is in Spark 1.0.2:\nhttps://github.com/apache/spark/pull/1409/files"}]}}
{"project": "SPARK", "issue_id": "SPARK-1098", "title": "Cleanup and document ClassTag stuff in Java API", "status": "Resolved", "priority": "Trivial", "reporter": "Aaron Davidson", "assignee": "Aaron Davidson", "labels": [], "created": "2014-02-16T16:21:00.000+0000", "updated": "2014-02-21T11:00:54.000+0000", "description": "There is significant repeated and poorly styled code related to ClassTags in the Java API. Additionally, our usage of ClassTags is very unusual, and deserves some form of documentation.", "comments": [], "derived": {"summary": "There is significant repeated and poorly styled code related to ClassTags in the Java API. Additionally, our usage of ClassTags is very unusual, and deserves some form of documentation.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Cleanup and document ClassTag stuff in Java API - There is significant repeated and poorly styled code related to ClassTags in the Java API. Additionally, our usage of ClassTags is very unusual, and deserves some form of documentation."}]}}
{"project": "SPARK", "issue_id": "SPARK-1099", "title": "Allow inferring number of cores with local[*]", "status": "Resolved", "priority": "Minor", "reporter": "Aaron Davidson", "assignee": "Aaron Davidson", "labels": [], "created": "2014-02-16T19:25:54.000+0000", "updated": "2016-04-30T03:32:03.000+0000", "description": "It seems reasonable that the default number of cores used by spark's local mode (when no value is specified) is drawn from the spark.cores.max configuration parameter (which, conveniently, is now settable as a command-line option in spark-shell).\n\nFor the sake of consistency, it's probable that this change would also entail making the default number of cores when spark.cores.max is NOT specified to be as many logical cores are on the machine (which is what standalone mode does). This too seems reasonable, as Spark is inherently a distributed system and I think it's expected that it should use multiple cores by default. However, it is a behavioral change, and thus requires caution.", "comments": ["@Araon I can work on this. Can anyone put as assignee. Thanks!", "@Araon, I wonder that your goal is to add a new config property named \"spark.max.cores\" , not existed \"spark.cores.max\" for each applicatoin. And in spark local cluster mode, one can specify the number of workers , and the n workers and the master is running on the same jvm . And the cores specified by your \"spark.max.cores\" is the total number of cores for the n workers or cores for each worker? I think its the total num of cores for n workes. And how we should pass the coresPerWorker to Worker.startSystemAndActor with our \"spark.max.cores\"? ", "Hey, [~qqsun8819] -- I did mean spark.cores.max, that was a typo in my head :)\n\nI believe some of your questions stem from looking at LocalSparkCluster, which is actually used only for the \"MASTER=local-cluster\\[2,2,512\\]\" syntax. When using standard local mode, we create a LocalBackend, which simply takes the number of cores as an argument. There is no worker paradigm in play, just one big Executor with a bunch of cores.\n\n(Sorry for delayed response, was travelling!)", "@Araon, PR is ready, and here I'll state what I do . This is really a behavioral change, so I want you to know what my patch is suitable . And I welcome any of your advice if you have a different opinion:\n1 I change the \"MASTER=local\" pattern of create LocalBackEnd . In the past, we passed 1 core to it . now it use a default cores\nThe reason here is that when someone use spark-shell to start local mode , Repl will use this \"MASTER=local\" pattern as default.\nSo if one also specify cores in the spark-shell command line, it will all go  in here. So here pass 1 core is not suitalbe reponding to our change here.\n2 In the LocalBackEnd , the \"totalCores\" variable are fetched following a different rule(in the past it just take in a userd passed cores, like 1 in \"MASTER=local\" pattern, 2 in \"MASTER=local[2]\" pattern\"\nrules: \na The second argument of LocalBackEnd 's constructor indicating cores have a default value which is Int.MaxValue. If user didn't pass it , its first default value is Int.MaxValue\nb In getMaxCores, we first compare the former value to Int.MaxValue. if it's not equal, we think that user has passed their desired value, so just use it \nc. If b is not satified, we then get cores from spark.cores.max, and we get real logical cores from Runtime. And if cores specified by spark.cores.max is bigger than logical cores, we use logical cores, otherwise we use spark.cores.max\n\n3 In SparkContextSchedulerCreationSuite 's test(\"local\") case, assertion is modified from 1 to logical cores, because \"MASTER=local\" pattern use default vaules.\n", "PR in  https://github.com/apache/spark/pull/110\n\n", "Reverted change due to build failure, reopened at https://github.com/apache/spark/pull/182", "User 'qqsun8819' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/110"], "derived": {"summary": "It seems reasonable that the default number of cores used by spark's local mode (when no value is specified) is drawn from the spark. cores.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Allow inferring number of cores with local[*] - It seems reasonable that the default number of cores used by spark's local mode (when no value is specified) is drawn from the spark. cores."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'qqsun8819' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/110"}]}}
{"project": "SPARK", "issue_id": "SPARK-1100", "title": "saveAsTextFile shouldn't clobber by default", "status": "Resolved", "priority": "Major", "reporter": "Diana Carroll", "assignee": "Patrick Wendell", "labels": [], "created": "2014-02-18T09:45:05.000+0000", "updated": "2014-04-29T18:50:44.000+0000", "description": "If I call rdd.saveAsTextFile with an existing directory, it will cheerfully and silently overwrite the files in there.  This is bad enough if it means I've accidentally blown away the results of a job that might have taken minutes or hours to run.  But it's worse if the second job happens to have fewer partitions than the first...in that case, my output directory now contains some \"part\" files from the earlier job, and some \"part\" files from the later job.  The only way to know the difference is timestamp.\n\nI wonder if Spark's saveAsTextFile shouldn't work more like Hadoop MapReduce which insists that the output directory not exist before the job starts.  Similarly HDFS won't override files by default.  Perhaps there could be an optional argument for saveAsTextFile that indicates if it should delete the existing directory before starting.  (I can't see any time I'd want to allow writing to an existing directory with data already in it.  Would the mix of output from different tasks ever be desirable?)", "comments": ["indeed, overwriting silently is risky\n\nbut some users have been used to this, e.g. their jobs are started periodically to update the data in a directory...\n\nI think we'd better make it as an option, if spark.overwrite = true (by default, current behaviour), then it is directly overwrite the directory, if spark.overwrite = false, it will reject the writing\n\nI would like to work on it, any admin can assign it to me?", "I added you [~CodingCat] to the developer list so you can assign tickets to yourself in the future.", "many thanks @Reynold Xin", "How about making it an option on the function call, that way it can be set by the developer for each individual write, rather than for a whole application or site?   Also maybe a warning or at least info message indicating that files are being overwritten?\n\nI still think it's questionable to overwrite individual files in a directory rather than clear out the directory, even if, as you say, I want a job to \"periodically update the data\".  If my first job's data has three partitions, it will save three files: part-00000, part-00001 and part-00002.  If my second job has two partitions it will overwrite part-00000 and part-00001 from the first job, but leave the third file part-00003 hanging around.  If I look in the directory, it's not easy to tell that my output data from the second job does NOT have three partitions/files, it only has two.  If I build a report from all three files, or have a script that assembles them into a single file, two thirds of the data in the report would correctly be from the most recent job, and one third would incorrectly be from the older job.  I'm having trouble imagining a use case where this would be desired behavior...\n\nThanks for looking at this!\n ", "Good point, so if the option is set to true, we will clear out the directory first, if false, we will reject the writing\n\nbut I'm still concerning about that since Spark has been running for years, there would be someone is occasionally utilizing  this \"feature\"....\n\nanyway, I would like to make a PR first (maybe in this week) and then revise it according to the feedbacks from others\n", "made a PR https://github.com/apache/incubator-spark/pull/626", "Hi, Diana Carroll, \n\nCan you confirm the left over old file problem happening on HDFS? mridulm said this should not happen\n\nI reproduced it in Local file system and S3, but I don't have a HDFS environment, \n\nSee our discussion here:https://github.com/apache/incubator-spark/pull/626\n\nBest,\n\nNan\n", "Ya I don't think we want Spark ever deleting HDFS directories. I'd propose doing something like Hadoop MR, except maybe just check if the directory already has output files in it rather than if it exists. This would avoid regressing behavior for users that, e.g., output data into a directory that contains other stuff."], "derived": {"summary": "If I call rdd. saveAsTextFile with an existing directory, it will cheerfully and silently overwrite the files in there.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "saveAsTextFile shouldn't clobber by default - If I call rdd. saveAsTextFile with an existing directory, it will cheerfully and silently overwrite the files in there."}, {"q": "What updates or decisions were made in the discussion?", "a": "Ya I don't think we want Spark ever deleting HDFS directories. I'd propose doing something like Hadoop MR, except maybe just check if the directory already has output files in it rather than if it exists. This would avoid regressing behavior for users that, e.g., output data into a directory that contains other stuff."}]}}
{"project": "SPARK", "issue_id": "SPARK-1101", "title": "Umbrella for hardening Spark on YARN", "status": "Resolved", "priority": "Major", "reporter": "Sanford Ryza", "assignee": "Sandy Ryza", "labels": [], "created": "2014-02-18T13:30:25.000+0000", "updated": "2014-05-07T01:15:39.000+0000", "description": "This is an umbrella JIRA to track near-term improvements for Spark on YARN.  I don't think huge changes are required - just fixing some bugs, plugging usability gaps, and enhancing documentation.", "comments": ["Closing this as all its subtasks are completed."], "derived": {"summary": "This is an umbrella JIRA to track near-term improvements for Spark on YARN. I don't think huge changes are required - just fixing some bugs, plugging usability gaps, and enhancing documentation.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Umbrella for hardening Spark on YARN - This is an umbrella JIRA to track near-term improvements for Spark on YARN. I don't think huge changes are required - just fixing some bugs, plugging usability gaps, and enhancing documentation."}, {"q": "What updates or decisions were made in the discussion?", "a": "Closing this as all its subtasks are completed."}]}}
{"project": "SPARK", "issue_id": "SPARK-1102", "title": "Create a saveAsNewAPIHadoopDataset method", "status": "Resolved", "priority": "Minor", "reporter": "Matei Alexandru Zaharia", "assignee": "Nan Zhu", "labels": ["Starter"], "created": "2014-02-18T13:50:27.000+0000", "updated": "2014-03-18T11:06:55.000+0000", "description": "Right now RDDs can only be saved as files using the new Hadoop API, not as \"datasets\" with no filename and just a JobConf. See http://codeforhire.com/2014/02/18/using-spark-with-mongodb/ for an example of how you have to give a bogus filename. For the old Hadoop API, we have saveAsHadoopDataset.", "comments": ["I'm just working on the other JIRA (SPARK-1100) involving the same part of code, I would like to fix this also", "Due to the code conflict, I can only fix this after the PR for SPARK-1100 is merged or closed", "anyway, made a PR https://github.com/apache/incubator-spark/pull/636\n\nI don't mind resolve the conflict manually"], "derived": {"summary": "Right now RDDs can only be saved as files using the new Hadoop API, not as \"datasets\" with no filename and just a JobConf. See http://codeforhire.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Create a saveAsNewAPIHadoopDataset method - Right now RDDs can only be saved as files using the new Hadoop API, not as \"datasets\" with no filename and just a JobConf. See http://codeforhire."}, {"q": "What updates or decisions were made in the discussion?", "a": "anyway, made a PR https://github.com/apache/incubator-spark/pull/636\n\nI don't mind resolve the conflict manually"}]}}
{"project": "SPARK", "issue_id": "SPARK-1103", "title": "Garbage collect RDD information inside of Spark", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Tathagata Das", "labels": [], "created": "2014-02-18T15:52:59.000+0000", "updated": "2015-09-14T07:43:06.000+0000", "description": "When Spark jobs run for a long period of time, state accumulates. This is dealt with now using TTL-based cleaning. Instead we should do proper garbage collection using weak references.", "comments": ["https://github.com/apache/spark/pull/126", "User 'tdas' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/126"], "derived": {"summary": "When Spark jobs run for a long period of time, state accumulates. This is dealt with now using TTL-based cleaning.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Garbage collect RDD information inside of Spark - When Spark jobs run for a long period of time, state accumulates. This is dealt with now using TTL-based cleaning."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'tdas' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/126"}]}}
{"project": "SPARK", "issue_id": "SPARK-1104", "title": "Worker should not block while killing executors", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Nan Zhu", "labels": [], "created": "2014-02-18T15:56:09.000+0000", "updated": "2014-04-24T22:57:12.000+0000", "description": "Sometimes due to large shuffles executors will take a long time shutting down. In particular this can happen if large numbers of shuffle files are around (this will be alleviated by SPARK-1103, but nonetheless...).\n\nThe symptom is you have DEAD workers sitting around in the UI and the existing workers keep trying to re-register but can't because they've been assumed dead.\n\nIf killing the executor happens in its own thread, or if the ExecutorRunner were an actor, this would not be a problem. For 0.9 I'd prefer the former approach since it minimizes code changes.", "comments": ["I went through the related code, if my understanding is correct, the difficulty is that how to call process.destroy() and waitFor() in a separate thread, \n\nmy proposal is to have a cleanup thread to maintain the process together with workerThread, \n\nHow do you think about that, Patrick?\n\n", "made the PR: https://github.com/apache/spark/pull/35", "any one would like to review the PR?"], "derived": {"summary": "Sometimes due to large shuffles executors will take a long time shutting down. In particular this can happen if large numbers of shuffle files are around (this will be alleviated by SPARK-1103, but nonetheless.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Worker should not block while killing executors - Sometimes due to large shuffles executors will take a long time shutting down. In particular this can happen if large numbers of shuffle files are around (this will be alleviated by SPARK-1103, but nonetheless."}, {"q": "What updates or decisions were made in the discussion?", "a": "any one would like to review the PR?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1105", "title": "config.yml has an error version number", "status": "Resolved", "priority": "Major", "reporter": "Nan Zhu", "assignee": "Nan Zhu", "labels": [], "created": "2014-02-18T16:24:11.000+0000", "updated": "2014-02-20T11:13:06.000+0000", "description": "as reported by http://apache-spark-user-list.1001560.n3.nabble.com/question-about-compiling-SimpleApp-td1689.html\n\nthe SCALA_VERSION in config.yml is wrong\n\nit should be 2.10.3", "comments": [], "derived": {"summary": "as reported by http://apache-spark-user-list. 1001560.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "config.yml has an error version number - as reported by http://apache-spark-user-list. 1001560."}]}}
{"project": "SPARK", "issue_id": "SPARK-1106", "title": "check key name and identity file before launch a cluster", "status": "Resolved", "priority": "Minor", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-02-18T18:02:55.000+0000", "updated": "2014-02-18T18:54:10.000+0000", "description": "I launched an EC2 cluster without providing a key name and an identity file. The error showed up after two minutes. It would be good to check those options before launch, given the fact that EC2 billing rounds up to hours.\n\nPR: https://github.com/apache/incubator-spark/pull/617", "comments": [], "derived": {"summary": "I launched an EC2 cluster without providing a key name and an identity file. The error showed up after two minutes.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "check key name and identity file before launch a cluster - I launched an EC2 cluster without providing a key name and an identity file. The error showed up after two minutes."}]}}
{"project": "SPARK", "issue_id": "SPARK-1107", "title": "Add shutdown hook on executor stop to stop running tasks", "status": "Resolved", "priority": "Major", "reporter": "Andrew Ash", "assignee": null, "labels": ["bulk-closed"], "created": "2014-02-18T20:40:57.000+0000", "updated": "2019-05-21T05:37:17.000+0000", "description": "Originally reported by aash:\n\nhttp://mail-archives.apache.org/mod_mbox/incubator-spark-dev/201402.mbox/%3CCA%2B-p3AHXYhpjXH9fr8jQ5%2B_gc%3DNHjLbOiJB9bHSahfEET5aHBQ%40mail.gmail.com%3E\n\nLatest in thread:\n\nhttp://mail-archives.apache.org/mod_mbox/incubator-spark-dev/201402.mbox/%3CCA+-p3AFi7vz=2Oty3CAA0G+5EKg+A84uVqrL9TgstVgwGYB_iw@mail.gmail.com%3E\n\nThe most popular approach is to add a shutdown hook that stops running tasks in the executors.", "comments": ["according mails, i add threadPool shutdown when kill task, is it right, it's my patch?", "We have a shutdown hook that stops the SparkContext, which is kind of related.", "I added a link to an issue which is about an executor setup hook, which i think is related and should be addressed together."], "derived": {"summary": "Originally reported by aash:\n\nhttp://mail-archives. apache.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add shutdown hook on executor stop to stop running tasks - Originally reported by aash:\n\nhttp://mail-archives. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "I added a link to an issue which is about an executor setup hook, which i think is related and should be addressed together."}]}}
{"project": "SPARK", "issue_id": "SPARK-1108", "title": "saveAsNewAPIHadoopFile throws NPE with TableOutputFormat", "status": "Resolved", "priority": "Minor", "reporter": "Bryn Keller", "assignee": "Bryn Keller", "labels": [], "created": "2014-02-18T23:31:06.000+0000", "updated": "2014-03-09T17:52:46.000+0000", "description": "When using HBase's TableOutputFormat, saveAsNewAPIHadoopFile throws NPE because the destination table is not set.", "comments": ["I'll create a pull request for this tomorrow.", "PR: https://github.com/apache/incubator-spark/pull/638\n\nMerged in https://github.com/apache/spark/commit/4d880304867b55a4f2138617b30600b7fa013b14\n\nStill needs to be merged to branch-0.9 AFAIK"], "derived": {"summary": "When using HBase's TableOutputFormat, saveAsNewAPIHadoopFile throws NPE because the destination table is not set.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "saveAsNewAPIHadoopFile throws NPE with TableOutputFormat - When using HBase's TableOutputFormat, saveAsNewAPIHadoopFile throws NPE because the destination table is not set."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR: https://github.com/apache/incubator-spark/pull/638\n\nMerged in https://github.com/apache/spark/commit/4d880304867b55a4f2138617b30600b7fa013b14\n\nStill needs to be merged to branch-0.9 AFAIK"}]}}
{"project": "SPARK", "issue_id": "SPARK-1109", "title": "wrong API docs for pyspark map function", "status": "Resolved", "priority": "Major", "reporter": "Diana Carroll", "assignee": "Prashant Sharma", "labels": [], "created": "2014-02-19T09:48:29.000+0000", "updated": "2020-02-07T17:26:39.000+0000", "description": "The source code/API docs for the pyspark RDD map function says:\n\n    def map(self, f, preservesPartitioning=False):\n        \"\"\"\n        Return a new RDD containing the distinct elements in this RDD.\n        \"\"\"\n        def func(split, iterator): return imap(f, iterator)\n        return PipelinedRDD(self, func, preservesPartitioning)\n\nI think that was incorrectly cut-and-pasted from the distinct() function, and should actually say \"Return a new RDD by applying a function to each element of this RDD.\"", "comments": ["User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/73"], "derived": {"summary": "The source code/API docs for the pyspark RDD map function says:\n\n    def map(self, f, preservesPartitioning=False):\n        \"\"\"\n        Return a new RDD containing the distinct elements in this RDD. \"\"\"\n        def func(split, iterator): return imap(f, iterator)\n        return PipelinedRDD(self, func, preservesPartitioning)\n\nI think that was incorrectly cut-and-pasted from the distinct() function, and should actually say \"Return a new RDD by applying a function to each element of this RDD.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "wrong API docs for pyspark map function - The source code/API docs for the pyspark RDD map function says:\n\n    def map(self, f, preservesPartitioning=False):\n        \"\"\"\n        Return a new RDD containing the distinct elements in this RDD. \"\"\"\n        def func(split, iterator): return imap(f, iterator)\n        return PipelinedRDD(self, func, preservesPartitioning)\n\nI think that was incorrectly cut-and-pasted from the distinct() function, and should actually say \"Return a new RDD by applying a function to each element of this RDD."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/73"}]}}
{"project": "SPARK", "issue_id": "SPARK-1110", "title": "Clean up and clarify use of SPARK_HOME", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Patrick McFadin", "labels": [], "created": "2014-02-19T15:31:12.000+0000", "updated": "2014-05-15T18:20:03.000+0000", "description": "In the spirit of SPARK-929 we should clean up the use of SPARK_HOME and, if possible, remove it entirely.\n\nWe need to look through what this is used for. One use was allowing applications to run different versions of Spark in standalone mode. For instance, someone could submit an application with a custom SPARK_HOME and the Worker would launch an Executor using a different path for Spark. This use case is not widely used and maybe should just be removed.\n\nThe existing constructors that take SPARK_HOME for this purpose should be deprecated and we should explain that SPARK_HOME is no longer used for this purpose.\n\nIf there are other legitimate reasons for SPARK_HOME, we can keep it around... we need to audit the uses of it.\n", "comments": ["There's some relevant discussion in https://github.com/apache/incubator-spark/pull/192.", "This was subsumed by the other configuration clean-up."], "derived": {"summary": "In the spirit of SPARK-929 we should clean up the use of SPARK_HOME and, if possible, remove it entirely. We need to look through what this is used for.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Clean up and clarify use of SPARK_HOME - In the spirit of SPARK-929 we should clean up the use of SPARK_HOME and, if possible, remove it entirely. We need to look through what this is used for."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was subsumed by the other configuration clean-up."}]}}
{"project": "SPARK", "issue_id": "SPARK-1111", "title": "URL Validation Throws Error for HDFS URL's", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Patrick McFadin", "labels": [], "created": "2014-02-19T22:26:27.000+0000", "updated": "2014-03-30T04:13:36.000+0000", "description": "The validation method incorrectly assumes Java's URL parser is okay with hdfs urls.", "comments": ["what error", "The issue was that we used new URL(userURL) as a way to verify that the URL was correct, but the URL constructor throws an Exception if the protocol is hdfs://, which is not good.\n\nWith [PR #625|https://github.com/apache/incubator-spark/pull/625], we instead check manually for URL-like syntax.", "thanksI know:-)"], "derived": {"summary": "The validation method incorrectly assumes Java's URL parser is okay with hdfs urls.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "URL Validation Throws Error for HDFS URL's - The validation method incorrectly assumes Java's URL parser is okay with hdfs urls."}, {"q": "What updates or decisions were made in the discussion?", "a": "thanksI know:-)"}]}}
{"project": "SPARK", "issue_id": "SPARK-1112", "title": "When spark.akka.frameSize > 10, task results bigger than 10MiB block execution", "status": "Resolved", "priority": "Blocker", "reporter": "Guillaume Pitel", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-02-20T08:34:26.000+0000", "updated": "2014-12-01T08:29:53.000+0000", "description": "When I set the spark.akka.frameSize to something over 10, the messages sent from the executors to the driver completely block the execution if the message is bigger than 10MiB and smaller than the frameSize (if it's above the frameSize, it's ok)\n\nWorkaround is to set the spark.akka.frameSize to 10. In this case, since 0.8.1, the blockManager deal with  the data to be sent. It seems slower than akka direct message though.\n\nThe configuration seems to be correctly read (see actorSystemConfig.txt), so I don't see where the 10MiB could come from ", "comments": ["Thanks for reporting this!  Does Spark hang, or does the worker throw an exception?  If the former, would you mind uploading the Spark worker log, and if the latter, can you add the stack trace?", "No Exception, and not \"hanging\" in the bad way the executors can sometime hang : if I kill the driver, the workers receive the shutdown signal and exit cleanly.\n\nHere are the logs :\n\nDRIVER :\n\n14/02/19 16:21:49 INFO TaskSetManager: Serialized task 12.0:13 as 2083 bytes in 0 ms\n14/02/19 16:21:49 INFO TaskSetManager: Starting task 12.0:14 as TID 2294 on executor 1: t4.exensa.loc (PROCESS_LOCAL)\n14/02/19 16:21:49 INFO TaskSetManager: Serialized task 12.0:14 as 2083 bytes in 0 ms\n14/02/19 16:21:49 INFO TaskSetManager: Starting task 12.0:15 as TID 2295 on executor 4: t3.exensa.loc (PROCESS_LOCAL)\n14/02/19 16:21:49 INFO TaskSetManager: Serialized task 12.0:15 as 2083 bytes in 1 ms\n14/02/19 16:21:49 INFO TaskSetManager: Starting task 12.0:16 as TID 2296 on executor 0: t0.exensa.loc (PROCESS_LOCAL)\n14/02/19 16:21:49 INFO TaskSetManager: Serialized task 12.0:16 as 2083 bytes in 2 ms\n14/02/19 16:21:49 INFO TaskSetManager: Starting task 12.0:17 as TID 2297 on executor 3: t1.exensa.loc (PROCESS_LOCAL)\n14/02/19 16:21:49 INFO TaskSetManager: Serialized task 12.0:17 as 2083 bytes in 1 ms\n14/02/19 16:21:49 INFO TaskSetManager: Starting task 12.0:18 as TID 2298 on executor 2: t5.exensa.loc (PROCESS_LOCAL)\n14/02/19 16:21:49 INFO TaskSetManager: Serialized task 12.0:18 as 2083 bytes in 1 ms\n14/02/19 16:21:49 INFO TaskSetManager: Starting task 12.0:19 as TID 2299 on executor 5: t6.exensa.loc (PROCESS_LOCAL)\n14/02/19 16:21:49 INFO TaskSetManager: Serialized task 12.0:19 as 2083 bytes in 1 ms\n\nEXECUTOR :\n14/02/19 15:21:53 INFO Executor: Serialized size of result for 2287 is 17229427\n14/02/19 15:21:53 INFO Executor: Sending result for 2287 directly to driver\n14/02/19 15:21:53 INFO Executor: Serialized size of result for 2299 is 17229262\n14/02/19 15:21:53 INFO Executor: Sending result for 2299 directly to driver\n14/02/19 15:21:53 INFO Executor: Finished task ID 2299\n14/02/19 15:21:53 INFO Executor: Finished task ID 2287\n14/02/19 15:21:53 INFO Executor: Serialized size of result for 2281 is 17229426\n14/02/19 15:21:53 INFO Executor: Sending result for 2281 directly to driver\n14/02/19 15:21:53 INFO Executor: Finished task ID 2281\n\nThere is a timezone difference between driver & executor\n\n", "I have a similar issue, I'm on spark-0.9.0 compiled with cdh-4.2.1\n\nFor me, serialized tasks over 10 MB do not reach executors. I've tried this with spark.akka.frameSize set to 160 and 10. The workaround suggested (setting spark.akka.frameSize to 10) does not work for me.\n\nI've confirmed that even if serialized tasks are just under 10MB, the executors do get them and the task is completed.\n\nSpark hangs. There are no exceptions or unusual ERROR/WARN/DEBUG logs in the driver, master, executor or worker daemon logs. The executors just don't seem to have received the tasks. The application UI shows the task status as running, but never progresses.\n\nHere are the last few lines from my driver and one executor:\n\nDRIVER:\n14/02/21 06:37:00 INFO TaskSetManager: Finished TID 797 in 53897 ms on spark-slave01 (progress: 78/80)\n14/02/21 06:37:00 INFO DAGScheduler: Completed ResultTask(9, 58)\n14/02/21 06:37:08 INFO TaskSetManager: Finished TID 768 in 75767 ms on spark-slave02 (progress: 79/80)\n14/02/21 06:37:08 INFO TaskSchedulerImpl: Remove TaskSet 9.0 from pool \n14/02/21 06:37:08 INFO DAGScheduler: Completed ResultTask(9, 69)\n14/02/21 06:37:08 INFO DAGScheduler: Stage 9 (reduceByKeyLocally at SKMeans.scala:174) finished in 99.048 s\n14/02/21 06:37:08 INFO SparkContext: Job finished: reduceByKeyLocally at SKMeans.scala:174, took 99.359019444 s\n14/02/21 06:37:09 INFO SparkContext: Starting job: reduceByKeyLocally at SKMeans.scala:174\n14/02/21 06:37:09 INFO DAGScheduler: Got job 7 (reduceByKeyLocally at SKMeans.scala:174) with 80 output partitions (allowLocal=false)\n14/02/21 06:37:09 INFO DAGScheduler: Final stage: Stage 10 (reduceByKeyLocally at SKMeans.scala:174)\n14/02/21 06:37:09 INFO DAGScheduler: Parents of final stage: List()\n14/02/21 06:37:09 INFO DAGScheduler: Missing parents: List()\n14/02/21 06:37:09 INFO DAGScheduler: Submitting Stage 10 (MapPartitionsRDD[28] at reduceByKeyLocally at SKMeans.scala:174), which has no missing parents\n14/02/21 06:37:10 INFO DAGScheduler: Submitting 80 missing tasks from Stage 10 (MapPartitionsRDD[28] at reduceByKeyLocally at SKMeans.scala:174)\n14/02/21 06:37:10 INFO TaskSchedulerImpl: Adding task set 10.0 with 80 tasks\n14/02/21 06:37:10 INFO TaskSetManager: Starting task 10.0:0 as TID 800 on executor 2: spark-slave01 (PROCESS_LOCAL)\n14/02/21 06:37:10 INFO TaskSetManager: Serialized task 10.0:0 as 10700743 bytes in 19 ms\n...<Starting task and Serialized task lines repeat for each of 40 tasks.>\n\nEXECUTOR: spark-slave01\n14/02/21 06:36:08 DEBUG Executor: Task 798's epoch is 3\n14/02/21 06:36:08 DEBUG CacheManager: Looking for partition rdd_4_60\n14/02/21 06:36:08 DEBUG BlockManager: Getting local block rdd_4_60\n14/02/21 06:36:08 DEBUG BlockManager: Level for block rdd_4_60 is StorageLevel(false, true, true, 1)\n14/02/21 06:36:08 DEBUG BlockManager: Getting block rdd_4_60 from memory\n14/02/21 06:36:08 INFO BlockManager: Found block rdd_4_60 locally\n14/02/21 06:36:21 INFO Executor: Serialized size of result for 765 is 1224222\n14/02/21 06:36:21 INFO Executor: Sending result for 765 directly to driver\n14/02/21 06:36:21 INFO Executor: Finished task ID 765\n14/02/21 06:36:34 INFO Executor: Serialized size of result for 790 is 1262463\n14/02/21 06:36:34 INFO Executor: Sending result for 790 directly to driver\n14/02/21 06:36:34 INFO Executor: Finished task ID 790\n14/02/21 06:36:37 INFO Executor: Serialized size of result for 784 is 1394816\n14/02/21 06:36:37 INFO Executor: Sending result for 784 directly to driver\n14/02/21 06:36:37 INFO Executor: Finished task ID 784\n14/02/21 06:36:38 INFO Executor: Serialized size of result for 787 is 1409571\n14/02/21 06:36:38 INFO Executor: Sending result for 787 directly to driver\n14/02/21 06:36:38 INFO Executor: Finished task ID 787\n14/02/21 06:36:41 INFO Executor: Serialized size of result for 798 is 1270321\n14/02/21 06:36:41 INFO Executor: Sending result for 798 directly to driver\n14/02/21 06:36:41 INFO Executor: Finished task ID 798\n14/02/21 06:36:50 INFO Executor: Serialized size of result for 792 is 1175064\n14/02/21 06:36:50 INFO Executor: Sending result for 792 directly to driver\n14/02/21 06:36:50 INFO Executor: Finished task ID 792\n14/02/21 06:36:52 INFO Executor: Serialized size of result for 794 is 1485354\n14/02/21 06:36:52 INFO Executor: Sending result for 794 directly to driver\n14/02/21 06:36:52 INFO Executor: Finished task ID 794\n14/02/21 06:37:00 INFO Executor: Serialized size of result for 797 is 1615486\n14/02/21 06:37:00 INFO Executor: Sending result for 797 directly to driver\n14/02/21 06:37:00 INFO Executor: Finished task ID 797\n\nRoshan", "Roshan, the issue you're seeing is different -- Guillaume's issue is when task results are too large to be sent using Akka (in which case Spark should use a different code path to send task results to the executor); your issue is when the task itself is too large, in which case Spark (in theory!) gives up and throw an error.  We should fix both problems, but would you mind opening a separate issue?", "Guillaume, just to clarify, the logs you pasted above are for when you set the maximum frame size to 16MiB?  I'm asking because the task results seem to be just slightly larger than 16MiB, which isn't the failure case you mentioned in your description.", "Sorry, I should have specified it. The frameSize was set to 512 for those logs. I've also tried with 16, and it works when results are over 16MB", "Cool thanks for clarifying!  Looking into this...", "When I set BOTH the property on driver with \n\nSystem.setProperty(\"spark.akka.frameSize\", 128) AND I pass the env parameter to SparkContext with SPARK_JAVA_OPTS = \"-Dspark.akka.frameSize=128\" \n\nThen it seems to works.\n\nSo maybe the problem comes from properties not being passed correctly to workers when executors are instanciated ?\n\nAlso, I'm using packaged binary distribution for CDH4 on a standalone cluster", "Hi,\n\nI just realized my driver wasn't picking up spark.akka.frameSize value, because of a problem in the way I was passing it in. However, my executor's were picking this value correctly from their conf/spark-env.sh files.\n\nNow, both sides, the driver and executors print the correct value for frameSize with spark.akka.logAkkaConfig=true.\n\nI also noticed that simply starting the driver with java -Dspark.akka.frameSize=200 does not propagate this automatically to the executors. Not that this is an issue. I guess I was just confused about the configuration.\n\nGuillaume, seems like you have the reverse situation as mine, ie. your drivers are correctly configured with the right frameSize, but the executors are still using the 10MB default?\n\nTo conclude, after ensuring that the driver is correctly configured with the right frameSize, so far, serialized tasks larger than 10MB are being received by the executors and run successfully.\n\nRoshan", "You're right Roshan.I was expecting the akka properties set before SparkContext creation to be propagated to the Executors (and based on what the Spark code does, it should be the case). \n\nI think it should be enforced for the whole akka stuff (timeouts and so on), as well as for the rest of spark properties", "Hey There,\n\nI spent some time playing with this and couldn't reproduce the issue. The driver should capture the options and pass them to executors. I just tested this with a local cluster and was able to verify the akka frame size is passed to executors even if it's not set in spark-env.sh where the executor launches.\n\n[~roshan] - what happens if you remove the setting from spark-env.sh on the executors and only set it at the driver. Does that work correctly?", "In my code I already had some options passed in SPARK_JAVA_OPTS in the env of the SparkContext, but nothing about the akka frameSize. Could it be related ?\n\nSince it was working correctly in 0.8.1, maybe it's related to the new SparkConf ?", "[~guillaumepitel] If you aren't setting akka.frameSize in SPARK_JAVA_OPTS then where are you setting it?\n\nWhat I was saying is that if you do System.setProperty(spark.akka.frameSize, XX) before you create the SparkContext it should collect this and send it to the executors correctly. One thing is if you set it after you create the SparkContext it won't work... are you doing this by any chance?", "No, I create the SparkContext after setting the properties (and I've nothing on my nodes for configuring the frameSize, it's a per-process configuration). \n\nSo before my workaround, I was just setting the property (and the environment in the UI was showing the right value) and passing a SPARK_JAVA_OPTS to the env of the SparkContext with \n{code}\nSystem.setProperty(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\nSystem.setProperty(\"spark.kryo.registrator\", registrator)\nSystem.setProperty(\"spark.kryo.referenceTracking\", \"false\")\nSystem.setProperty(\"spark.kryoserializer.buffer.mb\", bufferSize.toString)\nSystem.setProperty(\"spark.locality.wait\", \"10000\")\nSystem.setProperty(\"spark.hadoop.mapreduce.output.fileoutputformat.compress\", \"true\")\nSystem.setProperty(\"spark.hadoop.mapreduce.output.fileoutputformat.compress.codec\", codec)\nSystem.setProperty(\"spark.hadoop.mapreduce.output.fileoutputformat.compress.type\", \"BLOCK\")\nSystem.setProperty(\"spark.akka.frameSize\", akkaFrameSize.toString)\n\nval sb = new StringBuilder()\nsb.append(\"-Dspark.storage.memoryFraction=\" + sparkMemoryFraction())\nsb.append(\" -Dspark.worker.timeout=\" + sparkWorkerTimeout())\nsb.append(\" -Dspark.akka.askTimeout=\" + sparkAkkaAskTimeout())\nsb.append(\" -Dspark.akka.timeout=\" + sparkAkkaTimeout())\nsb.append(\" -Dspark.shuffle.consolidateFiles=\" + sparkShuffleConsolidateFiles())\n\nvar env = new HashMap[String, String]()\nenv += \"SPARK_JAVA_OPTS\" -> sb.toString()\n\nval sc = new SparkContext(sparkMaster(), appName, sparkHome(), jars(), env)\n{code}\n\nThat caused a problem (the akka frame size seemed to be passed to the executor, but only after the creation of the actorSystem, because it was taking the right code path, but the akka system didn't seem to be properly configured).\n\nNow if I add this to my code :\n\n{code}\nsb.append(\" -Dspark.akka.frameSize=\" + akkaFrameSize)\n{code}\n\nIt works\n", "@Patrick Wendell -\n\nCase 1.\nDRIVER:\nI start the driver with java -Dspark.akka.frameSize=200 -D... -cp .....\n\nEXECUTOR:\nspark-env.sh on workers with frameSize specified: \nexport SPARK_JAVA_OPTS='-Dspark.local.dir=/data/spark/tmp -Dspark.akka.logAkkaConfig=true -Dspark.akka.frameSize=160 -Dspark.akka.timeout=100 -Dspark.akka.askTimeout=30 -Dspark.akka.logLifecycleEvents=true -Dspark.worker.timeout=200'\n\nExecutor akka config log reports frameSize=160\n\nCase 2. \nDRIVER:\nI start the driver with java -Dspark.akka.frameSize=200 -D... -cp ....\n\nEXECUTOR:\nspark-env.sh on workers with frameSize NOT specified: \nexport SPARK_JAVA_OPTS='-Dspark.local.dir=/data/spark/tmp -Dspark.akka.logAkkaConfig=true -Dspark.akka.timeout=100 -Dspark.akka.askTimeout=30 -Dspark.akka.logLifecycleEvents=true -Dspark.worker.timeout=200'\n\nExecutor akka config log on workers has no frameSize and executor process(by ps waux) has no frameSize. Here, I suppose it picks up the default 10MB.\n\nCase 3. \nDRIVER:\nThis time I export SPARK_JAVA_OPTS=\"-Dspark.akka.frameSize=220\" on the dirver host, before running the driver with java -Dspark.akka.frameSize=200 -D... -cp ....\nI'm not reading SPARK_JAVA_OPTS in the driver code.\nDriver's Akka config log, says frameSize=200.\n\nEXECUTOR1 on HOST1:\nNo frameSize specified in spark-env\nExecutor akka config log says frameSize=220.\n\nEXECUTOR2 on HOST2:\nframeSize=160 in spark-env\nExecutor akka config log says frameSize=220, \n\nps waux shows the process was started with this cmd:\n/usr/java/default/bin/java -cp sparkJar.jar -Dspark.local.dir=/data/spark/tmp -Dspark.akka.logAkkaConfig=true -Dspark.akka.frameSize=160 -Dspark.akka.timeout=100 -Dspark.akka.askTimeout=30 -Dspark.akka.logLifecycleEvents=true -Dspark.worker.timeout=200 -Dspark.akka.frameSize=220 -Xms3072M -Xmx3072M org.apache.spark.executor.CoarseGrainedExecutorBackend .....\n\nSo the SPARK_JAVA_OPTS from both the executor and the driver are appended, but since java overrides the first frameSize with the second, the executor runs with frameSize=220\n\nCase 4.\nDRIVER:\nThis time I export SPARK_JAVA_OPTS=\"-Dspark.akka.frameSize=220\" on the driver host, but don't specify frameSize in the launch command.\nAgain, I don't SPARK_JAVA_OPTS in the driver code.\nNo frameSize in driver's akka config log. I expect driver picks up the default 10MB frameSize.\n\nExecutors are the same as in case3.\n\nWhat I'm seeing is that, you can specify the frameSize for a driver in the launch command as java -Dspark.akka.frameSize property, and for the executors in their respective spark-env.sh. If you specify SPARK_JAVA_OPTS on the driver side, then this will override the value for the executors, but not for the driver. I don't use spark-class.sh to launch my driver, because somewhere I read that its meant for spark internal classes and examples. I also currently build my SparkContext directly, without using SparkConf.\n\nThis not an issue for me any longer. That being said, it would have saved me loads of time, if the logs had provided some indication that sending serialized tasks to the executors had failed because they were larger than 10MB. Also, the documentation could be a bit clearer about what is set where and which property overrides or is overridden.\n\nThanks.", "Hi all, \n\nI'm very new to Spark and doing some tests, I've experienced similar issue.\n(tested with Spark Shell, 0.9.1, r3.8xlarge instance on EC2 - 32 core / 244GiB MEM)\n\nI was trying to broadcast 700MB of data and Spark hangs when I run collect() method for the data. \n\nHere's the strange things :\n1) when I tried \n{code}val userInfo = sc.textFile(\"file:///spark/logs/user_sign_up2.csv\").map{line => val split = line.split(\",\"); (split(1), split)}\nval userInfoMap = userInfo.collectAsMap\n{code}\nit runs well.\n2) when I tried \n{code}val userInfo = sc.textFile(\"file:///spark/logs/user_sign_up2.csv\").map{line => val split = line.split(\",\"); (split(1), split(5))} \nval userInfoMap = userInfo.collectAsMap\n{code}\nSpark hangs.\n3) when I slightly control the data size using sample() method or cutting the data file, it runs well. \n\nOur team investigated logs from master and worker then we found worker finished all tasks but master couldn't retrieve the result from a task the result size larger than 10MB\n\nWe tried to apply the workaround setting spark.akka.frameSize to 9, it works like a charm.\n\nI guess it might hard to reproduce the issue, please contact me if there's need of testing or getting logs. \n\nThanks!", "I'm curious, why did you want to make the frameSize this big -- are the tasks themselves also big or just the results? There might be other buffers in Akka that can't be made bigger than this. It's possible that this changed in a newer Akka version (because larger frame sizes used to work before).", "[~matei]\nI've found the default of spark.akka.frameSize is 10 from the config document, http://spark.apache.org/docs/0.9.1/configuration.html\njust tried to slightly larger and smaller (11 and 9) values.\n\nI did collect() method on the userInfo and it might contains large data. (edited the first comment.)\n\n", "[~matei] Do you know which akka version we should use to be able to use big frame size. ", "To follow up this thread, I have done some experiments when the frameSize is around 10MB .\n\n1) spark.akka.frameSize = 10\nIf one of the partition size is very close to 10MB, say 9.97MB, the execution blocks without any exception or warning. Worker finished the task to send the serialized result, and then throw exception saying hadoop IPC client connection stops (changing the logging to debug level). However, the master never receives the results and the program just hangs.\nBut if sizes for all the partitions less than some number btw 9.96MB amd 9.97MB, the program works fine.\n2) spark.akka.frameSize = 9\nwhen the partition size is just a little bit smaller than 9MB, it fails as well.\n\nThis bug behavior is not exactly what spark-1112 is about, could you please guide me how to open a separate bug when the serialization size is very close to 10MB. \n\nThanks a lot", "I have filed a bug https://issues.apache.org/jira/browse/SPARK-2156", "We were able to reproduce this - thanks for reporting it.", "Awesome, looking forward to the fix. At least better error or exception message would be helpful. ", "PR: https://github.com/apache/spark/pull/1124", "This is fixed in the 1.0 branch via:\nhttps://github.com/apache/spark/pull/1172", "Fixed in 1.1.0 via:\nhttps://github.com/apache/spark/pull/1132", "Can a clear workaround be specified for this bug please? For those unable to upgrade to run on 1.0.1  or 1.1.0 in production, general instructions on the workaround are required. This is a huge blocker for current production deployments (even on 1.0.0) otherwise. For instance, running a saveAsTextFile() on an RDD (~400MB) causes execution to freeze with the last log statements seen on the driver being:\n\n14/06/25 16:38:55 INFO spark.SparkContext: Starting job: saveAsTextFile at Test.java:99\n14/06/25 16:38:55 INFO scheduler.DAGScheduler: Got job 6 (saveAsTextFile at Test.java:99) with 2 output partitions (allowLocal=false)\n14/06/25 16:38:55 INFO scheduler.DAGScheduler: Final stage: Stage 6(saveAsTextFile at Test.java:99)\n14/06/25 16:38:55 INFO scheduler.DAGScheduler: Parents of final stage: List()\n14/06/25 16:38:55 INFO scheduler.DAGScheduler: Missing parents: List()\n14/06/25 16:38:55 INFO scheduler.DAGScheduler: Submitting Stage 6 (MappedRDD[558] at saveAsTextFile at Test.java:99), which has no missing parents\n14/06/25 16:38:55 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 6 (MappedRDD[558] at saveAsTextFile at Test.java:99)\n14/06/25 16:38:55 INFO scheduler.TaskSchedulerImpl: Adding task set 6.0 with 2 tasks\n14/06/25 16:38:55 INFO scheduler.TaskSetManager: Starting task 6.0:0 as TID 5 on executor 1: somehost.corp (PROCESS_LOCAL)\n14/06/25 16:38:55 INFO scheduler.TaskSetManager: Serialized task 6.0:0 as 351777 bytes in 36 ms\n14/06/25 16:38:55 INFO scheduler.TaskSetManager: Starting task 6.0:1 as TID 6 on executor 0: someotherhost.corp (PROCESS_LOCAL)\n14/06/25 16:38:55 INFO scheduler.TaskSetManager: Serialized task 6.0:1 as 186453 bytes in 16 ms\n\nThe test setup for reproducing this issue has two slaves (each with 24G) running spark standalone. The driver runs with Xmx 4G.\n\nThanks.", "[~reachbach] If you are running on standalone mode, it might work if you go on every node in your cluster and add the following to spark-env.sh:\n\n{code}\nexport SPARK_JAVA_OPTS=\"-Dspark.akka.frameSize=XXX\"\n{code}\n\nHowever, this work around will only work if every job in your cluster is using the same frame size (XXX).\n\nThe main recommendation is to upgrade to 1.0.1. We are very conservative about what we merge into maintenance branches, so we recommend users upgrade immediately once we release them.", "This is not resolved yet because it needs to be back ported into 0.9", "PR for branch-0.9: https://github.com/apache/spark/pull/1455", "Issue resolved by pull request 1455\n[https://github.com/apache/spark/pull/1455]", "Does anyone test in version0.9.2I found it also failed , while  v1.0.1 & v1.1.0 is ok. ", "Looks like the \"Fix Versions\" accidentally got overwritten during a backport / cherry-pick, so I've restored them based on the issue history."], "derived": {"summary": "When I set the spark. akka.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "When spark.akka.frameSize > 10, task results bigger than 10MiB block execution - When I set the spark. akka."}, {"q": "What updates or decisions were made in the discussion?", "a": "Looks like the \"Fix Versions\" accidentally got overwritten during a backport / cherry-pick, so I've restored them based on the issue history."}]}}
{"project": "SPARK", "issue_id": "SPARK-1113", "title": "External Spilling Bug - hash collision causes NoSuchElementException", "status": "Resolved", "priority": "Blocker", "reporter": "Andrew Or", "assignee": "Andrew Or", "labels": [], "created": "2014-02-20T15:53:19.000+0000", "updated": "2014-03-09T17:43:00.000+0000", "description": "When reading KV pairs back from disk, ExternalAppendOnlyMap maintains a StreamBuffer for each spilled file. These StreamBuffers are ordered by key hash code, and a hash of Int.MAX_VALUE signifies that the corresponding StreamBuffer is empty.\n\nHowever, Int.MAX_VALUE is a perfectly legitimate hash value. If there exists a key with this value, then ExternalAppendOnlyMap does not differentiate between empty StreamBuffers and StreamBuffers that contain only this key. As a result, a NoSuchElementException is thrown - https://github.com/apache/incubator-spark/blob/95d28ff3d0d20d9c583e184f9e2c5ae842d8a4d9/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala#L304.\n\njava.util.NoSuchElementException (java.util.NoSuchElementException)\norg.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:277)\norg.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:212)\norg.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:29)", "comments": ["PR opened at https://github.com/apache/incubator-spark/pull/624"], "derived": {"summary": "When reading KV pairs back from disk, ExternalAppendOnlyMap maintains a StreamBuffer for each spilled file. These StreamBuffers are ordered by key hash code, and a hash of Int.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "External Spilling Bug - hash collision causes NoSuchElementException - When reading KV pairs back from disk, ExternalAppendOnlyMap maintains a StreamBuffer for each spilled file. These StreamBuffers are ordered by key hash code, and a hash of Int."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR opened at https://github.com/apache/incubator-spark/pull/624"}]}}
{"project": "SPARK", "issue_id": "SPARK-1114", "title": "Patch to allow PySpark to use existing JVM and Gateway", "status": "Resolved", "priority": "Minor", "reporter": "Ahir Reddy", "assignee": "Ahir Reddy", "labels": [], "created": "2014-02-20T18:04:15.000+0000", "updated": "2014-02-20T21:22:02.000+0000", "description": "Changes to PySpark implementation of SparkConf to take existing SparkConf JVM handle. Change to PySpark SparkContext to allow subclass specific context initialization.\n\nhttps://github.com/apache/incubator-spark/pull/622", "comments": [], "derived": {"summary": "Changes to PySpark implementation of SparkConf to take existing SparkConf JVM handle. Change to PySpark SparkContext to allow subclass specific context initialization.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Patch to allow PySpark to use existing JVM and Gateway - Changes to PySpark implementation of SparkConf to take existing SparkConf JVM handle. Change to PySpark SparkContext to allow subclass specific context initialization."}]}}
{"project": "SPARK", "issue_id": "SPARK-1115", "title": "Better error message when python worker process dies", "status": "Resolved", "priority": "Major", "reporter": "Sanford Ryza", "assignee": "Bouke van der Bijl", "labels": [], "created": "2014-02-21T01:36:30.000+0000", "updated": "2014-02-26T14:58:34.000+0000", "description": "Right now all I see is:\n\n{code}\njava.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:177)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:55)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:42)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:89)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:53)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:232)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:53)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)\n\tat org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:724)\n{code}", "comments": ["Fixed by https://github.com/apache/incubator-spark/pull/644"], "derived": {"summary": "Right now all I see is:\n\n{code}\njava. io.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Better error message when python worker process dies - Right now all I see is:\n\n{code}\njava. io."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed by https://github.com/apache/incubator-spark/pull/644"}]}}
{"project": "SPARK", "issue_id": "SPARK-1116", "title": "The spark-shell will fail to start when Spark is deployed using the tar.gz file built by ./make-distribution.", "status": "Resolved", "priority": "Major", "reporter": "Bernardo Gomez Palacio", "assignee": null, "labels": [], "created": "2014-02-21T02:07:07.000+0000", "updated": "2014-02-22T23:04:37.000+0000", "description": "The spark-shell will fail to start when Spark is deployed using the tar.gz file built by ./make-distribution. \n\nReplicate by: \n1. executing a ./make-distribution.sh \n2. copy the generated tar.gz file to a directory and extract. \n3. cd into the spark directory that was expanded. e.g. spark-0.9.0-incubating \n4. execute ./bin/spark-shell. \n\nCause: \nThe ./bin/spark-shell is unable to find the Main class to start and has no reference to a CLASSPATH. The problem is generated by the spark-class shell that fails to setup a CLASSPATH if Spark is a RELEASE. ", "comments": ["Fix provided under:\nhttps://github.com/apache/incubator-spark/pull/627"], "derived": {"summary": "The spark-shell will fail to start when Spark is deployed using the tar. gz file built by.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "The spark-shell will fail to start when Spark is deployed using the tar.gz file built by ./make-distribution. - The spark-shell will fail to start when Spark is deployed using the tar. gz file built by."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fix provided under:\nhttps://github.com/apache/incubator-spark/pull/627"}]}}
{"project": "SPARK", "issue_id": "SPARK-1144", "title": "Run Apache RAT In SBT Build to Catch License Errors", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Prashant Sharma", "labels": [], "created": "2014-02-21T09:48:07.000+0000", "updated": "2020-11-19T04:06:55.000+0000", "description": "For reference look at how Kafka used to do this:\n\nhttps://github.com/apache/kafka/commit/4ad98872ad25b93a10c368edc77b59ed4032d3af", "comments": ["Also it might be good to e-mail the kafka list about this, because it looks like this isn't still in use in their code (maybe they came up with a better way).", "Looks like they are still using it and just that they think having a special target for it is pointless, since it is callable simply as java -jar rat.jar. \n\n------Message attached-----\nHi Prashant , rat is still run as part of the release process\nhttps://cwiki.apache.org/confluence/display/KAFKA/Release+Process (this\ndocument is going to change in the next day or so for the 0.8.1 release now\nthat we moved to Gradle and away from SBT, fyi).... Gradle also has a\nlicense mechanism too (btw) that we use now in the build script\nhttps://github.com/apache/kafka/blob/0.8.1/build.gradle#L30\n\nWe removed the bash script to run rat (easy enough to run it by command\nline)... It didn't make sense to have a bash script that is used\ninfrequently by a small few for what is a one line of command to run\nanyways... I don't think we are going to need it anymore but I will still\nrun it prior to release.\n\n/*******************************************\n Joe Stein\n Founder, Principal Consultant\n Big Data Open Source Security LLC\n http://www.stealth.ly\n Twitter: @allthingshadoop <http://www.twitter.com/allthingshadoop>\n********************************************/\n\n\nOn Wed, Feb 26, 2014 at 5:23 AM, Prashant Sharma <scrapcodes@gmail.com>wrote:\n\n> Hi all,\n>\n> I am one of the contributors to Apache Spark and was looking into RAT for\n> catching licensing errors in the codebase. From the git log it is clear\n> KAFKA once had Apache RAT in the build and was later removed. It is not\n> clear what was the alternative taken. This information would be helpful, in\n> the sense we can learn from your experience.\n>\n> --\n> Prashant\n>", "we can use an equivalent of that gradle plugin. https://github.com/Banno/sbt-license-plugin (I doubt if they work properly !)", "Hey [~prashant] I think for this one we want to use RAT because it has Apache-specific guidelines. I actually think it's fine to just run RAT from a script provided that we can configure it with excludes and such. I think RAT supports doing this using an XML file even if you aren't using a RAT plug-in for the build tool. It's fine if this is isolated from the build (actually maybe that's better since we don't need to worry about it separately from maven and sbt), we can just call it in the test script when merging pull requests.", "User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/125"], "derived": {"summary": "For reference look at how Kafka used to do this:\n\nhttps://github. com/apache/kafka/commit/4ad98872ad25b93a10c368edc77b59ed4032d3af.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Run Apache RAT In SBT Build to Catch License Errors - For reference look at how Kafka used to do this:\n\nhttps://github. com/apache/kafka/commit/4ad98872ad25b93a10c368edc77b59ed4032d3af."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/125"}]}}
{"project": "SPARK", "issue_id": "SPARK-1117", "title": "Update accumulator docs", "status": "Resolved", "priority": "Trivial", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-02-21T14:23:21.000+0000", "updated": "2014-02-21T22:45:26.000+0000", "description": "The current doc hints spark doesn't support accumulators of type `Long`, which is wrong.", "comments": ["PR: https://github.com/apache/incubator-spark/pull/631"], "derived": {"summary": "The current doc hints spark doesn't support accumulators of type `Long`, which is wrong.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Update accumulator docs - The current doc hints spark doesn't support accumulators of type `Long`, which is wrong."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR: https://github.com/apache/incubator-spark/pull/631"}]}}
{"project": "SPARK", "issue_id": "SPARK-1118", "title": "Executor state shows as KILLED even the application is finished normally", "status": "Resolved", "priority": "Minor", "reporter": "Nan Zhu", "assignee": "Kan Zhang", "labels": [], "created": "2014-02-21T19:12:03.000+0000", "updated": "2014-06-05T21:33:01.000+0000", "description": "This seems weird, ExecutorState has no option of FINISHED, a terminated executor can only be KILLED, FAILED, LOST", "comments": ["just a hint: the key to resolve this is to find out the reason of generating DisassociatedEvent~ \n\nmaybe we should add a new deploymessage to send out before the driver exits normally"], "derived": {"summary": "This seems weird, ExecutorState has no option of FINISHED, a terminated executor can only be KILLED, FAILED, LOST.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Executor state shows as KILLED even the application is finished normally - This seems weird, ExecutorState has no option of FINISHED, a terminated executor can only be KILLED, FAILED, LOST."}, {"q": "What updates or decisions were made in the discussion?", "a": "just a hint: the key to resolve this is to find out the reason of generating DisassociatedEvent~ \n\nmaybe we should add a new deploymessage to send out before the driver exits normally"}]}}
{"project": "SPARK", "issue_id": "SPARK-1119", "title": "Make examples and assembly jar naming consistent between maven/sbt", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "labels": [], "created": "2014-02-21T22:37:15.000+0000", "updated": "2014-04-23T17:21:30.000+0000", "description": "Right now it's somewhat different. Also it should be consistent with what the classpath and example scripts calculate.\n\nThis makes them consistent:\nspark-assembly-1.0.0-SNAPSHOT-hadoop1.0.4.jar\nspark-examples-1.0.0-SNAPSHOT-hadoop1.0.4.jar", "comments": [], "derived": {"summary": "Right now it's somewhat different. Also it should be consistent with what the classpath and example scripts calculate.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Make examples and assembly jar naming consistent between maven/sbt - Right now it's somewhat different. Also it should be consistent with what the classpath and example scripts calculate."}]}}
{"project": "SPARK", "issue_id": "SPARK-1120", "title": "Send all dependency logging through slf4j", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Sean R. Owen", "labels": [], "created": "2014-02-22T19:50:20.000+0000", "updated": "2015-01-15T09:08:40.000+0000", "description": "There are a few dependencies that pull in other logging frameworks which don't get routed correctly. We should include the relevant slf4j adapters and exclude those logging libraries.", "comments": ["Oops there is already an issue for this"], "derived": {"summary": "There are a few dependencies that pull in other logging frameworks which don't get routed correctly. We should include the relevant slf4j adapters and exclude those logging libraries.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Send all dependency logging through slf4j - There are a few dependencies that pull in other logging frameworks which don't get routed correctly. We should include the relevant slf4j adapters and exclude those logging libraries."}, {"q": "What updates or decisions were made in the discussion?", "a": "Oops there is already an issue for this"}]}}
{"project": "SPARK", "issue_id": "SPARK-1121", "title": "Only add avro if the build is for Hadoop 0.23.X and SPARK_YARN is set", "status": "Resolved", "priority": "Major", "reporter": "Patrick McFadin", "assignee": "Prashant Sharma", "labels": [], "created": "2014-02-22T21:01:15.000+0000", "updated": "2020-02-07T17:26:40.000+0000", "description": "The reason why this is needed is that in the 0.23.X versions of hadoop-client the avro dependency is fully excluded:\n\nhttp://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client/0.23.10/hadoop-client-0.23.10.pom\n\nIn later versions 2.2.X the avro dependency is correctly inherited from hadoop-common:\nhttp://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client/2.2.0/hadoop-client-2.2.0.pom\n\nSo as a workaround Spark currently depends on Avro directly in the sbt and scala builds. This is a bit ugly so I'd like to propose the following:\n\n1. In the Maven build just remove avro and make a note on the building-with-maven page that they will need to manually add avro for this build.\n2. On sbt only add the avro dependency if the version is 0.23.X and SPARK_YARN is true. Also we only need to add avro not both {avro, avro-ipc} like is there now.", "comments": ["The dependencies couldn't be added in maven through the profiles?\n\nThis is really user unfriendly for anyone using maven with hadoop 0.23.", "Was this change tested against maven with YARN on Hadoop 2.2?  I'm running into:\n\n[ERROR]   The project org.apache.spark:yarn-parent_2.10:1.0.0-incubating-SNAPSHOT (/home/sandy/spark/spark/yarn/pom.xml) has 2 errors\n[ERROR]     'dependencies.dependency.version' for org.apache.avro:avro:jar is missing. @ line 55, column 17\n[ERROR]     'dependencies.dependency.version' for org.apache.avro:avro-ipc:jar is missing. @ line 59, column 17\n", "I'll fix this today sorry about that.\n\n---\nsent from my phone\nOn Feb 28, 2014 9:11 AM, \"Sandy Ryza (JIRA)\" <\n\n", "User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/6", "User 'pwendell' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/37"], "derived": {"summary": "The reason why this is needed is that in the 0. 23.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Only add avro if the build is for Hadoop 0.23.X and SPARK_YARN is set - The reason why this is needed is that in the 0. 23."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'pwendell' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/37"}]}}
{"project": "SPARK", "issue_id": "SPARK-1122", "title": "Collect the RDD and send to each partition to form a new RDD", "status": "Resolved", "priority": "Minor", "reporter": "Shuo Xiang", "assignee": null, "labels": [], "created": "2014-02-22T21:30:54.000+0000", "updated": "2015-03-01T10:58:18.000+0000", "description": " Two methods (allCollect, allCollectBroadcast) are added to RDD[T], which output a new RDD[Array[T]] instance with each partition containing all of the\nrecords of the original RDD stored in a single Array[T] instance (the\nsame as RDD.collect). This functionality can be useful in machine learning tasks that require sharing updated parameters across partitions.", "comments": ["PR: https://github.com/apache/incubator-spark/pull/635/", "Is this still live? It seems like this doesn't need special operations. You can collect() an RDD and broadcast it, right?", "You can also accomplish this with {{mapPartitions}} and simply convert the {{Iterator}} you get for each partition into an {{Array}}."], "derived": {"summary": "Two methods (allCollect, allCollectBroadcast) are added to RDD[T], which output a new RDD[Array[T]] instance with each partition containing all of the\nrecords of the original RDD stored in a single Array[T] instance (the\nsame as RDD. collect).", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Collect the RDD and send to each partition to form a new RDD - Two methods (allCollect, allCollectBroadcast) are added to RDD[T], which output a new RDD[Array[T]] instance with each partition containing all of the\nrecords of the original RDD stored in a single Array[T] instance (the\nsame as RDD. collect)."}, {"q": "What updates or decisions were made in the discussion?", "a": "You can also accomplish this with {{mapPartitions}} and simply convert the {{Iterator}} you get for each partition into an {{Array}}."}]}}
{"project": "SPARK", "issue_id": "SPARK-1123", "title": "saveAsNewAPIHadoopFile throws java.lang.InstantiationException all the time", "status": "Closed", "priority": "Major", "reporter": "Nan Zhu", "assignee": null, "labels": [], "created": "2014-02-23T00:37:14.000+0000", "updated": "2014-02-23T10:20:07.000+0000", "description": "scala> val a = sc.textFile(\"/Users/nanzhu/code/incubator-spark/LICENSE\", 2).map(line => (\"a\", \"b\"))\n\nscala> a.saveAsNewAPIHadoopFile(\"/Users/nanzhu/code/output_rdd\")\njava.lang.InstantiationException\n\tat sun.reflect.InstantiationExceptionConstructorAccessorImpl.newInstance(InstantiationExceptionConstructorAccessorImpl.java:48)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n\tat java.lang.Class.newInstance(Class.java:374)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:632)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:590)\n\tat $iwC$$iwC$$iwC$$iwC.<init>(<console>:15)\n\tat $iwC$$iwC$$iwC.<init>(<console>:20)\n\tat $iwC$$iwC.<init>(<console>:22)\n\tat $iwC.<init>(<console>:24)\n\tat <init>(<console>:26)\n\tat .<init>(<console>:30)\n\tat .<clinit>(<console>)\n\tat .<init>(<console>:7)\n\tat .<clinit>(<console>)\n\tat $print(<console>)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:774)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1042)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:611)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:642)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:606)\n\tat org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:790)\n\tat org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:835)\n\tat org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:747)\n\tat org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:595)\n\tat org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:602)\n\tat org.apache.spark.repl.SparkILoop.loop(SparkILoop.scala:605)\n\tat org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:928)\n\tat org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:878)\n\tat org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:878)\n\tat scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)\n\tat org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:878)\n\tat org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:970)\n\tat org.apache.spark.repl.Main$.main(Main.scala:31)\n\tat org.apache.spark.repl.Main.main(Main.scala)\n\n\n-------\n\nI'm not sure about the reason, but a work-around is to discard using outputFormatClass in the parameter list, but get the class of outputFormat by job.getOutputFormatClass\n    \n\n", "comments": ["an example of the work-around mentioned above is in \n\nhttps://github.com/CodingCat/incubator-spark/commit/478abdc6dd88eaad644e3850666dc40bf9d1f1a9", "just forgot to pass class information explicitly"], "derived": {"summary": "scala> val a = sc. textFile(\"/Users/nanzhu/code/incubator-spark/LICENSE\", 2).", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "saveAsNewAPIHadoopFile throws java.lang.InstantiationException all the time - scala> val a = sc. textFile(\"/Users/nanzhu/code/incubator-spark/LICENSE\", 2)."}, {"q": "What updates or decisions were made in the discussion?", "a": "just forgot to pass class information explicitly"}]}}
{"project": "SPARK", "issue_id": "SPARK-1124", "title": "Infinite NullPointerException failures due to a null in map output locations", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "labels": [], "created": "2014-02-23T19:51:11.000+0000", "updated": "2014-02-26T09:56:06.000+0000", "description": "The following spark-shell code leads to an infinite retry of the last stage in Spark 0.9:\n\n{code}\nval data = sc.parallelize(1 to 100, 2).map(x => {throw new NullPointerException; (x, x)}).reduceByKey(_ + _)\n\ndata.count()    // This first one terminates correctly with just an NPE\n\ndata.count()    // This second one never terminates, it keeps failing over and over\n{code}\n\nThe problem seems to be that when there's an NPE in the map stage, we erroneously add map output locations for it, so the next job on the RDD runs only the reduce stage. Those tasks keep failing but they count as a fetch failure, so it keeps retrying.", "comments": ["Fixed in https://github.com/apache/incubator-spark/pull/641.", "Test comment", "Test comment for email integration", "Another test comment for email", "Another test comment"], "derived": {"summary": "The following spark-shell code leads to an infinite retry of the last stage in Spark 0. 9:\n\n{code}\nval data = sc.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Infinite NullPointerException failures due to a null in map output locations - The following spark-shell code leads to an infinite retry of the last stage in Spark 0. 9:\n\n{code}\nval data = sc."}, {"q": "What updates or decisions were made in the discussion?", "a": "Another test comment"}]}}
{"project": "SPARK", "issue_id": "SPARK-1125", "title": "The maven build error for Spark Examples", "status": "Resolved", "priority": "Major", "reporter": "Guoqiang Li", "assignee": null, "labels": [], "created": "2014-02-24T00:17:02.000+0000", "updated": "2014-08-20T08:21:49.000+0000", "description": " mvn -v\nApache Maven 3.1.1 (0728685237757ffbf44136acec0402957f723d9a; 2013-09-17 23:22:22+0800)\nMaven home: /usr/local/Cellar/maven/3.1.1/libexec\nJava version: 1.7.0_51, vendor: Oracle Corporation\nJava home: /Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre\nDefault locale: zh_CN, platform encoding: UTF-8\nOS name: \"mac os x\", version: \"10.9.1\", arch: \"x86_64\", family: \"mac\"", "comments": ["This error occurs when the maven uses a http proxy. ", "I see you reopened the pull request, but haven't you found that it's just your proxy interfering? then it's nothing to do with Spark, and you already found the Maven settings to make it work, which are not necessary for everyone else that is not on your network.", "When someone adds a http proxy configuration maven, he'll get an error when compiling the Spark. We can not determine all of the configuration http proxy, but we can guarantee configure http proxy maven compile spark will not appear this error.", "It's accurate to say that, if your proxy is breaking HTTPS connections, and you do not configure workarounds in Maven, you will get an error from any project that accesses a repo over HTTPS. Your change in the pull request does not fix this problem. This is not something for which some general configuration would resolve the issue.", "maven settings.xml :\n{code:xml}   \n    <proxy>\n      <id>optional</id>\n      <active>true</active>\n      <protocol>http</protocol>\n      <host>127.0.0.1</host>\n      <port>8087</port>\n      <nonProxyHosts>localhost|127.0.0.1|local</nonProxyHosts>\n    </proxy>\n {code}\n\ndo this\n{code}\n mvn -U -Pyarn -Dhadoop.version=2.3.0 -Dyarn.version=2.3.0 compile  -X >> http_proxy.txt\n{code}", "If I put this in my settings.xml, my build completely fails. I am sure it makes your build work, so you should set this. But this is not something that is an issue with Spark that needs to be patched.", "In China, no proxy or vpn is not connected *maven.twttr.com* , therefore there are a lot of people need this http proxy configuratio in settings.xml", "Building the current master using Maven has the same compiler error\n{code}\ngit checkout 5d98cfc1c8fb17fbbeacc7192ac21c0b038cbd16 \nmvn -U  -Pyarn -Dhadoop.version=2.2.0 -Dyarn.version=2.2.0 -DskipTests clean  install \n{code}", "Apache Maven 3.2.1 can work."], "derived": {"summary": "mvn -v\nApache Maven 3. 1.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "The maven build error for Spark Examples - mvn -v\nApache Maven 3. 1."}, {"q": "What updates or decisions were made in the discussion?", "a": "Apache Maven 3.2.1 can work."}]}}
{"project": "SPARK", "issue_id": "SPARK-1126", "title": "spark-submit script for running compiled binaries", "status": "Resolved", "priority": "Blocker", "reporter": "Sandy Ryza", "assignee": "Sandy Ryza", "labels": [], "created": "2014-02-24T00:42:15.000+0000", "updated": "2014-03-30T05:40:55.000+0000", "description": "It would be useful to have a script, roughly similar to the \"hadoop jar\" command that is used for running compiled binaries against Spark.\n\nThe script would do two things:\n* Set up the Spark classpath on the client side, so that users don't need to know where Spark jars are installed or bundle all of Spark inside their app jar.\n* Provide a layer over the different modes that apps can be run, so that the same spark-jar invocation could run the driver inside a YARN application master or in the client process, depending on the cluster setup.", "comments": ["Just as another requirement, we might want the same script to work for Python. It can be called spark-app or spark-submit or something.", "That makes sense to me. So I'm thinking:\n\nspark-app <app jar> <main class> [<args>]\n\nwhere args are:\nworker-memory - Memory requested from scheduler per executor.\nworker-cores - Cores requested from scheduler per executor.\nnum-workers - Number of executors.\nmaster-memory - Memory requested from scheduler for driver.  Only applies on yarn-standalone mode.\nmaster-cores - Memory requested from scheduler for driver.  Only applies on yarn-standalone mode.\nmaster-max-heap - Max heap size for the driver JVM.\ndeploy-mode - yarn-client, yarn-standalone, standalone-standalone, or standalone-client (could maybe use better names here, the confusing thing is that \"standalone\" refers to both a cluster manager and a deploy mode)\nsupervise - Whether to automatically restart driver on failure.  Only works on standalone-standalone mode, though we should be able to add support for this in yarn-standalone as well.\nadd-jars - Additional jars that should be on the driver and executor classpaths.\nfiles - Files to place next to all executors.  Only works in yarn-standalone and yarn-client mode.\narchives - Archives to extract next to all executors.  Only works in yarn-standalone and yarn-client mode.\nqueue - Queue/pool to submit the application to.  Only works in yarn-standalone and yarn-client mode.\nargs - Arguments to pass to the driver.\n\nIt would be nice for deploy-mode to be settable by an environment variable.  Passing the option would override it.  This would allow cluster operators to set up a default deploy mode for their cluster and not require users to think about it.  A user could specify a particular deploy mode if it matters to them.\n\nBecause many of these don't apply to every mode, it's also worth considering some sort of mechanism for delegating options down to particular modes.  But I think this might be hairier and not add much.\n", "Hey Sandy, a few comments here:\n\n- As I mentioned above, we probably want this to work for Python too in a complete design. In that case you'd probably take either a .py file or a .egg.\n\n- I think deploy-mode should be separated into two pieces: a cluster URL and a flag for whether to run the driver on the cluster versus locally. Our cluster URL system already supports specifying different types of clusters, so it would be ideal to reuse it here.\n\n- Supported deploy modes should include Mesos (maybe you don't allow running the driver in Mesos right now, but it can certainly be outside as a client).\n\n- Some of these flags overlap with settings you can put in your SparkConf object today, such as worker memory, cluster URL (if that becomes a flag) and to some extent JARs. How will these be passed through? One option is to set the Java system properties for them when you execute the user's app (e.g. -Dspark.executor.memory=2g), which are going to populate the Conf by default unless the user overwrites them in their program. I don't know how much of an issue the latter will be but we can create a workaround if it is.\n\n- The master-memory and master-cores should really be called driver-memory and driver-cores, and they apply to in-cluster submission on the standalone mode too.\n\n- If we want this to become the standard, the script should also work on Windows. This makes it considerably hairier, to the point where we might want this to be a Scala class, though in that case the JVM startup overhead for launching it is kind of painful.\n\nAnyway, I do think this would be a great feature to have, but before you implement it, it would be great to see a more detailed design that takes into account these points. In particular the main thing I'm worried about is creating inconsistency across languages, operating systems or deployment modes. If we make this change and update all the docs on deploying applications and such, it would be nice to only have to make it once.", "Thanks for taking a look Matei.  I attached a design doc with an amended version of what I posted above.\n\nbq. Our cluster URL system already supports specifying different types of clusters, so it would be ideal to reuse it here.\nIn the YARN case, the cluster URL encapsulates both the cluster manager and the deploy mode (client vs. standalone), while in the standalone case, it includes only the former.  The design splits these apart for spark-app's arguments, but I wanted to highlight this.\n\nAnother thing I wanted to ask about was memory configuraton.  In Hadoop, the memory requested as a cap from the cluster manager is controlled separately from the memory given as max heap opts to the JVM.  While this is clunky for a lot of reasons, an advantage is that it allows accounting for a process using memory off-heap, either through direct buffers or by forking a subprocess.  If Spark wants to handle these situations, it might make sense to eventually make the amount of padding between the JVM heap and requested memory configurable?\n\nWorking on non-Linux platforms will be very difficult if the script is written in bash.  Even across Linux and Macs, there appears to be no shared utility for parsing long-form options.  A python script could avoid the JVM startup overhead.  I suppose this adds a python dependency, but most platforms now include python by default.  Scala also sounds reasonable to me if that makes the most sense to you.", "[~sandy] Hey Sandy - is it not possible to just manually parse the options in bash rather than use a library? Second, what are the semantics of addJars, is this going to add a jar where the driver program is (and then jars are distributed through the normal path through Spark)? Just want to be clear because there is also addJar inside of Spark context... ", "I suppose bash is turing complete, but it would be very painful and error-prone to use it for this.  It also wouldn't solve the Windows issue.\n\nyarn-standalone mode supports an addJars parameter, which will place local jars on the cluster and add them to the YARN distributed cache so that they can be localized for containers.  I misunderstood and thought that other deploy modes had a similar way of specifying jars to add via command line.  So we can just document that it only works for yarn-standalone mode.  To promote consistency, it could also make sense to add environment variables that would allow other deploy modes to add jars outside of code.  We could also possibly reform the functionality in yarn-standalone mode somehow.\n", "Python isn't available by default on Windows, so we probably can't use that.\n\nRegarding the cluster URL, it's okay if we change URL formats slightly as part of this feature so that you use \"yarn\" for both in-cluster and out-of-cluster clients, but you have a separate flag for \"run the client in the cluster\". I don't think we should combine these two properties (what type of cluster is it and do I want the driver inside) into one flag, because you just end up with flags that are all possible combinations of the two features.\n\nRegarding adding JARs, I believe driver submission in the standalone cluster could also be extended to take multiple JARs. It's okay if the option is only for YARN at first but it will be very confusing for users if they have to submit one way to YARN and another way to standalone clusters, so I'd look into adding that.", "bq. Regarding the cluster URL, it's okay if we change URL formats slightly as part of this feature\nBy this, do you mean changing the yarn URL formats in existing code or just as interpreted by the spark-app script?  I.e. should this still work: \"MASTER=yarn-client ./bin/spark-shell\"?\n\nbq. Regarding adding JARs, I believe driver submission in the standalone cluster could also be extended to take multiple JARs.\nCool, I filed SPARK-1142 for this work.", "Would it be best to just go with Scala?  In non-python cases, we can avoid extra JVM startup time by running the user class in-process instead of forking a new JVM.", "Yes, I think it's fine to do it in Scala.", "Is there any feature-branch that is covering this work. Will like to contribute.", "Here's the pull request: https://github.com/apache/spark/pull/86", "Thanks, will look into it too.", "Github user pwendell commented on a diff in the pull request:\n\n    https://github.com/apache/spark/pull/86#discussion_r11095306\n  \n    --- Diff: docs/cluster-overview.md ---\n    @@ -50,6 +50,47 @@ The system currently supports three cluster managers:\n     In addition, Spark's [EC2 launch scripts](ec2-scripts.html) make it easy to launch a standalone\n     cluster on Amazon EC2.\n     \n    +# Launching Applications\n    --- End diff --\n    \n    Alright let's punt this to a broader doc clean-up for 1.0 which we can do during the QA phase. I think that ideally yes, we should replace all of the mentions of the other clients with this.\n", "Github user pwendell commented on the pull request:\n\n    https://github.com/apache/spark/pull/86#issuecomment-39009977\n  \n    Hey @sryza I'm going to submit a PR with some suggested follow-on changes, but I think we can go ahead and merge this for now as a starting point. Thanks for your work on this!\n", "Github user asfgit closed the pull request at:\n\n    https://github.com/apache/spark/pull/86\n"], "derived": {"summary": "It would be useful to have a script, roughly similar to the \"hadoop jar\" command that is used for running compiled binaries against Spark. The script would do two things:\n* Set up the Spark classpath on the client side, so that users don't need to know where Spark jars are installed or bundle all of Spark inside their app jar.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "spark-submit script for running compiled binaries - It would be useful to have a script, roughly similar to the \"hadoop jar\" command that is used for running compiled binaries against Spark. The script would do two things:\n* Set up the Spark classpath on the client side, so that users don't need to know where Spark jars are installed or bundle all of Spark inside their app jar."}, {"q": "What updates or decisions were made in the discussion?", "a": "Github user asfgit closed the pull request at:\n\n    https://github.com/apache/spark/pull/86"}]}}
{"project": "SPARK", "issue_id": "SPARK-1127", "title": "Add saveAsHBase to PairRDDFunctions", "status": "Resolved", "priority": "Major", "reporter": "Haosdent Huang", "assignee": "Haosdent Huang", "labels": [], "created": "2014-02-24T01:50:42.000+0000", "updated": "2014-12-10T15:52:27.000+0000", "description": "Support to save data in HBase.", "comments": ["I begin to do some HBase support works in Spark. It you have a better approach about this issue, give me advice. Thanks in advance.", "The pull request: https://github.com/apache/spark/pull/123", "In terms of a better approach, what I am beginning to wonder is whether we should be looking for a higher level of abstraction, with data structures, code, API, etc. that can be used by multiple NoSQL, key-value data stores.  That way we may be able to avoid some of the duplicate development and maintenance costs to support HBase, Cassandra, and whatever broadly similar external datastore someone next wants to use with Spark.", "But the data structures are quite different from different NoSQL database. Add an saveAsNoSQL method and configure which NoSQL through JobConf?", "Yes, undoubtedly there are a lot of differences among these various external datastore options, and there can't be just one concrete implementation within Spark to satisfy all of them.  All that I am saying is that before we go too far down the road of supporting completely independent implementations for each of these, at least one somebody should take the time to figure out whether there are common abstractions that can be usefully shared among Spark/NoSQL connectors.\n\nI'm certainly not that somebody up to this point.\n\nAnd all of this is more of a meta-discussion than a discussion of your specific Issue/Pull Request, so lack of resolution of this meta-issue shouldn't be seen as blocking your effort.  It can just as easily (and equally inappropriately) be hung on Cassandra PRs or other external datastore PRs.", "Thank you for your explaination. I am misunderstand what you say before. How about create a \"NoSQLRDDFunctions\" and move \"saveAsHBase\" to it?", "That might work.  You might also look at the various *SaveToCassandra methods in Calliope or at other Spark+Cassandra efforts to see whether there is commonality that might be abstracted to a common NoSQLRDDFunctions or something like that.", "OK, let me have a try. Thank you for you advice. :-)", "You're welcome.  I'm not sure that you will end up anyplace useful, but it will be helpful for someone to at least have taken a serious look at and reported back on whether a higher-level abstraction makes sense.", "I agree that this should be done through an extra library instead of PairRDDFunctions. I actually think the easiest way to do it is just a `HBaseUtils` class with a static method that takes an RDD and information on how to save it. While it's less magical for Scala users, it will also make it easy to call from Java. We used this approach to move external data sources to separate modules in Spark 0.9; for example see http://spark.incubator.apache.org/docs/latest/api/external/flume/index.html#org.apache.spark.streaming.flume.FlumeUtils$ .\n\nThis class should go into a separate `spark-hbase` module located in the `external` folder in the code so that it doesn't bring the HBase dependencies into the default build. Users who want those can link to `spark-hbase`.", "Thank you. I also consider add saveAsHBase to PairRDDFunctions would bring dependence problem before. But I don't have other idea to workaround that. Now I realize that I could make a new external module to achieve this. Thank you very much.", "@matei @Matei Zaharia I have update the pull request. Could you help me to review it? https://github.com/apache/spark/pull/194 Thank you very much.", "[~pwendell] I have update the code in https://github.com/apache/spark/pull/194. Could you review it again and give any advice? Thank you in advance.", "ping [~pwendell] I saw 1.0 have been released, any plan about this issue? :-)", "I looked at this a little bit. It seems to me it is best to go through SchemaRDD in Spark SQL to provide this support. In the future, SchemaRDD should probably become the narrow waist for all structured data support. ", "[~rxin] I have added SchemaRDD support. Could you provide more details about your advice for this patch? Thank you very much.", "According to Reynold,\nFirst half of the external data source API (for reading but not writing) is already in 1.2:\nhttps://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala", "Given the discussion in both PRs, this looks like a WontFix, and consensus was it should proceed in a separate project. Questions about SchemaRDD and 1.2 sound like a new topic."], "derived": {"summary": "Support to save data in HBase.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add saveAsHBase to PairRDDFunctions - Support to save data in HBase."}, {"q": "What updates or decisions were made in the discussion?", "a": "Given the discussion in both PRs, this looks like a WontFix, and consensus was it should proceed in a separate project. Questions about SchemaRDD and 1.2 sound like a new topic."}]}}
{"project": "SPARK", "issue_id": "SPARK-1128", "title": "hadoop task properties not set while using InputFormat", "status": "Resolved", "priority": "Major", "reporter": "Costin Leau", "assignee": "Nan Zhu", "labels": [], "created": "2014-02-24T11:55:59.000+0000", "updated": "2014-03-24T21:56:02.000+0000", "description": "The task specific Hadoop properties, in particular `mapred.task.id` and `mapred.tip.id` (or `mapred.task.partition`) are not set when calling `InputFormat#getRecordReader`.\nImplementations that rely on such properties will fail at this point as no information about the current task environment is provided even though the job is 'theoretically' in progress.\n\nI've noticed `SparkHadoopWriter.scala` sets this properties - it would be nice to have the same thing applied for reading data as well.\n\nThanks!", "comments": ["https://github.com/apache/spark/pull/101"], "derived": {"summary": "The task specific Hadoop properties, in particular `mapred. task.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "hadoop task properties not set while using InputFormat - The task specific Hadoop properties, in particular `mapred. task."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/101"}]}}
{"project": "SPARK", "issue_id": "SPARK-1129", "title": "use a predefined seed when seed is zero in XORShiftRandom", "status": "Resolved", "priority": "Major", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-02-24T18:10:17.000+0000", "updated": "2014-03-18T19:30:08.000+0000", "description": "If the seed is zero, XORShift generates all zeros, which would create unexpected result.", "comments": ["PR: https://github.com/apache/incubator-spark/pull/645", "Merged.", "To mark resolved.", "Merged."], "derived": {"summary": "If the seed is zero, XORShift generates all zeros, which would create unexpected result.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "use a predefined seed when seed is zero in XORShiftRandom - If the seed is zero, XORShift generates all zeros, which would create unexpected result."}, {"q": "What updates or decisions were made in the discussion?", "a": "To mark resolved."}]}}
{"project": "SPARK", "issue_id": "SPARK-1130", "title": "Clean up project documentation navigation menu", "status": "Resolved", "priority": "Minor", "reporter": "Reynold Xin", "assignee": "Reynold Xin", "labels": [], "created": "2014-02-24T18:44:39.000+0000", "updated": "2014-03-18T10:01:07.000+0000", "description": "1. Create a top level Configuration menu\n\n2. Move most stuff in More into Deployment\n\n3. Rename More to Development\n\n", "comments": [], "derived": {"summary": "1. Create a top level Configuration menu\n\n2.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Clean up project documentation navigation menu - 1. Create a top level Configuration menu\n\n2."}]}}
{"project": "SPARK", "issue_id": "SPARK-1131", "title": "Better document the --args option for yarn-standalone mode", "status": "Closed", "priority": "Major", "reporter": "Sanford Ryza", "assignee": "Karthik Kambatla", "labels": [], "created": "2014-02-24T20:30:38.000+0000", "updated": "2014-09-12T21:53:39.000+0000", "description": "It took me a while to figure out that the correct way to use it with multiple arguments was to include the option multiple times.\n\nI.e.\n--args arg1\n--args arg2\n\ninstead of\n--args \"arg1 arg2\" ", "comments": ["This is probably obsolete now with spark-submit.", "--args is now deprecated. We use --arg instead."], "derived": {"summary": "It took me a while to figure out that the correct way to use it with multiple arguments was to include the option multiple times. I.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Better document the --args option for yarn-standalone mode - It took me a while to figure out that the correct way to use it with multiple arguments was to include the option multiple times. I."}, {"q": "What updates or decisions were made in the discussion?", "a": "--args is now deprecated. We use --arg instead."}]}}
{"project": "SPARK", "issue_id": "SPARK-1132", "title": "Persisting Web UI through refactoring the SparkListener interface", "status": "Resolved", "priority": "Blocker", "reporter": "Andrew Or", "assignee": "Andrew Or", "labels": [], "created": "2014-02-24T20:46:18.000+0000", "updated": "2015-05-10T17:35:04.000+0000", "description": "This issue is a spin-off from another issue - https://spark-project.atlassian.net/browse/SPARK-969\n\nThe main issue with the existing Spark Web UI is that its information is lost as soon as the application terminates. This is the direct result of the SparkUI being coupled with SparkContext, which is stopped when the application is finished.\n\nThe attached document proposes to tackle this by logging SparkListenerEvents to persist information displayed on the Web UI. We take this opportunity to replace the existing format for storing this information, HTML, with one that is more flexible, JSON. This allows further post-hoc analysis of a particular Spark application beyond simply reviving the Web UI.", "comments": ["Sorry I haven't had time to follow the PR for this.  \n\nSo this jira refactored the listener interface and added the ability to store events to a file as well as ability for Master to reload from those files, is that correct?  \n\nIf the Master is restarted does can it pick up those files and finished applications?  Could you perhaps summarize what all was added?", "Hi Thomas,\n\nYour understanding is correct. The bulk of the change is within components accessible from SparkContext. This mainly includes (1) adding a special purpose listener to log Spark events to persisted storage, (2) allowing SparkUI to render either from live events (from SparkContext) or from replayed events (from logs), and (3) refactoring events in such a way that makes this possible.\n\nChanges made to Master, on the other hand, only represent a use case of this new functionality. The main change here is simply that Master now renders an after-the-fact SparkUI from these event logs (if any) after the associated application finishes. With or without Master, the application can choose to log events for other purposes, e.g. parsing them in a script for post-hoc analysis. For this reason, it is certainly possible for Master to pick up these files after restarting, as long as it maintains information for the associated application.\n\nAndrew", "Hi Team,\n\nI see the issue is resolved and Fix Version/s: is -1.0.0.\nIs 1.0.0 is spark version ?\n\nWhere i can get the spark version 1.0.0.\n\nCurrently i am getting below error -\n\n15/05/10 08:42:20 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n15/05/10 08:42:20 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n15/05/10 08:42:20 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n15/05/10 08:42:20 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n15/05/10 08:42:20 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n15/05/10 08:42:20 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1750 bytes result sent to driver\n15/05/10 08:42:20 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 207 ms on localhost (1/1)\n15/05/10 08:42:20 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n15/05/10 08:42:20 INFO DAGScheduler: Stage 0 (count at SparkFilter.java:22) finished in 0.225 s\n15/05/10 08:42:20 INFO DAGScheduler: Job 0 finished: count at SparkFilter.java:22, took 0.314437 s\n0\n15/05/10 08:42:20 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception\njava.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.spark.scheduler.EventLoggingListener$$anonfun$logEvent$3.apply(EventLoggingListener.scala:144)\n\tat org.apache.spark.scheduler.EventLoggingListener$$anonfun$logEvent$3.apply(EventLoggingListener.scala:144)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.EventLoggingListener.logEvent(EventLoggingListener.scala:144)\n\tat org.apache.spark.scheduler.EventLoggingListener.onStageCompleted(EventLoggingListener.scala:165)\n\tat org.apache.spark.scheduler.SparkListenerBus$class.onPostEvent(SparkListenerBus.scala:32)\n\tat org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)\n\tat org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)\n\tat org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:53)\n\tat org.apache.spark.util.AsynchronousListenerBus.postToAll(AsynchronousListenerBus.scala:36)\n\tat org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(AsynchronousListenerBus.scala:76)\n\tat org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply(AsynchronousListenerBus.scala:61)\n\tat org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply(AsynchronousListenerBus.scala:61)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1617)\n\tat org.apache.spark.util.AsynchronousListenerBus$$anon$1.run(AsynchronousListenerBus.scala:60)\nCaused by: java.io.IOException: Filesystem closed\n\tat org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:792)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.flushOrSync(DFSOutputStream.java:1998)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.hflush(DFSOutputStream.java:1959)\n\tat org.apache.hadoop.fs.FSDataOutputStream.hflush(FSDataOutputStream.java:130)\n\t... 19 more\n"], "derived": {"summary": "This issue is a spin-off from another issue - https://spark-project. atlassian.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Persisting Web UI through refactoring the SparkListener interface - This issue is a spin-off from another issue - https://spark-project. atlassian."}, {"q": "What updates or decisions were made in the discussion?", "a": "Hi Team,\n\nI see the issue is resolved and Fix Version/s: is -1.0.0.\nIs 1.0.0 is spark version ?\n\nWhere i can get the spark version 1.0.0.\n\nCurrently i am getting below error -\n\n15/05/10 08:42:20 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n15/05/10 08:42:20 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n15/05/10 08:42:20 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n15/05/10 08:42:20 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n15/05/10 08:42:20 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n15/05/10 08:42:20 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1750 bytes result sent to driver\n15/05/10 08:42:20 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 207 ms on localhost (1/1)\n15/05/10 08:42:20 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n15/05/10 08:42:20 INFO DAGScheduler: Stage 0 (count at SparkFilter.java:22) finished in 0.225 s\n15/05/10 08:42:20 INFO DAGScheduler: Job 0 finished: count at SparkFilter.java:22, took 0.314437 s\n0\n15/05/10 08:42:20 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception\njava.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.spark.scheduler.EventLoggingListener$$anonfun$logEvent$3.apply(EventLoggingListener.scala:144)\n\tat org.apache.spark.scheduler.EventLoggingListener$$anonfun$logEvent$3.apply(EventLoggingListener.scala:144)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.EventLoggingListener.logEvent(EventLoggingListener.scala:144)\n\tat org.apache.spark.scheduler.EventLoggingListener.onStageCompleted(EventLoggingListener.scala:165)\n\tat org.apache.spark.scheduler.SparkListenerBus$class.onPostEvent(SparkListenerBus.scala:32)\n\tat org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)\n\tat org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)\n\tat org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:53)\n\tat org.apache.spark.util.AsynchronousListenerBus.postToAll(AsynchronousListenerBus.scala:36)\n\tat org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(AsynchronousListenerBus.scala:76)\n\tat org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply(AsynchronousListenerBus.scala:61)\n\tat org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply(AsynchronousListenerBus.scala:61)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1617)\n\tat org.apache.spark.util.AsynchronousListenerBus$$anon$1.run(AsynchronousListenerBus.scala:60)\nCaused by: java.io.IOException: Filesystem closed\n\tat org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:792)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.flushOrSync(DFSOutputStream.java:1998)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.hflush(DFSOutputStream.java:1959)\n\tat org.apache.hadoop.fs.FSDataOutputStream.hflush(FSDataOutputStream.java:130)\n\t... 19 more"}]}}
{"project": "SPARK", "issue_id": "SPARK-1133", "title": "Add a new small files input for MLlib, which will return an RDD[(fileName, content)]", "status": "Resolved", "priority": "Minor", "reporter": "Xusen Yin", "assignee": "Xusen Yin", "labels": ["IO", "MLLib,", "hadoop"], "created": "2014-02-25T06:16:09.000+0000", "updated": "2014-04-04T18:13:37.000+0000", "description": "As I am moving forward to write a LDA (Latent Dirichlet Allocation) implementation to Spark MLlib, I find that a small files input API is useful, so I write a smallTextFiles() to support it.\n\nsmallTextFiles() digests a directory of text files, then return an RDD\\[(String, String)\\], the former String is the file name, while the latter one is the contents of the text file.\n\nsmallTextFiles() can be used for local disk I/O, or HDFS I/O, just like the textFiles() in SparkContext. In the scenario of LDA, there are 2 common uses:\n\n1. smallTextFiles() is used to preprocess local disk files, i.e. combine those files into a huge one, then transfer it onto HDFS to do further process, such as LDA clustering.\n\n2. It is also used to transfer the raw directory of small files onto HDFS (though it is not recommended, because it will cost too many namenode entries), then clustering it directly with LDA.", "comments": [], "derived": {"summary": "As I am moving forward to write a LDA (Latent Dirichlet Allocation) implementation to Spark MLlib, I find that a small files input API is useful, so I write a smallTextFiles() to support it. smallTextFiles() digests a directory of text files, then return an RDD\\[(String, String)\\], the former String is the file name, while the latter one is the contents of the text file.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add a new small files input for MLlib, which will return an RDD[(fileName, content)] - As I am moving forward to write a LDA (Latent Dirichlet Allocation) implementation to Spark MLlib, I find that a small files input API is useful, so I write a smallTextFiles() to support it. smallTextFiles() digests a directory of text files, then return an RDD\\[(String, String)\\], the former String is the file name, while the latter one is the contents of the text file."}]}}
{"project": "SPARK", "issue_id": "SPARK-1134", "title": "ipython won't run standalone python script", "status": "Resolved", "priority": "Major", "reporter": "Diana Carroll", "assignee": "Diana Carroll", "labels": ["pyspark"], "created": "2014-02-25T07:39:34.000+0000", "updated": "2014-04-03T22:50:12.000+0000", "description": "Using Spark 0.9.0, python 2.6.6, and ipython 1.1.0.\n\nThe problem: If I want to run a python script as a standalone app, the docs say I should execute the command \"pyspark myscript.py\".  This works as long as IPYTHON=0.  But if IPYTHON=1 this doesn't work.\n\nThis problem arose for me because I tried to save myself typing by setting IPYTHON=1 in my shell profile script. Which then meant I was unable to execute pyspark standalone scripts.\n\nMy analysis: \nin the pyspark script, command line arguments are simply ignored if ipython is used:\n{code}if [[ \"$IPYTHON\" = \"1\" ]] ; then\n  exec ipython $IPYTHON_OPTS\nelse\n  exec \"$PYSPARK_PYTHON\" \"$@\"\nfi{code}\n\nI thought I could get around this by changing the script to pass $@.  However, this doesn't work: doing so results in an error saying multiple spark contexts can't be run at once.\n\nThis is because of a feature?/bug? of ipython related to the PYTHONSTARTUP environment variable.  the pyspark script sets this variable to point to the python/shell.py script, which initializes the Spark Context.  In regular python, the PYTHONSTARTUP script runs ONLY if python is invoked in interactive mode; if run with a script, it ignores the variable.  iPython runs that script every time, regardless.  Which means it will always execute Spark's shell.py script to initialize the spark context even when it was invoked with a script.\n\nProposed solution:\nshort term: add this information to the Spark docs regarding iPython.  Something like \"Note, iPython can only be used interactively.  Use regular Python to execute pyspark script files.\"\nlong term: change the pyspark script to tell if arguments are passed in; if so, just call python instead of pyspark, or don't set the PYTHONSTARTUP variable?  Or maybe fix shell.py to detect if it's being invoked in non-interactively and not initialize sc.\n", "comments": ["Just tested this change to pyspark script file and it seems to work:\n\n{code}if [[ \"$IPYTHON\" = \"1\" && $# = 0 ]] ; then\n  exec ipython $IPYTHON_OPTS\nelse\n  exec \"$PYSPARK_PYTHON\" \"$@\"\nfi{code}", "This sounds good, do you want to send in a pull request? Otherwise someone else can fix it as suggested.", "Well, I would if I could.  I tried following the advice and tutorial here:\nhttps://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark\nBut when I attempted to push, I apparently lack the access rights to do\nthat.  Sorry.\n\n\n\nOn Tue, Feb 25, 2014 at 6:42 PM, Matei Zaharia (JIRA) <\n\n", "I was able to submit a pull request: \nhttps://github.com/apache/spark/pull/227 (which makes this fix and also removes IPYTHONOPTS)\n(the original pull request was https://github.com/apache/spark/pull/83)"], "derived": {"summary": "Using Spark 0. 9.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "ipython won't run standalone python script - Using Spark 0. 9."}, {"q": "What updates or decisions were made in the discussion?", "a": "I was able to submit a pull request: \nhttps://github.com/apache/spark/pull/227 (which makes this fix and also removes IPYTHONOPTS)\n(the original pull request was https://github.com/apache/spark/pull/83)"}]}}
{"project": "SPARK", "issue_id": "SPARK-1135", "title": "Anchors broken in latest docs due to bad JavaScript code", "status": "Resolved", "priority": "Minor", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "labels": [], "created": "2014-02-25T17:22:30.000+0000", "updated": "2014-03-21T00:12:30.000+0000", "description": "A recent PR that added Java vs Scala tabs for streaming also inadvertently added some bad code to a document.ready handler, breaking our other handler that manages scrolling to anchors correctly with the floating top bar. As a result the section title ended up always being hidden below the top bar.", "comments": [], "derived": {"summary": "A recent PR that added Java vs Scala tabs for streaming also inadvertently added some bad code to a document. ready handler, breaking our other handler that manages scrolling to anchors correctly with the floating top bar.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Anchors broken in latest docs due to bad JavaScript code - A recent PR that added Java vs Scala tabs for streaming also inadvertently added some bad code to a document. ready handler, breaking our other handler that manages scrolling to anchors correctly with the floating top bar."}]}}
{"project": "SPARK", "issue_id": "SPARK-1136", "title": "Fix FaultToleranceTest for Docker 0.8.1", "status": "Resolved", "priority": "Minor", "reporter": "Aaron Davidson", "assignee": "Aaron Davidson", "labels": [], "created": "2014-02-25T21:42:53.000+0000", "updated": "2014-04-15T03:32:48.000+0000", "description": "Several changes were made between Docker 0.6 (when our spark-test docker files were created) and the current version of Docker, 0.8.1. There are two relevant to the FaultToleranceTest that causes it to fail:\n\n1) A random host name is assigned to Docker containers. This host name, unlike the IP address, is not reachable from outside the container, but by default we'll try to use it as the Worker's Akka host. This fails when a newly-elected Master attempts to recover a Worker, since the Worker is not actually reachable at the host address it connected from.\n\n2) IP addresses are now reassigned immediately upon container recycling. This means that we can confuse \"old\" and \"new\" Workers or Masters that happened to be assigned the same IP address. The main obvious issue that arises is when a Worker gets a \"attempted to re-register\" exception when it takes on a previous Worker's IP address during Master recovery.", "comments": [], "derived": {"summary": "Several changes were made between Docker 0. 6 (when our spark-test docker files were created) and the current version of Docker, 0.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Fix FaultToleranceTest for Docker 0.8.1 - Several changes were made between Docker 0. 6 (when our spark-test docker files were created) and the current version of Docker, 0."}]}}
{"project": "SPARK", "issue_id": "SPARK-1137", "title": "ZK Persistence Engine crashes if stored data has wrong serialVersionUID", "status": "Resolved", "priority": "Major", "reporter": "Aaron Davidson", "assignee": "Aaron Davidson", "labels": [], "created": "2014-02-25T21:47:12.000+0000", "updated": "2018-07-25T20:44:08.000+0000", "description": "The ZooKeeperPersistenceEngine contains information about concurrently existing Masters and Workers. This information, as the name suggests, is persistent in the event of a Master failure/restart. If the Spark version is upgraded, the Master will crash with a Java serialization exception when trying to re-read the persisted data.\n\nInstead of crashing (indefinitely), the Master should probably just ignore the prior data.", "comments": ["User 'aarondav' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/4"], "derived": {"summary": "The ZooKeeperPersistenceEngine contains information about concurrently existing Masters and Workers. This information, as the name suggests, is persistent in the event of a Master failure/restart.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "ZK Persistence Engine crashes if stored data has wrong serialVersionUID - The ZooKeeperPersistenceEngine contains information about concurrently existing Masters and Workers. This information, as the name suggests, is persistent in the event of a Master failure/restart."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'aarondav' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/4"}]}}
{"project": "SPARK", "issue_id": "SPARK-1138", "title": "Spark 0.9.0 does not work with Hadoop / HDFS", "status": "Resolved", "priority": "Major", "reporter": "Sam Abeyratne", "assignee": null, "labels": [], "created": "2014-02-26T03:13:25.000+0000", "updated": "2014-10-13T13:01:25.000+0000", "description": "UPDATE: This problem is certainly related to trying to use Spark 0.9.0 and the latest cloudera Hadoop / HDFS in the same jar.  It seems no matter how I fiddle with the deps, the do not play nice together.\n\nI'm getting a java.util.concurrent.TimeoutException when trying to create a spark context with 0.9.  I cannot, whatever I do, change the timeout.  I've tried using System.setProperty, the SparkConf mechanism of creating a SparkContext and the -D flags when executing my jar.  I seem to be able to run simple jobs from the spark-shell OK, but my more complicated jobs require external libraries so I need to build jars and execute them.\n\nSome code that causes this:\n\nprintln(\"Creating config\")\n    val conf = new SparkConf()\n      .setMaster(clusterMaster)\n      .setAppName(\"MyApp\")\n      .setSparkHome(sparkHome)\n      .set(\"spark.akka.askTimeout\", parsed.getOrElse(timeouts, \"100\"))\n      .set(\"spark.akka.timeout\", parsed.getOrElse(timeouts, \"100\"))\n\n    println(\"Creating sc\")\n\n    implicit val sc = new SparkContext(conf)\n\nThe output:\n\nCreating config\nCreating sc\nlog4j:WARN No appenders could be found for logger (akka.event.slf4j.Slf4jLogger).\nlog4j:WARN Please initialize the log4j system properly.\nlog4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n[ERROR] [02/26/2014 11:05:25.491] [main] [Remoting] Remoting error: [Startup timed out] [\nakka.remote.RemoteTransportException: Startup timed out\n\tat akka.remote.Remoting.akka$remote$Remoting$$notifyError(Remoting.scala:129)\n\tat akka.remote.Remoting.start(Remoting.scala:191)\n\tat akka.remote.RemoteActorRefProvider.init(RemoteActorRefProvider.scala:184)\n\tat akka.actor.ActorSystemImpl._start$lzycompute(ActorSystem.scala:579)\n\tat akka.actor.ActorSystemImpl._start(ActorSystem.scala:577)\n\tat akka.actor.ActorSystemImpl.start(ActorSystem.scala:588)\n\tat akka.actor.ActorSystem$.apply(ActorSystem.scala:111)\n\tat akka.actor.ActorSystem$.apply(ActorSystem.scala:104)\n\tat org.apache.spark.util.AkkaUtils$.createActorSystem(AkkaUtils.scala:96)\n\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:126)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:139)\n\tat com.adbrain.accuracy.EvaluateAdtruthIDs$.main(EvaluateAdtruthIDs.scala:40)\n\tat com.adbrain.accuracy.EvaluateAdtruthIDs.main(EvaluateAdtruthIDs.scala)\nCaused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]\n\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)\n\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)\n\tat scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)\n\tat scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)\n\tat scala.concurrent.Await$.result(package.scala:107)\n\tat akka.remote.Remoting.start(Remoting.scala:173)\n\t... 11 more\n]\nException in thread \"main\" java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]\n\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)\n\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)\n\tat scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)\n\tat scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)\n\tat scala.concurrent.Await$.result(package.scala:107)\n\tat akka.remote.Remoting.start(Remoting.scala:173)\n\tat akka.remote.RemoteActorRefProvider.init(RemoteActorRefProvider.scala:184)\n\tat akka.actor.ActorSystemImpl._start$lzycompute(ActorSystem.scala:579)\n\tat akka.actor.ActorSystemImpl._start(ActorSystem.scala:577)\n\tat akka.actor.ActorSystemImpl.start(ActorSystem.scala:588)\n\tat akka.actor.ActorSystem$.apply(ActorSystem.scala:111)\n\tat akka.actor.ActorSystem$.apply(ActorSystem.scala:104)\n\tat org.apache.spark.util.AkkaUtils$.createActorSystem(AkkaUtils.scala:96)\n\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:126)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:139)\n\tat com.adbrain.accuracy.EvaluateAdtruthIDs$.main(EvaluateAdtruthIDs.scala:40)\n\tat com.adbrain.accuracy.EvaluateAdtruthIDs.main(EvaluateAdtruthIDs.scala)", "comments": ["I've narrowed it down to something to do with the hadoop version.  If I include\n\n\"org.apache.hadoop\" % \"hadoop-client\" % \"2.0.0-cdh4.5.0\"\n\nthen this causes the timeout exception.  But if I do NOT include it, I get the client protocol exception.", "FWIW we also see this in integration tests for CDH5 (i.e. Hadoop 2-based). I don't have anything to add other than, yes, can't change the timeout, and I suspect that may be the issue, since other tests that initialize a Spark streaming context are fine. I tried setting akka.remote.startup-timeout on the command line, in AkkaUtils, in the construction of the SparkConf for the unit tests, etc. I always see it use a value of 10000.", "Sorry, I actually got further with this and forgot to update the ticket.  This seems symptomatic of having the incorrect version of Scala on some nodes of the cluster, i.e. 2.9.3 instead of 2.10.3.  My sysadm found this, and forced 2.10.3 across the cluster.  He also mentioned resolvers in sbt build file might have been wrong, and finally we determined that\n\n\"org.apache.hadoop\" % \"hadoop-client\" % \"2.0.0-mr1-cdh4.5.0\"\n\nworks better.\n\nMy sysadm claims some of your documentation for setting up cluster is out of date and this is why he accidentally used 2.9.3. I'll try to dig out exactly what part of the documentation needs updating.\n\nThanks", "This is helpful info. I think it's worth putting aside the timeout settings.\n\nIt seems weird that changing the Hadoop version would affect Scala versions, since Hadoop 2.0.x has nothing to do with Scala. (Nor does 2.3.x as far as I can tell by looking through the dependency graph.)\n\nYou're talking about a cluster but is this also how you solved a failure in the unit tests? I mean this also fails in the FlumeStreamSuite.scala test, when pointed at Hadoop 2.3.\n\nIs \"your\" = Cloudera or Spark here?", "The hadoop version didn't change the version of Scala, it just somehow made things hit an error that was caused by the wrong version of scala on the cluster.\n\nNo, the failure didn't occur in unit tests.\n\n// Is \"your\" = Cloudera or Spark here? //\n\nSorry, don't understand the question.", "OK. Maybe a similar symptom but different cause. My failure is observed in the unit tests. I am still looking into this then.\n\nYou mentioned CDH and Spark, and so did I. I am at Cloudera. I was asking about your comment that \"your documentation is out of date\" -- didn't know whether it was something I could try to fix on the CDH side if that's what you meant.", "Ah, I mean Spark deployment documentation, not CDH.", "I also hit this, but underlying error  seems different for me:\n\nBy the way, this is the underlying error for me: \n\njava.lang.VerifyError: (class: org/jboss/netty/channel/socket/nio/NioWorkerPool, method: createWorker signature: (Ljava/util/concurrent/Executor;)Lorg/jboss/netty/channel/socket/nio/AbstractNioWorker;) Wrong return type in function \n        at akka.remote.transport.netty.NettyTransport.<init>(NettyTransport.scala:282) \n        at akka.remote.transport.netty.NettyTransport.<init>(NettyTransport.scala:239) \n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) \n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39) \n\n\n\nI also have found a successful workaround, for me:\n\nJust add Akka 2.2.4 to your dependencies. \n\nI am pretty sure the problem is because Spark decided to shade Akka 2.2.3.", "Actually, I believe the problem is related to Netty.\n\nI also observed that when I pull in Hadoop 2.2.x / CDH5, then I still get the timeouts.   That's because Hadoop is pulling in its own version of Netty.\n\nWhat's more, Spark itself pulls in netty 3.6.6.Final (through akka, avro, and other deps) as well as netty-all 4.0.3.Final.\n\nThe final magic incantation that seems to work is to force netty 3.6.6.Final as a dependency.\n\n\n    \"org.apache.spark\" %% \"spark-core\" % \"0.9.0-incubating\" % \"provided\" exclude(\"io.netty\", \"netty-all\"),\n    // Force netty version.  This avoids some Spark netty dependency problem.\n    \"io.netty\" % \"netty\" % \"3.6.6.Final\",\n    \"org.apache.hadoop\" % \"hadoop-client\" % \"2.2.0-cdh5.0.0-beta-2\" excludeAll(excludeJackson,\n      excludeNetty, excludeAsm, excludeCglib)\n\nThis works for me regardless of using the default Hadoop client of 1.0.4, or 2.2.0.", "I'm also getting the same exception when I include any of the following (useful for reading hdfs directly)\n\n\"org.apache.hadoop\" % \"hadoop-core\" % \"2.0.0-mr1-cdh4.5.0\"\n\"org.apache.hadoop\" % \"hadoop-core\" % \"2.0.0-mr1-cdh4.6.0\"\n\nit seems to work in my unit tests with:\n\n\"org.apache.hadoop\" % \"hadoop-core\" % \"2.0.0-cdh4.5.0\"\n\nBut when I run it with \"hadoop jar\" it fails\n\nIt seems it is not completely possible to use the latest Hadoop / HDFS with Spark 0.9.0, that or one needs to spend a few months writing a hideously complicated build file.  I'd be very grateful if any of the Spark guys could, upon every release, provide SBT build file examples on how to get Spark working with various versions of Hadoop.  Given that Spark is nearly always on a stack that includes Hadoop & HDFS, it seems pretty necessary to be able to use both together.  I really enjoy using Spark, but it I need to use HDFS and Hadoop with it!\n\nMany thanks", "Sam, did you try either of my solutions?\n\nOn Tue, Mar 25, 2014 at 6:08 AM, sam (JIRA)\n\n\n\n-- \n--\nEvan Chan\nStaff Engineer\nev@ooyala.com  |\n", "Sorry, forgot to say, yes, tried adding 2.2.4.  Also tried excluding netty, but \"excludeNetty\" is not defined in my build file, so I tried something like:\n\n\"org.apache.spark\" % \"spark-core_2.10\" % \"0.9.0-incubating\"  exclude(\"io.netty\", \"netty-all\"),\n\"io.netty\" % \"netty\" % \"3.6.6.Final\",\n\"org.apache.hadoop\" % \"hadoop-client\" % \"2.0.0-mr1-cdh4.5.0\" exclude(\"io.netty\", \"netty-all\"),\n\"org.apache.hadoop\" % \"hadoop-core\" % \"2.0.0-cdh4.5.0\" exclude(\"io.netty\", \"netty-all\"),\n\nEventually managed to interact with hdfs by using java -cp and specifying full URI's to the files.  Trying with hadoop jar and just using the paths causes the timeout problem. ", "I believe the real underlying cause is just the overall ballooning of\ndependencies, for which I mostly blame Hadoop (and for me, Parquet....\nthough Parquet is merged into trunk now).  There must be a real\nlightweight hadoop client jar that doesn't pull in the rest of the\nworld!\n\nOn Tue, Mar 25, 2014 at 9:22 AM, sam (JIRA)\n\n\n\n-- \n--\nEvan Chan\nStaff Engineer\nev@ooyala.com  |\n", "diff of working and unworking dependency trees", "I don't know if it helps or not, but...\n\nI've just started to hit this error in one of my tests.\n\nI'm building with Maven, if two dependencies depend on different versions of a transitive dependency, the first listed seems to win.\n\nIf I depend on spark before hbase, my test fails with the listed error.\nIf I depend on hbase before spark, my test succeeds\n\nThe two dependencies are:\n    <dependency>\n      <groupId>org.apache.hbase</groupId>\n      <artifactId>hbase</artifactId>\n      <version>0.94.15-cdh4.6.0</version>\n      <exclusions>\n        <exclusion>\n          <artifactId>asm</artifactId>\n          <groupId>asm</groupId>\n        </exclusion>\n        <exclusion>\n          <artifactId>slf4j-api</artifactId>\n          <groupId>org.slf4j</groupId>\n        </exclusion>\n        <exclusion>\n          <artifactId>slf4j-log4j12</artifactId>\n          <groupId>org.slf4j</groupId>\n        </exclusion>\n      </exclusions>\n    </dependency>\n\n    <dependency>\n      <groupId>org.apache.spark</groupId>\n      <artifactId>spark-core_2.10</artifactId>\n      <version>0.9.0-incubating</version>\n    </dependency>\n\nMy previous comment has an attachment which is the diff between the resultant dependency:tree outputs,\n\n\nI hope this helps.", "^^ Yes, I think in SBT you can specify which order to include dependencies via merge strategies.  I also have Scalding as a dep in my project and that is bound to pull in a lot too.\n\n< start SBT rant> What would be really cool, is if someone made a tool to auto generate SBT build files.  You simply input the list of technologies you want to use and the versions installed, e.g. hbase, 0.94.15-cdh4.6.0, Spark 0.9.0, Log4j, testing frameworks, etc, then it generates an SBT build file that actually works (handles all the crazy dependency nonsense, resolvers, merge strategies, etc, etc).  Even better, you run the script on the cluster master(s) and it works out what versions you have installed, so you just specify HBase, Spark, etc.  Why is it that decades on from Makefiles building projects is just as hideous?! < end rant >", "Sam: although SBT is complicated, I think the real problem is the lack of\nclean dependencies in much of the Java ecosystem, and this may partly be\nbecause of the lack of a real packaging system and way to delineate and\ncontain deps.  Project Jigsaw might solve this problem.    For example, how\nmuch better would the Spark world be if there was a truly clean Hadoop\ndependency jar?\n\n\nOn Thu, Mar 27, 2014 at 9:25 AM, sam (JIRA) <\n\n\n\n\n-- \n--\nEvan Chan\nStaff Engineer\nev@ooyala.com  |\n\n<http://www.ooyala.com/>\n<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>\n", "Just want to chime in that I also encountered this stack trace, and the problem was an older Netty (in particular, I have Netty 3.4 on my classpath). Once I included Netty 3.6.6, the problem went away. ", "This is no longer observed in the unit tests. The comments here say it was a Netty dependency problem, and I know that has since been cleaned up. Suggest this is resolved then?", "Yeah let's resolve this until we can reproduce it in the current master branch.", "I built spark master with 'sbt/sbt assembly publish-local' and had issues with my hadoop version, which is CDH 4.4. \n\nThen I built with CDH 4.4, via: 'SPARK_HADOOP_VERSION=2.0.0-mr1-cdh4.4.0 sbt/sbt assembly publish-local'. Note: I did not clean. I saw this issue. This is with Spark trunk when the released version is 1.0.1. \n\nThen I cleaned and rebuilt. The issue persists. \n\nWhat should I do?\n", "See https://github.com/apache/spark/pull/455", "I am using Cloudera Version 4.2.1, Spark 1.1.0 and Scala 2.10.4; Observing similar error \n\nERROR Remoting: Remoting error: [Startup failed] [\nakka.remote.RemoteTransportException: Startup failed\n\tat akka.remote.Remoting.akka$remote$Remoting$$notifyError(Remoting.scala:129)\n\tat akka.remote.Remoting.start(Remoting.scala:194)\n\tat akka.remote.RemoteActorRefProvider.init(RemoteActorRefProvider.scala:184)\n\tat akka.actor.ActorSystemImpl._start$lzycompute(ActorSystem.scala:579)\n\tat akka.actor.ActorSystemImpl._start(ActorSystem.scala:577)\n\tat akka.actor.ActorSystemImpl.start(ActorSystem.scala:588)\n\tat akka.actor.ActorSystem$.apply(ActorSystem.scala:111)\n\tat akka.actor.ActorSystem$.apply(ActorSystem.scala:104)\n\tat org.apache.spark.util.AkkaUtils$.org$apache$spark$util$AkkaUtils$$doCreateActorSystem(AkkaUtils.scala:121)\n\tat org.apache.spark.util.AkkaUtils$$anonfun$1.apply(AkkaUtils.scala:54)\n\tat org.apache.spark.util.AkkaUtils$$anonfun$1.apply(AkkaUtils.scala:53)\n\tat org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1446)\n...\nalong with \nException in thread \"main\" org.jboss.netty.channel.ChannelException: Failed to bind to: <my-host-name>/10.65.42.145:0\n\tat org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272)\n\tat akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:391)\n\tat akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:388)\n\tat scala.util.Success$$anonfun$map$1.apply(Try.scala:206)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat scala.util.Success.map(Try.scala:206)\n\tat scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)\n\tat scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)\n...\n\nFor the second error I tried to update the /etc/hosts file with IP address of my host name and updated the spark-env.sh files with same IP address as suggested in other answer but still struck with the above issues.\n\nI tried adding Netty 3.6.6 to the dependency but still didn't get resolved. "], "derived": {"summary": "UPDATE: This problem is certainly related to trying to use Spark 0. 9.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Spark 0.9.0 does not work with Hadoop / HDFS - UPDATE: This problem is certainly related to trying to use Spark 0. 9."}, {"q": "What updates or decisions were made in the discussion?", "a": "I am using Cloudera Version 4.2.1, Spark 1.1.0 and Scala 2.10.4; Observing similar error \n\nERROR Remoting: Remoting error: [Startup failed] [\nakka.remote.RemoteTransportException: Startup failed\n\tat akka.remote.Remoting.akka$remote$Remoting$$notifyError(Remoting.scala:129)\n\tat akka.remote.Remoting.start(Remoting.scala:194)\n\tat akka.remote.RemoteActorRefProvider.init(RemoteActorRefProvider.scala:184)\n\tat akka.actor.ActorSystemImpl._start$lzycompute(ActorSystem.scala:579)\n\tat akka.actor.ActorSystemImpl._start(ActorSystem.scala:577)\n\tat akka.actor.ActorSystemImpl.start(ActorSystem.scala:588)\n\tat akka.actor.ActorSystem$.apply(ActorSystem.scala:111)\n\tat akka.actor.ActorSystem$.apply(ActorSystem.scala:104)\n\tat org.apache.spark.util.AkkaUtils$.org$apache$spark$util$AkkaUtils$$doCreateActorSystem(AkkaUtils.scala:121)\n\tat org.apache.spark.util.AkkaUtils$$anonfun$1.apply(AkkaUtils.scala:54)\n\tat org.apache.spark.util.AkkaUtils$$anonfun$1.apply(AkkaUtils.scala:53)\n\tat org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1446)\n...\nalong with \nException in thread \"main\" org.jboss.netty.channel.ChannelException: Failed to bind to: <my-host-name>/10.65.42.145:0\n\tat org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272)\n\tat akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:391)\n\tat akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:388)\n\tat scala.util.Success$$anonfun$map$1.apply(Try.scala:206)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat scala.util.Success.map(Try.scala:206)\n\tat scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)\n\tat scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)\n...\n\nFor the second error I tried to update the /etc/hosts file with IP address of my host name and updated the spark-env.sh files with same IP address as suggested in other answer but still struck with the above issues.\n\nI tried adding Netty 3.6.6 to the dependency but still didn't get resolved."}]}}
{"project": "SPARK", "issue_id": "SPARK-1139", "title": "APIs like saveAsNewAPIHadoopFile are actually a mixture of old and new Hadoop API", "status": "Resolved", "priority": "Major", "reporter": "Nan Zhu", "assignee": "Nan Zhu", "labels": ["hadoop"], "created": "2014-02-26T05:13:01.000+0000", "updated": "2014-02-28T15:39:02.000+0000", "description": "In the new Hadoop API , mapreduce.*, the JobConf are actually replaced by mapreduce.Job\n\nbut in Spark, the APIs using new Hadoop APIs are still receiving conf as parameter, seems a bit weird...", "comments": ["For what it's worth, I agree with this, as it will also get rid of some deprecation warnings. And to be clear this does not mean using Hadoop 2-only APIs. These new APIs existed in 1.0.4, the oldest version that Spark supports.\n\nNan I volunteer to help if I can help make the patch or review.", "Sean, thank you very much for the comments, \n\nActually these new APIs are included in Hadoop from 2009 (from the slides I shared in the mail list) and I found this issue with a Spark compiled with 1.0.4 compiled \n\nThanks for your reminding, I will check if they made some changes in 2.x version\n\nThough I have started writing the patch after sending the email, it will be highly appreciated if you can help to review my PR when they are ready", "Yes, I was saying the same thing. I wanted to make sure people understood that you are suggesting using \"new\" APIs that are also present in Hadoop 1.x. Sometimes people think \"new Hadoop APIs\" refers to things in Hadoop 2.x+, like YARN.", "The current API looks reasonable to me.  Are you suggesting that saveAsNewAPIHadoopFile should take a Job instead of a Configuration?  JobConf was not exactly replaced by Job.  It would be equally reasonable to say that it was replaced by Configuration (which the Job accepts as an argument).  In fact, the trend in the new APIs is more towards setting configuration options via the Configuration object than via Job.  Many of the setter methods that existed in JobConf were not transferred over to Job (e.g. setQueueName).  Job is meant to encapsulate functionality for running a MapReduce job on the cluster.  From a higher level, it makes sense to me that an API meant for saving a file would take configuration options, but it would be weird for it to take a \"Job\" that it's not going to run.\n\n", "Agree with that -- Configuration is the more natural replacement, not Job. But in any event it should be replaced with something since JobConf is deprecated. Any of the .mapred. classes should not be used at this point.", "I agree with that Job contains more functionalities for running than for configuration\n\nbut in the current implementation, due to the fact that methods like OutputFormat.setOutputPath only accepts Job as the parameter, the first thing after we receive a Configuration is to create a new Job object...(though we can set output path by configuration.set() )\n\nthe other example I have given in the mail list is input methods, (paste in the last part of the post), where we create a job object from a Configuration and then get its Configuration member for passing as the parameter, and inside NewHadoopRDD, we re-construct the jobcontext object again...\n\nI know that most of these are due to the design of Hadoop APIs, but if we accept Job directly, it can simplify the implementation, or say making it more reasonable (for the setters you mentioned, we assume users should set via Configuration before passing Job object as the parameter)\n\nThank you for the comments\n\n--------\n\n\nval updatedConf = job.getConfiguration\nnew NewHadoopRDD(this, fClass, kClass, vClass, updatedConf)\n\nthen we create a jobContext based on this configuration object\n\nNewHadoopRDD.scala (L74)\nval jobContext = newJobContext(conf, jobId)\nval rawSplits = inputFormat.getSplits(jobContext).toArray\n\n\n", "bq. But in any event it should be replaced with something since JobConf is deprecated.\nLet me know if I'm missing your meaning here, but there's nothing in the saveAsNewAPIHadoopFile method signature that requires using any deprecated objects or methods.  If there are specific deprecation warnings you have in mind, we can track them down to what in the implementation is causing them? \n\nbq. if we accept Job directly, it can simplify the implementation\nI think it's better to have an API that makes sense than a simple underlying implementation.  Also, there's no reason the implementation needs to construct this Job object.  We could equivalently call conf.set(FileInputFormat.OUT_DIR, path).", "I think Sean is talking about the future when JobConf is deprecated and in current implementation, it is using Configuration\n\nYes, I was saying that we can call conf.set() to avoid new a Job object, but that can only solve the problem in output function (yes...the current implementation needs to be revised); static methods like InputFomat.getSplits() still needs JobContext, so in any way we have to pass Configuration with getConfiguration() of mapreduce.Job and reconstruct  mapreduce.Job with new Job(Configuration conf)....\n\n Anyway, I would like to keep the APIs unchanged based on our discussions and reconsideration about the cost of changing APIs and the benefit brought by reducing two or three lines of code.....", "`JobConf` itself is actually not deprecated; a lot of its API is. I misspoke sort of.\n\nI took this issue to really be about `.mapred.` vs `.mapreduce.` classes. As I understand, the former are not formally deprecated, but are certainly meant to be superseded by the latter. There has been a lot of back-and-forth on this, leading to the current pretty confusing situation, but that's my understanding of the intent.\n\nIt seems better to not mix these two parallel but separate APIs. And if you pick one pick `.mapreduce.`, as I see increasingly the new Hadoop 2.x code using the new package and new property names.\n\nSo, I don't think this is a big issue, but it's bigger than what you originally noted Nan. I do think it's worth touching up everywhere just for sanity and future-proofing. Leave `Job` if we must; I mean the migration to `.mapreduce.`\n\nWhat if I were so bold as to create a patch anyway?", "I think the newHadoopAPI functions are OK, since they are actually using Configuration...As Sandy said, we can revise the current implementation to reduce the chance of getConfiguration, reconstruct Job, and get Configuration and reconstruct again, \n\nSean, do you mean you want to fix saveHadoopFile family functions? I think they are intended to accept JobConf to support those old APIs, I think in future when Hadoop community deprecates those APIs, we only need to add @deprecate there?\n ", "To my surprise, Job in mapreduce package are still using JobConf in some setters, \n\n{code}\nprotected final org.apache.hadoop.mapred.JobConf conf;\n\npublic void setOutputFormatClass(Class<? extends OutputFormat> cls\n                                   ) throws IllegalStateException {\n    ensureState(JobState.DEFINE);\n    conf.setClass(OUTPUT_FORMAT_CLASS_ATTR, cls, OutputFormat.class);\n  }\n\n{code}", "Well, before I say much more, I should actually look at all these occurrences of .mapred. in more detail. From similar surgery on another project I'm aware that there are weird gotchas -- like new APIs using old ones, or some method not yet existing in the new one. \n\nI am not concerned about Job vs JobConf per se but about using .mapreduce. where there is no point in staying with a .mapred. class. \n\nYou can leave this closed and leave it on me to come back at some point with a patch for consideration.", "sure", "I take it back. After looking at the code, most of the usages are in contexts where there is explicitly a method for the old and new APIs. There are a few instances where we could avoid the old API classes altogether, sure. The JobConf situation is a little tricky.\n\nOverall I think there should probably be a later changelist that deletes the \"old API\" versions of methods and then at that time change these other things. Really, the scope of the API issue is small.", "yes, I think we can keep the code as this at the moment, \n\nwhen the old Hadoop API is explicitly deprecated, we just add annotations or sweep out all mapred.* from Spark"], "derived": {"summary": "In the new Hadoop API , mapreduce. *, the JobConf are actually replaced by mapreduce.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "APIs like saveAsNewAPIHadoopFile are actually a mixture of old and new Hadoop API - In the new Hadoop API , mapreduce. *, the JobConf are actually replaced by mapreduce."}, {"q": "What updates or decisions were made in the discussion?", "a": "yes, I think we can keep the code as this at the moment, \n\nwhen the old Hadoop API is explicitly deprecated, we just add annotations or sweep out all mapred.* from Spark"}]}}
{"project": "SPARK", "issue_id": "SPARK-1140", "title": "Remove references to ClusterScheduler", "status": "Resolved", "priority": "Trivial", "reporter": "Kay Ousterhout", "assignee": "Kay Ousterhout", "labels": [], "created": "2014-02-26T16:00:20.000+0000", "updated": "2014-02-26T22:53:27.000+0000", "description": "There are a bunch of references to ClusterScheduler in comments and in the name of tests.  ClusterScheduler has been replaced by TaskSchedulerImpl, so the references should be updated accordingly.", "comments": [], "derived": {"summary": "There are a bunch of references to ClusterScheduler in comments and in the name of tests. ClusterScheduler has been replaced by TaskSchedulerImpl, so the references should be updated accordingly.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Remove references to ClusterScheduler - There are a bunch of references to ClusterScheduler in comments and in the name of tests. ClusterScheduler has been replaced by TaskSchedulerImpl, so the references should be updated accordingly."}]}}
{"project": "SPARK", "issue_id": "SPARK-1141", "title": "Parallelize Task Serialization", "status": "Closed", "priority": "Major", "reporter": "Kay Ousterhout", "assignee": "OuyangJin", "labels": [], "created": "2014-02-26T16:12:11.000+0000", "updated": "2017-02-07T06:45:04.000+0000", "description": "When task closures are large, task serialization becomes the bottleneck in scheduling.  We should parallelize task serialization to alleviate this problem.", "comments": ["I can work on this \nBut @Kay, when you mean Parallelize, you mean use for a single task to paralize its serlization or for a taskSet use multiple threads and each thread working on a single task", "The latter -- have multiple threads that each serialize a particular task.", "Also, I think the first step for this should probably be a short design doc explaining how to do this, because I think it will be a somewhat invasive patch.", "@Kay Thanks for your advice. I'll write a design doc for this and attach it on JIRA. And I can start coding after you think the design is good to go", "Add a initial design doc for it.@Kay. Any advice is welcomed ", "Thanks for writing this up!  I won't be able to look at this until Wednesday but will definitely take a look then.", "@Kay, it doesn't matter. Looking forward to your advice", "I took a look at the design doc -- thanks for writing that up!\n\nYou propose adding a thread pool in each TaskSetManager for doing the serialization; did you think about adding the thread pool in TaskSchedulerImpl instead / what was your reasoning behind having the thread pool in TaskSetManager?\n\nOne use case to consider, if you haven't already, is that when the cluster is very busy (so all or most of the workers are in use), TaskSchedulerImpl.resourceOffer() will typically only get called with an offer for a single worker (because that worker just finished a task, and all of the other workers are busy).  We should make sure that, even in this case, the task serialization can be parallelized.", "@Kay.Thanks very much for your review advice\nOf course we can put this thread pool inside TaskSchedulerImpl, and I think this solution is more or less neatly than putting it inside TaskSetManager.\nI really didn't consider your use case. Actually  my original solution wants to create a asynchronized status between TaskSetManager and TaskSchedulerImpl, that is , it that loop , everytime we want    serialize a task, we add it to TaskSetManager's own threadpool, and when the loop is finished, this threadpool has all wanted \" to be serialized \"task. and we have to wait for all serialization worker thread finish and record some states. I have to admit that my original solution is somewhat complicated. And put a threadpool inside TaskSchedulerImpl is simpler and neatly. Sorry for that.\n\nSo if we put threadpool inside TaskSchedulerImpl, my  implementation will be like this: TaskSetManager.resourceOffer will return a TaskDescWithoutSerializeTask object , this object will be a half-copy of TaskDescrption exception _serializedTask ByteBffer, instead, it will contain a Task object and seriailze part inside TaskSetManager.resourceOffer will be moved to TaskSchedulerImpl's \"Runnable\" working thread which will be placed inside threadpool.\n@Kay,do you think this design make sense. Today I implmented mainly part of dev code, and I can send this PR which is WIP. And I think we should add test case for this change\nThanks!\n", "This sounds like a good plan and submitting the WIP PR sounds great!  Thanks!", "Any update on this?  I'd like this to get into 1.0; if you don't have time to work on this more I'll have some time to work on it Wednesday and can submit a patch.", "sorry for late reply .I work on this the whole last weekend ,and one test case just failed, and I 'm not sure if it's related to my patch becuause lack of time. I can send my PR and you can review it first. And I 'll look into the fail case later today", "PR is https://github.com/apache/spark/pull/214\n", "Sorry again for late respons(just thinking of post the PR when all test case passed)", "PR updated, failed case DriverSuite has already pass", "Because Spark now broadcasts the task binary, task serialization is no longer a bottleneck in the scheduler, and implementing this feature adds significant scheduler complexity."], "derived": {"summary": "When task closures are large, task serialization becomes the bottleneck in scheduling. We should parallelize task serialization to alleviate this problem.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Parallelize Task Serialization - When task closures are large, task serialization becomes the bottleneck in scheduling. We should parallelize task serialization to alleviate this problem."}, {"q": "What updates or decisions were made in the discussion?", "a": "Because Spark now broadcasts the task binary, task serialization is no longer a bottleneck in the scheduler, and implementing this feature adds significant scheduler complexity."}]}}
{"project": "SPARK", "issue_id": "SPARK-1142", "title": "Allow adding jars on app submission, outside of code", "status": "Resolved", "priority": "Major", "reporter": "Sanford Ryza", "assignee": "Sandy Ryza", "labels": [], "created": "2014-02-26T17:09:55.000+0000", "updated": "2015-02-09T18:45:58.000+0000", "description": "yarn-standalone mode supports an option that allows adding jars that will be distributed on the cluster with job submission.  Providing similar functionality for other app submission modes will allow the spark-app script proposed in SPARK-1126 to support an add-jars option that works for every submit mode.", "comments": ["This already exists - you can use the --jars flag to spark-submit or set 'spark.jars' manually."], "derived": {"summary": "yarn-standalone mode supports an option that allows adding jars that will be distributed on the cluster with job submission. Providing similar functionality for other app submission modes will allow the spark-app script proposed in SPARK-1126 to support an add-jars option that works for every submit mode.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Allow adding jars on app submission, outside of code - yarn-standalone mode supports an option that allows adding jars that will be distributed on the cluster with job submission. Providing similar functionality for other app submission modes will allow the spark-app script proposed in SPARK-1126 to support an add-jars option that works for every submit mode."}, {"q": "What updates or decisions were made in the discussion?", "a": "This already exists - you can use the --jars flag to spark-submit or set 'spark.jars' manually."}]}}
{"project": "SPARK", "issue_id": "SPARK-1143", "title": "ClusterSchedulerSuite (soon to be TaskSchedulerImplSuite) does not actually test the ClusterScheduler/TaskSchedulerImpl", "status": "Resolved", "priority": "Minor", "reporter": "Kay Ousterhout", "assignee": "Kay Ousterhout", "labels": [], "created": "2014-02-26T17:15:59.000+0000", "updated": "2015-01-09T17:47:33.000+0000", "description": "This test should probably be both refactored and renamed -- it really tests the Pool / fair scheduling mechanisms and completely bypasses the scheduling code in TaskSchedulerImpl and TaskSetManager.", "comments": ["made  a PR: https://github.com/apache/spark/pull/339\n", "User 'kayousterhout' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/3967"], "derived": {"summary": "This test should probably be both refactored and renamed -- it really tests the Pool / fair scheduling mechanisms and completely bypasses the scheduling code in TaskSchedulerImpl and TaskSetManager.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "ClusterSchedulerSuite (soon to be TaskSchedulerImplSuite) does not actually test the ClusterScheduler/TaskSchedulerImpl - This test should probably be both refactored and renamed -- it really tests the Pool / fair scheduling mechanisms and completely bypasses the scheduling code in TaskSchedulerImpl and TaskSetManager."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'kayousterhout' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/3967"}]}}
{"project": "SPARK", "issue_id": "SPARK-1145", "title": "Memory mapping with many small blocks can cause JVM allocation failures", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "labels": [], "created": "2014-02-27T00:20:03.000+0000", "updated": "2017-12-11T12:59:19.000+0000", "description": "During a shuffle each block or block segment is memory mapped to a file. When the segments are very small and there are a large number of them, the memory maps can start failing and eventually the JVM will terminate. It's not clear exactly what's happening but it appears that when the JVM terminates about 265MB of virtual address space is used by memory mapped files. This doesn't seem affected at all by `-XXmaxdirectmemorysize` - AFAIK that option is just to give the JVM its own self imposed limit rather than allow it to run into OS limits. \n\nAt the time of JVM failure it appears the overall OS memory becomes scarce, so it's possible there are overheads for each memory mapped file that are adding up here. One overhead is that the memory mapping occurs at the granularity of pages, so if blocks are really small there is natural overhead required to pad to the page boundary.\n\nIn the particular case where I saw this, the JVM was running 4 reducers, each of which was trying to access about 30,000 blocks for a total of 120,000 concurrent reads. At about 65,000 open files it crapped out. In this case each file was about 1000 bytes.\n\nUser should really be coalescing or using fewer reducers if they have 1000 byte shuffle files, but I expect this to happen nonetheless. My proposal was that if the file is smaller than a few pages, we should just read it into a java buffer and not bother to memory map it. Memory mapping huge numbers of small files in the JVM is neither recommended or good for performance, AFAIK.\n\nBelow is the stack trace:\n{code}\n14/02/27 08:32:35 ERROR storage.BlockManagerWorker: Exception handling buffer message\njava.io.IOException: Map failed\n  at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:888)\n  at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:89)\n  at org.apache.spark.storage.BlockManager.getLocalBytes(BlockManager.scala:285)\n  at org.apache.spark.storage.BlockManagerWorker.getBlock(BlockManagerWorker.scala:90)\n  at org.apache.spark.storage.BlockManagerWorker.processBlockMessage(BlockManagerWorker.scala:69)\n  at org.apache.spark.storage.BlockManagerWorker$$anonfun$2.apply(BlockManagerWorker.scala:44)\n  at org.apache.spark.storage.BlockManagerWorker$$anonfun$2.apply(BlockManagerWorker.scala:44)\n  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n  at scala.collection.Iterator$class.foreach(Iterator.scala:727)\n  at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n  at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n  at org.apache.spark.storage.BlockMessageArray.foreach(BlockMessageArray.scala:28)\n  at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n  at org.apache.spark.storage.BlockMessageArray.map(BlockMessageArray.scala:28)\n  at org.apache.spark.storage.BlockManagerWorker.onBlockMessageReceive(BlockManagerWorker.scala:44)\n  at org.apache.spark.storage.BlockManagerWorker$$anonfun$1.apply(BlockManagerWorker.scala:34)\n  at org.apache.spark.storage.BlockManagerWorker$$anonfun$1.apply(BlockManagerWorker.scala:34)\n  at org.apache.spark.network.ConnectionManager.org$apache$spark$network$ConnectionManager$$handleMessage(ConnectionManager.scala:512)\n  at org.apache.spark.network.ConnectionManager$$anon$8.run(ConnectionManager.scala:478)\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n{code}\n\nAnd the JVM error log had a bunch of entries like this:\n\n{code}\n7f4b48f89000-7f4b48f8a000 r--s 00000000 ca:30 1622077901                 /mnt4/spark/spark-local-20140227020022-227c/26/shuffle_0_22312_38\n7f4b48f8a000-7f4b48f8b000 r--s 00000000 ca:20 545892715                  /mnt3/spark/spark-local-20140227020022-5ef5/3a/shuffle_0_26808_20\n7f4b48f8b000-7f4b48f8c000 r--s 00000000 ca:50 1622480741                 /mnt2/spark/spark-local-20140227020022-315b/1c/shuffle_0_29013_19\n7f4b48f8c000-7f4b48f8d000 r--s 00000000 ca:30 10082610                   /mnt4/spark/spark-local-20140227020022-227c/3b/shuffle_0_28002_9\n7f4b48f8d000-7f4b48f8e000 r--s 00000000 ca:50 1622268539                 /mnt2/spark/spark-local-20140227020022-315b/3e/shuffle_0_23983_17\n7f4b48f8e000-7f4b48f8f000 r--s 00000000 ca:50 1083068239                 /mnt2/spark/spark-local-20140227020022-315b/37/shuffle_0_25505_22\n7f4b48f8f000-7f4b48f90000 r--s 00000000 ca:30 9921006                    /mnt4/spark/spark-local-20140227020022-227c/31/shuffle_0_24072_95\n7f4b48f90000-7f4b48f91000 r--s 00000000 ca:50 10441349                   /mnt2/spark/spark-local-20140227020022-315b/20/shuffle_0_27409_47\n7f4b48f91000-7f4b48f92000 r--s 00000000 ca:50 10406042                   /mnt2/spark/spark-local-20140227020022-315b/0e/shuffle_0_26481_84\n7f4b48f92000-7f4b48f93000 r--s 00000000 ca:50 1622268192                 /mnt2/spark/spark-local-20140227020022-315b/14/shuffle_0_23818_92\n7f4b48f93000-7f4b48f94000 r--s 00000000 ca:50 1082957628                 /mnt2/spark/spark-local-20140227020022-315b/09/shuffle_0_22824_45\n7f4b48f94000-7f4b48f95000 r--s 00000000 ca:20 1082199965                 /mnt3/spark/spark-local-20140227020022-5ef5/00/shuffle_0_1429_13\n7f4b48f95000-7f4b48f96000 r--s 00000000 ca:20 10940995                   /mnt3/spark/spark-local-20140227020022-5ef5/38/shuffle_0_28705_44\n7f4b48f96000-7f4b48f97000 r--s 00000000 ca:10 17456971                   /mnt/spark/spark-local-20140227020022-b372/28/shuffle_0_23055_72\n7f4b48f97000-7f4b48f98000 r--s 00000000 ca:30 9853895                    /mnt4/spark/spark-local-20140227020022-227c/08/shuffle_0_22797_42\n7f4b48f98000-7f4b48f99000 r--s 00000000 ca:20 1622089728                 /mnt3/spark/spark-local-20140227020022-5ef5/27/shuffle_0_24017_97\n7f4b48f99000-7f4b48f9a000 r--s 00000000 ca:50 1082937570                 /mnt2/spark/spark-local-20140227020022-315b/24/shuffle_0_22291_38\n7f4b48f9a000-7f4b48f9b000 r--s 00000000 ca:30 10056604                   /mnt4/spark/spark-local-20140227020022-227c/2f/shuffle_0_27408_59\n{code}", "comments": ["Issue resolved by pull request 43\n[https://github.com/apache/spark/pull/43]"], "derived": {"summary": "During a shuffle each block or block segment is memory mapped to a file. When the segments are very small and there are a large number of them, the memory maps can start failing and eventually the JVM will terminate.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Memory mapping with many small blocks can cause JVM allocation failures - During a shuffle each block or block segment is memory mapped to a file. When the segments are very small and there are a large number of them, the memory maps can start failing and eventually the JVM will terminate."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 43\n[https://github.com/apache/spark/pull/43]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1146", "title": "Vagrant to setup Spark cluster locally", "status": "Resolved", "priority": "Major", "reporter": "Binh Nguyen", "assignee": null, "labels": ["script"], "created": "2014-02-27T00:43:16.000+0000", "updated": "2014-12-10T15:50:02.000+0000", "description": "We should use Vagrant to create a local clusters of VMs. It will allow developers run and test Spark Cluster on their dev machines.\n\nIt could be expanded to YARN and Mesos cluster mode but initial focus will be on standalone.", "comments": ["ticket for this PR: https://github.com/apache/spark/pull/26", "(Warning: I've developed a little script to help find JIRAs whose PRs are resolved one way or the other, and so should be resolved. There may be a number of these coming in the next day or two.)\n\nThe discussion in the PR indicates this will be a separate project if anything, currently hosted at https://github.com/ngbinh/spark-vagrant"], "derived": {"summary": "We should use Vagrant to create a local clusters of VMs. It will allow developers run and test Spark Cluster on their dev machines.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Vagrant to setup Spark cluster locally - We should use Vagrant to create a local clusters of VMs. It will allow developers run and test Spark Cluster on their dev machines."}, {"q": "What updates or decisions were made in the discussion?", "a": "(Warning: I've developed a little script to help find JIRAs whose PRs are resolved one way or the other, and so should be resolved. There may be a number of these coming in the next day or two.)\n\nThe discussion in the PR indicates this will be a separate project if anything, currently hosted at https://github.com/ngbinh/spark-vagrant"}]}}
{"project": "SPARK", "issue_id": "SPARK-1147", "title": "spark-project.org still goes to http://spark.incubator.apache.org/", "status": "Resolved", "priority": "Major", "reporter": "Jyotiska NK", "assignee": "Andy Konwinski", "labels": [], "created": "2014-02-27T01:56:04.000+0000", "updated": "2014-03-04T14:08:23.000+0000", "description": "http://spark-project.org/ still leads to http://spark.incubator.apache.org/, forwarding should be updated to http://spark.apache.org/", "comments": ["This is fixed now any traffic going to spark-project.org/PATH goes to spark.apache.org/PATH."], "derived": {"summary": "http://spark-project. org/ still leads to http://spark.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "spark-project.org still goes to http://spark.incubator.apache.org/ - http://spark-project. org/ still leads to http://spark."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is fixed now any traffic going to spark-project.org/PATH goes to spark.apache.org/PATH."}]}}
{"project": "SPARK", "issue_id": "SPARK-1148", "title": "Suggestions for exception handling (avoid potential bugs)", "status": "Resolved", "priority": "Major", "reporter": "Ding Yuan", "assignee": null, "labels": [], "created": "2014-02-27T17:23:27.000+0000", "updated": "2014-12-16T22:10:05.000+0000", "description": "Hi Spark developers,\nWe are a group of researchers on software reliability. Recently we did a study and found that majority of the most severe failures in data-analytic systems are caused by bugs in exception handlers  that it is hard to anticipate all the possible real-world error scenarios. Therefore we built a simple checking tool that automatically detects some bug patterns that have caused some very severe real-world failures. I am reporting a few cases here. Any feedback is much appreciated!\n\nDing\n\n=========================\nCase 1:\nLine: 1249, File: \"org/apache/spark/SparkContext.scala\"\n{noformat}\n1244:         val scheduler = try {\n1245:           val clazz = Class.forName(\"org.apache.spark.scheduler.cluster.YarnClusterScheduler\")\n1246:           val cons = clazz.getConstructor(classOf[SparkContext])\n1247:           cons.newInstance(sc).asInstanceOf[TaskSchedulerImpl]\n1248:         } catch {\n1249:           // TODO: Enumerate the exact reasons why it can fail\n1250:           // But irrespective of it, it means we cannot proceed !\n1251:           case th: Throwable => {\n1252:             throw new SparkException(\"YARN mode not available ?\", th)\n1253:           }\n{noformat}\n\nThe comment suggests the specific exceptions should be enumerated here.\nThe try block could throw the following exceptions:\n\nClassNotFoundException\nNegativeArraySizeException\nNoSuchMethodException\nSecurityException\nInstantiationException\nIllegalAccessException\nIllegalArgumentException\nInvocationTargetException\nClassCastException\n==========================================\n=========================\nCase 2:\nLine: 282, File: \"org/apache/spark/executor/Executor.scala\"\n{noformat}\n265:         case t: Throwable => {\n266:           val serviceTime = (System.currentTimeMillis() - taskStart).toInt\n267:           val metrics = attemptedTask.flatMap(t => t.metrics)\n268:           for (m <- metrics) {\n269:             m.executorRunTime = serviceTime\n270:             m.jvmGCTime = gcTime - startGCTime\n271:           }\n272:           val reason = ExceptionFailure(t.getClass.getName, t.toString, t.getStackTrace, metrics)\n273:           execBackend.statusUpdate(taskId, TaskState.FAILED, ser.serialize(reason))\n274:\n275:           // TODO: Should we exit the whole executor here? On the one hand, the failed task may\n276:           // have left some weird state around depending on when the exception was thrown, but on\n277:           // the other hand, maybe we could detect that when future tasks fail and exit then.\n278:           logError(\"Exception in task ID \" + taskId, t)\n279:           //System.exit(1)\n280:         }\n281:       } finally {\n282:         // TODO: Unregister shuffle memory only for ResultTask\n283:         val shuffleMemoryMap = env.shuffleMemoryMap\n284:         shuffleMemoryMap.synchronized {\n285:           shuffleMemoryMap.remove(Thread.currentThread().getId)\n286:         }\n287:         runningTasks.remove(taskId)\n288:       }\n{noformat}\n\nFrom the comment in this Throwable exception handler it seems to suggest that the system should just exit?\n==========================================\n=========================\nCase 3:\nLine: 70, File: \"org/apache/spark/network/netty/FileServerHandler.java\"\n\n{noformat}\n66:       try {\n67:         ctx.write(new DefaultFileRegion(new FileInputStream(file)\n68:           .getChannel(), fileSegment.offset(), fileSegment.length()));\n69:       } catch (Exception e) {\n70:           LOG.error(\"Exception: \", e);\n71:       }\n{noformat}\n\n\"Exception\" is too general. The try block only throws \"FileNotFoundException\".\nAlthough there is nothing wrong with it now, but later if code evolves this\nmight cause some other exceptions to be swallowed.\n==========================================", "comments": ["@Ding Yuan,  First of all, nice work\nI can track this JIRA if any admins is willing to do this , and can put me as Assignee\nThere are a few details I'm curious about the analysis result from your rearch team  @Ding\n1 All 3 cases you metioned are about Exception handling. I wonder the code analysis tool your team buildl to analysis spark code is concentrate on analying Exception handling? And how many kind of exception handling your tool can analysis?(what I can see in the case is over catching , detailed catching and unexceped exit in handling exception)\n2 Your tool is whether static analysis(not running spark when analying) or runtime analysis(running spark when analying). If in runtime analying,  what 's your runtime env\n3 The coverage range for your tool. Only in core module or all spark modules\nThanks for your work!", "Hey thanks for the response [~qqsun8819], it would be great if you could help to fix these. \nRe your questions:\n1. Yes, our tool focus on some trivial buggy patterns in exception handling code where we found caused some catastrophic failures. The patterns you mentioned are exactly the ones our tool tries to detect. \n2. It's a static analysis tool on byte code. \n3. It covers all jar files, and i believe i ran it on all spark modules.\nIn fact, we are planning to open-source it soon after some packaging. if you're interested I will let you know once we have done this.\n\ncheers,", "@Ding Great.Looking forward to your further plan. And you can attach a complete report generate by your tool in this JIRA(if not violate any copyright for your research team) so that I can get a complete graph of this kind of problem in spark code. And I'm very pleased to be notified by your further progress of your tool , at any time . \nThanks!", "In fact, the above mentioned cases are all we have found. In the software projects we scanned so far, Spark actually has the fewest warnings. ", "Good.I'll check current case you reported", "[~d.yuan] Several problems like this have been fixed since. Can you open a PR to resolve any others that your tool has found or is it all fixed now?", "Glad to know these cases are addressed! As for version 0.9.0, I have reported all the cases (only three of them). I have opened another JIRA: https://issues.apache.org/jira/browse/SPARK-4863 to report the warnings on spark-1.1.1. ", "[~d.yuan] I don't know that these have been addressed; some issues like it have been, if I recall correctly. Whatever the status, I think it's useful to focus on master and/or later branches. Would it be OK to close this in favor of SPARK-4863?", "of course, please just close this one then. Thanks!"], "derived": {"summary": "Hi Spark developers,\nWe are a group of researchers on software reliability. Recently we did a study and found that majority of the most severe failures in data-analytic systems are caused by bugs in exception handlers  that it is hard to anticipate all the possible real-world error scenarios.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Suggestions for exception handling (avoid potential bugs) - Hi Spark developers,\nWe are a group of researchers on software reliability. Recently we did a study and found that majority of the most severe failures in data-analytic systems are caused by bugs in exception handlers  that it is hard to anticipate all the possible real-world error scenarios."}, {"q": "What updates or decisions were made in the discussion?", "a": "of course, please just close this one then. Thanks!"}]}}
{"project": "SPARK", "issue_id": "SPARK-1149", "title": "Bad partitioners can cause Spark to hang", "status": "Resolved", "priority": "Minor", "reporter": "Bryn Keller", "assignee": null, "labels": [], "created": "2014-02-27T21:00:03.000+0000", "updated": "2014-10-13T18:05:14.000+0000", "description": "While implementing a unit test for lookup, I accidentally created a situation where a partitioner returned a partition number that was outside its range. It should have returned 0 or 1, but in the last case, it returned a -1. \n\nRather than reporting the problem via an exception, Spark simply hangs during the unit test run.\n\nWe should catch this bad behavior by partitioners and throw an exception.\n\ntest(\"lookup with bad partitioner\") {\n    val pairs = sc.parallelize(Array((1,2), (3,4), (5,6), (5,7)))\n\n    val p = new Partitioner {\n      def numPartitions: Int = 2\n\n      def getPartition(key: Any): Int = key.hashCode() % 2\n    }\n    val shuffled = pairs.partitionBy(p)\n\n    assert(shuffled.partitioner === Some(p))\n    assert(shuffled.lookup(1) === Seq(2))\n    assert(shuffled.lookup(5) === Seq(6,7))\n    assert(shuffled.lookup(-1) === Seq())\n  }", "comments": ["I opened a pull request:[#44|https://github.com/apache/spark/pull/44] . Spark should not to hang.", "Looks like Patrick merged this into master in March. It might have been fixed for ... 1.0?"], "derived": {"summary": "While implementing a unit test for lookup, I accidentally created a situation where a partitioner returned a partition number that was outside its range. It should have returned 0 or 1, but in the last case, it returned a -1.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Bad partitioners can cause Spark to hang - While implementing a unit test for lookup, I accidentally created a situation where a partitioner returned a partition number that was outside its range. It should have returned 0 or 1, but in the last case, it returned a -1."}, {"q": "What updates or decisions were made in the discussion?", "a": "Looks like Patrick merged this into master in March. It might have been fixed for ... 1.0?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1150", "title": "repo location in create_release script out of date", "status": "Resolved", "priority": "Minor", "reporter": "Nan Zhu", "assignee": "Nan Zhu", "labels": [], "created": "2014-02-28T07:56:02.000+0000", "updated": "2014-03-01T17:27:31.000+0000", "description": "repo location in create_release script out of date", "comments": ["https://github.com/apache/spark/pull/48"], "derived": {"summary": "repo location in create_release script out of date.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "repo location in create_release script out of date - repo location in create_release script out of date."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/48"}]}}
{"project": "SPARK", "issue_id": "SPARK-1151", "title": "dev merge_spark_pr.py still references incubator-spark", "status": "Resolved", "priority": "Major", "reporter": "Thomas Graves", "assignee": "Thomas Graves", "labels": [], "created": "2014-02-28T09:04:11.000+0000", "updated": "2014-02-28T18:28:54.000+0000", "description": "We need to update the script to use spark.git instead of incubator-spark.git\n", "comments": [], "derived": {"summary": "We need to update the script to use spark. git instead of incubator-spark.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "dev merge_spark_pr.py still references incubator-spark - We need to update the script to use spark. git instead of incubator-spark."}]}}
{"project": "SPARK", "issue_id": "SPARK-1152", "title": "ArrayStoreException on mapping RDD on cluster", "status": "Resolved", "priority": "Blocker", "reporter": "Andrew Kerr", "assignee": null, "labels": [], "created": "2014-02-28T10:50:16.000+0000", "updated": "2014-03-28T10:31:21.000+0000", "description": "With this code:\n{code}\nimport org.apache.spark.{SparkConf, SparkContext, Partitioner}\nimport org.apache.spark.SparkContext._\n\nobject twitterAggregation extends App {\n\n  val conf = new SparkConf()\n      .setMaster(\"spark://ec2-x-x-x-x.compute-1.amazonaws.com:7077\")\n      //.setMaster(\"local\")\n      .setAppName(\"foo\")\n      .setJars(List(\"target/scala-2.10/foo_2.10-0.0.1.jar\"))\n      .setSparkHome(\"/root/spark/\")\n  val sc = new SparkContext(conf)\n  sc.parallelize(Seq(\"b\")).map(identity).collect\n}\n{code}\n\nI get this:\n{code}\n14/02/28 18:41:10 WARN scheduler.TaskSetManager: Lost TID 1 (task 0.0:1)\n14/02/28 18:41:10 INFO scheduler.DAGScheduler: Completed ResultTask(0, 0)\n14/02/28 18:41:10 WARN scheduler.TaskSetManager: Loss was due to java.lang.ArrayStoreException\njava.lang.ArrayStoreException: [Ljava.lang.String;\n\tat scala.runtime.ScalaRunTime$.array_update(ScalaRunTime.scala:88)\n\tat scala.Array$.slowcopy(Array.scala:81)\n\tat scala.Array$.copy(Array.scala:107)\n\tat scala.collection.mutable.ResizableArray$class.copyToArray(ResizableArray.scala:77)\n\tat scala.collection.mutable.ArrayBuffer.copyToArray(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.copyToArray(TraversableOnce.scala:241)\n\tat scala.collection.AbstractTraversable.copyToArray(Traversable.scala:105)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:249)\n\tat scala.collection.AbstractTraversable.toArray(Traversable.scala:105)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1157)\n\tat org.apache.spark.rdd.RDD$$anonfun$4.apply(RDD.scala:602)\n\tat org.apache.spark.rdd.RDD$$anonfun$4.apply(RDD.scala:602)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:884)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:884)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:53)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)\n\tat org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\n{code}\n\nThis only happens when running against a cluster as an app (sbt package && sbt play).\n\nWith a master of \"local\", or running on the spark shell on a cluster, code runs without error.", "comments": ["cannot reproduce it...\n\n{code}\nobject SparkTest {\n  def main(args: Array[String]) {\n    val conf = new SparkConf()\n      //.setMaster(\"spark://127.0.0.1:7077\")\n      .setMaster(\"local\")\n      .setAppName(\"foo\")\n      .setJars(List(\"target/scala-2.10/spark_test_2.10-1.0.jar\"))\n      .setSparkHome(\"/root/spark/\")\n    val sc = new SparkContext(conf)\n    sc.parallelize(Seq(\"b\")).map(identity).collect\n  }\n}\n{code}\n\n{code}\nNans-MacBook-Pro:spark_test nanzhu$ sbt run \nLoading /Users/nanzhu/Downloads/sbt/bin/sbt-launch-lib.bash\n[info] Loading global plugins from /Users/nanzhu/.sbt/0.13/plugins\n[info] Set current project to spark_test (in build file:/Users/nanzhu/code/spark_test/spark_test/)\n[info] Compiling 1 Scala source to /Users/nanzhu/code/spark_test/spark_test/target/scala-2.10/classes...\n[info] Running SparkTest \n14/02/28 14:24:09 INFO slf4j.Slf4jLogger: Slf4jLogger started\n14/02/28 14:24:09 INFO Remoting: Starting remoting\n14/02/28 14:24:09 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://spark@192.168.2.15:52451]\n14/02/28 14:24:09 INFO Remoting: Remoting now listens on addresses: [akka.tcp://spark@192.168.2.15:52451]\n14/02/28 14:24:09 INFO spark.SparkEnv: Registering BlockManagerMaster\n14/02/28 14:24:09 INFO storage.DiskBlockManager: Created local directory at /var/folders/hs/28p73t310w931t4n21d954_80000gn/T/spark-local-20140228142409-a829\n14/02/28 14:24:09 INFO storage.MemoryStore: MemoryStore started with capacity 589.2 MB.\n14/02/28 14:24:09 INFO network.ConnectionManager: Bound socket to port 52452 with id = ConnectionManagerId(192.168.2.15,52452)\n14/02/28 14:24:09 INFO storage.BlockManagerMaster: Trying to register BlockManager\n14/02/28 14:24:09 INFO storage.BlockManagerMasterActor$BlockManagerInfo: Registering block manager 192.168.2.15:52452 with 589.2 MB RAM\n14/02/28 14:24:09 INFO storage.BlockManagerMaster: Registered BlockManager\n14/02/28 14:24:09 INFO spark.HttpServer: Starting HTTP Server\n14/02/28 14:24:09 INFO server.Server: jetty-7.6.8.v20121106\n14/02/28 14:24:09 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:52453\n14/02/28 14:24:09 INFO broadcast.HttpBroadcast: Broadcast server started at http://192.168.2.15:52453\n14/02/28 14:24:09 INFO spark.SparkEnv: Registering MapOutputTracker\n14/02/28 14:24:09 INFO spark.HttpFileServer: HTTP File server directory is /var/folders/hs/28p73t310w931t4n21d954_80000gn/T/spark-ae2ccb3e-f492-446e-8769-b38345233e87\n14/02/28 14:24:09 INFO spark.HttpServer: Starting HTTP Server\n14/02/28 14:24:09 INFO server.Server: jetty-7.6.8.v20121106\n14/02/28 14:24:09 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:52454\n14/02/28 14:24:10 INFO server.Server: jetty-7.6.8.v20121106\n14/02/28 14:24:10 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/storage/rdd,null}\n14/02/28 14:24:10 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/storage,null}\n14/02/28 14:24:10 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/stages/stage,null}\n14/02/28 14:24:10 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/stages/pool,null}\n14/02/28 14:24:10 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/stages,null}\n14/02/28 14:24:10 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/environment,null}\n14/02/28 14:24:10 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/executors,null}\n14/02/28 14:24:10 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/metrics/json,null}\n14/02/28 14:24:10 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/static,null}\n14/02/28 14:24:10 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/,null}\n14/02/28 14:24:10 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040\n14/02/28 14:24:10 INFO ui.SparkUI: Started Spark Web UI at http://192.168.2.15:4040\n2014-02-28 14:24:10.219 java[14706:5f07] Unable to load realm info from SCDynamicStore\n14/02/28 14:24:25 INFO spark.SparkContext: Added JAR target/scala-2.10/spark_test_2.10-1.0.jar at http://192.168.2.15:52454/jars/spark_test_2.10-1.0.jar with timestamp 1393615465324\n14/02/28 14:24:25 INFO spark.SparkContext: Starting job: collect at SparkTest.scala:12\n14/02/28 14:24:25 INFO scheduler.DAGScheduler: Got job 0 (collect at SparkTest.scala:12) with 1 output partitions (allowLocal=false)\n14/02/28 14:24:25 INFO scheduler.DAGScheduler: Final stage: Stage 0 (collect at SparkTest.scala:12)\n14/02/28 14:24:25 INFO scheduler.DAGScheduler: Parents of final stage: List()\n14/02/28 14:24:25 INFO scheduler.DAGScheduler: Missing parents: List()\n14/02/28 14:24:25 INFO scheduler.DAGScheduler: Submitting Stage 0 (MappedRDD[1] at map at SparkTest.scala:12), which has no missing parents\n14/02/28 14:24:25 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from Stage 0 (MappedRDD[1] at map at SparkTest.scala:12)\n14/02/28 14:24:25 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks\n14/02/28 14:24:25 INFO scheduler.TaskSetManager: Starting task 0.0:0 as TID 0 on executor localhost: localhost (PROCESS_LOCAL)\n14/02/28 14:24:25 INFO scheduler.TaskSetManager: Serialized task 0.0:0 as 1389 bytes in 19 ms\n14/02/28 14:24:25 INFO executor.Executor: Running task ID 0\n14/02/28 14:24:25 INFO executor.Executor: Fetching http://192.168.2.15:52454/jars/spark_test_2.10-1.0.jar with timestamp 1393615465324\n14/02/28 14:24:25 INFO util.Utils: Fetching http://192.168.2.15:52454/jars/spark_test_2.10-1.0.jar to /var/folders/hs/28p73t310w931t4n21d954_80000gn/T/fetchFileTemp1181057927078676157.tmp\n14/02/28 14:24:26 INFO executor.Executor: Adding file:/var/folders/hs/28p73t310w931t4n21d954_80000gn/T/spark-1b6e6349-ef69-4b02-a9da-4ba05beb2371/spark_test_2.10-1.0.jar to class loader\n14/02/28 14:24:26 INFO executor.Executor: Serialized size of result for 0 is 529\n14/02/28 14:24:26 INFO executor.Executor: Sending result for 0 directly to driver\n14/02/28 14:24:26 INFO executor.Executor: Finished task ID 0\n14/02/28 14:24:26 INFO scheduler.TaskSetManager: Finished TID 0 in 400 ms on localhost (progress: 0/1)\n14/02/28 14:24:26 INFO scheduler.TaskSchedulerImpl: Remove TaskSet 0.0 from pool \n14/02/28 14:24:26 INFO scheduler.DAGScheduler: Completed ResultTask(0, 0)\n14/02/28 14:24:26 INFO scheduler.DAGScheduler: Stage 0 (collect at SparkTest.scala:12) finished in 0.419 s\n14/02/28 14:24:26 INFO spark.SparkContext: Job finished: collect at SparkTest.scala:12, took 0.54146 s\nNot interrupting system thread Thread[Keep-Alive-Timer,8,system]\n14/02/28 14:24:26 INFO network.ConnectionManager: Selector thread was interrupted!\n[success] Total time: 22 s, completed 28-Feb-2014 2:24:26 PM\n\n{code}", "Let me reiterate: this code works just fine with a master of \"local\". The exception occurs when the master is the master of a cluster (in my case 4 slaves, built using Spark's ec2 scripts).", "I just tried to reproduce this on master using a local cluster but no luck:\n\n{code}\n$ ./bin/spark-class org.apache.spark.deploy.master.Master\n$ ./bin/spark-class org.apache.spark.deploy.worker.Worker <spark cluster url>\n$ MASTER=<spark cluster url> ./bin/spark-shell\nscala> sc.parallelize(Seq(\"b\")).map(identity).collect\nres3: Array[String] = Array(b)\n\n{code}", "run in a ec2 cluster with 5 slaves, no luck..........:-) ", "[~andrewkerr] - I wonder if maybe there is some dependency conflict or issue with the code you are keeping inside of your jar that is causing this to fail. Are there any helpful error messages in the executor logs (in spark/work on the slave)?", "I'm sorry, I just realised I hadn't specified that I have not had this problem when using the Spark shell. I seem to have thought \"shell\" and typed \"cluster\". I've had this problem only an using app and only pointing at a cluster, not local.\n\nI'll update the report with more details soon.", "And now I can't reproduce this myself.", "I'm wondering if somehow the slaves received a corrupted, old or inconsistent (set of) jar(s). This would explain why there is no problem in local mode. Come to think of it, line numbers in exceptions didn't necessarily match up with those in the source.", "I reproduced it last night with master of local[4].   It doesn't happen all the time.\n\nSee:  https://github.com/ooyala/spark-jobserver\nBranch:  velvia/fix-classloading-bug\nCommit:  e58c789\n\nRun:  sbt  job-server/test\nFailing test is in JobManagerSpec:181, \"should properly serialize case classes and other job jar classes\"\nSee the test output log in  job-server-test.log for the ArrayStoreException.  \n\n-Evan", "This was fixed by a patch that went in to master recently. I'll mark as duplicate.", "Here is the exception stack trace from the unit test output:\n\n[2014-03-28 10:21:08,608] INFO  .apache.spark.SparkContext [] [] - Successfully stopped SparkContext\n[2014-03-28 10:21:08,609] INFO  rovider$RemotingTerminator [] [akka://spark/system/remoting-terminator] - Shutting down remote daemon.\njava.lang.ArrayStoreException: [Lspark.jobserver.Animal;\n        at scala.runtime.ScalaRunTime$.array_update(ScalaRunTime.scala:93)\n        at org.apache.spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:870)\n        at org.apache.spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:870)\n        at org.apache.spark.scheduler.JobWaiter.taskSucceeded(JobWaiter.scala:56)\n        at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:859)\n        at org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:616)\n        at org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:207)\n        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)\n        at akka.actor.ActorCell.invoke(ActorCell.scala:456)\n        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)\n        at akka.dispatch.Mailbox.run(Mailbox.scala:219)\n        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:385)\n        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n\nThis might be related to serialization.  I don't think Scala case classes by default extend Serializable...", "Patrick, do you have the patch / PR #, and will this be backported to 0.9.1?", "I linked the JIRA here - we'll probably wait until 0.9.2 because it's an invasive change involving classloaders."], "derived": {"summary": "With this code:\n{code}\nimport org. apache.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "ArrayStoreException on mapping RDD on cluster - With this code:\n{code}\nimport org. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "I linked the JIRA here - we'll probably wait until 0.9.2 because it's an invasive change involving classloaders."}]}}
{"project": "SPARK", "issue_id": "SPARK-1153", "title": "Generalize VertexId in GraphX so that UUIDs can be used as vertex IDs.", "status": "Resolved", "priority": "Major", "reporter": "Deepak Nulu", "assignee": null, "labels": [], "created": "2014-02-28T10:51:24.000+0000", "updated": "2017-12-07T03:12:09.000+0000", "description": "Currently, {{VertexId}} is a type-synonym for {{Long}}. I would like to be able to use {{UUID}} as the vertex ID type because the data I want to process with GraphX uses that type for its primay-keys. Others might have a different type for their primary-keys. Generalizing {{VertexId}} (with a type class) will help in such cases.", "comments": ["The following description of {{VertexId}}:\n\n{code:title=org/apache/spark/graphx/package.scala|borderStyle=solid}\n  /**\n   * A 64-bit vertex identifier that uniquely identifies a vertex within a graph. It does not need\n   * to follow any ordering or any constraints other than uniqueness.\n   */\n  type VertexId = Long\n{code}\n\nmade me hopeful that I would be able to make {{VertexId}} a type-class, change method/function signatures and fix all the compile errors, without needing to understand GraphX code. To test this, I changed {{VertexId}} to:\n\n{code:title=org/apache/spark/graphx/package.scala|borderStyle=solid}\n  type VertexId = java.util.UUID\n{code}\n\nUnfortunately, the compile errors seem to indicate that it is not just a matter of fixing compile errors:\n\n{noformat}\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/Edge.scala:31: type mismatch;\n[error]  found   : Int(0)\n[error]  required: org.apache.spark.graphx.VertexId\n[error]     (which expands to)  java.util.UUID\n[error]     var srcId: VertexId = 0,\n[error]                           ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/Edge.scala:32: type mismatch;\n[error]  found   : Int(0)\n[error]  required: org.apache.spark.graphx.VertexId\n[error]     (which expands to)  java.util.UUID\n[error]     var dstId: VertexId = 0,\n[error]                           ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/Edge.scala:60: value - is not a member of org.apache.spark.graphx.VertexId\n[error]       (if (a.srcId != b.srcId) a.srcId - b.srcId else a.dstId - b.dstId).toInt\n[error]                                        ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/Edge.scala:60: value - is not a member of org.apache.spark.graphx.VertexId\n[error]       (if (a.srcId != b.srcId) a.srcId - b.srcId else a.dstId - b.dstId).toInt\n[error]                                                               ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/GraphLoader.scala:74: type mismatch;\n[error]  found   : Long\n[error]  required: org.apache.spark.graphx.VertexId\n[error]     (which expands to)  java.util.UUID\n[error]             builder.add(dstId, srcId, 1)\n[error]                         ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/PartitionStrategy.scala:78: type mismatch;\n[error]  found   : Long(1125899906842597L)\n[error]  required: org.apache.spark.graphx.VertexId\n[error]     (which expands to)  java.util.UUID\n[error]       val mixingPrime: VertexId = 1125899906842597L\n[error]                                   ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/PartitionStrategy.scala:79: overloaded method value abs with alternatives:\n[error]   (x: Double)Double <and>\n[error]   (x: Float)Float <and>\n[error]   (x: Long)Long <and>\n[error]   (x: Int)Int\n[error]  cannot be applied to (org.apache.spark.graphx.VertexId)\n[error]       val col: PartitionID = ((math.abs(src) * mixingPrime) % ceilSqrtNumParts).toInt\n[error]                                     ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/PartitionStrategy.scala:80: overloaded method value abs with alternatives:\n[error]   (x: Double)Double <and>\n[error]   (x: Float)Float <and>\n[error]   (x: Long)Long <and>\n[error]   (x: Int)Int\n[error]  cannot be applied to (org.apache.spark.graphx.VertexId)\n[error]       val row: PartitionID = ((math.abs(dst) * mixingPrime) % ceilSqrtNumParts).toInt\n[error]                                     ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/PartitionStrategy.scala:91: type mismatch;\n[error]  found   : Long(1125899906842597L)\n[error]  required: org.apache.spark.graphx.VertexId\n[error]     (which expands to)  java.util.UUID\n[error]       val mixingPrime: VertexId = 1125899906842597L\n[error]                                   ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/PartitionStrategy.scala:92: overloaded method value abs with alternatives:\n[error]   (x: Double)Double <and>\n[error]   (x: Float)Float <and>\n[error]   (x: Long)Long <and>\n[error]   (x: Int)Int\n[error]  cannot be applied to (org.apache.spark.graphx.VertexId)\n[error]       (math.abs(src) * mixingPrime).toInt % numParts\n[error]             ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/PartitionStrategy.scala:115: overloaded method value min with alternatives:\n[error]   (x: Double,y: Double)Double <and>\n[error]   (x: Float,y: Float)Float <and>\n[error]   (x: Long,y: Long)Long <and>\n[error]   (x: Int,y: Int)Int\n[error]  cannot be applied to (org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId)\n[error]       val lower = math.min(src, dst)\n[error]                        ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/PartitionStrategy.scala:116: overloaded method value max with alternatives:\n[error]   (x: Double,y: Double)Double <and>\n[error]   (x: Float,y: Float)Float <and>\n[error]   (x: Long,y: Long)Long <and>\n[error]   (x: Int,y: Int)Int\n[error]  cannot be applied to (org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId)\n[error]       val higher = math.max(src, dst)\n[error]                         ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/EdgePartition.scala:165: value < is not a member of org.apache.spark.graphx.VertexId\n[error]       while (j < other.size && other.srcIds(j) < srcId) { j += 1 }\n[error]                                                ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/EdgePartition.scala:167: value < is not a member of org.apache.spark.graphx.VertexId\n[error]         while (j < other.size && other.srcIds(j) == srcId && other.dstIds(j) < dstId) { j += 1 }\n[error]                                                                              ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/Serializers.scala:34: type mismatch;\n[error]  found   : java.util.UUID\n[error]  required: Long\n[error]         writeVarLong(msg._1, optimizePositive = false)\n[error]                          ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/Serializers.scala:55: type mismatch;\n[error]  found   : org.apache.spark.graphx.VertexId\n[error]     (which expands to)  java.util.UUID\n[error]  required: Long\n[error]         writeVarLong(msg.vid, optimizePositive = false)\n[error]                          ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/Serializers.scala:65: type mismatch;\n[error]  found   : Long\n[error]  required: org.apache.spark.graphx.VertexId\n[error]     (which expands to)  java.util.UUID\n[error]         new VertexBroadcastMsg[Int](0, a, b).asInstanceOf[T]\n[error]                                        ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/Serializers.scala:79: type mismatch;\n[error]  found   : org.apache.spark.graphx.VertexId\n[error]     (which expands to)  java.util.UUID\n[error]  required: Long\n[error]         writeVarLong(msg.vid, optimizePositive = false)\n[error]                          ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/Serializers.scala:89: type mismatch;\n[error]  found   : Long\n[error]  required: org.apache.spark.graphx.VertexId\n[error]     (which expands to)  java.util.UUID\n[error]         new VertexBroadcastMsg[Long](0, a, b).asInstanceOf[T]\n[error]                                         ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/Serializers.scala:103: type mismatch;\n[error]  found   : org.apache.spark.graphx.VertexId\n[error]     (which expands to)  java.util.UUID\n[error]  required: Long\n[error]         writeVarLong(msg.vid, optimizePositive = false)\n[error]                          ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/Serializers.scala:113: type mismatch;\n[error]  found   : Long\n[error]  required: org.apache.spark.graphx.VertexId\n[error]     (which expands to)  java.util.UUID\n[error]         new VertexBroadcastMsg[Double](0, a, b).asInstanceOf[T]\n[error]                                           ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/Serializers.scala:127: type mismatch;\n[error]  found   : org.apache.spark.graphx.VertexId\n[error]     (which expands to)  java.util.UUID\n[error]  required: Long\n[error]         writeVarLong(msg._1, optimizePositive = false)\n[error]                          ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/Serializers.scala:151: type mismatch;\n[error]  found   : org.apache.spark.graphx.VertexId\n[error]     (which expands to)  java.util.UUID\n[error]  required: Long\n[error]         writeVarLong(msg._1, optimizePositive = false)\n[error]                          ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/Serializers.scala:175: type mismatch;\n[error]  found   : org.apache.spark.graphx.VertexId\n[error]     (which expands to)  java.util.UUID\n[error]  required: Long\n[error]         writeVarLong(msg._1, optimizePositive = false)\n[error]                          ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/lib/ConnectedComponents.scala:41: value < is not a member of org.apache.spark.graphx.VertexId\n[error]       if (edge.srcAttr < edge.dstAttr) {\n[error]                        ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/lib/ConnectedComponents.scala:43: value > is not a member of org.apache.spark.graphx.VertexId\n[error]       } else if (edge.srcAttr > edge.dstAttr) {\n[error]                               ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/lib/ConnectedComponents.scala:51: overloaded method value min with alternatives:\n[error]   (x: Double,y: Double)Double <and>\n[error]   (x: Float,y: Float)Float <and>\n[error]   (x: Long,y: Long)Long <and>\n[error]   (x: Int,y: Int)Int\n[error]  cannot be applied to (org.apache.spark.graphx.VertexId, Long)\n[error]       vprog = (id, attr, msg) => math.min(attr, msg),\n[error]                                       ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/lib/ConnectedComponents.scala:52: type mismatch;\n[error]  found   : Iterator[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId)]\n[error]  required: Iterator[(org.apache.spark.graphx.VertexId, Long)]\n[error]       sendMsg = sendMessage,\n[error]                 ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/lib/StronglyConnectedComponents.scala:75: type mismatch;\n[error]  found   : Long(9223372036854775807L)\n[error]  required: org.apache.spark.graphx.VertexId\n[error]     (which expands to)  java.util.UUID\n[error]         sccWorkGraph, Long.MaxValue, activeDirection = EdgeDirection.Out)(\n[error]                            ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/util/GraphGenerators.scala:54: type mismatch;\n[error]  found   : Int\n[error]  required: org.apache.spark.graphx.VertexId\n[error]     (which expands to)  java.util.UUID\n[error]       src => (src, sampleLogNormal(mu, sigma, numVertices))\n[error]               ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/util/GraphGenerators.scala:57: value toInt is not a member of org.apache.spark.graphx.VertexId\n[error]       generateRandomEdges(v._1.toInt, v._2, numVertices)\n[error]                                ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/util/GraphGenerators.scala:59: type mismatch;\n[error]  found   : org.apache.spark.rdd.RDD[Nothing]\n[error]  required: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Int]]\n[error] Note: Nothing <: org.apache.spark.graphx.Edge[Int], but class RDD is invariant in type T.\n[error] You may wish to define T as +T instead. (SLS 4.5)\n[error]     Graph(vertices, edges, 0)\n[error]                     ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/util/GraphGenerators.scala:64: type mismatch;\n[error]  found   : Int\n[error]  required: org.apache.spark.graphx.VertexId\n[error]     (which expands to)  java.util.UUID\n[error]     Array.fill(maxVertexId) { Edge[Int](src, rand.nextInt(maxVertexId), 1) }\n[error]                                         ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/util/GraphGenerators.scala:64: type mismatch;\n[error]  found   : Int\n[error]  required: org.apache.spark.graphx.VertexId\n[error]     (which expands to)  java.util.UUID\n[error]     Array.fill(maxVertexId) { Edge[Int](src, rand.nextInt(maxVertexId), 1) }\n[error]                                                          ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/util/GraphGenerators.scala:129: type mismatch;\n[error]  found   : Int\n[error]  required: org.apache.spark.graphx.VertexId\n[error]     (which expands to)  java.util.UUID\n[error]     Edge[Int](src, dst, 1)\n[error]               ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/util/GraphGenerators.scala:129: type mismatch;\n[error]  found   : Int\n[error]  required: org.apache.spark.graphx.VertexId\n[error]     (which expands to)  java.util.UUID\n[error]     Edge[Int](src, dst, 1)\n[error]                    ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/util/GraphGenerators.scala:209: type mismatch;\n[error]  found   : Int\n[error]  required: org.apache.spark.graphx.VertexId\n[error]     (which expands to)  java.util.UUID\n[error]     def sub2ind(r: Int, c: Int): VertexId = r * cols + c\n[error]                                                      ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/util/GraphGenerators.scala:231: type mismatch;\n[error]  found   : Int\n[error]  required: org.apache.spark.graphx.VertexId\n[error]     (which expands to)  java.util.UUID\n[error]     val edges: RDD[(VertexId, VertexId)] = sc.parallelize(1 until nverts).map(vid => (vid, 0))\n[error]                                                                                       ^\n[error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/util/GraphGenerators.scala:231: type mismatch;\n[error]  found   : Int(0)\n[error]  required: org.apache.spark.graphx.VertexId\n[error]     (which expands to)  java.util.UUID\n[error]     val edges: RDD[(VertexId, VertexId)] = sc.parallelize(1 until nverts).map(vid => (vid, 0))\n[error]                                                                                            ^\n[error] 39 errors found\n{noformat}", "This is a good feedback - my concern is it will have a pretty significant drop in performance if strings are used (we rely on hashing integers pretty heavily). Let us think about it more. If the demand is huge, we can prioritize this and make it optional. ", "With the change being requested, Longs can still be used by those who can use Longs and want that performance. The overhead (if any) will come from the type-class mechanism (or whatever mechanism is chosen for making the vertex-ID a customizable type), and I don't know how much that overhead will be.", "An alternative approach, that I have been using: \n1 Use a preprocessing step that maps UUID to an Long.\n2. Build graph based on Longs\n\nFor Mapping in step 1:\n- Rank your uuids.\n- some kind of has function?\n\nFor 1, graphx can provide a tool to generate map.\n\nI will like to hear how others are building graphs out of non-Long node types.\n\n\n", "I like npanj's approach.\nIt's universal. You treat UUID as attribute.\n\nLike the procedure from http://spark.apache.org/docs/latest/graphx-programming-guide.html\n\n// Connect to the Spark cluster\n== Build Graph (build VertexID if necessary)\n// Load my user data and parse into tuples of user id and attribute list\n// Parse the edge data which is already in userId -> userId format\n// Attach the user attributes\n== Clean Graph\n// Some users may not have attributes so we set them as empty, Restrict the graph to users with usernames and names\n== Compute\n// Compute the PageRank\n== Get Result\n// Get the attributes of the top pagerank users", "FWIW, UUID.getMostSignificantBits() or getLeastSignificantBits() can be used to generate a Long, with low collision probability.\n\nUsing any type for the ID is still preferred.", "I am currently using zipWithUniqueId() to get a VertexID for my data, but that means that after getting the VertexIDs, I have to go to the edges data to look for each of those strings and assign the Id I got from the previous step. \n\nI agree it would be nice to be able to choose a different tipe of ID, leaving the user to decide whether he prefers performance or usability.", "We would also really like a general customized ID available for Vertex. We've been using zipwithIndex to create IDs for  now, however, it is a hassle process-wise because we never have a stable ID:  any update to a new version of Graph with incremental input data requires a total rebuild of vertex/edges, or we will need another infrastructure to serve as an ID service: additional cost/maintenance. We already have unique IDs for all of our data entities. It would make processing/maintenance much easier if our stable IDs can be used directly", "No activity in 2 years", "We are also running into this issue and would like this feature. For our use case and our data size, even a low risk of hash collisions is not acceptable, so we have to have a reliable way to form unique ids from our current unique string ids.\n\nI'm going to work on a patch next week.\n\nSince this is marked \"won't fix\" due to inactivity, what's the process if a PR is submitted? (Sorry, new to the Apache contribution process.)", "[~ntietz] changing this will very likely make performance regress for long ids, due to the lack of specialization. \n\nYou might want to look into graphframes for more general graph functionalities too: https://github.com/graphframes/graphframes ", "Thanks for the reply.\n\nI think that GraphFrames is not quite sufficient to meet our needs here but I will dive in further. My focus this week is on addressing our problem with hash collisions in forming graph vertex ids, so you may hear more from me.\n\nCould you say some more about where it will likely make performance regress? I am diving into the source this week, but pointers toward specific things to watch out for would be helpful.", "The main thing is that we encode the data assuming integer ids, and are using specialized data structure for int ids. If we change to generic types, the memory footprint will increase, and the performance will decrease too.\n", "Hi Nicholas, we got the same needs here, and we delayed a fix until today where we found that collisions reach an arbitrary (low) level.\nAs JJ Zhang said, I'm not confortable with a solution that produces everyday a new order (and consequently a new ID) but keeping a dictionnary of key/value seems costly given the number of data we're dealing with.\n\nHave you got a chance to make some experiments on the best way to solve this problem ? ", "The decision we eventually made was to migrate as much of our code out of GraphX as we could (moving to writing more directly in Spark). We were running into other potential performance issues with GraphX and we could not do the kind of checkpointing we wanted to, so it was a workable solution for us. We wound up with minimal GraphX code (and when I left the company we were close to being able to remove ALL of it).\n\nAt the end, we just dealt with the pain of managing consistent IDs ourselves and joining them in. It was not ideal, but it worked and the performance hit was made up for in other areas where we were able to migrate off of GraphX.", ":)  why do not add string or uuid support?"], "derived": {"summary": "Currently, {{VertexId}} is a type-synonym for {{Long}}. I would like to be able to use {{UUID}} as the vertex ID type because the data I want to process with GraphX uses that type for its primay-keys.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Generalize VertexId in GraphX so that UUIDs can be used as vertex IDs. - Currently, {{VertexId}} is a type-synonym for {{Long}}. I would like to be able to use {{UUID}} as the vertex ID type because the data I want to process with GraphX uses that type for its primay-keys."}, {"q": "What updates or decisions were made in the discussion?", "a": ":)  why do not add string or uuid support?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1154", "title": "Spark fills up disk with app-* folders", "status": "Resolved", "priority": "Critical", "reporter": "Evan Chan", "assignee": "Mingyu Kim", "labels": ["starter"], "created": "2014-02-28T11:40:28.000+0000", "updated": "2014-07-24T18:55:15.000+0000", "description": "Current version of Spark fills up the disk with many app-* folders:\n\n$ ls /var/lib/spark\napp-20140210022347-0597  app-20140212173327-0627  app-20140218154110-0657  app-20140225232537-0017  app-20140225233548-0047\napp-20140210022407-0598  app-20140212173347-0628  app-20140218154130-0658  app-20140225232551-0018  app-20140225233556-0048\napp-20140210022427-0599  app-20140212173754-0629  app-20140218164232-0659  app-20140225232611-0019  app-20140225233603-0049\napp-20140210022447-0600  app-20140212182235-0630  app-20140218165133-0660  app-20140225232802-0020  app-20140225233610-0050\napp-20140210022508-0601  app-20140212182256-0631  app-20140218165148-0661  app-20140225232822-0021  app-20140225233617-0051\napp-20140210022528-0602  app-20140213000014-0632  app-20140218165225-0662  app-20140225232940-0022  app-20140225233624-0052\napp-20140211024356-0603  app-20140213002026-0633  app-20140218165249-0663  app-20140225233002-0023  app-20140225233631-0053\napp-20140211024417-0604  app-20140213154948-0634  app-20140218172030-0664  app-20140225233056-0024  app-20140225233725-0054\napp-20140211024437-0605  app-20140213171810-0635  app-20140218193853-0665  app-20140225233108-0025  app-20140225233731-0055\napp-20140211024457-0606  app-20140213193637-0636  app-20140218194442-0666  app-20140225233124-0026  app-20140225233733-0056\napp-20140211024517-0607  app-20140214011513-0637  app-20140218194746-0667  app-20140225233133-0027  app-20140225233734-0057\napp-20140211024538-0608  app-20140214012151-0638  app-20140218194822-0668  app-20140225233147-0028  app-20140225233749-0058\napp-20140211193443-0609  app-20140214013134-0639  app-20140218212317-0669  app-20140225233208-0029  app-20140225233759-0059\napp-20140211195210-0610  app-20140214013332-0640  app-20140225180142-0000  app-20140225233215-0030  app-20140225233809-0060\napp-20140211213935-0611  app-20140214013642-0641  app-20140225180411-0001  app-20140225233224-0031  app-20140225233828-0061\napp-20140211214227-0612  app-20140214014246-0642  app-20140225180431-0002  app-20140225233232-0032  app-20140225234719-0062\napp-20140211215317-0613  app-20140214014607-0643  app-20140225180452-0003  app-20140225233239-0033  app-20140226032845-0063\napp-20140211224601-0614  app-20140214184943-0644  app-20140225180512-0004  app-20140225233320-0034  app-20140226033004-0064\napp-20140212022206-0615  app-20140214185118-0645  app-20140225180533-0005  app-20140225233328-0035  app-20140226033119-0065\napp-20140212022226-0616  app-20140214185851-0646  app-20140225180553-0006  app-20140225233354-0036  app-20140226033334-0066\napp-20140212022246-0617  app-20140214222856-0647  app-20140225181115-0007  app-20140225233402-0037  app-20140226033354-0067\napp-20140212043704-0618  app-20140214231312-0648  app-20140225181244-0008  app-20140225233409-0038  app-20140226033538-0068\napp-20140212043724-0619  app-20140214231434-0649  app-20140225182051-0009  app-20140225233416-0039  app-20140226033826-0069\napp-20140212043745-0620  app-20140214231542-0650  app-20140225183009-0010  app-20140225233426-0040  app-20140226034002-0070\napp-20140212044016-0621  app-20140214231616-0651  app-20140225184133-0011  app-20140225233432-0041  app-20140226034053-0071\napp-20140212044203-0622  app-20140214233016-0652  app-20140225184318-0012  app-20140225233439-0042  app-20140226034234-0072\napp-20140212044224-0623  app-20140214233037-0653  app-20140225184709-0013  app-20140225233447-0043  app-20140226034426-0073\napp-20140212045034-0624  app-20140218153242-0654  app-20140225184844-0014  app-20140225233526-0044  app-20140226034447-0074\napp-20140212045119-0625  app-20140218153341-0655  app-20140225190051-0015  app-20140225233534-0045\napp-20140212173310-0626  app-20140218153442-0656  app-20140225232516-0016  app-20140225233540-0046\n\nThis problem is particularly bad if you have a whole bunch of fast jobs.   \nAlso what makes the problem worse is that any jars for jobs is downloaded into the app-* folder, so that fills up the disk particularly fast.\n\nI would like to propose two things:\n1) Spark should have a cleanup thread (or actor) which periodically removes old app-* folders;   This should not be the responsibility of people deploying Spark.\n2) The download of jars should not go to each app-* folder.  This wastes a huge amount of space because most jobs use the same jars.  Maybe I can open a separate ticket for this.", "comments": ["FWIW Jeff H. here had observed the same thing and had the same request. I'm only sorry I'm not qualified to propose the actual change here but do agree this would help some intense deployments.", "+1 I've observed this too", "The issue with having a clean-up is that these are user logs which some people might want around so it's sort of dangerous to just go around deleting stuff. In the past the assumption has been that the cluster administrator would be responsible for cleaning the logs (as with other long lived services which generate logs). One idea would be to have an optional TTL value where logs are deleted after that time - would that help here?", "How about zipping the old log files after a certain amount of time(configurable) and then delete them afterwards(again configurable, as Patrick has already pointed out)?\n\nAdvantages:\n- will save good amout of disk space\n- without losing log files too soon\n- will give enough time to the cluster administrator to delete the log files either on his own or as per the the configured time", "Also, as Evan pointed out for the second issue, instead of saving the same jars in each folder, we can save them in a single common folder. This can drastically reduce the disk usage.", "It sounds like there are two configuration knobs:\n- spark.old.logs.delete.ttl \n- spark.old.logs.archive.ttl\n\nPersonally I'm actually more keen to work on the jar issue as I think it'll save the most space for us, but the TTL should be easy to implement too.", "I'm also interested in the cleanup of old jars. I have dozens of copies of\nold jars that are all identical throughout these work directories and the\ndisk space starts to add up over time.\n\nSent from my mobile phone\nOn Mar 4, 2014 11:44 PM, \"Evan Chan (JIRA)\" <\n\n", "This all seems reasonable. Storing all the jars in the same directory will require some extra effort because if two applications add a jar with the same name but different content you want to have two distinct copies. It's definitely do-able but will require a bit of effort. Anyways I think consolidating the jars and/or having a TTL are both reasonable strategies.", "I am interested in working on this issue. Please assign it to me. Or can I send a pull request rightaway?", "Feel free to send one right away!  Please put \"SPARK-1154\" at the beginning\nof the title and send the pull request to the new GitHub project (no longer\nincubator) at http://github.com/apache/spark\n\nThanks Piyush!\n\n\nOn Wed, Mar 5, 2014 at 11:37 PM, Piyush Kansal (JIRA) <\n\n", "Hey Andrew,\n\nI am a newbie. Looks like pull request can only be sent once the patch is\nready. In the meantime, does this issue needs to be assigned to me to\nensure that no one else ends up wasting his efforts?\n\nThanks.\n\n\nOn Wed, Mar 5, 2014 at 11:48 PM, Andrew Ash (JIRA) <\n\n\n\n\n-- \nBest,\nPiyush Kansal\nhttp://about.me/piyushkansal\n", "I just assigned it to you.  Looking forward to seeing your patch when it's ready!", "Thanks!\nOn Mar 6, 2014 1:57 AM, \"Andrew Ash (JIRA)\" <\n\n", "Piyush,\n\nWhich part of this ticket are you working on, the cleaning up of app folders, the relocation of jar saving to a central dir, or both?", "Yeah, I'm going to do this next week.   This is pretty important for how we\nuse Spark, and I imagine others too.   We only have about a week left  :(\n\nSome guidance on how to test this would be good.\n\n\nOn Thu, Mar 27, 2014 at 6:05 AM, Piyush Kansal (JIRA) <\n\n\n\n\n-- \n--\nEvan Chan\nStaff Engineer\nev@ooyala.com  |\n\n<http://www.ooyala.com/>\n<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>\n", "I looked at the commit, and it seems like it wipes out app-* based on the last modification time of the directory itself. Because the modification time of a directory only changes when a child is added or removed, this may wipe out the app-* directory of a running Spark application if it has been running for more than TTL (unless new files/jars are added to the app directory once every while). I believe it should check the latest modification time of all the descendents of the app-* directory to decide whether to delete it or not. Am I mistaken?", "[~mkim] yes you are correct - this is broken. Checkout SPARK-1860. Are you interested in fixing this?", "Yes, I'd be happy to take it. Please feel free to assign it to me.", "For the record, this is Evan's PR that closed this ticket: https://github.com/apache/spark/pull/288"], "derived": {"summary": "Current version of Spark fills up the disk with many app-* folders:\n\n$ ls /var/lib/spark\napp-20140210022347-0597  app-20140212173327-0627  app-20140218154110-0657  app-20140225232537-0017  app-20140225233548-0047\napp-20140210022407-0598  app-20140212173347-0628  app-20140218154130-0658  app-20140225232551-0018  app-20140225233556-0048\napp-20140210022427-0599  app-20140212173754-0629  app-20140218164232-0659  app-20140225232611-0019  app-20140225233603-0049\napp-20140210022447-0600  app-20140212182235-0630  app-20140218165133-0660  app-20140225232802-0020  app-20140225233610-0050\napp-20140210022508-0601  app-20140212182256-0631  app-20140218165148-0661  app-20140225232822-0021  app-20140225233617-0051\napp-20140210022528-0602  app-20140213000014-0632  app-20140218165225-0662  app-20140225232940-0022  app-20140225233624-0052\napp-20140211024356-0603  app-20140213002026-0633  app-20140218165249-0663  app-20140225233002-0023  app-20140225233631-0053\napp-20140211024417-0604  app-20140213154948-0634  app-20140218172030-0664  app-20140225233056-0024  app-20140225233725-0054\napp-20140211024437-0605  app-20140213171810-0635  app-20140218193853-0665  app-20140225233108-0025  app-20140225233731-0055\napp-20140211024457-0606  app-20140213193637-0636  app-20140218194442-0666  app-20140225233124-0026  app-20140225233733-0056\napp-20140211024517-0607  app-20140214011513-0637  app-20140218194746-0667  app-20140225233133-0027  app-20140225233734-0057\napp-20140211024538-0608  app-20140214012151-0638  app-20140218194822-0668  app-20140225233147-0028  app-20140225233749-0058\napp-20140211193443-0609  app-20140214013134-0639  app-20140218212317-0669  app-20140225233208-0029  app-20140225233759-0059\napp-20140211195210-0610  app-20140214013332-0640  app-20140225180142-0000  app-20140225233215-0030  app-20140225233809-0060\napp-20140211213935-0611  app-20140214013642-0641  app-20140225180411-0001  app-20140225233224-0031  app-20140225233828-0061\napp-20140211214227-0612  app-20140214014246-0642  app-20140225180431-0002  app-20140225233232-0032  app-20140225234719-0062\napp-20140211215317-0613  app-20140214014607-0643  app-20140225180452-0003  app-20140225233239-0033  app-20140226032845-0063\napp-20140211224601-0614  app-20140214184943-0644  app-20140225180512-0004  app-20140225233320-0034  app-20140226033004-0064\napp-20140212022206-0615  app-20140214185118-0645  app-20140225180533-0005  app-20140225233328-0035  app-20140226033119-0065\napp-20140212022226-0616  app-20140214185851-0646  app-20140225180553-0006  app-20140225233354-0036  app-20140226033334-0066\napp-20140212022246-0617  app-20140214222856-0647  app-20140225181115-0007  app-20140225233402-0037  app-20140226033354-0067\napp-20140212043704-0618  app-20140214231312-0648  app-20140225181244-0008  app-20140225233409-0038  app-20140226033538-0068\napp-20140212043724-0619  app-20140214231434-0649  app-20140225182051-0009  app-20140225233416-0039  app-20140226033826-0069\napp-20140212043745-0620  app-20140214231542-0650  app-20140225183009-0010  app-20140225233426-0040  app-20140226034002-0070\napp-20140212044016-0621  app-20140214231616-0651  app-20140225184133-0011  app-20140225233432-0041  app-20140226034053-0071\napp-20140212044203-0622  app-20140214233016-0652  app-20140225184318-0012  app-20140225233439-0042  app-20140226034234-0072\napp-20140212044224-0623  app-20140214233037-0653  app-20140225184709-0013  app-20140225233447-0043  app-20140226034426-0073\napp-20140212045034-0624  app-20140218153242-0654  app-20140225184844-0014  app-20140225233526-0044  app-20140226034447-0074\napp-20140212045119-0625  app-20140218153341-0655  app-20140225190051-0015  app-20140225233534-0045\napp-20140212173310-0626  app-20140218153442-0656  app-20140225232516-0016  app-20140225233540-0046\n\nThis problem is particularly bad if you have a whole bunch of fast jobs. Also what makes the problem worse is that any jars for jobs is downloaded into the app-* folder, so that fills up the disk particularly fast.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Spark fills up disk with app-* folders - Current version of Spark fills up the disk with many app-* folders:\n\n$ ls /var/lib/spark\napp-20140210022347-0597  app-20140212173327-0627  app-20140218154110-0657  app-20140225232537-0017  app-20140225233548-0047\napp-20140210022407-0598  app-20140212173347-0628  app-20140218154130-0658  app-20140225232551-0018  app-20140225233556-0048\napp-20140210022427-0599  app-20140212173754-0629  app-20140218164232-0659  app-20140225232611-0019  app-20140225233603-0049\napp-20140210022447-0600  app-20140212182235-0630  app-20140218165133-0660  app-20140225232802-0020  app-20140225233610-0050\napp-20140210022508-0601  app-20140212182256-0631  app-20140218165148-0661  app-20140225232822-0021  app-20140225233617-0051\napp-20140210022528-0602  app-20140213000014-0632  app-20140218165225-0662  app-20140225232940-0022  app-20140225233624-0052\napp-20140211024356-0603  app-20140213002026-0633  app-20140218165249-0663  app-20140225233002-0023  app-20140225233631-0053\napp-20140211024417-0604  app-20140213154948-0634  app-20140218172030-0664  app-20140225233056-0024  app-20140225233725-0054\napp-20140211024437-0605  app-20140213171810-0635  app-20140218193853-0665  app-20140225233108-0025  app-20140225233731-0055\napp-20140211024457-0606  app-20140213193637-0636  app-20140218194442-0666  app-20140225233124-0026  app-20140225233733-0056\napp-20140211024517-0607  app-20140214011513-0637  app-20140218194746-0667  app-20140225233133-0027  app-20140225233734-0057\napp-20140211024538-0608  app-20140214012151-0638  app-20140218194822-0668  app-20140225233147-0028  app-20140225233749-0058\napp-20140211193443-0609  app-20140214013134-0639  app-20140218212317-0669  app-20140225233208-0029  app-20140225233759-0059\napp-20140211195210-0610  app-20140214013332-0640  app-20140225180142-0000  app-20140225233215-0030  app-20140225233809-0060\napp-20140211213935-0611  app-20140214013642-0641  app-20140225180411-0001  app-20140225233224-0031  app-20140225233828-0061\napp-20140211214227-0612  app-20140214014246-0642  app-20140225180431-0002  app-20140225233232-0032  app-20140225234719-0062\napp-20140211215317-0613  app-20140214014607-0643  app-20140225180452-0003  app-20140225233239-0033  app-20140226032845-0063\napp-20140211224601-0614  app-20140214184943-0644  app-20140225180512-0004  app-20140225233320-0034  app-20140226033004-0064\napp-20140212022206-0615  app-20140214185118-0645  app-20140225180533-0005  app-20140225233328-0035  app-20140226033119-0065\napp-20140212022226-0616  app-20140214185851-0646  app-20140225180553-0006  app-20140225233354-0036  app-20140226033334-0066\napp-20140212022246-0617  app-20140214222856-0647  app-20140225181115-0007  app-20140225233402-0037  app-20140226033354-0067\napp-20140212043704-0618  app-20140214231312-0648  app-20140225181244-0008  app-20140225233409-0038  app-20140226033538-0068\napp-20140212043724-0619  app-20140214231434-0649  app-20140225182051-0009  app-20140225233416-0039  app-20140226033826-0069\napp-20140212043745-0620  app-20140214231542-0650  app-20140225183009-0010  app-20140225233426-0040  app-20140226034002-0070\napp-20140212044016-0621  app-20140214231616-0651  app-20140225184133-0011  app-20140225233432-0041  app-20140226034053-0071\napp-20140212044203-0622  app-20140214233016-0652  app-20140225184318-0012  app-20140225233439-0042  app-20140226034234-0072\napp-20140212044224-0623  app-20140214233037-0653  app-20140225184709-0013  app-20140225233447-0043  app-20140226034426-0073\napp-20140212045034-0624  app-20140218153242-0654  app-20140225184844-0014  app-20140225233526-0044  app-20140226034447-0074\napp-20140212045119-0625  app-20140218153341-0655  app-20140225190051-0015  app-20140225233534-0045\napp-20140212173310-0626  app-20140218153442-0656  app-20140225232516-0016  app-20140225233540-0046\n\nThis problem is particularly bad if you have a whole bunch of fast jobs. Also what makes the problem worse is that any jars for jobs is downloaded into the app-* folder, so that fills up the disk particularly fast."}, {"q": "What updates or decisions were made in the discussion?", "a": "For the record, this is Evan's PR that closed this ticket: https://github.com/apache/spark/pull/288"}]}}
{"project": "SPARK", "issue_id": "SPARK-1155", "title": "Clean up and document use of SparkEnv", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Andrew Or", "labels": [], "created": "2014-02-28T15:36:54.000+0000", "updated": "2016-01-05T21:18:42.000+0000", "description": "We should provide better documentation explaining what SparkEnv is and why it needs to be thread local (basically, to allow it to be accessed inside of closures on executors). Also, in cases where SparkEnv is being accessed on the driver we should access it through the associated SparkContext rather than through the thread local. Finally, we should see if it's possible to just remove this as a thread local and instead make it a static singleton that the exeucutor sets once. This last thing might not be possible if, under certain code paths, this is used on the driver.", "comments": ["This issue is no longer relevant now that SparkEnv is no longer a thread-local."], "derived": {"summary": "We should provide better documentation explaining what SparkEnv is and why it needs to be thread local (basically, to allow it to be accessed inside of closures on executors). Also, in cases where SparkEnv is being accessed on the driver we should access it through the associated SparkContext rather than through the thread local.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Clean up and document use of SparkEnv - We should provide better documentation explaining what SparkEnv is and why it needs to be thread local (basically, to allow it to be accessed inside of closures on executors). Also, in cases where SparkEnv is being accessed on the driver we should access it through the associated SparkContext rather than through the thread local."}, {"q": "What updates or decisions were made in the discussion?", "a": "This issue is no longer relevant now that SparkEnv is no longer a thread-local."}]}}
{"project": "SPARK", "issue_id": "SPARK-1156", "title": "Allow spark-ec2 to login to a cluster with 0 slaves", "status": "Resolved", "priority": "Minor", "reporter": "Nicholas Chammas", "assignee": "Nan Zhu", "labels": [], "created": "2014-03-01T21:14:24.000+0000", "updated": "2014-03-07T07:23:20.000+0000", "description": "{{spark-ec2}} allows you to launch a cluster with no slaves. \n\nHowever, if you try to login to such a cluster, you get the following error:\n\n{code}\nSearching for existing cluster <cluster-name>...\nFound 1 master(s), 0 slaves\nERROR: Could not find slaves in group <cluster-name>-slaves\n{code}\n\n{{spark-ec2}} should allow you to connect to a cluster with no slaves.", "comments": ["PR: https://github.com/apache/spark/pull/58"], "derived": {"summary": "{{spark-ec2}} allows you to launch a cluster with no slaves. However, if you try to login to such a cluster, you get the following error:\n\n{code}\nSearching for existing cluster <cluster-name>.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Allow spark-ec2 to login to a cluster with 0 slaves - {{spark-ec2}} allows you to launch a cluster with no slaves. However, if you try to login to such a cluster, you get the following error:\n\n{code}\nSearching for existing cluster <cluster-name>."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR: https://github.com/apache/spark/pull/58"}]}}
{"project": "SPARK", "issue_id": "SPARK-1157", "title": "L-BFGS Optimizer", "status": "Resolved", "priority": "Major", "reporter": "DB Tsai", "assignee": "DB Tsai", "labels": [], "created": "2014-03-01T23:37:59.000+0000", "updated": "2014-04-15T18:14:01.000+0000", "description": "L-BFGS (Limited-memory BFGS) is an optimization algorithm like BFGS which uses an approximation to the inverse of Hessian matrix to steer its search through the variable space, but where BFGS stores a dense nxn approximation to the inverse Hessian, L-BFGS only stores a few vectors to represent the approximation.\n\nFor high dimensional optimization problems, the Newton method or BFGS is not applicable since the amount of memory needed to store the Hessian will grow exponentially, while L-BFGS only stores couple vectors. \n\nOne of the use case can be training large-scale logistic regression with so many features.\n\nWe'll use breeze implementation of L-BFGS.", "comments": ["PR: https://github.com/apache/spark/pull/353"], "derived": {"summary": "L-BFGS (Limited-memory BFGS) is an optimization algorithm like BFGS which uses an approximation to the inverse of Hessian matrix to steer its search through the variable space, but where BFGS stores a dense nxn approximation to the inverse Hessian, L-BFGS only stores a few vectors to represent the approximation. For high dimensional optimization problems, the Newton method or BFGS is not applicable since the amount of memory needed to store the Hessian will grow exponentially, while L-BFGS only stores couple vectors.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "L-BFGS Optimizer - L-BFGS (Limited-memory BFGS) is an optimization algorithm like BFGS which uses an approximation to the inverse of Hessian matrix to steer its search through the variable space, but where BFGS stores a dense nxn approximation to the inverse Hessian, L-BFGS only stores a few vectors to represent the approximation. For high dimensional optimization problems, the Newton method or BFGS is not applicable since the amount of memory needed to store the Hessian will grow exponentially, while L-BFGS only stores couple vectors."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR: https://github.com/apache/spark/pull/353"}]}}
{"project": "SPARK", "issue_id": "SPARK-1158", "title": "Fix flaky RateLimitedOutputStreamSuite", "status": "Resolved", "priority": "Major", "reporter": "Reynold Xin", "assignee": "Reynold Xin", "labels": ["flaky-test"], "created": "2014-03-02T16:25:45.000+0000", "updated": "2014-12-19T16:16:31.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Fix flaky RateLimitedOutputStreamSuite"}]}}
{"project": "SPARK", "issue_id": "SPARK-1159", "title": "Add Shortest-path computations to graphx.lib", "status": "Resolved", "priority": "Minor", "reporter": "Andres Perez", "assignee": null, "labels": [], "created": "2014-03-02T16:30:05.000+0000", "updated": "2015-09-07T08:29:22.000+0000", "description": "Add a landmark-based shortest-path computation to org.apache.spark.graphx.lib, to gather the lengths of shortest paths to a given set of nodes across the whole graph.\n\nSee PR: https://github.com/apache/spark/pull/10", "comments": ["The pull-request no longer adds any dependencies."], "derived": {"summary": "Add a landmark-based shortest-path computation to org. apache.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add Shortest-path computations to graphx.lib - Add a landmark-based shortest-path computation to org. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "The pull-request no longer adds any dependencies."}]}}
{"project": "SPARK", "issue_id": "SPARK-1160", "title": "Deprecate RDD.toArray", "status": "Resolved", "priority": "Minor", "reporter": "Matei Alexandru Zaharia", "assignee": "Nan Zhu", "labels": [], "created": "2014-03-02T17:42:41.000+0000", "updated": "2014-03-12T19:50:01.000+0000", "description": "It's redundant with collect() and the name doesn't make sense in Java, where we return a List (we can't return an array due to the way Java generics work). It's also missing in Python.", "comments": ["https://github.com/apache/spark/pull/105"], "derived": {"summary": "It's redundant with collect() and the name doesn't make sense in Java, where we return a List (we can't return an array due to the way Java generics work). It's also missing in Python.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Deprecate RDD.toArray - It's redundant with collect() and the name doesn't make sense in Java, where we return a List (we can't return an array due to the way Java generics work). It's also missing in Python."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/105"}]}}
{"project": "SPARK", "issue_id": "SPARK-1161", "title": "Add saveAsObjectFile and SparkContext.objectFile in Python", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Kan Zhang", "labels": [], "created": "2014-03-02T17:45:12.000+0000", "updated": "2014-06-04T01:18:59.000+0000", "description": "It can use pickling for serialization and a SequenceFile on disk similar to the JVM versions of these.", "comments": ["PR: https://github.com/apache/spark/pull/755", "Merged this in -- thanks Kan!"], "derived": {"summary": "It can use pickling for serialization and a SequenceFile on disk similar to the JVM versions of these.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add saveAsObjectFile and SparkContext.objectFile in Python - It can use pickling for serialization and a SequenceFile on disk similar to the JVM versions of these."}, {"q": "What updates or decisions were made in the discussion?", "a": "Merged this in -- thanks Kan!"}]}}
{"project": "SPARK", "issue_id": "SPARK-1162", "title": "Add top() and takeOrdered() to PySpark", "status": "Resolved", "priority": "Minor", "reporter": "Matei Alexandru Zaharia", "assignee": "Prashant Sharma", "labels": [], "created": "2014-03-02T17:48:33.000+0000", "updated": "2015-12-10T15:06:16.000+0000", "description": null, "comments": ["User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/93"], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add top() and takeOrdered() to PySpark"}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/93"}]}}
{"project": "SPARK", "issue_id": "SPARK-1163", "title": "Miscellaneous missing PySpark methods", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Prashant Sharma", "labels": [], "created": "2014-03-02T17:51:04.000+0000", "updated": "2020-02-07T17:26:44.000+0000", "description": "The following utility / debugging methods on RDD are missing:\n- name\n- setName\n- generator\n- setGenerator\n- id\n- toDebugString\n- getStorageLevel\n\nWould be nice to add them in.\n", "comments": [], "derived": {"summary": "The following utility / debugging methods on RDD are missing:\n- name\n- setName\n- generator\n- setGenerator\n- id\n- toDebugString\n- getStorageLevel\n\nWould be nice to add them in.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Miscellaneous missing PySpark methods - The following utility / debugging methods on RDD are missing:\n- name\n- setName\n- generator\n- setGenerator\n- id\n- toDebugString\n- getStorageLevel\n\nWould be nice to add them in."}]}}
{"project": "SPARK", "issue_id": "SPARK-1164", "title": "Deprecate RDD.reduceByKeyToDriver", "status": "Resolved", "priority": "Minor", "reporter": "Matei Alexandru Zaharia", "assignee": "Prashant Sharma", "labels": [], "created": "2014-03-02T17:54:24.000+0000", "updated": "2020-02-07T17:26:41.000+0000", "description": "It's missing in Java and Python and it's just an alias for reduceByKeyLocally.", "comments": ["User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/72"], "derived": {"summary": "It's missing in Java and Python and it's just an alias for reduceByKeyLocally.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Deprecate RDD.reduceByKeyToDriver - It's missing in Java and Python and it's just an alias for reduceByKeyLocally."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/72"}]}}
{"project": "SPARK", "issue_id": "SPARK-1165", "title": "Add RDD.intersection() to Java and Python", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Prashant Sharma", "labels": [], "created": "2014-03-02T17:55:25.000+0000", "updated": "2020-02-07T17:26:41.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add RDD.intersection() to Java and Python"}]}}
{"project": "SPARK", "issue_id": "SPARK-1166", "title": "leftover vpc_id may block the creation of new ec2 cluster", "status": "Closed", "priority": "Major", "reporter": "Nan Zhu", "assignee": "Nan Zhu", "labels": [], "created": "2014-03-02T18:55:17.000+0000", "updated": "2014-07-22T20:30:59.000+0000", "description": "When I run the spark-ec2 script to build ec2 cluster in EC2, for some reason, I always received errors as following:\n\n{code}\n\nSetting up security groups...\nERROR:boto:400 Bad Request\nERROR:boto:<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Response><Errors><Error><Code>InvalidParameterValue</Code><Message>Invalid value 'null' for protocol. VPC security group rules must specify protocols explicitly.</Message></Error></Errors><RequestID>fc56f0ba-915a-45b6-8555-05d4dd0f14ee</RequestID></Response>\nTraceback (most recent call last):\n  File \"./spark_ec2.py\", line 813, in <module>\n    main()\n  File \"./spark_ec2.py\", line 806, in main\n    real_main()\n  File \"./spark_ec2.py\", line 689, in real_main\n    conn, opts, cluster_name)\n  File \"./spark_ec2.py\", line 244, in launch_cluster\n    slave_group.authorize(src_group=master_group)\n  File \"/Users/nanzhu/code/spark/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/ec2/securitygroup.py\", line 184, in authorize\n  File \"/Users/nanzhu/code/spark/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/ec2/connection.py\", line 2181, in authorize_security_group\n  File \"/Users/nanzhu/code/spark/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/connection.py\", line 944, in get_status\nboto.exception.EC2ResponseError: EC2ResponseError: 400 Bad Request\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Response><Errors><Error><Code>InvalidParameterValue</Code><Message>Invalid value 'null' for protocol. VPC security group rules must specify protocols explicitly.</Message></Error></Errors><RequestID>fc56f0ba-915a-45b6-8555-05d4dd0f14ee</RequestID></Response>\n\n{code}", "comments": ["I looked at the boto implementation, it turns out that the new created master_group and slave_group has a valid vpc_id, so that when the following code execute, boto thinks that we should pass the protocol type explicitly\n\n{code}\n\ngroup_name = None\n        if not self.vpc_id:\n            group_name = self.name\n        group_id = None\n        if self.vpc_id:\n            group_id = self.id\n        src_group_name = None\n        src_group_owner_id = None\n        src_group_group_id = None\n        if src_group:\n            cidr_ip = None\n            src_group_owner_id = src_group.owner_id\n            if not self.vpc_id:\n                src_group_name = src_group.name\n            else:\n                if hasattr(src_group, 'group_id'):\n                    src_group_group_id = src_group.group_id\n                else:\n                    src_group_group_id = src_group.id\n        status = self.connection.authorize_security_group(group_name,\n                                                          src_group_name,\n                                                          src_group_owner_id,\n                                                          ip_protocol,\n                                                          from_port,\n                                                          to_port,\n                                                          cidr_ip,\n                                                          group_id,\n                                                          src_group_group_id)\n\n{code}", "cannot reproduce after several weeks....", "Actually i've got the same error today  , and i can't laucnh a cluster :\n\nhere is the output :\n\n./spark-ec2 -i ~/amazonhdp.pem -k amazonhdp -s 4 -t m1.small launch hadoopi\nSetting up security groups...\nCreating security group hadoopi-master\nCreating security group hadoopi-slaves\nERROR:boto:400 Bad Request\nERROR:boto:<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Response><Errors><Error><Code>InvalidGroup.NotFound</Code><Message>The security group 'sg-2ec1b84b' does not exist</Message></Error></Errors><RequestID>6554c7a8-f68a-4032-ad63-65106e2de9b3</RequestID></Response>\nTraceback (most recent call last):\n  File \"./spark_ec2.py\", line 856, in <module>\n    main()\n  File \"./spark_ec2.py\", line 848, in main\n    real_main()\n  File \"./spark_ec2.py\", line 731, in real_main\n    conn, opts, cluster_name)\n  File \"./spark_ec2.py\", line 252, in launch_cluster\n    master_group.authorize('tcp', 50030, 50030, '0.0.0.0/0')\n  File \"/opt/spark-1.0.1-bin-hadoop2/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/ec2/securitygroup.py\", line 184, in authorize\n  File \"/opt/spark-1.0.1-bin-hadoop2/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/ec2/connection.py\", line 2181, in authorize_security_group\n  File \"/opt/spark-1.0.1-bin-hadoop2/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/connection.py\", line 944, in get_status\nboto.exception.EC2ResponseError: EC2ResponseError: 400 Bad Request\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Response><Errors><Error><Code>InvalidGroup.NotFound</Code><Message>The security group 'sg-2ec1b84b' does not exist</Message></Error></Errors><RequestID>6554c7a8-f68a-4032-ad63-65106e2de9b3</RequestID></Response>\n", "I've been able to reproduce, but not consistently. On this occasion, I had previously started an instance, didn't use it for anything and soon there after shut it down.  Don't know if that means anything.  Using spark 1.0 from \n\nspark-ec2 -k mykeypair -i ~/.aws/mykeypair.pem -s 7 -r \"us-west-2\" -t r3.large launch \"sparck-cluster\"\nSetting up security groups...\nCreating security group sparck-cluster-master\nCreating security group sparck-cluster-slaves\nERROR:boto:400 Bad Request\nERROR:boto:<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Response><Errors><Error><Code>InvalidGroup.NotFound</Code><Message>The security group 'sg-185fe07d' does not exist</Message></Error></Errors><RequestID>80f1e1e3-e340-4cd2-ba64-53c13525ab2b</RequestID></Response>\nTraceback (most recent call last):\n  File \"./spark_ec2.py\", line 909, in <module>\n    main()\n  File \"./spark_ec2.py\", line 901, in main\n    real_main()\n  File \"./spark_ec2.py\", line 779, in real_main\n    (master_nodes, slave_nodes) = launch_cluster(conn, opts, cluster_name)\n  File \"./spark_ec2.py\", line 279, in launch_cluster\n    master_group.authorize(src_group=slave_group)\n  File \"/Users/szalwinb/playpen/spark/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/ec2/securitygroup.py\", line 184, in authorize\n  File \"/Users/szalwinb/playpen/spark/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/ec2/connection.py\", line 2150, in authorize_security_group\n  File \"/Users/szalwinb/playpen/spark/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/ec2/connection.py\", line 2093, in authorize_security_group_deprecated\n  File \"/Users/szalwinb/playpen/spark/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/connection.py\", line 944, in get_status\nboto.exception.EC2ResponseError: EC2ResponseError: 400 Bad Request\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Response><Errors><Error><Code>InvalidGroup.NotFound</Code><Message>The security group 'sg-185fe07d' does not exist</Message></Error></Errors><RequestID>80f1e1e3-e340-4cd2-ba64-53c13525ab2b</RequestID></Response>", "To resolve, I go to https://console.aws.amazon.com/vpc/home?region=us-west-2#securityGroups: and manually delete the security groups and then I'm able to start up a cluster."], "derived": {"summary": "When I run the spark-ec2 script to build ec2 cluster in EC2, for some reason, I always received errors as following:\n\n{code}\n\nSetting up security groups. ERROR:boto:400 Bad Request\nERROR:boto:<?xml version=\"1.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "leftover vpc_id may block the creation of new ec2 cluster - When I run the spark-ec2 script to build ec2 cluster in EC2, for some reason, I always received errors as following:\n\n{code}\n\nSetting up security groups. ERROR:boto:400 Bad Request\nERROR:boto:<?xml version=\"1."}, {"q": "What updates or decisions were made in the discussion?", "a": "To resolve, I go to https://console.aws.amazon.com/vpc/home?region=us-west-2#securityGroups: and manually delete the security groups and then I'm able to start up a cluster."}]}}
{"project": "SPARK", "issue_id": "SPARK-1167", "title": "Remove metrics-ganglia from default build due to LGPL issue", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Patrick McFadin", "labels": [], "created": "2014-03-02T20:16:28.000+0000", "updated": "2014-03-30T04:13:39.000+0000", "description": "The metrics ganglia code depends on an LGPL library which we can't distribute with Spark. More information can be found here:\nhttps://groups.google.com/forum/#!searchin/metrics-user/lgpl/metrics-user/1tQd_qZHQNE/TqAfXYwh7OUJ\n\nWe should isolate this code in a separate module inside of an `/extras` folder and have a build flag in Maven/SBT (e.g. `-Pganglia`) that will pull this in if desired. That way users can still use it if they do a special build.", "comments": [], "derived": {"summary": "The metrics ganglia code depends on an LGPL library which we can't distribute with Spark. More information can be found here:\nhttps://groups.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Remove metrics-ganglia from default build due to LGPL issue - The metrics ganglia code depends on an LGPL library which we can't distribute with Spark. More information can be found here:\nhttps://groups."}]}}
{"project": "SPARK", "issue_id": "SPARK-1168", "title": "Add foldByKey to PySpark", "status": "Resolved", "priority": "Minor", "reporter": "Matei Alexandru Zaharia", "assignee": "Prashant Sharma", "labels": [], "created": "2014-03-02T20:18:06.000+0000", "updated": "2020-02-07T17:26:40.000+0000", "description": null, "comments": ["I'd like to take this issue; can you assign it to me?", "Prashant actually just did it (https://github.com/apache/spark/pull/115), but thanks for offering!", "User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/115"], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add foldByKey to PySpark"}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/115"}]}}
{"project": "SPARK", "issue_id": "SPARK-1169", "title": "Add countApproxDistinctByKey to PySpark", "status": "Resolved", "priority": "Minor", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "2014-03-02T20:18:52.000+0000", "updated": "2016-01-18T13:51:07.000+0000", "description": null, "comments": ["Should this implement approximate jobs, or is a linear probabilistic approach using a standard job acceptable? ", "On current master (1.2) I see that rdd.py now has a countApproxDistinct() method, but I don't see one for countApproxDistinctByKey() so this ticket is half completed.", "Does anyone still want this feature?  I'd like to close out old JIRAs that we won't fix in order to clear the Python backlog.  If nobody chimes in in favor of this feature, I'll close this.", "I would like this feature. "], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add countApproxDistinctByKey to PySpark"}, {"q": "What updates or decisions were made in the discussion?", "a": "I would like this feature."}]}}
{"project": "SPARK", "issue_id": "SPARK-1170", "title": "Add histogram() to PySpark", "status": "Resolved", "priority": "Minor", "reporter": "Matei Alexandru Zaharia", "assignee": "Josh Rosen", "labels": [], "created": "2014-03-02T20:19:09.000+0000", "updated": "2020-02-07T17:23:29.000+0000", "description": null, "comments": ["PR available here https://github.com/apache/spark/pull/122", "Hi [~dwmclary] and [~prashant],\n\nIt looks like your pull requests implement slightly different versions of histogram, one that uses evenly-spaced buckets and another that uses a user-provided array of buckets.  Since the Scala API supports both, it would be nice for Python to do the same.  Does one of you want to merge both of these pull requests together into a single patch that supports both styles of histogram?", "User 'nrchandan' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1783", "Davies is working on this."], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add histogram() to PySpark"}, {"q": "What updates or decisions were made in the discussion?", "a": "Davies is working on this."}]}}
{"project": "SPARK", "issue_id": "SPARK-1171", "title": "when executor is removed, we should reduce totalCores instead of just freeCores on that executor ", "status": "Resolved", "priority": "Major", "reporter": "Nan Zhu", "assignee": "Nan Zhu", "labels": [], "created": "2014-03-02T20:32:49.000+0000", "updated": "2014-03-05T14:03:31.000+0000", "description": "When the executor is removed, the current implementation will only reduce the freeCores on that executor. Actually we should reduce the totalCores...\n\nThe impact of this bug is that the default parallelism of a job may be set incorrectly after an executor is removed (so an RDD may get split into more partitions than the amount of parallelism in the cluster).  In other words, the bug leads to performance degredation but no correctness problem.", "comments": ["proposed a PR: https://github.com/apache/spark/pull/63"], "derived": {"summary": "When the executor is removed, the current implementation will only reduce the freeCores on that executor. Actually we should reduce the totalCores.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "when executor is removed, we should reduce totalCores instead of just freeCores on that executor  - When the executor is removed, the current implementation will only reduce the freeCores on that executor. Actually we should reduce the totalCores."}, {"q": "What updates or decisions were made in the discussion?", "a": "proposed a PR: https://github.com/apache/spark/pull/63"}]}}
{"project": "SPARK", "issue_id": "SPARK-1172", "title": "Improve naming of the BlockManager classes", "status": "Resolved", "priority": "Minor", "reporter": "Patrick Wendell", "assignee": "Andrew Or", "labels": [], "created": "2014-03-02T22:10:09.000+0000", "updated": "2020-05-17T18:21:44.000+0000", "description": "There is something called BlockManagerMaster that doesn't run on the driver (only on the Executors). There is a thing called BlockManagerWorker that is not really a \"worker\" component but a bridge between the connection manager and the block manager. I found these names confusing on my first read of this code and in the last week two other people have asked me to explain it to them. So perhaps we can find more intuitive names here... :)", "comments": ["Totally agree with this, the name is confusing.....\n\nbut, why BlockMaster is not running on Driver, \n\nTo my understanding, BlockMaster is a wrapper of BlockMasterActor...in executor side, BlockMasterActor is actually a reference to the driver-end real actor....\n\nDid I misunderstand something? ", "Here's a quick clarification of the current naming scheme:\n\nBlockMaster runs on both executors the driver. On the driver, it's only used in the local mode.\n\nBlockManagerMaster runs on both executors and the driver. It handles the communication from each BlockManager to the BlockManagerMasterActor. On the driver, this is a local inter-thread communication.\n\nBlockManagerMasterActor exists only on the driver. It maintains an overview of which blocks are where and other useful information. It does not necessarily know about all blocks, however, because there is an option in the BlockManager to update a block without telling master (i.e. BlockManagerMasterActor).\n\nBlockManagerSlaveActor exists on both executors and the driver. It handles the communication from the BlockManagerMasterActor to each BlockManager. As before, this is a local inter-thread communication on the driver.", "TD and I discussed more about this. I think this issue is part of a broader refactor of the BlockManager* interface. We should really maintain the abstraction that each actor should only be for sending/receiving messages, rather than maintaining state, which should be done in other classes (e.g. BlockManagerMaster).\n\nA bigger issue is that currently the only Akka interface between the driver and the executors is through the BlockManager* interface. This has not been a problem in the past, until we started to use this for other things (e.g. cleaning up shuffles in the MapOutputTracker in the automatic cleanup patch), which becomes quite ugly as BlockManager* classes begin to take in more and more things as parameters.", "Okay let's close this until we do a braoder refactoring of the block manger."], "derived": {"summary": "There is something called BlockManagerMaster that doesn't run on the driver (only on the Executors). There is a thing called BlockManagerWorker that is not really a \"worker\" component but a bridge between the connection manager and the block manager.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Improve naming of the BlockManager classes - There is something called BlockManagerMaster that doesn't run on the driver (only on the Executors). There is a thing called BlockManagerWorker that is not really a \"worker\" component but a bridge between the connection manager and the block manager."}, {"q": "What updates or decisions were made in the discussion?", "a": "Okay let's close this until we do a braoder refactoring of the block manger."}]}}
{"project": "SPARK", "issue_id": "SPARK-1173", "title": "Improve scala streaming docs", "status": "Resolved", "priority": "Minor", "reporter": "Aaron Kimball", "assignee": "Aaron Kimball", "labels": [], "created": "2014-03-02T23:12:32.000+0000", "updated": "2014-03-02T23:30:57.000+0000", "description": "Clarify imports to add implicit conversions to DStream and fix other small typos in the streaming intro documentation.", "comments": ["I added a small patch for this and sent a pull req. at https://github.com/apache/spark/pull/64"], "derived": {"summary": "Clarify imports to add implicit conversions to DStream and fix other small typos in the streaming intro documentation.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Improve scala streaming docs - Clarify imports to add implicit conversions to DStream and fix other small typos in the streaming intro documentation."}, {"q": "What updates or decisions were made in the discussion?", "a": "I added a small patch for this and sent a pull req. at https://github.com/apache/spark/pull/64"}]}}
{"project": "SPARK", "issue_id": "SPARK-1174", "title": "Adding port configuration for HttpFileServer", "status": "Resolved", "priority": "Minor", "reporter": "Egor Pakhomov", "assignee": "Egor Pahomov", "labels": [], "created": "2014-03-03T04:15:29.000+0000", "updated": "2014-09-02T01:18:36.000+0000", "description": "I run spark in big organization, where to open port accessible to other computers in network, I need to create a ticket on DevOps and it executes for days. I can't have port for some spark service to be changed all the time. I need ability to configure this port.", "comments": ["I think it is a good feature to have, I will try to implement this", "Sorry, that I haven't attached pull request right away https://github.com/apache/incubator-spark/pull/657", "do you mind closing that and reopen in the new repo? https://github.com/apache/spark\n", "Sure, but can you me explain difference between these two repositories?", "incubator-spark is for the incubation stage of the project, now it is not used any more", "https://github.com/apache/spark/pull/81/"], "derived": {"summary": "I run spark in big organization, where to open port accessible to other computers in network, I need to create a ticket on DevOps and it executes for days. I can't have port for some spark service to be changed all the time.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Adding port configuration for HttpFileServer - I run spark in big organization, where to open port accessible to other computers in network, I need to create a ticket on DevOps and it executes for days. I can't have port for some spark service to be changed all the time."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/81/"}]}}
{"project": "SPARK", "issue_id": "SPARK-1175", "title": "on shutting down a long running job, the cluster does not accept new jobs and gets hung", "status": "Resolved", "priority": "Major", "reporter": "Tal Sliwowicz", "assignee": "Nan Zhu", "labels": ["shutdown", "worker"], "created": "2014-03-03T22:27:35.000+0000", "updated": "2014-05-04T08:12:00.000+0000", "description": "When shutting down a long processing job (24+ hours) that runs periodically on the same context and generates a lot of shuffles (many hundreds of GB) the spark workers get hung for a long while and the cluster does not accept new jobs. The only way to proceed is to kill -9 the workers.\nThis is a big problem because when multiple contexts run on the same cluster, one mast stop them all for a simple restart.\nThe context is stopped using sc.stop()\nThis happens both in standalone mode and under mesos.\n\nWe suspect this is caused by the \"delete Spark local dirs\" thread. Attached a thread dump of the worker. Also, the relevant part may be:\n\n\"SIGTERM handler\" - Thread t@41040\n   java.lang.Thread.State: BLOCKED\n\tat java.lang.Shutdown.exit(Shutdown.java:168)\n\t- waiting to lock <69eab6a3> (a java.lang.Class) owned by \"SIGTERM handler\" t@41038\n\tat java.lang.Terminator$1.handle(Terminator.java:35)\n\tat sun.misc.Signal$1.run(Signal.java:195)\n\tat java.lang.Thread.run(Thread.java:662)\n\n   Locked ownable synchronizers:\n\t- None\n\n\"delete Spark local dirs\" - Thread t@40\n   java.lang.Thread.State: RUNNABLE\n\tat java.io.UnixFileSystem.delete0(Native Method)\n\tat java.io.UnixFileSystem.delete(UnixFileSystem.java:251)\n\tat java.io.File.delete(File.java:904)\n\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:482)\n\tat org.apache.spark.util.Utils$$anonfun$deleteRecursively$1.apply(Utils.scala:479)\n\tat org.apache.spark.util.Utils$$anonfun$deleteRecursively$1.apply(Utils.scala:478)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)\n\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:478)\n\tat org.apache.spark.util.Utils$$anonfun$deleteRecursively$1.apply(Utils.scala:479)\n\tat org.apache.spark.util.Utils$$anonfun$deleteRecursively$1.apply(Utils.scala:478)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)\n\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:478)\n\tat org.apache.spark.storage.DiskBlockManager$$anon$1$$anonfun$run$2.apply(DiskBlockManager.scala:141)\n\tat org.apache.spark.storage.DiskBlockManager$$anon$1$$anonfun$run$2.apply(DiskBlockManager.scala:139)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)\n\tat org.apache.spark.storage.DiskBlockManager$$anon$1.run(DiskBlockManager.scala:139)\n\n   Locked ownable synchronizers:\n\t- None\n\n\"SIGTERM handler\" - Thread t@41038\n   java.lang.Thread.State: WAITING\n\tat java.lang.Object.wait(Native Method)\n\t- waiting on <355c6c8d> (a org.apache.spark.storage.DiskBlockManager$$anon$1)\n\tat java.lang.Thread.join(Thread.java:1186)\n\tat java.lang.Thread.join(Thread.java:1239)\n\tat java.lang.ApplicationShutdownHooks.runHooks(ApplicationShutdownHooks.java:79)\n\tat java.lang.ApplicationShutdownHooks$1.run(ApplicationShutdownHooks.java:24)\n\tat java.lang.Shutdown.runHooks(Shutdown.java:79)\n\tat java.lang.Shutdown.sequence(Shutdown.java:123)\n\tat java.lang.Shutdown.exit(Shutdown.java:168)\n\t- locked <69eab6a3> (a java.lang.Class)\n\tat java.lang.Terminator$1.handle(Terminator.java:35)\n\tat sun.misc.Signal$1.run(Signal.java:195)\n\tat java.lang.Thread.run(Thread.java:662)\n\n   Locked ownable synchronizers:\n\t- None\n\n", "comments": ["it is related to https://spark-project.atlassian.net/browse/SPARK-1104?\n\n", "Seems related, but not sure - symptoms are the same though.", "I spent some time to look at the code, here is my basic analysis, I think these two issues are closely related\n\nBlockManager and many other components are started for each Executor, while the Executor object is created by CoarseExecuterBackend. Many shutdown hooks are registered for all of them...\n\nCoarseExecutorBackend is actually started in ExecutorRunner, where   a workerThread is created to start CoarseExecutorBackend process through command line (specified in appDesc) (the process is handled by a process object inside ExecutorRunner)\n\nHowever, once the process is killed, i.e. process.destroy() is called, a lot of cleanup work is triggered. The current implementation calls ExecutorRunner.kill() in Worker thread, causing the blocking of worker on shutdown of some application. (what confuses me is since the shutdownhooks are started in separate thread, why the worker is still blocked? release buffer in stdout and stderr takes time?)\n", "or, process.destroy() is a blocking method, it only returns after shutdown thread returns.....\n\nIn my PR, I moved calling of that method to the workerThread, I think it should resolve the issue\n\n", "Great!\nCan you point me to the PR? Is it - https://github.com/apache/spark/pull/35 ?", "yes, that's it", "Do you think it will be included in 1.0?", "It has been there for a long time...I'm not sure...and obviously, I need to refresh it......", "This prevents us from having a real automated solution for fail over when the driver fails. We cannot automatically start a new driver because spark is stuck on cleanup.", "BTW, did you see workers as DEAD in the UI?", "Yes", "this should have been fixed in https://github.com/apache/spark/commit/f99af8529b6969986f0c3e03f6ff9b7bb9d53ece"], "derived": {"summary": "When shutting down a long processing job (24+ hours) that runs periodically on the same context and generates a lot of shuffles (many hundreds of GB) the spark workers get hung for a long while and the cluster does not accept new jobs. The only way to proceed is to kill -9 the workers.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "on shutting down a long running job, the cluster does not accept new jobs and gets hung - When shutting down a long processing job (24+ hours) that runs periodically on the same context and generates a lot of shuffles (many hundreds of GB) the spark workers get hung for a long while and the cluster does not accept new jobs. The only way to proceed is to kill -9 the workers."}, {"q": "What updates or decisions were made in the discussion?", "a": "this should have been fixed in https://github.com/apache/spark/commit/f99af8529b6969986f0c3e03f6ff9b7bb9d53ece"}]}}
{"project": "SPARK", "issue_id": "SPARK-1176", "title": "Adding port configuration for HttpBroadcast", "status": "Resolved", "priority": "Minor", "reporter": "Egor Pakhomov", "assignee": null, "labels": [], "created": "2014-03-04T02:34:12.000+0000", "updated": "2014-09-21T14:13:43.000+0000", "description": "I run spark in big organization, where to open port accessible to other computers in network, I need to create a ticket on DevOps and it executes for days. I can't have port for some spark service to be changed all the time. I need ability to configure this port.", "comments": ["https://github.com/apache/incubator-spark/pull/658/", "[~epakhomov] it looks like this was resolved by SPARK-2157. i'm going to close this, but please feel free to re-open if it is still an issue for you."], "derived": {"summary": "I run spark in big organization, where to open port accessible to other computers in network, I need to create a ticket on DevOps and it executes for days. I can't have port for some spark service to be changed all the time.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Adding port configuration for HttpBroadcast - I run spark in big organization, where to open port accessible to other computers in network, I need to create a ticket on DevOps and it executes for days. I can't have port for some spark service to be changed all the time."}, {"q": "What updates or decisions were made in the discussion?", "a": "[~epakhomov] it looks like this was resolved by SPARK-2157. i'm going to close this, but please feel free to re-open if it is still an issue for you."}]}}
{"project": "SPARK", "issue_id": "SPARK-1177", "title": "Allow SPARK_JAR to be set in system properties", "status": "Closed", "priority": "Minor", "reporter": "Egor Pakhomov", "assignee": null, "labels": [], "created": "2014-03-04T02:55:52.000+0000", "updated": "2014-09-21T14:03:42.000+0000", "description": "I'd like to be able to do from my scala code:\n  System.setProperty(\"SPARK_YARN_APP_JAR\", SparkContext.jarOfClass(this.getClass).head)\n  System.setProperty(\"SPARK_JAR\", SparkContext.jarOfClass(SparkContext.getClass).head)\n\nAnd do nothing on OS level.", "comments": ["I'll create pull request in about a minute", "correct pull request https://github.com/apache/spark/pull/82", "[~epakhomov] it looks like this has been resolved in other change, for instance being able to use spark.yarn.jar. i'm going to close this, but feel free to re-open if you think it is still important."], "derived": {"summary": "I'd like to be able to do from my scala code:\n  System. setProperty(\"SPARK_YARN_APP_JAR\", SparkContext.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Allow SPARK_JAR to be set in system properties - I'd like to be able to do from my scala code:\n  System. setProperty(\"SPARK_YARN_APP_JAR\", SparkContext."}, {"q": "What updates or decisions were made in the discussion?", "a": "[~epakhomov] it looks like this has been resolved in other change, for instance being able to use spark.yarn.jar. i'm going to close this, but feel free to re-open if you think it is still important."}]}}
{"project": "SPARK", "issue_id": "SPARK-1178", "title": "missing document about spark.scheduler.revive.interval", "status": "Resolved", "priority": "Trivial", "reporter": "Nan Zhu", "assignee": "Nan Zhu", "labels": [], "created": "2014-03-04T05:08:11.000+0000", "updated": "2014-03-04T11:20:08.000+0000", "description": "The configuration on spark.scheduler.revive.interval is undocumented but actually used\n\nhttps://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala#L64\n\n", "comments": [], "derived": {"summary": "The configuration on spark. scheduler.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "missing document about spark.scheduler.revive.interval - The configuration on spark. scheduler."}]}}
{"project": "SPARK", "issue_id": "SPARK-1179", "title": "Allow SPARK_YARN_APP_JAR to be set in system properties", "status": "Resolved", "priority": "Minor", "reporter": "Egor Pakhomov", "assignee": null, "labels": [], "created": "2014-03-04T05:14:57.000+0000", "updated": "2014-03-05T01:22:14.000+0000", "description": "I'd like to be able to do from my scala code:\nSystem.setProperty(\"SPARK_YARN_APP_JAR\", SparkContext.jarOfClass(this.getClass).head)\nSystem.setProperty(\"SPARK_JAR\", SparkContext.jarOfClass(SparkContext.getClass).head)\n\nAnd do nothing on OS level.\n", "comments": ["https://github.com/apache/incubator-spark/pull/660", "also, do you mind closing your PR and open in the new repo, https://github.com/apache/spark ?", "SPARK_YARN_APP_JAR has been removed with https://github.com/apache/incubator-spark/pull/553"], "derived": {"summary": "I'd like to be able to do from my scala code:\nSystem. setProperty(\"SPARK_YARN_APP_JAR\", SparkContext.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Allow SPARK_YARN_APP_JAR to be set in system properties - I'd like to be able to do from my scala code:\nSystem. setProperty(\"SPARK_YARN_APP_JAR\", SparkContext."}, {"q": "What updates or decisions were made in the discussion?", "a": "SPARK_YARN_APP_JAR has been removed with https://github.com/apache/incubator-spark/pull/553"}]}}
{"project": "SPARK", "issue_id": "SPARK-1180", "title": "Allow to provide a custom persistence engine", "status": "Resolved", "priority": "Minor", "reporter": "Jacek Lewandowski", "assignee": "Prashant Sharma", "labels": [], "created": "2014-03-04T06:15:06.000+0000", "updated": "2015-02-01T21:39:22.000+0000", "description": "Currently Spark supports only predefined ZOOKEEPER and FILESYSTEM persistence engines. It would be nice to give a possibility to provide custom persistence engine by specifying a class name in {{spark.deploy.recoveryMode}}.\n", "comments": [], "derived": {"summary": "Currently Spark supports only predefined ZOOKEEPER and FILESYSTEM persistence engines. It would be nice to give a possibility to provide custom persistence engine by specifying a class name in {{spark.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Allow to provide a custom persistence engine - Currently Spark supports only predefined ZOOKEEPER and FILESYSTEM persistence engines. It would be nice to give a possibility to provide custom persistence engine by specifying a class name in {{spark."}]}}
{"project": "SPARK", "issue_id": "SPARK-1181", "title": "'mvn test' fails out of the box since sbt assembly does not necessarily exist", "status": "Closed", "priority": "Major", "reporter": "Sean R. Owen", "assignee": null, "labels": ["assembly", "maven", "sbt", "test"], "created": "2014-03-04T10:36:27.000+0000", "updated": "2015-01-15T09:07:31.000+0000", "description": "The test suite requires that \"sbt assembly\" has been run in order for some tests (like DriverSuite) to pass. The tests themselves say as much.\n\nThis means that a \"mvn test\" from a fresh clone fails.\n\nThere's a pretty simple fix, to have Maven's test-compile phase invoke \"sbt assembly\". I suppose the only downside is re-invoking \"sbt assembly\" each time tests are run.\n\nI'm open to ideas about how to set this up more intelligently but it would be a generally good thing if the Maven build's tests passed out of the box.", "comments": ["Per discussion on the pull request, this can be made to work by manually invoking 'package' first. See https://github.com/apache/spark/pull/77"], "derived": {"summary": "The test suite requires that \"sbt assembly\" has been run in order for some tests (like DriverSuite) to pass. The tests themselves say as much.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "'mvn test' fails out of the box since sbt assembly does not necessarily exist - The test suite requires that \"sbt assembly\" has been run in order for some tests (like DriverSuite) to pass. The tests themselves say as much."}, {"q": "What updates or decisions were made in the discussion?", "a": "Per discussion on the pull request, this can be made to work by manually invoking 'package' first. See https://github.com/apache/spark/pull/77"}]}}
{"project": "SPARK", "issue_id": "SPARK-1182", "title": "Sort the configuration parameters in configuration.md", "status": "Resolved", "priority": "Minor", "reporter": "Reynold Xin", "assignee": "Brennon York", "labels": [], "created": "2014-03-04T10:55:27.000+0000", "updated": "2020-02-07T17:20:54.000+0000", "description": "It is a little bit confusing right now since the config options are all over the place in some arbitrarily sorted order.\n\nhttps://github.com/apache/spark/blob/master/docs/configuration.md\n\n", "comments": ["I'm working on the other issue on the parameters, so just sorting the properties conveniently. \n\nhttps://github.com/apache/spark/pull/85/files", "This can be assigned to Nan Zhu, I hope.", "The current PR seems to be https://github.com/apache/spark/pull/2312 but its purpose is not just sorting.\n\nIs this JIRA resolved then? The current ordering is not alphabetical but does logically group \"spark.foo.*\" props together, and puts them in order of importance, sort of. (With maybe one exception or two.)\n\n", "I think the ticket is to make it alphabetical so it preserves both logical grouping as well as making it easier to find.", "User 'brennonyork' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/3863", "All, given the PR that [~srowen] pointed out doesn't resolve sorting of all the options I went ahead and manually sorted it this once with the hope that, as it gets updated, we can preserve the logical and grouped sorting moving forward.", "Given [~joshrosen]'s comments on the PR making merge-conflict hell, would it be better just to scratch this as an issue and close everything out? Its either that or deal with all the merge conflicts for any / all backports moving forward. Thoughts?", "cc [~pwendell] what do you think? I think it is probably still worth it to do that, since the config page is sort of a mess. The backporting documentation fix thing should be small. I don't think we backport documentation fixes that often.\n\n\n", "Actually when I filed this ticket, I don't think there was extensive high level grouping of config options (there was maybe two or three). I took a look at the latest grouping and they looked reasonable. \n\nA few problems with the existing grouping:\n\n1. Networking section actually contains some shuffle settings. Those should be moved into shuffle.\n\n2. Application Properties contains some serialization settings. Those should be moved into Compression and Serialization.\n\n3. Runtime Environment contains some Mesos specific setting. All the way down on the page we link to each resource manager's own page for resource manager specific settings.\n\n4. Scheduling section contains a mesos specific setting (spark.mesos.coarse).\n\n5. It is sort of arbitrary that \"Dynamic allocation\" has its own section (at the very least, \"a\" should be upper case.)\n\n6. \"spark.ui.view.acls\" is in security, but it also belongs in UI. Where do we put stuff like this?\n\n\nWithin each section, config options should probably be sorted alphabetically.\n\n", "[~rxin] I've incorporated all the changes you mentioned (save for #6) and updated all merge conflicts thus far (not fun haha). If we want to move forward with this I'd ask that we move with a bit of brevity on this one before I need to fix a ton of merge conflicts again :/ Thanks!", "Merged. Thanks for doing this, [~boyork]. \n\nCouple minor things that would be great to be addressed as a followup PR:\n\n1. Link to the Mesos configuration table rather than just the Mesos page.\n\n2. If possible, make Mesos/YARN/Standalone appear in the table of contents on top.\n"], "derived": {"summary": "It is a little bit confusing right now since the config options are all over the place in some arbitrarily sorted order. https://github.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Sort the configuration parameters in configuration.md - It is a little bit confusing right now since the config options are all over the place in some arbitrarily sorted order. https://github."}, {"q": "What updates or decisions were made in the discussion?", "a": "Merged. Thanks for doing this, [~boyork]. \n\nCouple minor things that would be great to be addressed as a followup PR:\n\n1. Link to the Mesos configuration table rather than just the Mesos page.\n\n2. If possible, make Mesos/YARN/Standalone appear in the table of contents on top."}]}}
{"project": "SPARK", "issue_id": "SPARK-1183", "title": "Inconsistent meaning of \"worker\" in docs", "status": "Resolved", "priority": "Major", "reporter": "Sanford Ryza", "assignee": "Sandy Ryza", "labels": [], "created": "2014-03-04T12:13:58.000+0000", "updated": "2014-04-04T20:50:07.000+0000", "description": "In the Spark Standalone Mode docs, \"worker\" refers to the long-running slave processes that start executor processes.  In most other docs, \"worker\" refers to the executor processes themselves.", "comments": ["The best thing to me would be to remove the word worker wherever possible.  I.e. refer to \"executor\" or \"slave\".  If master/slave isn't PC, we could use lord/vassal instead?"], "derived": {"summary": "In the Spark Standalone Mode docs, \"worker\" refers to the long-running slave processes that start executor processes. In most other docs, \"worker\" refers to the executor processes themselves.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Inconsistent meaning of \"worker\" in docs - In the Spark Standalone Mode docs, \"worker\" refers to the long-running slave processes that start executor processes. In most other docs, \"worker\" refers to the executor processes themselves."}, {"q": "What updates or decisions were made in the discussion?", "a": "The best thing to me would be to remove the word worker wherever possible.  I.e. refer to \"executor\" or \"slave\".  If master/slave isn't PC, we could use lord/vassal instead?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1184", "title": "Update the distribution tar.gz to include spark-assembly jar", "status": "Resolved", "priority": "Major", "reporter": "Mark Grover", "assignee": "Mark Grover", "labels": [], "created": "2014-03-04T14:30:28.000+0000", "updated": "2014-04-18T21:07:05.000+0000", "description": "This JIRA tracks 2 things:\n1. There seems to be something going on in our assembly generation logic because of which are two assembly jars.\nSomething like:\n{code}spark-assembly_2.10-1.0.0-SNAPSHOT.jar{code}\nand \n{code}spark-assembly_2.10-1.0.0-SNAPSHOT-hadoop2.0.5-alpha.jar{code}\n\nThe former is pretty bogus and doesn't contain any class files and should be gotten rid of. The latter contains all the good stuff. It essentially is the uber jar generated by the maven-shade-plugin\n\n2. The current bigtop-dist profile that builds the maven assembly (a .tar.gz file) using the maven-assembly-plugin includes the bogus jar and not the legit spark-assembly jar. We should get rid of the first one from this assembly (which would happen when we fix #1) and put the legit uber jar in it.\n\n3. Also, the bigtop-dist profile is meant to exclude the hadoop related jars from the distribution. It does a good job of doing so for org.apache.hadoop jars but misses the avro and zookeeper jars that are also provided by hadoop land.", "comments": ["Committed quite a while ago:\nhttps://github.com/apache/spark/commit/cda381f88cc03340fdf7b2d681699babbae2a56e\n\nResolving"], "derived": {"summary": "This JIRA tracks 2 things:\n1. There seems to be something going on in our assembly generation logic because of which are two assembly jars.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Update the distribution tar.gz to include spark-assembly jar - This JIRA tracks 2 things:\n1. There seems to be something going on in our assembly generation logic because of which are two assembly jars."}, {"q": "What updates or decisions were made in the discussion?", "a": "Committed quite a while ago:\nhttps://github.com/apache/spark/commit/cda381f88cc03340fdf7b2d681699babbae2a56e\n\nResolving"}]}}
{"project": "SPARK", "issue_id": "SPARK-1185", "title": "In Spark Programming Guide, \"Master URLs\" should mention yarn-client", "status": "Resolved", "priority": "Major", "reporter": "Sanford Ryza", "assignee": "Sanford Ryza", "labels": [], "created": "2014-03-04T14:42:07.000+0000", "updated": "2014-11-06T17:42:54.000+0000", "description": "It would also be helpful to mention that the reason a host:post isn't required for YARN mode is that it comes from the Hadoop configuration.", "comments": [], "derived": {"summary": "It would also be helpful to mention that the reason a host:post isn't required for YARN mode is that it comes from the Hadoop configuration.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "In Spark Programming Guide, \"Master URLs\" should mention yarn-client - It would also be helpful to mention that the reason a host:post isn't required for YARN mode is that it comes from the Hadoop configuration."}]}}
{"project": "SPARK", "issue_id": "SPARK-1186", "title": "Enrich the Spark Shell to support additional arguments.", "status": "Closed", "priority": "Major", "reporter": "Bernardo Gomez Palacio", "assignee": null, "labels": [], "created": "2014-03-04T15:59:00.000+0000", "updated": "2014-04-19T07:15:20.000+0000", "description": "Enrich the Spark Shell functionality to support the following options.\n\n{code:title=spark-shell.sh|borderStyle=solid}\nUsage: spark-shell [OPTIONS]\n\nOPTIONS:\n    -h  --help                           : Print this help information.\n    -c  --cores                         : The maximum number of cores to be used by the Spark Shell.\n    -em --executor-memory    : The memory used by each executor of the Spark Shell, the number\n                                                is followed by m for megabytes or g for gigabytes, e.g. \"1g\".\n    -dm --driver-memory         : The memory used by the Spark Shell, the number is followed\n                                                by m for megabytes or g for gigabytes, e.g. \"1g\".\n    -m  --master                       : A full string that describes the Spark Master, defaults to \"local\"\n                                                 e.g. \"spark://localhost:7077\".\n    --log-conf                           : Enables logging of the supplied SparkConf as INFO at start of the\n                                                Spark Context.\n\ne.g.\n    spark-shell -m spark://localhost:7077 -c 4 -dm 512m -em 2g\n{code}\n\n**Note**: the options described above are not visually aligned due JIRA's rendering, in the bash CLI they are.", "comments": ["Pull request provided\nhttps://github.com/apache/incubator-spark/pull/661", "Updated the options to be in-line with [SPARK-1126]", "Migrated the pull-request to the Spark Github Repository\nhttps://github.com/apache/spark/pull/84", "After a rebase based on apache/master and a squash commit I was unable to kill the previous pull-request (84) and had to open a new one (116).\n\nhttps://github.com/apache/spark/pull/116", "Github user asfgit closed the pull request at:\n\n    https://github.com/apache/spark/pull/116\n"], "derived": {"summary": "Enrich the Spark Shell functionality to support the following options. {code:title=spark-shell.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Enrich the Spark Shell to support additional arguments. - Enrich the Spark Shell functionality to support the following options. {code:title=spark-shell."}, {"q": "What updates or decisions were made in the discussion?", "a": "Github user asfgit closed the pull request at:\n\n    https://github.com/apache/spark/pull/116"}]}}
{"project": "SPARK", "issue_id": "SPARK-1187", "title": "Missing Pyspark methods", "status": "Resolved", "priority": "Major", "reporter": "Prabin Banka", "assignee": "Prabin Banka", "labels": [], "created": "2014-03-04T21:47:42.000+0000", "updated": "2014-03-06T12:46:33.000+0000", "description": "Following methods in SparkContext are missing,\n1) setJobGroup()\n2) setLocalProperty()\n3) getLocalProperty()\n4) sparkUser()\n\n", "comments": [], "derived": {"summary": "Following methods in SparkContext are missing,\n1) setJobGroup()\n2) setLocalProperty()\n3) getLocalProperty()\n4) sparkUser().", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Missing Pyspark methods - Following methods in SparkContext are missing,\n1) setJobGroup()\n2) setLocalProperty()\n3) getLocalProperty()\n4) sparkUser()."}]}}
{"project": "SPARK", "issue_id": "SPARK-1188", "title": "GraphX triplets not working properly", "status": "Resolved", "priority": "Major", "reporter": "Kev Alan", "assignee": "Daniel Darabos", "labels": [], "created": "2014-03-05T00:13:56.000+0000", "updated": "2014-05-29T08:20:34.000+0000", "description": "I followed the GraphX tutorial at http://ampcamp.berkeley.edu/big-data-mini-course/graph-analytics-with-graphx.html \n\non a local stand-alone cluster (Spark version 0.9.0) with two workers. Somehow, the graph.triplets is not returning what it should -- only Eds and Frans.\n\n```\nscala> graph.edges.toArray\n14/03/04 16:15:57 INFO SparkContext: Starting job: collect at EdgeRDD.scala:51\n14/03/04 16:15:57 INFO DAGScheduler: Got job 5 (collect at EdgeRDD.scala:51) with 1 output partitions (allowLocal=false)\n14/03/04 16:15:57 INFO DAGScheduler: Final stage: Stage 27 (collect at EdgeRDD.scala:51)\n14/03/04 16:15:57 INFO DAGScheduler: Parents of final stage: List()\n14/03/04 16:15:57 INFO DAGScheduler: Missing parents: List()\n14/03/04 16:15:57 INFO DAGScheduler: Submitting Stage 27 (MappedRDD[36] at map at EdgeRDD.scala:51), which has no missing parents\n14/03/04 16:15:57 INFO DAGScheduler: Submitting 1 missing tasks from Stage 27 (MappedRDD[36] at map at EdgeRDD.scala:51)\n14/03/04 16:15:57 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks\n14/03/04 16:15:57 INFO TaskSetManager: Starting task 27.0:0 as TID 11 on executor localhost: localhost (PROCESS_LOCAL)\n14/03/04 16:15:57 INFO TaskSetManager: Serialized task 27.0:0 as 2068 bytes in 1 ms\n14/03/04 16:15:57 INFO Executor: Running task ID 11\n14/03/04 16:15:57 INFO BlockManager: Found block rdd_2_0 locally\n14/03/04 16:15:57 INFO Executor: Serialized size of result for 11 is 936\n14/03/04 16:15:57 INFO Executor: Sending result for 11 directly to driver\n14/03/04 16:15:57 INFO Executor: Finished task ID 11\n14/03/04 16:15:57 INFO TaskSetManager: Finished TID 11 in 13 ms on localhost (progress: 0/1)\n14/03/04 16:15:57 INFO DAGScheduler: Completed ResultTask(27, 0)\n14/03/04 16:15:57 INFO TaskSchedulerImpl: Remove TaskSet 27.0 from pool\n14/03/04 16:15:57 INFO DAGScheduler: Stage 27 (collect at EdgeRDD.scala:51) finished in 0.015 s\n14/03/04 16:15:57 INFO SparkContext: Job finished: collect at EdgeRDD.scala:51, took 0.023602266 s\nres7: Array[org.apache.spark.graphx.Edge[Int]] = Array(Edge(2,1,7), Edge(2,4,2), Edge(3,2,4), Edge(3,6,3), Edge(4,1,1), Edge(5,2,2), Edge(5,3,8), Edge(5,6,3))\n\n\nscala> graph.vertices.toArray\n14/03/04 16:16:18 INFO SparkContext: Starting job: toArray at <console>:27\n14/03/04 16:16:18 INFO DAGScheduler: Got job 6 (toArray at <console>:27) with 1 output partitions (allowLocal=false)\n14/03/04 16:16:18 INFO DAGScheduler: Final stage: Stage 28 (toArray at <console>:27)\n14/03/04 16:16:18 INFO DAGScheduler: Parents of final stage: List(Stage 32, Stage 29)\n14/03/04 16:16:18 INFO DAGScheduler: Missing parents: List()\n14/03/04 16:16:18 INFO DAGScheduler: Submitting Stage 28 (VertexRDD[15] at RDD at VertexRDD.scala:52), which has no missing parents\n14/03/04 16:16:18 INFO DAGScheduler: Submitting 1 missing tasks from Stage 28 (VertexRDD[15] at RDD at VertexRDD.scala:52)\n14/03/04 16:16:18 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks\n14/03/04 16:16:18 INFO TaskSetManager: Starting task 28.0:0 as TID 12 on executor localhost: localhost (PROCESS_LOCAL)\n14/03/04 16:16:18 INFO TaskSetManager: Serialized task 28.0:0 as 2426 bytes in 0 ms\n14/03/04 16:16:18 INFO Executor: Running task ID 12\n14/03/04 16:16:18 INFO BlockManager: Found block rdd_14_0 locally\n14/03/04 16:16:18 INFO Executor: Serialized size of result for 12 is 947\n14/03/04 16:16:18 INFO Executor: Sending result for 12 directly to driver\n14/03/04 16:16:18 INFO Executor: Finished task ID 12\n14/03/04 16:16:18 INFO TaskSetManager: Finished TID 12 in 13 ms on localhost (progress: 0/1)\n14/03/04 16:16:18 INFO DAGScheduler: Completed ResultTask(28, 0)\n14/03/04 16:16:18 INFO TaskSchedulerImpl: Remove TaskSet 28.0 from pool\n14/03/04 16:16:18 INFO DAGScheduler: Stage 28 (toArray at <console>:27) finished in 0.015 s\n14/03/04 16:16:18 INFO SparkContext: Job finished: toArray at <console>:27, took 0.027839851 s\nres9: Array[(org.apache.spark.graphx.VertexId, (String, Int))] = Array((4,(David,42)), (2,(Bob,27)), (6,(Fran,50)), (5,(Ed,55)), (3,(Charlie,65)), (1,(Alice,28)))\n\n\nscala> graph.triplets.toArray\n14/03/04 16:16:30 INFO SparkContext: Starting job: toArray at <console>:27\n14/03/04 16:16:30 INFO DAGScheduler: Got job 7 (toArray at <console>:27) with 1 output partitions (allowLocal=false)\n14/03/04 16:16:31 INFO DAGScheduler: Final stage: Stage 33 (toArray at <console>:27)\n14/03/04 16:16:31 INFO DAGScheduler: Parents of final stage: List(Stage 34)\n14/03/04 16:16:31 INFO DAGScheduler: Missing parents: List()\n14/03/04 16:16:31 INFO DAGScheduler: Submitting Stage 33 (ZippedPartitionsRDD2[32] at zipPartitions at GraphImpl.scala:60), which has no missing parents\n14/03/04 16:16:31 INFO DAGScheduler: Submitting 1 missing tasks from Stage 33 (ZippedPartitionsRDD2[32] at zipPartitions at GraphImpl.scala:60)\n14/03/04 16:16:31 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks\n14/03/04 16:16:31 INFO TaskSetManager: Starting task 33.0:0 as TID 13 on executor localhost: localhost (PROCESS_LOCAL)\n14/03/04 16:16:31 INFO TaskSetManager: Serialized task 33.0:0 as 3322 bytes in 1 ms\n14/03/04 16:16:31 INFO Executor: Running task ID 13\n14/03/04 16:16:31 INFO BlockManager: Found block rdd_2_0 locally\n14/03/04 16:16:31 INFO BlockManager: Found block rdd_31_0 locally\n14/03/04 16:16:31 INFO Executor: Serialized size of result for 13 is 931\n14/03/04 16:16:31 INFO Executor: Sending result for 13 directly to driver\n14/03/04 16:16:31 INFO Executor: Finished task ID 13\n14/03/04 16:16:31 INFO TaskSetManager: Finished TID 13 in 17 ms on localhost (progress: 0/1)\n14/03/04 16:16:31 INFO DAGScheduler: Completed ResultTask(33, 0)\n14/03/04 16:16:31 INFO TaskSchedulerImpl: Remove TaskSet 33.0 from pool\n14/03/04 16:16:31 INFO DAGScheduler: Stage 33 (toArray at <console>:27) finished in 0.019 s\n14/03/04 16:16:31 INFO SparkContext: Job finished: toArray at <console>:27, took 0.037909394 s\nres10: Array[org.apache.spark.graphx.EdgeTriplet[(String, Int),Int]] = Array(((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3))\n```\n", "comments": ["The problem is that we are reusing an EdgeTriplet object. Try force a copy before you do the collect.\n\ne.g.\n\ntriplets.map(_.copy).collect()", "This has bitten us as well. I am no expert, but my impression is that re-using an object in these iterators was a terrible idea.\n\nConsider this example:\n\nscala> val g = graphx.util.GraphGenerators.starGraph(sc, 5)\nscala> g.edges.collect\nArray(Edge(1,0,1), Edge(2,0,1), Edge(3,0,1), Edge(4,0,1))\nscala> g.edges.map(x => x).collect\nArray(Edge(4,0,1), Edge(4,0,1), Edge(4,0,1), Edge(4,0,1))\n\nIt can and does go very wrong in practice too. Consider this:\n\nscala> g.edges.saveAsObjectFile(\"edges\")\nscala> sc.objectFile[graphx.Edge[Int]](\"edges\").collect\nArray(Edge(4,0,1), Edge(4,0,1), Edge(4,0,1), Edge(4,0,1))\n\nIndeed map(_.copy) is a good workaround. But seems like this is a nasty trap laid out for your users. Did you measure the performance gain? If not, or if it's insignificant, I would suggest not re-using the object in the iterators. It would simplify the GraphX source too. However, if it is significant, one idea is to offer separate mapFast() methods that do re-use, while the more commonly used map() would offer the more commonly expected (copying) semantics.\n\nAlso I wish GraphX offered graph save/load functionality. If it did, I would only have discovered this bug a few weeks from now :).", "Sorry, I forgot to test your workaround before commenting. It does not work.\n\nscala> g.triplets.map(_.copy).collect\n<console>:16: error: missing arguments for method copy in class Edge;\nfollow this method with `_' if you want to treat it as a partially applied function\n              g.triplets.map(_.copy).collect\n                               ^\nYou can of course write a copy function yourself.\n\nI'd be happy to work on this. Can I just get rid of the re-use, or would you prefer another approach?", "Okay, it was just missing the parentheses:\n\nscala> g.triplets.map(_.copy()).collect\nres1: Array[org.apache.spark.graphx.Edge[Int]] = Array(Edge(1,0,1), Edge(2,0,1), Edge(3,0,1), Edge(4,0,1))\n\n(Scala novice here :).)", "I've sent a pull request to eliminate the re-use (https://github.com/apache/spark/pull/276). The change has no performance impact and simplifies the code.", "The changes are in the master branch now. I can't figure out how to close a JIRA ticket :).", "I added you to contributor list so you should be able to edit in the future. Cheers.", "Adding a link to the commit: https://github.com/apache/spark/commit/78236334e4ca7518b6d7d9b38464dbbda854a777#diff-a2b19aac11cb2fbe9962b5d2290ea77e"], "derived": {"summary": "I followed the GraphX tutorial at http://ampcamp. berkeley.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "GraphX triplets not working properly - I followed the GraphX tutorial at http://ampcamp. berkeley."}, {"q": "What updates or decisions were made in the discussion?", "a": "Adding a link to the commit: https://github.com/apache/spark/commit/78236334e4ca7518b6d7d9b38464dbbda854a777#diff-a2b19aac11cb2fbe9962b5d2290ea77e"}]}}
{"project": "SPARK", "issue_id": "SPARK-1189", "title": "Support authentication between Spark components", "status": "Resolved", "priority": "Major", "reporter": "Thomas Graves", "assignee": "Thomas Graves", "labels": [], "created": "2014-03-05T06:01:26.000+0000", "updated": "2014-03-06T17:09:24.000+0000", "description": "Add Security to Spark - Akka, Http, ConnectionManager, UI use servlets.\n\nsee PR https://github.com/apache/spark/pull/33.\n\nThe design doc is written in the javadoc for the SecurityManager.\n\nThis is to add basic authentication to the spark communication channels using a shared secret.  The UI can also be protected by adding Filters. \n\nThe main focus of this jira is for Spark on Yarn.  For spark on Yarn the Hadoop UGI can be used to distributed the shared secret when the application is deployed and there will be a separate shared secret for each application.\n\nThe authentication mechanisms added here can also be used in other deploys( standalone, mesos). But the distribution method will be from a simply config that needs to be shared amongst all the master/workers and application. ", "comments": ["Hey Tom just renamed this to be more clear about the scope. Hope that's alright, just want people to understand that this patch is focused on authentication."], "derived": {"summary": "Add Security to Spark - Akka, Http, ConnectionManager, UI use servlets. see PR https://github.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support authentication between Spark components - Add Security to Spark - Akka, Http, ConnectionManager, UI use servlets. see PR https://github."}, {"q": "What updates or decisions were made in the discussion?", "a": "Hey Tom just renamed this to be more clear about the scope. Hope that's alright, just want people to understand that this patch is focused on authentication."}]}}
{"project": "SPARK", "issue_id": "SPARK-1190", "title": "Do not initialize log4j if slf4j log4j backend is not being used", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "labels": [], "created": "2014-03-05T11:09:16.000+0000", "updated": "2014-10-06T04:29:40.000+0000", "description": "I already have a patch here just need to test it and commit. IIRC there were some issues with the maven build.\n\nhttps://github.com/apache/incubator-spark/pull/573\nhttps://github.com/pwendell/incubator-spark/commit/66594e88e5be50fca073a7ef38fa62db4082b3c8\n\ninitialization with: java.lang.StackOverflowError\n\tat java.lang.ThreadLocal.access$400(ThreadLocal.java:72)\n\tat java.lang.ThreadLocal$ThreadLocalMap.getEntry(ThreadLocal.java:376)\n\tat java.lang.ThreadLocal$ThreadLocalMap.access$000(ThreadLocal.java:261)\n\tat java.lang.ThreadLocal.get(ThreadLocal.java:146)\n\tat java.lang.StringCoding.deref(StringCoding.java:63)\n\tat java.lang.StringCoding.encode(StringCoding.java:330)\n\tat java.lang.String.getBytes(String.java:916)\n\tat java.io.UnixFileSystem.getBooleanAttributes0(Native Method)\n\tat java.io.UnixFileSystem.getBooleanAttributes(UnixFileSystem.java:242)\n\tat java.io.File.exists(File.java:813)\n\tat sun.misc.URLClassPath$FileLoader.getResource(URLClassPath.java:1080)\n\tat sun.misc.URLClassPath$FileLoader.findResource(URLClassPath.java:1047)\n\tat sun.misc.URLClassPath.findResource(URLClassPath.java:176)\n\tat java.net.URLClassLoader$2.run(URLClassLoader.java:551)\n\tat java.net.URLClassLoader$2.run(URLClassLoader.java:549)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findResource(URLClassLoader.java:548)\n\tat java.lang.ClassLoader.getResource(ClassLoader.java:1147)\n\tat org.apache.spark.Logging$class.initializeLogging(Logging.scala:109)\n\tat org.apache.spark.Logging$class.initializeIfNecessary(Logging.scala:97)\n\tat org.apache.spark.Logging$class.log(Logging.scala:36)\n\tat org.apache.spark.util.Utils$.log(Utils.scala:47)", "comments": [], "derived": {"summary": "I already have a patch here just need to test it and commit. IIRC there were some issues with the maven build.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Do not initialize log4j if slf4j log4j backend is not being used - I already have a patch here just need to test it and commit. IIRC there were some issues with the maven build."}]}}
{"project": "SPARK", "issue_id": "SPARK-1191", "title": "Convert configs to use SparkConf", "status": "Resolved", "priority": "Major", "reporter": "Thomas Graves", "assignee": null, "labels": [], "created": "2014-03-05T11:20:12.000+0000", "updated": "2015-11-08T11:12:17.000+0000", "description": "There are many places in the yarn code that still use System.setProperty. We should convert those to use the SparkConf.  \n\nOne specific example is SPARK_YARN_MODE. There are others in like ApplicationMaster and Client.\n\nNote that currently some configs can't be set in sparkConf and properly picked up with SparkContext as sparkConf isn't really shared with the SparkContext.  The only time we can get the sparkContext is after its been instantiated which is to late.  ", "comments": ["Note specifically on the SPARK_YARN_MODE config we may want to find a better way to indicate yarn mode. Its possible that if the user does certain operations before creating SparkContext in yarn-client mode that it actually create the SparkHadoopUtil.hadoop with the wrong class (it doesn't use the YarnSparkHadoopUtil when it should).", "User 'CodingCat' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/2312", "Is this JIRA still relevant? The PR seems to be closed. ", "At this stage I don't think this will proceed"], "derived": {"summary": "There are many places in the yarn code that still use System. setProperty.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Convert configs to use SparkConf - There are many places in the yarn code that still use System. setProperty."}, {"q": "What updates or decisions were made in the discussion?", "a": "At this stage I don't think this will proceed"}]}}
{"project": "SPARK", "issue_id": "SPARK-1192", "title": "Around 30 parameters in Spark are used but undocumented and some are having confusing name", "status": "Resolved", "priority": "Major", "reporter": "Nan Zhu", "assignee": "Nan Zhu", "labels": [], "created": "2014-03-05T16:22:48.000+0000", "updated": "2015-02-12T20:37:28.000+0000", "description": "I grep the code in core component, I found that around 30 parameters in the implementation is actually used but undocumented. By reading the source code, I found that some of them are actually very useful for the user.\n\nI suggest to make a complete document on the parameters. \n\nAlso some parameters are having confusing names\n\n\nspark.shuffle.copier.threads - this parameters is to control how many threads you will use when you start a Netty-based shuffle service....but from the name, we cannot get this information\n\nspark.shuffle.sender.port - the similar problem with the above one, when you use Netty-based shuffle receiver, you will have to setup a Netty-based sender...this parameter is to setup the port used by the Netty sender, but the name cannot convey this information\n", "comments": ["PR is actually at https://github.com/apache/spark/pull/2312 and is misnamed. Is this still live though?", "User 'CodingCat' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/2312", "yes, I resubmitted https://github.com/apache/spark/pull/2312 for Matei's request (removed some, add some....)\n\nit's still valid", "PR was withdrawn; this probably deserves a rethink if it were reconsidered anyway so let's resolve."], "derived": {"summary": "I grep the code in core component, I found that around 30 parameters in the implementation is actually used but undocumented. By reading the source code, I found that some of them are actually very useful for the user.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Around 30 parameters in Spark are used but undocumented and some are having confusing name - I grep the code in core component, I found that around 30 parameters in the implementation is actually used but undocumented. By reading the source code, I found that some of them are actually very useful for the user."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR was withdrawn; this probably deserves a rethink if it were reconsidered anyway so let's resolve."}]}}
{"project": "SPARK", "issue_id": "SPARK-1193", "title": "Inconsistent indendation between pom.xmls", "status": "Resolved", "priority": "Major", "reporter": "Sanford Ryza", "assignee": "Sandy Ryza", "labels": [], "created": "2014-03-05T18:01:30.000+0000", "updated": "2014-04-04T20:50:55.000+0000", "description": "e.g. core/pom.xml uses 4 spaces and graphx/pom.xml uses 2", "comments": ["https://github.com/apache/spark/pull/91"], "derived": {"summary": "e. g.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Inconsistent indendation between pom.xmls - e. g."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/91"}]}}
{"project": "SPARK", "issue_id": "SPARK-1194", "title": "The same-RDD rule for cache replacement is not properly implemented", "status": "Resolved", "priority": "Major", "reporter": "liancheng", "assignee": "liancheng", "labels": [], "created": "2014-03-06T07:45:12.000+0000", "updated": "2014-03-07T23:28:59.000+0000", "description": "The same-RDD rule for cache replacement described in the original RDD paper prevents cycling partitions from the same RDD in and out. [Commit 6098f7e|https://github.com/apache/spark/commit/6098f7e87a88d0b847c402b95510cb07352db643#diff-f82f9761c5f006f4b2a7efb57ccb7699R182] meant to implement this, but I believe it introduced some problems.\n\nIn the current implementation, when selecting candidate blocks to be swapped out, once we find a block from the same RDD that the block to be stored belongs to, [cache eviction fails  and aborts|https://github.com/apache/spark/commit/6098f7e87a88d0b847c402b95510cb07352db643#diff-f82f9761c5f006f4b2a7efb57ccb7699R185]. -Also, LRU eviction (as described in the paper) is not employed.-\n\nA possible cache eviction strategy can be: keep selecting blocks _not_ from the RDD that the block to be stored belongs to until either enough free space can be ensured (cache eviction succeeds) or all such blocks are checked (cache eviction fails).\n\n-LRU should also be employed, but not necessarily in this issue.-\n\nAny thoughts? Especially, did I miss any apparent facts behind current implementation?\n\n*Update*: LRU is implemented with {{LinkedHashMap}} by setting constructor argument {{accessOrder}} to {{true}}.", "comments": ["Is it possible to make the replacement policy as pluggable?"], "derived": {"summary": "The same-RDD rule for cache replacement described in the original RDD paper prevents cycling partitions from the same RDD in and out. [Commit 6098f7e|https://github.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "The same-RDD rule for cache replacement is not properly implemented - The same-RDD rule for cache replacement described in the original RDD paper prevents cycling partitions from the same RDD in and out. [Commit 6098f7e|https://github."}, {"q": "What updates or decisions were made in the discussion?", "a": "Is it possible to make the replacement policy as pluggable?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1195", "title": "set map_input_file environment variable in PipedRDD", "status": "Resolved", "priority": "Major", "reporter": "Thomas Graves", "assignee": "Thomas Graves", "labels": [], "created": "2014-03-06T09:07:47.000+0000", "updated": "2014-03-07T10:38:34.000+0000", "description": "Hadoop uses the config mapreduce.map.input.file to indicate the input filename to the map when the input split is of type FileSplit.  Some of the hadoop input and output formats set or use this config.  This config can also be used by user code.   \n\nPipedRDD runs an external process and the configs aren't available to that process.  Hadoop Streaming does something very similar and the way they make configs available is exporting them into the environment replacing '.' with '_'.    Spark should also export this variable when launching the pipe command so the user code has access to that config.\n\nNote that the config mapreduce.map.input.file is the new one, the old one which is deprecated but not yet removed is map.input.file. So we should handle both.\n", "comments": ["https://github.com/apache/spark/pull/94"], "derived": {"summary": "Hadoop uses the config mapreduce. map.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "set map_input_file environment variable in PipedRDD - Hadoop uses the config mapreduce. map."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/94"}]}}
{"project": "SPARK", "issue_id": "SPARK-1196", "title": "val variables not available within RDD map on cluster app; are on shell or local", "status": "Resolved", "priority": "Major", "reporter": "Andrew Kerr", "assignee": null, "labels": [], "created": "2014-03-06T12:05:59.000+0000", "updated": "2014-11-08T10:14:39.000+0000", "description": "When this code\n{code}\ndef foo = \"foo\"\n  val bar = \"bar\"\n\n  val data = sc.parallelize(Seq(\"a\"))\n\n  data.map{a => print(1,foo,bar);a}.map{a => print(2,foo,bar);a}.map{a => print(3,foo,bar);a}.collect()\n{code}\nis run on a cluster on the spark shell a slave's stdout is\n{code}\n(1,foo,bar)(2,foo,bar)(3,foo,bar)\n{code}\nas expected.\n\nHowever when the code\n{code}\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.SparkContext._\n\nobject twitterAggregation extends App {\n\n  val conf = new SparkConf()\n    .setMaster(\"spark://xx.compute-1.amazonaws.com:7077\")\n    .setAppName(\"testCase\")\n    .setJars(List(\"target/scala-2.10/spark-test-case_2.10-1.0.jar\"))\n    .setSparkHome(\"/root/spark/\")\n  val sc = new SparkContext(conf)\n\n  def foo = \"foo\"\n  val bar = \"bar\"\n\n  val data = sc.parallelize(Seq(\"a\"))\n\n  data.map{a => print(1,foo,bar);a}.map{a => print(2,foo,bar);a}.map{a => print(3,foo,bar);a}.collect()\n}\n{code}\nis run against a cluster as an application via sbt the stdout on a slave is\n{code}\n(1,foo,null)(2,foo,null)(3,foo,null)\n{code}\nThe variable declared with val is now null when the anon functions in the map are executed.\n\nWhen the application is run in local mode the output is \n{code}\n(1,foo,bar)(2,foo,bar)(3,foo,bar)\n{code}\nas wanted.\n\nbuild.sbt is \n{code}\nname := \"spark-test-case\"\n\nversion := \"1.0\"\n\nscalaVersion:=\"2.10.3\"\n\nresolvers ++= Seq(\"Akka Repository\" at \"http://repo.akka.io/releases/\")\n\nlibraryDependencies ++= Seq(\"org.apache.spark\" % \"spark-core_2.10\" % \"0.9.0-incubating\")\n{code}\n\nTo avoid firewall and NAT issues the project directory is rsynced onto the master where is is build with SBT 0.13.1\n{code}\nwget http://repo.scala-sbt.org/scalasbt/sbt-native-packages/org/scala-sbt/sbt/0.13.1/sbt.rpm && rpm --install sbt.rpm\nsbt package && sbt run\n{code}\n\nCluster created with scripts in the hadoop2 0.9.0 download.", "comments": ["Hi Andrew !\nCan you kindly re-confirm if the issue persists ? It's not reproducible for a standalone cluster with 3 slaves. I tried against the same scala / spark versions."], "derived": {"summary": "When this code\n{code}\ndef foo = \"foo\"\n  val bar = \"bar\"\n\n  val data = sc. parallelize(Seq(\"a\"))\n\n  data.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "val variables not available within RDD map on cluster app; are on shell or local - When this code\n{code}\ndef foo = \"foo\"\n  val bar = \"bar\"\n\n  val data = sc. parallelize(Seq(\"a\"))\n\n  data."}, {"q": "What updates or decisions were made in the discussion?", "a": "Hi Andrew !\nCan you kindly re-confirm if the issue persists ? It's not reproducible for a standalone cluster with 3 slaves. I tried against the same scala / spark versions."}]}}
{"project": "SPARK", "issue_id": "SPARK-1197", "title": "Rename yarn-standalone and fix up docs for running on YARN", "status": "Resolved", "priority": "Major", "reporter": "Sanford Ryza", "assignee": "Sandy Ryza", "labels": [], "created": "2014-03-06T12:31:52.000+0000", "updated": "2014-04-04T20:50:47.000+0000", "description": "yarn-standalone is a confusing name because the use of \"standalone\" is different than the use in the sense of Spark standalone cluster manager.  It would also be nice to fix up some typos in the YARN docs and add a section on how to view container logs.", "comments": ["https://github.com/apache/spark/pull/95"], "derived": {"summary": "yarn-standalone is a confusing name because the use of \"standalone\" is different than the use in the sense of Spark standalone cluster manager. It would also be nice to fix up some typos in the YARN docs and add a section on how to view container logs.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Rename yarn-standalone and fix up docs for running on YARN - yarn-standalone is a confusing name because the use of \"standalone\" is different than the use in the sense of Spark standalone cluster manager. It would also be nice to fix up some typos in the YARN docs and add a section on how to view container logs."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/95"}]}}
{"project": "SPARK", "issue_id": "SPARK-1198", "title": "Allow pipes tasks to run in different sub-directories", "status": "Resolved", "priority": "Major", "reporter": "Thomas Graves", "assignee": "Thomas Graves", "labels": [], "created": "2014-03-06T12:45:14.000+0000", "updated": "2014-04-05T00:17:11.000+0000", "description": "Currently when a task runs, its working directory is the same as all the other tasks running on that Worker.  If the tasks happen to output files to that working directory with the same name, collisions happen.\n\nWe should add an option to allow the tasks to run in separate sub-directories to avoid those conflicts. \n\nI should clarify that the specific concern is when running the pipes command.", "comments": [], "derived": {"summary": "Currently when a task runs, its working directory is the same as all the other tasks running on that Worker. If the tasks happen to output files to that working directory with the same name, collisions happen.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Allow pipes tasks to run in different sub-directories - Currently when a task runs, its working directory is the same as all the other tasks running on that Worker. If the tasks happen to output files to that working directory with the same name, collisions happen."}]}}
{"project": "SPARK", "issue_id": "SPARK-1199", "title": "Type mismatch in Spark shell when using case class defined in shell", "status": "Resolved", "priority": "Blocker", "reporter": "Andrew Kerr", "assignee": "Prashant Sharma", "labels": [], "created": "2014-03-06T12:51:03.000+0000", "updated": "2016-06-15T18:08:57.000+0000", "description": "*NOTE: This issue was fixed in 1.0.1, but the fix was reverted in Spark 1.0.2 pending further testing. The final fix will be in Spark 1.1.0.*\n\nDefine a class in the shell:\n{code}\ncase class TestClass(a:String)\n{code}\n\nand an RDD\n{code}\nval data = sc.parallelize(Seq(\"a\")).map(TestClass(_))\n{code}\n\ndefine a function on it and map over the RDD\n{code}\ndef itemFunc(a:TestClass):TestClass = a\ndata.map(itemFunc)\n{code}\n\nError:\n{code}\n<console>:19: error: type mismatch;\n found   : TestClass => TestClass\n required: TestClass => ?\n              data.map(itemFunc)\n{code}\n\nSimilarly with a mapPartitions:\n{code}\ndef partitionFunc(a:Iterator[TestClass]):Iterator[TestClass] = a\ndata.mapPartitions(partitionFunc)\n{code}\n\n{code}\n<console>:19: error: type mismatch;\n found   : Iterator[TestClass] => Iterator[TestClass]\n required: Iterator[TestClass] => Iterator[?]\nError occurred in an application involving default arguments.\n              data.mapPartitions(partitionFunc)\n{code}\n\nThe behavior is the same whether in local mode or on a cluster.\n\nThis isn't specific to RDDs. A Scala collection in the Spark shell has the same problem.\n\n{code}\nscala> Seq(TestClass(\"foo\")).map(itemFunc)\n<console>:15: error: type mismatch;\n found   : TestClass => TestClass\n required: TestClass => ?\n              Seq(TestClass(\"foo\")).map(itemFunc)\n                                        ^\n{code}\n\nWhen run in the Scala console (not the Spark shell) there are no type mismatch errors.", "comments": ["This looks similar to the problem reported in https://groups.google.com/d/msg/spark-users/bwAmbUgxWrA/HwP4Nv4adfEJ", "The workaround from the above link doesn't help.\n\n{code}\nobject TestClasses {\ncase class TestClass(a:String)\n}\nimport TestClasses._\ndef itemFunc(a:TestClass):TestClass = a\nSeq(TestClass(\"foo\")).map(itemFunc)\n{code}\n\n{code}\n<console>:22: error: type mismatch;\n found   : TestClasses.TestClass => TestClasses.TestClass\n required: TestClasses.TestClass => ?\n              data.map(itemFunc)\n                       ^\n{code}\n\nFrom the Spark shell both locally and on a cluster.", "I think I may be hitting a related issue when trying to created nested classes for use in SparkSQL.\n\n{code}\nscala> case class A(a: String)\ndefined class A\nscala> case class B(b: A)\ndefined class B\nscala> B(A(\"a\"))\n<console>:22: error: type mismatch;\n found   : A\n required: A\n              B(A(\"a\"))\n                 ^\n{code}\n\nIn this case, the described workaround does work, but is kind of annoying.  Mostly I want to bump this thread since Matei had talked about maybe fixing this using macros?  Is that something we would like to consider for a future release.", "+1 to fixing this. We're affected as well. Classes defined in Shell are inner classes, and therefore cannot be easily instantiated by reflection. They need additional reference to the outer object, which is non-trivial to obtain (is it obtainable at all without modifying Spark?). \n\n{noformat}\nscala> class Test\ndefined class Test\n\nscala> new Test\nres5: Test = $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$Test@4f755864\n\n// good, so there is a default constructor and we can call it through reflection?\n// not so fast...\nscala> classOf[Test].getConstructor()\njava.lang.NoSuchMethodException: $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$Test.<init>()\n...\n\nscala> classOf[Test].getConstructors()(0)\nres7: java.lang.reflect.Constructor[_] = public $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$Test($iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC)\n       \n{noformat}\n\nThe workaround does not work for us.\n", "I have something of a workaround:\n\n{code}\nobject MyTypes {\n  case class TestClass(a:Int)\n}\n\nobject MyLogic {\n  import MyClasses._\n  def fn(b:TestClass) = TestClass(b.a * 2)\n  val result = Seq(TestClass(1)).map(fn)\n}\n\nMyLogic.result\n// Seq[MyTypes.TestClass] = List(TestClass(2))\n{code}\n\nStill can't access TestClass outside an object.", "See also additional test cases in https://issues.apache.org/jira/browse/SPARK-1836 which has now been marked as a duplicate.", "Prashant said he could look into this - so I'm assigning it to him.", "One work around is to use `:paste` command of repl to work with these kind of scenarios. So if you use :paste and put the whole thing at once it will work nicely. I am just mentioning it because I found it, we also have a slightly better fix on github PR. ", "More examples on https://issues.apache.org/jira/browse/SPARK-2330 which should also be a duplicate", "Resolved via:\nhttps://github.com/apache/spark/pull/1179", "Just a note, I've reverted this fix in branch-1.0 the fix here caused other issues that were worse than the original bug (SPARK-2452). This will be fixed in 1.1.", "User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1179", "User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1176", "I have problems with declaring case classes in shell, Spark 1.6\n\nThis doesn't work for me:\n\n{code}\nscala> case class ABCD()\ndefined class ABCD\n\nscala> new ABCD()\nres33: ABCD = ABCD()\n\nscala> classOf[ABCD].getConstructor()\njava.lang.NoSuchMethodException: $iwC$$iwC$ABCD.<init>()\n at java.lang.Class.getConstructor0(Class.java:3074)\n at java.lang.Class.getConstructor(Class.java:1817)\n\nscala> classOf[ABCD].getConstructors()\nres31: Array[java.lang.reflect.Constructor[_]] = Array(public $iwC$$iwC$ABCD($iwC$$iwC))\n{code}", "All classes defined in the REPL are inner classes due to the way compilation works.  Therefore there is not going to be a no-arg constructor.  This is expected behavior.", " Michael Armbrust,\nin my use case I have a library that relies on having a default constructor and I want to use this library in the REPL. Any workaround for that?", "You will have to define your case classes in a jar instead of the REPL.", "Thanks, any other options? I want to be able to define classes in the REPL.", "Not that I know of.  Also, please use the spark-user list instead of JIRA for tech support questions :)", "Did you try the :paste option ?", "Yes, I did. It doesn't help, the inner class still doesn't have a no-arg constructor visible with a reflection."], "derived": {"summary": "*NOTE: This issue was fixed in 1. 0.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Type mismatch in Spark shell when using case class defined in shell - *NOTE: This issue was fixed in 1. 0."}, {"q": "What updates or decisions were made in the discussion?", "a": "Yes, I did. It doesn't help, the inner class still doesn't have a no-arg constructor visible with a reflection."}]}}
{"project": "SPARK", "issue_id": "SPARK-1200", "title": "Make it possible to use unmanaged AM in yarn-client mode", "status": "Resolved", "priority": "Major", "reporter": "Sanford Ryza", "assignee": "Sandy Ryza", "labels": [], "created": "2014-03-06T14:08:54.000+0000", "updated": "2016-01-16T13:31:14.000+0000", "description": "Using an unmanaged AM in yarn-client mode would allow apps to start up faster, but not requiring the container launcher AM to be launched on the cluster.", "comments": ["You know, we could benefit all YARN apps if the AM launch request could also include a list of container requests to satisfy. This would remove the pipeline of client->AM->containers, and start requesting containers earlier. The allocated container list could just come in as callbacks once the AM is live, as container losses do on AM restart.\n\nIt means the client would have to come up with an initial assessment of the priority containers to escalate. But publishing that information at launch time would help with the (proposed) Gang scheduling.", "I think Sandy (Ryza) reported this too but I doubt this is still on the radar"], "derived": {"summary": "Using an unmanaged AM in yarn-client mode would allow apps to start up faster, but not requiring the container launcher AM to be launched on the cluster.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Make it possible to use unmanaged AM in yarn-client mode - Using an unmanaged AM in yarn-client mode would allow apps to start up faster, but not requiring the container launcher AM to be launched on the cluster."}, {"q": "What updates or decisions were made in the discussion?", "a": "I think Sandy (Ryza) reported this too but I doubt this is still on the radar"}]}}
{"project": "SPARK", "issue_id": "SPARK-1201", "title": "Do not materialize partitions whenever possible in BlockManager", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Andrew Or", "labels": [], "created": "2014-03-06T14:49:28.000+0000", "updated": "2014-09-16T17:43:14.000+0000", "description": "This is a slightly more complex version of SPARK-942 where we try to avoid unrolling iterators in other situations where it is possible. SPARK-942 focused on the case where the DISK_ONLY storage level was used. There are other cases though, such as if data is stored serialized and in memory and but there is not enough memory left to store the RDD.", "comments": ["What causes this to not be fixable within the scope of 1.0.1?", "Depends on when we release 1.0.1. I am actually working on it at this moment, and I am close to having a PR.", "Okay, but my question is really whether resolution of this issue will require new API that will exclude it from consideration from 1.0.x or whether this will be just implementation details that can be considered a bug fix and included in the maintenance branch.", "It will mainly be a bug fix that doesn't change the API.", "This was solved by SPARK-1777."], "derived": {"summary": "This is a slightly more complex version of SPARK-942 where we try to avoid unrolling iterators in other situations where it is possible. SPARK-942 focused on the case where the DISK_ONLY storage level was used.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Do not materialize partitions whenever possible in BlockManager - This is a slightly more complex version of SPARK-942 where we try to avoid unrolling iterators in other situations where it is possible. SPARK-942 focused on the case where the DISK_ONLY storage level was used."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was solved by SPARK-1777."}]}}
{"project": "SPARK", "issue_id": "SPARK-1202", "title": "Add a \"cancel\" button in the UI for stages", "status": "Resolved", "priority": "Critical", "reporter": "Patrick Wendell", "assignee": "Sundeep Narravula", "labels": [], "created": "2014-03-06T21:24:39.000+0000", "updated": "2014-04-21T17:55:38.000+0000", "description": "Seems like this would be really useful for people. It's not that hard, we just need to lookup the jobs associated with the stage and kill them. Might involve exposing some additional API's in SparkContext.", "comments": ["Github user sundeepn commented on the pull request:\n\n    https://github.com/apache/spark/pull/246#issuecomment-39011156\n  \n    @andrewor14 / @kayousterhout.  Thanks for the comments.\n    I have posted an additional commit with the changes. I have checked them in my branch but some reason they are not showing up here. Do you want me to issue another new pull request? Let me know.\n    \n    https://github.com/sundeepn/spark/commit/32c2ea58e5053623e5bf5955e52f7b7cd640ff64\n\n", "Github user AmplabJenkins commented on the pull request:\n\n    https://github.com/apache/spark/pull/246#issuecomment-39011323\n  \n     Build triggered. Build is starting -or- tests failed to complete.\n", "Github user AmplabJenkins commented on the pull request:\n\n    https://github.com/apache/spark/pull/246#issuecomment-39011326\n  \n    Build started. Build is starting -or- tests failed to complete.\n", "Github user AmplabJenkins commented on the pull request:\n\n    https://github.com/apache/spark/pull/246#issuecomment-39011375\n  \n    Build finished. Build is starting -or- tests failed to complete.\n", "Github user AmplabJenkins commented on the pull request:\n\n    https://github.com/apache/spark/pull/246#issuecomment-39011376\n  \n    Build is starting -or- tests failed to complete.\n    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/13581/\n", "Github user kayousterhout commented on the pull request:\n\n    https://github.com/apache/spark/pull/246#issuecomment-39011464\n  \n    It looks like github is just moving slowly today...the commit just got pulled in.  I took another look at this and have a question: what happens for stages that are used for multiple jobs?  Right now, stageIdToJobId in the UI code you added just maps a stage to a single job id.  So, if stage0 is used by JobA and jobB, the ui code only stores one of these jobs, and then cancelJob() will only be called for one of the jobs.  cancelJob() ultimately calls DAGScheduler.handleJobCancellation(), which only cancels the stages that are independent to the job.  So, because stage0 is not independent to either of the jobs, it won't get cancelled.  Did I misunderstand this?\n", "Github user kayousterhout commented on a diff in the pull request:\n\n    https://github.com/apache/spark/pull/246#discussion_r11095610\n  \n    --- Diff: core/src/main/scala/org/apache/spark/ui/jobs/JobProgressListener.scala ---\n    @@ -116,6 +118,16 @@ private[ui] class JobProgressListener(conf: SparkConf) extends SparkListener {\n     \n         val stages = poolToActiveStages.getOrElseUpdate(poolName, new HashMap[Int, StageInfo]())\n         stages(stage.stageId) = stage\n    +    \n    +    // Extract Job ID and double check if we have the details\n    +    val jobId = Option(stageSubmitted.properties).flatMap {\n    +      p => Option(p.getProperty(\"spark.job.id\"))\n    +    }.getOrElse(\"-1\").toInt\n    --- End diff --\n    \n    When we chatted about this I remember you saying that this code is to handle the case where the stage runs locally at the driver...but from glancing at the DAGScheduler code, it looks like onStageSubmitted() never gets called for the locally-run tasks, and they never show up at the UI.\n    \n    When do you need this code / when will the jobIdToStageIds mapping not already be set up correctly by OnJobStart?\n", "Github user sundeepn commented on a diff in the pull request:\n\n    https://github.com/apache/spark/pull/246#discussion_r11095628\n  \n    --- Diff: core/src/main/scala/org/apache/spark/ui/jobs/JobProgressListener.scala ---\n    @@ -116,6 +118,16 @@ private[ui] class JobProgressListener(conf: SparkConf) extends SparkListener {\n     \n         val stages = poolToActiveStages.getOrElseUpdate(poolName, new HashMap[Int, StageInfo]())\n         stages(stage.stageId) = stage\n    +    \n    +    // Extract Job ID and double check if we have the details\n    +    val jobId = Option(stageSubmitted.properties).flatMap {\n    +      p => Option(p.getProperty(\"spark.job.id\"))\n    +    }.getOrElse(\"-1\").toInt\n    --- End diff --\n    \n    Well, this is only to ensure we can handle things if we get any scenarios where the onJobStart does not arrive before stageSubmitted. I am not familiar with the scheduling code sufficiently to rule that out. If you are sure, I can take this out.\n", "Github user kayousterhout commented on a diff in the pull request:\n\n    https://github.com/apache/spark/pull/246#discussion_r11095648\n  \n    --- Diff: core/src/main/scala/org/apache/spark/ui/jobs/JobProgressListener.scala ---\n    @@ -116,6 +118,16 @@ private[ui] class JobProgressListener(conf: SparkConf) extends SparkListener {\n     \n         val stages = poolToActiveStages.getOrElseUpdate(poolName, new HashMap[Int, StageInfo]())\n         stages(stage.stageId) = stage\n    +    \n    +    // Extract Job ID and double check if we have the details\n    +    val jobId = Option(stageSubmitted.properties).flatMap {\n    +      p => Option(p.getProperty(\"spark.job.id\"))\n    +    }.getOrElse(\"-1\").toInt\n    --- End diff --\n    \n    Ah cool -- I looked at the ordering of the JobStart and StageSubmitted events more closely and I think you can safely remove this.\n", "Github user sundeepn commented on the pull request:\n\n    https://github.com/apache/spark/pull/246#issuecomment-39012183\n  \n    > what happens for stages that are used for multiple jobs?\n    \n    I misunderstood our conversation on job to stage mapping the other day. As you see, the code will currently not handle multiple job mappings. Is there a simple example I can use to generate such a scenario?\n"], "derived": {"summary": "Seems like this would be really useful for people. It's not that hard, we just need to lookup the jobs associated with the stage and kill them.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add a \"cancel\" button in the UI for stages - Seems like this would be really useful for people. It's not that hard, we just need to lookup the jobs associated with the stage and kill them."}, {"q": "What updates or decisions were made in the discussion?", "a": "Github user sundeepn commented on the pull request:\n\n    https://github.com/apache/spark/pull/246#issuecomment-39012183\n  \n    > what happens for stages that are used for multiple jobs?\n    \n    I misunderstood our conversation on job to stage mapping the other day. As you see, the code will currently not handle multiple job mappings. Is there a simple example I can use to generate such a scenario?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1203", "title": "spark-shell on yarn-client race in properly getting hdfs delegation tokens", "status": "Closed", "priority": "Blocker", "reporter": "Thomas Graves", "assignee": "Thomas Graves", "labels": [], "created": "2014-03-07T11:31:02.000+0000", "updated": "2015-04-09T12:05:50.000+0000", "description": "There seems to be a race when using the spark-shell on yarn (yarn-client mode) as to when it gets the hdfs delegation tokens.  \n\nIt appears to be that if  you do an action that causes it to get a delegation token before the workers are launched things work fine, if you want until after the workers launch you get an error about not having a delegation token.\n\n", "comments": ["So this appears to have been something environment related.  Or perhaps the KDC was having issues.  I can't reproduce it now.", "I just hit something like this, an intermittent failure with HdfsTest in yarn-client mode. Tom, does your error look like this:\n\n{noformat}\n14/03/14 17:57:15 INFO cluster.YarnClientSchedulerBackend: Application report from ASM:\n         appMasterRpcPort: 0\n         appStartTime: 1394845025101\n         yarnAppState: RUNNING\n\n14/03/14 17:57:17 INFO cluster.YarnClientClusterScheduler: YarnClientClusterScheduler.postStartHook done\n14/03/14 17:57:17 INFO storage.MemoryStore: ensureFreeSpace(215427) called with curMem=0, maxMem=1383491174\n14/03/14 17:57:17 INFO storage.MemoryStore: Block broadcast_0 stored as values to memory (estimated size 210.4 KB, free 1319.2 MB)\nException in thread \"main\" org.apache.hadoop.ipc.RemoteException(java.io.IOException): Delegation Token can be issued only with kerberos or web authentication\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getDelegationToken(FSNamesystem.java:6211)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getDelegationToken(NameNodeRpcServer.java:461)\n        ...\n        at org.apache.hadoop.hdfs.DFSClient.getDelegationToken(DFSClient.java:920)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.getDelegationToken(DistributedFileSystem.java:1336)\n        at org.apache.hadoop.fs.FileSystem.collectDelegationTokens(FileSystem.java:527)\n        at org.apache.hadoop.fs.FileSystem.addDelegationTokens(FileSystem.java:505)\n        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:121)\n        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:100)\n        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:80)\n        at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:202)\n{noformat}", "I'll give it a try again today to see if I can reproduce also.", "Ok I'm able to reproduce it again.  Debbugging it.", "So I was able to reproduce this issue when doing a saveAsTextFile on the client.  If you wait for a while before doing anything I was hitting this.  If you do operations immediately it doesn't happen.    It looks like we aren't propogating the credentials properly. \n\nI have not been able to reproduce it running HdfsTest.  That is a different case since your reading the file.  That should be propogating the credentials in SparkContext.hadoopRDD.  if you have more details please let me know.   You are sure you were kinit'd?", "Note the reason this works if you do it quickly is because the original connection to the namenode is up and it reuses it.", "https://github.com/apache/spark/pull/173", "I found out why I was having problem with HdfsTest. I was integrating on 0.9 and missed the changes that set YARN_CLIENT_MODE in a couple of places. I got that working. And I also verified the fix for this bug. Thanks a lot, Tom!", "Seems I meet with the similar issue using saveAsNewAPIHadoopDataset\nI tried to reproduce with create SC and then sleep for 10mins, then do saveAsNewAPIHadoopDataset, still not able to reproduce, can u give some advice on how to reproduce, thx"], "derived": {"summary": "There seems to be a race when using the spark-shell on yarn (yarn-client mode) as to when it gets the hdfs delegation tokens. It appears to be that if  you do an action that causes it to get a delegation token before the workers are launched things work fine, if you want until after the workers launch you get an error about not having a delegation token.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "spark-shell on yarn-client race in properly getting hdfs delegation tokens - There seems to be a race when using the spark-shell on yarn (yarn-client mode) as to when it gets the hdfs delegation tokens. It appears to be that if  you do an action that causes it to get a delegation token before the workers are launched things work fine, if you want until after the workers launch you get an error about not having a delegation token."}, {"q": "What updates or decisions were made in the discussion?", "a": "Seems I meet with the similar issue using saveAsNewAPIHadoopDataset\nI tried to reproduce with create SC and then sleep for 10mins, then do saveAsNewAPIHadoopDataset, still not able to reproduce, can u give some advice on how to reproduce, thx"}]}}
{"project": "SPARK", "issue_id": "SPARK-1204", "title": "EC2 scripts upload private key", "status": "Resolved", "priority": "Minor", "reporter": "Jaka Jancar", "assignee": null, "labels": [], "created": "2014-03-07T12:03:05.000+0000", "updated": "2015-01-11T07:29:03.000+0000", "description": "The EC2 scripts upload the private key that you supply.\n\nAt least, this needs to be clearly documented in --help.\n\nIdeally, only the public key would be uploaded to all the instances. For the instances to connect to one another, a fresh key should be generated (locally or on the master).", "comments": ["cc [~shivaram] Do you know off the top of your head if this is still the case?", "No - This was fixed in https://github.com/apache/spark/commit/b98572c70ad3932381a55f23f82600d7e435d2eb \nWe generate a new ssh key on the master for each cluster launch and then copy that to all the slave machines\n", "Resolving this issue per Shivaram's comment."], "derived": {"summary": "The EC2 scripts upload the private key that you supply. At least, this needs to be clearly documented in --help.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "EC2 scripts upload private key - The EC2 scripts upload the private key that you supply. At least, this needs to be clearly documented in --help."}, {"q": "What updates or decisions were made in the discussion?", "a": "Resolving this issue per Shivaram's comment."}]}}
{"project": "SPARK", "issue_id": "SPARK-1205", "title": "Clean up callSite/origin/generator", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "labels": [], "created": "2014-03-08T12:01:56.000+0000", "updated": "2014-03-30T04:15:33.000+0000", "description": "There is some overlap and very little documentation in these classes. It would be good to clarify their use and clean them up.", "comments": [], "derived": {"summary": "There is some overlap and very little documentation in these classes. It would be good to clarify their use and clean them up.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Clean up callSite/origin/generator - There is some overlap and very little documentation in these classes. It would be good to clarify their use and clean them up."}]}}
{"project": "SPARK", "issue_id": "SPARK-1206", "title": "Add python support for  average and other summary satistics", "status": "Closed", "priority": "Minor", "reporter": "Holden Karau", "assignee": null, "labels": [], "created": "2014-03-08T14:09:25.000+0000", "updated": "2014-11-14T10:27:32.000+0000", "description": "We have a number of summary statistics in the DoubleRDDFunctions.scala. We should implement these in python too.", "comments": ["whoops this is allready done I just didn't look in the correct place.", "Closing per [~holdenk_amp] as already done."], "derived": {"summary": "We have a number of summary statistics in the DoubleRDDFunctions. scala.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add python support for  average and other summary satistics - We have a number of summary statistics in the DoubleRDDFunctions. scala."}, {"q": "What updates or decisions were made in the discussion?", "a": "Closing per [~holdenk_amp] as already done."}]}}
{"project": "SPARK", "issue_id": "SPARK-1207", "title": "Make python support for histograms", "status": "Resolved", "priority": "Minor", "reporter": "Holden Karau", "assignee": null, "labels": [], "created": "2014-03-08T14:10:14.000+0000", "updated": "2014-07-27T01:31:02.000+0000", "description": null, "comments": ["I'll give this a shot this weekend."], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Make python support for histograms"}, {"q": "What updates or decisions were made in the discussion?", "a": "I'll give this a shot this weekend."}]}}
{"project": "SPARK", "issue_id": "SPARK-1208", "title": "after some hours of working the :4040 monitoring UI stops working.", "status": "Resolved", "priority": "Major", "reporter": "Tal Sliwowicz", "assignee": null, "labels": [], "created": "2014-03-08T23:28:53.000+0000", "updated": "2014-09-29T07:52:45.000+0000", "description": "\nThis issue is inconsistent, but it did not exist in prior versions.\nThe Driver app otherwise works normally.\n\nThe log file below is from the driver.\n\n\n2014-03-09 07:24:55,837 WARN  [qtp1187052686-17453] AbstractHttpConnection  - /stages/\njava.util.NoSuchElementException: None.get\n        at scala.None$.get(Option.scala:313)\n        at scala.None$.get(Option.scala:311)\n        at org.apache.spark.ui.jobs.StageTable.org$apache$spark$ui$jobs$StageTable$$stageRow(StageTable.scala:114)\n        at org.apache.spark.ui.jobs.StageTable$$anonfun$toNodeSeq$1.apply(StageTable.scala:39)\n        at org.apache.spark.ui.jobs.StageTable$$anonfun$toNodeSeq$1.apply(StageTable.scala:39)\n        at org.apache.spark.ui.jobs.StageTable$$anonfun$stageTable$1.apply(StageTable.scala:57)\n        at org.apache.spark.ui.jobs.StageTable$$anonfun$stageTable$1.apply(StageTable.scala:57)\n        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n        at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n        at scala.collection.AbstractTraversable.map(Traversable.scala:105)\n        at org.apache.spark.ui.jobs.StageTable.stageTable(StageTable.scala:57)\n        at org.apache.spark.ui.jobs.StageTable.toNodeSeq(StageTable.scala:39)\n        at org.apache.spark.ui.jobs.IndexPage.render(IndexPage.scala:81)\n        at org.apache.spark.ui.jobs.JobProgressUI$$anonfun$getHandlers$3.apply(JobProgressUI.scala:59)\n        at org.apache.spark.ui.jobs.JobProgressUI$$anonfun$getHandlers$3.apply(JobProgressUI.scala:59)\n        at org.apache.spark.ui.JettyUtils$$anon$1.handle(JettyUtils.scala:61)\n        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1040)\n        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:976)\n        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)\n        at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52)\n        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)\n        at org.eclipse.jetty.server.Server.handle(Server.java:363)\n        at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:483)\n        at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:920)\n        at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:982)\n        at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:635)\n        at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)\n        at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)\n        at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:628)\n        at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)\n        at java.lang.Thread.run(Thread.java:662)\n", "comments": ["Hey just have a few questions to help narrow this down:\n\n- Are you using the fair scheduler here?\n- Does this happen when a large number of stages is present (e.g. 1000+)?\n- Do you get an exception one time, every time, or intermittantly when you access the jobs page?\n\nLooking at the code it looks like it can't correctly lookup the pool name. In theory the deletion of the stage itself and the entry in the lookup table corresponding to the stage should be atomic, but maybe one is somehow happening without the other.\n\n", "Hi,\n1. Yes - the FAIR scheduler, although I am almost positive this happened in FIFO (the default) too. If important, we can switch to FIFO and see.\n2. Yes (10s of thousands, our job runs repeatedly on the same context), although this is issue is inconsistent. In most cases, even with a very large number of stages, this does not happen.\n3. The issue is not easily reproducible. It does not occur always (I do not have a scenario to make it start happening), but once it starts happening, it is totally consistent in the sense that it happens always.", "This issue does not happen with the FIFO scheduler. ", "This appears to be a similar, if not the same issue, as in SPARK-2643. The discussion in the PR indicates this was resolved by a subsequent change: https://github.com/apache/spark/pull/1854#issuecomment-55061571"], "derived": {"summary": "This issue is inconsistent, but it did not exist in prior versions. The Driver app otherwise works normally.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "after some hours of working the :4040 monitoring UI stops working. - This issue is inconsistent, but it did not exist in prior versions. The Driver app otherwise works normally."}, {"q": "What updates or decisions were made in the discussion?", "a": "This appears to be a similar, if not the same issue, as in SPARK-2643. The discussion in the PR indicates this was resolved by a subsequent change: https://github.com/apache/spark/pull/1854#issuecomment-55061571"}]}}
{"project": "SPARK", "issue_id": "SPARK-1209", "title": "SparkHadoop{MapRed,MapReduce}Util should not use package org.apache.hadoop", "status": "Closed", "priority": "Major", "reporter": "Sandy Ryza", "assignee": "Sean R. Owen", "labels": [], "created": "2014-03-09T22:19:12.000+0000", "updated": "2015-01-15T09:08:41.000+0000", "description": "It's private, so the change won't break compatibility", "comments": ["I did notice the same thing and did start some work on it:\nhttps://github.com/markgrover/spark/compare/master...package_fix\n\nI haven't tried compiling it yet but I will do so soonish. If you don't mind, I'd love to get this JIRA assigned to me.\nThanks for filing it!", "Thanks Sandy!", "It doesn't look like this was actually fixed.", "ok, I will take over. Thanks Sandy.", "Yes, I wonder too, does SparkHadoopMapRedUtil and SparkHadoopMapReduceUtil need to live in {{org.apache.hadoop}} anymore? I assume they may have in the past to access some package-private Hadoop code. But I've tried moving them under {{org.apache.spark}} and compiling versus a few Hadoop versions and it all seems fine.\n\nAm I missing something or is this worth changing? it's private to Spark (well, org.apache right now by necessity) so think it's fair game to move.\n\nSee https://github.com/srowen/spark/tree/SPARK-1209", "Definitely worth changing, in my opinion.  This has been bothering me for a while", "It's possible this previously used package-private code that it doesn't need any more. I agree it's better not to pollute the Hadoop namespace, but I also think many people use this and I don't think it would be justified to break this API for the purpose of cleanliness of our own code. IIRC we tried to make this private[spark] earlier but people asked to open it up.\n\nI think it would be okay to deprecate the old one and add forwarder methods to a new thing inside of the spark package. But wholesale moving it IMO isn't justified for the purpose of cleanliness alone.", "Actually a full set of forwarders might be overkill anyways since there's not much code.", "Hm, it's {{private[apache]}} though. Couldn't this only be used by people writing in the {{org.apache}} namespace? naturally a project might do just this to access this code, but I hadn't though this was promised as an stable API. People that pull this trick can I suppose declare their hack in {{org.apache.spark}}, although that's a source change.\n\nI can set up a forwarder and deprecate to see how that looks but wanted to check if it's really these classes in question that are being used outside Spark.", "User 'srowen' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/2814", "Hey Sean the class I thought this pertained to was SparkHadoopUtil:\n\nhttps://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala", "... and why wouldn't you, that's the title of the JIRA, oops. It's not that class that moves or even changes actually, and yes it should not move. Let me fix the title and fix my PR too. Maybe that's a more palatable change.", "Got it! Yeah so anything that is internal, of course would be great to move it to the Spark namespace if it's not necessary to be in Hadoop.", "User 'srowen' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/3048"], "derived": {"summary": "It's private, so the change won't break compatibility.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "SparkHadoop{MapRed,MapReduce}Util should not use package org.apache.hadoop - It's private, so the change won't break compatibility."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'srowen' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/3048"}]}}
{"project": "SPARK", "issue_id": "SPARK-1216", "title": "Add a OneHotEncoder for handling categorical features", "status": "Resolved", "priority": "Minor", "reporter": "Sanford Ryza", "assignee": "Sandy Ryza", "labels": [], "created": "2014-03-09T23:38:11.000+0000", "updated": "2015-02-26T11:23:24.000+0000", "description": "It would be nice to add something to MLLib to make it easy to do one-of-K encoding of categorical features.\n\nSomething like:\nhttp://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html", "comments": ["could merge these two issues:\nhttps://issues.apache.org/jira/browse/SPARK-1303", "[~sandyr] This is basically https://issues.apache.org/jira/browse/SPARK-4081 and Joseph has a PR for it now?", "(Addressing old comments I just saw now...)\n[~jaggi] I'd recommend keeping this separate from [https://issues.apache.org/jira/browse/SPARK-1303] since it is a different kind of transformation.\n[~srowen] This is a bit different from [SPARK-4081]; see the comment in my PR: [https://github.com/apache/spark/pull/3000#issuecomment-62630207].  I'd recommend keeping them separate.\n\nI don't immediately see a good way to organize these, but it could be worth discussing.", "I think this is duplicated by a similar JIRA for the new API."], "derived": {"summary": "It would be nice to add something to MLLib to make it easy to do one-of-K encoding of categorical features. Something like:\nhttp://scikit-learn.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add a OneHotEncoder for handling categorical features - It would be nice to add something to MLLib to make it easy to do one-of-K encoding of categorical features. Something like:\nhttp://scikit-learn."}, {"q": "What updates or decisions were made in the discussion?", "a": "I think this is duplicated by a similar JIRA for the new API."}]}}
{"project": "SPARK", "issue_id": "SPARK-1210", "title": "Prevent ContextClassLoader of Actor from becoming ClassLoader of Executor", "status": "Resolved", "priority": "Blocker", "reporter": "Takuya Ueshin", "assignee": "Takuya Ueshin", "labels": [], "created": "2014-03-10T01:34:36.000+0000", "updated": "2014-03-28T10:32:05.000+0000", "description": "Constructor of {{org.apache.spark.executor.Executor}} should not set context class loader of current thread, which is backend Actor's thread.\n\nRun the following code in local-mode REPL.\n\n{quote}\nscala> case class Foo(i: Int)\nscala> val ret = sc.parallelize((1 to 100).map(Foo), 10).collect\n{quote}\n\nThis causes errors as follows:\n\n{quote}\nERROR actor.OneForOneStrategy: [L$line5.$read$$iwC$$iwC$$iwC$$iwC$Foo;\njava.lang.ArrayStoreException: [L$line5.$read$$iwC$$iwC$$iwC$$iwC$Foo;\n     at scala.runtime.ScalaRunTime$.array_update(ScalaRunTime.scala:88)\n     at org.apache.spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:870)\n     at org.apache.spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:870)\n     at org.apache.spark.scheduler.JobWaiter.taskSucceeded(JobWaiter.scala:56)\n     at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:859)\n     at org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:616)\n     at org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:207)\n     at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)\n     at akka.actor.ActorCell.invoke(ActorCell.scala:456)\n     at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)\n     at akka.dispatch.Mailbox.run(Mailbox.scala:219)\n     at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)\n     at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n     at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n     at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n     at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n{quote}\n\nThis is because the class loaders to deserialize result {{Foo}} instances might be different from backend Actor's, and the Actor's class loader should be the same as Driver's.\n\nSee PR: https://github.com/apache/spark/pull/15", "comments": ["\\[edited from an earlier comment\\]\n\nJust to clarify there are two different issues going on here. One is that the classloader used in the executors is not set-up to correctly delegate to the active classloader in the thread that created it. Instead it delegates to the class loader of the class that defines Executor.scala. This is incorrect delegation and results in a problem where, when running in local mode, the Executor things there isn't an existing definition of a case class defined in the repl (e.g. Foo) and it goes and gets one over the HTTP served and ends up with a technically different definition of Foo. When that ends up back in the driver code it thinks the types are different and freaks out.\n\nA second issue is that there is code that, when the executor is created, sets the ClassLoader of  the thread in which the Executor is created. This is not actually needed because user classes can only be created/referenced inside of the run() method of a TaskRunner. Also in some cases, such as when an Executor is created inside of an Actor with multiple threads, this leads to an inconstent state amongst threads in which executor code is executed. This can have bad consequences when the Executor and the DAGScheduler are sharing an ActorSystem because the DAGScheduler's classloader can get redefined. The code was added here:\nhttps://github.com/apache/spark/commit/b864c36a#diff-40f975518e47d34cc8ecd7a49440228dR50", "I merged this into master. This might be worth back porting into 0.9 at some point.", "I've created SPARK-1346 to track back-porting this into 0.9.2"], "derived": {"summary": "Constructor of {{org. apache.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Prevent ContextClassLoader of Actor from becoming ClassLoader of Executor - Constructor of {{org. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "I've created SPARK-1346 to track back-porting this into 0.9.2"}]}}
{"project": "SPARK", "issue_id": "SPARK-1211", "title": "In ApplicationMaster, set spark.master system property to \"yarn-cluster\"", "status": "Resolved", "priority": "Major", "reporter": "Sanford Ryza", "assignee": "Sandy Ryza", "labels": [], "created": "2014-03-10T16:18:35.000+0000", "updated": "2014-04-04T20:50:50.000+0000", "description": "This would make it so that users don't need to pass it in to their SparkConf.  It won't break anything for apps that already pass it in.", "comments": ["https://github.com/apache/spark/pull/118"], "derived": {"summary": "This would make it so that users don't need to pass it in to their SparkConf. It won't break anything for apps that already pass it in.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "In ApplicationMaster, set spark.master system property to \"yarn-cluster\" - This would make it so that users don't need to pass it in to their SparkConf. It won't break anything for apps that already pass it in."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/118"}]}}
{"project": "SPARK", "issue_id": "SPARK-1230", "title": "Enable SparkContext.addJars() to load classes not in CLASSPATH", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": null, "labels": [], "created": "2014-03-10T22:46:26.000+0000", "updated": "2014-05-15T18:31:17.000+0000", "description": null, "comments": ["I forget what this actually means (hah) so I'm gonna close it for now."], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Enable SparkContext.addJars() to load classes not in CLASSPATH"}, {"q": "What updates or decisions were made in the discussion?", "a": "I forget what this actually means (hah) so I'm gonna close it for now."}]}}
{"project": "SPARK", "issue_id": "SPARK-1231", "title": "DEAD worker should recover automaticly", "status": "Closed", "priority": "Major", "reporter": "Tianyi Wang", "assignee": null, "labels": ["dead", "recover", "worker"], "created": "2014-03-12T06:07:34.000+0000", "updated": "2014-11-14T10:46:45.000+0000", "description": "master should send a response when DEAD worker sending a heartbeat.\nso worker could clean all applications and drivers and re-register itself to master.", "comments": ["Sorry [~tianyi], when I did my search for prior tickets in SPARK-3736 I didn't find this one.  The issue of workers not recovering when disconnected has since been resolved though and will be released as part of Spark 1.2.0, so I'm closing this ticket as a duplicate.\n\nPlease let us know if you have any troubles with that implementation once you start testing it.\n\nThanks for reporting the bug!\nAndrew"], "derived": {"summary": "master should send a response when DEAD worker sending a heartbeat. so worker could clean all applications and drivers and re-register itself to master.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "DEAD worker should recover automaticly - master should send a response when DEAD worker sending a heartbeat. so worker could clean all applications and drivers and re-register itself to master."}, {"q": "What updates or decisions were made in the discussion?", "a": "Sorry [~tianyi], when I did my search for prior tickets in SPARK-3736 I didn't find this one.  The issue of workers not recovering when disconnected has since been resolved though and will be released as part of Spark 1.2.0, so I'm closing this ticket as a duplicate.\n\nPlease let us know if you have any troubles with that implementation once you start testing it.\n\nThanks for reporting the bug!\nAndrew"}]}}
{"project": "SPARK", "issue_id": "SPARK-1232", "title": "maven hadoop 0.23 yarn-alpha build broken", "status": "Resolved", "priority": "Blocker", "reporter": "Thomas Graves", "assignee": "Thomas Graves", "labels": [], "created": "2014-03-12T08:17:49.000+0000", "updated": "2014-03-31T14:29:57.000+0000", "description": "The maven hadoop 0.23 yarn build was broken.  It looks like the avro dependency got removed by:\n\nhttps://github.com/apache/spark/pull/91/", "comments": ["broken by SPARK-1193"], "derived": {"summary": "The maven hadoop 0. 23 yarn build was broken.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "maven hadoop 0.23 yarn-alpha build broken - The maven hadoop 0. 23 yarn build was broken."}, {"q": "What updates or decisions were made in the discussion?", "a": "broken by SPARK-1193"}]}}
{"project": "SPARK", "issue_id": "SPARK-1233", "title": "spark on hadoop 0.23 yarn fails to run: java.lang.NoSuchFieldException: DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH", "status": "Resolved", "priority": "Blocker", "reporter": "Thomas Graves", "assignee": "Thomas Graves", "labels": [], "created": "2014-03-12T09:05:08.000+0000", "updated": "2014-03-20T12:17:19.000+0000", "description": "Trying to run on Yarn (hadoop 0.23) I get the following exception:\n\nException in thread \"main\" java.lang.NoSuchFieldException: DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH\n        at java.lang.Class.getField(Class.java:1569)\n        at org.apache.spark.deploy.yarn.ClientBase$.getDefaultMRApplicationClasspath(ClientBase.scala:417)\n        at org.apache.spark.deploy.yarn.ClientBase$$anonfun$6.apply(ClientBase.scala:393)\n        at org.apache.spark.deploy.yarn.ClientBase$$anonfun$6.apply(ClientBase.scala:393)\n        at scala.Option.getOrElse(Option.scala:120)\n        at org.apache.spark.deploy.yarn.ClientBase$.populateHadoopClasspath(ClientBase.scala:392)\n        at org.apache.spark.deploy.yarn.ClientBase$.populateClasspath(ClientBase.scala:444)\n        at org.apache.spark.deploy.yarn.ClientBase$class.setupLaunchEnv(ClientBase.scala:274)\n        at org.apache.spark.deploy.yarn.Client.setupLaunchEnv(Client.scala:37)\n        at org.apache.spark.deploy.yarn.Client.runApp(Client.scala:67)\n        at org.apache.spark.deploy.yarn.Client.run(Client.scala:84)\n        at org.apache.spark.deploy.yarn.Client$.main(Client.scala:177)\n        at org.apache.spark.deploy.yarn.Client.main(Client.scala)\n", "comments": ["https://github.com/apache/spark/pull/129\n\nbroken by https://github.com/apache/spark/pull/102 -> SPARK-1064"], "derived": {"summary": "Trying to run on Yarn (hadoop 0. 23) I get the following exception:\n\nException in thread \"main\" java.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "spark on hadoop 0.23 yarn fails to run: java.lang.NoSuchFieldException: DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH - Trying to run on Yarn (hadoop 0. 23) I get the following exception:\n\nException in thread \"main\" java."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/129\n\nbroken by https://github.com/apache/spark/pull/102 -> SPARK-1064"}]}}
{"project": "SPARK", "issue_id": "SPARK-1234", "title": "clean up typos and grammar issues in Spark on YARN page", "status": "Resolved", "priority": "Minor", "reporter": "Diana Carroll", "assignee": null, "labels": [], "created": "2014-03-12T10:03:51.000+0000", "updated": "2014-10-13T17:58:57.000+0000", "description": "The \"Launch spark application with yarn-client mode\" section of this of this page has several incomplete sentences, typos, etc.etc.  http://spark.incubator.apache.org/docs/latest/running-on-yarn.html\n", "comments": ["pull request 130", "Given the discussion in https://github.com/apache/spark/pull/130 , this was abandoned, but I also don't see the bad text on that page anymore anyhow. It probably got improved in another subsequent update."], "derived": {"summary": "The \"Launch spark application with yarn-client mode\" section of this of this page has several incomplete sentences, typos, etc. etc.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "clean up typos and grammar issues in Spark on YARN page - The \"Launch spark application with yarn-client mode\" section of this of this page has several incomplete sentences, typos, etc. etc."}, {"q": "What updates or decisions were made in the discussion?", "a": "Given the discussion in https://github.com/apache/spark/pull/130 , this was abandoned, but I also don't see the bad text on that page anymore anyhow. It probably got improved in another subsequent update."}]}}
{"project": "SPARK", "issue_id": "SPARK-1235", "title": "DAGScheduler ignores exceptions thrown in handleTaskCompletion", "status": "Resolved", "priority": "Blocker", "reporter": "Kay Ousterhout", "assignee": "Nan Zhu", "labels": [], "created": "2014-03-12T13:05:56.000+0000", "updated": "2014-04-25T23:06:13.000+0000", "description": "If an exception gets thrown in the handleTaskCompletion method, the method exits, but the exception is caught somewhere (not clear where) and the DAGScheduler keeps running.  Jobs hang as a result -- because not all of the task completion code gets run.\n\nThis was first reported by Brad Miller on the mailing list: http://apache-spark-user-list.1001560.n3.nabble.com/Fwd-pyspark-crash-on-mesos-td2256.html and this behavior seems to have changed since 0.8 (when, based on Brad's description, it sounds like an exception in handleTaskCompletion would cause the DAGScheduler to crash), suggesting that this may be related to the Scala 2.10.3.\n\nTo reproduce this problem, add \"throw new Exception(\"foo\")\" anywhere in handleTaskCompletion and run any job locally.  The job will hang and you can see the exception get printed in the logs.", "comments": ["A big, relevant difference post-0.8 is that the DAGScheduler event loop was replaced with the eventProcessActor.  That means that an uncaught exception thrown during that actor's processing of a message will kill that actor and restart a new one (while preserving the existing message queue without the message that resulted in the prior actor's death.)  You can see some of the logging of that happening in Brad's 0.9 stack trace that shows \"ERROR OneForOneStrategy\" -- see http://doc.akka.io/docs/akka/2.0.5/general/supervision.html.\n\nAt least in current versions of Akka, the handling of the actor's death and any necessary cleanup before restarting a fresh eventProcessActor would need to be done in the preRestart and postRestart hooks -- but I'm not 100% certain that the techniques are the same when using the Akka versions that Spark does.  ", "Hi, I will work on this issue\n\nActually the problem is more about handleTaskCompletion, if any exception happens during the event processing of DAGScheduler, the system will hang, \n\nwe need the fault-tolerance strategy when implement the akka actor", "PR: https://github.com/apache/spark/pull/186", "Github user AmplabJenkins commented on the pull request:\n\n    https://github.com/apache/spark/pull/186#issuecomment-38996202\n  \n     Merged build triggered. Build is starting -or- tests failed to complete.\n", "Github user AmplabJenkins commented on the pull request:\n\n    https://github.com/apache/spark/pull/186#issuecomment-38996211\n  \n    Merged build started. Build is starting -or- tests failed to complete.\n", "Github user CodingCat commented on the pull request:\n\n    https://github.com/apache/spark/pull/186#issuecomment-38997523\n  \n    It seems that when Jenkins is very busy, some weird thing can happen, in the last test, DAGScheduler even failed to create eventProcessingActor...\n    \n    I'm retesting it\n", "Github user AmplabJenkins commented on the pull request:\n\n    https://github.com/apache/spark/pull/186#issuecomment-38997636\n  \n    All automated tests passed.\n    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/13572/\n", "Github user AmplabJenkins commented on the pull request:\n\n    https://github.com/apache/spark/pull/186#issuecomment-38997635\n  \n    Merged build finished. All automated tests passed.\n", "Resolved in https://github.com/apache/spark/pull/186."], "derived": {"summary": "If an exception gets thrown in the handleTaskCompletion method, the method exits, but the exception is caught somewhere (not clear where) and the DAGScheduler keeps running. Jobs hang as a result -- because not all of the task completion code gets run.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "DAGScheduler ignores exceptions thrown in handleTaskCompletion - If an exception gets thrown in the handleTaskCompletion method, the method exits, but the exception is caught somewhere (not clear where) and the DAGScheduler keeps running. Jobs hang as a result -- because not all of the task completion code gets run."}, {"q": "What updates or decisions were made in the discussion?", "a": "Resolved in https://github.com/apache/spark/pull/186."}]}}
{"project": "SPARK", "issue_id": "SPARK-1236", "title": "Update Jetty to 9", "status": "Resolved", "priority": "Critical", "reporter": "Reynold Xin", "assignee": "Andrew Or", "labels": [], "created": "2014-03-12T16:30:49.000+0000", "updated": "2014-11-05T10:45:22.000+0000", "description": "See https://github.com/apache/spark/pull/113", "comments": ["We merged this then decided to revert it due to a JVM 7 dependency. But let's try to at least upgrade to Jetty 8 before Spark 1.0"], "derived": {"summary": "See https://github. com/apache/spark/pull/113.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Update Jetty to 9 - See https://github. com/apache/spark/pull/113."}, {"q": "What updates or decisions were made in the discussion?", "a": "We merged this then decided to revert it due to a JVM 7 dependency. But let's try to at least upgrade to Jetty 8 before Spark 1.0"}]}}
{"project": "SPARK", "issue_id": "SPARK-1237", "title": "Implicit ALS is not efficient in computing YtY", "status": "Resolved", "priority": "Major", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-03-12T18:01:11.000+0000", "updated": "2014-03-18T19:21:18.000+0000", "description": "Computing YtY can be implemented using BLAS's DSPR operations instead of generating y_i y_i^T and then combining them.", "comments": ["PR: https://github.com/apache/spark/pull/131", "Merged."], "derived": {"summary": "Computing YtY can be implemented using BLAS's DSPR operations instead of generating y_i y_i^T and then combining them.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Implicit ALS is not efficient in computing YtY - Computing YtY can be implemented using BLAS's DSPR operations instead of generating y_i y_i^T and then combining them."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR: https://github.com/apache/spark/pull/131"}]}}
{"project": "SPARK", "issue_id": "SPARK-1238", "title": "User should be able to set a random seed in ALS", "status": "Resolved", "priority": "Major", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-03-12T18:02:35.000+0000", "updated": "2014-03-18T19:21:29.000+0000", "description": "In order to reproduce results, we should allow users to set a random seed in ALS.", "comments": ["PR: https://github.com/apache/spark/pull/131"], "derived": {"summary": "In order to reproduce results, we should allow users to set a random seed in ALS.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "User should be able to set a random seed in ALS - In order to reproduce results, we should allow users to set a random seed in ALS."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR: https://github.com/apache/spark/pull/131"}]}}
{"project": "SPARK", "issue_id": "SPARK-1239", "title": "Improve fetching of map output statuses", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Thomas Graves", "labels": [], "created": "2014-03-12T18:55:23.000+0000", "updated": "2018-09-07T15:27:33.000+0000", "description": "Instead we should modify the way we fetch map output statuses to take both a mapper and a reducer - or we should just piggyback the statuses on each task. ", "comments": ["Hey [~CodingCat] I actually asked [~andrewor] to look at this already", "Oh, that's fine", "Is anyone working on this? If so, could someone please update the jira? Thanks!", "I think [~andrewor] has this in his backlog but it's not actively being worked on. [~bcwalrus] do you or [~sandyr] want to take a crack?", "[~pwendell] I'd like to take a crack at this since it is affecting one of our customers. ", "I have reassigned it to you Kostas.", "For large statuses, would we expect that to exceed {{spark.akka.frameSize}} and cause the below exception?\n\n{noformat}\n2014-09-14T01:34:21.305 ERROR [spark-akka.actor.default-dispatcher-4] org.apache.spark.MapOutputTrackerMasterActor - Map output statuses were 13920119 bytes which exceeds spark.akka.frameSize (10485760 bytes).\n{noformat}", "Yes, the current state of the art is to just increase the frame size.", "+1, we run into this issue as well.", "I'm re-assigning this to me since I've been working in this code recently and plan to completely re-write MapOutputStatusTracker to address this and a few other issues.\n\nI like the idea of piggybacking the status on the task launching RPC itself; if we took the approach of having reducers specify their reduce ids when fetching the statuses, then this would add an extra RPC for every task launch, which could result in a big latency increase.", "Apologies for not commenting on this JIRA sooner but since I'm new to Spark, it has taken a bit of time to wrap my head around how the different schedulers (DAG and Task) interoperate. I'm currently investigating completely removing the MapOutputTracker class. Instead, the DAGScheduler can push down the required map status' to the next stage. I'm thinking of modifying the DAGScheduler.submitMissingTasks(..) to take in (or query) the mapStatus' of the previous completed stage. When a new ShuffleMapTask gets created, we pass the filtered mapStatus' necessary for that task. The map status data can be stored in the TaskContext and used in the BlockStoreShuffleFetcher when block data is being read (currently uses the MapOutputTracker). What I'm investigating currently is how to filter the data when I create the ShuffleMapTask - the BlockStoreShuffleFetcher uses a shuffleId which I don't seem to have access to in the DAGScheduler.\n\nI haven't yet written any code to test this out so if anyone has any concerns please let me know. Also, [~joshrosen] pointed out that the size of the map output structure even after it has been filtered could still be very large. This is worth investigating.", "Hey Kostas - there are a few other bugs that required a refactoring to solve them that will subsume this issue, so I think that's why [~joshrosen] is grabbing it. This change will involve fairly significant surgery to a very complex part of Spark, so it might not be the best beginner issue. If Josh's other changes don't end up fixing this, we can leave this open as an independent issue and see if it can be done surgically.", "[~pwendell] Is there any update on this ? This is fairly commonly hitting us, and we are at 1Gig for framesize already now ...\n", "How many reduce side tasks do you have? Can you please attach your your logs that show the OOM errors/", "It would be helpful if any users who have observed this could comment on the JIRA and give workload information. This has been more on the back burner since we've heard few reports of it on the mailing list, etc...", "Hitting akka framesize for map outputtracker is very easy since we fetch whole output (m * r) - while I cant get into specifics of our jobs or share logs; but it is easy to see this hitting 1G for 100k mappers and 50k reducers.\nIf this is not being looked into currently, I can add it to my list of things to fix - but if there is already work being done, I dont want to duplicate it.\n\nEven something trivial like what was done in task result would suffice (if we dont want the additional overhead of per per reduce map output generation at master).", "Hi, all. We have faced this issue many times. And I've seen about a dozen unanswered mailing lists where guys saw this problem. Currently, we have 250 000 map tasks and the same amount of reduce tasks. We have 200 slave nodes. The driver has 80 GB RAM. First we observed akka frame size limit exception and after increasing the limit we see OOM. Here is the corresponding part of the log:\n{noformat}\n...\n15/07/27 17:22:56 INFO TaskSchedulerImpl: Adding task set 1.0 with 262144 tasks\n15/07/27 17:22:57 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 233766, 10.47.190.240, PROCESS_LOCAL, 1215 bytes)\n15/07/27 17:22:57 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 233767, 10.145.26.133, PROCESS_LOCAL, 1215 bytes)\n15/07/27 17:22:57 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 233768, 10.51.191.206, PROCESS_LOCAL, 1215 bytes)\n...\n15/07/27 17:22:57 INFO TaskSetManager: Starting task 3197.0 in stage 1.0 (TID 236963, 10.99.197.178, PROCESS_LOCAL, 1215 bytes)\n15/07/27 17:22:57 INFO TaskSetManager: Starting task 3198.0 in stage 1.0 (TID 236964, 10.65.148.16, PROCESS_LOCAL, 1215 bytes)\n15/07/27 17:22:57 INFO TaskSetManager: Starting task 3199.0 in stage 1.0 (TID 236965, 10.123.204.224, PROCESS_LOCAL, 1215 bytes)\n15/07/27 17:22:57 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.145.30.250:38441 (size: 3.8 KB, free: 4.1 GB)\n15/07/27 17:22:57 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.140.170.222:35810 (size: 3.8 KB, free: 4.1 GB)\n15/07/27 17:22:57 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.7.205.149:43761 (size: 3.8 KB, free: 4.1 GB)\n...\n5/07/27 17:22:57 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.165.146.7:37388 (size: 3.8 KB, free: 4.1 GB)\n15/07/27 17:22:57 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.153.254.79:49517 (size: 3.8 KB, free: 4.1 GB)\n15/07/27 17:22:57 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.95.198.154:53675 (size: 3.8 KB, free: 4.1 GB)\n15/07/27 17:24:41 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 166509346 bytes\n15/07/27 17:24:41 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.109.157.235:39740\n15/07/27 17:24:41 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.166.156.78:59382\n15/07/27 17:24:41 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.152.41.131:47968\n...\n15/07/27 17:24:41 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.140.253.251:44621\n15/07/27 17:24:41 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.153.254.79:42648\n15/07/27 17:24:41 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.169.230.246:45473\n15/07/27 17:25:31 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.146.43.5:49989 in memory (size: 3.2 KB, free: 3.4 GB)\n15/07/27 17:27:25 ERROR ActorSystemImpl: Uncaught fatal error from thread [sparkDriver-akka.remote.default-remote-dispatcher-47] shutting down ActorSystem [sparkDriver]\njava.lang.OutOfMemoryError: Java heap space\n        at java.util.Arrays.copyOf(Arrays.java:3236)\n        at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n        at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n        at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n        at java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n        at java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786)\n        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189)\n        at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n        at akka.serialization.JavaSerializer$$anonfun$toBinary$1.apply$mcV$sp(Serializer.scala:129)\n        at akka.serialization.JavaSerializer$$anonfun$toBinary$1.apply(Serializer.scala:129)\n        at akka.serialization.JavaSerializer$$anonfun$toBinary$1.apply(Serializer.scala:129)\n        at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)\n        at akka.serialization.JavaSerializer.toBinary(Serializer.scala:129)\n        at akka.remote.MessageSerializer$.serialize(MessageSerializer.scala:36)\n        at akka.remote.EndpointWriter$$anonfun$serializeMessage$1.apply(Endpoint.scala:845)\n        at akka.remote.EndpointWriter$$anonfun$serializeMessage$1.apply(Endpoint.scala:845)\n        at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)\n        at akka.remote.EndpointWriter.serializeMessage(Endpoint.scala:844)\n        at akka.remote.EndpointWriter.writeSend(Endpoint.scala:747)\n        at akka.remote.EndpointWriter$$anonfun$2.applyOrElse(Endpoint.scala:722)\n        at akka.actor.Actor$class.aroundReceive(Actor.scala:465)\n        at akka.remote.EndpointActor.aroundReceive(Endpoint.scala:415)\n        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)\n        at akka.actor.ActorCell.invoke(ActorCell.scala:487)\n        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)\n        at akka.dispatch.Mailbox.run(Mailbox.scala:220)\n        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)\n        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n...\n{noformat}\n\nDo you need any additional info?", "I can also add some data. I have a ShuffleMapStage with 82,714 tasks and then a ResultStage with 222,609 tasks. The driver cannot serialize this:\n\n{noformat}\njava.lang.OutOfMemoryError: Requested array size exceeds VM limit\n        at java.util.Arrays.copyOf(Arrays.java:2271) ~[na:1.7.0_79]\n        at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:113) ~[na:1.7.0_79]\n        at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93) ~[na:1.7.0_79]\n        at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:140) ~[na:1.7.0_79]\n        at java.util.zip.DeflaterOutputStream.deflate(DeflaterOutputStream.java:253) ~[na:1.7.0_79]\n        at java.util.zip.DeflaterOutputStream.write(DeflaterOutputStream.java:211) ~[na:1.7.0_79]\n        at java.util.zip.GZIPOutputStream.write(GZIPOutputStream.java:146) ~[na:1.7.0_79]\n        at java.io.ObjectOutputStream$BlockDataOutputStream.writeBlockHeader(ObjectOutputStream.java:1893) ~[na:1.7.0_79]\n        at java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1874) ~[na:1.7.0_79]\n        at java.io.ObjectOutputStream$BlockDataOutputStream.flush(ObjectOutputStream.java:1821) ~[na:1.7.0_79]\n        at java.io.ObjectOutputStream.flush(ObjectOutputStream.java:718) ~[na:1.7.0_79]\n        at java.io.ObjectOutputStream.close(ObjectOutputStream.java:739) ~[na:1.7.0_79]\n        at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$2.apply$mcV$sp(MapOutputTracker.scala:362) ~[spark-assembly-1.4.0-hadoop2.4.0.jar:1.4.0]\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1294) ~[spark-assembly-1.4.0-hadoop2.4.0.jar:1.4.0]\n        at org.apache.spark.MapOutputTracker$.serializeMapStatuses(MapOutputTracker.scala:361) ~[spark-assembly-1.4.0-hadoop2.4.0.jar:1.4.0]\n        at org.apache.spark.MapOutputTrackerMaster.getSerializedMapOutputStatuses(MapOutputTracker.scala:312) ~[spark-assembly-1.4.0-hadoop2.4.0.jar:1.4.0]\n        at org.apache.spark.MapOutputTrackerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(MapOutputTracker.scala:49) ~[spark-assembly-1.4.0-hadoop2.4.0.jar:1.4.0]\n{noformat}\n\nI see {{getSerializedMapOutputStatuses}} has changed a lot since 1.4.0 but it still returns an array sized proportional to _M * R_. How can this be part of a scalable system? How is this not a major issue for everyone? Am I doing something wrong?\n\nI'm now thinking that maybe if you have an overwhelming majority of empty or non-empty blocks, the bitmap will compress very well. But it's possible that I am ending up with a relatively even mix of empty and non-empty blocks, killing the compression. I have about 40 billion lines, _M * R_ is about 20 billion, so this seems plausible.\n\nIt's also possible that I should have larger partitions. Due to the processing I do it's not possible -- it leads to the executors OOMing. But larger partitions would not be a scalable solution anyway. If _M_ and _R_ are reasonable now with some number of lines per partition, then when your data size doubles they will also double and _M * R_ will quadruple. At some point the number of lines per map output will be low enough that compression becomes ineffective.\n\nI see https://issues.apache.org/jira/browse/SPARK-11271 has recently decreased the map status size by 20%. That means in Spark 1.6 I will be able to process 1/sqrt(0.8) or 12% more data than now. The way I understand the situation the improvement required is orders of magnitude larger than that. I'm currently hitting this issue with 5 TB of input. If I tried processing 5 PB, the map status would be a million times larger.\n\nI like the premise of this JIRA ticket of not building the map status table in the first place. But a colleague of mine asks if perhaps we could even avoid tracking this data in the driver. If the driver just provided the reducers with the list of mappers they could each just ask the mappers directly for the list of blocks they should fetch.", "> How is this not a major issue for everyone?\nDaniel, I had exactly the same thoughts, but I found out that most of Spark users have small data. Most users need about 10 servers. I was even more surprised when I had to fix SPARK-6246 myself. Spark was not able to launch more than 100 instances on Amazon.", "I have another user hitting this also.  The above mentions other issues that need to be addressed in MapOutputStatusTracker do you have links to those other issues?", "I've read an interesting article about the \"Kylix\" butterfly allreduce (http://www.cs.berkeley.edu/~jfc/papers/14/Kylix.pdf). I think this is a direct solution to this problem and the authors say integration with Spark should be \"easy\".\n\nPerhaps the same approach could be simulated within the current Spark shuffle implementation. I think the idea is to break up the M*R shuffle into an M*K and a K*R shuffle, where K is much less then M or R. So those K partitions will be large, but that should be fine.", "So I have been looking at this and testing a few changes out.\n\nThere are a few issues here but if we are looking at solving the driver memory bloat issue then I think this comes down to flow control issue.  The Driver is trying to respond to all the map status requests and is shoving them out to Netty quicker then netty can send them and we end up using a lot of memory very quickly.  Yes you can try to reduce the size of the MapStatuses but you can only do that to a point and you could still have the this issue.\n\nThere are multiple possible ways to solve this.  The approach I have been looking at is having the MapOutputTracker have its own queue and thread pool for handling requests.  This gives us the flexibility to do multiple things:\n\n- We can make the reply synchronous (ie it waits for response from netty to start next reply) without blocking the normal dispatcher threads which do things like handling heartbeats, thus giving us flow control. We can decide to do this only if the map output status are above a certain size or do it all the time.  You can adjust the thread pool size to handle more in parallel.  you could make this more sophisticated in the future if we want to have some sort of send queue rather then blocking each thread.  \n- We can easily synchronize incoming requests without blocking dispatcher threads so we don't serialize the same MapStatus multiple times.  Background - one other problem I've been seeing is that you get a bunch of requests for map status in at once, we have a lot of dispatchers threads running in parallel, all of those do the check to see if the map status is cached, all of them report its not, and you have multiple threads all serializing the exact same map output statuses.\n- doesn't limit us with sending map status with Task data.  ie if we want to change Spark in the future to start Reducer tasks before all map tasks finish (MapReduce does this now) this more easily works with that.  \n \nI still need to do some more testing on this but I wanted to see what people thought of this approach?\n\nWhat I have implemented right now is a queue and threadpool in the MapOutputTracker to handle the requests.  if its over 5MB (still deciding on this size) then when it replies it waits for it to actually send before grabbing the next request.\nFor the second bullet above I did a somewhat simpler approach for now and when registerMapOutputs is called I have it cache the map status output then instead of waiting for a request to come in. This helps as it will make sure the last one is cached but if you have multiple then the others still won't be in the cache.  We could either have it cache more or take an approach like I mention above to have it just synchronize and cache one upon the first request.\n\nOne of the large jobs I'm using to test this is shuffling 15TB of data using 202000 map tasks going down to 500 reducers.  The driver originally was using 20GB of memory, with my changes I was able to successfully run it with 5GB.\nThe job has 50mb of serialized map output statuses and before my changes it took executors 40-70 seconds to fetch the map output status, with my change using 8 threads it took roughly the same.  I need to get some more exact statistics here.", "[~tgraves] For the last part (waiting bit) - why not make the threshold where you use Broadcast instead of direct serialization such that the problem 'goes away' ? For my case, I was using a fairly high number, but nothing stopping us from using say 1mb - which means number of outstanding requests which will cause memory issue becomes extremely high to the point of being not possible practically.\nIn general, I dont like the point about waiting for IO to complete - different nodes might have different loads, which can cause driver not to respond to fast nodes because slow nodes cause the response not to be sent (over time).", "I do like the idea of broadcast and originally when I had tried it I had the issue mentioned in the second bullet point, but as long as we are synchronizing on the requests so we only broadcast it once we should be ok.\nIt does seem to have some further constraints though too.  With a sufficient large job I don't think it matters but what if we only have a small number of reducers, we broadcast it to all executors when only a couple need it. I guess that doesn't hurt much unless the other executors start going to the executors your reducers are on and add more load to them.  Should be pretty minimal though.\nBroadcast also seems to make less sense when using the dynamic allocation.  At least I've seen issues when executors go away, it fails fetch from that one, has to retry, etc, adding additional time.  We recently specifically fixed one issue with this to make it go get locations again after certain number of failures.  That time should be less now that we fixed that but I'll have to run the numbers.\n\nI'll do some more analysis/testing of this and see if that really matters.  \n\nwith a sufficient number of threads I don't think a few slow nodes would make much of a difference here, if you have that many slow nodes the shuffle itself is going to be impacted which I would see as a larger affect. The slow nodes could just as well affect the broadcast as well.  Hopefully you skip those as it takes longer for those to get a chunk, buts its possible that once that slow one has a chunk or two, more and more executors start going to that one for the broadcast data instead of the driver thus slowing down more transfers.\n\nBut its a good point and my current method would truly block (for a certain time) rather then being slow.  Note that there is a timeout on waiting for the send to happen and when it does it closes the connection and executor would retry.  You don't have to worry about that with broadcast.\n\nI'll do some more analysis with that approach.\n\nI wish Netty had some other built in mechanisms for flow control.", "User 'tgravescs' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/12113", "Issue resolved by pull request 12113\n[https://github.com/apache/spark/pull/12113]", "I am hitting a similar error on Spark 2.3.1 running on EMR cluster of 500 r3.2xlarge to process ~40 TB.\r\n\r\nThere are ~ 200,000 map tasks that succeed. We use aggregateByKey with numPartition=300,000 but when the reduce sides tasks start the driver (which has 50GB memory) is failing with:\r\n\r\n18/09/07 15:10:49 WARN Utils: Suppressing exception in finally: null java.lang.OutOfMemoryError at java.io.ByteArrayOutputStream.hugeCapacity(ByteArrayOutputStream.java:123) at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:117) at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93) at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153) at java.util.zip.DeflaterOutputStream.deflate(DeflaterOutputStream.java:253) at java.util.zip.DeflaterOutputStream.write(DeflaterOutputStream.java:211) at java.util.zip.GZIPOutputStream.write(GZIPOutputStream.java:145) at java.io.ObjectOutputStream$BlockDataOutputStream.writeBlockHeader(ObjectOutputStream.java:1894) at java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1875) at java.io.ObjectOutputStream$BlockDataOutputStream.flush(ObjectOutputStream.java:1822) at java.io.ObjectOutputStream.flush(ObjectOutputStream.java:719) at java.io.ObjectOutputStream.close(ObjectOutputStream.java:740) at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$2.apply$mcV$sp(MapOutputTracker.scala:790) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1389) at org.apache.spark.MapOutputTracker$.serializeMapStatuses(MapOutputTracker.scala:789) at org.apache.spark.ShuffleStatus.serializedMapStatus(MapOutputTracker.scala:174) at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:397) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Exception in thread \"map-output-dispatcher-0\" java.lang.OutOfMemoryError at java.io.ByteArrayOutputStream.hugeCapacity(ByteArrayOutputStream.java:123) at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:117) at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93) at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153) at java.util.zip.DeflaterOutputStream.deflate(DeflaterOutputStream.java:253) at java.util.zip.DeflaterOutputStream.write(DeflaterOutputStream.java:211) at java.util.zip.GZIPOutputStream.write(GZIPOutputStream.java:145) at java.io.ObjectOutputStream$BlockDataOutputStream.writeBlockHeader(ObjectOutputStream.java:1894) at java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1875) at java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189) at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174) at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348) at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply$mcV$sp(MapOutputTracker.scala:787) at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply(MapOutputTracker.scala:786) at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply(MapOutputTracker.scala:786) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1380) at org.apache.spark.MapOutputTracker$.serializeMapStatuses(MapOutputTracker.scala:789) at org.apache.spark.ShuffleStatus.serializedMapStatus(MapOutputTracker.scala:174) at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:397) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Suppressed: java.lang.OutOfMemoryError at java.io.ByteArrayOutputStream.hugeCapacity(ByteArrayOutputStream.java:123) at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:117) at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93) at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153) at java.util.zip.DeflaterOutputStream.deflate(DeflaterOutputStream.java:253) at java.util.zip.DeflaterOutputStream.write(DeflaterOutputStream.java:211) at java.util.zip.GZIPOutputStream.write(GZIPOutputStream.java:145) at java.io.ObjectOutputStream$BlockDataOutputStream.writeBlockHeader(ObjectOutputStream.java:1894) at java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1875) at java.io.ObjectOutputStream$BlockDataOutputStream.flush(ObjectOutputStream.java:1822) at java.io.ObjectOutputStream.flush(ObjectOutputStream.java:719) at java.io.ObjectOutputStream.close(ObjectOutputStream.java:740) at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$2.apply$mcV$sp(MapOutputTracker.scala:790) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1389) ... 6 more\r\n\r\n\r\n\r\n"], "derived": {"summary": "Instead we should modify the way we fetch map output statuses to take both a mapper and a reducer - or we should just piggyback the statuses on each task.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Improve fetching of map output statuses - Instead we should modify the way we fetch map output statuses to take both a mapper and a reducer - or we should just piggyback the statuses on each task."}, {"q": "What updates or decisions were made in the discussion?", "a": "I am hitting a similar error on Spark 2.3.1 running on EMR cluster of 500 r3.2xlarge to process ~40 TB.\r\n\r\nThere are ~ 200,000 map tasks that succeed. We use aggregateByKey with numPartition=300,000 but when the reduce sides tasks start the driver (which has 50GB memory) is failing with:\r\n\r\n18/09/07 15:10:49 WARN Utils: Suppressing exception in finally: null java.lang.OutOfMemoryError at java.io.ByteArrayOutputStream.hugeCapacity(ByteArrayOutputStream.java:123) at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:117) at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93) at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153) at java.util.zip.DeflaterOutputStream.deflate(DeflaterOutputStream.java:253) at java.util.zip.DeflaterOutputStream.write(DeflaterOutputStream.java:211) at java.util.zip.GZIPOutputStream.write(GZIPOutputStream.java:145) at java.io.ObjectOutputStream$BlockDataOutputStream.writeBlockHeader(ObjectOutputStream.java:1894) at java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1875) at java.io.ObjectOutputStream$BlockDataOutputStream.flush(ObjectOutputStream.java:1822) at java.io.ObjectOutputStream.flush(ObjectOutputStream.java:719) at java.io.ObjectOutputStream.close(ObjectOutputStream.java:740) at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$2.apply$mcV$sp(MapOutputTracker.scala:790) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1389) at org.apache.spark.MapOutputTracker$.serializeMapStatuses(MapOutputTracker.scala:789) at org.apache.spark.ShuffleStatus.serializedMapStatus(MapOutputTracker.scala:174) at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:397) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Exception in thread \"map-output-dispatcher-0\" java.lang.OutOfMemoryError at java.io.ByteArrayOutputStream.hugeCapacity(ByteArrayOutputStream.java:123) at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:117) at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93) at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153) at java.util.zip.DeflaterOutputStream.deflate(DeflaterOutputStream.java:253) at java.util.zip.DeflaterOutputStream.write(DeflaterOutputStream.java:211) at java.util.zip.GZIPOutputStream.write(GZIPOutputStream.java:145) at java.io.ObjectOutputStream$BlockDataOutputStream.writeBlockHeader(ObjectOutputStream.java:1894) at java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1875) at java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189) at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174) at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348) at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply$mcV$sp(MapOutputTracker.scala:787) at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply(MapOutputTracker.scala:786) at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply(MapOutputTracker.scala:786) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1380) at org.apache.spark.MapOutputTracker$.serializeMapStatuses(MapOutputTracker.scala:789) at org.apache.spark.ShuffleStatus.serializedMapStatus(MapOutputTracker.scala:174) at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:397) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Suppressed: java.lang.OutOfMemoryError at java.io.ByteArrayOutputStream.hugeCapacity(ByteArrayOutputStream.java:123) at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:117) at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93) at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153) at java.util.zip.DeflaterOutputStream.deflate(DeflaterOutputStream.java:253) at java.util.zip.DeflaterOutputStream.write(DeflaterOutputStream.java:211) at java.util.zip.GZIPOutputStream.write(GZIPOutputStream.java:145) at java.io.ObjectOutputStream$BlockDataOutputStream.writeBlockHeader(ObjectOutputStream.java:1894) at java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1875) at java.io.ObjectOutputStream$BlockDataOutputStream.flush(ObjectOutputStream.java:1822) at java.io.ObjectOutputStream.flush(ObjectOutputStream.java:719) at java.io.ObjectOutputStream.close(ObjectOutputStream.java:740) at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$2.apply$mcV$sp(MapOutputTracker.scala:790) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1389) ... 6 more"}]}}
{"project": "SPARK", "issue_id": "SPARK-1240", "title": "takeSample called on empty RDD never ends", "status": "Resolved", "priority": "Major", "reporter": "Jaroslav Kamenik", "assignee": "Nan Zhu", "labels": [], "created": "2014-03-13T03:04:18.000+0000", "updated": "2014-03-16T22:38:01.000+0000", "description": "It seems, that method takeSample ends in infinite loop if called on empty RDD, trying to collect enough samples.\n\nval list = List\\[String\\](\"aaa\")\n\nval rdd = sc.parallelize(list)\nrdd.takeSample(true, 1, System.nanoTime.toInt)\n\nval empty = rdd.filter(_ => false)\nempty.takeSample(true, 1, System.nanoTime.toInt)\n\n\n", "comments": ["Made a PR: \n\nhttps://github.com/apache/spark/pull/135"], "derived": {"summary": "It seems, that method takeSample ends in infinite loop if called on empty RDD, trying to collect enough samples. val list = List\\[String\\](\"aaa\")\n\nval rdd = sc.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "takeSample called on empty RDD never ends - It seems, that method takeSample ends in infinite loop if called on empty RDD, trying to collect enough samples. val list = List\\[String\\](\"aaa\")\n\nval rdd = sc."}, {"q": "What updates or decisions were made in the discussion?", "a": "Made a PR: \n\nhttps://github.com/apache/spark/pull/135"}]}}
{"project": "SPARK", "issue_id": "SPARK-1241", "title": "Support sliding in RDD", "status": "Resolved", "priority": "Major", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-03-13T12:05:59.000+0000", "updated": "2015-05-01T23:25:30.000+0000", "description": "Sliding is useful for operations like creating n-grams, calculating total variation, numerical integration, etc.", "comments": ["PR: https://github.com/apache/spark/pull/136", "Hi,\n\nI'm investigating use of Spark for matching patterns in symbolically represented time series where the sliding functionality as available from scala iterators would make life a lot easier.\n\nFrom the ticket I'd say this functionality is implemented (status resolved, fix version 1.0.0, ...), but I can't find it in the docs and the PR indicates that this functionality hasn't made it into Spark (just yet ...). Is this functionality available?\n\nCheers,\nFrens\n\nBackground: I want to compare strings of segments to other (larger) strings of segments. As segment strings may be split up over partitions, the more straight forward aproaches I could come up with don't work.", "This is implemented MLlib: https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/rdd/SlidingRDD.scala . You can check the discussion about where to put it here: https://github.com/apache/spark/pull/136 ."], "derived": {"summary": "Sliding is useful for operations like creating n-grams, calculating total variation, numerical integration, etc.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support sliding in RDD - Sliding is useful for operations like creating n-grams, calculating total variation, numerical integration, etc."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is implemented MLlib: https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/rdd/SlidingRDD.scala . You can check the discussion about where to put it here: https://github.com/apache/spark/pull/136 ."}]}}
{"project": "SPARK", "issue_id": "SPARK-1242", "title": "Add aggregate to python API", "status": "Resolved", "priority": "Trivial", "reporter": "Holden Karau", "assignee": "Holden Karau", "labels": [], "created": "2014-03-13T19:24:38.000+0000", "updated": "2014-04-25T06:08:36.000+0000", "description": null, "comments": ["Submitted a pull request for this https://github.com/apache/spark/pull/139"], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add aggregate to python API"}, {"q": "What updates or decisions were made in the discussion?", "a": "Submitted a pull request for this https://github.com/apache/spark/pull/139"}]}}
{"project": "SPARK", "issue_id": "SPARK-1243", "title": "spark compilation error", "status": "Resolved", "priority": "Major", "reporter": "Qiuzhuang Lian", "assignee": null, "labels": [], "created": "2014-03-13T20:33:54.000+0000", "updated": "2014-10-13T17:55:17.000+0000", "description": "After issuing git pull from git master, spark could not compile any longer\n\nHere is the error message, it seems that it is related to jetty upgrade.@rxin\n\n> \n> \n> compile\n[info] Compiling 301 Scala sources and 19 Java sources to E:\\projects\\amplab\\spark\\core\\target\\scala-2.10\\classes...\n[warn] Class java.nio.channels.ReadPendingException not found - continuing with a stub.\n[error] \n[error]      while compiling: E:\\projects\\amplab\\spark\\core\\src\\main\\scala\\org\\apache\\spark\\HttpServer.scala\n[error]         during phase: erasure\n[error]      library version: version 2.10.3\n[error]     compiler version: version 2.10.3\n[error]   reconstructed args: -Xmax-classfile-name 120 -deprecation -bootclasspath C:\\Java\\jdk1.6.0_27\\jre\\lib\\resources.jar;C:\\Java\\jdk1.6.0_27\\jre\\lib\\rt.jar;C:\\Java\\jdk1.6.0_27\\jre\\lib\\sunrsasign.jar;C:\\Java\\jdk1.6.0_27\\jre\\lib\\jsse.jar;C:\\Java\\jdk1.6.0_27\\jre\\lib\\jce.jar;C:\\Java\\jdk1.6.0_27\\jre\\lib\\charsets.jar;C:\\Java\\jdk1.6.0_27\\jre\\lib\\modules\\jdk.boot.jar;C:\\Java\\jdk1.6.0_27\\jre\\classes;C:\\Users\\Kand\\.sbt\\boot\\scala-2.10.3\\lib\\scala-library.jar -unchecked -classpath E:\\projects\\amplab\\spark\\core\\target\\scala-2.10\\classes;E:\\projects\\amplab\\spark\\lib_managed\\jars\\netty-all-4.0.17.Final.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jetty-server-9.1.3.v20140225.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\javax.servlet-api-3.1.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jetty-http-9.1.3.v20140225.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jetty-util-9.1.3.v20140225.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jetty-io-9.1.3.v20140225.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jetty-plus-9.1.3.v20140225.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jetty-webapp-9.1.3.v20140225.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jetty-xml-9.1.3.v20140225.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jetty-servlet-9.1.3.v20140225.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jetty-security-9.1.3.v20140225.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jetty-jndi-9.1.3.v20140225.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\javax.servlet-2.5.0.v201103041518.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\guava-14.0.1.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jsr305-1.3.9.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\log4j-1.2.17.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\slf4j-api-1.7.5.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\slf4j-log4j12-1.7.5.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jul-to-slf4j-1.7.5.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jcl-over-slf4j-1.7.5.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-daemon-1.0.10.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\compress-lzf-1.0.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\snappy-java-1.0.5.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\akka-remote_2.10-2.2.3-shaded-protobuf.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\akka-actor_2.10-2.2.3-shaded-protobuf.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\config-1.0.2.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\netty-3.6.6.Final.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\protobuf-java-2.4.1-shaded.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\uncommons-maths-1.2.2a.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\akka-slf4j_2.10-2.2.3-shaded-protobuf.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\json4s-jackson_2.10-3.2.6.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\json4s-core_2.10-3.2.6.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\json4s-ast_2.10-3.2.6.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\paranamer-2.6.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\scalap-2.10.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\scala-compiler-2.10.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\scala-reflect-2.10.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\jackson-databind-2.3.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\jackson-annotations-2.3.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\jackson-core-2.3.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\colt-1.2.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\concurrent-1.3.4.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\mesos-0.13.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\protobuf-java-2.4.1.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-net-2.2.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jets3t-0.7.1.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-httpclient-3.1.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\hadoop-client-1.0.4.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\hadoop-core-1.0.4.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\xmlenc-0.52.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-codec-1.4.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-math-2.1.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-configuration-1.6.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-collections-3.2.1.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-lang-2.4.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-digester-1.8.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-beanutils-1.7.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-beanutils-core-1.8.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-el-1.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\hsqldb-1.8.0.10.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\oro-2.0.8.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jackson-mapper-asl-1.0.1.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jackson-core-asl-1.0.1.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\curator-recipes-2.4.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\curator-framework-2.4.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\curator-client-2.4.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\zookeeper-3.4.5.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jline-0.9.94.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\metrics-core-3.0.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\metrics-jvm-3.0.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\metrics-json-3.0.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\metrics-graphite-3.0.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\chill_2.10-0.3.1.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\chill-java-0.3.1.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\kryo-2.21.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\reflectasm-1.07-shaded.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\minlog-1.2.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\objenesis-1.2.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\stream-2.5.1.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\fastutil-6.5.7.jar\n[error] \n[error]   last tree to typer: TypeTree(class Logging$class)\n[error]               symbol: class Logging$class in package spark (flags: abstract <trait> <implclass>)\n[error]    symbol definition: abstract class Logging$class extends Logging\n[error]                  tpe: org.apache.spark.Logging$class\n[error]        symbol owners: class Logging$class -> package spark\n[error]       context owners: method start -> class HttpServer -> package spark\n[error] \n[error] == Enclosing template or block ==\n[error] \n[error] Apply( // def setIdleTimeout(x$1: Long): Unit in class AbstractConnector\n[error]   \"connector\".\"setIdleTimeout\" // def setIdleTimeout(x$1: Long): Unit in class AbstractConnector\n[error]   60000L\n[error] )\n[error] \n[error] == Expanded type of tree ==\n[error] \n[error] TypeRef(TypeSymbol(abstract class Logging$class extends Logging))\n[error] \n[error] uncaught exception during compilation: java.lang.AssertionError\n[trace] Stack trace suppressed: run 'last core/compile:compile' for the full output.\n[error] (core/compile:compile) java.lang.AssertionError: assertion failed: java.nio.channels.ReadPendingException\n[error] Total time: 73 s, completed Mar 14, 2014 11:24:14 AM\n", "comments": ["i see same thing with java 6, but not with java 7", "Yes, it compiles successfully with JDK 7. Currently, Spark build scripts marks to use JDK  6. Should we update them to reflect to use JDK 7 instead?\n", "This appears to be long since resolved by something else, perhaps a subsequent change to Jetty deps. I have never seen this personally, and Jenkins builds are fine."], "derived": {"summary": "After issuing git pull from git master, spark could not compile any longer\n\nHere is the error message, it seems that it is related to jetty upgrade. @rxin\n\n> \n> \n> compile\n[info] Compiling 301 Scala sources and 19 Java sources to E:\\projects\\amplab\\spark\\core\\target\\scala-2.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "spark compilation error - After issuing git pull from git master, spark could not compile any longer\n\nHere is the error message, it seems that it is related to jetty upgrade. @rxin\n\n> \n> \n> compile\n[info] Compiling 301 Scala sources and 19 Java sources to E:\\projects\\amplab\\spark\\core\\target\\scala-2."}, {"q": "What updates or decisions were made in the discussion?", "a": "This appears to be long since resolved by something else, perhaps a subsequent change to Jetty deps. I have never seen this personally, and Jenkins builds are fine."}]}}
{"project": "SPARK", "issue_id": "SPARK-1244", "title": "Log an exception if map output status message exceeds frame size", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Andrew Or", "labels": [], "created": "2014-03-13T21:56:03.000+0000", "updated": "2014-03-30T04:15:14.000+0000", "description": "This causes a silent failure if not set correctly.\nThe associated PR - https://github.com/apache/spark/pull/147", "comments": [], "derived": {"summary": "This causes a silent failure if not set correctly. The associated PR - https://github.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Log an exception if map output status message exceeds frame size - This causes a silent failure if not set correctly. The associated PR - https://github."}]}}
{"project": "SPARK", "issue_id": "SPARK-1245", "title": "Can't read EMR HBase cluster from properly built Cloudera Spark Cluster.", "status": "Resolved", "priority": "Major", "reporter": "Sam Abeyratne", "assignee": null, "labels": [], "created": "2014-03-14T03:04:13.000+0000", "updated": "2014-11-12T16:42:29.000+0000", "description": "Can't read EMR HBase cluster from properly built Cloudera Spark Cluster.\n\nIf I scp hadoop-yarn-client-2.2.0.jar from our EMR hbase cluster lib dir and manually add it as a lib to my jar it does NOT give me a noSuchMethod error, but does give me a weird EOF exception (see below).\n\nUsually I use SBT to build Jars, but the EMR distros are very strange I can't find a proper repository for them.  I'm thinking only thing we can do is get our sysadm to rebuild the hbase cluster to use a proper cloudera hbase / hadoop.\n\nSBT Dependencies include: \n\"org.apache.spark\" % \"spark-core_2.10\" % \"0.9.0-incubating\",\n\"org.apache.hbase\" % \"hbase\" % \"0.94.7\",\n\n14/03/11 19:08:06 WARN scheduler.TaskSetManager: Lost TID 95 (task 0.0:3)\n14/03/11 19:08:06 WARN scheduler.TaskSetManager: Loss was due to java.io.EOFException\njava.io.EOFException\n\tat java.io.ObjectInputStream$BlockDataInputStream.readFully(ObjectInputStream.java:2744)\n\tat java.io.ObjectInputStream.readFully(ObjectInputStream.java:1015)\n\tat org.apache.hadoop.io.WritableUtils.readCompressedByteArray(WritableUtils.java:39)\n\tat org.apache.hadoop.io.WritableUtils.readCompressedString(WritableUtils.java:87)\n\tat org.apache.hadoop.io.WritableUtils.readCompressedStringArray(WritableUtils.java:185)\n\tat org.apache.hadoop.conf.Configuration.readFields(Configuration.java:2433)\n\tat org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:280)\n\tat org.apache.hadoop.io.ObjectWritable.readFields(ObjectWritable.java:75)\n\tat org.apache.spark.SerializableWritable.readObject(SerializableWritable.scala:39)\n\tat sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:40)\n\tat org.apache.spark.broadcast.HttpBroadcast$.read(HttpBroadcast.scala:165)\n\tat org.apache.spark.broadcast.HttpBroadcast.readObject(HttpBroadcast.scala:56)\n\tat sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)\n\tat scala.collection.immutable.$colon$colon.readObject(List.scala:362)\n\tat sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)\n\tat scala.collection.immutable.$colon$colon.readObject(List.scala:362)\n\tat sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:40)\n\tat org.apache.spark.scheduler.ResultTask$.deserializeInfo(ResultTask.scala:63)\n\tat org.apache.spark.scheduler.ResultTask.readExternal(ResultTask.scala:139)\n\tat java.io.ObjectInputStream.readExternalData(ObjectInputStream.java:1837)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:40)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:62)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:195)\n\tat org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)", "comments": ["How can I create topics on: https://groups.google.com/forum/#!forum/spark-users ??", "I'm guessing this is now either obsolete, or, a case of matching HBase / Hadoop versions exactly. Spark should be \"provided\", and not marking as such may mean the Spark Hadoop / cluster Hadoop / HBase Hadoop deps are colliding."], "derived": {"summary": "Can't read EMR HBase cluster from properly built Cloudera Spark Cluster. If I scp hadoop-yarn-client-2.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Can't read EMR HBase cluster from properly built Cloudera Spark Cluster. - Can't read EMR HBase cluster from properly built Cloudera Spark Cluster. If I scp hadoop-yarn-client-2."}, {"q": "What updates or decisions were made in the discussion?", "a": "I'm guessing this is now either obsolete, or, a case of matching HBase / Hadoop versions exactly. Spark should be \"provided\", and not marking as such may mean the Spark Hadoop / cluster Hadoop / HBase Hadoop deps are colliding."}]}}
{"project": "SPARK", "issue_id": "SPARK-1246", "title": "Add min max to the stat counter.", "status": "Resolved", "priority": "Minor", "reporter": "Prashant Sharma", "assignee": null, "labels": [], "created": "2014-03-14T04:43:25.000+0000", "updated": "2020-02-07T17:28:01.000+0000", "description": "Augment the stat counter class with min max functions. It might be convenient to calculate all that in one go. ", "comments": ["Currently working on it. (wish I had assign rights.)", "This was fixed by Daniel McClary here: https://github.com/apache/spark/pull/144", "User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/140"], "derived": {"summary": "Augment the stat counter class with min max functions. It might be convenient to calculate all that in one go.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add min max to the stat counter. - Augment the stat counter class with min max functions. It might be convenient to calculate all that in one go."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/140"}]}}
{"project": "SPARK", "issue_id": "SPARK-1247", "title": "Support some of the RDD double functions on float and int as well.", "status": "Resolved", "priority": "Minor", "reporter": "Prashant Sharma", "assignee": "Prashant Sharma", "labels": [], "created": "2014-03-14T04:45:20.000+0000", "updated": "2020-02-07T17:28:00.000+0000", "description": "Pyspark is already agnostic, but currently Java and scala provides the implicit functions like stats/ histogram etc only if the RDD is on Double. This can be extended easily to support more numeric types. ", "comments": ["Trying to work on it. ", "This seems to be fixed by the following function:\n\n{code}\n  implicit def numericRDDToDoubleRDDFunctions[T](rdd: RDD[T])(implicit num: Numeric[T]) =\n    new DoubleRDDFunctions(rdd.map(x => num.toDouble(x)))\n{code}", "Yes, this already works on {{RDD[Int]}} for example via implicits, which existed since 0.9 at least:\n\n{code}\nscala> val ints = sc.parallelize(Array(1,2,3))\nints: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:21\nscala> ints.mean\n...\n2.0\n{code}\n\nI think it may be prohibitive to specialize all this in Java, but, an RDD can easily be manually mapped to {{Double}}s in Java."], "derived": {"summary": "Pyspark is already agnostic, but currently Java and scala provides the implicit functions like stats/ histogram etc only if the RDD is on Double. This can be extended easily to support more numeric types.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support some of the RDD double functions on float and int as well. - Pyspark is already agnostic, but currently Java and scala provides the implicit functions like stats/ histogram etc only if the RDD is on Double. This can be extended easily to support more numeric types."}, {"q": "What updates or decisions were made in the discussion?", "a": "Yes, this already works on {{RDD[Int]}} for example via implicits, which existed since 0.9 at least:\n\n{code}\nscala> val ints = sc.parallelize(Array(1,2,3))\nints: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:21\nscala> ints.mean\n...\n2.0\n{code}\n\nI think it may be prohibitive to specialize all this in Java, but, an RDD can easily be manually mapped to {{Double}}s in Java."}]}}
{"project": "SPARK", "issue_id": "SPARK-1248", "title": "Spark build error with Apache Hadoop(Cloudera CDH4)", "status": "Resolved", "priority": "Major", "reporter": "Guoqiang Li", "assignee": "Sean R. Owen", "labels": [], "created": "2014-03-14T05:03:46.000+0000", "updated": "2015-01-15T09:08:42.000+0000", "description": "{code}\nSPARK_HADOOP_VERSION=2.0.0-cdh4.5.0 SPARK_YARN=true sbt/sbt assembly -d > error.log\n{code}", "comments": [], "derived": {"summary": "{code}\nSPARK_HADOOP_VERSION=2. 0.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Spark build error with Apache Hadoop(Cloudera CDH4) - {code}\nSPARK_HADOOP_VERSION=2. 0."}]}}
{"project": "SPARK", "issue_id": "SPARK-1249", "title": "Cannot create graphx.Graph with no edges", "status": "Resolved", "priority": "Major", "reporter": "Daniel Darabos", "assignee": null, "labels": [], "created": "2014-03-14T07:16:48.000+0000", "updated": "2014-03-14T08:16:39.000+0000", "description": "Let's say I want a graph with a single node, no edges. (I actually want this for a unit test.)\n\nscala> val vs:spark.rdd.RDD[(spark.graphx.VertexId, String)] = sc.makeRDD(Seq((1L, \"x\")))\nscala> val es:spark.rdd.RDD[spark.graphx.Edge[String]] = sc.makeRDD(Seq())\nscala> val g:spark.graphx.Graph[String, String] = spark.graphx.Graph(vs, es)\njava.lang.IllegalArgumentException: Positive number of slices required\n\tat org.apache.spark.rdd.ParallelCollectionRDD$.slice(ParallelCollectionRDD.scala:116)\n\tat org.apache.spark.rdd.ParallelCollectionRDD.getPartitions(ParallelCollectionRDD.scala:95)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:207)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:205)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:205)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:31)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:207)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:205)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:205)\n\tat org.apache.spark.Partitioner$.defaultPartitioner(Partitioner.scala:58)\n\tat org.apache.spark.graphx.EdgeRDD$$anonfun$1.apply(EdgeRDD.scala:45)\n\tat org.apache.spark.graphx.EdgeRDD$$anonfun$1.apply(EdgeRDD.scala:45)\n\tat scala.Option.orElse(Option.scala:257)\n\tat org.apache.spark.graphx.EdgeRDD.<init>(EdgeRDD.scala:45)\n\tat org.apache.spark.graphx.impl.GraphImpl$.createEdgeRDD(GraphImpl.scala:373)\n\tat org.apache.spark.graphx.impl.GraphImpl$.apply(GraphImpl.scala:319)\n\tat org.apache.spark.graphx.Graph$.apply(Graph.scala:411)\n\nMy impression is that raising an IllegalArgumentException in ParallelCollectionRDD.slice() is a mistake. It should just return a Seq(Seq()). This would match the behavior of how \"\".split(\"x\") returns Seq(\"\").\n\nLet me know if I'm missing something. I'm new to Spark/GraphX. But it looks like there is no test for corner cases with no nodes/edges in graphx/GraphSuite.scala. Probably an oversight?\n\nI'm happy to send a patch for this if you think I'm right.", "comments": ["Oh, I've now discovered spark.rdd.EmptyRDD! So that's how you do it. It does not seem very convenient to have to treat empty data as a special case though. What if I'm reading the data from a file, and the graph may or may not have edges?\n\nThanks for bearing with me!", "Looks like generally if an RDD ends up empty, it does not result in this exception, just when created with sc.makeRDD(). Don't mind me then."], "derived": {"summary": "Let's say I want a graph with a single node, no edges. (I actually want this for a unit test.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Cannot create graphx.Graph with no edges - Let's say I want a graph with a single node, no edges. (I actually want this for a unit test."}, {"q": "What updates or decisions were made in the discussion?", "a": "Looks like generally if an RDD ends up empty, it does not result in this exception, just when created with sc.makeRDD(). Don't mind me then."}]}}
{"project": "SPARK", "issue_id": "SPARK-1250", "title": "Misleading comments in Spark startup scripts", "status": "Resolved", "priority": "Trivial", "reporter": "Sumedh Mungee", "assignee": "Sumedh Mungee", "labels": [], "created": "2014-03-14T11:13:51.000+0000", "updated": "2014-05-25T20:57:24.000+0000", "description": "A couple of the scripts in bin/ (run-example, spark-class) have the following comment in the code:\n\n{noformat}\n# Figure out where the Scala framework is installed\n{noformat}\n\nThis suggests that the Scala framework is required to be installed before running Spark, which is misleading for newcomers. Instead, the comment should say:\n\n{noformat}\n# Figure out where Spark is installed\n{noformat}\n\nWill submit a pull request for this.", "comments": ["Submitted pull request: https://github.com/apache/spark/pull/843"], "derived": {"summary": "A couple of the scripts in bin/ (run-example, spark-class) have the following comment in the code:\n\n{noformat}\n# Figure out where the Scala framework is installed\n{noformat}\n\nThis suggests that the Scala framework is required to be installed before running Spark, which is misleading for newcomers. Instead, the comment should say:\n\n{noformat}\n# Figure out where Spark is installed\n{noformat}\n\nWill submit a pull request for this.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Misleading comments in Spark startup scripts - A couple of the scripts in bin/ (run-example, spark-class) have the following comment in the code:\n\n{noformat}\n# Figure out where the Scala framework is installed\n{noformat}\n\nThis suggests that the Scala framework is required to be installed before running Spark, which is misleading for newcomers. Instead, the comment should say:\n\n{noformat}\n# Figure out where Spark is installed\n{noformat}\n\nWill submit a pull request for this."}, {"q": "What updates or decisions were made in the discussion?", "a": "Submitted pull request: https://github.com/apache/spark/pull/843"}]}}
{"project": "SPARK", "issue_id": "SPARK-1251", "title": "Support for optimizing and executing structured queries", "status": "Resolved", "priority": "Blocker", "reporter": "Michael Armbrust", "assignee": "Michael Armbrust", "labels": [], "created": "2014-03-14T12:57:00.000+0000", "updated": "2014-03-26T13:33:21.000+0000", "description": "This is a proposal to add support for optimizing and executing relational queries in a manner that integrates tightly with the Core Spark API.  The goal is to allow Spark users to both run SQL queries over data that is currently stored in RDDs as well as run SQL queries over external data sources (such as Hive), returning the results as an RDD.\nh1. Components\nSpark SQL support will be broken into three major components.\nh2. Catalyst\nAn implementation-agnostic framework for manipulating trees of relational operators and expressions.  Catalyst was first discussed during [a talk at the most recent Spark Summit|http://spark-summit.org/talk/armbrust-catalyst-a-query-optimization-framework-for-spark-and-shark/], and a more detailed design document for Catalyst can be found [here|https://docs.google.com/document/d/1Hc_Ehtr0G8SQUg69cmViZsMi55_Kf3tISD9GPGU5M1Y/edit?usp=sharing].\n\nCatalyst provides three main features: a TreeNode library for transforming trees that are expressed as Scala case classes, a logical plan representation for relational operators, and an expression library.  A key design point here was to build an extensible system that could be extended for Streaming SQL and other dataflows that need optimization.\nh3. Public Interfaces\nCatalyst has no interfaces that are directly exposed to Spark users.\nh3. Dependencies\nCatalysts only dependency is on the [scala-logging interface for SLF4J by Typesafe|https://github.com/typesafehub/scala-logging].  This logging library integrates with Sparks existing logging infrastructure, with the added benefit of making logging statements that are below the current logging level even cheaper through the use of Scala macros.\nh2. Spark SQL\nThis component includes a set of standard physical operators (e.g. Project, Filter, Hash Join, Nested Loop Join, etc.), as well as a set of planning strategies that are used to select the specific physical operators that will be used to execute a given logical query plan.\nh3. Public Interfaces\nSqlContext, which takes as an argument a standard SparkContext.  This interface provides the ability to register RDDs as tables and run over them SQL statements expressed as strings, returning the results as an RDD. There is also support for registering data stored in Parquet as a table. Finally, there is an experimental DSL that allows queries to be expressed using a LINQ-like syntax.\nh3. Dependencies\nSpark SQLs only dependency is on the Parquet libraries, which have an active community and a small transitive dependency footprint.\nh2. Hive Support\nThe Hive module adds support to Spark SQL for interacting with data and queries in the Hive ecosystem.  This includes:\n* A mapping from a HiveQL AST to  catalyst logical plans / expression trees.\n* An interface that allows queries to reference tables that are extant in a Hive MetaStore.\n* A table scan operator that can read data from Hive SerDes.\n* Wrappers for existing Hive UDFs, UDAFs, and UDTFs.\n* Support for passing DDL commands back to hive for execution.\n\nh3. Public Interfaces\nThe Hive module provides a HiveContext which extends SqlContext with the ability to interact with existing Hive deployments using HiveQL.\nh3. Dependencies\nThe Hive module has dependencies on Hive 0.12.0, specifically hive-metastore, hive-exec, and hive-serde.  While these dependencies are on an unmodified version of Hive that can be obtained from Maven Central, they do introduce significant transitive dependencies to the project.  Due to this large dependency tree, and to avoid possible conflicts with dependencies of existing Spark applications, the Hive module is not included in the Spark assembly.  Instead, there is a separate, optional Hive assembly that is used only when present.\nh1. Changes to other Spark Components\nDependencies on Catalyst and Spark SQL are added to the Spark assembly.  The compute classpath is also changed to use the optional hive assembly if present.\nh1. Testing infrastructure\nEach of the submodules of Spark SQL has unit tests.  There is also a framework for executing a subset of the query tests that are included in the Hive distribution, which augments the test coverage with an additional 647 multi-part tests. When this framework runs, the query test files are broken up into individual statements and executed using Spark SQL.  The result of each query is then compared with a golden answer that has been pre-generated using a stock Hive system.  While these tests are primarily intended to ensure Hive compatibility, they also act as integration tests for the entire Spark SQL task.\nh1. Relationship to Shark\nUnlike Shark, Spark SQL does not act as a drop in replacement for Hive or the HiveServer.  Instead this new feature is intended to make it easier for Spark developers to run queries over structured data, using either SQL or the query DSL.  After this sub-project graduates from Alpha status it will likely become a new optimizer/backend for the Shark project.\n", "comments": [], "derived": {"summary": "This is a proposal to add support for optimizing and executing relational queries in a manner that integrates tightly with the Core Spark API. The goal is to allow Spark users to both run SQL queries over data that is currently stored in RDDs as well as run SQL queries over external data sources (such as Hive), returning the results as an RDD.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support for optimizing and executing structured queries - This is a proposal to add support for optimizing and executing relational queries in a manner that integrates tightly with the Core Spark API. The goal is to allow Spark users to both run SQL queries over data that is currently stored in RDDs as well as run SQL queries over external data sources (such as Hive), returning the results as an RDD."}]}}
{"project": "SPARK", "issue_id": "SPARK-1252", "title": "On YARN, use container-log4j.properties for executors", "status": "Resolved", "priority": "Critical", "reporter": "Sandy Ryza", "assignee": "Sandy Ryza", "labels": [], "created": "2014-03-14T14:09:52.000+0000", "updated": "2014-04-07T18:30:05.000+0000", "description": "YARN provides a log4j.properties file that's distinct from the NodeManager log4j.properties.  Containers are supposed to use this so that they don't try to write to the NodeManager log file.", "comments": ["https://github.com/apache/spark/pull/148"], "derived": {"summary": "YARN provides a log4j. properties file that's distinct from the NodeManager log4j.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "On YARN, use container-log4j.properties for executors - YARN provides a log4j. properties file that's distinct from the NodeManager log4j."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/148"}]}}
{"project": "SPARK", "issue_id": "SPARK-1253", "title": "Need to load mapred-site.xml for reading mapreduce.application.classpath", "status": "Resolved", "priority": "Major", "reporter": "Sanford Ryza", "assignee": null, "labels": [], "created": "2014-03-14T14:21:22.000+0000", "updated": "2015-12-23T18:03:45.000+0000", "description": "In Spark on YARN, we use mapreduce.application.classpath to discover the location of the MR jars so that we can add them executor classpaths.\n\nThis config comes from mapred-site.xml, which we aren't loading.", "comments": ["[~sandyr], Do you want to work on this issue? I would like to provide a PR for this if you don't mind, Thanks.", "I'm 99% sure Sandy was never working on it, so have unassigned it. I am also not sure this is still relevant.", "Thanks [~srowen] for letting me know. I tried to reproduce it but seems it is loading the mapred-site.xml file and giving the configurations updated in that file. I think it is not a problem anymore and it can be closed unless there are no other expectations here."], "derived": {"summary": "In Spark on YARN, we use mapreduce. application.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Need to load mapred-site.xml for reading mapreduce.application.classpath - In Spark on YARN, we use mapreduce. application."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks [~srowen] for letting me know. I tried to reproduce it but seems it is loading the mapred-site.xml file and giving the configurations updated in that file. I think it is not a problem anymore and it can be closed unless there are no other expectations here."}]}}
{"project": "SPARK", "issue_id": "SPARK-1254", "title": "Consolidate, order, and harmonize repository declarations in Maven/SBT builds", "status": "Resolved", "priority": "Minor", "reporter": "Sean R. Owen", "assignee": "Sean R. Owen", "labels": [], "created": "2014-03-14T14:33:30.000+0000", "updated": "2015-01-15T09:08:42.000+0000", "description": "This suggestion addresses a few minor suboptimalities with how repositories are handled.\n\n1) Use HTTPS consistently to access repos, instead of HTTP\n\n2) Consolidate repository declarations in the parent POM file, in the case of the Maven build, so that their ordering can be controlled to put the fully optional Cloudera repo at the end, after required repos. (This was prompted by the untimely failure of the Cloudera repo this week, which made the Spark build fail. #2 would have prevented that.)\n\n3) Update SBT build to match Maven build in this regard\n\n4) Update SBT build to *not* refer to Sonatype snapshot repos. This wasn't in Maven, and a build generally would not refer to external snapshots, but I'm not 100% sure on this one.", "comments": ["This was partially reverted to move Maven's repo back to HTTP:\n\nhttps://git-wip-us.apache.org/repos/asf?p=spark.git;a=commitdiff;h=abf6714e27cf07a13819b35a4ca50ff9bb28b65c;hp=646e55405b433fdedc9601dab91f99832b641f87\n"], "derived": {"summary": "This suggestion addresses a few minor suboptimalities with how repositories are handled. 1) Use HTTPS consistently to access repos, instead of HTTP\n\n2) Consolidate repository declarations in the parent POM file, in the case of the Maven build, so that their ordering can be controlled to put the fully optional Cloudera repo at the end, after required repos.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Consolidate, order, and harmonize repository declarations in Maven/SBT builds - This suggestion addresses a few minor suboptimalities with how repositories are handled. 1) Use HTTPS consistently to access repos, instead of HTTP\n\n2) Consolidate repository declarations in the parent POM file, in the case of the Maven build, so that their ordering can be controlled to put the fully optional Cloudera repo at the end, after required repos."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was partially reverted to move Maven's repo back to HTTP:\n\nhttps://git-wip-us.apache.org/repos/asf?p=spark.git;a=commitdiff;h=abf6714e27cf07a13819b35a4ca50ff9bb28b65c;hp=646e55405b433fdedc9601dab91f99832b641f87"}]}}
{"project": "SPARK", "issue_id": "SPARK-1255", "title": "Allow user to pass Serializer object instead of class name for shuffle.", "status": "Resolved", "priority": "Major", "reporter": "Reynold Xin", "assignee": "Reynold Xin", "labels": [], "created": "2014-03-15T01:10:45.000+0000", "updated": "2014-03-16T09:58:13.000+0000", "description": "This is more general than simply passing a string name and leaves more room for performance optimizations.\n\nNote that this is technically an API breaking change - but I suspect nobody else in this world has used this API other than me in GraphX and Shark.\n", "comments": ["Pull request submitted: https://github.com/apache/spark/pull/149"], "derived": {"summary": "This is more general than simply passing a string name and leaves more room for performance optimizations. Note that this is technically an API breaking change - but I suspect nobody else in this world has used this API other than me in GraphX and Shark.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "Allow user to pass Serializer object instead of class name for shuffle. - This is more general than simply passing a string name and leaves more room for performance optimizations. Note that this is technically an API breaking change - but I suspect nobody else in this world has used this API other than me in GraphX and Shark."}, {"q": "What updates or decisions were made in the discussion?", "a": "Pull request submitted: https://github.com/apache/spark/pull/149"}]}}
{"project": "SPARK", "issue_id": "SPARK-1256", "title": "Master web UI and Worker web UI returns a 404 error", "status": "Resolved", "priority": "Major", "reporter": "Guoqiang Li", "assignee": "Guoqiang Li", "labels": [], "created": "2014-03-15T03:58:22.000+0000", "updated": "2014-03-19T10:57:45.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Master web UI and Worker web UI returns a 404 error"}]}}
{"project": "SPARK", "issue_id": "SPARK-1257", "title": "Endless running task when using pyspark with input file containing a long line", "status": "Resolved", "priority": "Major", "reporter": "Hanchen Su", "assignee": "Josh Rosen", "labels": [], "created": "2014-03-15T04:36:33.000+0000", "updated": "2014-07-25T21:53:33.000+0000", "description": "When launching any pyspark applications with an input file containing a very long line(about 70000 characters), the job will be hanging and never stops. The application UI shows that there is a task running endlessly.\n\nThere will be no problem using the scala version with the same input.", "comments": ["I suspect that this was caused by SPARK-1043, which has been fixed for Spark 0.9.1.", "recommend close as resolved w/ option for filer to reopen if the issue reproduces in 1.0 /cc: [~pwendell] [~joshrosen]"], "derived": {"summary": "When launching any pyspark applications with an input file containing a very long line(about 70000 characters), the job will be hanging and never stops. The application UI shows that there is a task running endlessly.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Endless running task when using pyspark with input file containing a long line - When launching any pyspark applications with an input file containing a very long line(about 70000 characters), the job will be hanging and never stops. The application UI shows that there is a task running endlessly."}, {"q": "What updates or decisions were made in the discussion?", "a": "recommend close as resolved w/ option for filer to reopen if the issue reproduces in 1.0 /cc: [~pwendell] [~joshrosen]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1258", "title": "RDD.countByValue optimization", "status": "Resolved", "priority": "Trivial", "reporter": "Jaroslav Kamenik", "assignee": null, "labels": [], "created": "2014-03-15T05:52:30.000+0000", "updated": "2014-09-15T14:47:10.000+0000", "description": "Class Object2LongOpenHashMap has method add(key, incr) (addTo in new version) for incrementation value assigned to the key. It should be faster than currently used  map.put(v, map.getLong(v) + 1L) .", "comments": ["same is used in RDD.countByValueApprox\n\nand similar in RDD.countByValue.mergeMaps - m1.put(entry.getKey, m1.getLong(entry.getKey) + entry.getLongValue)\n\nand GroupedCountEvaluator.merge - sums.put(entry.getKey, sums.getLong(entry.getKey) + entry.getLongValue)", "I'm taking the liberty of closing this, since this refers to an optimization using fastutil classes, which were removed from Spark. An equivalent optimization is employed now, using Spark's OpenHashMap."], "derived": {"summary": "Class Object2LongOpenHashMap has method add(key, incr) (addTo in new version) for incrementation value assigned to the key. It should be faster than currently used  map.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "RDD.countByValue optimization - Class Object2LongOpenHashMap has method add(key, incr) (addTo in new version) for incrementation value assigned to the key. It should be faster than currently used  map."}, {"q": "What updates or decisions were made in the discussion?", "a": "I'm taking the liberty of closing this, since this refers to an optimization using fastutil classes, which were removed from Spark. An equivalent optimization is employed now, using Spark's OpenHashMap."}]}}
{"project": "SPARK", "issue_id": "SPARK-1259", "title": "Make RDD locally iterable", "status": "Resolved", "priority": "Minor", "reporter": "Egor Pakhomov", "assignee": "Egor Pakhomov", "labels": [], "created": "2014-03-16T05:10:25.000+0000", "updated": "2015-01-28T20:40:49.000+0000", "description": "I've got big RDD(1gb) in yarn cluster. On local machine, which use this cluster I have only 512 mb. I'd like to iterate over values in RDD on my local machine. I can't use collect(), because it would create too big array locally which more then my heap. I need some iterative way.", "comments": ["https://github.com/apache/spark/pull/156", "Fix version should probably be 1.0.x, since this is new functionality, not bug fix.", "Ah yes good catch."], "derived": {"summary": "I've got big RDD(1gb) in yarn cluster. On local machine, which use this cluster I have only 512 mb.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Make RDD locally iterable - I've got big RDD(1gb) in yarn cluster. On local machine, which use this cluster I have only 512 mb."}, {"q": "What updates or decisions were made in the discussion?", "a": "Ah yes good catch."}]}}
{"project": "SPARK", "issue_id": "SPARK-1260", "title": "faster construction of features with intercept", "status": "Resolved", "priority": "Major", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-03-17T11:47:41.000+0000", "updated": "2014-03-18T19:29:16.000+0000", "description": "The current implementation uses `Array(1.0, features: _*)` to construct a new array with intercept. This is not efficient for big arrays because `Array.apply` uses a for loop that iterates over the arguments. `Array.+:` is a better choice here.", "comments": ["PR: https://github.com/apache/spark/pull/161", "To mark fix versions.", "To mark resolved.", "merged."], "derived": {"summary": "The current implementation uses `Array(1. 0, features: _*)` to construct a new array with intercept.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "faster construction of features with intercept - The current implementation uses `Array(1. 0, features: _*)` to construct a new array with intercept."}, {"q": "What updates or decisions were made in the discussion?", "a": "To mark resolved."}]}}
{"project": "SPARK", "issue_id": "SPARK-1261", "title": "Docs don't explain how to run Python examples", "status": "Resolved", "priority": "Major", "reporter": "Diana Carroll", "assignee": "Diana Carroll", "labels": [], "created": "2014-03-17T12:02:47.000+0000", "updated": "2014-03-17T17:37:51.000+0000", "description": "The main Spark docs overview page has a section called Running the Examples and Shell, which explains how to start both the Scala and Python spark shells, but only how to run the Scala examples, not the python examples.", "comments": [], "derived": {"summary": "The main Spark docs overview page has a section called Running the Examples and Shell, which explains how to start both the Scala and Python spark shells, but only how to run the Scala examples, not the python examples.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Docs don't explain how to run Python examples - The main Spark docs overview page has a section called Running the Examples and Shell, which explains how to start both the Scala and Python spark shells, but only how to run the Scala examples, not the python examples."}]}}
{"project": "SPARK", "issue_id": "SPARK-1262", "title": "Numerical drift in computation of matrix inverse leads to invalid results in ALS", "status": "Resolved", "priority": "Major", "reporter": "Michael Allman", "assignee": null, "labels": [], "created": "2014-03-17T16:34:01.000+0000", "updated": "2015-01-21T06:58:28.000+0000", "description": "In what follows I cannot offer an expert analysis and remedy, however I will describe the problem I'm seeing and the strategy I've used to mitigate it.\n\nThe ALS {{updateBlock()}} method includes a call to {{Solve.solvePositive()}} from JBlas. Generally speaking, when we call {{solvePositive(A, B)}} for symmetric, positive definite {{A}}, this method computes {{x}} from the matrix equation {{Ax = B}}. Or, in other words, it computes {{A}}^-1^{{B}}. As mentioned, one of the preconditions on A is that it be symmetric. In ALS, we call this method on {{fullXtX}} or {{fullXtX.add(YtY.value.get)}}, both of which should be symmetric. However, for implicit ALS and rank > 1, some kind of imprecision in the computation of this inverse tends to make successive values of {{fullXtX.add(YtY.value.get)}} less and less symmetric. From my experience this can and does alter the model produced by ALS significantly, leading to very different and counterintuitive user-item recommendations compared to a solution in which this asymmetry does not exist.\n\nAn approach I've seen taken against this problem from the Oryx codebase is to cast each value in the vector returned from {{Solve.solvePositive()}} to a float. That has worked for me.\n\nI've also tried alternative implementations of a linear system solver. The solver that Oryx uses, {{RRQRDecomposition}} from commons math v3, seems to lead to better solutions, but still drifts unless the results are cast to floats.\n\nThe Colt numerical computing libraries include a solver called {{QRDecomposition}}, which may be superior to the one used in JBlas. However, I haven't tested it.\n\nI'm working on a unit test to exercise this bug. I discovered it by \"tracing\" the behavior of {{ALS.scala}} with various logging statements inserted into the code.\n\nIf nothing else, add a line before the calls to {{Solve.solvePositive()}} in {{updateBlock()}} to validate that {{fullXtX}} or {{fullXtX.add(YtY.value.get)}} are symmetric.", "comments": ["Solve.solvePositive calls NativeBLAS.dposv, which only looks at the upper triangular part of the matrix to compute Cholesky factorization while ignoring the lower triangular part. It doesn't seem to be the root cause of the problem. It would be super helpful if you can create a unit test to re-produce the bug. -Xiangrui", "Agree, though this might still be a symptom of something of interest. Michael is it YtY that is not symmetric? I can't see how fullXtX would not be symmetric. If it's not, somehow I'd want to figure out just why. Maybe I can look with a debugger too.\n\nAre you saying you see this in Oryx too? which matrix \"still drifts\"?\nThe use of floats is not driven by this issue but simply storage economy. It might mask some round-off error somewhere as a side-effect.\n\nFWIW Oryx does not use jblas or the Cholesky decomposition. It uses the QR decomposition to solve Ax=B. QR is somewhat slower but might give better accuracy. This alone could be the difference. I had never run empirical tests to figure out how much it might impact the results. I suppose you could hack up the code to use CholeskyDecomposition to see if it gives more similar results (I can help offline with that.)", "Producing a unit test for this may be infeasible at this time. For one thing, the \"unit\" here is hidden behind private methods. For another, there's no way to initialize the algorithm to a known value. What if I provided a patched version of ALS.scala with log tracing and a driver program instead? I can make the problem visually obvious that way.", "A patched ALS with a sample data would be sufficient. Do worry about private methods.", "The underlying assertion of this bug report is that the implementation is producing some asymmetric matrices that should be symmetric. However, I misread my debugging session output. The output matrices I was interpreting as asymmetric are in fact symmetric."], "derived": {"summary": "In what follows I cannot offer an expert analysis and remedy, however I will describe the problem I'm seeing and the strategy I've used to mitigate it. The ALS {{updateBlock()}} method includes a call to {{Solve.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Numerical drift in computation of matrix inverse leads to invalid results in ALS - In what follows I cannot offer an expert analysis and remedy, however I will describe the problem I'm seeing and the strategy I've used to mitigate it. The ALS {{updateBlock()}} method includes a call to {{Solve."}, {"q": "What updates or decisions were made in the discussion?", "a": "The underlying assertion of this bug report is that the implementation is producing some asymmetric matrices that should be symmetric. However, I misread my debugging session output. The output matrices I was interpreting as asymmetric are in fact symmetric."}]}}
{"project": "SPARK", "issue_id": "SPARK-1263", "title": "Implicit ALS unnecessarily recomputes factor matrices", "status": "Closed", "priority": "Major", "reporter": "Michael Allman", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-03-17T17:05:29.000+0000", "updated": "2014-03-17T23:57:31.000+0000", "description": "Implicit ALS unnecessarily recomputes the user and product factor matrices. As suggested by Xiangrui Meng on the mailing list, this is probably due to their reuse in computing YtY and YtCuY. I suggest persisting these matrices to avoid this duplicate computation. I'm working on a simple pull request to address this issue.", "comments": ["https://spark-project.atlassian.net/browse/SPARK-1266", "The PR mentioned in SPARK-1266 should fix this."], "derived": {"summary": "Implicit ALS unnecessarily recomputes the user and product factor matrices. As suggested by Xiangrui Meng on the mailing list, this is probably due to their reuse in computing YtY and YtCuY.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Implicit ALS unnecessarily recomputes factor matrices - Implicit ALS unnecessarily recomputes the user and product factor matrices. As suggested by Xiangrui Meng on the mailing list, this is probably due to their reuse in computing YtY and YtCuY."}, {"q": "What updates or decisions were made in the discussion?", "a": "The PR mentioned in SPARK-1266 should fix this."}]}}
{"project": "SPARK", "issue_id": "SPARK-1264", "title": "Documentation for setting heap sizes across all configurations", "status": "Resolved", "priority": "Major", "reporter": "Andrew Ash", "assignee": null, "labels": [], "created": "2014-03-17T18:33:32.000+0000", "updated": "2016-01-04T14:45:17.000+0000", "description": "As a user, there are lots of places to configure heap sizes, and it takes a bit of trial and error to figure out how to configure what you want.\n\nWe need some more clear documentation on how set these for the cross product of Spark components (master, worker, executor, driver, shell) and deployment modes (Standalone, YARN, Mesos, EC2?).\n\nI'm happy to do the authoring if someone can help pull together the relevant details.\n\nHere's the best I've got so far:\n\n{noformat}\n# Standalone cluster\n\nMaster - SPARK_DAEMON_MEMORY - default: 512mb\nWorker - SPARK_DAEMON_MEMORY vs SPARK_WORKER_MEMORY? - default: ?  See WorkerArguments.inferDefaultMemory()\nExecutor - spark.executor.memory\nDriver - SPARK_DRIVER_MEMORY - default: 512mb\nShell - A pre-built driver so SPARK_DRIVER_MEMORY - default: 512mb\n\n# EC2 cluster\n\nMaster - ?\nWorker - ?\nExecutor - ?\nDriver - ?\nShell - ?\n\n# Mesos cluster\n\nMaster - SPARK_DAEMON_MEMORY\nWorker - SPARK_DAEMON_MEMORY\nExecutor - SPARK_EXECUTOR_MEMORY\nDriver - SPARK_DRIVER_MEMORY\nShell - A pre-built driver so SPARK_DRIVER_MEMORY\n\n# YARN cluster\n\nMaster - SPARK_MASTER_MEMORY ?\nWorker - SPARK_WORKER_MEMORY ?\nExecutor - SPARK_EXECUTOR_MEMORY\nDriver - SPARK_DRIVER_MEMORY\nShell - A pre-built driver so SPARK_DRIVER_MEMORY\n{noformat}", "comments": ["Here's what I've seen can control what in various cluster configurations, but I'm not sure I caught them all.\n\nMaster:\nSPARK_DAEMON_MEMORY\nSPARK_MASTER_MEMORY\nSPARK_MASTER_OPTS (Xmx)\n\nWorker:\nSPARK_DAEMON_MEMORY\nSPARK_WORKER_MEMORY\nSPARK_WORKER_OPTS (Xmx)\n\nExecutor:\nSPARK_EXECUTOR_MEMORY\nSPARK_EXECUTOR_OPTS (Xmx)\nspark.executor.memory\n\nDriver:\nSPARK_DRIVER_MEMORY\nSPARK_REPL_OPTS (Xmx)", "In YARN mode, SPARK_MASTER_MEMORY and SPARK_WORKER_MEMORY were actually misnomers -- they actually configured the driver and executor memory, respectively. We have deprecated it so it is no longer documented.\n\nIn standalone mode, SPARK_WORKER_MEMORY is actually something completely different: it is the amount of memory that a worker advertises as available for drivers to launch executors. The sum of the memory used by executors spawned from a worker cannot exceed SPARK_WORKER_MEMORY. It is documented in our standalone cluster configuration.\n\nThe *_OPTS series cannot be generally used to configure memory, because ./spark-class ALWAYS sets Xms and Xmx, even when we just use the default memory of 512MB.\n\nAlso, not related to memory, but SPARK_JAVA_OPTS is used by all four components.", "I think this is mostly obsolete by now"], "derived": {"summary": "As a user, there are lots of places to configure heap sizes, and it takes a bit of trial and error to figure out how to configure what you want. We need some more clear documentation on how set these for the cross product of Spark components (master, worker, executor, driver, shell) and deployment modes (Standalone, YARN, Mesos, EC2?).", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Documentation for setting heap sizes across all configurations - As a user, there are lots of places to configure heap sizes, and it takes a bit of trial and error to figure out how to configure what you want. We need some more clear documentation on how set these for the cross product of Spark components (master, worker, executor, driver, shell) and deployment modes (Standalone, YARN, Mesos, EC2?)."}, {"q": "What updates or decisions were made in the discussion?", "a": "I think this is mostly obsolete by now"}]}}
{"project": "SPARK", "issue_id": "SPARK-1265", "title": "Fix 404 not found error in UI introduced in Jetty 9.0 upgrade", "status": "Closed", "priority": "Major", "reporter": "Andrew Or", "assignee": "Andrew Or", "labels": [], "created": "2014-03-17T19:50:10.000+0000", "updated": "2014-03-19T10:57:45.000+0000", "description": "We recently upgraded Jetty from v7.6.8 to v9.1.3. This introduced a 404 not found HTTP error when accessing the root of any UI.\n\nThe problem is that the existing code attaches two handlers to the root, one of which is a handler for static resources. In Jetty 9.1.3, we are no longer allowed to do this. Instead, we are supposed to use ResourceHandler rather than ServletContextHandler for serving static resources: http://stackoverflow.com/questions/10284584/serving-static-files-w-embedded-jetty.", "comments": ["Fixed in 1256, or PR #150."], "derived": {"summary": "We recently upgraded Jetty from v7. 6.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Fix 404 not found error in UI introduced in Jetty 9.0 upgrade - We recently upgraded Jetty from v7. 6."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed in 1256, or PR #150."}]}}
{"project": "SPARK", "issue_id": "SPARK-1266", "title": "Persist factors in implicit ALS", "status": "Resolved", "priority": "Major", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-03-17T21:40:44.000+0000", "updated": "2014-03-18T17:21:55.000+0000", "description": "In implicit ALS computation, the user or product factor is used twice in each iteration. Caching can certainly help accelerate the computation.", "comments": ["PR: https://github.com/apache/spark/pull/165"], "derived": {"summary": "In implicit ALS computation, the user or product factor is used twice in each iteration. Caching can certainly help accelerate the computation.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Persist factors in implicit ALS - In implicit ALS computation, the user or product factor is used twice in each iteration. Caching can certainly help accelerate the computation."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR: https://github.com/apache/spark/pull/165"}]}}
{"project": "SPARK", "issue_id": "SPARK-1270", "title": "An optimized gradient descent implementation", "status": "Resolved", "priority": "Major", "reporter": "Xusen Yin", "assignee": null, "labels": ["GradientDescent", "MLLib,"], "created": "2014-03-17T23:24:27.000+0000", "updated": "2016-01-16T13:31:28.000+0000", "description": "Current implementation of GradientDescent is inefficient in some aspects, especially in high-latency network. I propose a new implementation of GradientDescent, which follows a parallelism model called GradientDescentWithLocalUpdate, inspired by Jeff Dean's DistBelief and Eric Xing's SSP. With a few modifications of runMiniBatchSGD, the GradientDescentWithLocalUpdate can outperform the original sequential version by about 4x without sacrificing accuracy, and can be easily adopted by most classification and regression algorithms in MLlib.", "comments": ["Yo, any follow up story on this one?\nI'm curious to know the local update part, as DistBelief has non-local model server shards.", "User 'yoshidakuy' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/10663"], "derived": {"summary": "Current implementation of GradientDescent is inefficient in some aspects, especially in high-latency network. I propose a new implementation of GradientDescent, which follows a parallelism model called GradientDescentWithLocalUpdate, inspired by Jeff Dean's DistBelief and Eric Xing's SSP.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "An optimized gradient descent implementation - Current implementation of GradientDescent is inefficient in some aspects, especially in high-latency network. I propose a new implementation of GradientDescent, which follows a parallelism model called GradientDescentWithLocalUpdate, inspired by Jeff Dean's DistBelief and Eric Xing's SSP."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'yoshidakuy' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/10663"}]}}
{"project": "SPARK", "issue_id": "SPARK-1267", "title": "Add a pip installer for PySpark", "status": "Resolved", "priority": "Minor", "reporter": "Prabin Banka", "assignee": "Holden Karau", "labels": ["pyspark"], "created": "2014-03-18T02:46:06.000+0000", "updated": "2016-11-17T04:16:31.000+0000", "description": "Please refer to this mail archive,\n\nhttp://mail-archives.apache.org/mod_mbox/spark-user/201311.mbox/%3CCAOEPXP7jKiw-3M8eh2giBcs8gEkZ1upHpGb=FqOUcVSCYwjhNg@mail.gmail.com%3E", "comments": ["We can write a simple setup.py file, for pyspark source distribution. \nAny end user, who intend to use pyspark modules need to do a pip install of pyspark and set the SPARK_HOME env variable, before importing the pyspark into this code. \nAlso, we can introduce one more environment variable, say SPARK_VERSION, this needs to be validated against the pyspark installed version, during the import time. A dictionary could be maintained in a text file under spark/python, to validate the compatibility of pyspark and spark.\n\nWill this be sufficient ? ", "@Josh.. please comment.", "I'm all for pip installable pyspark, but I'm confused about the ideal way to install the pyspark code.  I'd also prefer to avoid introducing an extra variable, SPARK_VERSION.  It seems to me that if we had the typical setup.py file that downloaded code from PyPi, then users would have to deal with differences in dependencies between the python version in PyPi and in their code pointed to by SPARK_HOME.   Additionally, users would still need to download the spark jars or set SPARK_HOME, which means two (possibly different) versions of the python code are flying around. The fact that users have to manage the version, download spark into SPARK_HOME, and pip install pyspark doesn't seem quite right.\n\nWhat do you think about this:  We create a setup.py file that requires SPARK_HOME be set in the environment (requiring that the user have downloaded Spark) BEFORE the pyspark code gets installed.  \n\nAn additional idea we could consider:  Then, when pip or a user calls pyspark, we have \"python setup.py install\" redirect to \"python setup.py develop.\"  This installs pyspark in \"development mode\" and means that the pyspark code pointed to by $SPARK_HOME/python is the source of truth.  (more about development mode here: https://pythonhosted.org/setuptools/setuptools.html#development-mode).  My thinking for this is that since users need to specify SPARK_HOME, we might as well keep the python library with the spark code (as it currently is) to avoid potential compatibility conflicts.  As a maintainer, we also don't need to update PyPi with the latest version of pyspark.  Using \"develop mode\" as default may be a bad idea.  I also don't know how to automatically prefer \"setup.py develop\" over \"setup.py install\".\n\nLast, and perhaps most obvious, if we create a setup.py file, we could also probably no longer include the py4j egg in the spark downloads as we'd rely on setuptools to provide the external libraries.\n\n", "[~adgaudio] I had similar reservations about the approach. I will try to investigate the possibility of using 'develop' mode.", "Because PySpark depends on Spark packages, Python user can not use it after 'pip install pyspark',  so there is not too much benefits from this.\n\nOnce we release PySpark separated from Spark, then we should keep the compatability across versions of PySpark and Spark, it will be a nightmare for us (we can not move fast to improve the implementation of PySpark).\n\nSo, I think we can not do this in near future. [~prabinb], do you mind to close the PR?\n", "Closing this PR for now. ", "User 'alope107' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/8318", "re-opening after discussion on mailing list and PR thread.", "User 'holdenk' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/15659", "Merged into master (2.2) and will consider for 2.1."], "derived": {"summary": "Please refer to this mail archive,\n\nhttp://mail-archives. apache.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add a pip installer for PySpark - Please refer to this mail archive,\n\nhttp://mail-archives. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "Merged into master (2.2) and will consider for 2.1."}]}}
{"project": "SPARK", "issue_id": "SPARK-1268", "title": "Adding XOR and AND-NOT operations to spark.util.collection.BitSet", "status": "Resolved", "priority": "Minor", "reporter": "Petko Nikolov", "assignee": null, "labels": ["starter"], "created": "2014-03-18T08:18:53.000+0000", "updated": "2014-04-30T00:41:51.000+0000", "description": "BitSet collection is missing some important bit-wise operations. Symmetric difference (xor) in particular is useful for computing some distance metrics (e.g. Hamming). Difference (and-not) as well.", "comments": ["PR: https://github.com/apache/spark/pull/172"], "derived": {"summary": "BitSet collection is missing some important bit-wise operations. Symmetric difference (xor) in particular is useful for computing some distance metrics (e.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Adding XOR and AND-NOT operations to spark.util.collection.BitSet - BitSet collection is missing some important bit-wise operations. Symmetric difference (xor) in particular is useful for computing some distance metrics (e."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR: https://github.com/apache/spark/pull/172"}]}}
{"project": "SPARK", "issue_id": "SPARK-1269", "title": "Add Ability to Make Distribution with Tachyon", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": null, "labels": [], "created": "2014-03-18T14:49:41.000+0000", "updated": "2014-03-30T04:15:23.000+0000", "description": "It would be nice to have a way where people can manually bundle Tachyon with Spark if they would like.", "comments": [], "derived": {"summary": "It would be nice to have a way where people can manually bundle Tachyon with Spark if they would like.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add Ability to Make Distribution with Tachyon - It would be nice to have a way where people can manually bundle Tachyon with Spark if they would like."}]}}
{"project": "SPARK", "issue_id": "SPARK-1271", "title": "Use Iterable[X] in co-group and group-by signatures", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Holden Karau", "labels": [], "created": "2014-03-18T17:37:19.000+0000", "updated": "2014-04-09T01:18:25.000+0000", "description": "This API change will allow us to externalize these things down the road. Note that this will be a backwards incompatible change for users, but the solution is very simple, in cases where previously `Seq`'s were expected, the user can just call `toSeq` on the returned iterable.", "comments": ["Working on it :)", "https://github.com/apache/spark/pull/242"], "derived": {"summary": "This API change will allow us to externalize these things down the road. Note that this will be a backwards incompatible change for users, but the solution is very simple, in cases where previously `Seq`'s were expected, the user can just call `toSeq` on the returned iterable.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Use Iterable[X] in co-group and group-by signatures - This API change will allow us to externalize these things down the road. Note that this will be a backwards incompatible change for users, but the solution is very simple, in cases where previously `Seq`'s were expected, the user can just call `toSeq` on the returned iterable."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/242"}]}}
{"project": "SPARK", "issue_id": "SPARK-1272", "title": "Don't fail job if some local directories are buggy", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": null, "labels": ["bulk-closed"], "created": "2014-03-18T18:02:16.000+0000", "updated": "2020-05-17T18:30:39.000+0000", "description": "If Spark cannot create shuffle directories inside of a local directory it might make sense to just log an error and continue, provided that at least one valid shuffle directory exists. Otherwise if a single disk is wonky the entire job can fail.\n\nThe down side is that this might mask failures if the person actually misconfigures the local directories to point to the wrong disk(s).", "comments": ["@Patrick  How about assign this to me", "[~qqsun8819] I'd also like to see this implemented, as it should be easy. I don't think you need it assigned (I can't assign it myself); just open a PR with your proposed change?", "BTW, is there a good approach to skip failed disk if {{DiskBlockManager}} is using hashing strategy?\nAFAIK, we can skip corrupted disk by using next disk until reaching a success or exhausted all disks. And do the same when trying to locate existing file. Is this viable?"], "derived": {"summary": "If Spark cannot create shuffle directories inside of a local directory it might make sense to just log an error and continue, provided that at least one valid shuffle directory exists. Otherwise if a single disk is wonky the entire job can fail.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Don't fail job if some local directories are buggy - If Spark cannot create shuffle directories inside of a local directory it might make sense to just log an error and continue, provided that at least one valid shuffle directory exists. Otherwise if a single disk is wonky the entire job can fail."}, {"q": "What updates or decisions were made in the discussion?", "a": "BTW, is there a good approach to skip failed disk if {{DiskBlockManager}} is using hashing strategy?\nAFAIK, we can skip corrupted disk by using next disk until reaching a success or exhausted all disks. And do the same when trying to locate existing file. Is this viable?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1273", "title": "MLlib v0.9.1 release", "status": "Resolved", "priority": "Major", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-03-18T19:17:47.000+0000", "updated": "2014-03-21T15:00:15.000+0000", "description": "Pick MLlib bug fixes, improvements, and doc updates since v0.9.0 for v0.9.1.", "comments": [], "derived": {"summary": "Pick MLlib bug fixes, improvements, and doc updates since v0. 9.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "MLlib v0.9.1 release - Pick MLlib bug fixes, improvements, and doc updates since v0. 9."}]}}
{"project": "SPARK", "issue_id": "SPARK-1274", "title": "Add dev scripts to merge PRs and create releases from master to branch-0.9", "status": "Closed", "priority": "Major", "reporter": "Tathagata Das", "assignee": "Tathagata Das", "labels": [], "created": "2014-03-18T20:41:31.000+0000", "updated": "2014-03-18T22:19:34.000+0000", "description": "Master branch of Spark has scripts in SPARK_HOME/dev to merge PRs and create release candidates. These should also be present in branch-0.9 for making future releases.", "comments": [], "derived": {"summary": "Master branch of Spark has scripts in SPARK_HOME/dev to merge PRs and create release candidates. These should also be present in branch-0.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add dev scripts to merge PRs and create releases from master to branch-0.9 - Master branch of Spark has scripts in SPARK_HOME/dev to merge PRs and create release candidates. These should also be present in branch-0."}]}}
{"project": "SPARK", "issue_id": "SPARK-1275", "title": "Made SPARK_HOME/dev/tests executable", "status": "Resolved", "priority": "Major", "reporter": "Tathagata Das", "assignee": "Tathagata Das", "labels": [], "created": "2014-03-18T22:45:46.000+0000", "updated": "2014-03-19T16:42:05.000+0000", "description": "dev/run-tests was causing Jenkins tests to fail.", "comments": [], "derived": {"summary": "dev/run-tests was causing Jenkins tests to fail.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Made SPARK_HOME/dev/tests executable - dev/run-tests was causing Jenkins tests to fail."}]}}
{"project": "SPARK", "issue_id": "SPARK-1276", "title": "Add a Simple History Server for the UI for Yarn / Mesos", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Andrew Or", "labels": [], "created": "2014-03-18T23:33:35.000+0000", "updated": "2014-04-10T17:40:43.000+0000", "description": "For the 1.0 release we should have a basic history server for non-standalone modes (e.g. Yarn, Mesos). It can just be initialized with a storage directory and watch for new applications. Ideally this should be able to respect UI security filters as well.\n\nThis might require us to add some sort of \"COMPLETED\" file or some synchronization to know when the UI has finished running so we don't try to load the file a bunch of times.", "comments": [], "derived": {"summary": "For the 1. 0 release we should have a basic history server for non-standalone modes (e.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add a Simple History Server for the UI for Yarn / Mesos - For the 1. 0 release we should have a basic history server for non-standalone modes (e."}]}}
{"project": "SPARK", "issue_id": "SPARK-1277", "title": "Automatically set the UI persistence directory based on cluster settings", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Andrew Or", "labels": [], "created": "2014-03-18T23:36:37.000+0000", "updated": "2014-04-16T16:22:20.000+0000", "description": "More details forthcoming", "comments": ["Subsumed by other configuration patches."], "derived": {"summary": "More details forthcoming.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Automatically set the UI persistence directory based on cluster settings - More details forthcoming."}, {"q": "What updates or decisions were made in the discussion?", "a": "Subsumed by other configuration patches."}]}}
{"project": "SPARK", "issue_id": "SPARK-1278", "title": "Improper use of SimpleDateFormat", "status": "Resolved", "priority": "Minor", "reporter": "Shixiong Zhu", "assignee": "Shixiong Zhu", "labels": [], "created": "2014-03-19T06:44:48.000+0000", "updated": "2014-03-21T16:41:08.000+0000", "description": "SimpleDateFormat is not thread-safe. Some places use the same SimpleDateFormat object without safeguard in the multiple threads. It will cause that the Web UI displays improper date.", "comments": ["PR: https://github.com/apache/spark/pull/179"], "derived": {"summary": "SimpleDateFormat is not thread-safe. Some places use the same SimpleDateFormat object without safeguard in the multiple threads.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Improper use of SimpleDateFormat - SimpleDateFormat is not thread-safe. Some places use the same SimpleDateFormat object without safeguard in the multiple threads."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR: https://github.com/apache/spark/pull/179"}]}}
{"project": "SPARK", "issue_id": "SPARK-1279", "title": "Stage.name return  \"apply at Option.scala:120\"", "status": "Resolved", "priority": "Major", "reporter": "Guoqiang Li", "assignee": null, "labels": [], "created": "2014-03-19T07:34:33.000+0000", "updated": "2014-09-27T18:52:54.000+0000", "description": null, "comments": ["Obvious accidental dupe of SPARK-1280"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Stage.name return  \"apply at Option.scala:120\""}, {"q": "What updates or decisions were made in the discussion?", "a": "Obvious accidental dupe of SPARK-1280"}]}}
{"project": "SPARK", "issue_id": "SPARK-1280", "title": "Stage.name return \"apply at Option.scala:120\"", "status": "Resolved", "priority": "Critical", "reporter": "Guoqiang Li", "assignee": "Aaron Davidson", "labels": [], "created": "2014-03-19T07:35:48.000+0000", "updated": "2014-09-27T18:53:40.000+0000", "description": null, "comments": ["Here's what the stack trace looks like:\n\n{code}\njava.lang.RuntimeException\n    at org.apache.spark.util.Utils$.getCallSiteInfo(Utils.scala:687)\n    at org.apache.spark.util.Utils$.formatCallSiteInfo$default$1(Utils.scala:723)\n    at org.apache.spark.SparkContext$$anonfun$getCallSite$1.apply(SparkContext.scala:880)\n    at org.apache.spark.SparkContext$$anonfun$getCallSite$1.apply(SparkContext.scala:880)\n    at scala.Option.getOrElse(Option.scala:120)\n    at org.apache.spark.SparkContext.getCallSite(SparkContext.scala:880)\n    at org.apache.spark.SparkContext.runJob(SparkContext.scala:898)\n    at org.apache.spark.SparkContext.runJob(SparkContext.scala:920)\n    at org.apache.spark.SparkContext.runJob(SparkContext.scala:934)\n    at org.apache.spark.SparkContext.runJob(SparkContext.scala:948)\n    at org.apache.spark.rdd.RDD.collect(RDD.scala:657)\n        ...\n    at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:981)\n    at org.apache.spark.repl.Main$.main(Main.scala:31)\n    at org.apache.spark.repl.Main.main(Main.scala)\n{code}\n\nIt appears this was accidentally introduced when SparkContext#getCallSite was changed to use the Option pattern."], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Stage.name return \"apply at Option.scala:120\""}, {"q": "What updates or decisions were made in the discussion?", "a": "Here's what the stack trace looks like:\n\n{code}\njava.lang.RuntimeException\n    at org.apache.spark.util.Utils$.getCallSiteInfo(Utils.scala:687)\n    at org.apache.spark.util.Utils$.formatCallSiteInfo$default$1(Utils.scala:723)\n    at org.apache.spark.SparkContext$$anonfun$getCallSite$1.apply(SparkContext.scala:880)\n    at org.apache.spark.SparkContext$$anonfun$getCallSite$1.apply(SparkContext.scala:880)\n    at scala.Option.getOrElse(Option.scala:120)\n    at org.apache.spark.SparkContext.getCallSite(SparkContext.scala:880)\n    at org.apache.spark.SparkContext.runJob(SparkContext.scala:898)\n    at org.apache.spark.SparkContext.runJob(SparkContext.scala:920)\n    at org.apache.spark.SparkContext.runJob(SparkContext.scala:934)\n    at org.apache.spark.SparkContext.runJob(SparkContext.scala:948)\n    at org.apache.spark.rdd.RDD.collect(RDD.scala:657)\n        ...\n    at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:981)\n    at org.apache.spark.repl.Main$.main(Main.scala:31)\n    at org.apache.spark.repl.Main.main(Main.scala)\n{code}\n\nIt appears this was accidentally introduced when SparkContext#getCallSite was changed to use the Option pattern."}]}}
{"project": "SPARK", "issue_id": "SPARK-1281", "title": "Partitioning in ALS", "status": "Resolved", "priority": "Minor", "reporter": "Xiangrui Meng", "assignee": "Tor Myklebust", "labels": [], "created": "2014-03-19T10:15:31.000+0000", "updated": "2014-04-23T02:23:42.000+0000", "description": "There are some minor issues about partitioning with the current implementation of ALS:\n\n1. Mod-based partitioner is used for mapping users/products to blocks. This might cause problems if the ids contains information. For example, the last digit may indicate the user/product type. This can be fixed by hashing.\n\n2. HashPartitioner is used on the initial partition. This is the same as the mod-based partitioner when the key is a positive integer. But it is certainly error-prone.", "comments": [], "derived": {"summary": "There are some minor issues about partitioning with the current implementation of ALS:\n\n1. Mod-based partitioner is used for mapping users/products to blocks.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Partitioning in ALS - There are some minor issues about partitioning with the current implementation of ALS:\n\n1. Mod-based partitioner is used for mapping users/products to blocks."}]}}
{"project": "SPARK", "issue_id": "SPARK-1282", "title": "Create spark-contrib repo for 1.0", "status": "Resolved", "priority": "Major", "reporter": "Evan Chan", "assignee": null, "labels": [], "created": "2014-03-19T16:08:31.000+0000", "updated": "2014-03-21T14:55:42.000+0000", "description": "Let's create a \"spark-contrib\" repo to host community projects for the Spark ecosystem that don't quite belong in core, but are very important nevertheless.\n\nIt would be linked to from official Spark documentation and web site, and help provide visibility for community projects.\n\nSome questions:\n- Who should host this repo, and where should it be hosted?\n    - Github would be a strong preference from usability standpoint\n    - There is talk that Apache might have some facility for this\n- Contents.  Should it simply be links?  Git submodules?\n", "comments": [], "derived": {"summary": "Let's create a \"spark-contrib\" repo to host community projects for the Spark ecosystem that don't quite belong in core, but are very important nevertheless. It would be linked to from official Spark documentation and web site, and help provide visibility for community projects.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Create spark-contrib repo for 1.0 - Let's create a \"spark-contrib\" repo to host community projects for the Spark ecosystem that don't quite belong in core, but are very important nevertheless. It would be linked to from official Spark documentation and web site, and help provide visibility for community projects."}]}}
{"project": "SPARK", "issue_id": "SPARK-1283", "title": "Create spark-contrib repo for 1.0", "status": "Resolved", "priority": "Major", "reporter": "Evan Chan", "assignee": null, "labels": [], "created": "2014-03-19T16:10:43.000+0000", "updated": "2015-01-27T17:58:24.000+0000", "description": "Let's create a \"spark-contrib\" repo to host community projects for the Spark ecosystem that don't quite belong in core, but are very important nevertheless.\n\nIt would be linked to from official Spark documentation and web site, and help provide visibility for community projects.\n\nSome questions:\n- Who should host this repo, and where should it be hosted?\n    - Github would be a strong preference from usability standpoint\n    - There is talk that Apache might have some facility for this\n- Contents.  Should it simply be links?  Git submodules?\n", "comments": ["I think the main Apache support for this is via Apache Extras (which is on Google Code). To me that seems pretty dated and probably not the best option:\nhttp://community.apache.org/apache-extras/faq.html\n\nIn my mind the coolest thing might be a github repo that uses submodules. In terms of hosting, it's possible the AMPLab could host it - I've reached out to them and it seems like there is some interest. We could also create a  new github account called spark-contrib and just give the Spark committers access to it somehow.", "+1 for Amplab-hosted spark-contrib or the new github account.\n\nAs for access to a spark-contrib github account, how about Spark committers + maintainers for the individual projects for each repo?", "I have another idea that may be simpler than a repo.\n\nWhy not just create a landing page on the Spark docs for contrib projects?    We can just edit it via standard PR process.   It would be simpler, and not require any hosting, faster to get done.", "<ping>  Any more comments?  Objections to creating a landing page for contrib projects in the Spark docs?", "[~velvia] Yeah - do you want to submit a PR? This seems like a good idea to me.", "Yep, will submit a PR>\n\n\n\n\n\n\n-- \nThe fruit of silence is prayer;\nthe fruit of prayer is faith;\nthe fruit of faith is love;\nthe fruit of love is service;\nthe fruit of service is peace.  -- Mother Teresa\n", "Is this obsolete given the existence of spark-packages.org now?", "Yeah this is not needed anymore, with spark-packages.org"], "derived": {"summary": "Let's create a \"spark-contrib\" repo to host community projects for the Spark ecosystem that don't quite belong in core, but are very important nevertheless. It would be linked to from official Spark documentation and web site, and help provide visibility for community projects.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Create spark-contrib repo for 1.0 - Let's create a \"spark-contrib\" repo to host community projects for the Spark ecosystem that don't quite belong in core, but are very important nevertheless. It would be linked to from official Spark documentation and web site, and help provide visibility for community projects."}, {"q": "What updates or decisions were made in the discussion?", "a": "Yeah this is not needed anymore, with spark-packages.org"}]}}
{"project": "SPARK", "issue_id": "SPARK-1284", "title": "pyspark hangs after IOError on Executor", "status": "Resolved", "priority": "Major", "reporter": "Jim Blomo", "assignee": "Davies Liu", "labels": [], "created": "2014-03-19T16:16:01.000+0000", "updated": "2014-10-02T21:23:03.000+0000", "description": "When running a reduceByKey over a cached RDD, Python fails with an exception, but the failure is not detected by the task runner.  Spark and the pyspark shell hang waiting for the task to finish.\n\nThe error is:\n{code}\nPySpark worker failed with exception:\nTraceback (most recent call last):\n  File \"/home/hadoop/spark/python/pyspark/worker.py\", line 77, in main\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/hadoop/spark/python/pyspark/serializers.py\", line 182, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/home/hadoop/spark/python/pyspark/serializers.py\", line 118, in dump_stream\n    self._write_with_length(obj, stream)\n  File \"/home/hadoop/spark/python/pyspark/serializers.py\", line 130, in _write_with_length\n    stream.write(serialized)\nIOError: [Errno 104] Connection reset by peer\n\n14/03/19 22:48:15 INFO scheduler.TaskSetManager: Serialized task 4.0:0 as 4257 bytes in 47 ms\nTraceback (most recent call last):\n  File \"/home/hadoop/spark/python/pyspark/daemon.py\", line 117, in launch_worker\n    worker(listen_sock)\n  File \"/home/hadoop/spark/python/pyspark/daemon.py\", line 107, in worker\n    outfile.flush()\nIOError: [Errno 32] Broken pipe\n{code}\n\nI can reproduce the error by running take(10) on the cached RDD before running reduceByKey (which looks at the whole input file).\n\nAffects Version 1.0.0-SNAPSHOT (4d88030486)", "comments": ["[~jblomo] -\n\nwill you add a reproducer script to this issue?\n\ni did a simple test based on what you suggested w/ the tip of master and could not reproduce -\n\n{code}\n$ ./dist/bin/pyspark\nPython 2.7.5 (default, Feb 19 2014, 13:47:28) \n[GCC 4.8.2 20131212 (Red Hat 4.8.2-7)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n...\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.0.0-SNAPSHOT\n      /_/\n\nUsing Python version 2.7.5 (default, Feb 19 2014 13:47:28)\nSparkContext available as sc.\n>>> data = sc.textFile('/etc/passwd')\n14/07/02 07:03:59 INFO MemoryStore: ensureFreeSpace(32816) called with curMem=0, maxMem=308910489\n14/07/02 07:03:59 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 32.0 KB, free 294.6 MB)\n>>> data.cache()\n/etc/passwd MappedRDD[1] at textFile at NativeMethodAccessorImpl.java:-2\n>>> data.take(10)\n...[expected output]...\n>>> data.flatMap(lambda line: line.split(':')).map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y).collect()\n...[expected output, no hang]...\n{code}", "[~jblomo], could you reproduce this on master or 1.1 branch?\n\nMaybe the pyspark did not hange after this error message, the take() had finished successfully before the error message pop up. The noisy error messages had been fixed in PR https://github.com/apache/spark/pull/1625 ", "I will try to reproduce on the 1.1 branch later this week, thanks for the update!", "Hi, having trouble compiling either master or branch-1.1, I sent a request to the mailing list for help.  Are there any compiled snapshots?", "[~jblomo] master should be buildable again, please give it another shot", "I think this is an logging issue ,should be fixed by https://github.com/apache/spark/pull/1625, so close it.\n\nIf anyone meet this again, we can reopen it."], "derived": {"summary": "When running a reduceByKey over a cached RDD, Python fails with an exception, but the failure is not detected by the task runner. Spark and the pyspark shell hang waiting for the task to finish.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "pyspark hangs after IOError on Executor - When running a reduceByKey over a cached RDD, Python fails with an exception, but the failure is not detected by the task runner. Spark and the pyspark shell hang waiting for the task to finish."}, {"q": "What updates or decisions were made in the discussion?", "a": "I think this is an logging issue ,should be fixed by https://github.com/apache/spark/pull/1625, so close it.\n\nIf anyone meet this again, we can reopen it."}]}}
{"project": "SPARK", "issue_id": "SPARK-1285", "title": "Back porting streaming doc updates to 0.9", "status": "Resolved", "priority": "Minor", "reporter": "Tathagata Das", "assignee": "Tathagata Das", "labels": [], "created": "2014-03-19T17:59:20.000+0000", "updated": "2014-03-20T12:40:23.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Back porting streaming doc updates to 0.9"}]}}
{"project": "SPARK", "issue_id": "SPARK-1286", "title": "Make usage of spark-env.sh idempotent", "status": "Resolved", "priority": "Minor", "reporter": "Aaron Davidson", "assignee": "Aaron Davidson", "labels": [], "created": "2014-03-19T18:28:03.000+0000", "updated": "2014-03-28T14:38:20.000+0000", "description": "Various spark scripts load spark-env.sh. This can cause growth of any variables that may be appended to (SPARK_CLASSPATH, SPARK_REPL_OPTS) and it makes the precedence order for options specified in spark-env.sh less clear.\n\nOne use-case for the latter is that we want to set options from the command-line of spark-shell, but these options will be overridden by subsequent loading of spark-env.sh. If we were to load the spark-env.sh first and then set our command-line options, we could guarantee correct precedence order.", "comments": ["Relevant: https://github.com/apache/incubator-spark/pull/326"], "derived": {"summary": "Various spark scripts load spark-env. sh.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Make usage of spark-env.sh idempotent - Various spark scripts load spark-env. sh."}, {"q": "What updates or decisions were made in the discussion?", "a": "Relevant: https://github.com/apache/incubator-spark/pull/326"}]}}
{"project": "SPARK", "issue_id": "SPARK-1287", "title": "yarn alpha and stable Client calculateAMMemory routines are different", "status": "Resolved", "priority": "Major", "reporter": "Thomas Graves", "assignee": null, "labels": [], "created": "2014-03-20T08:51:52.000+0000", "updated": "2014-08-22T21:50:10.000+0000", "description": "The yarn alpha version of calculateAMMemory takes into account the minimum resource capability and also subtracts out YarnAllocationHandler.MEMORY_OVERHEAD.\n\nThe yarn stable version just sets the -Xmx to whatever is passed in by the user.\n\nThe 2 of these should be the same. \n   \nPersonally I also think its weird how spark currently takes whatever user passes in for memory and adds YarnAllocationHandler.MEMORY_OVERHEAD to request to RM. This can be confusing to users.  We should revisit all of this and commonize the stable/alpha code where possible.", "comments": ["duplicate of SPARK-2140"], "derived": {"summary": "The yarn alpha version of calculateAMMemory takes into account the minimum resource capability and also subtracts out YarnAllocationHandler. MEMORY_OVERHEAD.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "yarn alpha and stable Client calculateAMMemory routines are different - The yarn alpha version of calculateAMMemory takes into account the minimum resource capability and also subtracts out YarnAllocationHandler. MEMORY_OVERHEAD."}, {"q": "What updates or decisions were made in the discussion?", "a": "duplicate of SPARK-2140"}]}}
{"project": "SPARK", "issue_id": "SPARK-1288", "title": "yarn stable finishApplicationMaster incomplete", "status": "Resolved", "priority": "Major", "reporter": "Thomas Graves", "assignee": "Thomas Graves", "labels": [], "created": "2014-03-20T09:44:46.000+0000", "updated": "2014-04-17T21:39:22.000+0000", "description": "The yarn stable version of ApplicationMaster.finishApplicationMaster is incomplete.  It doesn't set the diagnostic message or app trcking url.", "comments": ["https://github.com/apache/spark/pull/362"], "derived": {"summary": "The yarn stable version of ApplicationMaster. finishApplicationMaster is incomplete.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "yarn stable finishApplicationMaster incomplete - The yarn stable version of ApplicationMaster. finishApplicationMaster is incomplete."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/362"}]}}
{"project": "SPARK", "issue_id": "SPARK-1289", "title": "improve yarn stable error logging when passing bad arguments", "status": "Resolved", "priority": "Major", "reporter": "Thomas Graves", "assignee": null, "labels": [], "created": "2014-03-20T10:31:36.000+0000", "updated": "2015-03-07T16:35:31.000+0000", "description": "There is a difference in the errors shown to the user between yarn alpha and yarn stable (I was running in yarn-cluster mode).   For instance if I call SparkHdfsLR with a non-existent hdfs file,  yarn alpha shows me the following error in the logs:\n\nCaused by: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://nn1.com:8020/user/testuser/lr_data_foo.txt\n        at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:231)\n        at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:251)\n\nI do the same things on yarn stable and all I see is:\n14/03/20 17:26:04 INFO yarn.ApplicationMaster: finishApplicationMaster with FAILED\n\n\nit doesn't give the user why it failed.", "comments": ["Obsolete because it concerns yarn-alpha"], "derived": {"summary": "There is a difference in the errors shown to the user between yarn alpha and yarn stable (I was running in yarn-cluster mode). For instance if I call SparkHdfsLR with a non-existent hdfs file,  yarn alpha shows me the following error in the logs:\n\nCaused by: org.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "improve yarn stable error logging when passing bad arguments - There is a difference in the errors shown to the user between yarn alpha and yarn stable (I was running in yarn-cluster mode). For instance if I call SparkHdfsLR with a non-existent hdfs file,  yarn alpha shows me the following error in the logs:\n\nCaused by: org."}, {"q": "What updates or decisions were made in the discussion?", "a": "Obsolete because it concerns yarn-alpha"}]}}
{"project": "SPARK", "issue_id": "SPARK-1290", "title": "PythonAccumulatorParam throws uncaught exception", "status": "Closed", "priority": "Major", "reporter": "Kay Ousterhout", "assignee": "Kay Ousterhout", "labels": [], "created": "2014-03-20T13:59:55.000+0000", "updated": "2014-03-20T14:24:24.000+0000", "description": "As reported here: http://apache-spark-user-list.1001560.n3.nabble.com/Fwd-pyspark-crash-on-mesos-td2256.html the PythonAccumulatorParam can throw a SparkException that doesn't get caught, leading to early exit of the DAGScheduler's handleTaskCompletion function.  While the DAGScheduler gets restarted, the offending task is never cleaned up properly and the corresponding job will hang.  We should catch this exception and fail the task as a result.\n\nThis is related to SPARK-1235 but should be fixed separately, since we should be doing our best to never throw exceptions inside of the DAGScheduler.", "comments": ["After further thought I've decided to close this -- I don't think we can gracefully handle this problem, because if we've lost connection with the Python master (which is when the exception gets thrown), we're not going to make any progress in the future."], "derived": {"summary": "As reported here: http://apache-spark-user-list. 1001560.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "PythonAccumulatorParam throws uncaught exception - As reported here: http://apache-spark-user-list. 1001560."}, {"q": "What updates or decisions were made in the discussion?", "a": "After further thought I've decided to close this -- I don't think we can gracefully handle this problem, because if we've lost connection with the Python master (which is when the exception gets thrown), we're not going to make any progress in the future."}]}}
{"project": "SPARK", "issue_id": "SPARK-1291", "title": "Link the spark UI to RM ui in yarn-client mode", "status": "Resolved", "priority": "Major", "reporter": "Thomas Graves", "assignee": "Guoqiang Li", "labels": [], "created": "2014-03-21T09:12:59.000+0000", "updated": "2014-07-15T18:53:32.000+0000", "description": "Currently when you run spark on yarn in the yarn-client mode the spark UI is not linked up to the Yarn Resource manager UI so its harder for a user of YARN to find the UI.  Note that in yarn-standalone/yarn-cluster mode it is properly linked up.\n\nIdeally the yarn-client UI should also be hooked up to the Yarn RM proxy for security.\n\nThe challenge with the yarn-client mode is that the UI is started before the application master and it doesn't know what the yarn proxy link is when the UI started. ", "comments": ["https://github.com/apache/spark/pull/1002 links the History UI but we still need it while its running.", "I was trying my hand at providing UI while the app is running. I have a working implementation except the fact the since the UI is started before AM, the UI does not know about APPLICATION_WEB_PROXY_BASE and thus relative paths in UI do not work. :(\n\nAny suggestions?", "PR: https://github.com/apache/spark/pull/1112", "[~gq] I was also able to get this working but your PR is much better & cleaner. I will try to test and report back.\n\nCheers!", "[~rahulsinghal.iitd] Thank you, looking forward to your feedback"], "derived": {"summary": "Currently when you run spark on yarn in the yarn-client mode the spark UI is not linked up to the Yarn Resource manager UI so its harder for a user of YARN to find the UI. Note that in yarn-standalone/yarn-cluster mode it is properly linked up.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Link the spark UI to RM ui in yarn-client mode - Currently when you run spark on yarn in the yarn-client mode the spark UI is not linked up to the Yarn Resource manager UI so its harder for a user of YARN to find the UI. Note that in yarn-standalone/yarn-cluster mode it is properly linked up."}, {"q": "What updates or decisions were made in the discussion?", "a": "[~rahulsinghal.iitd] Thank you, looking forward to your feedback"}]}}
{"project": "SPARK", "issue_id": "SPARK-1292", "title": "In-Memory Columnar Representation for Catalyst", "status": "Resolved", "priority": "Major", "reporter": "Michael Armbrust", "assignee": "liancheng", "labels": [], "created": "2014-03-21T13:17:10.000+0000", "updated": "2014-03-23T12:10:22.000+0000", "description": "This ticket is only to track the first implementation of this.  Support for compressions and other features will be done separately.", "comments": ["Corresponding PR here: https://github.com/apache/spark/pull/205"], "derived": {"summary": "This ticket is only to track the first implementation of this. Support for compressions and other features will be done separately.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "In-Memory Columnar Representation for Catalyst - This ticket is only to track the first implementation of this. Support for compressions and other features will be done separately."}, {"q": "What updates or decisions were made in the discussion?", "a": "Corresponding PR here: https://github.com/apache/spark/pull/205"}]}}
{"project": "SPARK", "issue_id": "SPARK-1293", "title": "Support for reading/writing complex types in Parquet", "status": "Resolved", "priority": "Major", "reporter": "Michael Armbrust", "assignee": "Andre Schumacher", "labels": [], "created": "2014-03-21T13:18:46.000+0000", "updated": "2014-06-20T06:48:17.000+0000", "description": "Complex types include: Arrays, Maps, and Nested rows (structs).", "comments": ["There is a preliminary pull request now: https://github.com/apache/spark/pull/360"], "derived": {"summary": "Complex types include: Arrays, Maps, and Nested rows (structs).", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support for reading/writing complex types in Parquet - Complex types include: Arrays, Maps, and Nested rows (structs)."}, {"q": "What updates or decisions were made in the discussion?", "a": "There is a preliminary pull request now: https://github.com/apache/spark/pull/360"}]}}
{"project": "SPARK", "issue_id": "SPARK-1294", "title": "Case insensitive resolution in HiveContext breaks the ability to access fields with upper case letters", "status": "Resolved", "priority": "Major", "reporter": "Michael Armbrust", "assignee": "Michael Armbrust", "labels": [], "created": "2014-03-21T16:55:50.000+0000", "updated": "2014-03-24T19:27:04.000+0000", "description": "Here's a test case:\n\n{code}\ncase class Data(a: Int, B: Int, n: Nested)\ncase class Nested(a: Int, B: Int)\n\n  test(\"case insensitivity with scala reflection\") {\n    // Test resolution with Scala Reflection\n    TestHive.sparkContext.parallelize(Data(1, 2, Nested(1,2)) :: Nil)\n      .registerAsTable(\"caseSensitivityTest\")\n\n    sql(\"SELECT a, b, A, B, n.a, n.b, n.A, n.B FROM caseSensitivityTest\")\n  }\n{code}", "comments": [], "derived": {"summary": "Here's a test case:\n\n{code}\ncase class Data(a: Int, B: Int, n: Nested)\ncase class Nested(a: Int, B: Int)\n\n  test(\"case insensitivity with scala reflection\") {\n    // Test resolution with Scala Reflection\n    TestHive. sparkContext.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Case insensitive resolution in HiveContext breaks the ability to access fields with upper case letters - Here's a test case:\n\n{code}\ncase class Data(a: Int, B: Int, n: Nested)\ncase class Nested(a: Int, B: Int)\n\n  test(\"case insensitivity with scala reflection\") {\n    // Test resolution with Scala Reflection\n    TestHive. sparkContext."}]}}
{"project": "SPARK", "issue_id": "SPARK-1295", "title": "Progress values on Spark UI progress bar wander off-screen", "status": "Resolved", "priority": "Minor", "reporter": "Aaron Davidson", "assignee": null, "labels": [], "created": "2014-03-21T17:27:39.000+0000", "updated": "2014-03-21T18:07:44.000+0000", "description": "Current:\n!https://f.cloud.github.com/assets/563652/2489083/8c127e80-b153-11e3-807c-048ebd45104b.png!\n\nExpected:\n!https://f.cloud.github.com/assets/563652/2489084/8c12cf5c-b153-11e3-8747-9d93ff6fceb4.png!", "comments": ["https://github.com/apache/spark/pull/201 has been created to fix this"], "derived": {"summary": "Current:\n!https://f. cloud.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Progress values on Spark UI progress bar wander off-screen - Current:\n!https://f. cloud."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/201 has been created to fix this"}]}}
{"project": "SPARK", "issue_id": "SPARK-1296", "title": "Make RDDs Covariant", "status": "Resolved", "priority": "Major", "reporter": "Michael Armbrust", "assignee": "Michael Armbrust", "labels": [], "created": "2014-03-21T18:37:23.000+0000", "updated": "2014-11-13T06:38:14.000+0000", "description": "First, what is the problem with RDDs not being covariant\n{code}\n// Consider a function that takes a Seq of some trait.\nscala> trait A { val a = 1 }\nscala> def f(as: Seq[A]) = as.map(_.a)\n\n// A list of a concrete version of that trait can be used in this function.\nscala> class B extends A\nscala> f(new B :: Nil)\nres0: Seq[Int] = List(1)\n\n// Now lets try the same thing with RDDs\nscala> def f(as: org.apache.spark.rdd.RDD[A]) = as.map(_.a)\n\nscala> val rdd = sc.parallelize(new B :: Nil)\nrdd: org.apache.spark.rdd.RDD[B] = ParallelCollectionRDD[2] at parallelize at <console>:42\n\n// :(\nscala> f(rdd)\n<console>:45: error: type mismatch;\n found   : org.apache.spark.rdd.RDD[B]\n required: org.apache.spark.rdd.RDD[A]\nNote: B <: A, but class RDD is invariant in type T.\nYou may wish to define T as +T instead. (SLS 4.5)\n              f(rdd)\n{code}\n\nh2. Is it possible to make RDDs covariant?\nProbably?  In terms of the public user interface, they are *mostly* covariant. (Internally we use the type parameter T in a lot of mutable state that breaks the covariance contract, but I think with casting we can 'promise' the compiler that we are behaving).  There are also a lot of complications with other types that we return which are invariant.\n\nh2. What will it take to make RDDs covariant?\nAs I mention above, all of our mutable internal state is going to require casting to avoid using T.  This seems to be okay, it makes our life only slightly harder. This extra work required because we are basically promising the compiler that even if an RDD is implicitly upcast, internally we are keeping all the checkpointed data of the correct type. Since an RDD is immutable, we are okay!\n\nWe also need to modify all the places where we use T in function parameters.  So for example:\n\n{code}\ndef ++[U >: T : ClassTag](other: RDD[U]): RDD[U] = this.union(other).asInstanceOf[RDD[U]]\n{code}\n\nWe are now allowing you to append an RDD of a less specific type, and then returning a less specific new RDD.  This I would argue is a good change. We are strictly improving the power of the RDD interface, while maintaining reasonable type semantics.\n\nh2. So, why wouldn't we do it?\nThere are a lot of places where we interact with invariant types.  We return both Maps and Arrays from a lot of public functions.  Arrays are invariant (but if we returned immutable sequences instead.... we would be good), and Maps are invariant in the Key (once again, immutable sequences of tuples would be great here).\n\nI don't think this is a deal breaker, and we may even be able to get away with it, without changing the returns types of these functions.  For example, I think that this should work, though once again requires make promises to the compiler:\n\n{code}\n  /**\n   * Return an array that contains all of the elements in this RDD.\n   */\n  def collect[U >: T](): Array[U] = {\n    val results = sc.runJob(this, (iter: Iterator[T]) => iter.toArray)\n    Array.concat(results: _*).asInstanceOf[Array[U]]\n  }\n{code}\n\nI started working on this [here|https://github.com/marmbrus/spark/tree/coveriantRDD].  Thoughts / suggestions are welcome!", "comments": ["For all intents and purposes, I think that making RDDs covariant would be strictly an improvement.\n\nIt would mean improvements like greater code reuse, e.g., by enabling more generic methods, e.g., operating on an RDD\\[T\\], where T <: Numeric, say.\n\n(Commenting on covariant RDDs + Scala/Pickling as per request)\n\nh3. How it affects the integration of Scala/Pickling\n\nMaking RDDs covariant however, would make it harder for the pickling framework to guarantee static serializers in most cases, without considering other non-trivial refactorings to Scala/Pickling and Spark. \n\nh4. Background\n\nA key principle behind Scala/Pickling is that, when the most type information about some object is available, the compiler can do most of the serialization work statically, resulting in an appreciable performance gain. \n\nThis is inline with most distributed frameworks that we've looked at  most attempt to serialize only very specific types, e.g., (arrays of) sealed types, final types, primitives, etc. \n\nSpark on the other hand does a better job of providing a higher level of abstraction to users. The result of this though, is that it's often trickier for frameworks like Scala/Pickling to always statically know what a user wants to serialize  creating more situations where we might have to fall back on runtime serializer generation  la Kryo et al. Making RDDs covariant would of course increase the number of those scenarios.\n\nh4. One idea to make covariant RDDs and Scala/Pickling play happily together\n\nI'm sure there are other possibilities, but one idea that just came up is the following.\n\nIn general, unrelated to Spark, one way to guarantee a fully-static pickler is by giving methods the following signature:\n\n{code}\n    def m[T](x: T)(implicit p: Pickler[T]) = ...\n{code}\n\nHowever, in general, a Pickler\\[T\\] cannot be used to pickle objects of type S such that S <: T. If RDDs were covariant, this means a Pickler\\[RDD\\[A\\]\\] would not be sufficient to pickle an RDD\\[B\\] even if B <: A.\n\nTo solve this problem, we could do the following:\n* rather than generating a pickler based on the static type of an RDD (RDD\\[A\\]) at the time of (or right before) serialization, one could generate a pickler when an RDD is created and its precise element type is known (RDD\\[B\\]), and store that pickler within the RDD.\n* at pickling time, we would obtain the pickler for an RDD of static type RDD\\[A\\] from what it has stored dynamically; thus, if the RDD has dynamic type RDD\\[B\\] we would get a Pickler\\[RDD\\[B\\]\\].\n\nDoing it this way, however, would require us to request an implicit Pickler whenever an RDD is created, which might be a pretty big interface change. I don't have enough intuition/experience yet with the Spark project to know how drastically you're open to change your interfaces, so it's impossible for me to gauge how much an inconvenience this might be. \n", "Heather, I'm curious, can you generate a Pickler given a TypeTag or ClassTag? We currently take an implicit ClassTag in the RDD constructor and hold onto it throughout for use in creating arrays of that type.", "Rephrasing Matei's question, I believe he's asking: \n\n{quote}I understand that youd like to potentially change the signature of methods used to create RDDs to take an implicit pickler in addition. However, because such creation methods, including the RDD constructors, already take an implicit ClassTag. If such a ClassTag\\[T\\] is not enough to create a Pickler\\[T\\], I am wondering whether it would be enough to change that ClassTag\\[T\\] to a TypeTag\\[T\\], so that more type information is made available. Would this change be enough to support what you suggested in the ticket?{quote}\n\nSo his suggestion boils down to:\n- *not* adding implicit picklers, but instead\n- changing the implicit ClassTag to an implicit TypeTag.\n\n\nThe problem is that ClassTags/TypeTags dont help when generating picklers statically. They can only be used to create runtime picklers. The reason is that the implicit macro that generates picklers only kicks in when implicit search tries to find an implicit SPickler\\[T\\] or an implicit DPickler\\[T\\] -- ClassTags/TypeTags dont help with that.\n\nHowever, the fact that ClassTags are already obtained for an RDDs element type is still interesting, because it means that within the corresponding constructor we can trigger the static generation macro, e.g., using implicitly\\[DPickler\\[T\\]\\]. *The obtained pickler could then be stored in a private field of the RDD.*\n\nIf RDDs would be made covariant, the fields type would need to be DPickler\\[T @uncheckedVariance\\] to disable variance checking. This is safe, since T is the actual type of elements stored in the RDD, and the only thing that can happen is that some elements are of a subtype of T. However, a DPickler\\[T\\] can pickle elements of type T or subtypes of T.\n\nThis is roughly how it could look like (not using real RDDs/DPicklers):\n\n{code}\n    scala> trait DPickler[T] {\n         |   def pickle(x: T): Unit = { println(\"pickling \" + x + \" with class \" + x.getClass) }\n         | }\n    defined trait DPickler\n\n    scala> class RDD[+T : ClassTag](xs: T*) {\n         |   val elems: List[T] = xs.toList\n         |   val p: DPickler[T @uncheckedVariance] = new DPickler[T] {}\n         | }\n    defined class RDD\n\n    scala> val rddi = new RDD[Int](1,2,3)\n    rddi: RDD[Int] = RDD@3eaf6fe7\n\n    scala> val rdda: RDD[Any] = rddi\n    rdda: RDD[Any] = RDD@3eaf6fe7\n\n    scala> val thePickler = rdda.p\n    thePickler: DPickler[Any] = RDD$$anon$1@b2c3415\n\n    scala> rdda.elems.foreach(x => thePickler.pickle(x))\n    pickling 1 with class class java.lang.Integer\n    pickling 2 with class class java.lang.Integer\n    pickling 3 with class class java.lang.Integer\n{code}\n", "Why is this a WONTFIX? Making RDDs covariant seems like a good idea.", "Because it's not possible to do without breaking compatibility."], "derived": {"summary": "First, what is the problem with RDDs not being covariant\n{code}\n// Consider a function that takes a Seq of some trait. scala> trait A { val a = 1 }\nscala> def f(as: Seq[A]) = as.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Make RDDs Covariant - First, what is the problem with RDDs not being covariant\n{code}\n// Consider a function that takes a Seq of some trait. scala> trait A { val a = 1 }\nscala> def f(as: Seq[A]) = as."}, {"q": "What updates or decisions were made in the discussion?", "a": "Because it's not possible to do without breaking compatibility."}]}}
{"project": "SPARK", "issue_id": "SPARK-1297", "title": "Upgrade HBase dependency to 0.98.0", "status": "Resolved", "priority": "Minor", "reporter": "Zhihong Yu", "assignee": "Ted Yu", "labels": [], "created": "2014-03-22T10:59:55.000+0000", "updated": "2014-11-11T13:16:59.000+0000", "description": "HBase 0.94.6 was released 11 months ago.\n\nUpgrade HBase dependency to 0.98.0", "comments": ["The attached patch passes compilation:\n{code}\n[INFO] Reactor Summary:\n[INFO]\n[INFO] Spark Project Parent POM .......................... SUCCESS [2.995s]\n[INFO] Spark Project Core ................................ SUCCESS [2:49.526s]\n[INFO] Spark Project Bagel ............................... SUCCESS [17.327s]\n[INFO] Spark Project GraphX .............................. SUCCESS [50.694s]\n[INFO] Spark Project ML Library .......................... SUCCESS [44.470s]\n[INFO] Spark Project Streaming ........................... SUCCESS [1:01.579s]\n[INFO] Spark Project Tools ............................... SUCCESS [9.838s]\n[INFO] Spark Project Catalyst ............................ SUCCESS [53.016s]\n[INFO] Spark Project SQL ................................. SUCCESS [45.189s]\n[INFO] Spark Project Hive ................................ SUCCESS [56.911s]\n[INFO] Spark Project REPL ................................ SUCCESS [38.044s]\n[INFO] Spark Project Assembly ............................ SUCCESS [23.543s]\n[INFO] Spark Project External Twitter .................... SUCCESS [14.849s]\n[INFO] Spark Project External Kafka ...................... SUCCESS [17.267s]\n[INFO] Spark Project External Flume ...................... SUCCESS [19.770s]\n[INFO] Spark Project External ZeroMQ ..................... SUCCESS [14.526s]\n[INFO] Spark Project External MQTT ....................... SUCCESS [12.361s]\n[INFO] Spark Project Examples ............................ SUCCESS [49.879s]\n{code}", "Did we ever do this?", "Thanks for checking, Reynold.\n\nUpgrading dependency to 0.98 requires pulling in more hbase modules.\nI somehow cannot access the old attachment.\n", "(CC [~ted.m]) I believe this will have some implications regarding Hadoop compatibility, but, I'm not an expert on this. I believe 0.96 has provisions for both Hadoop 1 and 2 compatibility; does 0.98?\n\nRight now HBase is only used in examples, so it would not matter greatly if that part only worked with Hadoop 2. I know there is some work to create a more proper HBase integration module, which might make this matter more.\n\nI also recall from talking to Ted that this is going to require some work with profiles to get right. Not only are 0.96+'s modules different, they differ from Hadoop 1 to Hadoop 2 as it has \"hadoop1-compat\" and \"hadoop2-compat\" artifacts. We would actually have to have a \"hadoop1\" profile in the entire build to make this work I think? or else I've missed a clever way to finesse this.\n\nOr, don't bother and say HBase integration is Hadoop 2+ only.\n\nProbably doable but needs some thought!", "We can add two profiles for 0.98: hbase-hadoop1 and hbase-hadoop2\nHere is snippet for hbase-hadoop2 dependencies where hbase-hadoop2.version is 0.98.4-hadoop2\n{code}\n          <dependency>\n        <groupId>org.apache.hbase</groupId>\n            <artifactId>hbase-hadoop-compat</artifactId>\n            <version>${hbase-hadoop2.version}</version>\n          </dependency>\n          <dependency>\n            <groupId>org.apache.hbase</groupId>\n            <artifactId>hbase-hadoop-compat</artifactId>\n            <version>${hbase-hadoop2.version}</version>\n            <type>test-jar</type>\n            <scope>test</scope>\n          </dependency>\n{code}", "Yes, the problem though is that at least one of the profiles has to be selected for the project to build, then, and I don't think we can ask everyone who builds it to always choose one. Now, let's say hbase-hadoop1 were marked as active by default. Then I think a caller could either specify nothing and compile vs Hadoop 1, or, specify \"-Phbase-hadoop2,-hbase-hadoop1\" to disable the one and enable the other.\n\nThat's closer to workable. Although I think then every time someone chooses a Hadoop 2 profile they have to know to add this invocation to make HBase work. Still kind of lame.\n\nOf course, the HBase Hadoop 2 deps can be copied into the various hadoop22, hadoop23, etc. profiles that already exist. The problem is there is no existing \"hadoop1\" profile. If there were, the Hadoop 1 deps could go there and the problem is solved.\n\nThis is why I wonder if this is best solved by introducing a \"hadoop1\" profile that is active by default, to solve this issue for all components. I *think* it means that people then have to write things like \"-Phadoop23,-hadoop1\", which is still lame.\n\nAm I missing a much easier solution, given this problem?  It's tricky!", "bq. the HBase Hadoop 2 deps can be copied into the various hadoop22, hadoop23, etc. profiles\n\nI thought about the above as well.\nFrom recent user inquiry on user@spark, build issues involved hadoop-2 profile(s). I would vote for the above action based on the fact that  hbase users are encouraged to deploy 0.98.x on hadoop-2\n\nOpinions from others are welcome.", "Tentative patch adds hbase-hadoop2 profile.", "This doesn't work with Hadoop 1 though. It also requires turning on an HBase profile for every build. See my comments above; I think this can be made friendlier with more work in the profiles. I think it requires a \"hadoop1\" profile to really solve this kind of problem for every components, not just HBase.", "Patch v4 adds two profiles to examples/pom.xml :\n\nhbase-hadoop1 (default)\nhbase-hadoop2\n\nI verified that compilation passes with either profile active.", "I think you may want to open a PR rather than post patches. Code reviews happen on github.com\n\nI see what you did there by triggering one or the other profile with the hbase.profile property. Yeah, that may be the least disruptive way to play this. But don't the profiles need to select the hadoop-compat module appropriate for Hadoop 1 vs Hadoop 2?", "HBase client doesn't need to specify dependency on hbase-hadoop1-compat or hbase-hadoop2-compat\n\nI can open a PR once there is positive feedback on the approach - I came from a project where reviews mostly happen on JIRA :-)\n\nCan someone assign this issue to me ?", "https://github.com/apache/spark/pull/1893", "User 'tedyu' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1893", "w.r.t. build, by default, hbase-hadoop1 would be used.\nIf user specifies any of the hadoop-2 profiles, hbase-hadoop2 should be specified as well.", "Yes I think you'd need to reflect that in changes to the build instructions. They are under docs/", "This is examples/pom.xml for spark-1.0.2", "Patch v5 is the aggregate of the 4 commits in the pull request.", "Here is sample command for building against 0.98 hbase:\n\nmvn -Dhbase.profile=hadoop2 -Phadoop-2.4,yarn -Dhadoop.version=2.4.1 -DskipTests clean package", "Please note: spark-1297-v5.txt is a level 0 patch.", "Patch v6 uses 0.98.5 hbase release.", "Patch v7 uses 0.98.7 hbase release", "Ted are you updating a pull request? patches aren't used in this project.", "Create a new pull request:\nhttps://github.com/apache/spark/pull/3115", "User 'tedyu' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/3115"], "derived": {"summary": "HBase 0. 94.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Upgrade HBase dependency to 0.98.0 - HBase 0. 94."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'tedyu' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/3115"}]}}
{"project": "SPARK", "issue_id": "SPARK-1298", "title": "Remove duplicate partition id checking", "status": "Resolved", "priority": "Minor", "reporter": "Nan Zhu", "assignee": "Nan Zhu", "labels": [], "created": "2014-03-23T11:36:53.000+0000", "updated": "2014-04-26T01:08:11.000+0000", "description": "In the current implementation, we check whether partitionIDs make sense in SparkContext.runJob()\n\nhttps://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkContext.scala#L896\n\nHowever, immediately following, in DAGScheduler (calling path SparkContext.runJob -> DAGScheduler.runJob -> DAGScheduler.submitJob), we check it again, (just missing a < 0 condition), \n\nhttps://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L432\n\nI propose to remove the SparkContext one and check it in DAGScheduler (which makes more sense, from my view)", "comments": ["addressed in https://github.com/apache/spark/pull/186"], "derived": {"summary": "In the current implementation, we check whether partitionIDs make sense in SparkContext. runJob()\n\nhttps://github.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Remove duplicate partition id checking - In the current implementation, we check whether partitionIDs make sense in SparkContext. runJob()\n\nhttps://github."}, {"q": "What updates or decisions were made in the discussion?", "a": "addressed in https://github.com/apache/spark/pull/186"}]}}
{"project": "SPARK", "issue_id": "SPARK-1299", "title": "making comments of RDD.doCheckpoint consistent with its usage", "status": "Resolved", "priority": "Trivial", "reporter": "Nan Zhu", "assignee": "Nan Zhu", "labels": [], "created": "2014-03-23T12:21:43.000+0000", "updated": "2014-04-26T01:07:46.000+0000", "description": "another trivial thing I found occasionally, the comments of function is saying that\n\n/**\n   * Performs the checkpointing of this RDD by saving this. It is called by the DAGScheduler\n   * after a job using this RDD has completed (therefore the RDD has been materialized and\n   * potentially stored in memory). doCheckpoint() is called recursively on the parent RDDs.\n   */\n\nactually this function is called in SparkContext.runJob\n\nwe can either change the comments or call it in DAGScheduler, I personally prefer the later one, as this calling seems like a auto-checkpoint , better put it in a non-user-facing component\n\n\n", "comments": ["addressed in https://github.com/apache/spark/pull/186"], "derived": {"summary": "another trivial thing I found occasionally, the comments of function is saying that\n\n/**\n   * Performs the checkpointing of this RDD by saving this. It is called by the DAGScheduler\n   * after a job using this RDD has completed (therefore the RDD has been materialized and\n   * potentially stored in memory).", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "making comments of RDD.doCheckpoint consistent with its usage - another trivial thing I found occasionally, the comments of function is saying that\n\n/**\n   * Performs the checkpointing of this RDD by saving this. It is called by the DAGScheduler\n   * after a job using this RDD has completed (therefore the RDD has been materialized and\n   * potentially stored in memory)."}, {"q": "What updates or decisions were made in the discussion?", "a": "addressed in https://github.com/apache/spark/pull/186"}]}}
{"project": "SPARK", "issue_id": "SPARK-1300", "title": "Clean-up and clarify private vs public fields in MLLib", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-03-23T13:10:11.000+0000", "updated": "2014-04-09T09:24:48.000+0000", "description": "Some of the MLLib implementations have random internal varaibles that are exposed and should be made private.\n\nFor the regression package, there are a few fields (optimizer, updater, gradient) that we should just make private[spark].\n\nFor other internal components we should annotate them as semi-private. I.e. they are deeper API's for more advanced developers.", "comments": [], "derived": {"summary": "Some of the MLLib implementations have random internal varaibles that are exposed and should be made private. For the regression package, there are a few fields (optimizer, updater, gradient) that we should just make private[spark].", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "Clean-up and clarify private vs public fields in MLLib - Some of the MLLib implementations have random internal varaibles that are exposed and should be made private. For the regression package, there are a few fields (optimizer, updater, gradient) that we should just make private[spark]."}]}}
{"project": "SPARK", "issue_id": "SPARK-1301", "title": "Add UI elements to collapse \"Aggregated Metrics by Executor\" pane on stage page", "status": "Resolved", "priority": "Minor", "reporter": "Matei Alexandru Zaharia", "assignee": "Alex Bozarth", "labels": ["Starter"], "created": "2014-03-23T18:49:11.000+0000", "updated": "2019-04-17T01:58:48.000+0000", "description": "This table is useful but it takes up a lot of space on larger clusters, hiding the more commonly accessed \"stage\" page. We could also move the table below if collapsing it is difficult.", "comments": ["Hi, \nI have modified the code to put the \"Aggregated Metrics by Executor\" table below the \"Tasks\" table.\nHere's the screenshot to it http://cl.ly/V0mw I can send a pull request if this is Ok, or should we need to implement Collapsing panel for the tables?\n\nThanks\nRegards.", "Is this still relevant now that this info is on a separate Executors tab or do I misunderstand? ", "User 'shankervalipireddy' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/5021", "User 'ryan-williams' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/5121", "[~srowen] I read this as referring to the \"Aggregated Metrics by Executor\" pane on the stage page, which is not in the Executors tab; it causes the more commonly accessed per-task table on the stage page to be many screen-heights below the fold when the number of executors is large.\n\nA similar argument could be made about the \"Distribution Across Executors\" table on the RDD page.", "User 'ajbozarth' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/13037", "Submitted a small fix similar to Ryan's above but in line with how it's done on the Jobs and Stages page", "Issue resolved by pull request 13037\n[https://github.com/apache/spark/pull/13037]"], "derived": {"summary": "This table is useful but it takes up a lot of space on larger clusters, hiding the more commonly accessed \"stage\" page. We could also move the table below if collapsing it is difficult.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add UI elements to collapse \"Aggregated Metrics by Executor\" pane on stage page - This table is useful but it takes up a lot of space on larger clusters, hiding the more commonly accessed \"stage\" page. We could also move the table below if collapsing it is difficult."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 13037\n[https://github.com/apache/spark/pull/13037]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1302", "title": "httpd doesn't start in spark-ec2 (cc2.8xlarge)", "status": "Resolved", "priority": "Minor", "reporter": "Shivaram Venkataraman", "assignee": "Shivaram Venkataraman", "labels": [], "created": "2014-03-23T23:56:44.000+0000", "updated": "2015-06-04T11:17:57.000+0000", "description": "In a cc2.8xlarge EC2 cluster launched from master branch, httpd won't start (i.e ganglia doesn't work). The reason seems to be httpd.conf is wrong (newer httpd version ?).  The config file contains a bunch of non-existent modules and this happens because we overwrite the default conf with our config file from spark-ec2. We could explore using patch or something like that to just apply the diff we need ", "comments": ["I seem to be hitting this issue with all hvm amis, but not pvm amis. This means that ganglia does not work out of the box for many of the newer instance types. Has anyone found a way to fix httpd for ganglia?", "There is an open PR by [~fredcons] here to solve this problem: https://github.com/mesos/spark-ec2/pull/76", "The PR fixed the issue for me, also the PR doesn't affect a lot of files, should be safe to merge, I hope somebody from the Spark team will find time to merge it into master, despite the fact that it has \"Minor\" priority.", "I'm getting this httpd error on t2.medium instance. Maybe you can describe recommended instance types in the documentation?\n\n{code}\nStarting httpd: httpd: Syntax error on line 153 of /etc/httpd/conf/httpd.conf: Cannot load modules/mod_authn_alias.so into server: /etc/httpd/modules/mod_authn_alias.so: cannot open shared object file: No such file or directory\n{code}", "It looks like this was basically resolved recently given the discussion at https://github.com/mesos/spark-ec2/pull/76  I wonder if [~soid] is able to access the latest change there? or if this is evidence that something wasn't quite right about the change?", "[~soid] Could you let us which Spark version you were using to launch the cluster. The fix for spark-ec2 was merged into `branch-1.3` (and the master branch)", "Indeed, I used 1.2.0. Sorry for the false alarm. ", "I am having this problem again in Spark 1.3.1. This patch helped https://github.com/grzegorz-dubicki/spark-ec2/commit/56332c0879bd8d15af123b64cc0923dc80f7fdd1. Can anyone confirm this?"], "derived": {"summary": "In a cc2. 8xlarge EC2 cluster launched from master branch, httpd won't start (i.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "httpd doesn't start in spark-ec2 (cc2.8xlarge) - In a cc2. 8xlarge EC2 cluster launched from master branch, httpd won't start (i."}, {"q": "What updates or decisions were made in the discussion?", "a": "I am having this problem again in Spark 1.3.1. This patch helped https://github.com/grzegorz-dubicki/spark-ec2/commit/56332c0879bd8d15af123b64cc0923dc80f7fdd1. Can anyone confirm this?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1303", "title": "Added discretization capability to MLlib.", "status": "Resolved", "priority": "Major", "reporter": "LIDIAgroup", "assignee": null, "labels": [], "created": "2014-03-24T09:43:52.000+0000", "updated": "2015-04-12T10:39:04.000+0000", "description": "Some time ago, we have commented with Ameet Talwalkar the possibilty of including both Feature Selection and Discretization algorithms to MLlib.\n\nIn this patch we've implemented Entropy Minimization Discretization following the algorithm described in the paper \"Multi-interval discretization of continuous-valued attributes for classification learning\" by Fayyad and Irani (1993). This is one of the most used Discretizers and is already included in most libraries like Weka, etc. This can be used as base for FS algorims and the NaiveBayes already included in MLlib.", "comments": ["Discretization: see also https://issues.apache.org/jira/browse/SPARK-1216\nCan you link the pull request here as well please?\n\nFeature selection: see also https://issues.apache.org/jira/browse/SPARK-1473", "User 'sramirez' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/5169", "Sounds like this should start outside MLlib: https://github.com/apache/spark/pull/216"], "derived": {"summary": "Some time ago, we have commented with Ameet Talwalkar the possibilty of including both Feature Selection and Discretization algorithms to MLlib. In this patch we've implemented Entropy Minimization Discretization following the algorithm described in the paper \"Multi-interval discretization of continuous-valued attributes for classification learning\" by Fayyad and Irani (1993).", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Added discretization capability to MLlib. - Some time ago, we have commented with Ameet Talwalkar the possibilty of including both Feature Selection and Discretization algorithms to MLlib. In this patch we've implemented Entropy Minimization Discretization following the algorithm described in the paper \"Multi-interval discretization of continuous-valued attributes for classification learning\" by Fayyad and Irani (1993)."}, {"q": "What updates or decisions were made in the discussion?", "a": "Sounds like this should start outside MLlib: https://github.com/apache/spark/pull/216"}]}}
{"project": "SPARK", "issue_id": "SPARK-1304", "title": "Job fails with spot instances (due to IllegalStateException: Shutdown in progress)", "status": "Resolved", "priority": "Minor", "reporter": "Alex Boisvert", "assignee": null, "labels": [], "created": "2014-03-24T10:30:25.000+0000", "updated": "2015-09-16T18:33:01.000+0000", "description": "We had a job running smoothly with spot instances until one of the spot instances got terminated ... which led to a series of \"IllegalStateException: Shutdown in progress\" and the job failed afterwards.\n\n14/03/24 06:07:52 WARN scheduler.TaskSetManager: Loss was due to java.lang.IllegalStateException\njava.lang.IllegalStateException: Shutdown in progress\n\tat java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:66)\n\tat java.lang.Runtime.addShutdownHook(Runtime.java:211)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1441)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:256)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:187)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:77)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:51)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:156)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:149)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:64)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:232)\n\tat org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:232)\n\tat org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:90)\n\tat org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:89)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:57)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$1.apply(PairRDDFunctions.scala:95)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$1.apply(PairRDDFunctions.scala:94)\n\tat org.apache.spark.rdd.RDD$$anonfun$3.apply(RDD.scala:471)\n\tat org.apache.spark.rdd.RDD$$anonfun$3.apply(RDD.scala:471)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:34)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:232)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:161)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:53)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)\n\tat org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:724)", "comments": ["This ShutdownHook was thrown by HDFS when creating a FileSystem object , and was just reported back to Sparkt and reported by TaskSetManager. And this may caused by HDFS not in normal mode or some fiile blocks read by Spark corrupted , You may check your NameNode log at that specific time to see what happened to HDFS ,and may be some DataNode log containing revelvant file blocks  ", "I think this is kinda to be expected; if services are shut down externally you may see some errors as services lose ability to talk to other services suddenly. Here HDFS was shut down from under Spark it seems.\n\nThere might be a chance of more graceful interleaving of shutdown once Hadoop 2.2+ is required; this is basically SPARK-6014. So I'm at least resolving as a dupe of that.", "Not a duplicate, in the sense that no instance running HDFS was shut down.  This job was running on AWS EMR using S3 as distributed filesystem, not HDFS.", "Just to clarify here that spot instances were only running Spark worker/executor.", "OK, but you are still going through Hadoop FS APIs (see stack trace); really I mean FileSystem API was shut down. \nIs this an error from the worker that was shut down by EC2? Then it is the same as in SPARK-6014; during a non-graceful shutdown, the Spark shutdown processes race with the Hadoop FS shutdown and you get results like this. ", "I see how it's similar to SPARK-6014 but the code paths seem different enough to keep two different issues open, as I suspect the resolution would be different for each.  If you disagree and can explain how a single fix can address both, I'd be happy to close this one as duplicate.", "Actually, I think I'm a little wrong about the cause, but am sort of right about the resolution: here we actually have the FileSystem API continuing to act during shutdown. This is much improved in later versions of Hadoop, like 2.2+, with ShutdownHookManager, so shouldn't happen there. I think better ordering of shutdown hooks might prevent some of the ways this could happen, but not all; that's what SPARK-6014 is about but it also requires Hadoop 2.2+ really.\n\nI actually don't know if there's any change in Spark that can always avoid this in Hadoop 1.x. It has to ask the FileSystem API for an object at many points and can't always be checking for shutdown. It may be 'harmless', just an artifact of the JVM being shut down ungracefully, which is going to cause processes to stop and possibly log an exception.\n\nIt's OK to leave it open for a while just in case. I doubt anything more will happen here but who knows. If there's no better ideas in couple months, close it.", "Yeah, I see what you mean by \"can't always be checking for shutdown\".   Not sure there's a good solution here either.   Thanks for giving it some consideration.  I'm fine closing it as WONTFIX."], "derived": {"summary": "We had a job running smoothly with spot instances until one of the spot instances got terminated. which led to a series of \"IllegalStateException: Shutdown in progress\" and the job failed afterwards.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Job fails with spot instances (due to IllegalStateException: Shutdown in progress) - We had a job running smoothly with spot instances until one of the spot instances got terminated. which led to a series of \"IllegalStateException: Shutdown in progress\" and the job failed afterwards."}, {"q": "What updates or decisions were made in the discussion?", "a": "Yeah, I see what you mean by \"can't always be checking for shutdown\".   Not sure there's a good solution here either.   Thanks for giving it some consideration.  I'm fine closing it as WONTFIX."}]}}
{"project": "SPARK", "issue_id": "SPARK-1305", "title": "Support persisting RDD's directly to Tachyon", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Haoyuan Li", "labels": [], "created": "2014-03-24T10:45:13.000+0000", "updated": "2020-05-17T18:21:33.000+0000", "description": "This is already an ongoing pull request - in a nutshell we want to support Tachyon as a storage level in Spark.", "comments": ["I'm interested in using this feature especially with SchemeRDD to be able cache intermediate  results.\nWhere I can find some code examples how to use it ? \nFrom SparkContext's code I see the following: \n\n  private[spark] def persistRDD(rdd: RDD[_]) {\n    persistentRdds(rdd.id) = rdd\n  }\n\n  /**\n   * Unpersist an RDD from memory and/or disk storage\n   */\n  private[spark] def unpersistRDD(rddId: Int, blocking: Boolean = true) {\n    env.blockManager.master.removeRdd(rddId, blocking)\n    persistentRdds.remove(rddId)\n    listenerBus.post(SparkListenerUnpersistRDD(rddId))\n  }\n\nSo  persist don't put RDD into tachyonStore of blockmanager ? How does this feature work ? \n"], "derived": {"summary": "This is already an ongoing pull request - in a nutshell we want to support Tachyon as a storage level in Spark.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support persisting RDD's directly to Tachyon - This is already an ongoing pull request - in a nutshell we want to support Tachyon as a storage level in Spark."}, {"q": "What updates or decisions were made in the discussion?", "a": "I'm interested in using this feature especially with SchemeRDD to be able cache intermediate  results.\nWhere I can find some code examples how to use it ? \nFrom SparkContext's code I see the following: \n\n  private[spark] def persistRDD(rdd: RDD[_]) {\n    persistentRdds(rdd.id) = rdd\n  }\n\n  /**\n   * Unpersist an RDD from memory and/or disk storage\n   */\n  private[spark] def unpersistRDD(rddId: Int, blocking: Boolean = true) {\n    env.blockManager.master.removeRdd(rddId, blocking)\n    persistentRdds.remove(rddId)\n    listenerBus.post(SparkListenerUnpersistRDD(rddId))\n  }\n\nSo  persist don't put RDD into tachyonStore of blockmanager ? How does this feature work ?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1306", "title": "no instructions provided for sbt assembly with Hadoop 2.2", "status": "Resolved", "priority": "Major", "reporter": "Diana Carroll", "assignee": null, "labels": [], "created": "2014-03-24T11:55:16.000+0000", "updated": "2014-10-13T17:56:52.000+0000", "description": "on the running-on-yarn.html page, in the section \"Building a YARN-Enabled Assembly JAR\", only the instructions for building for \"old\" Hadoop (2.0.5) are provided.  There's a comment that \"The build process now also supports new YARN versions (2.2.x). See below.\"\n\nHowever, the only mention below is a single sentence which says \"See Building Spark with Maven for instructions on how to build Spark using the Maven process.\"  There are no instructions for building with sbt. This is different than in prior versions of the docs, in which a whole paragraph was provided.\n\nI'd like to see the command line to build for Hadoop 2.2 included right at the top of the page. Also remove the bit about how it is \"now\" supported.   Hadoop 2.2 is now the \"norm\", no longer an exception, as I see it. \n\nUnfortunately I'm not sure exactly what the command should be.  I tried this, but got errors:\nSPARK_HADOOP_VERSION=2.2.0 SPARK_YARN=true sbt/sbt assembly", "comments": ["I think this was obviated by subsequent changes to this documentation. SBT is no longer the focus, but, building-spark.md now has more comprehensive documentation on building with YARN, including these recent versions."], "derived": {"summary": "on the running-on-yarn. html page, in the section \"Building a YARN-Enabled Assembly JAR\", only the instructions for building for \"old\" Hadoop (2.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "no instructions provided for sbt assembly with Hadoop 2.2 - on the running-on-yarn. html page, in the section \"Building a YARN-Enabled Assembly JAR\", only the instructions for building for \"old\" Hadoop (2."}, {"q": "What updates or decisions were made in the discussion?", "a": "I think this was obviated by subsequent changes to this documentation. SBT is no longer the focus, but, building-spark.md now has more comprehensive documentation on building with YARN, including these recent versions."}]}}
{"project": "SPARK", "issue_id": "SPARK-1307", "title": "don't use term 'standalone' to refer to a Spark Application", "status": "Resolved", "priority": "Major", "reporter": "Diana Carroll", "assignee": "Sean R. Owen", "labels": [], "created": "2014-03-24T12:13:21.000+0000", "updated": "2014-10-15T04:39:50.000+0000", "description": "In the \"Quick Start Guide\" for Scala, there are three sections entitled \"A Standalone App in Scala\", \"A Standalone App in Java\" and \"A Standalone App in Python.\"  \n\nIn these sections, the word \"standalone\" is meant to refer to a Spark application that is run outside of the Spark Shell. This nomenclature is quite confusing, because the cluster management framework included in Spark is called \"Spark Standalone\"...this overlap of terms has resulted in at least one person (me) think that a \"standalone app\" was somehow related to a \"standalone cluster\"...and that in order to run my app on a Standalone Spark cluster, I had to write follow the instructions to write a Standalone app.\n\nFortunately, the only place I can find this usage of \"standalone\" to refer to an application is on the Quick Start page.   I think those three sections should instead be retitled as\nWriting a Spark Application in Scala\nWriting a Spark Application in Java\nWriting a Spark Application in Python\nand rephrased to remove the use of the term \"standalone\".", "comments": ["User 'srowen' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/2787", "Yeah this is a good idea - there is no reason to overload that name, it's just confusing.", "Issue resolved by pull request 2787\n[https://github.com/apache/spark/pull/2787]"], "derived": {"summary": "In the \"Quick Start Guide\" for Scala, there are three sections entitled \"A Standalone App in Scala\", \"A Standalone App in Java\" and \"A Standalone App in Python. \"  \n\nIn these sections, the word \"standalone\" is meant to refer to a Spark application that is run outside of the Spark Shell.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "don't use term 'standalone' to refer to a Spark Application - In the \"Quick Start Guide\" for Scala, there are three sections entitled \"A Standalone App in Scala\", \"A Standalone App in Java\" and \"A Standalone App in Python. \"  \n\nIn these sections, the word \"standalone\" is meant to refer to a Spark application that is run outside of the Spark Shell."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 2787\n[https://github.com/apache/spark/pull/2787]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1308", "title": "Add getNumPartitions() method to PySpark RDDs", "status": "Resolved", "priority": "Minor", "reporter": "Nicholas Chammas", "assignee": "Syed A. Hashmi", "labels": [], "created": "2014-03-24T14:02:34.000+0000", "updated": "2014-06-09T07:10:28.000+0000", "description": "In Spark, you can do this:\n\n{code}\n// Scala\nval a = sc.parallelize(List(1, 2, 3, 4), 4)\na.partitions.size\n{code}\n\nPlease make this possible in PySpark too.\n\nThe work-around available is quite simple:\n\n{code}\n# Python\na = sc.parallelize([1, 2, 3, 4], 4)\na._jrdd.splits().size()\n{code}", "comments": ["Created [PR #128|https://github.com/apache/spark/pull/218].", "Created pull request https://github.com/apache/spark/pull/995 to address this issue.\n\n[~matei]: Can you please assign this JIRA to me and review the PR?"], "derived": {"summary": "In Spark, you can do this:\n\n{code}\n// Scala\nval a = sc. parallelize(List(1, 2, 3, 4), 4)\na.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add getNumPartitions() method to PySpark RDDs - In Spark, you can do this:\n\n{code}\n// Scala\nval a = sc. parallelize(List(1, 2, 3, 4), 4)\na."}, {"q": "What updates or decisions were made in the discussion?", "a": "Created pull request https://github.com/apache/spark/pull/995 to address this issue.\n\n[~matei]: Can you please assign this JIRA to me and review the PR?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1309", "title": "sbt assemble-deps no longer works", "status": "Resolved", "priority": "Blocker", "reporter": "Shivaram Venkataraman", "assignee": "Aaron Davidson", "labels": [], "created": "2014-03-24T16:49:13.000+0000", "updated": "2014-04-07T00:50:51.000+0000", "description": "After the Catalyst merge the sbt assemble-deps workflow no longer works. Here are the steps to reproduce\n\nsbt/sbt clean\nsbt/sbt assemble-deps\n./bin/spark-shell\nError: Could not find or load main class org.apache.spark.repl.Main\n\nThe error comes from the fact that compute-classpath.sh does not include the class files if the hive assembly jar is found.\nOne fix would be to not build the hive assembly jar when assemble-deps is called.", "comments": ["This is fixed by virtue of SPARK-1314 being merged."], "derived": {"summary": "After the Catalyst merge the sbt assemble-deps workflow no longer works. Here are the steps to reproduce\n\nsbt/sbt clean\nsbt/sbt assemble-deps.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "sbt assemble-deps no longer works - After the Catalyst merge the sbt assemble-deps workflow no longer works. Here are the steps to reproduce\n\nsbt/sbt clean\nsbt/sbt assemble-deps."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is fixed by virtue of SPARK-1314 being merged."}]}}
{"project": "SPARK", "issue_id": "SPARK-1310", "title": "Add support for  cross validation to MLLibb", "status": "Resolved", "priority": "Minor", "reporter": "Holden Karau", "assignee": "Holden Karau", "labels": [], "created": "2014-03-24T17:13:12.000+0000", "updated": "2014-08-18T05:54:45.000+0000", "description": "See MLI-2", "comments": ["can you provide a link to your solution?"], "derived": {"summary": "See MLI-2.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add support for  cross validation to MLLibb - See MLI-2."}, {"q": "What updates or decisions were made in the discussion?", "a": "can you provide a link to your solution?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1311", "title": "Use map side distinct in collect vertex ids from edges graphx", "status": "Resolved", "priority": "Minor", "reporter": "Holden Karau", "assignee": "Ankur Dave", "labels": [], "created": "2014-03-24T17:14:41.000+0000", "updated": "2014-07-19T22:58:34.000+0000", "description": "See GRAPH-1", "comments": ["This was resolved by PR #497, which removed collectVertexIds and instead performed the operation as a side effect of constructing the routing tables: https://github.com/apache/spark/pull/497/files#diff-8ea535724b3f014cfef17284b3e783feR397"], "derived": {"summary": "See GRAPH-1.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Use map side distinct in collect vertex ids from edges graphx - See GRAPH-1."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was resolved by PR #497, which removed collectVertexIds and instead performed the operation as a side effect of constructing the routing tables: https://github.com/apache/spark/pull/497/files#diff-8ea535724b3f014cfef17284b3e783feR397"}]}}
{"project": "SPARK", "issue_id": "SPARK-1312", "title": "Batch should read based on the batch interval provided in the StreamingContext", "status": "Resolved", "priority": "Critical", "reporter": "Sanjay Awatramani", "assignee": "Tathagata Das", "labels": ["sliding", "streaming", "window"], "created": "2014-03-24T22:06:15.000+0000", "updated": "2015-06-01T21:16:58.000+0000", "description": "This problem primarily affects sliding window operations in spark streaming.\n\nConsider the following scenario:\n- a DStream is created from any source. (I've checked with file and socket)\n- No actions are applied to this DStream\n- Sliding Window operation is applied to this DStream and an action is applied to the sliding window.\nIn this case, Spark will not even read the input stream in the batch in which the sliding interval isn't a multiple of batch interval. Put another way, it won't read the input when it doesn't have to apply the window function. This is happening because all transformations in Spark are lazy.\n\nHow to fix this or workaround it (see line#3):\nJavaStreamingContext stcObj = new JavaStreamingContext(confObj, new Duration(1 * 60 * 1000));\nJavaDStream<String> inputStream = stcObj.textFileStream(\"/Input\");\ninputStream.print(); // This is the workaround\nJavaDStream<String> objWindow = inputStream.window(new Duration(windowLen*60*1000), new Duration(slideInt*60*1000));\nobjWindow.dstream().saveAsTextFiles(\"/Output\", \"\");\n\nThe \"Window operations\" example on the streaming guide implies that Spark will read the stream in every batch, which is not happening because of the lazy transformations.\nWherever sliding window would be used, in most of the cases, no actions will be taken on the pre-window batch, hence my gut feeling was that Streaming would read every batch if any actions are being taken in the windowed stream.\nAs per Tathagata,\n\"Ideally every batch should read based on the batch interval provided in the StreamingContext.\"\n\nRefer the original thread on http://apache-spark-user-list.1001560.n3.nabble.com/Sliding-Window-operations-do-not-work-as-documented-tp2999.html for more details, including Tathagata's conclusion.", "comments": ["This has probably been solved in Spark 1.2.0 with changes in how blocks are assigned to batches.", "I will try to add a unit test to make sure this bug has been solved. ", "I have verified this in current Spark and I am closing this JIRa. "], "derived": {"summary": "This problem primarily affects sliding window operations in spark streaming. Consider the following scenario:\n- a DStream is created from any source.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Batch should read based on the batch interval provided in the StreamingContext - This problem primarily affects sliding window operations in spark streaming. Consider the following scenario:\n- a DStream is created from any source."}, {"q": "What updates or decisions were made in the discussion?", "a": "I have verified this in current Spark and I am closing this JIRa."}]}}
{"project": "SPARK", "issue_id": "SPARK-1313", "title": "Shark- JDBC driver ", "status": "Resolved", "priority": "Minor", "reporter": "Abhishek Tripathi", "assignee": null, "labels": ["Hive,JDBC", "Shark,"], "created": "2014-03-24T23:44:36.000+0000", "updated": "2014-09-29T12:51:49.000+0000", "description": "Hi, \nI'm trying to get JDBC/any driver that can connect to Shark using Java  and execute the Shark/hive query. Can you plz advise if such connector/driver is available ?\n\nThanks\nAbhishek", "comments": ["Please see: https://cwiki.apache.org/confluence/display/Hive/HiveClient#HiveClient-JDBC\n\nThe Hive JDBC client should work exactly the same when connecting to SharkServer", "This looks like it was a question more than anything, and was answered."], "derived": {"summary": "Hi, \nI'm trying to get JDBC/any driver that can connect to Shark using Java  and execute the Shark/hive query. Can you plz advise if such connector/driver is available ?\n\nThanks\nAbhishek.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Shark- JDBC driver  - Hi, \nI'm trying to get JDBC/any driver that can connect to Shark using Java  and execute the Shark/hive query. Can you plz advise if such connector/driver is available ?\n\nThanks\nAbhishek."}, {"q": "What updates or decisions were made in the discussion?", "a": "This looks like it was a question more than anything, and was answered."}]}}
{"project": "SPARK", "issue_id": "SPARK-1314", "title": "sbt assembly builds hive jar", "status": "Resolved", "priority": "Major", "reporter": "Aaron Davidson", "assignee": "Aaron Davidson", "labels": [], "created": "2014-03-25T01:19:42.000+0000", "updated": "2014-04-07T00:50:38.000+0000", "description": "Running \"sbt/sbt assembly\" will create a spark-hive-assembly jar which causes all subsequent operations using spark-class to use the Hive jar instead of the Spark one. For people who use this workflow, this negates most of the use of extracting out Hive to its own assembly.\n\nOne way to get around this would be to create an environment variable which controls whether or not we include the Hive dependencies into the normal spark assembly jar. This would be in line with the behavior of similarly \"optional\" components such as Yarn and Ganglia.", "comments": [], "derived": {"summary": "Running \"sbt/sbt assembly\" will create a spark-hive-assembly jar which causes all subsequent operations using spark-class to use the Hive jar instead of the Spark one. For people who use this workflow, this negates most of the use of extracting out Hive to its own assembly.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "sbt assembly builds hive jar - Running \"sbt/sbt assembly\" will create a spark-hive-assembly jar which causes all subsequent operations using spark-class to use the Hive jar instead of the Spark one. For people who use this workflow, this negates most of the use of extracting out Hive to its own assembly."}]}}
{"project": "SPARK", "issue_id": "SPARK-1315", "title": "spark on yarn-alpha with mvn on master branch won't build", "status": "Resolved", "priority": "Blocker", "reporter": "Thomas Graves", "assignee": "Sean R. Owen", "labels": [], "created": "2014-03-25T07:10:02.000+0000", "updated": "2015-01-15T09:08:40.000+0000", "description": "I try to build off master branch using maven to build yarn-alpha but get the following errors.\n\nmvn  -Dyarn.version=0.23.10 -Dhadoop.version=0.23.10  -Pyarn-alpha  clean package -DskipTests \n\n-----\n[ERROR] /home/tgraves/y-spark-git/tools/src/main/scala/org/apache/spark/tools/GenerateMIMAIgnore.scala:25: object runtime i\ns not a member of package reflect [ERROR] import scala.reflect.runtime.universe.runtimeMirror\n[ERROR]                      ^\n[ERROR] /home/tgraves/y-spark-git/tools/src/main/scala/org/apache/spark/tools/GenerateMIMAIgnore.scala:40: not found: value runtimeMirror\n[ERROR]   private val mirror = runtimeMirror(classLoader)\n[ERROR]                        ^\n[ERROR] /home/tgraves/y-spark-git/tools/src/main/scala/org/apache/spark/tools/GenerateMIMAIgnore.scala:92: object tools is not a member of package scala\n[ERROR]     scala.tools.nsc.io.File(\".mima-excludes\").\n[ERROR]           ^\n[ERROR] three errors found\n", "comments": ["I believe this was broken by SPARK-1094  - https://github.com/apache/spark/pull/20", "Sorry this was my bad - I didn't realize that scala reflection wasn't included by default:\n\nhttps://github.com/apache/spark/pull/234/files", "Sorry for breaking this guys. Thanks for the quick fix."], "derived": {"summary": "I try to build off master branch using maven to build yarn-alpha but get the following errors. mvn  -Dyarn.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "spark on yarn-alpha with mvn on master branch won't build - I try to build off master branch using maven to build yarn-alpha but get the following errors. mvn  -Dyarn."}, {"q": "What updates or decisions were made in the discussion?", "a": "Sorry for breaking this guys. Thanks for the quick fix."}]}}
{"project": "SPARK", "issue_id": "SPARK-1316", "title": "Remove use of Commons IO", "status": "Resolved", "priority": "Minor", "reporter": "Sean R. Owen", "assignee": "Sean R. Owen", "labels": [], "created": "2014-03-25T08:11:42.000+0000", "updated": "2015-01-15T09:08:38.000+0000", "description": "(This follows from a side point on SPARK-1133, in discussion of the PR: https://github.com/apache/spark/pull/164 )\n\nCommons IO is barely used in the project, and can easily be replaced with equivalent calls to Guava or the existing Spark Utils.scala class.\n\nRemoving a dependency feels good, and this one in particular can get a little problematic since Hadoop uses it too.", "comments": ["PR: https://github.com/apache/spark/pull/1173\n\nActually, Commons IO is not even a dependency right now."], "derived": {"summary": "(This follows from a side point on SPARK-1133, in discussion of the PR: https://github. com/apache/spark/pull/164 )\n\nCommons IO is barely used in the project, and can easily be replaced with equivalent calls to Guava or the existing Spark Utils.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Remove use of Commons IO - (This follows from a side point on SPARK-1133, in discussion of the PR: https://github. com/apache/spark/pull/164 )\n\nCommons IO is barely used in the project, and can easily be replaced with equivalent calls to Guava or the existing Spark Utils."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR: https://github.com/apache/spark/pull/1173\n\nActually, Commons IO is not even a dependency right now."}]}}
{"project": "SPARK", "issue_id": "SPARK-1317", "title": "sbt doesn't work for building Spark programs", "status": "Resolved", "priority": "Major", "reporter": "Diana Carroll", "assignee": null, "labels": [], "created": "2014-03-25T09:08:03.000+0000", "updated": "2014-11-11T14:15:31.000+0000", "description": "I don't know if this is a doc bug or a product bug, because I don't know how it is supposed to work.\n\nThe Spark quick start guide page has a section that walks you through creating a \"standalone\" Spark app in Scala.  I think the instructions worked in 0.8.1 but I can't get them to work in 0.9.0.\n\nThe instructions have you create a directory structure in the \"canonical\" sbt format, but do not tell you where to locate this directory.  However, after setting up the structure, the tutorial then instructs you to use the command \n{code}sbt/sbt package{code}\nwhich implies that the working directory must be SPARK_HOME.\n\nI tried it both ways: creating a \"mysparkapp\" directory right in SPARK_HOME and creating it in my home directory.  Neither worked, with different results:\n- if I create a \"mysparkapp\" directory as instructed in SPARK_HOME, cd to SPARK_HOME and run the command sbt/sbt package as specified, it packages ALL of Spark...but does not build my own app.\n- if I create a \"mysparkapp\" directory elsewhere, cd to that directory, and run the command there, I get an error:\n{code}\n$SPARK_HOME/sbt/sbt package\nawk: cmd. line:1: fatal: cannot open file `./project/build.properties' for reading (No such file or directory)\nAttempting to fetch sbt\n/usr/lib/spark/sbt/sbt: line 33: sbt/sbt-launch-.jar: No such file or directory\n/usr/lib/spark/sbt/sbt: line 33: sbt/sbt-launch-.jar: No such file or directory\nOur attempt to download sbt locally to sbt/sbt-launch-.jar failed. Please install sbt manually from http://www.scala-sbt.org/\n{code}\n\nSo, either:\n1: the Spark distribution of sbt can only be used to build Spark itself, not you own code...in which case the quick start guide is wrong, and should instead say that users should install sbt separately\nOR\n2: the Spark distribution of sbt CAN be used, with property configuration, in which case that configuration should be documented (I wasn't able to figure it out, but I didn't try that hard either)\nOR\n3: the Spark distribution of sbt is *supposed* to be able to build Spark apps, but is configured incorrectly in the product, in which case there's a product bug rather than a doc bug\n\nAlthough this is not a show-stopper, because the obvious workaround is to simply install sbt separately, I think at least updating the docs is pretty high priority, because most people learning Spark start with that Quick Start page, which doesn't work.\n\n(If it's doc issue #1, let me know, and I'll fix the docs myself.  :-) )", "comments": ["PS if you're still interested in this, I am pretty sure #1 is the correct answer. I would use my own sbt (or really, the SBT support in my IDE perhaps, or Maven) to build my own app."], "derived": {"summary": "I don't know if this is a doc bug or a product bug, because I don't know how it is supposed to work. The Spark quick start guide page has a section that walks you through creating a \"standalone\" Spark app in Scala.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "sbt doesn't work for building Spark programs - I don't know if this is a doc bug or a product bug, because I don't know how it is supposed to work. The Spark quick start guide page has a section that walks you through creating a \"standalone\" Spark app in Scala."}, {"q": "What updates or decisions were made in the discussion?", "a": "PS if you're still interested in this, I am pretty sure #1 is the correct answer. I would use my own sbt (or really, the SBT support in my IDE perhaps, or Maven) to build my own app."}]}}
{"project": "SPARK", "issue_id": "SPARK-1318", "title": "Alignment of the Spark Shell with Spark Submit.", "status": "Resolved", "priority": "Major", "reporter": "Bernardo Gomez Palacio", "assignee": null, "labels": ["spark-shell", "spark-submit"], "created": "2014-03-25T10:43:36.000+0000", "updated": "2014-07-09T20:48:17.000+0000", "description": "Align the Spark-Shell with Spark-App/Spark-Submit so that the Spark-Shell offers either the same or a subset of configuration options as the Spark-App does. Ideally this should be implemented in such a way that the Spark-Shell wraps Spark-App therefore minimizing duplication.\n\nAs of now SPARK-1126 is not yet finished and there is an open question regarding how much Bash vs Scala should be leveraged. For a CLI user Bash will provide faster feedback, ergo you don't need to wait for a JVM to start. That said, from a development perspective, Scala might be a better option since we can minimize the code for scripts that target specific operating systems, mainly *NIX vs Windows.\n\nAdditional References:\n\n* Discussion over [Spark-1186 Github Pull 116|https://github.com/apache/spark/pull/116].\n* [SPARK-1126:Spark-App|https://spark-project.atlassian.net/browse/SPARK-1126]\n* [SPARK-1186:Enrich Spark-Shell|https://spark-project.atlassian.net/browse/SPARK-1186]", "comments": ["Bellow a baseline of how the Spark-Shell CLI can align with Spark-App CLI options. I think it will be good to discuss and prune out the options that actually don't make sense to have in the Spark-Shell.\n\n{code}\n${txtbld}Usage${txtrst}: spark-shell [OPTIONS]\n\n${txtbld}OPTIONS${txtrst}:\n    -h  --help          : Print this help information.\n    --executor-memory   : The memory used by each executor of the Spark Shell, the number \n                          is followed by m for megabytes or g for gigabytes, e.g. \"1g\".\n    --driver-memory     : The memory used by the Spark Shell, the number is followed \n                          by m for megabytes or g for gigabytes, e.g. \"1g\", defaults to 512Mb.\n    --master            : A full string that describes the Spark Master, defaults to \"local\"\n                          e.g. \"spark://localhost:7077\".\n    --log-conf          : Enables logging of the supplied SparkConf as INFO at start of the\n                          Spark Context.\n\n${txtbld}Spark standalone with cluster deploy mode only${txtrst}:\n    --driver-cores      : Cores for driver.\n    --supervise         : Whether to restart the driver on failure.\n\n${txtbld}Spark standalone and Mesos only${txtrst}:\n    --total-executor-cores : CORES Total cores for all executors.\n\n${txtbld}YARN-only${txtrst}:\n    --executor-cores    : Number of cores per executor (Default: 1).\n    --executor-memory   : Memory per executor (e.g. 1000M, 2G) (Default: 1G).\n    --queue QUEUE       : The YARN queue to submit the application to (Default: 'default').\n    --num-executors NUM : Number of executors to start (Default: 2).\n    --files FILES       : Comma separated list of files to be placed next to all executors.\n    --archives ARCHIVES : Comma separated list of archives to be extracted next to all executors.\n\ne.g.\n    spark-shell -m spark://localhost:7077 -c 4 -dm 512m -em 2g\n{code}", "Fixed in https://github.com/apache/spark/pull/116"], "derived": {"summary": "Align the Spark-Shell with Spark-App/Spark-Submit so that the Spark-Shell offers either the same or a subset of configuration options as the Spark-App does. Ideally this should be implemented in such a way that the Spark-Shell wraps Spark-App therefore minimizing duplication.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Alignment of the Spark Shell with Spark Submit. - Align the Spark-Shell with Spark-App/Spark-Submit so that the Spark-Shell offers either the same or a subset of configuration options as the Spark-App does. Ideally this should be implemented in such a way that the Spark-Shell wraps Spark-App therefore minimizing duplication."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed in https://github.com/apache/spark/pull/116"}]}}
{"project": "SPARK", "issue_id": "SPARK-1319", "title": "The current code effectively ignores spark.task.cpus", "status": "Closed", "priority": "Major", "reporter": "Kay Ousterhout", "assignee": "Shivaram Venkataraman", "labels": [], "created": "2014-03-25T12:33:33.000+0000", "updated": "2014-03-25T13:06:40.000+0000", "description": "Currently, if spark.task.cpus is set to > 1, we still treat the task as if it only needs 1 core (for both standalone mode and with Mesos).  So, we're effectively ignoring the parameter. \n\nThe parameter only gets used in TaskSchedulerImpl.resourceOffers() as it's assigning tasks, but it is then ignored upstream when CoarseGrainedSchedulerBackend and MesosSchedulerBackend account for the number of free cores.", "comments": ["PR here: https://github.com/apache/spark/pull/219", "Fixed by https://github.com/apache/spark/pull/219"], "derived": {"summary": "Currently, if spark. task.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "The current code effectively ignores spark.task.cpus - Currently, if spark. task."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed by https://github.com/apache/spark/pull/219"}]}}
{"project": "SPARK", "issue_id": "SPARK-1320", "title": "cogroup and groupby should pass an iterator", "status": "Resolved", "priority": "Major", "reporter": "Holden Karau", "assignee": null, "labels": [], "created": "2014-03-25T13:17:57.000+0000", "updated": "2014-03-25T13:31:24.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "cogroup and groupby should pass an iterator"}]}}
{"project": "SPARK", "issue_id": "SPARK-1321", "title": "Use Guava's top k implementation rather than our custom priority queue", "status": "Resolved", "priority": "Critical", "reporter": "Reynold Xin", "assignee": "Reynold Xin", "labels": [], "created": "2014-03-25T14:51:58.000+0000", "updated": "2014-03-30T05:07:05.000+0000", "description": "Guava's top k implementation (in Ordering) is much faster than the BoundedPriorityQueue implementation for roughly sorted input (10 - 20X faster), and still faster for purely random input (2 - 5X). \n\nWe should switch to using that in Spark. \n", "comments": ["pull request https://github.com/apache/spark/pull/229"], "derived": {"summary": "Guava's top k implementation (in Ordering) is much faster than the BoundedPriorityQueue implementation for roughly sorted input (10 - 20X faster), and still faster for purely random input (2 - 5X). We should switch to using that in Spark.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Use Guava's top k implementation rather than our custom priority queue - Guava's top k implementation (in Ordering) is much faster than the BoundedPriorityQueue implementation for roughly sorted input (10 - 20X faster), and still faster for purely random input (2 - 5X). We should switch to using that in Spark."}, {"q": "What updates or decisions were made in the discussion?", "a": "pull request https://github.com/apache/spark/pull/229"}]}}
{"project": "SPARK", "issue_id": "SPARK-1322", "title": "Fix ordering of top() in Python", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Prashant Sharma", "labels": [], "created": "2014-03-25T15:00:50.000+0000", "updated": "2020-02-07T17:26:43.000+0000", "description": "Right now top() returns values in ascending order. It should be consistent with Scala and return them in descending order. /cc [~rxin]", "comments": ["User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/235"], "derived": {"summary": "Right now top() returns values in ascending order. It should be consistent with Scala and return them in descending order.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Fix ordering of top() in Python - Right now top() returns values in ascending order. It should be consistent with Scala and return them in descending order."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/235"}]}}
{"project": "SPARK", "issue_id": "SPARK-1323", "title": "Job hangs with java.io.UTFDataFormatException when reading strings > 65536 bytes. ", "status": "Resolved", "priority": "Major", "reporter": "Karthik", "assignee": null, "labels": ["pyspark"], "created": "2014-03-25T15:18:48.000+0000", "updated": "2014-03-30T05:43:31.000+0000", "description": "Steps to reproduce in Python\n{code:borderStyle=solid}\nst = ''.join(['1' for i in range(65537)])\nsc.parallelize([st]).saveAsTextFile(\"testfile\")\nsc.textFile('testfile').count()\n{code}\nThe last line never completes..  Looking at the logs (with DEBUG enabled) reveals the exception, here is the stack trace\n{code:borderStyle=solid}\n14/03/25 15:03:34 INFO PythonRDD: stdin writer to Python finished early\n14/03/25 15:03:34 DEBUG PythonRDD: stdin writer to Python finished early\njava.io.UTFDataFormatException: encoded string too long: 65537 bytes\n        at java.io.DataOutputStream.writeUTF(DataOutputStream.java:364)\n        at java.io.DataOutputStream.writeUTF(DataOutputStream.java:323)\n        at org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$2.apply(PythonRDD.scala:222)\n        at org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$2.apply(PythonRDD.scala:221)\n        at scala.collection.Iterator$class.foreach(Iterator.scala:727)\n        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n        at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:221)\n        at org.apache.spark.api.python.PythonRDD$$anon$2.run(PythonRDD.scala:81)\n{code}\n", "comments": ["I believe this was fixed in 1.0.0-SNAPSHOT with https://github.com/apache/incubator-spark/commit/1381fc72f7a34f690a98ab72cec8ffb61e0e564d#diff-0a67bc4d171abe4df8eb305b0f4123a2", "Yes good call. I think this is fixed in 0.9.1 (upcoming) and 1.0.0"], "derived": {"summary": "Steps to reproduce in Python\n{code:borderStyle=solid}\nst = ''. join(['1' for i in range(65537)])\nsc.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Job hangs with java.io.UTFDataFormatException when reading strings > 65536 bytes.  - Steps to reproduce in Python\n{code:borderStyle=solid}\nst = ''. join(['1' for i in range(65537)])\nsc."}, {"q": "What updates or decisions were made in the discussion?", "a": "Yes good call. I think this is fixed in 0.9.1 (upcoming) and 1.0.0"}]}}
{"project": "SPARK", "issue_id": "SPARK-1324", "title": "Spark UI Should Not Try to Bind to SPARK_PUBLIC_DNS", "status": "Resolved", "priority": "Critical", "reporter": "Patrick Wendell", "assignee": "Patrick McFadin", "labels": [], "created": "2014-03-25T16:37:15.000+0000", "updated": "2014-03-30T04:14:13.000+0000", "description": "Right now the Spark UI will try to bind to the SPARK_PUBLIC_DNS. This is not correct - the SPARK_PUBLIC_DNS name denotes how Spark components should advertise to other services/components, not the actual bind address.", "comments": ["Might be worth back porting this if users want it in 0.9. Not doing it yet, we'll see if anyone requests it."], "derived": {"summary": "Right now the Spark UI will try to bind to the SPARK_PUBLIC_DNS. This is not correct - the SPARK_PUBLIC_DNS name denotes how Spark components should advertise to other services/components, not the actual bind address.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Spark UI Should Not Try to Bind to SPARK_PUBLIC_DNS - Right now the Spark UI will try to bind to the SPARK_PUBLIC_DNS. This is not correct - the SPARK_PUBLIC_DNS name denotes how Spark components should advertise to other services/components, not the actual bind address."}, {"q": "What updates or decisions were made in the discussion?", "a": "Might be worth back porting this if users want it in 0.9. Not doing it yet, we'll see if anyone requests it."}]}}
{"project": "SPARK", "issue_id": "SPARK-1325", "title": "The maven build error for Spark Tools", "status": "Resolved", "priority": "Major", "reporter": "Guoqiang Li", "assignee": null, "labels": [], "created": "2014-03-25T20:58:09.000+0000", "updated": "2014-03-26T11:33:26.000+0000", "description": null, "comments": ["can you please add a description?  Is this the same as SPARK-1315?", "Yes, it is the same\n[The fix PR|https://github.com/apache/spark/pull/234]", "This is the same issue as SPARK-1315. Since that one has a better description I'm gonna close this as a duplicate."], "derived": {"summary": "", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "The maven build error for Spark Tools"}, {"q": "What updates or decisions were made in the discussion?", "a": "This is the same issue as SPARK-1315. Since that one has a better description I'm gonna close this as a duplicate."}]}}
{"project": "SPARK", "issue_id": "SPARK-1326", "title": "make-distribution.sh's Tachyon support relies on GNU sed", "status": "Resolved", "priority": "Minor", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "2014-03-25T22:35:26.000+0000", "updated": "2014-05-08T09:16:19.000+0000", "description": "It fails on Mac OS X, with {{sed: 1: \"/Users/matei/ ...\": invalid command code m}}\n\n", "comments": ["fixed by PR: https://github.com/apache/spark/pull/264"], "derived": {"summary": "It fails on Mac OS X, with {{sed: 1: \"/Users/matei/. \": invalid command code m}}.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "make-distribution.sh's Tachyon support relies on GNU sed - It fails on Mac OS X, with {{sed: 1: \"/Users/matei/. \": invalid command code m}}."}, {"q": "What updates or decisions were made in the discussion?", "a": "fixed by PR: https://github.com/apache/spark/pull/264"}]}}
{"project": "SPARK", "issue_id": "SPARK-1327", "title": "GLM needs to check addIntercept for intercept and weights", "status": "Resolved", "priority": "Major", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-03-25T23:10:49.000+0000", "updated": "2014-03-31T00:41:52.000+0000", "description": "GLM needs to check addIntercept for intercept and weights.", "comments": ["PR: https://github.com/apache/spark/pull/236", "Partial fix for 0.9.1 where adding intercepts will fail fast. ", "Merged into master and backported to 0.9.x."], "derived": {"summary": "GLM needs to check addIntercept for intercept and weights.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "GLM needs to check addIntercept for intercept and weights - GLM needs to check addIntercept for intercept and weights."}, {"q": "What updates or decisions were made in the discussion?", "a": "Merged into master and backported to 0.9.x."}]}}
{"project": "SPARK", "issue_id": "SPARK-1328", "title": "Current implementation of Standard Deviation in MLUtils may cause catastrophic cancellation, and loss precision.", "status": "Resolved", "priority": "Major", "reporter": "Xusen Yin", "assignee": "Xusen Yin", "labels": ["MLLib,", "statistics", "vector"], "created": "2014-03-26T03:28:42.000+0000", "updated": "2014-04-12T02:45:03.000+0000", "description": "Standard Deviation (SD) is used for dataset normalization, which is useful in the training process of Lasso, etc. Current implementation of SD is using the second-order expectations equation E^2( x )-E(x^2), which is not a stable algorithm facing with floating point computing. \n\nInstead of that, the first-order equation performs better. \n\nMoreover, MLutils is not a right place to hold standard statistics methods, It is more suitable that put it in the VectorRDDFunctions. Some other affected machine learning algorithms should also be refined.", "comments": [], "derived": {"summary": "Standard Deviation (SD) is used for dataset normalization, which is useful in the training process of Lasso, etc. Current implementation of SD is using the second-order expectations equation E^2( x )-E(x^2), which is not a stable algorithm facing with floating point computing.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Current implementation of Standard Deviation in MLUtils may cause catastrophic cancellation, and loss precision. - Standard Deviation (SD) is used for dataset normalization, which is useful in the training process of Lasso, etc. Current implementation of SD is using the second-order expectations equation E^2( x )-E(x^2), which is not a stable algorithm facing with floating point computing."}]}}
{"project": "SPARK", "issue_id": "SPARK-1329", "title": "ArrayIndexOutOfBoundsException if graphx.Graph has more edge partitions than node partitions", "status": "Resolved", "priority": "Major", "reporter": "Daniel Darabos", "assignee": "Ankur Dave", "labels": [], "created": "2014-03-26T10:25:47.000+0000", "updated": "2015-02-15T11:53:54.000+0000", "description": "To reproduce, let's look at a graph with two nodes in one partition, and two edges between them split across two partitions:\nscala> val vs = sc.makeRDD(Seq(1L->null, 2L->null), 1)\nscala> val es = sc.makeRDD(Seq(graphx.Edge(1, 2, null), graphx.Edge(2, 1, null)), 2)\nscala> val g = graphx.Graph(vs, es)\n\nEverything seems fine, until GraphX needs to join the two RDDs:\nscala> g.triplets.collect\n[...]\njava.lang.ArrayIndexOutOfBoundsException: 1\n\tat org.apache.spark.graphx.impl.RoutingTable$$anonfun$2$$anonfun$apply$3.apply(RoutingTable.scala:76)\n\tat org.apache.spark.graphx.impl.RoutingTable$$anonfun$2$$anonfun$apply$3.apply(RoutingTable.scala:75)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.graphx.impl.RoutingTable$$anonfun$2.apply(RoutingTable.scala:75)\n\tat org.apache.spark.graphx.impl.RoutingTable$$anonfun$2.apply(RoutingTable.scala:73)\n\tat org.apache.spark.rdd.RDD$$anonfun$1.apply(RDD.scala:450)\n\tat org.apache.spark.rdd.RDD$$anonfun$1.apply(RDD.scala:450)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:34)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:71)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:230)\n\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:85)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:232)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:161)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:53)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)\n\tat org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\n\nThe bug is fairly obvious in RoutingTable.createPid2Vid() -- it creates an array of length vertices.partitions.size, and then looks up partition IDs from the edges.partitionsRDD in it.\n\nA graph usually has more edges than nodes. So it is natural to have more edge partitions than node partitions.", "comments": ["Thanks - [~darabos]. Do you mind submitting a pull request to fix this?", "The bug being obvious does not mean the fix is obvious too :). But I'll give it a try!", "Thanks - really appreciate it. Let me know if you run into problems. ", "Sorry, I haven't looked into fixing this. We ended up not using GraphX, so we are no longer affected by the bug.", "This was resolved by #368: https://github.com/apache/spark/pull/368"], "derived": {"summary": "To reproduce, let's look at a graph with two nodes in one partition, and two edges between them split across two partitions:\nscala> val vs = sc. makeRDD(Seq(1L->null, 2L->null), 1)\nscala> val es = sc.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "ArrayIndexOutOfBoundsException if graphx.Graph has more edge partitions than node partitions - To reproduce, let's look at a graph with two nodes in one partition, and two edges between them split across two partitions:\nscala> val vs = sc. makeRDD(Seq(1L->null, 2L->null), 1)\nscala> val es = sc."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was resolved by #368: https://github.com/apache/spark/pull/368"}]}}
{"project": "SPARK", "issue_id": "SPARK-1330", "title": "compute_classpath.sh has extra echo which prevents spark-class from working", "status": "Resolved", "priority": "Blocker", "reporter": "Thomas Graves", "assignee": "Thomas Graves", "labels": [], "created": "2014-03-26T13:29:43.000+0000", "updated": "2014-03-27T09:56:40.000+0000", "description": "if I just use spark-class to try to run an example on yarn it errors out because of the echo put in compute_classpath.sh under the hive assembly check:\n\n echo \"Hive assembly found, including hive support.  If this isn't desired run sbt hive/clean.\"\n\nThis causes the classpath to look like:\n\nexec /home/y/share/yjava_jdk/java//bin/java -cp Hive assembly found, including hive support.  If this isn't desired run sbt hive/clean.\n/home/user1/user1cs-spark/assembly/target/scala-2.10/spark-assembly-1.0.0-SNAPSHOT-hadoop0.23.10.jar:/home/user1/user1cs-spark/conf:/home/user1/user1cs-spark/lib_managed/jars/datanucleus-api-jdo-3.2.1.jar:/home/user1/user1cs-spark/lib_managed/jars/datanucleus-core-3.2.2.jar:/home/user1/user1cs-spark/lib_managed/jars/datanucleus-rdbms-3.2.1.jar:/home/user1/user1cs-spark/sql/hive/target/scala-2.10//spark-hive-assembly-1.0.0-SNAPSHOT-hadoop0.23.10.jar:/home/user1/yarn_install//conf -Djava.library.path= -Xms512m -Xmx512m org.apache.spark.deploy.yarn.Client --jar examples/target/scala-2.10/spark-examples-assembly-1.0.0-SNAPSHOT.jar --class org.apache.spark.examples.SparkPi --args yarn-cluster", "comments": ["echo was introduced by SPARK-1251", "The comment in compute_classpath.sh also seems to be wrong.  It says hive assembly isn't included unless you specifically build it with sbt hive/assembly.  I did not do this.  I built it with SPARK_HADOOP_VERSION=0.23.10 SPARK_YARN=true sbt/sbt assembly and it built it.", "https://github.com/apache/spark/pull/241", "I merged this in so that things start working again. Note that there are more fixes related to this going in with https://github.com/apache/spark/pull/237,SPARK-1314"], "derived": {"summary": "if I just use spark-class to try to run an example on yarn it errors out because of the echo put in compute_classpath. sh under the hive assembly check:\n\n echo \"Hive assembly found, including hive support.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "compute_classpath.sh has extra echo which prevents spark-class from working - if I just use spark-class to try to run an example on yarn it errors out because of the echo put in compute_classpath. sh under the hive assembly check:\n\n echo \"Hive assembly found, including hive support."}, {"q": "What updates or decisions were made in the discussion?", "a": "I merged this in so that things start working again. Note that there are more fixes related to this going in with https://github.com/apache/spark/pull/237,SPARK-1314"}]}}
{"project": "SPARK", "issue_id": "SPARK-1331", "title": "Graceful shutdown of Spark Streaming computation", "status": "Resolved", "priority": "Blocker", "reporter": "Tathagata Das", "assignee": "Tathagata Das", "labels": [], "created": "2014-03-26T17:10:54.000+0000", "updated": "2014-04-08T07:00:59.000+0000", "description": "Current version of StreamingContext.stop() directly kills all the data receivers (NetworkReceiver) without waiting for the data already received to be persisted and processed. Fixing this requires the following.\n1. Each receiver, when it gets a stop signal from the driver, should stop receiving, and then wait for all the received data to have been persisted and reported to the driver.\n2. The driver, after stopping all the receivers, should wait for all the received to be processed. ", "comments": [], "derived": {"summary": "Current version of StreamingContext. stop() directly kills all the data receivers (NetworkReceiver) without waiting for the data already received to be persisted and processed.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Graceful shutdown of Spark Streaming computation - Current version of StreamingContext. stop() directly kills all the data receivers (NetworkReceiver) without waiting for the data already received to be persisted and processed."}]}}
{"project": "SPARK", "issue_id": "SPARK-1332", "title": "Improve Spark Streaming's Network Receiver and InputDStream API for future stability", "status": "Resolved", "priority": "Blocker", "reporter": "Tathagata Das", "assignee": "Tathagata Das", "labels": [], "created": "2014-03-26T17:20:44.000+0000", "updated": "2014-04-22T05:02:27.000+0000", "description": "The current Network Receiver API makes it slightly complicated to right a new receiver as one needs to create an instance of BlockGenerator as shown in SocketReceiver \nhttps://github.com/apache/spark/blob/master/streaming/src/main/scala/org/apache/spark/streaming/dstream/SocketInputDStream.scala#L51\n\nExposing the BlockGenerator interface has made it harder to improve the receiving process. The API of NetworkReceiver (which was not a very stable API anyways) needs to be change if we are to ensure future stability. \n\nAdditionally, the functions like streamingContext.socketStream that create input streams, return DStream objects. That makes it hard to expose functionality (say, rate limits) unique to input dstreams. They should return InputDStream or NetworkInputDStream.\n", "comments": [], "derived": {"summary": "The current Network Receiver API makes it slightly complicated to right a new receiver as one needs to create an instance of BlockGenerator as shown in SocketReceiver \nhttps://github. com/apache/spark/blob/master/streaming/src/main/scala/org/apache/spark/streaming/dstream/SocketInputDStream.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Improve Spark Streaming's Network Receiver and InputDStream API for future stability - The current Network Receiver API makes it slightly complicated to right a new receiver as one needs to create an instance of BlockGenerator as shown in SocketReceiver \nhttps://github. com/apache/spark/blob/master/streaming/src/main/scala/org/apache/spark/streaming/dstream/SocketInputDStream."}]}}
{"project": "SPARK", "issue_id": "SPARK-1333", "title": "Java API for running SQL queries", "status": "Resolved", "priority": "Blocker", "reporter": "Michael Armbrust", "assignee": "Michael Armbrust", "labels": [], "created": "2014-03-26T20:14:34.000+0000", "updated": "2014-04-03T22:47:04.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Java API for running SQL queries"}]}}
{"project": "SPARK", "issue_id": "SPARK-1334", "title": "RDD names should be settable from PySpark", "status": "Resolved", "priority": "Trivial", "reporter": "Harrison Brundage", "assignee": null, "labels": [], "created": "2014-03-26T21:17:28.000+0000", "updated": "2014-03-27T09:56:10.000+0000", "description": "It would be nice to be able to set the Java RDD's name attribute via PySpark for human readability in the Spark web ui.", "comments": ["https://github.com/apache/spark/pull/249 will resolve this. ", "I believe this already exists as name/setName in rdd.py. Feel free to re-open if that's not correct.", "@Patrick Wendell sorry for the noise, my bad.", "No prob - feel free to submit other patches if there is stuff missing from python!"], "derived": {"summary": "It would be nice to be able to set the Java RDD's name attribute via PySpark for human readability in the Spark web ui.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "RDD names should be settable from PySpark - It would be nice to be able to set the Java RDD's name attribute via PySpark for human readability in the Spark web ui."}, {"q": "What updates or decisions were made in the discussion?", "a": "No prob - feel free to submit other patches if there is stuff missing from python!"}]}}
{"project": "SPARK", "issue_id": "SPARK-1335", "title": "Also increase perm gen / code cache for scalatest when invoked via Maven build", "status": "Resolved", "priority": "Major", "reporter": "Sean R. Owen", "assignee": "Sean R. Owen", "labels": [], "created": "2014-03-27T03:09:39.000+0000", "updated": "2015-01-15T09:08:41.000+0000", "description": "I am observing build failures when the Maven build reaches tests in the new SQL components. (I'm on Java 7 / OSX 10.9). The failure is the usual complaint from scala, that it's out of permgen space, or that JIT out of code cache space.\n\nI see that various build scripts increase these both for SBT. This change simply adds these settings to scalatest's arguments. Works for me and seems a bit more consistent.\n\n(In the PR I'm going to tack on some other little changes too -- see PR.)", "comments": ["This problem is also messing up our jenkins maven builds, so thanks a bunch for digging around.", "The problem also appeared in branch 1.1. The following command fails. \n{{mvn  -Pyarn-alpha -Phive -Dhadoop.version=2.0.0-cdh4.5.0 -DskipTests  package}} .\n I'm on Java 6 / OSX 10.9.4"], "derived": {"summary": "I am observing build failures when the Maven build reaches tests in the new SQL components. (I'm on Java 7 / OSX 10.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Also increase perm gen / code cache for scalatest when invoked via Maven build - I am observing build failures when the Maven build reaches tests in the new SQL components. (I'm on Java 7 / OSX 10."}, {"q": "What updates or decisions were made in the discussion?", "a": "The problem also appeared in branch 1.1. The following command fails. \n{{mvn  -Pyarn-alpha -Phive -Dhadoop.version=2.0.0-cdh4.5.0 -DskipTests  package}} .\n I'm on Java 6 / OSX 10.9.4"}]}}
{"project": "SPARK", "issue_id": "SPARK-1336", "title": "Reduce Verbosity of QA Scripts", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "labels": [], "created": "2014-03-27T10:22:31.000+0000", "updated": "2015-12-10T15:06:50.000+0000", "description": "Looking at the log output on jenkins is becoming a big data problem. In some cases, the reason for failures is also confusing. There are a few ways we can make this more sane.\n\n- When running style checks, we should pipe the output to a file. The, if the tests fail we should grep the file for \"error\" and print out a message saying that the style checks failed and printing all the lines with failures. Probably this means writing a different script for the style checks similar to what we have for the rat checks.\n\n- When running tests and running MIMA, we should try to silence all of the Resolving messages. Something like this might work:\n\nsbt/sbt test |  grep -v \"info.*Resolving\"\n\nIdeally we want to do it in a way where we minimize the possibility of hiding legitimate output. \n\n\n- ", "comments": ["Github user ScrapCodes commented on the pull request:\n\n    https://github.com/apache/spark/pull/262#issuecomment-38998370\n  \n    There was one more alternative i.e. we could have added log4j.properties with loglevel at ERROR. But this should also be fine. \n", "Github user AmplabJenkins commented on the pull request:\n\n    https://github.com/apache/spark/pull/262#issuecomment-38998417\n  \n     Merged build triggered. Build is starting -or- tests failed to complete.\n", "Github user AmplabJenkins commented on the pull request:\n\n    https://github.com/apache/spark/pull/262#issuecomment-38998421\n  \n    Merged build started. Build is starting -or- tests failed to complete.\n", "Github user AmplabJenkins commented on the pull request:\n\n    https://github.com/apache/spark/pull/262#issuecomment-39000583\n  \n    Merged build finished. All automated tests passed.\n", "Github user AmplabJenkins commented on the pull request:\n\n    https://github.com/apache/spark/pull/262#issuecomment-39000585\n  \n    All automated tests passed.\n    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/13574/\n", "Github user asfgit closed the pull request at:\n\n    https://github.com/apache/spark/pull/262\n", "User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/262", "User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/321"], "derived": {"summary": "Looking at the log output on jenkins is becoming a big data problem. In some cases, the reason for failures is also confusing.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Reduce Verbosity of QA Scripts - Looking at the log output on jenkins is becoming a big data problem. In some cases, the reason for failures is also confusing."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/321"}]}}
{"project": "SPARK", "issue_id": "SPARK-1337", "title": "Application web UI garbage collects newest stages instead old ones", "status": "Resolved", "priority": "Major", "reporter": "Dmitry Bugaychenko", "assignee": "Patrick Wendell", "labels": [], "created": "2014-03-27T11:12:06.000+0000", "updated": "2014-04-04T05:35:32.000+0000", "description": "When running task with many stages (eg. streaming task) and spark.ui.retainedStages set to small value (100) application UI removes newest stages keeping 90 old ones... ", "comments": ["https://github.com/apache/spark/pull/320\n\nThis is a simple fix if people want to pull this in and build Spark on their own."], "derived": {"summary": "When running task with many stages (eg. streaming task) and spark.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Application web UI garbage collects newest stages instead old ones - When running task with many stages (eg. streaming task) and spark."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/320\n\nThis is a simple fix if people want to pull this in and build Spark on their own."}]}}
{"project": "SPARK", "issue_id": "SPARK-1338", "title": "Create Additional Style Rules", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Prashant Sharma", "labels": [], "created": "2014-03-27T11:20:31.000+0000", "updated": "2014-12-16T10:41:09.000+0000", "description": "There are a few other rules that would be helpful to have. Also we should add tests for these rules because it's easy to get them wrong. I gave some example comparisons from a javascript style checker.\n\nRequire spaces in type declarations:\ndef foo:String = X // no\ndef foo: String = XXX\n\ndef x:Int = 100 // no\nval x: Int = 100\n\nRequire spaces after keywords:\nif(x - 3) // no\nif (x + 10)\nSee: requireSpaceAfterKeywords from\nhttps://github.com/mdevils/node-jscs\n\nDisallow spaces inside of parentheses:\nval x = ( 3 + 5 ) // no\nval x = (3 + 5)\n\nSee: disallowSpacesInsideParentheses from\nhttps://github.com/mdevils/node-jscs\n\nRequire spaces before and after binary operators:\nSee: requireSpaceBeforeBinaryOperators\nSee: disallowSpaceAfterBinaryOperators\nfrom https://github.com/mdevils/node-jscs\n", "comments": ["Blocking unicode operators could be a good idea too -- Eclipse apparently doesn't handle them well SPARK-2182", "The PR for this was abandoned. What's the thinking on these style-rule JIRAs? there are a number still open.", "This is fixed by updating the scalastyle version. Applying these styles accross the code base can be a different issue."], "derived": {"summary": "There are a few other rules that would be helpful to have. Also we should add tests for these rules because it's easy to get them wrong.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Create Additional Style Rules - There are a few other rules that would be helpful to have. Also we should add tests for these rules because it's easy to get them wrong."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is fixed by updating the scalastyle version. Applying these styles accross the code base can be a different issue."}]}}
{"project": "SPARK", "issue_id": "SPARK-1339", "title": "Build error: org.eclipse.paho:mqtt-client", "status": "Resolved", "priority": "Major", "reporter": "Ken Williams", "assignee": null, "labels": [], "created": "2014-03-27T14:52:12.000+0000", "updated": "2014-10-13T17:33:19.000+0000", "description": "Using Maven, I'm unable to build the 0.9.0 distribution I just downloaded.  The Maven error is:\n\n{code}\n[ERROR] Failed to execute goal on project spark-examples_2.10: Could not resolve dependencies for project org.apache.spark:spark-examples_2.10:jar:0.9.0-incubating: Could not find artifact org.eclipse.paho:mqtt-client:jar:0.4.0 in nexus\n{code}\n\nMy Maven version is 3.2.1, running on Java 1.7.0, using Scala 2.10.4.\n\nIs there an additional Maven repository I should add or something?\n\nIf I go into the {{pom.xml}} and comment out the {{external/mqtt}} and {{examples}} modules, the build succeeds.  I'm fine without the MQTT stuff, but I would really like to get the examples working because I haven't played with Spark before.", "comments": ["I forgot to mention, I'm building the {{package}} target against Hadoop 2.2.0:\n\n{code}\nmvn -U -Pyarn -Dhadoop.version=2.2.0 -Dyarn.version=2.2.0 -DskipTests package\n{code}", "(Just cruising some old issues) I can't reproduce this, and this is a general symptom of a repo not being accesible. It's actually nothing to do with mqtt-client per se. Also, we've fixed some repo issues along the way.", "That's probably the case - fine to close this ticket then."], "derived": {"summary": "Using Maven, I'm unable to build the 0. 9.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Build error: org.eclipse.paho:mqtt-client - Using Maven, I'm unable to build the 0. 9."}, {"q": "What updates or decisions were made in the discussion?", "a": "That's probably the case - fine to close this ticket then."}]}}
{"project": "SPARK", "issue_id": "SPARK-1340", "title": "Some Spark Streaming receivers are not restarted when worker fails", "status": "Resolved", "priority": "Critical", "reporter": "Tathagata Das", "assignee": "Tathagata Das", "labels": [], "created": "2014-03-27T15:24:36.000+0000", "updated": "2014-05-15T20:25:44.000+0000", "description": "For some streams like Kafka stream, the receiver do not get restarted if the worker running the receiver fails. ", "comments": ["Any udpate on when this fix would be available?", "I havent explicitly tested this, but this should be fixed after after a whole refactoring in the receiver API done in https://github.com/apache/spark/pull/300\n\nTo elaborate further, the new refactored receiver ensures that the task that launches the receiver does not complete until the receiver is explicitly shutdown. So if the receiver fails with an exception it should get relaunched. Well, ideally. This still needs to be tested.\n", "Resolved with \nhttps://issues.apache.org/jira/browse/SPARK-1332"], "derived": {"summary": "For some streams like Kafka stream, the receiver do not get restarted if the worker running the receiver fails.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Some Spark Streaming receivers are not restarted when worker fails - For some streams like Kafka stream, the receiver do not get restarted if the worker running the receiver fails."}, {"q": "What updates or decisions were made in the discussion?", "a": "Resolved with \nhttps://issues.apache.org/jira/browse/SPARK-1332"}]}}
{"project": "SPARK", "issue_id": "SPARK-1341", "title": "Ability to control the data rate in Spark Streaming ", "status": "Resolved", "priority": "Major", "reporter": "Tathagata Das", "assignee": null, "labels": [], "created": "2014-03-27T15:31:00.000+0000", "updated": "2014-07-10T23:05:39.000+0000", "description": "To be able to control the rate at which data is received through Spark Streaming's receivers.\n", "comments": ["We implemented a quick fix for kafka message handler with messages per second rate limit\n\n{code}\n  // Handles Kafka Messages with rate limit\n  private class MessageHandlerWithRateLimit[K: ClassTag, V: ClassTag](stream: KafkaStream[K, V], rate: Long)\n    extends Runnable {\n    def run() {\n      logInfo(\"Starting MessageHandler.\")\n      var lastTs: Long = System.currentTimeMillis()\n      var currentTs: Long = 0l\n      var toreceive = rate\n      for (msgAndMetadata <- stream) {\n        currentTs = System.currentTimeMillis()\n        if (currentTs - lastTs > 1000) {\n          toreceive = rate\n          lastTs = currentTs\n        }\n        blockGenerator +=(msgAndMetadata.key, msgAndMetadata.message)\n        toreceive-=1\n        if (toreceive < 0) {\n          Thread.sleep(Math.max(50, 1000 + lastTs - currentTs))\n        }\n      }\n    }\n  }\n{code}\n\nit solved OOM problem during reading kafka topic from begin", "Thats good! Though the generic solution that applies to all receivers is to have rate limiting capability on all input sources. I think the best way to implement this is to use the BlockGenerator, since that is used by all the network receivers to receive data. ", "I don't think that doing ratelimit on all sources is good idea.\nIn kafka and other streams that can be reconstucted it is ok.\n\nBut on generic tcp/network stream it means to lost messsages. \nMay be it is ok to lost, and not to die with OOM, but may be it is wrong.", "I am working in an implementation of this feature in the BlockGenerator. It will be configurable, allowing for not throttling, so there shouldn't be an issue with lost messages.", "Great! Happy to help you out if you need.\n", "[~tdas] Can you take a look?\n\nhttps://github.com/apache/spark/pull/945", "Thanks Isaac. This has been merged, and is going to be very useful!"], "derived": {"summary": "To be able to control the rate at which data is received through Spark Streaming's receivers.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Ability to control the data rate in Spark Streaming  - To be able to control the rate at which data is received through Spark Streaming's receivers."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks Isaac. This has been merged, and is going to be very useful!"}]}}
{"project": "SPARK", "issue_id": "SPARK-1342", "title": "Bump Scala version to 2.10.4", "status": "Closed", "priority": "Trivial", "reporter": "Mark Hamstra", "assignee": "Mark Hamstra", "labels": [], "created": "2014-03-27T16:48:04.000+0000", "updated": "2014-05-17T17:20:41.000+0000", "description": "Start using the newest Scala 2.10 release.", "comments": ["https://github.com/apache/spark/pull/259", "It appears that the pull request got closed yet the issue is still in progress. Does this need a better solution? Is there anything left before the issue's closed?", "Should be closed -- there's nothing left to do, and we've been using 2.10.4 for awhile now."], "derived": {"summary": "Start using the newest Scala 2. 10 release.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Bump Scala version to 2.10.4 - Start using the newest Scala 2. 10 release."}, {"q": "What updates or decisions were made in the discussion?", "a": "Should be closed -- there's nothing left to do, and we've been using 2.10.4 for awhile now."}]}}
{"project": "SPARK", "issue_id": "SPARK-1343", "title": "PySpark OOMs without caching", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Davies Liu", "labels": [], "created": "2014-03-27T22:49:27.000+0000", "updated": "2014-07-28T22:22:49.000+0000", "description": "There have been several reports on the list of PySpark 0.9 OOMing even if it does simple maps and counts, whereas 0.9 didn't. This may be due to either the batching added to serialization, or due to invalid serialized data which makes the Java side allocate an overly large array. Needs investigating for 1.0.", "comments": ["Maybe it's related to partitionBy() with small number of partitions, the data in one partition will send to JVM as several huge bytearray, they will cost huge memory before writing into disks, because default spark.serializer.objectStreamReset is too large.\n\nHopefully, PR-1568 and PR-1460 will fix these issues.\n\nClose this now, will re-open it if it happens again.", "https://github.com/apache/spark/pull/1460\n\nhttps://github.com/apache/spark/pull/1568"], "derived": {"summary": "There have been several reports on the list of PySpark 0. 9 OOMing even if it does simple maps and counts, whereas 0.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "PySpark OOMs without caching - There have been several reports on the list of PySpark 0. 9 OOMing even if it does simple maps and counts, whereas 0."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/1460\n\nhttps://github.com/apache/spark/pull/1568"}]}}
{"project": "SPARK", "issue_id": "SPARK-1344", "title": "Scala API docs for top methods", "status": "Resolved", "priority": "Major", "reporter": "Diana Carroll", "assignee": "Sean R. Owen", "labels": [], "created": "2014-03-28T07:36:58.000+0000", "updated": "2014-11-10T01:42:40.000+0000", "description": "The RDD.top() methods are documented as follows:\nbq. Returns the top *K* elements from this RDD using the natural ordering for *T*.\nbq. Returns the top *K* elements from this RDD as defined by the specified Comparator[[T]].\n\nI believe those should read\nbq. Returns the top *num* elements from this RDD using the natural ordering for *K*.\nbq. Returns the top *num* elements from this RDD as defined by the specified Comparator[[K]].\n", "comments": ["No, the original is correct.  K is the number of elements; T is the type of the elements. ", "This doesn't make sense to me.  The signature of the method is \ntop(num: Int): List[(K, V)]\n\nSo there's a parameter \"num\" but the description doesn't reference it.\n\nThere's no T in the signature, and the description *does* reference it.  (Perhaps it is assumed that all uses of \"T\" anywhere refer to the type of something?)\n\nThe only K in the signature is in the return value (the key of the returned tuples).  This doesn't make sense with \"...top K references...\"\n\nI don't see how this description helps a user understand how to use the function.\n\n", "Which docs/comments are you looking at?  When dealing with an RDD\\[(K, V)\\] there is room for confusion since the way data scientists refer to getting the top-ranked results is Top-K, where K is the number of ranked items fetched, whereas our generic way of referring to an RDD of key-value pairs is RDD\\[(K, V)\\], where K is the type of the keys.  On top of that, top() doesn't require a PairRDD, so it is available on a simple RDD\\[T\\] as {{def top(num: Int)(implicit ord: Ordering\\[T\\]): Array\\[T\\]}} -- and that's where the original wording comes from.  ", "Oh, I think I figured out the misunderstanding between your comment and mine:\nThere are multiple \"top\" functions.\n\nI am specifically looking at JavaPairRDD.top:\nhttp://spark.incubator.apache.org/docs/latest/api/core/index.html#org.apache.spark.api.java.JavaPairRDD\n{quote}\n{{def top(num: Int): List[[(K, V)]]}}\nReturns the top K elements from this RDD using the natural ordering for T.\nnum - the number of top elements to return\nreturns - an array of top elements\n{quote}\n\nThis one I see is different than, say, RDD.top:\nhttp://spark.incubator.apache.org/docs/latest/api/core/index.html#org.apache.spark.rdd.RDD\n{quote}\n{{def top(num: Int)(implicit ord: Ordering[[T]]): Array[[T]]}}\nReturns the top K elements from this RDD as defined by the specified implicit Ordering[[T]].\nnum - the number of top elements to return\nord - the implicit ordering for T\nreturns- an array of top elements\n{quote}\n\n\n\n\n", "Our comments crossed paths.\n\nAt any rate, regardless of the origin of the phrasing, I found it confusing to see a reference to a non-existent parameter \"K\" in the description.  I'm not a data scientist, just a programmer, trying to use the API to understand how to call a function, and I believe that my proposed phrasing is both correct and the least confusing.  \"Room for confusion\" is not a plus in docs.\n", "The problem with your re-wording, though, is that it is not correct.  Since top() for RDD\\[(K, V)\\] is inherited from RDD\\[T\\], the Ordering is parameterized on (K, V), not just K.  Typically for PairRDD, this will be a lexicographical ordering of the pairs, but it could be any user-defined Ordering\\[(K, V)\\]. ", "Fair enough; what I meant was that I prefer my wording for \"num\" over the non-existent \"K\" for how many items to return.\n\nSo how about:\n{{def top(num: Int): List[[(K, V)]]}}\nReturns the top num elements from this RDD using the natural ordering for the elements of the RDD.\nand\n{{def top(num: Int, comp: Comparator[[(K, V)]]): List[[(K, V)]]}}\nReturns the top num elements from this RDD as defined by the specified Comparator[[(K,V)]].", "For the specific case of JavaPairRDD, that's fine; but you won't be able to make equivalent changes for Scala RDDs that inherit from org.apache.spark.rdd.RDD", "User 'srowen' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/3168"], "derived": {"summary": "The RDD. top() methods are documented as follows:\nbq.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Scala API docs for top methods - The RDD. top() methods are documented as follows:\nbq."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'srowen' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/3168"}]}}
{"project": "SPARK", "issue_id": "SPARK-1345", "title": "spark on yarn 0.23 using maven doesn't build", "status": "Resolved", "priority": "Blocker", "reporter": "Thomas Graves", "assignee": "Thomas Graves", "labels": [], "created": "2014-03-28T07:51:43.000+0000", "updated": "2014-03-31T14:30:37.000+0000", "description": "The spark on yarn hadoop 0.23 build on maven fails:\n\nERROR] /home/tgraves/tgravescs-spark/sql/core/src/main/scala/org/apache/spark/sql/parquet/ParquetRelation.scala:62: org.apache.hadoop.fs.Path does not have a constructor\n[ERROR]       .readMetaData(new Path(path))\n[ERROR]                     ^\n[ERROR] /home/tgraves/tgravescs-spark/sql/core/src/main/scala/org/apache/spark/sql/parquet/ParquetRelation.scala:116: org.apache.hadoop.fs.Path does not have a constructor\n[ERROR]     val origPath = new Path(pathStr)\n[ERROR]                    ^\n", "comments": ["I think the more relevant part of the error is:\n\n[WARNING] Class org.apache.avro.reflect.Stringable not found - continuing with a stub.\n[WARNING] Caught: java.lang.NullPointerException while parsing annotations in /home/hadoopqa/.m2/repository/org/apache/hadoop/hadoop-common/0.23.9/hadoop-common-0.23.9.jar(org/apache/hadoop/io/Text.class)\n[ERROR] error while loading Text, class file '/home/hadoopqa/.m2/repository/org/apache/hadoop/hadoop-common/0.23.9/hadoop-common-0.23.9.jar(org/apache/hadoop/io/Text.class)' is broken\n\nBasically this is happening because the pom file doesn't include the avro dependency that hadoop 0.23 needs.", "https://github.com/apache/spark/pull/263"], "derived": {"summary": "The spark on yarn hadoop 0. 23 build on maven fails:\n\nERROR] /home/tgraves/tgravescs-spark/sql/core/src/main/scala/org/apache/spark/sql/parquet/ParquetRelation.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "spark on yarn 0.23 using maven doesn't build - The spark on yarn hadoop 0. 23 build on maven fails:\n\nERROR] /home/tgraves/tgravescs-spark/sql/core/src/main/scala/org/apache/spark/sql/parquet/ParquetRelation."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/263"}]}}
{"project": "SPARK", "issue_id": "SPARK-1346", "title": "Backport SPARK-1210 into 0.9 branch", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Sean R. Owen", "labels": [], "created": "2014-03-28T10:25:22.000+0000", "updated": "2015-03-14T08:51:31.000+0000", "description": "We should backport this after the 0.9.1 release happens.", "comments": ["Q: is anything being backported to 0.9 at this point? would there ever be another release that needs this, or is it done?", "Although it may be unlikely an 0.9.3 release happens, I just made this back-port anyway. It's a simple change."], "derived": {"summary": "We should backport this after the 0. 9.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Backport SPARK-1210 into 0.9 branch - We should backport this after the 0. 9."}, {"q": "What updates or decisions were made in the discussion?", "a": "Although it may be unlikely an 0.9.3 release happens, I just made this back-port anyway. It's a simple change."}]}}
{"project": "SPARK", "issue_id": "SPARK-1347", "title": "SHARK error when running in server mode: java.net.BindException: Address already in use", "status": "Closed", "priority": "Major", "reporter": "Test", "assignee": null, "labels": [], "created": "2014-03-28T11:18:08.000+0000", "updated": "2014-04-01T21:43:02.000+0000", "description": "Start spark on cluster machine\nthen start the shark in server mode using:\n./bin/shark --service sharkserver <port>\n\nNow connect to shark using client as :\n./bin/shark -h <server-host> -p <server-port>\n\nCheck the hive.log:\n2014-03-28 10:24:05,391 WARN  component.AbstractLifeCycle (AbstractLifeCycle.java:setFailed(204)) - FAILED org.eclipse.jetty.server.Server@5eb98063: java.net.BindException: Address already in use\njava.net.BindException: Address already in use\n\tat sun.nio.ch.Net.bind0(Native Method)\n\tat sun.nio.ch.Net.bind(Net.java:444)\n\tat sun.nio.ch.Net.bind(Net.java:436)\n\tat sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)\n\tat sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)\n\tat org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)\n\tat org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)\n\tat org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)\n\tat org.eclipse.jetty.server.Server.doStart(Server.java:286)\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)\n\tat org.apache.spark.ui.JettyUtils$$anonfun$1.apply$mcV$sp(JettyUtils.scala:118)\n\tat org.apache.spark.ui.JettyUtils$$anonfun$1.apply(JettyUtils.scala:118)\n\tat org.apache.spark.ui.JettyUtils$$anonfun$1.apply(JettyUtils.scala:118)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat org.apache.spark.ui.JettyUtils$.connect$1(JettyUtils.scala:118)\n\tat org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:129)\n\tat org.apache.spark.ui.SparkUI.bind(SparkUI.scala:57)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:159)\n\tat shark.SharkContext.<init>(SharkContext.scala:42)\n\tat shark.SharkContext.<init>(SharkContext.scala:61)\n\tat shark.SharkEnv$.initWithSharkContext(SharkEnv.scala:81)\n\tat shark.SharkEnv$.init(SharkEnv.scala:41)\n\tat shark.SharkEnv$.fixUncompatibleConf(SharkEnv.scala:48)\n\tat shark.SharkCliDriver$.main(SharkCliDriver.scala:165)\n\tat shark.SharkCliDriver.main(SharkCliDriver.scala)\n", "comments": ["It looks like the the Spark web UI is failing to bind to its port.  Are you running another copy of Spark on the same machine or is something else listening on 4040?  If so you should change the port for the web ui by setting \"spark.ui.port\" in SparkConf.  More details on configuration can be found here: http://spark.apache.org/docs/0.9.0/configuration.html"], "derived": {"summary": "Start spark on cluster machine\nthen start the shark in server mode using:. /bin/shark --service sharkserver <port>\n\nNow connect to shark using client as :.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "SHARK error when running in server mode: java.net.BindException: Address already in use - Start spark on cluster machine\nthen start the shark in server mode using:. /bin/shark --service sharkserver <port>\n\nNow connect to shark using client as :."}, {"q": "What updates or decisions were made in the discussion?", "a": "It looks like the the Spark web UI is failing to bind to its port.  Are you running another copy of Spark on the same machine or is something else listening on 4040?  If so you should change the port for the web ui by setting \"spark.ui.port\" in SparkConf.  More details on configuration can be found here: http://spark.apache.org/docs/0.9.0/configuration.html"}]}}
{"project": "SPARK", "issue_id": "SPARK-1348", "title": "Spark UI's do not bind to localhost interface anymore", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Kan Zhang", "labels": [], "created": "2014-03-28T14:05:48.000+0000", "updated": "2016-01-16T21:29:51.000+0000", "description": "When running the shell or standalone master, it no longer binds to localhost. I think this may have been caused by the security patch. We should figure out what caused it and revert to the old behavior. Maybe we want to always bind to `localhost` or just to bind to all interfaces.", "comments": ["JettyUtils.startJettyServer() used to bind to all interfaces, however, SPARK-1060 change that to only bind to a specific interface (preferably a non-loopback address).\n\nIf you want to revert to previous behavior, here's the patch.\n\nhttps://github.com/apache/spark/pull/318", "Okay we fixed this for 1.0. The decision was just to revert to the old behavior of binding to all interfaces. Note that this means SPARK_LOCAL_IP is not relevant for the UI."], "derived": {"summary": "When running the shell or standalone master, it no longer binds to localhost. I think this may have been caused by the security patch.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Spark UI's do not bind to localhost interface anymore - When running the shell or standalone master, it no longer binds to localhost. I think this may have been caused by the security patch."}, {"q": "What updates or decisions were made in the discussion?", "a": "Okay we fixed this for 1.0. The decision was just to revert to the old behavior of binding to all interfaces. Note that this means SPARK_LOCAL_IP is not relevant for the UI."}]}}
{"project": "SPARK", "issue_id": "SPARK-1349", "title": "spark-shell's repl history is shared with the scala repl", "status": "Resolved", "priority": "Minor", "reporter": "Aaron Davidson", "assignee": "Aaron Davidson", "labels": [], "created": "2014-03-28T15:05:44.000+0000", "updated": "2014-04-07T00:44:08.000+0000", "description": "When developing in Scala, it is relatively common to try things out in a scala interpreter. Running spark-shell, however, reveals that the command history is shared between these two largely independent programs. This should be avoided.", "comments": [], "derived": {"summary": "When developing in Scala, it is relatively common to try things out in a scala interpreter. Running spark-shell, however, reveals that the command history is shared between these two largely independent programs.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "spark-shell's repl history is shared with the scala repl - When developing in Scala, it is relatively common to try things out in a scala interpreter. Running spark-shell, however, reveals that the command history is shared between these two largely independent programs."}]}}
{"project": "SPARK", "issue_id": "SPARK-1350", "title": "YARN ContainerLaunchContext should use cluster's JAVA_HOME", "status": "Resolved", "priority": "Major", "reporter": "Sandy Ryza", "assignee": "Sandy Ryza", "labels": [], "created": "2014-03-28T16:22:57.000+0000", "updated": "2015-01-09T04:19:18.000+0000", "description": "{code}\n    var javaCommand = \"java\"\n    val javaHome = System.getenv(\"JAVA_HOME\")\n    if ((javaHome != null && !javaHome.isEmpty()) || env.isDefinedAt(\"JAVA_HOME\")) {\n      javaCommand = Environment.JAVA_HOME.$() + \"/bin/java\"\n    }\n{code}\n\nCurrently, if JAVA_HOME is specified on the client, it will be used instead of the value given on the cluster.  This makes it so that Java must be installed in the same place on the client as on the cluster.\n\nThis is a possibly incompatible change that we should get in before 1.0.", "comments": ["I assume you are referring to the one in ClientBase?\n\nIt actually does not use the same java as specified by JAVA_HOME on the client.  If JAVA_HOME is set on the client then it just adds $JAVA_HOME/bin to the java command to launch the AM. Yarn can be configured (and is the default config) to put JAVA_HOME into the container launch script, so the launch command will use the JAVA_HOME specified by yarn.   I agree its a bit hacky and could be improved though.  On MapReduce it just always uses \"$JAVA_HOME/bin/java\"\n\nThe main idea is that I want $JAVA_HOME to be included in the launch command so that its not relying on the default install of java on the system.  I want it to use the JAVA_HOME specified by yarn and I also want to allow it to be overridden so that a user could choose to run either 32 bit or 64 bit jdk.  \n\nexport SPARK_YARN_USER_ENV=\"JAVA_HOME=/myyarinstall/jdk64/current\"\n\nI was told this goes against how spark is in general setup where it relies on whatever is installed on the system so this was the compromise.\n\n", "bq. It actually does not use the same java as specified by JAVA_HOME on the client.\nOops, you're right.  I was misunderstanding the code.  This is less of a big deal than I thought.\n\nbq. The main idea is that I want $JAVA_HOME to be included in the launch command so that its not relying on the default install of java on the system.\nI 100% agree that this is the right thing to do.  What still seems a little weird to me is that the use of JAVA_HOME on the cluster depends on whether the client JAVA_HOME is set.\n\nbq. I was told this goes against how spark is in general setup where it relies on whatever is installed on the system so this was the compromise.\nSo this has already been discussed and always using JAVA_HOME in the launch command (never just \"java\") was rejected for this reason?  If all YARN apps accessed java in the same way, the way that MapReduce does, I think it would make things a lot simpler operationally.", "I agree it is weird, lets bring this back up and see if we can just always use $JAVA_HOME.  This is yarn specific code so shouldn't affect other parts of spark.  I don't know if anyone would run YARN without this being set.\n\nPatrick, Mridul,  any objections to changing this?", "+ [~pwendell] and [~mridulm@yahoo-inc.com]", "\nWe will need a way to configure JAVA_HOME (like the 32bit vs 64bit case Tom mentioned about : which is used quite often)\n\nTom, is there a way to do this within context of yarn ? Ability to override the JAVA_HOME used via some config ?\n\n- If yes, we should leverage that and always use $JAVA_HOME/bin/java in our code.\n\nIf not, let us continue with what we currently have : it is the best we can currently do.", "bq. Tom, is there a way to do this within context of yarn ? Ability to override the JAVA_HOME used via some config ?\nThe submitter can set a different JAVA_HOME in SPARK_YARN_USER_ENV.  Any properties placed there will override versions of these properties in the YARN NodeManager's environment.", "You mistook my question; I meant in context of yarn - not in context of spark over yarn.", "Ah, sorry.  YARN doesn't have any configs for this.  The container environment is the environment of the NodeManager process, a few additional variables YARN adds such as the container ID and user name, and anything specified by the application in the ContainerLaunchContext.", "https://github.com/apache/spark/pull/313", "[~sandyr] Using Environment.JAVA_HOME.$() causes issues while submitting spark applications from a windows box into a Yarn cluster running on Linux (with spark master set as yarn-client). This is because Environment.JAVA_HOME.$() resolves to %JAVA_HOME% which results in not a valid executable on Linux. Is this a Spark issue or a YARN issue?", " which version of hadoop are you using?  Spark is using Environment.JAVA_HOME.$() but Environment is a hadoop class and its supposed to handle windows:\n\n   public String $() {\n      if (Shell.WINDOWS) {\n        return \"%\" + variable + \"%\";\n      } else {\n        return \"$\" + variable;\n      }\n    }", "I am using hadoop 2.5.0 (CDH). Agreed that it handles for windows. But the use case I am talking about is when SparkContext is created programmatically on a windows machine and is used to submit jobs on a yarn cluster running on Linux. As per above code, %JAVA_HOME%/bin/java will be generated as one of the commands by ClientBase and submitted to YARN cluster. This will obviously fail while YARN tries to execute the container as % is treated differently in linux."], "derived": {"summary": "{code}\n    var javaCommand = \"java\"\n    val javaHome = System. getenv(\"JAVA_HOME\")\n    if ((javaHome != null && !javaHome.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "YARN ContainerLaunchContext should use cluster's JAVA_HOME - {code}\n    var javaCommand = \"java\"\n    val javaHome = System. getenv(\"JAVA_HOME\")\n    if ((javaHome != null && !javaHome."}, {"q": "What updates or decisions were made in the discussion?", "a": "I am using hadoop 2.5.0 (CDH). Agreed that it handles for windows. But the use case I am talking about is when SparkContext is created programmatically on a windows machine and is used to submit jobs on a yarn cluster running on Linux. As per above code, %JAVA_HOME%/bin/java will be generated as one of the commands by ClientBase and submitted to YARN cluster. This will obviously fail while YARN tries to execute the container as % is treated differently in linux."}]}}
{"project": "SPARK", "issue_id": "SPARK-1351", "title": "Documentation Improvements for Spark 1.0", "status": "Resolved", "priority": "Critical", "reporter": "Patrick Wendell", "assignee": "Matei Alexandru Zaharia", "labels": [], "created": "2014-03-29T21:32:06.000+0000", "updated": "2015-03-02T16:12:43.000+0000", "description": "Umbrella to track necessary doc improvements. We can break these out into other JIRA's over time.\n\n- Use grouping in the RDD and SparkContext scaladocs. See Schema RDD:\nhttp://people.apache.org/~pwendell/catalyst-docs/api/sql/core/index.html#org.apache.spark.sql.SchemaRDD\n- Use spark-submit script wherever possible in docs.\n- Have package-level documentation in Scaladoc. Also these can be grouped so that the o.a.s package doc looks nice.", "comments": ["Hi, I would like to be part of this work, \n\nI compiled a list containing all parameters currently being used in Spark and sorted in dictionary order \n\nhttps://github.com/apache/spark/pull/85\n\nAs mentioned by Patrick, we'd filter out those parameters which are not expected to be accessed by the user", "I resolved this since, well, it was targeted for 1.0.0. All but 2 issues were resolved; 1 seemed like it had no more to be done (SPARK-1858) and one I converted into a stand-alone improvement (SPARK-1564)."], "derived": {"summary": "Umbrella to track necessary doc improvements. We can break these out into other JIRA's over time.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Documentation Improvements for Spark 1.0 - Umbrella to track necessary doc improvements. We can break these out into other JIRA's over time."}, {"q": "What updates or decisions were made in the discussion?", "a": "I resolved this since, well, it was targeted for 1.0.0. All but 2 issues were resolved; 1 seemed like it had no more to be done (SPARK-1858) and one I converted into a stand-alone improvement (SPARK-1564)."}]}}
{"project": "SPARK", "issue_id": "SPARK-1352", "title": "Improve robustness of spark-submit script", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "labels": [], "created": "2014-03-30T04:58:21.000+0000", "updated": "2015-12-10T15:06:37.000+0000", "description": "After playing a bit with the spark submit script there are a few issues:\n\n- There is no way to debug the output.\n- Some simple invalid inputs like\n./spark-submit --help\n./spark-submit\ncause null pointer exceptions\n- We should give users a deprecation warning for the old scripts.", "comments": ["GitHub user pwendell opened a pull request:\n\n    https://github.com/apache/spark/pull/271\n\n    SPARK-1352: Improve robustness of spark-submit script\n\n    1. Better error messages when required arguments are missing.\n    2. Support for unit testing cases where presented arguments are invalid.\n    3. Bug fix: Only use environment varaibles when they are set (otherwise will cause NPE).\n    4. A verbose mode to aid debugging.\n    5. Visibility of several variables is set to private.\n    6. Deprecation warning for existing scripts.\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/pwendell/spark spark-submit\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/spark/pull/271.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #271\n    \n----\ncommit 9146deffea1a1afcd5bde8823fd7b06a473fb33a\nAuthor: Patrick Wendell <pwendell@gmail.com>\nDate:   2014-03-30T04:47:51Z\n\n    SPARK-1352: Improve robustness of spark-submit script\n    \n    1. Better error messages when required arguments are missing.\n    2. Support for unit testing cases where presented arguments are invalid.\n    3. Bug fix: Only use environment varaibles when they are set (otherwise will cause NPE).\n    4. A verbose mode to aid debugging.\n    5. Visibility of several variables is set to private.\n    6. Deprecation warning for existing scripts.\n\n----\n", "User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/261"], "derived": {"summary": "After playing a bit with the spark submit script there are a few issues:\n\n- There is no way to debug the output. - Some simple invalid inputs like.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Improve robustness of spark-submit script - After playing a bit with the spark submit script there are a few issues:\n\n- There is no way to debug the output. - Some simple invalid inputs like."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/261"}]}}
{"project": "SPARK", "issue_id": "SPARK-1353", "title": "IllegalArgumentException when writing to disk", "status": "Resolved", "priority": "Minor", "reporter": "Jim Blomo", "assignee": null, "labels": [], "created": "2014-03-30T06:30:56.000+0000", "updated": "2020-05-17T18:21:00.000+0000", "description": "The Executor may fail when trying to mmap a file bigger than Integer.MAX_VALUE due to the constraints of FileChannel.map (http://docs.oracle.com/javase/7/docs/api/java/nio/channels/FileChannel.html#map(java.nio.channels.FileChannel.MapMode, long, long)).  The signature takes longs, but the size value must be less than MAX_VALUE.  This manifests with the following backtrace:\n\njava.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE\n        at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:828)\n        at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:98)\n        at org.apache.spark.storage.BlockManager.doGetLocal(BlockManager.scala:337)\n        at org.apache.spark.storage.BlockManager.getLocal(BlockManager.scala:281)\n        at org.apache.spark.storage.BlockManager.get(BlockManager.scala:430)\n        at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:38)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)\n        at org.apache.spark.api.python.PythonRDD$$anon$2.run(PythonRDD.scala:85)", "comments": ["I have the same problem. spark-assembly-1.0.0-hadoop2.2.0 .\n", "This is due to limitation in spark which is being addressed in https://issues.apache.org/jira/browse/SPARK-1476.", "2014-06-16 12:27:30,910 WARN [Result resolver thread-3] [org.apache.spark.scheduler.TaskSetManager:62] - Loss was due to java.lang.IllegalArgumentException\njava.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE\n        at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:789)\n        at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:89)\n        at org.apache.spark.storage.DiskStore.getValues(DiskStore.scala:105)\n        at org.apache.spark.storage.BlockManager.getLocalFromDisk(BlockManager.scala:265)\n        at org.apache.spark.storage.BlockFetcherIterator$BasicBlockFetcherIterator$$anonfun$getLocalBlocks$1.apply(BlockFetcherIterator.scala:205)\n        at org.apache.spark.storage.BlockFetcherIterator$BasicBlockFetcherIterator$$anonfun$getLocalBlocks$1.apply(BlockFetcherIterator.scala:204)\n        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n        at org.apache.spark.storage.BlockFetcherIterator$BasicBlockFetcherIterator.getLocalBlocks(BlockFetcherIterator.scala:204)\n        at org.apache.spark.storage.BlockFetcherIterator$BasicBlockFetcherIterator.initialize(BlockFetcherIterator.scala:235)\n        at org.apache.spark.storage.BlockManager.getMultiple(BlockManager.scala:452)\n        at org.apache.spark.BlockStoreShuffleFetcher.fetch(BlockStoreShuffleFetcher.scala:77)\n        at org.apache.spark.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:130)\n        at org.apache.spark.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:121)\n        at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)\n        at scala.collection.immutable.List.foreach(List.scala:318)at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:34)\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:34)\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)\n        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:161)\n        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)\n        at org.apache.spark.scheduler.Task.run(Task.scala:53)\n        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)\n        at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)\n        at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1438)\n        at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:722)\n        at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)\n        at org.apache.spark.CoGroupedRDD.compute(CoGroupedRDD.scala:121)\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)\n        \n......\n2014-06-16 12:27:30,943 INFO [main] [org.apache.spark.scheduler.DAGScheduler:50] - Failed to run count"], "derived": {"summary": "The Executor may fail when trying to mmap a file bigger than Integer. MAX_VALUE due to the constraints of FileChannel.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "IllegalArgumentException when writing to disk - The Executor may fail when trying to mmap a file bigger than Integer. MAX_VALUE due to the constraints of FileChannel."}, {"q": "What updates or decisions were made in the discussion?", "a": "2014-06-16 12:27:30,910 WARN [Result resolver thread-3] [org.apache.spark.scheduler.TaskSetManager:62] - Loss was due to java.lang.IllegalArgumentException\njava.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE\n        at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:789)\n        at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:89)\n        at org.apache.spark.storage.DiskStore.getValues(DiskStore.scala:105)\n        at org.apache.spark.storage.BlockManager.getLocalFromDisk(BlockManager.scala:265)\n        at org.apache.spark.storage.BlockFetcherIterator$BasicBlockFetcherIterator$$anonfun$getLocalBlocks$1.apply(BlockFetcherIterator.scala:205)\n        at org.apache.spark.storage.BlockFetcherIterator$BasicBlockFetcherIterator$$anonfun$getLocalBlocks$1.apply(BlockFetcherIterator.scala:204)\n        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n        at org.apache.spark.storage.BlockFetcherIterator$BasicBlockFetcherIterator.getLocalBlocks(BlockFetcherIterator.scala:204)\n        at org.apache.spark.storage.BlockFetcherIterator$BasicBlockFetcherIterator.initialize(BlockFetcherIterator.scala:235)\n        at org.apache.spark.storage.BlockManager.getMultiple(BlockManager.scala:452)\n        at org.apache.spark.BlockStoreShuffleFetcher.fetch(BlockStoreShuffleFetcher.scala:77)\n        at org.apache.spark.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:130)\n        at org.apache.spark.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:121)\n        at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)\n        at scala.collection.immutable.List.foreach(List.scala:318)at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:34)\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:34)\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)\n        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:161)\n        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)\n        at org.apache.spark.scheduler.Task.run(Task.scala:53)\n        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)\n        at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)\n        at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1438)\n        at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:722)\n        at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)\n        at org.apache.spark.CoGroupedRDD.compute(CoGroupedRDD.scala:121)\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)\n        \n......\n2014-06-16 12:27:30,943 INFO [main] [org.apache.spark.scheduler.DAGScheduler:50] - Failed to run count"}]}}
{"project": "SPARK", "issue_id": "SPARK-1354", "title": "Fail to resolve attribute when query with table name as a qualifer in SQLContext", "status": "Resolved", "priority": "Major", "reporter": "Saisai Shao", "assignee": null, "labels": [], "created": "2014-03-30T06:58:22.000+0000", "updated": "2014-03-30T17:05:35.000+0000", "description": "For SQLContext with SimpleCatelog, table name does not register into attribute as a qualifier, so query like \"SELECT * FROM records JOIN records1 ON records.key = records1,key\" will be failed. The logical plan cannot resolve \"records.key\" because of missing qualifier \"records\". The physical plan shows as below\n\n    Project [*]\n     Filter ('records.key = 'records1.key)\n      CartesianProduct\n       ExistingRdd [key#0,value#1], MappedRDD[2] at map at basicOperators.scala:124\n       ParquetTableScan [key#2,value#3], (ParquetRelation ParquetFile, pair.parquet), None)\n\nAnd the exception shows:\n\norg.apache.spark.sql.catalyst.errors.package$TreeNodeException: No function to evaluate expression. type: UnresolvedAttribute, tree: 'records.key\n        at org.apache.spark.sql.catalyst.expressions.Expression.apply(Expression.scala:54)\n        at org.apache.spark.sql.catalyst.expressions.Equals.apply(predicates.scala:112)\n        at org.apache.spark.sql.execution.Filter$$anonfun$2$$anonfun$apply$1.apply(basicOperators.scala:43)\n        at org.apache.spark.sql.execution.Filter$$anonfun$2$$anonfun$apply$1.apply(basicOperators.scala:43)\n        at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:390)\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n        at scala.collection.Iterator$class.foreach(Iterator.scala:727)\n        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n        at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:643)\n        at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:643)\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:936)\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:936)\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)\n        at org.apache.spark.scheduler.Task.run(Task.scala:52)\n        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)\n        at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:46)\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\n        at java.lang.Thread.run(Thread.java:722)\n\n", "comments": ["GitHub user jerryshao opened a pull request:\n\n    https://github.com/apache/spark/pull/272\n\n    [SPARK-1354][SQL] Add tableName as a qualifier for SimpleCatelogy\n\n    Fix attribute unresolved when query with table name as a qualifier in SQLContext with SimplCatelog, details please see [SPARK-1354](https://issues.apache.org/jira/browse/SPARK-1354?jql=project%20%3D%20SPARK).\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/jerryshao/apache-spark qualifier-fix\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/spark/pull/272.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #272\n    \n----\ncommit 795017045a671d0f3827a97b7d57db2980e8c1fd\nAuthor: jerryshao <saisai.shao@intel.com>\nDate:   2014-03-30T02:58:21Z\n\n    Add tableName as a qualifier for SimpleCatelogy\n\n----\n"], "derived": {"summary": "For SQLContext with SimpleCatelog, table name does not register into attribute as a qualifier, so query like \"SELECT * FROM records JOIN records1 ON records. key = records1,key\" will be failed.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Fail to resolve attribute when query with table name as a qualifer in SQLContext - For SQLContext with SimpleCatelog, table name does not register into attribute as a qualifier, so query like \"SELECT * FROM records JOIN records1 ON records. key = records1,key\" will be failed."}, {"q": "What updates or decisions were made in the discussion?", "a": "GitHub user jerryshao opened a pull request:\n\n    https://github.com/apache/spark/pull/272\n\n    [SPARK-1354][SQL] Add tableName as a qualifier for SimpleCatelogy\n\n    Fix attribute unresolved when query with table name as a qualifier in SQLContext with SimplCatelog, details please see [SPARK-1354](https://issues.apache.org/jira/browse/SPARK-1354?jql=project%20%3D%20SPARK).\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/jerryshao/apache-spark qualifier-fix\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/spark/pull/272.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #272\n    \n----\ncommit 795017045a671d0f3827a97b7d57db2980e8c1fd\nAuthor: jerryshao <saisai.shao@intel.com>\nDate:   2014-03-30T02:58:21Z\n\n    Add tableName as a qualifier for SimpleCatelogy\n\n----"}]}}
{"project": "SPARK", "issue_id": "SPARK-1355", "title": "Switch website to the Apache CMS", "status": "Closed", "priority": "Major", "reporter": "Joe Schaefer", "assignee": null, "labels": [], "created": "2014-03-30T18:40:16.000+0000", "updated": "2014-04-11T22:44:11.000+0000", "description": "Jekyll is ancient history useful for small blogger sites and little else.  Why not upgrade to the Apache CMS?  It supports the same on-disk format for .md files and interfaces with pygments for code highlighting.  Thrift recently switched from nanoc to the CMS and loves it!", "comments": ["Overall we are fairly satisfied with Jekyll. Could you provide more concrete benefits for why Apache CMS would be better?", "Another thing is that we use Jekyll to make Spark's own documentation. This way anyone can download and host internal published copies of it:\nhttp://spark.incubator.apache.org/docs/latest/\n", "It will require a lot of soul searching to deal with the fact that the Apache CMS is light years more advanced than jekyll.  It's so fast you can probably throw all your existing documentation into it just for simplicity so you don't need to manage it separately, but if you did the CMS's extpaths.txt support is there to make it easy.\n\nThink about it. Discuss with your peers.  And when you are ready just let INFRA know.", "I've been down this road as well, Thrift started with jekyll for website generation and we spent more time making it not act like a blog and more like a website. We eventually switched to nanoc and had a version in testing with middleman, both of these where much easier and flexible than jekyll. After some back and forth with [~joes] and some necessary additions to meet our use case we switched to the Apache CMS. ", "Some other benefits include being able to edit your documents online as well as anonymous checkouts, meaning anyone, not just committers, can take a proper stab at fixing any aspects of your web site and/or documentation.", "People are coming out of the woodwork to recommend the Apache CMS!  You'd have to be crazy to pass on this offer of a better website for your entire community to use and enjoy.", "This decision should be driven by the project and not by infra. If people from the project want to +1 this we can make it more critical.", "It just looks funny that a cutting edge project like Spark should rely on a vanilla cookie-cutter blog-site generator like jekyll to manage its website assets.  Go for broke and grasp the brass ring- bring your website technology to new levels with the Apache CMS!", "Resources are limited as we progress toward our 1.0 release.  I can't see reallocating those commitments just to avoid looking funny in the estimation of some observers.  If someone not otherwise occupied wants to contribute the work to convert to Apache CMS, that's another thing.", "Nonesense- you have plenty of time just lack the appropriate prioritization for this task, which should be marked \"Critical\" as we are trying to help you help yourselves.  Do yourselves a solid and get it done this week to avoid further embarrassment, mkay?", "That looked more like an insult than a contribution.", "Look again- it's free advice!", "Joe, I also can't imagine how this is \"Critical\". The Apache CMS is a fine home-grown tool, but not everyone needs/wants to use it. Your comments sound odd from someone I'd imagine has been around a while. I'd suggest being constructive, as in any JIRA, by putting out the work -- patch, steps needed, details, particular arguments, etc.", "Arguments abound, patches are pointless- I'm not doing the migration, you are.  I'm giving this till COB today for someone to bump this to Critical and mean it this time.", "April Fools, apparently. Though this was opened on 30 March? ", "Meow.  Rome wasn't built in a day...", "I have to say this was pretty good, I avoided this topic on the 30th because I thought it would be messy :). Maybe this is one reason to upgrade to CMS."], "derived": {"summary": "Jekyll is ancient history useful for small blogger sites and little else. Why not upgrade to the Apache CMS?  It supports the same on-disk format for.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Switch website to the Apache CMS - Jekyll is ancient history useful for small blogger sites and little else. Why not upgrade to the Apache CMS?  It supports the same on-disk format for."}, {"q": "What updates or decisions were made in the discussion?", "a": "I have to say this was pretty good, I avoided this topic on the 30th because I thought it would be messy :). Maybe this is one reason to upgrade to CMS."}]}}
{"project": "SPARK", "issue_id": "SPARK-1356", "title": "[STREAMING] Annotate developer and experimental API's", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Tathagata Das", "labels": [], "created": "2014-03-31T00:26:43.000+0000", "updated": "2014-04-26T01:53:57.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "[STREAMING] Annotate developer and experimental API's"}]}}
{"project": "SPARK", "issue_id": "SPARK-1357", "title": "[MLLIB] Annotate developer and experimental API's", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-03-31T00:27:36.000+0000", "updated": "2014-04-09T20:43:54.000+0000", "description": null, "comments": ["I know I'm late to this party, but I just had a look and wanted to throw out a few last minute ideas.\n\n(Do you not want to just declare all of MLlib experimental? is it really 1.0? that's a fairly significant set of shackles to put on for a long time.)\n\nOK, that aside, I have two suggestions to mark as experimental:\n\n1. ALS Rating object assumes users and items are Int. I suggest it will be eventually interesting to support String, or at least switch to Long.\n\n2. Per old MLLIB-29, I feel pretty certain that ClassificationModel can't return RDD[Double], and will want to support returning a distribution over labels at some point. Similarly the input to it and RegressionModel seems like it will have to change to encompass something more than Vector to properly allow for categorical values. DecisionTreeModel has the same issue but is experimental (and doesn't integrate with these APIs?)\n\nThe point is not so much whether one agrees with these, but whether there is a non-trivial chance of wanting to change something this year.\n\nOther parts that I'm interested in personally look pretty strong. Humbly submitted.", "Hi Sean, \n\nActually, you came in just in time. This was only the first pass, and we are still accepting API visibility/annotation patches during the QA period. MLlib is still a beta component of Spark, so \"1.0\" doesn't mean it is stable. And we still accept additions (JIRA submitted before April 1) to MLlib, as Patrick announced in the dev mailing list.\n\n(I do want to mark all of MLlib experimental to reserve the right to change in the future, but we need to find a balance point here.)\n\nI agree that it is future-proof to switch id type from Int to Long in ALS. The extra storage requirement is 8 bytes per rating. Inside ALS, we also re-partition the ratings, which needs extra storage. We need to consider whether we want to switch to Long completely or provide an option to use Long ids. Could you submit a patch, either marking ALS experimental or allowing using Long ids?\n\nI don't think String type is necessary because we can alway creates a map between String ids and Long ids. A String id usually costs more than a Long id. For the same reason, classification uses Double for labels.\n\nPlease submit a patch for APIs you don't feel comfortable to say \"stable\" or marked \"experimental/developer\" by me but you think the other way. It would be great to keep the discussion going. Thanks!\n\nBest,\nXiangrui", "Yeah I think it's reasonable to say that the core ALS API is only in terms of numeric IDs and leave a higher-level translation to the caller. Longs give that much more space to hash into.\n\nThe \"cost\" in terms of memory of something like a String is just a reference, so roughly the same as a Double anyway. I think the more important question is whether Double is too hacky API-wise as a representation of fundamentally non-numeric data. That's up for debate, but yeah the question here is more about reserving the right to change.\n\nI'll submit a PR that marks the items I mention as experimental, for consideration. See if it seems reasonable."], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "[MLLIB] Annotate developer and experimental API's"}, {"q": "What updates or decisions were made in the discussion?", "a": "Yeah I think it's reasonable to say that the core ALS API is only in terms of numeric IDs and leave a higher-level translation to the caller. Longs give that much more space to hash into.\n\nThe \"cost\" in terms of memory of something like a String is just a reference, so roughly the same as a Double anyway. I think the more important question is whether Double is too hacky API-wise as a representation of fundamentally non-numeric data. That's up for debate, but yeah the question here is more about reserving the right to change.\n\nI'll submit a PR that marks the items I mention as experimental, for consideration. See if it seems reasonable."}]}}
{"project": "SPARK", "issue_id": "SPARK-1358", "title": "Continuous integrated test should be involved in Spark ecosystem ", "status": "Resolved", "priority": "Major", "reporter": "Andrew Xia", "assignee": null, "labels": [], "created": "2014-03-31T05:40:57.000+0000", "updated": "2016-01-06T09:59:48.000+0000", "description": "Currently, Spark only contains unit test and performance test, but I think it is not enough for customer to evaluate status about their cluster and spark version they  will used, and it is necessary to build continuous integrated test for spark development , it could included \n1. complex applications test cases for spark/spark streaming/graphx....\n2. stresss test cases\n3. fault tolerance test cases\n4......", "comments": ["This is a neat idea - any examples of other open source projects that have published integration tests?", "Do you mean this? Some similar project for Hadoop called bigtop (http://bigtop.apache.org/)", "I've heard these sorts of extended tests called end to end tests.  A sample one for Spark could be to stand up an HDFS cluster, load some data into it, stand up a parallel Spark cluster, read data out of HDFS, and compute some kind of aggregate.\n\nThey tend to require significant hardware though in order to run because they are much more intense and can be long-running.  [~pwendell] is that kind of extended hardware available?  I know we currently run some CI on Amplab Jenkins but I'm uncertain how much additional capacity it could support.\n\ncc [~shaneknapp]", "[~aash] -- depending on the hardware reqs, we might be able to make this happen.  we have some \"extra\" hardware, so a small (3-5 server) dedicated spark e2e test cluster could be deployed.  we're still in the process of building out our new infrastructure and will have a better idea of what we will have available for this really soon.", "ok, so i can dedicate 4 servers to this by EOY.  :)\n\nnow, let's talk hardware specs...  each machine will have 32 cores and 256G ram, but i'm curious what you guys would want for each mach's total disk space.  could you guys give me an upper and lower bound of what would be acceptable?\n\nwe've got a bunch of hardware in the lab that we can use, but we're about to embark on some (what i hope is our final) hardware config rejiggering and i am currently trying to spec out what disks go where.\n\ni will also be available to help set up and get all of this stuff running.\n\nlet me know!", "Those machines sound more than powerful enough to handle the types of long-running tests we're talking about on this ticket.\n\nAs for disk space, I've found that there's typically a decent multiplier from memory to disk, so for this hardware to be representative of what I'm used to working with I'd expect somewhere between 4 and 10TB, typically on the higher end of that scale.  We probably don't need that much for testing, but at least for my onsite use that's standard.\n\nFor reference also the AWS instances that Databricks used for the Terasort record had 8x800GB SSDs in RAID0", "8x800G SSDs sounds pretty hawt, but we're going to be on spinny disks for the test cluster...  :)\n\ni can definitely swing 4T/ea for these machs, and i'll talk to our sysadmin and see if we can squeeze another 2T in there.\n\nthanks for the info, and i'll update this ticket as i get updates.  i'm pretty certain these machs have already been moved out of our lab and in to the datacenter, which means we can get these up and running Really Soon Now[tm].", "Did this pre-date amplab Jenkins? there are already a lot of integration tests going on there, with different combinations of Hadoop versions, etc. Outside the project there are integration and stress tests going on in, say, Cloudera too. I don't know if there is a further actionable item for Spark here."], "derived": {"summary": "Currently, Spark only contains unit test and performance test, but I think it is not enough for customer to evaluate status about their cluster and spark version they  will used, and it is necessary to build continuous integrated test for spark development , it could included \n1. complex applications test cases for spark/spark streaming/graphx.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Continuous integrated test should be involved in Spark ecosystem  - Currently, Spark only contains unit test and performance test, but I think it is not enough for customer to evaluate status about their cluster and spark version they  will used, and it is necessary to build continuous integrated test for spark development , it could included \n1. complex applications test cases for spark/spark streaming/graphx."}, {"q": "What updates or decisions were made in the discussion?", "a": "Did this pre-date amplab Jenkins? there are already a lot of integration tests going on there, with different combinations of Hadoop versions, etc. Outside the project there are integration and stress tests going on in, say, Cloudera too. I don't know if there is a further actionable item for Spark here."}]}}
{"project": "SPARK", "issue_id": "SPARK-1359", "title": "SGD implementation is not efficient", "status": "Resolved", "priority": "Major", "reporter": "Xiangrui Meng", "assignee": null, "labels": ["bulk-closed"], "created": "2014-03-31T08:17:14.000+0000", "updated": "2019-05-21T04:14:46.000+0000", "description": "The SGD implementation samples a mini-batch to compute the stochastic gradient. This is not efficient because examples are provided via an iterator interface. We have to scan all of them to obtain a sample.", "comments": ["[~mengxr] If this issue is still of interest and nobody is working on it , I can start implementation.", "[~mbaddar] Since the current ann in mllib depends on `GradientDescent`, we should modify the efficienty.\nHow do we evaluate new implementation against the current implementation? And What are better tasks to evaluate it?\n- Metrics\n1. Convergence Effieiency\n2. Compute Cost\n3. Compute Time\n4. Other\n- Task\n1. Logistic Regression and Linear Regression with random generated data\n2. Logistic Regression and Linear Regression with any Kaggle data\n3. Other\n\nI make an implementation of Parallelized Stochastic Gradient Descent.\nhttps://github.com/yu-iskw/spark-parallelized-sgd", "i think by randomly shuffle partitions and do gradient Descent by toLocalIterator is better and without compromising accuracy ", "Do we care much about this now, since {{mllib}}'s SGD is in maintenance mode and currently {{ml}} supports only L-BFGS (or IRLS)? Shall we close this off?"], "derived": {"summary": "The SGD implementation samples a mini-batch to compute the stochastic gradient. This is not efficient because examples are provided via an iterator interface.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "SGD implementation is not efficient - The SGD implementation samples a mini-batch to compute the stochastic gradient. This is not efficient because examples are provided via an iterator interface."}, {"q": "What updates or decisions were made in the discussion?", "a": "Do we care much about this now, since {{mllib}}'s SGD is in maintenance mode and currently {{ml}} supports only L-BFGS (or IRLS)? Shall we close this off?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1360", "title": "Add Timestamp Support", "status": "Resolved", "priority": "Blocker", "reporter": "Cheng Hao", "assignee": "Cheng Hao", "labels": [], "created": "2014-03-31T08:32:54.000+0000", "updated": "2014-04-03T22:33:41.000+0000", "description": "Add Timestamp Support for Catalyst/SQLParser/HiveQl", "comments": ["I have created an PR to resolve this. [PR275|https://github.com/apache/spark/pull/275], but I have no idea how to assign this task to myself in the jira."], "derived": {"summary": "Add Timestamp Support for Catalyst/SQLParser/HiveQl.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add Timestamp Support - Add Timestamp Support for Catalyst/SQLParser/HiveQl."}, {"q": "What updates or decisions were made in the discussion?", "a": "I have created an PR to resolve this. [PR275|https://github.com/apache/spark/pull/275], but I have no idea how to assign this task to myself in the jira."}]}}
{"project": "SPARK", "issue_id": "SPARK-1361", "title": "DAGScheduler throws NullPointerException occasionally", "status": "Resolved", "priority": "Major", "reporter": "Fei Wang", "assignee": null, "labels": [], "created": "2014-03-31T09:10:48.000+0000", "updated": "2014-05-04T04:17:17.000+0000", "description": "DAGScheduler throws this NullPointerException below occasionally when running spark apps.\njava.lang.NullPointerException\nat org.apache.spark.scheduler.DAGScheduler.executorAdded(DAGScheduler.scala:186)\nat org.apache.spark.scheduler.TaskSchedulerImpl.executorAdded(TaskSchedulerImpl.scala:409)\nat org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$1.apply(TaskSchedulerImpl.scala:210)\nat org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$1.apply(TaskSchedulerImpl.scala:206)\nat scala.collection.mutable.ArraySeq.foreach(ArraySeq.scala:73)\nat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers(TaskSchedulerImpl.scala:206)\nat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverActor.makeOffers(CoarseGrainedSchedulerBackend.scala:130)\nat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverActor$$anonfun$receive$1.applyOrEls", "comments": ["this is an order issue of construction and starting of the taskScheduler and the dagScheduler in SparkContext.  now taskScheduler start before  dagScheduler, so executor backend may register executor even before dagScheduler(or the eventProcessActor) has not initialized. in this situation it will throw NPE", "Hi [~scwf], are there any update for this issue?\nDo you mind my taking over addressing this issue?", "Hi Kousuke Saruta, this issue has already resolved. \nhttps://github.com/apache/spark/pull/186"], "derived": {"summary": "DAGScheduler throws this NullPointerException below occasionally when running spark apps. java.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "DAGScheduler throws NullPointerException occasionally - DAGScheduler throws this NullPointerException below occasionally when running spark apps. java."}, {"q": "What updates or decisions were made in the discussion?", "a": "Hi Kousuke Saruta, this issue has already resolved. \nhttps://github.com/apache/spark/pull/186"}]}}
{"project": "SPARK", "issue_id": "SPARK-1362", "title": "Web UI should provide page of showing statistics and stage list for a given job", "status": "Resolved", "priority": "Major", "reporter": "Fei Wang", "assignee": "Josh Rosen", "labels": [], "created": "2014-03-31T09:38:27.000+0000", "updated": "2014-12-13T22:36:11.000+0000", "description": "Now spark provide the page of stage, but in spark the conception level is like this-- app > job > stage > task. Page of job is better to monitor the jobs in one app, and only page of stage we can not distinguish  jobs easily sometimes", "comments": ["This was fixed by SPARK-4145, which was included in Spark 1.2.0."], "derived": {"summary": "Now spark provide the page of stage, but in spark the conception level is like this-- app > job > stage > task. Page of job is better to monitor the jobs in one app, and only page of stage we can not distinguish  jobs easily sometimes.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Web UI should provide page of showing statistics and stage list for a given job - Now spark provide the page of stage, but in spark the conception level is like this-- app > job > stage > task. Page of job is better to monitor the jobs in one app, and only page of stage we can not distinguish  jobs easily sometimes."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was fixed by SPARK-4145, which was included in Spark 1.2.0."}]}}
{"project": "SPARK", "issue_id": "SPARK-1363", "title": "Add streaming support for Spark SQL module", "status": "Resolved", "priority": "Major", "reporter": "Saisai Shao", "assignee": "Saisai Shao", "labels": [], "created": "2014-03-31T11:09:00.000+0000", "updated": "2015-09-15T20:47:44.000+0000", "description": "Currently there exists some projects like Pig On Storm, SQL on storm (Squall, SQLstream) that can query over streaming data, but for Spark Streaming, it is a blank area. It will be a good feature to add streaming supported SQL to Spark SQL.\n\nFrom semantic perspective, DStream is quite alike RDD, they both have join, filter, groupBy operators and so on, also DStream is backed by RDD, so it is transplant-able and reusable from existing spark plan.\n\nAlso Catalyst has a clear division for each step, we can fully use its parse and logical plan analysis steps,  with only different physical plan.\n\nSo here we propose to add streaming support in Catalyst.", "comments": ["Here is is our early version of design doc, details will be changes. And our develop branch is https://github.com/thunderain-project/StreamSQL, it is greatly appreciated if you can give us some feedbacks and comments.", "Thanks for publishing the code!  Would it be possible to recreate the repository as a fork of apache/spark?  That would make it much easier to diff the branches and eventually make a pull request.", "[~marmbrus],  It was originated from the apache/spark repo. This link https://github.com/thunderain-project/StreamSQL/compare/apache:master...streamsql-dev  shows the diffs more clearly. ", "Hi Michael, thanks for your advice, I've recreated the repo as a fork of apache/spark. It is greatly helpful if you can give me some comments. Thanks a lot."], "derived": {"summary": "Currently there exists some projects like Pig On Storm, SQL on storm (Squall, SQLstream) that can query over streaming data, but for Spark Streaming, it is a blank area. It will be a good feature to add streaming supported SQL to Spark SQL.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add streaming support for Spark SQL module - Currently there exists some projects like Pig On Storm, SQL on storm (Squall, SQLstream) that can query over streaming data, but for Spark Streaming, it is a blank area. It will be a good feature to add streaming supported SQL to Spark SQL."}, {"q": "What updates or decisions were made in the discussion?", "a": "Hi Michael, thanks for your advice, I've recreated the repo as a fork of apache/spark. It is greatly helpful if you can give me some comments. Thanks a lot."}]}}
{"project": "SPARK", "issue_id": "SPARK-1364", "title": "DataTypes missing from ScalaReflection", "status": "Resolved", "priority": "Blocker", "reporter": "Michael Armbrust", "assignee": "Michael Armbrust", "labels": [], "created": "2014-03-31T16:06:16.000+0000", "updated": "2014-04-03T01:15:18.000+0000", "description": "BigDecimal, possibly others.", "comments": ["https://github.com/apache/spark/pull/293"], "derived": {"summary": "BigDecimal, possibly others.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "DataTypes missing from ScalaReflection - BigDecimal, possibly others."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/293"}]}}
{"project": "SPARK", "issue_id": "SPARK-1365", "title": "Fix RateLimitedOutputStream Test", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "labels": [], "created": "2014-03-31T17:40:52.000+0000", "updated": "2014-03-31T23:26:54.000+0000", "description": "This test currently assumes that calling Thread.sleep(X) will sleep exactly X milliseconds. In fact, it can sleep X or more milliseconds so sometimes the test fails.", "comments": [], "derived": {"summary": "This test currently assumes that calling Thread. sleep(X) will sleep exactly X milliseconds.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Fix RateLimitedOutputStream Test - This test currently assumes that calling Thread. sleep(X) will sleep exactly X milliseconds."}]}}
{"project": "SPARK", "issue_id": "SPARK-1366", "title": "The sql function should be consistent between different types of SQLContext", "status": "Resolved", "priority": "Blocker", "reporter": "Michael Armbrust", "assignee": "Michael Armbrust", "labels": [], "created": "2014-03-31T17:47:42.000+0000", "updated": "2014-04-05T20:09:42.000+0000", "description": "Right now calling `context.sql` will cause things to be parsed with different parsers, which is kinda confusing. Instead HiveContext should have a specialized `hiveql` method that uses the HiveQL parser.\n\nAlso need to update the documentation.", "comments": ["https://github.com/apache/spark/pull/319"], "derived": {"summary": "Right now calling `context. sql` will cause things to be parsed with different parsers, which is kinda confusing.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "The sql function should be consistent between different types of SQLContext - Right now calling `context. sql` will cause things to be parsed with different parsers, which is kinda confusing."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/319"}]}}
{"project": "SPARK", "issue_id": "SPARK-1367", "title": "NPE when joining Parquet Relations", "status": "Resolved", "priority": "Blocker", "reporter": "Michael Armbrust", "assignee": "Andre Schumacher", "labels": [], "created": "2014-03-31T17:51:57.000+0000", "updated": "2014-04-03T22:33:26.000+0000", "description": "{code}\n  test(\"self-join parquet files\") {\n    val x = ParquetTestData.testData.subquery('x)\n    val y = ParquetTestData.testData.newInstance.subquery('y)\n    val query = x.join(y).where(\"x.myint\".attr === \"y.myint\".attr)\n    query.collect()\n  }\n{code}", "comments": ["I believe this issue can be closed due to\nhttps://github.com/apache/spark/commit/2861b07bb030f72769f5b757b4a7d4a635807140\n?", "No, in that commit there is a TODO as the testcase still NPEs.  We still need to remove the @transient from ParquetTableScan.  If you don't have time to do this I can.", "That should be now fixed when https://github.com/apache/spark/pull/195 is merged. Sorry, since the test you had added did not actually failed (as I understood the TODO) I thought your patch had solved the problem."], "derived": {"summary": "{code}\n  test(\"self-join parquet files\") {\n    val x = ParquetTestData. testData.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "NPE when joining Parquet Relations - {code}\n  test(\"self-join parquet files\") {\n    val x = ParquetTestData. testData."}, {"q": "What updates or decisions were made in the discussion?", "a": "That should be now fixed when https://github.com/apache/spark/pull/195 is merged. Sorry, since the test you had added did not actually failed (as I understood the TODO) I thought your patch had solved the problem."}]}}
{"project": "SPARK", "issue_id": "SPARK-1368", "title": "HiveTableScan is slow", "status": "Resolved", "priority": "Major", "reporter": "Michael Armbrust", "assignee": "Cheng Lian", "labels": [], "created": "2014-03-31T17:53:52.000+0000", "updated": "2014-05-29T22:27:13.000+0000", "description": "The major issues here are the use of functional programming (.map, .foreach) and the creation of a new Row object for each output tuple. We should switch to while loops in the critical path and a single MutableRow per partition.", "comments": ["Corresponding PR: https://github.com/apache/spark/pull/758"], "derived": {"summary": "The major issues here are the use of functional programming (. map,.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "HiveTableScan is slow - The major issues here are the use of functional programming (. map,."}, {"q": "What updates or decisions were made in the discussion?", "a": "Corresponding PR: https://github.com/apache/spark/pull/758"}]}}
{"project": "SPARK", "issue_id": "SPARK-1369", "title": "HiveUDF wrappers are slow", "status": "Resolved", "priority": "Major", "reporter": "Michael Armbrust", "assignee": null, "labels": [], "created": "2014-03-31T17:54:41.000+0000", "updated": "2015-09-15T20:42:32.000+0000", "description": "The major issues here are that we are using a lot of functional programming (.map) and creating new writeable objects for each input row. We should switch to while loops and reuse the writeable objects when possible.", "comments": ["As we add more and more native functions this is less important."], "derived": {"summary": "The major issues here are that we are using a lot of functional programming (. map) and creating new writeable objects for each input row.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "HiveUDF wrappers are slow - The major issues here are that we are using a lot of functional programming (. map) and creating new writeable objects for each input row."}, {"q": "What updates or decisions were made in the discussion?", "a": "As we add more and more native functions this is less important."}]}}
{"project": "SPARK", "issue_id": "SPARK-1370", "title": "HashJoin should stream one relation", "status": "Resolved", "priority": "Major", "reporter": "Michael Armbrust", "assignee": "Michael Armbrust", "labels": [], "created": "2014-03-31T18:16:39.000+0000", "updated": "2014-03-31T22:27:56.000+0000", "description": "Right now we materialize all of the results for a partition in memory.\n\nThis has been started here: https://github.com/apache/spark/pull/250", "comments": [], "derived": {"summary": "Right now we materialize all of the results for a partition in memory. This has been started here: https://github.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "HashJoin should stream one relation - Right now we materialize all of the results for a partition in memory. This has been started here: https://github."}]}}
{"project": "SPARK", "issue_id": "SPARK-1371", "title": "HashAggregate should stream tuples and avoid doing an extra count", "status": "Resolved", "priority": "Blocker", "reporter": "Michael Armbrust", "assignee": "Michael Armbrust", "labels": [], "created": "2014-03-31T18:17:19.000+0000", "updated": "2014-04-07T07:32:05.000+0000", "description": null, "comments": ["https://github.com/apache/spark/pull/295"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "HashAggregate should stream tuples and avoid doing an extra count"}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/295"}]}}
{"project": "SPARK", "issue_id": "SPARK-1372", "title": "Expose in-memory columnar caching for tables.", "status": "Resolved", "priority": "Blocker", "reporter": "Michael Armbrust", "assignee": "Michael Armbrust", "labels": [], "created": "2014-03-31T18:36:42.000+0000", "updated": "2014-04-02T05:44:00.000+0000", "description": null, "comments": ["https://github.com/apache/spark/pull/282"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Expose in-memory columnar caching for tables."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/282"}]}}
{"project": "SPARK", "issue_id": "SPARK-1373", "title": "Compression for In-Memory Columnar storage", "status": "Resolved", "priority": "Blocker", "reporter": "Michael Armbrust", "assignee": "Cheng Lian", "labels": [], "created": "2014-03-31T18:40:46.000+0000", "updated": "2014-04-30T00:15:04.000+0000", "description": null, "comments": ["https://github.com/apache/spark/pull/285", "Note that the code is in a large part adapted from Shark, including: https://github.com/amplab/shark/blob/master/src/test/scala/shark/memstore2/column/CompressionAlgorithmSuite.scala"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Compression for In-Memory Columnar storage"}, {"q": "What updates or decisions were made in the discussion?", "a": "Note that the code is in a large part adapted from Shark, including: https://github.com/amplab/shark/blob/master/src/test/scala/shark/memstore2/column/CompressionAlgorithmSuite.scala"}]}}
{"project": "SPARK", "issue_id": "SPARK-1374", "title": "Python API for running SQL queries", "status": "Resolved", "priority": "Blocker", "reporter": "Michael Armbrust", "assignee": "Ahir Reddy", "labels": [], "created": "2014-03-31T18:42:34.000+0000", "updated": "2014-04-15T07:08:31.000+0000", "description": null, "comments": ["This is dependent on [SPARK-1333]", "https://github.com/apache/spark/pull/363"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Python API for running SQL queries"}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/363"}]}}
{"project": "SPARK", "issue_id": "SPARK-1375", "title": "spark-submit script additional cleanup", "status": "Resolved", "priority": "Major", "reporter": "Sandy Ryza", "assignee": "Sandy Ryza", "labels": [], "created": "2014-03-31T20:07:47.000+0000", "updated": "2014-04-04T20:29:13.000+0000", "description": "A couple small things:\n* accept \"yarn-cluster\" where \"yarn-standalone\" is accepted\n* As discussed on the SPARK-1126 PR, call the first argument \"app jar\" instead of \"jar\" or \"primary binary\"", "comments": ["https://github.com/apache/spark/pull/278"], "derived": {"summary": "A couple small things:\n* accept \"yarn-cluster\" where \"yarn-standalone\" is accepted\n* As discussed on the SPARK-1126 PR, call the first argument \"app jar\" instead of \"jar\" or \"primary binary\".", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "spark-submit script additional cleanup - A couple small things:\n* accept \"yarn-cluster\" where \"yarn-standalone\" is accepted\n* As discussed on the SPARK-1126 PR, call the first argument \"app jar\" instead of \"jar\" or \"primary binary\"."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/278"}]}}
{"project": "SPARK", "issue_id": "SPARK-1376", "title": "In the yarn-cluster submitter, rename \"args\" option to \"arg\"", "status": "Resolved", "priority": "Major", "reporter": "Sandy Ryza", "assignee": "Sandy Ryza", "labels": [], "created": "2014-03-31T20:18:18.000+0000", "updated": "2014-04-23T02:39:20.000+0000", "description": "This was discussed in the SPARK-1126 PR.  \"args\" will be kept around for backwards compatibility.", "comments": ["https://github.com/apache/spark/pull/279", "There was follow up work on this by [~mengxr] in https://github.com/apache/spark/pull/485"], "derived": {"summary": "This was discussed in the SPARK-1126 PR. \"args\" will be kept around for backwards compatibility.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "In the yarn-cluster submitter, rename \"args\" option to \"arg\" - This was discussed in the SPARK-1126 PR. \"args\" will be kept around for backwards compatibility."}, {"q": "What updates or decisions were made in the discussion?", "a": "There was follow up work on this by [~mengxr] in https://github.com/apache/spark/pull/485"}]}}
{"project": "SPARK", "issue_id": "SPARK-1377", "title": "Upgrade Jetty to 8.1.14v20131031", "status": "Resolved", "priority": "Major", "reporter": "Andrew Or", "assignee": null, "labels": [], "created": "2014-03-31T22:26:50.000+0000", "updated": "2014-11-05T10:43:31.000+0000", "description": "Previous version was 7.6.8v20121106. The only difference between Jetty 7 and Jetty 8 is that the former uses Servlet API 2.5, while the latter uses Servlet API 3.0.", "comments": [], "derived": {"summary": "Previous version was 7. 6.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Upgrade Jetty to 8.1.14v20131031 - Previous version was 7. 6."}]}}
{"project": "SPARK", "issue_id": "SPARK-1378", "title": "Build error: org.eclipse.paho:mqtt-client", "status": "Resolved", "priority": "Major", "reporter": "Ken Williams", "assignee": null, "labels": [], "created": "2014-03-31T22:29:19.000+0000", "updated": "2015-11-05T15:22:33.000+0000", "description": "Using Maven, I'm unable to build the 0.9.0 distribution I just downloaded. I attempt like so:\n\n{code}\nmvn -U -Pyarn -Dhadoop.version=2.2.0 -Dyarn.version=2.2.0 -DskipTests package\n{code}\n\nThe Maven error is:\n\n{code}\n[ERROR] Failed to execute goal on project spark-examples_2.10: Could not resolve dependencies for project org.apache.spark:spark-examples_2.10:jar:0.9.0-incubating: Could not find artifact org.eclipse.paho:mqtt-client:jar:0.4.0 in nexus\n{code}\n\nMy Maven version is 3.2.1, running on Java 1.7.0, using Scala 2.10.4.\nIs there an additional Maven repository I should add or something?\n\nIf I go into the {{pom.xml}} and comment out the {{external/mqtt}} and {{examples}} modules, the build succeeds. I'm fine without the MQTT stuff, but I would really like to get the examples working because I haven't played with Spark before.", "comments": ["This was issue https://spark-project.atlassian.net/browse/SPARK-1339 in the previous Jira instance.  This is my third try submitting this ticket, the second one somehow ended up in Mahout's queue.", "FWIW this builds fine for me from master right now. Can you try that?\n\nThis was observed recently when the Cloudera repo choked. It's OK now, but I suppose it could be caused by network problems or proxies. I don't suppose those are a factor?\n\nFinally, there was a change to improve the repo config after the incident above. That may happen to avoid whatever you're experiencing with 0.9.0. Hence, this may already be fixed, and that can be checked by building master.", "I've had other people tell me this is an issue as well when building behind a corporate firewall. Unfortunately don't have any more information than that at this point :(", "Try this [PR25|https://github.com/apache/spark/pull/25], you should be able to solve the problem", "I just checked out master and attempted a build again, and got the same failure.\n\nI'm behind a corporate firewall, but we don't use a proxy so I don't think that should be an issue.\n\nI'll try the PR25 patch, but I couldn't help noticing that @srowen didn't seem to like the approach.", "In that case, it seemed that it was clearly a proxy issue since adding proxy settings resolved the problem. That much is fine but not something to put in the general build. It also wouldn't help to add another repo... er, well, it's better to simply configure for access to the standard repos.\n\nTry building the examples module (that's where it fails right?) with \"-X\" to output a lot more debug info. It is probably saying in there somewhere why it can't access the repos, and there may be a clue there. For example do you have anything that would proxy an HTTPS connection? that could break access.\n\nAlso use \"-U\" to make sure it is not caching lookup failures from previous runs.\n\nIf you have a home machine or can try from home that might also rule out network issues.", "I commented over on that PR - the patch no longer applies, and it looks like the repo is now defined in the main {{pom.xml}} anyway.\n\nFor now I have a private branch against {{v0.9.0-incubating}} where I'm commenting out the 2 mentions of MQTT in the pom files, and deleting the {{examples/src/main/scala/org/apache/spark/streaming/examples/MQTTWordCount.scala}} file.  The build goes forward fine with these changes.", "Thanks Sean - for me the failure is actually in the \"Spark Project External MQTT\" phase, and then it skips the \"Spark Project Examples\" phase because of the failure.\n\nI'm indeed using the {{-U}} flag with my build, and just to be sure I nuked {{~/.m2/repository/org/eclipse/paho/mqtt-client}} before running.\n\nI ran {{mvn -X -U -Pyarn -Dhadoop.version=2.2.0 -Dyarn.version=2.2.0 -DskipTests --projects external/mqtt package}} and got reams of output - let me parse through that to see if I can see the problem, if not I'll stick it in a Gist and ask for help.", "Significant update - I tried moving away my {{~/.m2/settings.xml}} to take it out of the equation, and everything worked.  So it looks like for me, the problem is local too, chalk it up to my inexperience with Maven I guess.\n\nThanks.", "A tip: It can be useful to create a file called something like ~/.m2/empty-settings.xml that contains nothing but <settings></settings>.  Then you can test a build with no local settings interference via 'mvn -s ~/.m2/empty-settings.xml ...'", "I imagine it is something to do with a proxy setting or repo defined in that file. Maybe you can't share them, but if you suspect one in particular was the cause, that could be a helpful point of reference for anyone that might encounter this later. ", "What resolved it on our end was to either run without a local MVN repo (moving the {{~/.m2/settings.xml}} out of the way) or adding the mqtt-repo (https://repo.eclipse.org/content/repositories/paho-releases) to our set of mirrors.", "This seems like still an issue:\nDownloading: https://repository.apache.org/content/repositories/releases/org/eclipse/paho/mqtt-client/0.4.0/mqtt-client-0.4.0.pom\nJul 3, 2014 6:22:27 PM org.apache.commons.httpclient.HttpMethodDirector executeWithRetry\nINFO: I/O exception (java.net.ConnectException) caught when processing request: Connection timed out\nJul 3, 2014 6:22:27 PM org.apache.commons.httpclient.HttpMethodDirector executeWithRetry\nINFO: Retrying request\nIt is failing to download.. I am running behind corporate firewall not sure if it has anything to do with that. I had my build stuck exactly like this earlier in the build process trying to download scala compiler jar file but after a few attempts it was able proceed/download the file. seems like repo issue.", "I had the same issue behind a corporate proxy with Nexus, here is my {~/.m2/settings.xml} that solves the issue:\n{code}\n<settings>\n  <mirrors>\n    <mirror>\n      <!--Spark compilation bug -->\n      <id>eclipse</id>\n      <mirrorOf>mqtt-repo</mirrorOf>\n      <url>http://proxy/nexus/content/repositories/eclipse-paho</url>\n    </mirror>\n    <mirror>\n      <!--This sends everything else to /public -->\n      <id>nexus</id>\n      <mirrorOf>*</mirrorOf>\n      <url>http://proxy/nexus/content/groups/public</url>\n    </mirror>\n  </mirrors>\n  <profiles>\n    <profile>\n      <id>nexus</id>\n      <!--Enable snapshots for the built in central repo to direct -->\n      <!--all requests to nexus via the mirror -->\n      <repositories>\n        <repository>\n          <id>central</id>\n          <url>http://central</url>\n          <releases><enabled>true</enabled></releases>\n          <snapshots><enabled>true</enabled></snapshots>\n        </repository>\n      </repositories>\n     <pluginRepositories>\n        <pluginRepository>\n          <id>central</id>\n          <url>http://central</url>\n          <releases><enabled>true</enabled></releases>\n          <snapshots><enabled>true</enabled></snapshots>\n        </pluginRepository>\n      </pluginRepositories>\n    </profile>\n  </profiles>\n  <activeProfiles>\n    <!--make the profile active all the time -->\n    <activeProfile>nexus</activeProfile>\n  </activeProfiles>\n</settings>\n{code}\n\nIt seems that there is a bug (maybe in Nexus), that blocks solving for dependencies of mqtt-repo in http://proxy/nexus/content/groups/public\nI hope my settings could help others."], "derived": {"summary": "Using Maven, I'm unable to build the 0. 9.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Build error: org.eclipse.paho:mqtt-client - Using Maven, I'm unable to build the 0. 9."}, {"q": "What updates or decisions were made in the discussion?", "a": "I had the same issue behind a corporate proxy with Nexus, here is my {~/.m2/settings.xml} that solves the issue:\n{code}\n<settings>\n  <mirrors>\n    <mirror>\n      <!--Spark compilation bug -->\n      <id>eclipse</id>\n      <mirrorOf>mqtt-repo</mirrorOf>\n      <url>http://proxy/nexus/content/repositories/eclipse-paho</url>\n    </mirror>\n    <mirror>\n      <!--This sends everything else to /public -->\n      <id>nexus</id>\n      <mirrorOf>*</mirrorOf>\n      <url>http://proxy/nexus/content/groups/public</url>\n    </mirror>\n  </mirrors>\n  <profiles>\n    <profile>\n      <id>nexus</id>\n      <!--Enable snapshots for the built in central repo to direct -->\n      <!--all requests to nexus via the mirror -->\n      <repositories>\n        <repository>\n          <id>central</id>\n          <url>http://central</url>\n          <releases><enabled>true</enabled></releases>\n          <snapshots><enabled>true</enabled></snapshots>\n        </repository>\n      </repositories>\n     <pluginRepositories>\n        <pluginRepository>\n          <id>central</id>\n          <url>http://central</url>\n          <releases><enabled>true</enabled></releases>\n          <snapshots><enabled>true</enabled></snapshots>\n        </pluginRepository>\n      </pluginRepositories>\n    </profile>\n  </profiles>\n  <activeProfiles>\n    <!--make the profile active all the time -->\n    <activeProfile>nexus</activeProfile>\n  </activeProfiles>\n</settings>\n{code}\n\nIt seems that there is a bug (maybe in Nexus), that blocks solving for dependencies of mqtt-repo in http://proxy/nexus/content/groups/public\nI hope my settings could help others."}]}}
{"project": "SPARK", "issue_id": "SPARK-1379", "title": "Calling .cache() on a SchemaRDD should do something more efficient than caching the individual row objects.", "status": "Resolved", "priority": "Major", "reporter": "Michael Armbrust", "assignee": "Michael Armbrust", "labels": [], "created": "2014-04-01T02:30:40.000+0000", "updated": "2014-10-03T19:35:56.000+0000", "description": "Since rows aren't black boxes we could use InMemoryColumnarTableScan.  This would significantly reduce GC pressure on the workers.", "comments": [], "derived": {"summary": "Since rows aren't black boxes we could use InMemoryColumnarTableScan. This would significantly reduce GC pressure on the workers.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Calling .cache() on a SchemaRDD should do something more efficient than caching the individual row objects. - Since rows aren't black boxes we could use InMemoryColumnarTableScan. This would significantly reduce GC pressure on the workers."}]}}
{"project": "SPARK", "issue_id": "SPARK-1380", "title": "Add sort-merge based cogroup/joins.", "status": "Resolved", "priority": "Major", "reporter": "Takuya Ueshin", "assignee": null, "labels": [], "created": "2014-04-01T08:33:00.000+0000", "updated": "2014-12-10T15:57:25.000+0000", "description": "I've written cogroup/joins based on 'Sort-Merge' algorithm.", "comments": ["Pull-requested: https://github.com/apache/spark/pull/283", "The PR discussion suggests this is WontFix."], "derived": {"summary": "I've written cogroup/joins based on 'Sort-Merge' algorithm.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add sort-merge based cogroup/joins. - I've written cogroup/joins based on 'Sort-Merge' algorithm."}, {"q": "What updates or decisions were made in the discussion?", "a": "The PR discussion suggests this is WontFix."}]}}
{"project": "SPARK", "issue_id": "SPARK-1381", "title": "Spark to Shark direct streaming", "status": "Resolved", "priority": "Major", "reporter": "Abhishek Tripathi", "assignee": null, "labels": ["performance"], "created": "2014-04-01T13:16:34.000+0000", "updated": "2014-09-29T08:00:27.000+0000", "description": "Hi,\nI'm trying to push data coming from Spark streaming to Shark cache table. \nI thought of using JDBC API but Shark(0.81) does not support direct insert statement  i.e \"    insert into emp values(2, \"Apia\")  \".\nI don't want to store Spark streaming into HDFS  and then copy that data to Shark table.\nCan somebody plz help\n1.  how can I directly point Spark streaming data to Shark table/cachedTable ? otherway how can Shark pickup data directly from Spark streaming? \n2. Does Shark0.81 has direct insert statement without referring to other table?\n\nIt is really stopping us to use Spark further more. need your assistant urgently.\n\nThanks in advance.\nAbhishek\n", "comments": ["It sounds like this is \"WontFix\" at this point, if there was a problem to begin with, as Shark is deprecated."], "derived": {"summary": "Hi,\nI'm trying to push data coming from Spark streaming to Shark cache table. I thought of using JDBC API but Shark(0.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Spark to Shark direct streaming - Hi,\nI'm trying to push data coming from Spark streaming to Shark cache table. I thought of using JDBC API but Shark(0."}, {"q": "What updates or decisions were made in the discussion?", "a": "It sounds like this is \"WontFix\" at this point, if there was a problem to begin with, as Shark is deprecated."}]}}
{"project": "SPARK", "issue_id": "SPARK-1382", "title": "NullPointerException when calling DStream.slice() before StreamingContext.start()", "status": "Resolved", "priority": "Minor", "reporter": "Alessandro Chacn", "assignee": "Shixiong Zhu", "labels": [], "created": "2014-04-01T14:12:28.000+0000", "updated": "2014-04-27T14:07:32.000+0000", "description": "If we call the DStream.slice() before StreamingContext.start() has been called, then zeroTime is still null, and it will throw a null pointer exception. Ideally, it should throw something like a \"ContextNotInitlalized\" exception.", "comments": ["PR: https://github.com/apache/spark/pull/365", "PR: https://github.com/apache/spark/pull/562"], "derived": {"summary": "If we call the DStream. slice() before StreamingContext.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "NullPointerException when calling DStream.slice() before StreamingContext.start() - If we call the DStream. slice() before StreamingContext."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR: https://github.com/apache/spark/pull/562"}]}}
{"project": "SPARK", "issue_id": "SPARK-1383", "title": "Spark-SQL: ParquetRelation improvements", "status": "Resolved", "priority": "Major", "reporter": "Andre Schumacher", "assignee": "Andre Schumacher", "labels": [], "created": "2014-04-01T14:44:13.000+0000", "updated": "2014-04-04T17:13:52.000+0000", "description": "Improve Spark-SQL's ParquetRelation as follows:\n- Instead of files a ParquetRelation is should be backed by a directory, which simplifies importing data from other sources\n- InsertIntoParquetTable operation should supports switching between overwriting or appending (at least in HiveQL)\n- tests should use the new API\n- Parquet logging should be forwarded to Log4J\n- It should be possible to enable compression (default compression for Parquet files: GZIP, as in parquet-mr)\n- OverwriteCatalog should support dropping of tables\n\n", "comments": ["Fixed by https://github.com/apache/spark/commit/fbebaedf26286ee8a75065822a3af1148351f828"], "derived": {"summary": "Improve Spark-SQL's ParquetRelation as follows:\n- Instead of files a ParquetRelation is should be backed by a directory, which simplifies importing data from other sources\n- InsertIntoParquetTable operation should supports switching between overwriting or appending (at least in HiveQL)\n- tests should use the new API\n- Parquet logging should be forwarded to Log4J\n- It should be possible to enable compression (default compression for Parquet files: GZIP, as in parquet-mr)\n- OverwriteCatalog should support dropping of tables.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Spark-SQL: ParquetRelation improvements - Improve Spark-SQL's ParquetRelation as follows:\n- Instead of files a ParquetRelation is should be backed by a directory, which simplifies importing data from other sources\n- InsertIntoParquetTable operation should supports switching between overwriting or appending (at least in HiveQL)\n- tests should use the new API\n- Parquet logging should be forwarded to Log4J\n- It should be possible to enable compression (default compression for Parquet files: GZIP, as in parquet-mr)\n- OverwriteCatalog should support dropping of tables."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed by https://github.com/apache/spark/commit/fbebaedf26286ee8a75065822a3af1148351f828"}]}}
{"project": "SPARK", "issue_id": "SPARK-1384", "title": "spark-shell on yarn on spark 0.9 branch doesn't always work with secure hdfs", "status": "Resolved", "priority": "Major", "reporter": "Thomas Graves", "assignee": "Thomas Graves", "labels": [], "created": "2014-04-01T17:23:24.000+0000", "updated": "2014-06-10T06:08:05.000+0000", "description": " I've found an issue with the spark-shell in yarn-client mode in the 0.9.1 rc3 release.  It doesn't work with secure HDFS unless you \nexport SPARK_YARN_MODE=true before starting the shell, or if you happen to do something immediately with HDFS.  If you wait for the connection to the namenode to timeout it will fail. \n \nThe fix actually went in to master branch  with the authentication changes I made in master but I never realized that change needed to apply to 0.9. \n\nhttps://github.com/apache/spark/commit/7edbea41b43e0dc11a2de156be220db8b7952d01#diff-0ae5b834ce90ec37c19af35aa7a5e1a0\nSee the SparkILoop diff.\n", "comments": ["https://github.com/apache/spark/pull/287"], "derived": {"summary": "I've found an issue with the spark-shell in yarn-client mode in the 0. 9.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "spark-shell on yarn on spark 0.9 branch doesn't always work with secure hdfs - I've found an issue with the spark-shell in yarn-client mode in the 0. 9."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/287"}]}}
{"project": "SPARK", "issue_id": "SPARK-1385", "title": "Use existing code-path for JSON de/serialization of BlockId", "status": "Resolved", "priority": "Minor", "reporter": "Andrew Or", "assignee": null, "labels": [], "created": "2014-04-01T20:33:51.000+0000", "updated": "2014-12-10T15:56:11.000+0000", "description": "BlockId.scala already takes care of JSON de/serialization by parsing the string to and from regex. This functionality is currently duplicated in util/JsonProtocol.scala.", "comments": ["PR is https://github.com/apache/spark/pull/289. This was merged in https://github.com/apache/spark/commit/de8eefa804e229635eaa29a78b9e9ce161ac58e1"], "derived": {"summary": "BlockId. scala already takes care of JSON de/serialization by parsing the string to and from regex.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Use existing code-path for JSON de/serialization of BlockId - BlockId. scala already takes care of JSON de/serialization by parsing the string to and from regex."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR is https://github.com/apache/spark/pull/289. This was merged in https://github.com/apache/spark/commit/de8eefa804e229635eaa29a78b9e9ce161ac58e1"}]}}
{"project": "SPARK", "issue_id": "SPARK-1386", "title": "Spark Streaming UI", "status": "Resolved", "priority": "Blocker", "reporter": "Tathagata Das", "assignee": "Tathagata Das", "labels": [], "created": "2014-04-01T20:46:47.000+0000", "updated": "2014-04-13T03:55:35.000+0000", "description": "When debugging Spark Streaming applications it is necessary to monitor certain metrics that are not shown in the Spark application UI. For example, what is average processing time of batches? What is the scheduling delay? Is the system able to process as fast as it is receiving data? How many records I am receiving through my receivers? \n\nWhile the StreamingListener interface introduced in the 0.9 provided some of this information, it could only be accessed programmatically. A UI that shows information specific to the streaming applications is necessary for easier debugging.", "comments": [], "derived": {"summary": "When debugging Spark Streaming applications it is necessary to monitor certain metrics that are not shown in the Spark application UI. For example, what is average processing time of batches? What is the scheduling delay? Is the system able to process as fast as it is receiving data? How many records I am receiving through my receivers? \n\nWhile the StreamingListener interface introduced in the 0.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Spark Streaming UI - When debugging Spark Streaming applications it is necessary to monitor certain metrics that are not shown in the Spark application UI. For example, what is average processing time of batches? What is the scheduling delay? Is the system able to process as fast as it is receiving data? How many records I am receiving through my receivers? \n\nWhile the StreamingListener interface introduced in the 0."}]}}
{"project": "SPARK", "issue_id": "SPARK-1387", "title": "Update build plugins,  avoid plugin version warning, centralize versions", "status": "Resolved", "priority": "Minor", "reporter": "Sean R. Owen", "assignee": "Sean R. Owen", "labels": [], "created": "2014-04-01T21:28:50.000+0000", "updated": "2014-04-07T00:42:14.000+0000", "description": "Another handful of small build changes to organize and standardize a bit, and avoid warnings:\n\n- Update Maven plugin versions for good measure\n- Since plugins need maven 3.0.4 already, require it explicitly (<3.0.4 had some bugs anyway)\n- Use variables to define versions across dependencies where they should move in lock step\n- ... and make this consistent between Maven/SBT\n", "comments": [], "derived": {"summary": "Another handful of small build changes to organize and standardize a bit, and avoid warnings:\n\n- Update Maven plugin versions for good measure\n- Since plugins need maven 3. 0.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Update build plugins,  avoid plugin version warning, centralize versions - Another handful of small build changes to organize and standardize a bit, and avoid warnings:\n\n- Update Maven plugin versions for good measure\n- Since plugins need maven 3. 0."}]}}
{"project": "SPARK", "issue_id": "SPARK-1388", "title": "ConcurrentModificationException in hadoop_common exposed by Spark", "status": "Resolved", "priority": "Major", "reporter": "Nishkam Ravi", "assignee": null, "labels": [], "created": "2014-04-02T01:15:41.000+0000", "updated": "2014-06-06T19:56:31.000+0000", "description": "The following exception occurs non-deterministically:\n\njava.util.ConcurrentModificationException\n        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:926)\n        at java.util.HashMap$KeyIterator.next(HashMap.java:960)\n        at java.util.AbstractCollection.addAll(AbstractCollection.java:341)\n        at java.util.HashSet.<init>(HashSet.java:117)\n        at org.apache.hadoop.conf.Configuration.<init>(Configuration.java:671)\n        at org.apache.hadoop.mapred.JobConf.<init>(JobConf.java:439)\n        at org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:110)\n        at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:154)\n        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:149)\n        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:64)\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)\n        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)\n        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)\n        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:34)\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)\n        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:161)\n        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)\n        at org.apache.spark.scheduler.Task.run(Task.scala:53)\n        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)\n        at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)\n        at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n        at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:744)\n", "comments": ["Here is a simple fix for this issue (patch attached). Verified with mvn compile, mvn test and mvn install. \nThis issue may be identical to SPARK-1097. ", "Yes this should be resolved as a duplicate instead.", "+1 for resolving"], "derived": {"summary": "The following exception occurs non-deterministically:\n\njava. util.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "ConcurrentModificationException in hadoop_common exposed by Spark - The following exception occurs non-deterministically:\n\njava. util."}, {"q": "What updates or decisions were made in the discussion?", "a": "+1 for resolving"}]}}
{"project": "SPARK", "issue_id": "SPARK-1389", "title": "Make numPartitions in Exchange configurable", "status": "Resolved", "priority": "Major", "reporter": "Michael Armbrust", "assignee": "Michael Armbrust", "labels": [], "created": "2014-04-02T05:22:28.000+0000", "updated": "2014-09-16T16:11:39.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Make numPartitions in Exchange configurable"}]}}
{"project": "SPARK", "issue_id": "SPARK-1390", "title": "Refactor RDD backed matrices", "status": "Resolved", "priority": "Blocker", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-04-02T05:30:34.000+0000", "updated": "2014-04-09T06:02:58.000+0000", "description": "The current interfaces of RDD backed matrices needs refactoring for v1.0 release. It would be better if we have a clear separation of local matrices and those backed by RDD. Right now, we have \n\n1. org.apache.spark.mllib.linalg.SparseMatrix, which is a wrapper over an RDD of matrix entries, i.e., coordinate list format.\n2. org.apache.spark.mllib.linalg.TallSkinnyDenseMatrix, which is a wrapper over RDD[Array[Double]], i.e. row-oriented format.\n\nWe will see naming collision when we introduce local SparseMatrix and the name TallSkinnyDenseMatrix is not exact if we switch to RDD[Vector] instead of RDD[Array[Double]]. It would be better to have \"RDD\" in the type name to suggest that operations will trigger a job.\n\nThe proposed names (all under org.apache.spark.mllib.linalg.rdd):\n\n1. RDDMatrix: trait for matrices backed by one or more RDDs\n2. CoordinateRDDMatrix: wrapper of RDD[RDDMatrixEntry]\n3. RowRDDMatrix: wrapper of RDD[Vector] whose rows do not have special ordering\n4. IndexedRowRDDMatrix: wrapper of RDD[(Long, Vector)] whose rows are associated with indices\n\nThe proposal is subject to charge, but it would be nice to make the changes before v1.0.", "comments": [], "derived": {"summary": "The current interfaces of RDD backed matrices needs refactoring for v1. 0 release.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Refactor RDD backed matrices - The current interfaces of RDD backed matrices needs refactoring for v1. 0 release."}]}}
{"project": "SPARK", "issue_id": "SPARK-1391", "title": "BlockManager cannot transfer blocks larger than 2G in size", "status": "Closed", "priority": "Major", "reporter": "Shivaram Venkataraman", "assignee": null, "labels": [], "created": "2014-04-02T05:34:28.000+0000", "updated": "2020-05-17T18:20:51.000+0000", "description": "If a task tries to remotely access a cached RDD block, I get an exception when the block size is > 2G. The exception is pasted below.\n\nMemory capacities are huge these days (> 60G), and many workflows depend on having large blocks in memory, so it would be good to fix this bug.\n\nI don't know if the same thing happens on shuffles if one transfer (from mapper to reducer) is > 2G.\n\n{noformat}\n14/04/02 02:33:10 ERROR storage.BlockManagerWorker: Exception handling buffer message\njava.lang.ArrayIndexOutOfBoundsException\n        at it.unimi.dsi.fastutil.io.FastByteArrayOutputStream.write(FastByteArrayOutputStream.java:96)\n        at it.unimi.dsi.fastutil.io.FastBufferedOutputStream.dumpBuffer(FastBufferedOutputStream.java:134)\n        at it.unimi.dsi.fastutil.io.FastBufferedOutputStream.write(FastBufferedOutputStream.java:164)\n        at java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1876)\n        at java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1785)\n        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1188)\n        at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)\n        at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:38)\n        at org.apache.spark.serializer.SerializationStream$class.writeAll(Serializer.scala:93)\n        at org.apache.spark.serializer.JavaSerializationStream.writeAll(JavaSerializer.scala:26)\n        at org.apache.spark.storage.BlockManager.dataSerializeStream(BlockManager.scala:913)\n        at org.apache.spark.storage.BlockManager.dataSerialize(BlockManager.scala:922)\n        at org.apache.spark.storage.MemoryStore.getBytes(MemoryStore.scala:102)\n        at org.apache.spark.storage.BlockManager.doGetLocal(BlockManager.scala:348)\n        at org.apache.spark.storage.BlockManager.getLocalBytes(BlockManager.scala:323)\n        at org.apache.spark.storage.BlockManagerWorker.getBlock(BlockManagerWorker.scala:90)\n        at org.apache.spark.storage.BlockManagerWorker.processBlockMessage(BlockManagerWorker.scala:69)\n        at org.apache.spark.storage.BlockManagerWorker$$anonfun$2.apply(BlockManagerWorker.scala:44)\n        at org.apache.spark.storage.BlockManagerWorker$$anonfun$2.apply(BlockManagerWorker.scala:44)\n        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n        at scala.collection.Iterator$class.foreach(Iterator.scala:727)\n        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n        at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n        at org.apache.spark.storage.BlockMessageArray.foreach(BlockMessageArray.scala:28)\n        at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n        at org.apache.spark.storage.BlockMessageArray.map(BlockMessageArray.scala:28)\n        at org.apache.spark.storage.BlockManagerWorker.onBlockMessageReceive(BlockManagerWorker.scala:44)\n        at org.apache.spark.storage.BlockManagerWorker$$anonfun$1.apply(BlockManagerWorker.scala:34)\n        at org.apache.spark.storage.BlockManagerWorker$$anonfun$1.apply(BlockManagerWorker.scala:34)\n        at org.apache.spark.network.ConnectionManager.org$apache$spark$network$ConnectionManager$$handleMessage(ConnectionManager.scala:661)\n        at org.apache.spark.network.ConnectionManager$$anon$9.run(ConnectionManager.scala:503)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:744)\n{noformat}\n", "comments": ["Hi Shivaram\n\nIs that the entire exception stack? Seems missed some information on that.\n\nWhich revision did you run on?", "It's supposed to have some detailed message after \"java.lang.ArrayIndexOutOfBoundsException\"\n\nlike {noformat}java.lang.ArrayIndexOutOfBoundsException: -2147483648{noformat}", "It is possible that it constructs AIOOBE without the line number. I may be stating the obvious but it is all but surely negative and due to int overflow. 2GB strongly suggests it anyway. Would be good to confirm. ", "[~sowen]\n\nYes, there is possibility. It's not a line number, it's instead should have an index or Otherewise it should be a  \n user defined exception or native exception.  I greped the fastutil source code, it won't throw AIOOBEs with empty message. \n\nAnd the line number is not corresponding to the v6.4.4\n\nsee line 96 of FastByteArrayOutputStream: \nhttp://grepcode.com/file/repo1.maven.org/maven2/it.unimi.dsi/fastutil/6.4.4/it/unimi/dsi/fastutil/io/FastByteArrayOutputStream.java\n\nHere is the possibility where throws AIOOBEs.  The position can be negative due to line \n{noformat}\npublic void write( final byte[] b, final int off, final int len ) throws IOException {\n...\n    if ( position + len > length ) length = position += len;\n...\n}\n{noformat}\n\nHere is the simulation with fastutils under the  version of 6.4.4 \n{noformat}\n\nimport it.unimi.dsi.fastutil.io.FastByteArrayOutputStream;\n\npublic class ArrayOutofIndex {\n\n  public static void main(String[] args) throws Exception {\n    FastByteArrayOutputStream outputStream = new FastByteArrayOutputStream(4096);\n    outputStream.position(-1);\n    outputStream.write('a');\n    outputStream.close();\n  }\n}\n\nException in thread \"main\" java.lang.ArrayIndexOutOfBoundsException: -1\n\tat it.unimi.dsi.fastutil.io.FastByteArrayOutputStream.write(FastByteArrayOutputStream.java:92)\n\tat mzhou.shuffle.perf.ArrayOutofIndex.main(ArrayOutofIndex.java:29)\n{noformat} \n\n{noformat}\nimport it.unimi.dsi.fastutil.io.FastByteArrayOutputStream;\n\npublic class ArrayOutofIndex {\n  public static void main(String[] args) throws Exception {\n    FastByteArrayOutputStream outputStream = new FastByteArrayOutputStream(4096);\n    outputStream.position(-1);\n    outputStream.write(new byte[1024], 0, 1024);\n    outputStream.close();\n  }\n}\n\nException in thread \"main\" java.lang.ArrayIndexOutOfBoundsException\n\tat java.lang.System.arraycopy(Native Method)\n\tat it.unimi.dsi.fastutil.io.FastByteArrayOutputStream.write(FastByteArrayOutputStream.java:98)\n\tat mzhou.shuffle.perf.ArrayOutofIndex.main(ArrayOutofIndex.java:29)\n{noformat}\n\nThe line number and stack info is not the same as that be reported.", "Oops yes I mean offset of course. Good investigation there. I am also not sure why the index would not show. ", "[~sowen] Yes, neither of the two simulation can exactly match the stack info Shivaram gave us. So I suspected that if something missed there.", "From the line number,  the fastutils version should be 6.5.7.\nThe error should be thrown by this line in FastByteArrayOutputStream.java\n{noformat}\nSystem.arraycopy( b, off, array, position, len );\n{noformat}", "Finally, I can explain the whole thing. Apologies to Shivaram, you didn't miss anything.\n\nFrom the above, we can see that the position can be a negative value. \nRun below code under fastutils 6.5.7 we will get\n{noformat}\nimport it.unimi.dsi.fastutil.io.FastByteArrayOutputStream;\npublic class ArrayIndex {\n  public static void main(String[] args) throws Exception {\n    FastByteArrayOutputStream outputStream = new FastByteArrayOutputStream(4096);\n    outputStream.position(Integer.MAX_VALUE);\n    outputStream.write(new byte[1024], 0, 1024);\n    outputStream.close();\n  }\n}\n\nException in thread \"main\" java.lang.ArrayIndexOutOfBoundsException\n\tat java.lang.System.arraycopy(Native Method)\n\tat it.unimi.dsi.fastutil.io.FastByteArrayOutputStream.write(FastByteArrayOutputStream.java:96)\n\tat mzhou.shuffle.perf.ArrayOutofIndex.main(ArrayOutofIndex.java:29)\n{noformat}\n\nWe can see the line number FastByteArrayOutputStream.java:96 is correct, the same as  Shivaram's. The only different is that the stack frame \"at java.lang.System.arraycopy(Native Method)\" does exists in Shivaram's report.\n\nAfter some investigation on jdk source code, I get the answer,  System.arraycopy got Just-in-Time compiled. Here is a simulation\n{noformat}\npublic class ArrayCopy {\n\n  public static void main(String[] args) throws Exception {\n    byte[] src = new byte[8];\n    byte[] dst = new byte[8];\n    for(int i = 0; i < 100000; i++) {\n      System.arraycopy(src,0,dst,0, dst.length);\n    }\n    System.arraycopy(src,0,dst,-1, dst.length);\n  }\n}\n\nException in thread \"main\" java.lang.ArrayIndexOutOfBoundsException\n\tat mzhou.shuffle.perf.ArrayOutofIndex.main(ArrayOutofIndex.java:36)\n{noformat}\n\nSee? That stack trace has gone.", "The JIT intrinsic will replace the original JNI implemetation of System.arraycopy. Hotspot jvm can't get the symbol of this native method,  this is the reason why that stack strace missed. ", "I just copied the stack trace from the stderr of the executor, but this was towards the end of a long job, so JIT could definitely be an explanation.\n\nIn case this motivates somebody to pick up this issue :), let me add some context. This is really painful in situations where you have a long chain of transformations cached in memory. Any non-local task launched after that will try to recompute the entire transformation chain as block transfers fail.\n\nIn terms of solutions, is fastutil the problem here ? Sean, will https://github.com/apache/spark/pull/266 address this by any chance ?", "No fastutil is nothing to do with it, if in fact the problem is int overflow in the offset. It's basically a limit of how big an array can be in Java.", "[~shivaram] \n\n[~sowen], if you don't mind, I will take this issue.  Actually, I was invited by Reynold to solve this problem. IMHO, fastutils at least can't cover the field \"position\"  be overflow. Maybe my solution is like yours, to replace fastutils with other libs, then we will have some collaboration.  May I? ", "Of course! Hardly my issue. Well, you could try my patch that replaces fastutil with alternatives. I doubt the standard ByteArrayOutputStream does differently though?\n\nBut we are always going to have a problem in that a Java byte array can only be so big because of the size of an int, regardless of stream position issues. This one could be deeper. ", "[~coderplay] That sounds great to me -- I don't seem to have permissions to assign issues though.", "Yeah I think the right solution is to not create one large array, but somehow either stream the data or break it up into smaller chunks", "Thank you guys.\n\n[~shivaram] Which fastutils version did you use? Is it 6.5.7 as I analyzed ?\n\nI will submit a quick and dirty patch these days, not expect to commit it. Just prove my analysis above in your real world cuz I have no spark deployment here to test. Shivaram, would you like to test for me? Thanks in advance.", "I am not using any fastutil version explicitly. I am just using Spark's master branch from around March 23rd. (The exact commit I am synced to is https://github.com/apache/spark/commit/8265dc7739caccc59bc2456b2df055ca96337fe4)", "Oh and yes, I'd be happy to test out any patch / WIP", "[~shivaram] Yeah, I meant the fastutils involved by spark. From your revision, it's weird that the version is 6.4.4... ", "I took a quick look into this. We are using a bunch of ByteBuffer's throughout the block manager and the communication layer. We need to replace that ByteBuffer with a different interface that can handle larger arrays. \n\nIt is fortunate that the underlying communication in Connection.scala actually breaks messages down into smaller trunks, so that's one less place to change. ", "Yes. Communication layer use ByteBuffer array to transfer messages, but the receiver will convert them back to BlockMessages where each block corresponding to one ByteBuffer, which can't be larger than 2GB. Those BlockMessages will be consumed by the connection caller in everywhere we can't control. \n\nOne approach is write an CompositeByteBuffer to overcome the 2GB limitation, but still can't break some other limitation of ByteBuffer interface, like ByteBuffer.position(),  ByteBuffer.capacity(),  ByteBuffer.remaining(), whose return values are still integers. ", "[~shivaram]\n\nIt should take a long time if we fundamentally solve the problem,  we need a ByteBuffer and an OutputStream that support more than 2GB data. Or change the data structure inside a block, for example , Array[ByteBuffer] to replace ByteBuffer.\n\nA short term approach is that take kryo serialization as the default ser instead of java ser which causes data inflation.  I am attaching a patch following this approach.  Due to I didn't test it in a real cluster, I am not sending a pull request currently. \n\nShivaram, please apply this patch and test it for me, thanks!", "Thanks for the patch. I will try this out in the next couple of days and get back.", "Any update on your test , [~shivaram] ?", "Sorry didn't get a chance to try this yet. Will try to do it tomorrow", "I tried to run with this patch yesterday, but unfortunately I dont think the non-local jobs were triggered during my run. I will try to synthetically force non-local tasks the next time around to verify this.", "Another place where this is relevant is here :\n\njava.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE\n        at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:789)\n        at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:98)\n        at org.apache.spark.storage.BlockManager.doGetLocal(BlockManager.scala:413)\n        at org.apache.spark.storage.BlockManager.getLocal(BlockManager.scala:339)\n        at org.apache.spark.storage.BlockManager.get(BlockManager.scala:506)\n        at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:39)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:233)\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)\n        at org.apache.spark.scheduler.Task.run(Task.scala:52)\n        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)\n        at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:43)\n        at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1262)\n        at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:42)\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:722)\n\nSo we might want to change the abstraction from single ByteBuffer to a sequence of bytebuffers ...", "[~coderplay] Have you solved this issue yet ? Or do you have any temporary solution to this problem? ", "I am having the same issue spark 1.1.0.  6 node cluster testing small file with 1.3GB size before moving to bigger cluster bigger files. It fails on flatmap operation.\n\n14/10/06 14:31:25 ERROR storage.BlockManagerWorker: Exception handling buffer message\njava.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE\n\tat sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:829)\n\tat org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:104)\n\tat org.apache.spark.storage.BlockManager.getLocalBytes(BlockManager.scala:379)\n\tat org.apache.spark.storage.BlockManagerWorker.getBlock(BlockManagerWorker.scala:100)\n\tat org.apache.spark.storage.BlockManagerWorker.processBlockMessage(BlockManagerWorker.scala:79)\n\tat org.apache.spark.storage.BlockManagerWorker$$anonfun$2.apply(BlockManagerWorker.scala:48)\n\tat org.apache.spark.storage.BlockManagerWorker$$anonfun$2.apply(BlockManagerWorker.scala:48)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat org.apache.spark.storage.BlockMessageArray.foreach(BlockMessageArray.scala:28)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n\tat org.apache.spark.storage.BlockMessageArray.map(BlockMessageArray.scala:28)\n\tat org.apache.spark.storage.BlockManagerWorker.onBlockMessageReceive(BlockManagerWorker.scala:48)\n\tat org.apache.spark.storage.BlockManagerWorker$$anonfun$1.apply(BlockManagerWorker.scala:38)\n\tat org.apache.spark.storage.BlockManagerWorker$$anonfun$1.apply(BlockManagerWorker.scala:38)\n\tat org.apache.spark.network.ConnectionManager.org$apache$spark$network$ConnectionManager$$handleMessage(ConnectionManager.scala:682)\n\tat org.apache.spark.network.ConnectionManager$$anon$10.run(ConnectionManager.scala:520)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)", "[~coderplay], I assume you are no longer looking at this, right?  I'm going to take a crack at this issue if you don't mind.  Here is my plan, copied from SPARK-1476 (now that I've untangled those issues a little bit):\n\nI'd like to start on it, with the following very minimal goals:\n\n1. Make it possible for blocks to be bigger than 2GB\n2. Maintain performance on smaller blocks\n\nie., I'm not going to try to do anything fancy to optimize performance of the large blocks. To that end, my plan is to\n\n1. create a {{LargeByteBuffer}} interface, which just has the same methods we use on {{ByteBuffer}}\n2. have one implementation that just wraps one ByteBuffer, and another which wraps a completely static set of {{ByteBuffer}} s (eg., if you map a 3 GB file, it'll just immediately map it to 2 {{ByteBuffer}} s, nothing fancy with only mapping the first half of the file until the second is needed etc. etc.)\n3. change {{ByteBuffer}} to {{LargeByteBuffer}} in {{BlockStore}}\n\nI see that about a year back there was a lot of discussion on this in SPARK-1476, and some alternate proposals. I'd like to push forward with a POC to try to move the discussion along again. I know there was some discussion about how important this is, and whether or not we want to support it. IMO this is a big limitation and results in a lot of frustration for the users, we really need a solution for this.\n\nI could still be missing something, but I believe this should also solve SPARK-3151", "Here is a minimal program to demonstrate the problem:\n\n{code}\nsc.parallelize(1 to 1e6.toInt, 1).map{i => new Array[Byte](2.2e3.toInt)}.persist(StorageLevel.DISK_ONLY).count()\n{code}\n\nthis only demonstrates the problem w/ {{DiskStore}} but a solution to this should apply to other cases if done correctly.  (probably need to come up with more test cases)", "The one complication here comes from the network transfer required by replication.  If we ignore {{NioBlockTransferService}} for now and just look at {{NettyBlockTransferService}}, the existing behavior is:\n\n1. replication results in a request to [{{NettyBlockTransferService#uploadBlocks}} | https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala#L126], which sends an {{UploadBlock}} msg to a peer.  The {{UploadBlock}} message contains the full payload, which is limited to 2GB currently.\n\n2. The message is received by [{{NettyBlockRpcServer}} | https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/network/netty/NettyBlockRpcServer.scala#L62] where it is simply deserialized and inserted into the local block manager.\n\nI'm thinking we could break a block apart into multiple messages, eg. {{UploadPartialBlock}}, with each message limited to 2GB (or even less).  Then {{NettyBlockRpcServer}} would queue up all the messages, and once it had received them all it would put them together and insert the block locally.\n\nMy concern with that approach is robustness -- what happens if some of the {{UploadPartialBlock}} s never make it, for whatever reason?  We wouldn't want {{NettyBlockRpcServer}} to simply hold on to those partial msgs in memory indefinitely.  Does it make sense to introduce a timeout?  When the first {{UploadPartialBlock}} is received, it would only wait for the rest of the msgs a limited time before dumping those partial blocks.", "design doc", "User 'squito' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/4857", "[~imranr] if you want to attempt something this large and core to the whole engine, it would be better to do this incrementally, especially when this is a part of the code that you are less familiar with.\n\nI'd suggest breaking this task down in the following way, and getting feedback incrementally as well.\n\n1. LargeByteBuffer interface. (This alone would deserve its own design doc and focus on how it integrates with ManagedBuffer)\n2. Block storage\n3. Block fetching\n4. Block upload\n\nUploading blocks > 2G is extremely rare, as it is rarely used outside of streaming, and streaming data blocks are usually small. It would also be much simpler to deal with upload if we change it to sending a msg to the other end and let the other end download the blocks instead.\n\n\n", "[~rxin] Sure thing, I can break it into multiple pieces.  though honestly, if we don't think we'll bother fixing some of these 2gb limits, step 0 should be putting in sane error messages on all the different cases where you can run into the 2gb limit.  Right now the errors are extremely confusing.\n\nIf we want to support only some very limited set of functionality, then we might not even need to have LargeByteBuffer interact at all w/ ManagedBuffer -- if we only want to support cached partitions, with no replication and no remote fetches, then I'm pretty sure the relevant paths in BlockManager never involve a ManagedBuffer.\n\nI'm not sure I understand what you mean that \"Uploading blocks > 2G is extremely rare\".  Are you saying that nobody would want to cache a partition > 2gb?  That nobody uses replication when caching?  Or that we don't need to support the combination?  Also if you have a broadcast variable over 2gb, TorrentBroadcast will store it in all one block on the driver.  It breaks it into smaller blocks when sending it executors, but not on the driver -- perhaps that could be changed to always break into smaller blocks on the driver as well.", "- I absolutely agree that better error messages is critical.\n- I'm saying it is rare that somebody wants to cache > 2gb partition and also use replication.\n- We can fix TorrentBroadcast without fixing block upload, because TorrentBroadcast splits data into chunks anyway.\n", "duping this to SPARK-6235.  If something is missing lets add it there."], "derived": {"summary": "If a task tries to remotely access a cached RDD block, I get an exception when the block size is > 2G. The exception is pasted below.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "BlockManager cannot transfer blocks larger than 2G in size - If a task tries to remotely access a cached RDD block, I get an exception when the block size is > 2G. The exception is pasted below."}, {"q": "What updates or decisions were made in the discussion?", "a": "duping this to SPARK-6235.  If something is missing lets add it there."}]}}
{"project": "SPARK", "issue_id": "SPARK-1392", "title": "Local spark-shell Runs Out of Memory With Default Settings", "status": "Resolved", "priority": "Major", "reporter": "Pat McDonough", "assignee": null, "labels": [], "created": "2014-04-02T05:49:02.000+0000", "updated": "2014-06-27T20:44:40.000+0000", "description": "Using the spark-0.9.0 Hadoop2 binary from the project download page, running the spark-shell locally in out of the box configuration, and attempting to cache all the attached data, spark OOMs with: java.lang.OutOfMemoryError: GC overhead limit exceeded\n\nYou can work around the issue by either decreasing spark.storage.memoryFraction or increasing SPARK_MEM", "comments": ["Running the following with the attached data results in the errors below:\n{code}\nscala> val explore = sc.textFile(\"/Users/pat/Projects/training-materials/Data/wiki_links\")\n...\nscala> explore.cache\nres1: org.apache.spark.rdd.RDD[String] = MappedRDD[1] at textFile at <console>:12\n...\nscala> explore.count\n...\n14/04/01 22:52:48 INFO HadoopRDD: Input split: file:/Users/pat/Projects/training-materials/Data/wiki_links/part-00007:0+25009430\n14/04/01 22:52:54 INFO MemoryStore: ensureFreeSpace(55520836) called with curMem=271402430, maxMem=309225062\n14/04/01 22:52:54 INFO MemoryStore: Will not store rdd_1_7 as it would require dropping another block from the same RDD\n14/04/01 22:52:54 INFO BlockManager: Dropping block rdd_1_7 from memory\n14/04/01 22:52:54 WARN BlockManager: Block rdd_1_7 could not be dropped from memory as it does not exist\n14/04/01 22:52:54 INFO BlockManagerMaster: Updated info of block rdd_1_7\n14/04/01 22:52:54 INFO BlockManagerMaster: Updated info of block rdd_1_7\n14/04/01 22:52:54 INFO Executor: Serialized size of result for 7 is 563\n14/04/01 22:52:54 INFO Executor: Sending result for 7 directly to driver\n14/04/01 22:52:54 INFO Executor: Finished task ID 7\n14/04/01 22:52:54 INFO TaskSetManager: Starting task 0.0:8 as TID 8 on executor localhost: localhost (PROCESS_LOCAL)\n14/04/01 22:52:54 INFO TaskSetManager: Serialized task 0.0:8 as 1606 bytes in 2 ms\n14/04/01 22:52:54 INFO Executor: Running task ID 8\n14/04/01 22:52:54 INFO TaskSetManager: Finished TID 7 in 6714 ms on localhost (progress: 7/10)\n14/04/01 22:52:54 INFO DAGScheduler: Completed ResultTask(0, 7)\n14/04/01 22:52:54 INFO BlockManager: Found block broadcast_0 locally\n14/04/01 22:52:54 INFO CacheManager: Partition rdd_1_8 not found, computing it\n14/04/01 22:52:54 INFO HadoopRDD: Input split: file:/Users/pat/Projects/training-materials/Data/wiki_links/part-00008:0+25904930\n14/04/01 22:52:59 INFO TaskSetManager: Starting task 0.0:9 as TID 9 on executor localhost: localhost (PROCESS_LOCAL)\n14/04/01 22:52:59 ERROR Executor: Exception in task ID 8\n{code}\n\n\n{noformat}\njava.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat java.nio.HeapCharBuffer.<init>(HeapCharBuffer.java:57)\n\tat java.nio.CharBuffer.allocate(CharBuffer.java:331)\n\tat java.nio.charset.CharsetDecoder.decode(CharsetDecoder.java:777)\n\tat org.apache.hadoop.io.Text.decode(Text.java:405)\n\tat org.apache.hadoop.io.Text.decode(Text.java:382)\n\tat org.apache.hadoop.io.Text.toString(Text.java:280)\n\tat org.apache.spark.SparkContext$$anonfun$textFile$1.apply(SparkContext.scala:344)\n\tat org.apache.spark.SparkContext$$anonfun$textFile$1.apply(SparkContext.scala:344)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:75)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:230)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:53)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)\n\tat org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\n{noformat}\n", "Attachment was too big, so here's a link: https://drive.google.com/file/d/0BwrkCxCycBCyTmlWYXp0MmdEakk/edit?usp=sharing", "Tried it with the hadoop-1.0.4 build and there was no OOM", "CC: [~matei] [~tdas]] [~adav] [~pwendell]\n\nHere's that OOM problem I talked to you guys about. I think TD's suggestion works best:\n\n * create a new property that sets the minimum amount of heap reserved for the system/application. Something similar to: {{spark.system.memoryReservedSize}} \n * account for that value prior to calculating memory available for storage and shuffle\n * set the new property at 300m by default (based on what we are seeing in a local spark-shell running JDK7 with the spark-0.9.0-hadoop-2 binary distribution). For heap's larger than 3g, it won't even come in to play.\n\nWe should also consider increasing the default spark.executor.memory beyond 512m if we are going to reserve half of it for spark itself.", "I mentioned this on the pull request, but I think this was an instance of SPARK-1777. I'm running some tests locally on the pull request there to determine whether that was the case.", "Okay great, I confirmed this is fixed by SPARK-1777. I tested as follows:\n\n{code}\nSPARK_HADOOP_VERSION=2.2.0 SPARK_YARN=true SPARK_HIVE=true sbt/sbt clean assembly/assembly\nsc.textFile(\"/tmp/wiki_links\").cache.count\n{code}\n\nThe wiki_links file was download and extracted from here:\nhttps://drive.google.com/file/d/0BwrkCxCycBCyTmlWYXp0MmdEakk/edit?usp=sharing\n\nThis worked with the proposed patch but failed with the default build.", "and by \"fixed by SPARK-1777\" he means https://github.com/apache/spark/pull/1165"], "derived": {"summary": "Using the spark-0. 9.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Local spark-shell Runs Out of Memory With Default Settings - Using the spark-0. 9."}, {"q": "What updates or decisions were made in the discussion?", "a": "and by \"fixed by SPARK-1777\" he means https://github.com/apache/spark/pull/1165"}]}}
{"project": "SPARK", "issue_id": "SPARK-1393", "title": "fix computePreferredLocations signature to not depend on underlying implementation", "status": "Resolved", "priority": "Major", "reporter": "Mridul Muralidharan", "assignee": null, "labels": [], "created": "2014-04-02T10:16:22.000+0000", "updated": "2014-04-05T23:43:41.000+0000", "description": "computePreferredLocations in core/src/main/scala/org/apache/spark/scheduler/InputFormatInfo.scala : change from using mutable HashMap/HashSet to Map/Set", "comments": ["Merged https://github.com/apache/spark/pull/302"], "derived": {"summary": "computePreferredLocations in core/src/main/scala/org/apache/spark/scheduler/InputFormatInfo. scala : change from using mutable HashMap/HashSet to Map/Set.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "fix computePreferredLocations signature to not depend on underlying implementation - computePreferredLocations in core/src/main/scala/org/apache/spark/scheduler/InputFormatInfo. scala : change from using mutable HashMap/HashSet to Map/Set."}, {"q": "What updates or decisions were made in the discussion?", "a": "Merged https://github.com/apache/spark/pull/302"}]}}
{"project": "SPARK", "issue_id": "SPARK-1394", "title": "calling system.platform on worker raises IOError", "status": "Resolved", "priority": "Major", "reporter": "Idan Zalzberg", "assignee": null, "labels": ["pyspark"], "created": "2014-04-02T18:29:07.000+0000", "updated": "2014-07-25T21:36:09.000+0000", "description": "A simple program that calls system.platform() on the worker fails most of the time (it works some times but very rarely).\nThis is critical since many libraries call that method (e.g. boto).\n\nHere is the trace of the attempt to call that method:\n\n\n\n$ /usr/local/spark/bin/pyspark\nPython 2.7.3 (default, Feb 27 2014, 20:00:17)\n[GCC 4.6.3] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n14/04/02 18:18:37 INFO Utils: Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n14/04/02 18:18:37 WARN Utils: Your hostname, qlika-dev resolves to a loopback address: 127.0.1.1; using 10.33.102.46 instead (on interface eth1)\n14/04/02 18:18:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n14/04/02 18:18:38 INFO Slf4jLogger: Slf4jLogger started\n14/04/02 18:18:38 INFO Remoting: Starting remoting\n14/04/02 18:18:39 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://spark@10.33.102.46:36640]\n14/04/02 18:18:39 INFO Remoting: Remoting now listens on addresses: [akka.tcp://spark@10.33.102.46:36640]\n14/04/02 18:18:39 INFO SparkEnv: Registering BlockManagerMaster\n14/04/02 18:18:39 INFO DiskBlockManager: Created local directory at /tmp/spark-local-20140402181839-919f\n14/04/02 18:18:39 INFO MemoryStore: MemoryStore started with capacity 294.6 MB.\n14/04/02 18:18:39 INFO ConnectionManager: Bound socket to port 43357 with id = ConnectionManagerId(10.33.102.46,43357)\n14/04/02 18:18:39 INFO BlockManagerMaster: Trying to register BlockManager\n14/04/02 18:18:39 INFO BlockManagerMasterActor$BlockManagerInfo: Registering block manager 10.33.102.46:43357 with 294.6 MB RAM\n14/04/02 18:18:39 INFO BlockManagerMaster: Registered BlockManager\n14/04/02 18:18:39 INFO HttpServer: Starting HTTP Server\n14/04/02 18:18:39 INFO HttpBroadcast: Broadcast server started at http://10.33.102.46:51803\n14/04/02 18:18:39 INFO SparkEnv: Registering MapOutputTracker\n14/04/02 18:18:39 INFO HttpFileServer: HTTP File server directory is /tmp/spark-9b38acb0-7b01-4463-b0a6-602bfed05a2b\n14/04/02 18:18:39 INFO HttpServer: Starting HTTP Server\n14/04/02 18:18:40 INFO SparkUI: Started Spark Web UI at http://10.33.102.46:4040\n14/04/02 18:18:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 0.9.0\n      /_/\n\nUsing Python version 2.7.3 (default, Feb 27 2014 20:00:17)\nSpark context available as sc.\n>>> import platform\n>>> sc.parallelize([1]).map(lambda x : platform.system()).collect()\n14/04/02 18:19:17 INFO SparkContext: Starting job: collect at <stdin>:1\n14/04/02 18:19:17 INFO DAGScheduler: Got job 0 (collect at <stdin>:1) with 1 output partitions (allowLocal=false)\n14/04/02 18:19:17 INFO DAGScheduler: Final stage: Stage 0 (collect at <stdin>:1)\n14/04/02 18:19:17 INFO DAGScheduler: Parents of final stage: List()\n14/04/02 18:19:17 INFO DAGScheduler: Missing parents: List()\n14/04/02 18:19:17 INFO DAGScheduler: Submitting Stage 0 (PythonRDD[1] at collect at <stdin>:1), which has no missing parents\n14/04/02 18:19:17 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (PythonRDD[1] at collect at <stdin>:1)\n14/04/02 18:19:17 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks\n14/04/02 18:19:17 INFO TaskSetManager: Starting task 0.0:0 as TID 0 on executor localhost: localhost (PROCESS_LOCAL)\n14/04/02 18:19:17 INFO TaskSetManager: Serialized task 0.0:0 as 2152 bytes in 12 ms\n14/04/02 18:19:17 INFO Executor: Running task ID 0\nPySpark worker failed with exception:\nTraceback (most recent call last):\n  File \"/usr/local/spark/python/pyspark/worker.py\", line 77, in main\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/pyspark/serializers.py\", line 182, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/usr/local/spark/python/pyspark/serializers.py\", line 117, in dump_stream\n    for obj in iterator:\n  File \"/usr/local/spark/python/pyspark/serializers.py\", line 171, in _batched\n    for item in iterator:\n  File \"<stdin>\", line 1, in <lambda>\n  File \"/usr/lib/python2.7/platform.py\", line 1306, in system\n    return uname()[0]\n  File \"/usr/lib/python2.7/platform.py\", line 1273, in uname\n    processor = _syscmd_uname('-p','')\n  File \"/usr/lib/python2.7/platform.py\", line 1030, in _syscmd_uname\n    rc = f.close()\nIOError: [Errno 10] No child processes\n\n14/04/02 18:19:17 ERROR Executor: Exception in task ID 0\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/pyspark/worker.py\", line 77, in main\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/pyspark/serializers.py\", line 182, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/usr/local/spark/python/pyspark/serializers.py\", line 117, in dump_stream\n    for obj in iterator:\n  File \"/usr/local/spark/python/pyspark/serializers.py\", line 171, in _batched\n    for item in iterator:\n  File \"<stdin>\", line 1, in <lambda>\n  File \"/usr/lib/python2.7/platform.py\", line 1306, in system\n    return uname()[0]\n  File \"/usr/lib/python2.7/platform.py\", line 1273, in uname\n    processor = _syscmd_uname('-p','')\n  File \"/usr/lib/python2.7/platform.py\", line 1030, in _syscmd_uname\n    rc = f.close()\nIOError: [Errno 10] No child processes\n\n        at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:131)\n        at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:153)\n        at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:96)\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)\n        at org.apache.spark.scheduler.Task.run(Task.scala:53)\n        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)\n        at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:744)\n", "comments": ["This seems to be related to the way the handle_sigchld method in daemon.py works.\nIn order to kill the zombie processes the worker calls os.waitpid on SIGCHLD. however. since using Popen also tries to do that eventually, you get a closed handle.\n\nSince platform.py is a native library, I would guess we should find a solution in pyspark (i.e. change the way handle_sigchld works, or maybe limit the processes it waits on)", "I have the same issue with Spark 0.9.1. Is there any workaround?", "If you have an __init__.py  you are sure to go through, you can add the following code to your file:\n\n{code}\n# PySpark adds a SIGCHLD signal handler, but that breaks other packages, so we remove it\ntry:\n    import signal\n    signal.signal(signal.SIGCHLD, signal.SIG_DFL)\nexcept: pass\n{code}\n\nIt's a work around, it would be better to have a \"smart\" signal handler that only handles the processes that are direct descendants of the daemon. I might try to get something like that out. ", "[~idanzalz] unfortunately, it had helped to avoid only one exception, so I commented signal binding in PySpark and these crashes went away. I hope it will be fixed somehow in next Spark release.", "We also bumping into the same issue. My I know, how and where can we comment the signal binding in pyspark?", "[~sgottipa] spark/python/pyspark/daemon.py:75     #signal.signal(SIGCHLD, handle_sigchld)", "For what it's worth the platform library is unfortunately called by a number of numerical libraries associated to machine learning.  In particular\n\nTheano calls on platform\nNumExpr calls on platform\nPandas calls on NumExpr\n\nThese libraries are very popular in the numerics / machine learning space.", "[~mrocklin] What? What is your reply for?", "Just trying to lend weight to this issue by adding context.  \nLet me know if this is an inappropriate use of this forum.", "i'm taking a look at this", "fyi, this certainly looks like the waitpid(0,...) cleanup handler is cleaning up more than it should be\n\nalso, fyi, if you comment it out you'll start accumulating defunct worker processes, which is not good", "the python daemon does two levels of forking. the manager forks workers and each worker forks sub-workers, who then handle connections.\n\nthe master has a sigchld handler to cleanup workers. the workers also have sigchld handlers to clean up sub-workers.\n\nthe problem here is the worker's handler is also installed on the sub-worker, which interferes with calls like platform.system() (but not os.uname, btw!)\n\ni assert that the sub-workers, who do not intentionally fork, should not be responsible for cleaning up unexpected sub-processes, and as such should not have a sigchld handler.\n\nif it's desired to have tighter control of process cleanup, namespaces should be used and it should be the manager's responsibility.\n\npull request - https://github.com/apache/spark/pull/1247", "this should be resolved, @aarondav"], "derived": {"summary": "A simple program that calls system. platform() on the worker fails most of the time (it works some times but very rarely).", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "calling system.platform on worker raises IOError - A simple program that calls system. platform() on the worker fails most of the time (it works some times but very rarely)."}, {"q": "What updates or decisions were made in the discussion?", "a": "this should be resolved, @aarondav"}]}}
{"project": "SPARK", "issue_id": "SPARK-1395", "title": "Cannot launch jobs on Yarn cluster with \"local:\" scheme in SPARK_JAR", "status": "Resolved", "priority": "Major", "reporter": "Marcelo Masiero Vanzin", "assignee": "Marcelo Masiero Vanzin", "labels": [], "created": "2014-04-02T20:23:57.000+0000", "updated": "2014-06-23T14:14:41.000+0000", "description": "If you define SPARK_JAR and friends to use \"local:\" URIs, you cannot submit a job on a Yarn cluster. e.g., I have:\n\nSPARK_JAR=local:/tmp/spark-assembly-1.0.0-SNAPSHOT-hadoop2.3.0-cdh5.0.0.jar\nSPARK_YARN_APP_JAR=local:/tmp/spark-examples-assembly-1.0.0-SNAPSHOT.jar\n\nAnd running SparkPi using bin/run-example yields this:\n\n14/04/02 13:23:33 INFO yarn.Client: Preparing Local resources\nException in thread \"main\" java.io.IOException: No FileSystem for scheme: local\n        at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2385)\n        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)\n        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)\n        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)\n        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)\n        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)\n        at org.apache.spark.deploy.yarn.ClientBase$class.org$apache$spark$deploy$yarn$ClientBase$$copyRemoteFile(ClientBase.scala:156)\n        at org.apache.spark.deploy.yarn.ClientBase$$anonfun$prepareLocalResources$3.apply(ClientBase.scala:217)\n        at org.apache.spark.deploy.yarn.ClientBase$$anonfun$prepareLocalResources$3.apply(ClientBase.scala:212)\n        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)\n        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)\n        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)\n        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)\n        at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)\n        at org.apache.spark.deploy.yarn.ClientBase$class.prepareLocalResources(ClientBase.scala:212)\n        at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:41)\n        at org.apache.spark.deploy.yarn.Client.runApp(Client.scala:76)\n        at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:81)\n        at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:129)\n        at org.apache.spark.SparkContext.<init>(SparkContext.scala:226)\n        at org.apache.spark.SparkContext.<init>(SparkContext.scala:96)\n        at org.apache.spark.examples.SparkPi$.main(SparkPi.scala:31)\n        at org.apache.spark.examples.SparkPi.main(SparkPi.scala)\n", "comments": ["https://github.com/apache/spark/pull/303", "This is broken again.", "https://github.com/apache/spark/pull/560"], "derived": {"summary": "If you define SPARK_JAR and friends to use \"local:\" URIs, you cannot submit a job on a Yarn cluster. e.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Cannot launch jobs on Yarn cluster with \"local:\" scheme in SPARK_JAR - If you define SPARK_JAR and friends to use \"local:\" URIs, you cannot submit a job on a Yarn cluster. e."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/560"}]}}
{"project": "SPARK", "issue_id": "SPARK-1396", "title": "DAGScheduler has a memory leak for cancelled jobs", "status": "Resolved", "priority": "Major", "reporter": "Kay Ousterhout", "assignee": "Kay Ousterhout", "labels": [], "created": "2014-04-02T21:16:29.000+0000", "updated": "2014-04-08T08:29:09.000+0000", "description": "When a job is cancelled, the DAGScheduler doesn't clean up all of the relevant state, leading to a memory leak.", "comments": ["https://github.com/apache/spark/pull/305"], "derived": {"summary": "When a job is cancelled, the DAGScheduler doesn't clean up all of the relevant state, leading to a memory leak.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "DAGScheduler has a memory leak for cancelled jobs - When a job is cancelled, the DAGScheduler doesn't clean up all of the relevant state, leading to a memory leak."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/305"}]}}
{"project": "SPARK", "issue_id": "SPARK-1397", "title": "SparkListeners should be notified when stages are cancelled", "status": "Resolved", "priority": "Critical", "reporter": "Kay Ousterhout", "assignee": "Kay Ousterhout", "labels": [], "created": "2014-04-02T21:27:51.000+0000", "updated": "2014-04-08T23:08:12.000+0000", "description": "Right now, we don't explicitly tell SparkListeners when stages get cancelled.  This leads to weird behavior in the UI, where when jobs are cancelled, some stages never move out of the \"Running Stages\" table.", "comments": [], "derived": {"summary": "Right now, we don't explicitly tell SparkListeners when stages get cancelled. This leads to weird behavior in the UI, where when jobs are cancelled, some stages never move out of the \"Running Stages\" table.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "SparkListeners should be notified when stages are cancelled - Right now, we don't explicitly tell SparkListeners when stages get cancelled. This leads to weird behavior in the UI, where when jobs are cancelled, some stages never move out of the \"Running Stages\" table."}]}}
{"project": "SPARK", "issue_id": "SPARK-1398", "title": "Remove FindBugs jsr305 dependency", "status": "Resolved", "priority": "Minor", "reporter": "Mark Hamstra", "assignee": "Mark Hamstra", "labels": [], "created": "2014-04-02T22:23:50.000+0000", "updated": "2014-10-13T17:32:52.000+0000", "description": "We're not making much use of FindBugs at this point, but findbugs-2.0.x is a drop-in replacement for 1.3.9 and does offer significant improvements (http://findbugs.sourceforge.net/findbugs2.html), so it's probably where we want to be for Spark 1.0.", "comments": ["+1 I would also volunteer to run it and propose fixes for whatever it finds. But it's Java-centric, and doesn't do nearly as much of use for Scala.\n\nI should say that IntelliJ has a lot of good Scala inspections, and, would love to take a crack at addressing the stuff it is finding already. I again volunteer to do that.\n\nBut, it generates a lot of change to review and things are super busy now. \nAlthough, things will only get busier :)", "UPDATE: We can actually get away with not including the jsr305 jar at all, so I've changed this JIRA and associated PR to do just that instead of bumping the FindBugs version.", "From the PR discussion, this had to be reverted because of some build problems, so I assume removing this .jar is a WontFix"], "derived": {"summary": "We're not making much use of FindBugs at this point, but findbugs-2. 0.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Remove FindBugs jsr305 dependency - We're not making much use of FindBugs at this point, but findbugs-2. 0."}, {"q": "What updates or decisions were made in the discussion?", "a": "From the PR discussion, this had to be reverted because of some build problems, so I assume removing this .jar is a WontFix"}]}}
{"project": "SPARK", "issue_id": "SPARK-1399", "title": "Reason for Stage Failure should be shown in UI", "status": "Resolved", "priority": "Major", "reporter": "Kay Ousterhout", "assignee": "Nan Zhu", "labels": [], "created": "2014-04-03T00:15:08.000+0000", "updated": "2014-04-21T21:28:42.000+0000", "description": "Right now, we don't show why a stage failed in the UI.  We have this information, and it would be useful for users to see (e.g., to see that a stage was killed because the job was cancelled).", "comments": ["FYI this outstanding pull request changes this behavior: https://github.com/apache/spark/pull/309, so probably don't make sense to work on this until that gets resolved.", "made the PR: https://github.com/apache/spark/pull/421", "i think the user defined accumulators of every task need to shown in ui ", "Lianhui, I think that issue is unrelated (although valid) -- can you file a separate JIRA for that?"], "derived": {"summary": "Right now, we don't show why a stage failed in the UI. We have this information, and it would be useful for users to see (e.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Reason for Stage Failure should be shown in UI - Right now, we don't show why a stage failed in the UI. We have this information, and it would be useful for users to see (e."}, {"q": "What updates or decisions were made in the discussion?", "a": "Lianhui, I think that issue is unrelated (although valid) -- can you file a separate JIRA for that?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1400", "title": "Spark Streaming's received data is not cleaned up from BlockManagers when not needed any more", "status": "Closed", "priority": "Major", "reporter": "Tathagata Das", "assignee": null, "labels": [], "created": "2014-04-03T00:55:16.000+0000", "updated": "2014-04-25T20:36:33.000+0000", "description": "Spark Streaming generates BlockRDDs with the data received over the network. These data blocks are not automatically cleared, they spill over from memory based on LRU, which slows down processing.", "comments": ["Duplicate https://issues.apache.org/jira/browse/SPARK-1592"], "derived": {"summary": "Spark Streaming generates BlockRDDs with the data received over the network. These data blocks are not automatically cleared, they spill over from memory based on LRU, which slows down processing.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "Spark Streaming's received data is not cleaned up from BlockManagers when not needed any more - Spark Streaming generates BlockRDDs with the data received over the network. These data blocks are not automatically cleared, they spill over from memory based on LRU, which slows down processing."}, {"q": "What updates or decisions were made in the discussion?", "a": "Duplicate https://issues.apache.org/jira/browse/SPARK-1592"}]}}
{"project": "SPARK", "issue_id": "SPARK-1401", "title": "Use mapParitions instead of map to avoid creating expensive object in GradientDescent optimizer", "status": "Closed", "priority": "Minor", "reporter": "DB Tsai", "assignee": null, "labels": ["easyfix", "performance"], "created": "2014-04-03T01:04:08.000+0000", "updated": "2014-04-03T02:10:41.000+0000", "description": "In GradientDescent, currently, each row of the input data will create its own gradient matrix object, and then we sum them up in the reducer. \n\nWe found that when the number of features are in the order of thousands, it becomes the bottleneck. The situation was worse when we tested with Newton optimizer due to that the dimension of hessian matrix is so huge. \n\nIn our testing, when the # of features are hundreds of thousands, the GC kicks in for each row of input, and it sometimes brings down the workers. \n\nWith aggregating the lossSum, and gradientSum using mapPartitions, we solved the GC issue, and scale better with # of features.", "comments": ["In SPARK-1212, this issue is addressed by aggregate. Gonna close it."], "derived": {"summary": "In GradientDescent, currently, each row of the input data will create its own gradient matrix object, and then we sum them up in the reducer. We found that when the number of features are in the order of thousands, it becomes the bottleneck.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "Use mapParitions instead of map to avoid creating expensive object in GradientDescent optimizer - In GradientDescent, currently, each row of the input data will create its own gradient matrix object, and then we sum them up in the reducer. We found that when the number of features are in the order of thousands, it becomes the bottleneck."}, {"q": "What updates or decisions were made in the discussion?", "a": "In SPARK-1212, this issue is addressed by aggregate. Gonna close it."}]}}
{"project": "SPARK", "issue_id": "SPARK-1402", "title": "3 more compression algorithms for in-memory columnar storage", "status": "Resolved", "priority": "Blocker", "reporter": "Cheng Lian", "assignee": "Cheng Lian", "labels": ["compression"], "created": "2014-04-03T02:26:29.000+0000", "updated": "2014-04-08T21:25:12.000+0000", "description": "This is a followup of SPARK-1373: Compression for In-Memory Columnar storage\n\n3 more compression algorithms for in-memory columnar storage should be implemented:\n\n* BooleanBitSet\n* IntDelta\n* LongDelta", "comments": ["Corresponding PR: https://github.com/apache/spark/pull/330"], "derived": {"summary": "This is a followup of SPARK-1373: Compression for In-Memory Columnar storage\n\n3 more compression algorithms for in-memory columnar storage should be implemented:\n\n* BooleanBitSet\n* IntDelta\n* LongDelta.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "3 more compression algorithms for in-memory columnar storage - This is a followup of SPARK-1373: Compression for In-Memory Columnar storage\n\n3 more compression algorithms for in-memory columnar storage should be implemented:\n\n* BooleanBitSet\n* IntDelta\n* LongDelta."}, {"q": "What updates or decisions were made in the discussion?", "a": "Corresponding PR: https://github.com/apache/spark/pull/330"}]}}
{"project": "SPARK", "issue_id": "SPARK-1403", "title": "Spark on Mesos does not set Thread's context class loader", "status": "Resolved", "priority": "Blocker", "reporter": "Bharath Bhushan", "assignee": null, "labels": [], "created": "2014-04-03T02:54:25.000+0000", "updated": "2015-07-14T02:59:57.000+0000", "description": "I can run spark 0.9.0 on mesos but not spark 1.0.0. This is because the spark executor on mesos slave throws a  java.lang.ClassNotFoundException for org.apache.spark.serializer.JavaSerializer.\n\nThe lengthy discussion is here: http://apache-spark-user-list.1001560.n3.nabble.com/java-lang-ClassNotFoundException-spark-on-mesos-td3510.html#a3513\n", "comments": ["In 0.9.0 I see that the classLoader is org.apache.spark.repl.ExecutorClassLoader (in SparkEnv.scala)\nIn 1.0.0 I see that the classLoader is null. My guess is that this should not be null.", "The underlying issue here is that we've made assumptions in various parts of the codebase that the context classloader is set on a thread. In general, we should relax these assumptions and just fallback to the classloader that loaded Spark. As a workaround this patch:\n\nhttps://github.com/apache/spark/pull/322/files\n\njust manually sets the classloader to the system class loader.", "I want to reopen this bug, because I can reproduce it at spark 1.3.0 + mesos 0.21.1 with \"run-example SparkPi\" \n", "A similar problem has been reported while running Spark on Mesos using MapR distribution. The current thread's context class loader is NULL inside the executor process causing NPE in MapR code.\n\nRefer to this discussion.\nhttp://answers.mapr.com/questions/163353/spark-from-apache-downloads-site-for-mapr.html#answer-163484", "Multiple users reporting this is occuring again in 1.3", "Per Kannan: Seeing this in Spark 1.2.2, 1.3.0, an 1.3.1.  1.2.0 does not produce the error. The case I have is a NPE caused by what appears to be this bug. \n\nTasks fail with the stack trace below.  Per a discussion on the relavent code in the MapR Shim, ] it is trying to get a root classloader to use for loading a bunch of classes. It uses the thread's context class loader (TCCL) and keeps going up the parent chain. The Null being sent causes the NPE.  The concern here is not so much (in this case) the MapR component not handling the NPE (Althought that should be addressed too) but that Spark is switching between versions.  Happy to provide any other details. \n\nFull Error on 1.3.1 on Mesos:\n15/05/19 09:31:26 INFO MemoryStore: MemoryStore started with capacity 1060.3 MB java.lang.NullPointerException at com.mapr.fs.ShimLoader.getRootClassLoader(ShimLoader.java:96) at com.mapr.fs.ShimLoader.injectNativeLoader(ShimLoader.java:232) at com.mapr.fs.ShimLoader.load(ShimLoader.java:194) at org.apache.hadoop.conf.CoreDefaultProperties.(CoreDefaultProperties.java:60) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:274) at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:1847) at org.apache.hadoop.conf.Configuration.getProperties(Configuration.java:2062) at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2272) at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2224) at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2141) at org.apache.hadoop.conf.Configuration.set(Configuration.java:992) at org.apache.hadoop.conf.Configuration.set(Configuration.java:966) at org.apache.spark.deploy.SparkHadoopUtil.newConfiguration(SparkHadoopUtil.scala:98) at org.apache.spark.deploy.SparkHadoopUtil.(SparkHadoopUtil.scala:43) at org.apache.spark.deploy.SparkHadoopUtil$.(SparkHadoopUtil.scala:220) at org.apache.spark.deploy.SparkHadoopUtil$.(SparkHadoopUtil.scala) at org.apache.spark.util.Utils$.getSparkOrYarnConfig(Utils.scala:1959) at org.apache.spark.storage.BlockManager.(BlockManager.scala:104) at org.apache.spark.storage.BlockManager.(BlockManager.scala:179) at org.apache.spark.SparkEnv$.create(SparkEnv.scala:310) at org.apache.spark.SparkEnv$.createExecutorEnv(SparkEnv.scala:186) at org.apache.spark.executor.MesosExecutorBackend.registered(MesosExecutorBackend.scala:70) java.lang.RuntimeException: Failure loading MapRClient. at com.mapr.fs.ShimLoader.injectNativeLoader(ShimLoader.java:283) at com.mapr.fs.ShimLoader.load(ShimLoader.java:194) at org.apache.hadoop.conf.CoreDefaultProperties.(CoreDefaultProperties.java:60) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:274) at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:1847) at org.apache.hadoop.conf.Configuration.getProperties(Configuration.java:2062) at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2272) at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2224) at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2141) at org.apache.hadoop.conf.Configuration.set(Configuration.java:992) at org.apache.hadoop.conf.Configuration.set(Configuration.java:966) at org.apache.spark.deploy.SparkHadoopUtil.newConfiguration(SparkHadoopUtil.scala:98) at org.apache.spark.deploy.SparkHadoopUtil.(SparkHadoopUtil.scala:43) at org.apache.spark.deploy.SparkHadoopUtil$.(SparkHadoopUtil.scala:220) at org.apache.spark.deploy.SparkHadoopUtil$.(SparkHadoopUtil.scala) at org.apache.spark.util.Utils$.getSparkOrYarnConfig(Utils.scala:1959) at org.apache.spark.storage.BlockManager.(BlockManager.scala:104) at org.apache.spark.storage.BlockManager.(BlockManager.scala:179) at org.apache.spark.SparkEnv$.create(SparkEnv.scala:310) at org.apache.spark.SparkEnv$.createExecutorEnv(SparkEnv.scala:186) at org.apache.spark.executor.MesosExecutorBackend.registered(MesosExecutorBackend.scala:70)", "This is occurring still in 1.4.0\n\nI've attempted to look through the code to determine what may have changed but the Class Loading code has shifted around quite a bit, and I could not pinpoint when the change, or which update changed the code to break it again.  If there is anything I can do to help troubleshoot, please advise. \n\n", "[~pwendell] at your convenience can you please triage this bug. It was originally opened as a blocker. I reopened but am not sure if it's a release blocker. I set the target release to 1.5 just so it shows up in triage queries since it's a reopened bug...", "Hey All,\n\nThis issue should remain fixed. [~mandoskippy] I think you are just running into a different issue that is also in some way related to classloading.\n\nCan you open a new JIRA for your issue, paste in the stack trace and give as much information as possible about the environment? Thanks!"], "derived": {"summary": "I can run spark 0. 9.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Spark on Mesos does not set Thread's context class loader - I can run spark 0. 9."}, {"q": "What updates or decisions were made in the discussion?", "a": "Hey All,\n\nThis issue should remain fixed. [~mandoskippy] I think you are just running into a different issue that is also in some way related to classloading.\n\nCan you open a new JIRA for your issue, paste in the stack trace and give as much information as possible about the environment? Thanks!"}]}}
{"project": "SPARK", "issue_id": "SPARK-1404", "title": "Non-exported spark-env.sh variables are no longer present in spark-shell", "status": "Resolved", "priority": "Major", "reporter": "Aaron Davidson", "assignee": "Aaron Davidson", "labels": [], "created": "2014-04-03T03:10:05.000+0000", "updated": "2014-04-04T16:50:52.000+0000", "description": "spark-env.sh is now only loaded at most once. This means that if users had variable settings like\n\n{{code}}\nFOO=1\nexport BAR=2\n{{code}}\n\nthen a user who runs spark-shell may source spark-env.sh in \"spark-shell\" to have the non-exported variables disappear from scope when \"spark-class\" is called. In effect, BAR is visible within Spark, but not FOO.", "comments": [], "derived": {"summary": "spark-env. sh is now only loaded at most once.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Non-exported spark-env.sh variables are no longer present in spark-shell - spark-env. sh is now only loaded at most once."}]}}
{"project": "SPARK", "issue_id": "SPARK-1405", "title": "parallel Latent Dirichlet Allocation (LDA) atop of spark in MLlib", "status": "Resolved", "priority": "Critical", "reporter": "Xusen Yin", "assignee": "Joseph K. Bradley", "labels": ["features"], "created": "2014-04-03T06:21:25.000+0000", "updated": "2016-02-19T18:10:04.000+0000", "description": "Latent Dirichlet Allocation (a.k.a. LDA) is a topic model which extracts topics from text corpus. Different with current machine learning algorithms in MLlib, instead of using optimization algorithms such as gradient desent, LDA uses expectation algorithms such as Gibbs sampling. \n\nIn this PR, I prepare a LDA implementation based on Gibbs sampling, with a wholeTextFiles API (solved yet), a word segmentation (import from Lucene), and a Gibbs sampling core.\n\nAlgorithm survey from Pedro: https://docs.google.com/document/d/13MfroPXEEGKgaQaZlHkg1wdJMtCN5d8aHJuVkiOrOK4/edit?usp=sharing\nAPI design doc from Joseph: https://docs.google.com/document/d/1kSsDqTeZMEB94Bs4GTd0mvdAmduvZSSkpoSfn-seAzo/edit?usp=sharing", "comments": ["btw, could this please be merged with the main? there are some conflicts", "User 'witgo' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1983", "Hi all\n[PR 1983|https://github.com/apache/spark/pull/1983] is OK to review. ", "[~xusen] and [~gq] Thanks for working on LDA! The major feedback of your implementations is the how models are stored.\n\n[~josephkb] and I had an offline discussion with Evan and Joey (AMPLab) on LDA's interface and implementation. For the input data, we recommend `RDD[(Int, Vector)]`, where each pair consists of the document id and its word distribution, which may come from a text vectorizer. For the output model, because the LDA model is huge (W*K + D*K), where W is the number of words, D is the number of documents, and K is the number of topics, we should store the model distributively for better scalability, e.g., in RDD[(Int, Vector)], or using Long for ids. Joey already had an LDA implementation using GraphX: \n\nhttps://github.com/jegonzal/graphx/blob/LDA/graph/src/main/scala/org/apache/spark/graph/algorithms/TopicModeling.scala\n\nWith GraphX, we can treat documents and words as graph nodes and topic assignments as edges. The code is easy to understand.\n\nThere is also a paper describing a distributed implementation of LDA on Spark that uses a DGSD-like partitioning of the doc-word matrix:\n\nhttp://jmlr.org/proceedings/papers/v36/qiu14.pdf\n\nAnyone interested in helping test those implementations?", "User 'witgo' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/2388", "Hi All. Just wanted to quickly introduce myself quickly. I am undergrad at UC Berkeley working in the Amplab and in particular with LDA (continuation of a grad class final project from last spring).\n\nGenerally speaking my focus will be to use one LDA implementation as a baseline (probably Joey's since it is fully distributed in all parts, particularly the token-topic matrix), write unit tests + test cases, and benchmark it at scale.", "Here are some related papers:\n[Towards Topic Modeling for Big Datd|http://arxiv.org/pdf/1405.4402v1.pdf]\n[Efficient Methods for Topic Model Inference on Streaming Document Collections|http://people.cs.umass.edu/~lmyao/papers/fast-topic-model10.pdf].\n", "[~Guoqiang Li] and [~pedrorodriguez], since there are already 4~5 implementations of LDA on Spark and [~dlwh] is also interested in one with partial labels, we do need to coordinate to avoid duplicate effort. I think the TODOs are:\n\n0. Make progress updates frequently.\n1. Test Joey's implementation and Guoqiang's (both on GraphX) on some common datasets. We also need to verify the correctness of the output, by comparing the result with some single machine solvers.\n2. Discuss the public APIs in MLlib. Because GraphX is an alpha component, we should not expose GraphX APIs in MLlib. See my previous comments on the input and model types.\n3. Have a standard implementation of LDA with Gibbs Sampling in MLlib. The target is v1.2, which means it should be merged by the end of Nov. Improvements can be made in future releases.\n\nCould you share your timeline? Thanks!", "[~mengxr], definitely a good idea to be coordinated about it. I have been working with Evan so have been giving status updates and making todos with him. I will post here on progress updates as well. \n\nI have been working on creating a design doc/reference which you can find here: https://docs.google.com/document/d/13MfroPXEEGKgaQaZlHkg1wdJMtCN5d8aHJuVkiOrOK4/edit?usp=sharing\n\nIt is for a large part, a way for us/me to keep notes while working, but I would like to take some of it and convert it to documentation. It primarily contains\n1. Relevant links to papers/code/repositories\n2. Thorough explanation/documentation of LDA and motivation behind the graph implementation (Joey's version)\n3. Testing steps (which data sets on what)\n4. Current todos (perhaps we should post them here primarily and update doc for consistency).\n\n1. Currently I am working on testing (unit test functions and correctness testing), refactoring, and extending Joey's implementation. The objective for this week is to have the mini-test running (a set of ~10 documents which acts as a sanity check). Goal for early next week is to be running on NIPS. I think the majority of time to get there will be putting the dataset in a parseable format (remove equations, stop words...) and insuring that the result looks correct. \n\nTo that end, we plan on running the same datasets through Graphlab for benchmarking machine/ML performance and a python implementation for ML performance/correctness. \n\nOnce we are there, the plan is to start looking at running on wikipedia.\n\n2. The code I am currently working on lives here:\nhttps://github.com/EntilZha/spark\nhttps://github.com/EntilZha/spark/blob/LDA/graphx/src/main/scala/org/apache/spark/graphx/lib/TopicModeling.scala\nwhich is within GraphX, with the other graph based algorithms.\n\n3. Prior to knowing about Joey's graph implementation, I wrote my own for a final project. I stopped working on it since the graph implementation should be more performant. Probably a good point of discussion if there should be a \"standard\" and graph implementation together. When you reference standard implementation, is there a particular implementation you are referring to that I can look at?\n\nTLDR timeline:\nEnd of this week: mini-dataset for sanity check + refactoring code + unit testing\nNext week: Format NIPS for input + run NIPS data set on Spark, GraphLab, and Python LDA. I will be away from Berkeley at a conference, but hope to still get those done.\nFrom there, we would like to get running on larger datasets for performance testing.", "[~pedrorodriguez] Thanks for the update and sharing the timeline! By \"standard implementation\" I mean LDA with Gibbs Sampling without special optimization, but still using GraphX. We should make the first PR simple, so it can pass the code review before the feature freeze for 1.2 (end of Oct). I had an offline discussion with [~gq]. He will do some performance comparison of existing LDA implementations. Once we have the numbers, let's pick one design and work together.\n\nCould you add http://jmlr.org/proceedings/papers/v36/qiu14.pdf to your design doc? The partitioning scheme they used is interesting, which we can explore in the future.", "Hi everyone.\nI did some performance comparison of PR 2388(contains a lot of optimization.) and Joey's implementation.\n\nTraining data: 253064 document, 29696335 words,  75496 unique words.\n All tests were run on precisely the same 4 node cluster, 36 executors(36 cores, 216g memory).\nIterative training 150 times, time-consuming in the following table\n\n||The number of topics||[PR 2388|https://github.com/apache/spark/pull/2388]||[Joey's implementation|https://github.com/jegonzal/graphx/blob/LDA/graph/src/main/scala/org/apache/spark/graph/algorithms/TopicModeling.scala]||\n|100 |43.95|47.98|\n|500|68.6|132.9|\n|2000|79.75|443|\n\n\n!performance_comparison.png!", "Hi everyone,\n\nSorry for taking so long for me to reply. As part of some contract work with Alpine, I've been working on yet another LDA implementation. We're actually implementing partially labeled lda[1], which is a strict generalization of LDA. The implementation is based on EM MAP inference, rather than Gibbs; EM has been shown to converge much more quickly (in number of iterations and wall time) and to better optima than Gibbs LDA[2]. It also has an interpretation when run in parallel. Collapsed Gibbs Sampling when run in parallel has no guarantees. EM is still guaranteed to converge to a local optimum.\n\nI'll post the code as soon as I clear it with Alpine.\n\n[1]http://nlp.stanford.edu/~dramage/papers/pldp-kdd11.pdf\n[2] http://mimno.infosci.cornell.edu/info6150/readings/UAI_09.pdf", "I should also mention it needs less space. Gibbs LDA needs to hold on to O(numTokensInDocument) words of memory. EM doesn't need any (persistent) memory at all beyond what's needed to represent the document.\n\nEM also only needs randomness for the initialization, which makes it easier to ensure that a serial implementation is doing the exact same thing as the parallel.", "Hi everyone\n[The PR 2388|https://github.com/apache/spark/pull/2388] is OK to review.", "Hi Guoqiang - is it correct that your runtimes are reported in minutes as opposed to seconds? In your tests, have you cached the input data? 45 minutes for 150 iterations over this small dataset seems slow to me. It would be great to get an idea of where the bottleneck is coming from. Is it the Gibbs step or something else?\n\nIs it possible to share the dataset you used for these experiments?\n\nThanks!", "Hi everyone\nThis is the latest performance test results \nAll tests were run on precisely the same 4 node cluster.\n36 executors(a total of36 cores, 216g memory).\nTraining data: 253064 document, 29696335 words, 75496 distinct words.Training iteration 150 times.\nThe spark configuration:\n{noformat}\nspark.akka.frameSize   20\nspark.executor.instances 36\nspark.rdd.compress true\nspark.executor.memory   6g\nspark.default.parallelism  72\nspark.broadcast.blockSize  8192\nspark.storage.memoryFraction 0.2\nspark.serializer org.apache.spark.serializer.KryoSerializer\nspark.kryo.registrator org.apache.spark.mllib.feature.TopicModelingKryoRegistrator\n{noformat}\n\nTime-consuming in the following table:\n\n||The number of topics||Time(minutes)\n|2000 |42.26\n|10000|49.47\n|100000|58.20\n|1000000|125.43\n", "I would like to compare the LSA formulations (sparse coding and PLSA with least square loss) from https://issues.apache.org/jira/browse/SPARK-2426 with LDA\n\nI added MAP metric for examples.MovieLensALS in https://issues.apache.org/jira/browse/SPARK-4231 but I am not sure how MAP can be used for topic modeling datasets....we need some perplexity measures...\n\nCould you guys please point me to the dataset and the quality measures that are being benchmarked on the LDA PR so that I can also test the LSA formulations in parallel ?\n", "I am not super familiar with LSA, so hopefully this seems reasonable.\n\nOn LDA, we are doing performance tests and tuning appropriately using partitions of wikipedia. Apart from that, I started working on a data generator based on the LDA generative model. This should be helpful for generating arbitrary parameter testing data (for example, it seems that PR2388 is using a data set with many documents contains few words, while wiki is the opposite situation). \n\nSo far our quality measure has been to run against graphlab LDA. Specifically, we ran against the NIPS data set to make sure our convergence for the neg log likelihood was reasonable. If thats helpful, I can give you that data set in a format which is easier to parse than what is online.", "For LSA you can find references on the PR. Microsoft paper ran LSA on wiki dataset but they compared map and ndcg measures...\n\nNIPS and wiki datasets both will help....I was thinking about wiki....could you please add the NIPS dataset as well and reference for it ?\n\nI will look into graphlab lda and what measures they are running...", "I will take a look at those when I get a chance (after AMPCamp probably).\n\nHere is a link to my GDrive with the NIPS data set: https://drive.google.com/folderview?id=0B0zg9iArSQQaRE1OR1lLQWNjZG8&usp=sharing\nThe github repo here is my benchmarking/testing wrapper program around LDA in case you want to see how I parse it in: https://github.com/EntilZha/nips-lda-spark\n\nGraphlab actually uses the same metric that I/we are using for LDA. If you paste this into a tex editor it is easier to see:\n\\mathcal{L}( w | z) & = T * \\left( \\log\\Gamma(W * \\beta) - W * \\log\\Gamma(\\beta) \\right) + \\\\\n    & \\sum_{t} \\left( \\left(\\sum_{w} \\log\\Gamma(N_{wt} + \\beta)\\right) -\n           \\log\\Gamma\\left( W * \\beta + \\sum_{w} N_{wt}  \\right) \\right) \\\\\n    & = T * \\left( \\log\\Gamma(W * \\beta) - W * \\log\\Gamma(\\beta) \\right) -\n        \\sum_{t} \\log\\Gamma\\left( W * \\beta + N_{t}  \\right) + \\\\\n    & \\sum_{w} \\sum_{t} \\log\\Gamma(N_{wt} + \\beta)   \\\\\n    \\\\\n    \\mathcal{L}(z) & = D * \\left(\\log\\Gamma(T * \\alpha) - T * \\log\\Gamma(\\alpha) \\right) + \\\\\n    & \\sum_{d} \\left( \\left(\\sum_{t}\\log\\Gamma(N_{td} + \\alpha)\\right) -\n        \\log\\Gamma\\left( T * \\alpha + \\sum_{t} N_{td} \\right) \\right) \\\\\n    \\\\\n    \\mathcal{L}(w,z) & = \\mathcal{L}(w | z) + \\mathcal{L}(z)\\\\\n    N_{td} =\\text{number of tokens with topic t in document d}\\\\\n    N_{wt} =\\text{number of tokens with topic t for word w}\n\nLDA (roughly) converges to -2.8e7 while graphlab (roughly) converges to -2.65e7 for comparable runtime (not iterations, couldn't figure out an easy way to get num iterations ran oddly).", "[~witgo] where can I access your dataset ? I got the NIPS dataset from Pedro but here the runtimes reported are on a different dataset...also should we use the same accuracy measure that Pedro is using ?", "[~debasish83] Yes, we should use the same dataset and  dataset processing methods should be the same\nHow about useing  the following the data and code? \nhttps://github.com/EntilZha/nips-lda-spark ", "We need a larger dataset as well where topics go to the range of 10000+...That range will stress factorization based LSA formulations since there is broadcast of factors at each step....NIPS dataset is small...Let's start with that...But we should test a large dataset like wikipedia as well..If there is a pre-processed version from either mahout or scikit-learn we can use that ?", "[~pedrorodriguez] did you write the metric in your repo as well ? That way I don't have to code it up again..", "I don't know of a larger data set, but I am working on an LDA data set generator based on the generative model. It should be good for benchmark testing but still be reasonable from the ML perspective.\n\nThe metric is in the LDA code (which is turned on and off with a flag on the LDA model). You can find it here in the logLikelihood function:\nhttps://github.com/EntilZha/spark/blob/LDA/graphx/src/main/scala/org/apache/spark/graphx/lib/LDA.scala", "OK, Where is the download URL?", "Not sure which download URL you are referring to?", "Sorry, I mean the wikipedia data download URL. How much text we need it? I think one billion words is appropriate.", "NIPS dataset is common for PLSA and additive regularization based matrix factorization formulations as well since the experiments in this paper focused on the NIPS dataset as well... \nhttp://www.machinelearning.ru/wiki/images/1/1f/Voron14aist.pdf\n\nI will be using NIPS dataset for quality experiments but for scaling experiments, wiki data is good...wiki data was demo-ed by Databricks in last spark summit...it will be great if we can get it from that demo", "[~gq] - Those are great numbers for a very high number of topics - it's a little tough to follow what's leading to the super-linear scaling in #topics in your code, though. Are you using FastLDA or something similar to speed up sampling? (http://www.ics.uci.edu/~newman/pubs/fastlda.pdf)\n\nPedro has been testing on a wikipedia dump on s3 which I provided. It's XML formatted, one document per line, so it's easy to parse. I will copy this to a requester-pays bucket (which will be free if you run your experiments on ec2) now so that everyone working on this can use it for testing.\n\nNIPS dataset seems fine for small-scale testing, but I think it's important that we test this implementation across a range of values for documents, words, topics, and tokens - hence, I think the data generator that Pedro is working on is a really good idea (and follows the convention of the existing data generators in MLlib). We'll have to be a little careful here, because some of the methods for making LDA fast rely on the fact that it tends to converge fast, and I expect that data generated by the model will be much easier to fit than real data.\n\nAlso, can we try and be consistent in our terminology - getting the # of unique words confused with all the words in a corpus is easy. I propose \"words\" and \"tokens\" for these two things.", "[~sparks] that will be awesome...I should be fine running experiments on EC2...", "Bucket has been created: \ns3://files.sparks.requester.pays/enwiki_category_text/ - All in all there are 181 ~50mb files (actually closer to 10GB). \n\nIt probably makes sense to use http://sweble.org/ or something to strip the boilerplate, etc. from the documents for the purposes of topic modeling.", "Finished an initial implementation of an LDA data generator. I have done some initial testing and it seems reasonable, but just initial testing at the moment. Will be looking at metrics other than \"it looks good\" to make sure that the data being generated is correct.\n\nImplementation: https://github.com/EntilZha/spark/blob/LDA/mllib/src/main/scala/org/apache/spark/mllib/util/LDADataGenerator.scala", "Hi all, there are several possible Spark LDA implementations out there (in PRs or public Github repos), and I believe the best thing to do is to:\n* settle on a simple API + implementation to start with\n* switch existing PRs which use alternate algorithms (EM, Gibbs sampling, variational EM, etc.) to use the same interface, where the inference algorithm can be set via a parameter\n\nTowards this goal, I've written [this design doc | https://docs.google.com/document/d/1kSsDqTeZMEB94Bs4GTd0mvdAmduvZSSkpoSfn-seAzo/edit?usp=sharing] which focuses on the API rather than algorithm design.  I'm also preparing a PR based on the simplest implementation I have been able to find, written by [~dlwh].  I should be able to submit it in a day or so.  It uses (non-variational) EM, which should be fast albeit maybe not as accurate as Gibbs sampling.\n\nI'd of course appreciate feedback on the design doc, as well as the actual PR.  It will be great to settle on a public API which can satisfy the many existing implementations of LDA in Spark.\n\nWhen we merge the initial LDA PR, [~mengxr] will be sure to include all of those who have participated as authors of Spark LDA PRs: [~akopich], [~witgo], [~yinxusen], [~dlwh], Pedro, [~jegonzal]\n", "Great design doc and solid proposal. \n\nI noticed the online variational EM mentioned in the doc, for which I have developed a spark implementation. The work was based on an actual customer scenario and has exhibited remarkable speed and economized memory usage. The result is as good as the batch LDA, and with handy support of stream text from the online nature. \n\nRight now we are turning it into graph-based and will perform further evaluation afterwards.  The algorithm looks promising to us and can be helpful in many cases. For now I dont find online LDA will make the API design more complicated as its more like an incremental work. Just want to bring up the possibility in case anyone finds a conflict.\n\nReference: [online LDA|https://www.cs.princeton.edu/~blei/papers/HoffmanBleiBach2010b.pdf] by [Matt Hoffman|http://www.cs.princeton.edu/~mdhoffma/] and [David M.Blei|http://www.cs.princeton.edu/~blei/topicmodeling.html] ", "That's great to hear that online variational has worked well for you so far.  As far as API design, I agree that the changes to the model API would be small if any.  I'm not as sure about the Estimator (algorithm) API, but it could probably follow existing streaming ML algorithms.", "Second on nice design doc and proposal. I agree that having an API design to satisfy with implementations would work well and allow for different algorithms.\n\nI am meeting with Evan later today and probably will talk about this and where we are on the LDA implementation I have been working on. I am currently getting some performance benchmarks related to scaling with respect to number of topics, which should be super-linear using the algorithm in this paper:\nhttp://www.ics.uci.edu/~newman/pubs/fastlda.pdf\n\nOverall status I think, is that the scaling is good, but it would be nice to improve the constant multiplier of work if possible. Beyond that, perhaps its time to start running larger scale performance tests on ec2 clusters. If all goes well, I am hoping to refactor to satisfy the API proposal and open a PR.", "Pedro, that sounds great.  I wouldn't mind at all if the underlying implementation in my PR were replaced by something better.", "Sounds good Joseph. Have some good news. I finished initial testing of runtime vs scaling of topics this morning. You can find the raw numbers below. The top set is from the fast lda from the paper above, the bottom set is using ordinary sampling. They are resultwise equivalent, but the fast version takes advantage that the majority of the \"mass\" of the cdf is concentrated in only a few topics, so it is smart to check those first.\nhttps://docs.google.com/spreadsheets/d/1RZLsnfLL2XmKWNJ6kPM_KaiDkTfQOYdXdG5EywlY3Os/edit?usp=sharing\n\nHere are descriptions of each time:\nSetup: time for setup operations, such as creating the graph\nResample: time used in the Gibbs sampling step to sample new topics\nUpdate: time spent applying the topic deltas to the histogram of each vertex\nGlobal: time spent updating the global histogram\n\nAs can be seen in the data, the time for resampling flattens/stops increasing after ~500 topics, achieving superlinear/constant compute time with number of topics, which is exactly what fast (Gibbs sampling) lda is suppose to do.\n\nAll these tests were run locally on my macbook pro with 3GB executor memory. I used the data generator with these parameters:\nalpha=beta=.01\nnDocs=300\nnWords=14036 (number of unique words)\nnTokensPerDoc=1000\nNumber of Tokens (equivalent to nDocs*nTokensPerDoc)=300000\nNumber of Iterations=10\n\nSince this was a fairly successful test, my next task is running on an ec2 cluster. To be able to compare easily to Guoqiang's testing, I will use a similar 4 node cluster and change the data generator parameters to create a similar data set, but run for less iterations (probably 10x less, so 15). My goal is to get that done over the weekend. If that goes well, then I will start refactoring to match the proposed API by Joseph.", "[~josephkb], I've read your proposal and I suggest to consider Stochastic Gradient Langevin Dynamics [1]. It was shown be ~100 times faster than Gibbs sampling [2]. Though, I'm not sure if it's implementable in terms of RDD. \n\n[1] http://papers.nips.cc/paper/4883-stochastic-gradient-riemannian-langevin-dynamics-on-the-probability-simplex.pdf\n[2] http://www.ics.uci.edu/~sungjia/icml2014_dist_v0.2.pdf", "[~pedrorodriguez]  Thanks for the test results!  Tests (including on actual clusters) will be great to verify improvements by future PRs.\n\n[~akopich]  Thank you for the references!  I'll add them to the design doc.\n\nPR almost ready...", "User 'jkbradley' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/4047", "Worked on some preliminary testing results, but ran into a snag which greatly limited what I could test.\n\nUsed 4 EC2 r3.2xlarge instances, which totals 32 executors with 215GB memory, so fairly close to tests run by [~gq].\n\nI have been using the data generator I wrote, but had not previously stress tested it for a large number of topics, and it is failing for some number of topics between 100-1000. I made some optimizations with mapPartitions, but still run into GC overhead problems. I am unsure if this is my fault or if somewhere breeze is generating lots of garbage when sampling from the Dirichlet/Multinomial distributions. Extra eyes would be great to see if it looks reasonable from GC perspective: https://github.com/EntilZha/spark/blob/LDA/mllib/src/main/scala/org/apache/spark/mllib/util/LDADataGenerator.scala\n\nAlternatively, [~gq], would it be possible to either get the data set you used for testing or are you using a different dataset for testing  [~josephkb]? It would be useful to have a common way to compare implementation performance\n\nHere are the test results I did get:\nDocs: 250,000\nWords: 75,000\nTokens: 30,000,000\nIterations: 15\nTopics: 100\nAlpha=Beta=.01\n\nSetup Time: 20s\nResampling Time: 80s\nUpdating Counts Time: 53s\nGlobal Counts Time: 4s\nTotal Time: 170s (2.83m)\n\nThis looks like a good improvement over the original numbers (red blue plot) extrapolation that this ran for 10x less iterations, it would be 28m vs ~45m. I am fairly confident that this time will scale fairly well with number of topics based on the previous test results I posted. I would be more than happy to run more benchmark tests if I can get access to the data set used for the other tests or what Joseph is using to test his PR. I am also going to start working on refactoring into Joseph's API, will open a PR once that is done, probably later this week. It would be great to have both Gibbs and EM for next release.", "We can use the demo scripts in word2vec to get the same corpus. \n{code}\nnormalize_text() {\n  awk '{print tolower($0);}' | sed -e \"s//'/g\" -e \"s//'/g\" -e \"s/''/ /g\" -e \"s/'/ ' /g\" -e \"s//\\\"/g\" -e \"s//\\\"/g\" \\\n  -e 's/\"/ \" /g' -e 's/\\./ \\. /g' -e 's/<br \\/>/ /g' -e 's/, / , /g' -e 's/(/ ( /g' -e 's/)/ ) /g' -e 's/\\!/ \\! /g' \\\n  -e 's/\\?/ \\? /g' -e 's/\\;/ /g' -e 's/\\:/ /g' -e 's/-/ - /g' -e 's/=/ /g' -e 's/=/ /g' -e 's/*/ /g' -e 's/|/ /g' \\\n  -e 's// /g' | tr 0-9 \" \"\n}\nwget http://www.statmt.org/wmt14/training-monolingual-news-crawl/news.2013.en.shuffled.gz\ngzip -d news.2013.en.shuffled.gz\nnormalize_text < news.2013.en.shuffled > data.txt\n{code}", "I'll try out the statmt dataset if that will be easier for everyone to access.\n\nUPDATE: Note: The statmt dataset is an odd one since each \"document\" is a single sentence.  I'll still try it since I could imagine a lot of users wanting to run LDA on tweets or other short documents, but I might continue with my previous tests first.", "Indeed, I have a couple students whose assignments involve Twitter data, and I am considering adding LDA to the mix. I would like to test it on our corpus... provided this feature is usable by a Spark novice: is it ?\n", "It has not yet been merged into Spark master, but hopefully will be soon.  The initial version should be usable.  We will continue to add improvements to the API, especially helper functionality such as prediction and summaries about the learned model.", "Here is a sampling faster branch(work in progress): https://github.com/witgo/spark/tree/lda_MH\n[It's|https://github.com/witgo/spark/tree/lda_MH] computational complexity is O(log(K))  K is the number of topic \n[#2388|https://github.com/apache/spark/pull/2388]'s computational complexity is  O(log(K)+ Nkd) ,  K is the number of topic  and  Ndk\n is the number of tokens in document d that are assigned to topic k", "Issue resolved by pull request 4047\n[https://github.com/apache/spark/pull/4047]", "Hi everyone, I'm sharing an implementation of [Online LDA|https://www.cs.princeton.edu/~blei/papers/HoffmanBleiBach2010b.pdf] at https://github.com/hhbyyh/OnlineLDA_Spark, and hope it can be helpful for anyone interested.\n\nThe work is based on the research from [Matt Hoffman|http://www.cs.princeton.edu/~mdhoffma/] and [David M. Blei|http://www.cs.princeton.edu/~blei/topicmodeling.html]. Based on its online nature, the algorithm \n1. scans the corpus (doc sets) only once. Thus it needs not locally store or collect the documents and can be handily applied to streaming document collections.\n2. breaks the massive corps into mini batches and takes one batch at a time, which downgrades memory and time consumption.\n3. approximates the posterior as well as traditional approaches. (generate comparable or better results).\n\nIn demo runs, current implementation (with many details to be improved)\n1. processed 8 millions short articles (Stackoverflow posts titles, avg length 9, K=10) in 15 minutes.\n2. processed entire English wiki dump set (5876K documents , avg length ~900 words/per doc, 30G on disk, K=10) in 2 hours and 17 minutes \nusing a 4-node cluster(20G memory, can be much less)\n\nTrial and suggestions are most welcome!", "Thanks everyone for all of your contributions, help and feedback!  The initial LDA implementation has been merged, but there are many improvements which remain to be done.  I've put a list of JIRAs here [https://issues.apache.org/jira/browse/SPARK-5572]\n\n\n[~yuhaoyan] +1 for online LDA.  (I made a JIRA for it.)"], "derived": {"summary": "Latent Dirichlet Allocation (a. k.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "parallel Latent Dirichlet Allocation (LDA) atop of spark in MLlib - Latent Dirichlet Allocation (a. k."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks everyone for all of your contributions, help and feedback!  The initial LDA implementation has been merged, but there are many improvements which remain to be done.  I've put a list of JIRAs here [https://issues.apache.org/jira/browse/SPARK-5572]\n\n\n[~yuhaoyan] +1 for online LDA.  (I made a JIRA for it.)"}]}}
{"project": "SPARK", "issue_id": "SPARK-1406", "title": "PMML model evaluation support via MLib", "status": "Resolved", "priority": "Major", "reporter": "Thomas Darimont", "assignee": "Vincenzo Selvaggio", "labels": [], "created": "2014-04-03T11:33:19.000+0000", "updated": "2015-07-06T18:58:28.000+0000", "description": "It would be useful if spark would provide support the evaluation of PMML models (http://www.dmg.org/v4-2/GeneralStructure.html).\n\nThis would allow to use analytical models that were created with a statistical modeling tool like R, SAS, SPSS, etc. with Spark (MLib) which would perform the actual model evaluation for a given input tuple. The PMML model would then just contain the \"parameterization\" of an analytical model.\n\nOther projects like JPMML-Evaluator do a similar thing.\nhttps://github.com/jpmml/jpmml/tree/master/pmml-evaluator", "comments": ["Yes, I was going to say, there's already a pretty good implementation of this. Why not simply call pmml-evaluator from within a Spark-based app to do scoring? does it really need any particular support in MLlib?\n\nMLlib does not create PMML right now, which would seem like to something to tackle before scoring them anyway.\n\nI have a meta-concern about piling on scope at such an early stage.", "Hi Sean,\n\nthanks for responding so quickly :)\n\nSure you can do that of course (thats what I currently do), but there IMHO many interesting use cases that would benefit from having direct PMML support, e.g.:\n1) Initialize an algorithm with a set of prepared parameters by loading a PMML file and evaluate the algorthm with spark's infrastructure.\n2) Abstract the configuration or construction of an Algorithm via some kind of Producer that gets the PMML model as an Input and returns a fully configured Spark representation of the algorithm which is encoded in the PMML.\n3) Support hot-replacing an algorithm (configuration) at runtime by just providing an updated PMML model to the spark infrastructure.\n4) Use the transformation / normalization for input values or make use of the dynamic model selection support build into PMML to select the appropriate algorithm (configuration) based on the input.\n\nYou could even use JPMML to get the PMML object model as a starting point.", "I think we should support PMML import/export in MLlib. PMML also provides feature transformations, which MLlib has very limited support at this time. The question is 1) how we take leverage on existing PMML packages, 2)  how many people volunteer.\n\nSean, it would be super helpful if you can share some experience on Oryx's PMML support, since I'm also not sure about whether this is the right time to start.", "PMML is the de facto serialization, so certainly the one to consider leveraging. It's just a serialization, so it's not by itself going to help with feature transformation.\n\nGiven data and PMML, it's fairly easy to use things like JPMML to do evaluation. You could write some thin wrapper code in MLlib to facilitate that, but it may not give a lot of marginal benefit.\n\nImport/export is a bit different. Again JPMML will do all the mechanisms of serializing an object model, so that need not be written.\n\nI think export is more important than import, mostly because I think of MLlib as a model builder, and therefore a producer rather than consumer of models. Export is also easier since you just need to write the glue code to translate some MLlib object into a JPMML representation, and only need to worry about dealing with the subset of PMML that covers whatever the MLlib output describes.\n\nImport is harder for the same reason -- you're not going to want to or be able to support everything PMML can describe, so it's already a question of trying to map the vocab as best you can to whatever MLlib supports. It's also less important, IMHO, since MLlib's value is more in making the model than doing something with it right now.\n\nI would suggest the import/export stuff be kept close, but separate, to the other MLlib code. Not a different module, just cleanly separated from the abstract representation.\n\nI think there's a whole project's worth of stuff one could do around consuming, managing, serving models!\n\nSo to summarize: I'd suggest scoping this to start as \"wire up all *Model files to JPMML equivalents, as an 'export' package\" or something.", "Thanks for sharing your thoughts! Feature transformation is part of the PMML standard. It provides primitives to describe feature transformations. It is very hard to describe feature transformation in practice, in most cases the result is some ad-hoc and non-exchangeable code, which is hard to reuse. I'm not a fan of XML, but as you mentioned, PMML is the de facto serialization.\n\nI feel that supporting feature transformation in PMML is as important as -- if not important than -- supporting exporting models to PMML. Especially, the former provides an entry point to MLlib while the latter provides an exit. (I admit that I'm a little selfish on this point.) Btw, Google Prediction API only supports PMML's feature transformation: https://developers.google.com/prediction/docs/pmml-schema", "Yes I understand transformations can be described in PMML. Do you mean parsing a transformation described in PMML and implementing the transformation? Yes that goes hand in hand with supporting import of a model in general.\n\nI would merely suggest this is a step that comes after several others in order of priority, like:\n- implementing feature transformations in the abstract in the code base, separately from the idea of PMML\n- implementing some form of model import via JPMML\n- implementing more functional in the Model classes to give a reason to want to import an external model into MLlib\n\n... and to me this is less useful at this point than export too. I say this because the power of MLlib/Spark right now is perceived to be model building, making it more producer than consumer at this stage.", "How about a PMML pickler/unpickler, written as an extension to:\nhttps://github.com/scala/pickling\n", "Hi, any progress on this issue now?\n", "I don't know anyone who is working on this feature. I set the target version to v1.2.0 for now.", "I agree with Sean, I could see the export to PMML quite useful as it will decouple an application (wanting only to do scoring) from the evaluation of the model that can run on a full blown Spark cluster.\n\nHowever, I am not sure about using JPMML to generate the PMML, for sure it will be the easier option, but what about licensing? \nhttps://github.com/jpmml/jpmml-model is BSD 3-Clause while of course Spark is Apache 2.0.", "BSD 3-Clause is compatible with AL2. Spark's distribution already distributes components under this license. See http://www.apache.org/dev/licensing-howto.html#permissive-deps\n\nThe license issue with JPMML is that its openscoring module is AGPL, which isn't compatible. https://github.com/jpmml/openscoring", "Thanks for clarifying.", "Hi, \nbased on what Sean suggested I had a go at this requirement, in particular the export of models to pmml as I find useful to decouple the producer (spark) and consumer (an app) of mining models.\n\nAttached details on the approach taken, if you think it is valid I could proceed with the implementation of the other exporter (so far only kmeans is supported). \n\nAlso attached the pmml exported for kmeans using the compiled spark-shell.", "User 'selvinsource' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/3062", "I put some comments on the PR. Thanks for starting on this. I think PMML interoperability is indeed helpful. \n\nSo, one big issue here is that MLlib does not at the moment have any notion of a schema. PMML does, and this is vital to actually using the model elsewhere. You have to document what the variables are so they can be matched up with the same variables in another tool. So it's not possible now to do anything but make a model with \"field_1\", \"field_2\", ... This calls into question whether PMML can be meaningfully exported at this point from MLlib? Maybe it will have to wait until other PRs go in that start to add schema.\n\nI also thought it would be a little better to separate the representation of a model, from utility methods to write the model to things like files. The latter can be at least separated out of the type hierarchy. I'm also wondering how much value it adds to design for non-PMML export at this stage.\n\n(Finally I have some code lying around here that will translate the MLlib logistic regression model to PMML. I can put that in the pot at a suitable time.)", "ModelExporter documentation (PDF)\nPMML xml example generated by the ModelExporter\nJPMMLEvaluator example using the exported model\n", "Updated document with model supported so far: KMeansModel, LogisticRegressionModel, SVMModel, LinearRegressionModel, RidgeRegressionModel, LassoModel", "Find at\nhttps://github.com/selvinsource/spark-pmml-exporter-validator\na simple validator project showing that the prediction made by Apache Spark and JPMML Evaluator (by loading the PMML exported from Spark) produces comparable results, therefore proving the PMML export from Apache Spark works as expected.", "Scala examples on usage of ModelExporter.toPMML(model,path):\nhttps://github.com/selvinsource/spark-pmml-exporter-validator/tree/master/src/main/resources/spark_shell_exporter\n\nExported PMML xml files:\nhttps://github.com/selvinsource/spark-pmml-exporter-validator/tree/master/src/main/resources/exported_pmml_models\n\nEvaluation using JPMML of the exported files:\nhttps://github.com/selvinsource/spark-pmml-exporter-validator/blob/master/src/main/java/org/selvinsource/spark_pmml_exporter_validator/SparkPMMLExporterValidator.java", "Issue resolved by pull request 3062\n[https://github.com/apache/spark/pull/3062]", "The PMML model export was partially addressed in PR #3062. The PMML model evaluation part will live outside the Spark codebase, possibly on spark-packages.org, due to license issues with jpmml-evaluator. I closed this JIRA. Please create new JIRAs for PMML model export for other models if someone is interested. Thanks everyone for the discussion!", "After liaising with DMG I got MLlib listed in the powered and example pages:\nhttp://www.dmg.org/products.html\nhttp://www.dmg.org/pmml_examples/index.html\n\n"], "derived": {"summary": "It would be useful if spark would provide support the evaluation of PMML models (http://www. dmg.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "PMML model evaluation support via MLib - It would be useful if spark would provide support the evaluation of PMML models (http://www. dmg."}, {"q": "What updates or decisions were made in the discussion?", "a": "After liaising with DMG I got MLlib listed in the powered and example pages:\nhttp://www.dmg.org/products.html\nhttp://www.dmg.org/pmml_examples/index.html"}]}}
{"project": "SPARK", "issue_id": "SPARK-1407", "title": "EventLogging to HDFS doesn't work properly on yarn", "status": "Closed", "priority": "Blocker", "reporter": "Thomas Graves", "assignee": "Thomas Graves", "labels": [], "created": "2014-04-03T14:06:59.000+0000", "updated": "2015-01-23T18:32:01.000+0000", "description": "When running on spark on yarn and accessing an HDFS file (like in the SparkHdfsLR example) while using the event logging configured to write logs to HDFS, it throws an exception at the end of the application. \n\nSPARK_JAVA_OPTS=-Dspark.eventLog.enabled=true -Dspark.eventLog.dir=hdfs:///history/spark/\n\n14/04/03 13:41:31 INFO yarn.ApplicationMaster$$anon$1: Invoking sc stop from shutdown hook\nException in thread \"Thread-41\" java.io.IOException: Filesystem closed\n        at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:398)\n        at org.apache.hadoop.hdfs.DFSOutputStream.hflush(DFSOutputStream.java:1465)\n        at org.apache.hadoop.hdfs.DFSOutputStream.sync(DFSOutputStream.java:1450)\n        at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:116)\n        at org.apache.spark.util.FileLogger$$anonfun$flush$2.apply(FileLogger.scala:137)\n        at org.apache.spark.util.FileLogger$$anonfun$flush$2.apply(FileLogger.scala:137)\n        at scala.Option.foreach(Option.scala:236)\n        at org.apache.spark.util.FileLogger.flush(FileLogger.scala:137)\n        at org.apache.spark.scheduler.EventLoggingListener.logEvent(EventLoggingListener.scala:69)\n        at org.apache.spark.scheduler.EventLoggingListener.onApplicationEnd(EventLoggingListener.scala:101)\n        at org.apache.spark.scheduler.SparkListenerBus$$anonfun$postToAll$13.apply(SparkListenerBus.scala:67)\n        at org.apache.spark.scheduler.SparkListenerBus$$anonfun$postToAll$13.apply(SparkListenerBus.scala:67)\n        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n        at org.apache.spark.scheduler.SparkListenerBus$class.postToAll(SparkListenerBus.scala:67)\n        at org.apache.spark.scheduler.LiveListenerBus.postToAll(LiveListenerBus.scala:31)\n        at org.apache.spark.scheduler.LiveListenerBus.post(LiveListenerBus.scala:78)\n        at org.apache.spark.SparkContext.postApplicationEnd(SparkContext.scala:1081)\n        at org.apache.spark.SparkContext.stop(SparkContext.scala:828)\n        at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$1.run(ApplicationMaster.scala:460)", "comments": ["Note that the logging also doesn't work on secure HDFS. \n\nException in thread \"Thread-3\" java.lang.reflect.UndeclaredThrowableException: Unknown exception in doAs\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1275)\n        at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:42)\n        at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:192)\nCaused by: java.security.PrivilegedActionException: java.lang.reflect.InvocationTargetException\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1262)\n        ... 2 more\nCaused by: java.lang.reflect.InvocationTargetException\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:601)\n        at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2$$anonfun$run$1.apply$mcV$sp(ApplicationMaster.scala:198)\n        at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:43)\n        at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)\n        ... 5 more\nCaused by: java.io.IOException: Can't replace _HOST pattern since client address is null\n        at org.apache.hadoop.security.SecurityUtil.getServerPrincipal(SecurityUtil.java:255)\n        at org.apache.hadoop.ipc.Client$ConnectionId.getRemotePrincipal(Client.java:1326)\n        at org.apache.hadoop.ipc.Client$ConnectionId.getConnectionId(Client.java:1298)\n        at org.apache.hadoop.ipc.WritableRpcEngine$Invoker.<init>(WritableRpcEngine.java:183)\n        at org.apache.hadoop.ipc.WritableRpcEngine.getProxy(WritableRpcEngine.java:236)\n        at org.apache.hadoop.ipc.RPC.getProtocolProxy(RPC.java:441)\n        at org.apache.hadoop.ipc.RPC.getProtocolProxy(RPC.java:387)\n        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:364)\n        at org.apache.hadoop.hdfs.DFSUtil.createRPCNamenode(DFSUtil.java:642)\n        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:346)\n        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:319)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:110)\n        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2160)\n        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:85)\n        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2194)\n        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2176)\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:306)\n        at org.apache.spark.util.Utils$.getHadoopFileSystem(Utils.scala:1022)\n        at org.apache.spark.util.FileLogger.<init>(FileLogger.scala:51)\n        at org.apache.spark.scheduler.EventLoggingListener.<init>(EventLoggingListener.scala:49)\n        at org.apache.spark.SparkContext.<init>(SparkContext.scala:172)\n        at org.apache.spark.SparkContext.<init>(SparkContext.scala:96)\n        at org.apache.spark.examples.SparkPi$.main(SparkPi.scala:31)\n        at org.apache.spark.examples.SparkPi.main(SparkPi.scala)\n        ... 12 more\nException in thread \"main\" java.lang.AssertionError: assertion failed\n        at scala.Predef$.assert(Predef.scala:165)\n        at org.apache.spark.deploy.yarn.ApplicationMaster.waitForSparkContextInitialized(ApplicationMaster.scala:234)\n        at org.apache.spark.deploy.yarn.ApplicationMaster.run(ApplicationMaster.scala:107)\n        at org.apache.spark.deploy.yarn.ApplicationMaster$.main(ApplicationMaster.scala:480)\n        at org.apache.spark.deploy.yarn.ApplicationMaster.main(ApplicationMaster.scala)\n14/04/08 14:23:03 INFO yarn.ApplicationMaster: AppMaster received a signal.\n14/04/08 14:23:03 INFO yarn.ApplicationMaster: Deleting staging directory .sparkStaging/application_1396660776541_63976\n", "Ignore the last comment about secure hdfs.  It turns out I had a invalid config.  It was missing a '/' in the hdfs spark.eventLog.dir location.", "[~tgraves] Did you test this with an application that explicitly calls stop() instead of relying on the shutdown hook in YARN? ", "I was just running SparkHdfsLR.  I also tried JavaWordCount, same issue.  SparkPi works since it doesn't access hdfs. ", "Unfortuantely I think SparkHDFSLR might just cal System.exit instead of stopping properly...", "That was the problem.  I modified SparkHdfsLR to not do the System.exit and the logging works now. I'll close this and open one to change the examples to not do the System.exit", "I encountered similar issue. It is not limited to YARN, but shows up in Standalone mode as well. When System.exit is invoked without calling sc.stop(), Hadoop FileSystem's shutdown hook gets called, which closes fs without flushing client buffer and event log file got truncated. Calling sc.stop() seems to work in my test, which does flush hadoopDataStream buffer before closing. However, this may not completely solve the problem as events are written to buffer asynchronously by SparkListenerBus thread.", "Made some change to drain SparkListenerBus' event queue before flushing the buffer of FileLogger and stopping it. This does require sc.stop() to be called for it work, though.\n\nhttps://github.com/apache/spark/pull/366", "One example of the exception I encountered. Note the exact events (onJobEnd in this case) could be different.\n\nException in thread \"SparkListenerBus\" java.io.IOException: Filesystem closed\n        at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:702)\n        at org.apache.hadoop.hdfs.DFSOutputStream.flushOrSync(DFSOutputStream.java:1832)\n        at org.apache.hadoop.hdfs.DFSOutputStream.hsync(DFSOutputStream.java:1815)\n        at org.apache.hadoop.hdfs.DFSOutputStream.hsync(DFSOutputStream.java:1798)\n        at org.apache.hadoop.fs.FSDataOutputStream.hsync(FSDataOutputStream.java:123)\n        at org.apache.spark.util.FileLogger$$anonfun$flush$2.apply(FileLogger.scala:138)\n        at org.apache.spark.util.FileLogger$$anonfun$flush$2.apply(FileLogger.scala:138)\n        at scala.Option.foreach(Option.scala:236)\n        at org.apache.spark.util.FileLogger.flush(FileLogger.scala:138)\n        at org.apache.spark.scheduler.EventLoggingListener.logEvent(EventLoggingListener.scala:64)\n        at org.apache.spark.scheduler.EventLoggingListener.onJobEnd(EventLoggingListener.scala:86)\n        at org.apache.spark.scheduler.SparkListenerBus$$anonfun$postToAll$4.apply(SparkListenerBus.scala:49)\n        at org.apache.spark.scheduler.SparkListenerBus$$anonfun$postToAll$4.apply(SparkListenerBus.scala:49)\n        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n        at org.apache.spark.scheduler.SparkListenerBus$class.postToAll(SparkListenerBus.scala:49)\n        at org.apache.spark.scheduler.LiveListenerBus.postToAll(LiveListenerBus.scala:31)\n        at org.apache.spark.scheduler.LiveListenerBus$$anon$1.run(LiveListenerBus.scala:61)\n", "I opened SPARK-1475 to track the above PR."], "derived": {"summary": "When running on spark on yarn and accessing an HDFS file (like in the SparkHdfsLR example) while using the event logging configured to write logs to HDFS, it throws an exception at the end of the application. SPARK_JAVA_OPTS=-Dspark.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "EventLogging to HDFS doesn't work properly on yarn - When running on spark on yarn and accessing an HDFS file (like in the SparkHdfsLR example) while using the event logging configured to write logs to HDFS, it throws an exception at the end of the application. SPARK_JAVA_OPTS=-Dspark."}, {"q": "What updates or decisions were made in the discussion?", "a": "I opened SPARK-1475 to track the above PR."}]}}
{"project": "SPARK", "issue_id": "SPARK-1408", "title": "Modify Spark on Yarn to point to the history server when app finishes", "status": "Resolved", "priority": "Major", "reporter": "Thomas Graves", "assignee": "Thomas Graves", "labels": [], "created": "2014-04-03T14:33:47.000+0000", "updated": "2014-04-17T21:37:49.000+0000", "description": "Once the spark history server is implemented in SPARK-1276, we should modify spark on yarn to point to the history server url from the Yarn ResourceManager UI when the application finishes. ", "comments": ["https://github.com/apache/spark/pull/362"], "derived": {"summary": "Once the spark history server is implemented in SPARK-1276, we should modify spark on yarn to point to the history server url from the Yarn ResourceManager UI when the application finishes.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Modify Spark on Yarn to point to the history server when app finishes - Once the spark history server is implemented in SPARK-1276, we should modify spark on yarn to point to the history server url from the Yarn ResourceManager UI when the application finishes."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/362"}]}}
{"project": "SPARK", "issue_id": "SPARK-1409", "title": "Flaky Test: \"actor input stream\" test in org.apache.spark.streaming.InputStreamsSuite", "status": "Resolved", "priority": "Major", "reporter": "Michael Armbrust", "assignee": "Tathagata Das", "labels": [], "created": "2014-04-03T19:04:32.000+0000", "updated": "2014-10-13T18:26:46.000+0000", "description": "Here are just a few cases:\nhttps://travis-ci.org/apache/spark/jobs/22151827\nhttps://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/13709/", "comments": ["I've disabled this test for now.", "The test seems to run fine when launched from Intellij Idea but not from sbt. I am not sure why yet. It is something to do with Akka's Prop (actually Typesafe Config it uses) being not serializable under certain conditions. I am afraid that there may be two versions of Props/Config in the classpath (when running from sbt) though I havent figured out how. The serialization error causes the test to fail, every time. \n\nSome change in the last couple of months resulted in this side effect. Comparing spark 0.9 branch (where tests run fine), there is no difference in the Akka / Typesafe Config. The only difference is saw is in the version of sbt - 0.12.1 for branch 0.9, 0.13.2 for master.\n\nSo, still not solution, bumping this to post 1.0", "Since this test was removed with SPARK-2805, safe to call this closed?"], "derived": {"summary": "Here are just a few cases:\nhttps://travis-ci. org/apache/spark/jobs/22151827\nhttps://amplab.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Flaky Test: \"actor input stream\" test in org.apache.spark.streaming.InputStreamsSuite - Here are just a few cases:\nhttps://travis-ci. org/apache/spark/jobs/22151827\nhttps://amplab."}, {"q": "What updates or decisions were made in the discussion?", "a": "Since this test was removed with SPARK-2805, safe to call this closed?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1410", "title": "Class not found exception with application launched from sbt 0.13.x", "status": "Resolved", "priority": "Major", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-04-03T20:20:06.000+0000", "updated": "2015-06-19T23:42:55.000+0000", "description": "sbt 0.13.x use its own loader but this is not available at worker side:\n\norg.apache.spark.SparkException: Job aborted: ClassNotFound with classloader: sbt.classpath.ClasspathFilter@47ed40d\n\nA workaround is to switch to sbt 0.12.4.", "comments": ["I don't understand how the classloader is ending up being required on the worker side.  Can you share a command to reproduce this?", "The code is available at https://github.com/mengxr/mllib-grid-search\n\nIf sbt.version is changed to 0.13.0 or 0.13.1, `sbt/sbt \"run ...\"` throws the following exception:\n\n~~~\n[error] (run-main-0) org.apache.spark.SparkException: Job aborted: ClassNotFound with classloader: sbt.classpath.ClasspathFilter@45307fb\norg.apache.spark.SparkException: Job aborted: ClassNotFound with classloader: sbt.classpath.ClasspathFilter@45307fb\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1010)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1008)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1008)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:595)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:595)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:595)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:146)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:456)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:219)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n~~~", "So I haven't tracked down why this is the case, but I think changing build.sbt to: scalaVersion := \"2.10.4\" instead might fix this problem.\n", "Had exact the same issue with sbt 0.13.0 and Scala 2.10.3.\nSetting scala version to 2.10.4 worked for me.", "Had the same issue.  Was trying to run a stand alone scala app from https://spark.apache.org/docs/0.9.0/quick-start.html with Joda and some other dependencies.\n\nsbt 0.13.2 was giving me this error, downgraded to 0.12.4 and everything worked.\nLater upgraded sbt back to 0.13.2 and upgraded scala from 2.10.3 to 2.10.4 and everything works.", "I ran into a similar issue SPARK-1923. It also fixed the issue to just upgrade to Scala 2.10.4. Thanks [~mengxr] for posting this - I was two hours into debugging Spark when I found this post :P\n\nSince this seems to be a Scala/SBT issue I'm going to mark it as resolved, but we can re-open it if users are still having issues post upgrade.", "Solution is to upgrade to Scala 2.10.4."], "derived": {"summary": "sbt 0. 13.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Class not found exception with application launched from sbt 0.13.x - sbt 0. 13."}, {"q": "What updates or decisions were made in the discussion?", "a": "Solution is to upgrade to Scala 2.10.4."}]}}
{"project": "SPARK", "issue_id": "SPARK-1411", "title": "When using spark.ui.retainedStages=n only the first n stages are kept, not the most recent.", "status": "Closed", "priority": "Major", "reporter": "Davis Shepherd", "assignee": null, "labels": [], "created": "2014-04-04T00:39:09.000+0000", "updated": "2014-04-04T05:55:15.000+0000", "description": "For any long running job with many stages, the web ui only shows the first n stages of the job (where spark.ui.retainedStages=n). The most recent stages are immediately dropped and are only visible for a brief time.  This renders the UI pretty useless after a pretty short amount of time for a long running non-streaming job. I am unsure as to whether similar results appear for streaming jobs.", "comments": ["Notice the large gap in stage ids and submitted time between the top two stages.", "[~dgshep] is this a duplicate? What is the other JIRA?", "Ah, got it SPARK-1337", "I submitted a pull request to fix 1337 as well.\nDavis\n\nOn Thu, Apr 3, 2014 at 5:52 PM, Patrick Wendell (JIRA) <jira@apache.org>\n\n"], "derived": {"summary": "For any long running job with many stages, the web ui only shows the first n stages of the job (where spark. ui.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "When using spark.ui.retainedStages=n only the first n stages are kept, not the most recent. - For any long running job with many stages, the web ui only shows the first n stages of the job (where spark. ui."}, {"q": "What updates or decisions were made in the discussion?", "a": "I submitted a pull request to fix 1337 as well.\nDavis\n\nOn Thu, Apr 3, 2014 at 5:52 PM, Patrick Wendell (JIRA) <jira@apache.org>"}]}}
{"project": "SPARK", "issue_id": "SPARK-1412", "title": "Disable partial aggregation automatically when reduction factor is low", "status": "Resolved", "priority": "Minor", "reporter": "Reynold Xin", "assignee": null, "labels": ["bulk-closed"], "created": "2014-04-04T01:55:12.000+0000", "updated": "2021-05-25T01:49:46.000+0000", "description": "Once we see enough number of rows in partial aggregation and don't observe any reduction, the aggregate operator should just turn off partial aggregation. ", "comments": ["The PRs for SPARK-2253 and SPARK-1412 were abandoned. Are both a WontFix?", "I think we should still do it - it's just that the current AppendOnlyMap isn't really built for it. We will probably revisit this in the future.\n"], "derived": {"summary": "Once we see enough number of rows in partial aggregation and don't observe any reduction, the aggregate operator should just turn off partial aggregation.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Disable partial aggregation automatically when reduction factor is low - Once we see enough number of rows in partial aggregation and don't observe any reduction, the aggregate operator should just turn off partial aggregation."}, {"q": "What updates or decisions were made in the discussion?", "a": "I think we should still do it - it's just that the current AppendOnlyMap isn't really built for it. We will probably revisit this in the future."}]}}
{"project": "SPARK", "issue_id": "SPARK-1413", "title": "Parquet messes up stdout and stdin when used in Spark REPL", "status": "Resolved", "priority": "Critical", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "2014-04-04T03:23:43.000+0000", "updated": "2014-04-10T17:37:33.000+0000", "description": "I have a simple Parquet file in \"foos.parquet\", but after I type this code, it freezes the shell, to the point where I can't read or write stuff:\n\nscala> val qc = new org.apache.spark.sql.SQLContext(sc); import qc._\nqc: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@1c0c8826\nimport qc._\n\nscala> qc.parquetFile(\"foos.parquet\").saveAsTextFile(\"bar\")\n\nThe job itself completes successfully, and \"bar\" contains the right text, but I can no longer see commands I type in, or further log output.", "comments": ["You can try this  [PR|https://github.com/apache/spark/pull/325]\n"], "derived": {"summary": "I have a simple Parquet file in \"foos. parquet\", but after I type this code, it freezes the shell, to the point where I can't read or write stuff:\n\nscala> val qc = new org.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Parquet messes up stdout and stdin when used in Spark REPL - I have a simple Parquet file in \"foos. parquet\", but after I type this code, it freezes the shell, to the point where I can't read or write stuff:\n\nscala> val qc = new org."}, {"q": "What updates or decisions were made in the discussion?", "a": "You can try this  [PR|https://github.com/apache/spark/pull/325]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1414", "title": "Python API for SparkContext.wholeTextFiles", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "labels": [], "created": "2014-04-04T18:46:02.000+0000", "updated": "2014-04-05T00:30:16.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Python API for SparkContext.wholeTextFiles"}]}}
{"project": "SPARK", "issue_id": "SPARK-1415", "title": "Add a minSplits parameter to wholeTextFiles", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Xusen Yin", "labels": ["Starter"], "created": "2014-04-04T18:47:59.000+0000", "updated": "2014-04-13T20:19:39.000+0000", "description": "This probably requires adding one to newAPIHadoopFile too.", "comments": ["Hi Matei, I just looked around in those Hadoop APIs. I find that the new Hadoop API deprecates the minSplit, instead of minSplit, they prefer minSplitSize and maxSplitSize to control the split. minSplit is negative correlated with maxSplitSize, so I think we have 2 ways to fix the issue:\n\n1. We just provide a new API with maxSplitSize, say, wholeTextFiles(path: String, maxSplitSize: Long);\n\n2. We write a delegation to compute the maxSplitSize using minSplit (easy to write, taking old Hadoop API as an example), and provide the API wholeTextFile(path: String, minSplit: Int);\n\nI also think we can provide the two APIs simultaneously. What do you think?", "Hey Xusen, that makes sense. I think that for consistency with our other API methods, we should add minSplits here, and we can compute maxSplitSize from it. Later on we can have versions of the methods that take a maxSplitSize. But on the old Hadoop API for example we can't easily change this, and it seems that a maxSplitSize is always possible to compute from minSplits."], "derived": {"summary": "This probably requires adding one to newAPIHadoopFile too.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add a minSplits parameter to wholeTextFiles - This probably requires adding one to newAPIHadoopFile too."}, {"q": "What updates or decisions were made in the discussion?", "a": "Hey Xusen, that makes sense. I think that for consistency with our other API methods, we should add minSplits here, and we can compute maxSplitSize from it. Later on we can have versions of the methods that take a maxSplitSize. But on the old Hadoop API for example we can't easily change this, and it seems that a maxSplitSize is always possible to compute from minSplits."}]}}
{"project": "SPARK", "issue_id": "SPARK-1416", "title": "Add support for SequenceFiles and binary Hadoop InputFormats in PySpark", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Nicholas Pentreath", "labels": [], "created": "2014-04-04T19:31:50.000+0000", "updated": "2014-06-10T17:16:50.000+0000", "description": "Just covering the basic Hadoop Writable types (e.g. primitives, arrays of primitives, text) should still let people store data more efficiently.", "comments": ["Implemented in https://github.com/apache/spark/pull/455", "That pull request also added generic InputFormats so I've updated the JIRA title."], "derived": {"summary": "Just covering the basic Hadoop Writable types (e. g.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add support for SequenceFiles and binary Hadoop InputFormats in PySpark - Just covering the basic Hadoop Writable types (e. g."}, {"q": "What updates or decisions were made in the discussion?", "a": "That pull request also added generic InputFormats so I've updated the JIRA title."}]}}
{"project": "SPARK", "issue_id": "SPARK-1417", "title": "Spark on Yarn - spark UI link from resourcemanager is broken", "status": "Resolved", "priority": "Blocker", "reporter": "Thomas Graves", "assignee": "Thomas Graves", "labels": [], "created": "2014-04-04T20:02:56.000+0000", "updated": "2014-04-11T15:28:52.000+0000", "description": "When running spark on yarn in yarn-cluster mode, spark registers a url with the Yarn ResourceManager to point to the spark UI.  This link is now broken. \n\nThe link should be something like: < resourcemanager >/proxy/< applicationId >\n\ninstead its coming back as < resourcemanager >/< host of am:port >", "comments": ["https://github.com/apache/spark/pull/344"], "derived": {"summary": "When running spark on yarn in yarn-cluster mode, spark registers a url with the Yarn ResourceManager to point to the spark UI. This link is now broken.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Spark on Yarn - spark UI link from resourcemanager is broken - When running spark on yarn in yarn-cluster mode, spark registers a url with the Yarn ResourceManager to point to the spark UI. This link is now broken."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/344"}]}}
{"project": "SPARK", "issue_id": "SPARK-1418", "title": "Python MLlib's _get_unmangled_rdd should uncache RDDs when training is done", "status": "Closed", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "2014-04-04T22:32:14.000+0000", "updated": "2015-02-20T23:58:07.000+0000", "description": "Right now when PySpark converts a Python RDD of NumPy vectors to a Java one, it caches the Java one, since many of the algorithms are iterative. We should call unpersist() at the end of the algorithm though to free cache space. In addition it may be good to persist the Java RDD with StorageLevel.MEMORY_AND_DISK instead of going back through the NumPy conversion.. it will almost certainly be faster.", "comments": ["This issue's description is now a little confusing, since {{_get_unmangled_rdd}} has been removed from the Python MLlib bindings.  Does anyone know if this issue is still relevant?  If so, could we update its description?  Otherwise, let's close this."], "derived": {"summary": "Right now when PySpark converts a Python RDD of NumPy vectors to a Java one, it caches the Java one, since many of the algorithms are iterative. We should call unpersist() at the end of the algorithm though to free cache space.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Python MLlib's _get_unmangled_rdd should uncache RDDs when training is done - Right now when PySpark converts a Python RDD of NumPy vectors to a Java one, it caches the Java one, since many of the algorithms are iterative. We should call unpersist() at the end of the algorithm though to free cache space."}, {"q": "What updates or decisions were made in the discussion?", "a": "This issue's description is now a little confusing, since {{_get_unmangled_rdd}} has been removed from the Python MLlib bindings.  Does anyone know if this issue is still relevant?  If so, could we update its description?  Otherwise, let's close this."}]}}
{"project": "SPARK", "issue_id": "SPARK-1419", "title": "Apache parent POM to version 14", "status": "Resolved", "priority": "Major", "reporter": "Mark Hamstra", "assignee": "Mark Hamstra", "labels": [], "created": "2014-04-05T00:03:43.000+0000", "updated": "2014-04-05T02:20:37.000+0000", "description": "Latest version of the Apache parent POM includes several improvements and bugfixes, including to the release plugin: http://svn.apache.org/viewvc/maven/pom/tags/apache-14/pom.xml?r1=HEAD&r2=1434717&diff_format=h\n", "comments": [], "derived": {"summary": "Latest version of the Apache parent POM includes several improvements and bugfixes, including to the release plugin: http://svn. apache.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Apache parent POM to version 14 - Latest version of the Apache parent POM includes several improvements and bugfixes, including to the release plugin: http://svn. apache."}]}}
{"project": "SPARK", "issue_id": "SPARK-1420", "title": "The maven build error for Spark Catalyst", "status": "Closed", "priority": "Major", "reporter": "Guoqiang Li", "assignee": null, "labels": [], "created": "2014-04-05T15:24:07.000+0000", "updated": "2014-05-25T20:20:26.000+0000", "description": null, "comments": ["{code}\nmvn -Pyarn -Dhadoop.version=2.3.0 -Dyarn.version=2.3.0 -DskipTests install\n{code} \n=>\n{code} \n[ERROR] /Users/witgo/work/code/java/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala:31: object runtime is not a member of package reflect\n[ERROR]   import scala.reflect.runtime.universe._\n{code}"], "derived": {"summary": "", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "The maven build error for Spark Catalyst"}, {"q": "What updates or decisions were made in the discussion?", "a": "{code}\nmvn -Pyarn -Dhadoop.version=2.3.0 -Dyarn.version=2.3.0 -DskipTests install\n{code} \n=>\n{code} \n[ERROR] /Users/witgo/work/code/java/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala:31: object runtime is not a member of package reflect\n[ERROR]   import scala.reflect.runtime.universe._\n{code}"}]}}
{"project": "SPARK", "issue_id": "SPARK-1421", "title": "Make MLlib work on Python 2.6", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "labels": [], "created": "2014-04-05T22:46:15.000+0000", "updated": "2014-04-06T03:53:47.000+0000", "description": "Currently it requires Python 2.7 because it uses some new APIs, but they should not be essential for running our code.", "comments": [], "derived": {"summary": "Currently it requires Python 2. 7 because it uses some new APIs, but they should not be essential for running our code.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Make MLlib work on Python 2.6 - Currently it requires Python 2. 7 because it uses some new APIs, but they should not be essential for running our code."}]}}
{"project": "SPARK", "issue_id": "SPARK-1422", "title": "Add scripts for launching Spark on Google Compute Engine", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "2014-04-05T23:06:26.000+0000", "updated": "2015-01-12T03:46:37.000+0000", "description": null, "comments": ["We have [a package for launching Spark on GCE|http://spark-packages.org/package/9] on the recently launched [Spark Packages|http://spark-packages.org/]. Do we still want to track this here?", "Good call NIck - yeah let's close this as being out of scope since it's being maintained elsewhere.", "[~pwendell] - I would consider doing this as well for the parent task, [SPARK-4399]."], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add scripts for launching Spark on Google Compute Engine"}, {"q": "What updates or decisions were made in the discussion?", "a": "[~pwendell] - I would consider doing this as well for the parent task, [SPARK-4399]."}]}}
{"project": "SPARK", "issue_id": "SPARK-1423", "title": "Add scripts for launching Spark on Windows Azure", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "2014-04-05T23:06:36.000+0000", "updated": "2015-05-08T08:05:36.000+0000", "description": null, "comments": ["should this be resolved as WONTFIX?", "Given the lack of activity and resolution of https://issues.apache.org/jira/browse/SPARK-1422 I think that's probably correct."], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add scripts for launching Spark on Windows Azure"}, {"q": "What updates or decisions were made in the discussion?", "a": "Given the lack of activity and resolution of https://issues.apache.org/jira/browse/SPARK-1422 I think that's probably correct."}]}}
{"project": "SPARK", "issue_id": "SPARK-1424", "title": "InsertInto should work on JavaSchemaRDD as well.", "status": "Resolved", "priority": "Blocker", "reporter": "Michael Armbrust", "assignee": "Michael Armbrust", "labels": [], "created": "2014-04-05T23:48:01.000+0000", "updated": "2014-04-16T06:05:37.000+0000", "description": null, "comments": ["More generally we should have flags to support the following:\n* Inserting data into an existing table\n* Creating a new table, only if it does not exist\n* Overwriting an existing table", "Started on this here: https://github.com/apache/spark/pull/354\n\nA few things, there is no way to createTableAs from a standard sql context as I'm not really sure where to put the files.\n\nAlso, it might be nice to have a create new table that doesn't fail if it exists, but instead appends to it.  This is going to require some minor tweaking in the execution engine though, where as the above options were just API extensions."], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "InsertInto should work on JavaSchemaRDD as well."}, {"q": "What updates or decisions were made in the discussion?", "a": "Started on this here: https://github.com/apache/spark/pull/354\n\nA few things, there is no way to createTableAs from a standard sql context as I'm not really sure where to put the files.\n\nAlso, it might be nice to have a create new table that doesn't fail if it exists, but instead appends to it.  This is going to require some minor tweaking in the execution engine though, where as the above options were just API extensions."}]}}
{"project": "SPARK", "issue_id": "SPARK-1425", "title": "PySpark can crash Executors if worker.py fails while serializing data", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "2014-04-06T01:02:07.000+0000", "updated": "2016-10-08T11:51:30.000+0000", "description": "The PythonRDD code that talks to the worker will keep calling stream.readInt() and allocating an array of that size. Unfortunately, if the worker gives it corrupted data, it will attempt to allocate a huge array and get an OutOfMemoryError. It would be better to use a different stream to give feedback, *or* only write an object out to the stream once it's been properly pickled to bytes or to a string.", "comments": ["Is this still an issue or do we have a repro case for it? The current framed serializer seems to only write out the object once its fully pickled, although we are still using the same pipe for both error messages and data.", "Almost certainly obsolete now"], "derived": {"summary": "The PythonRDD code that talks to the worker will keep calling stream. readInt() and allocating an array of that size.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "PySpark can crash Executors if worker.py fails while serializing data - The PythonRDD code that talks to the worker will keep calling stream. readInt() and allocating an array of that size."}, {"q": "What updates or decisions were made in the discussion?", "a": "Almost certainly obsolete now"}]}}
{"project": "SPARK", "issue_id": "SPARK-1426", "title": "Make MLlib work with NumPy versions older than 1.7", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Sandeep Singh", "labels": ["Starter"], "created": "2014-04-06T01:22:00.000+0000", "updated": "2014-04-15T07:22:08.000+0000", "description": "Currently it requires NumPy 1.7 due to using the copyto method (http://docs.scipy.org/doc/numpy/reference/generated/numpy.copyto.html) for extracting data out of an array, but we could add a fallback for older versions.", "comments": ["https://github.com/apache/spark/pull/391", "https://github.com/apache/spark/pull/391"], "derived": {"summary": "Currently it requires NumPy 1. 7 due to using the copyto method (http://docs.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Make MLlib work with NumPy versions older than 1.7 - Currently it requires NumPy 1. 7 due to using the copyto method (http://docs."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/391"}]}}
{"project": "SPARK", "issue_id": "SPARK-1427", "title": "HQL Examples Don't Work", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Michael Armbrust", "labels": [], "created": "2014-04-06T05:49:00.000+0000", "updated": "2014-04-07T20:55:15.000+0000", "description": "{code}\nscala> hql(\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING)\")\n14/04/05 22:40:29 INFO ParseDriver: Parsing command: CREATE TABLE IF NOT EXISTS src (key INT, value STRING)\n14/04/05 22:40:30 INFO ParseDriver: Parse Completed\n14/04/05 22:40:30 INFO Driver: <PERFLOG method=Driver.run>\n14/04/05 22:40:30 INFO Driver: <PERFLOG method=TimeToSubmit>\n14/04/05 22:40:30 INFO Driver: <PERFLOG method=compile>\n14/04/05 22:40:30 INFO Driver: <PERFLOG method=parse>\n14/04/05 22:40:30 INFO ParseDriver: Parsing command: CREATE TABLE IF NOT EXISTS src (key INT, value STRING)\n14/04/05 22:40:30 INFO ParseDriver: Parse Completed\n14/04/05 22:40:30 INFO Driver: </PERFLOG method=parse start=1396762830162 end=1396762830163 duration=1>\n14/04/05 22:40:30 INFO Driver: <PERFLOG method=semanticAnalyze>\n14/04/05 22:40:30 INFO SemanticAnalyzer: Starting Semantic Analysis\n14/04/05 22:40:30 INFO SemanticAnalyzer: Creating table src position=27\n14/04/05 22:40:30 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore\n14/04/05 22:40:30 INFO ObjectStore: ObjectStore, initialize called\n14/04/05 22:40:30 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored\n14/04/05 22:40:30 WARN BoneCPConfig: Max Connections < 1. Setting to 20\n14/04/05 22:40:32 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=\"Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order\"\n14/04/05 22:40:32 INFO ObjectStore: Initialized ObjectStore\n14/04/05 22:40:33 WARN BoneCPConfig: Max Connections < 1. Setting to 20\n14/04/05 22:40:33 INFO HiveMetaStore: 0: get_table : db=default tbl=src\n14/04/05 22:40:33 INFO audit: ugi=patrick\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=src\t\n14/04/05 22:40:33 INFO Datastore: The class \"org.apache.hadoop.hive.metastore.model.MFieldSchema\" is tagged as \"embedded-only\" so does not have its own datastore table.\n14/04/05 22:40:33 INFO Datastore: The class \"org.apache.hadoop.hive.metastore.model.MOrder\" is tagged as \"embedded-only\" so does not have its own datastore table.\n14/04/05 22:40:34 INFO Driver: Semantic Analysis Completed\n14/04/05 22:40:34 INFO Driver: </PERFLOG method=semanticAnalyze start=1396762830163 end=1396762834001 duration=3838>\n14/04/05 22:40:34 INFO Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)\n14/04/05 22:40:34 INFO Driver: </PERFLOG method=compile start=1396762830146 end=1396762834006 duration=3860>\n14/04/05 22:40:34 INFO Driver: <PERFLOG method=Driver.execute>\n14/04/05 22:40:34 INFO Driver: Starting command: CREATE TABLE IF NOT EXISTS src (key INT, value STRING)\n14/04/05 22:40:34 INFO Driver: </PERFLOG method=TimeToSubmit start=1396762830146 end=1396762834016 duration=3870>\n14/04/05 22:40:34 INFO Driver: <PERFLOG method=runTasks>\n14/04/05 22:40:34 INFO Driver: </PERFLOG method=runTasks start=1396762834016 end=1396762834016 duration=0>\n14/04/05 22:40:34 INFO Driver: </PERFLOG method=Driver.execute start=1396762834006 end=1396762834017 duration=11>\n14/04/05 22:40:34 INFO Driver: OK\n14/04/05 22:40:34 INFO Driver: <PERFLOG method=releaseLocks>\n14/04/05 22:40:34 INFO Driver: </PERFLOG method=releaseLocks start=1396762834019 end=1396762834019 duration=0>\n14/04/05 22:40:34 INFO Driver: </PERFLOG method=Driver.run start=1396762830146 end=1396762834019 duration=3873>\n14/04/05 22:40:34 INFO Driver: <PERFLOG method=releaseLocks>\n14/04/05 22:40:34 INFO Driver: </PERFLOG method=releaseLocks start=1396762834019 end=1396762834020 duration=1>\njava.lang.AssertionError: assertion failed: No plan for NativeCommand CREATE TABLE IF NOT EXISTS src (key INT, value STRING)\n\n\tat scala.Predef$.assert(Predef.scala:179)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.apply(QueryPlanner.scala:59)\n\tat org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:218)\n\tat org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:218)\n\tat org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:219)\n\tat org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:219)\n\tat org.apache.spark.sql.SchemaRDDLike$class.toString(SchemaRDDLike.scala:44)\n\tat org.apache.spark.sql.SchemaRDD.toString(SchemaRDD.scala:93)\n\tat java.lang.String.valueOf(String.java:2854)\n\tat scala.runtime.ScalaRunTime$.stringOf(ScalaRunTime.scala:331)\n\tat scala.runtime.ScalaRunTime$.replStringOf(ScalaRunTime.scala:337)\n\tat .<init>(<console>:10)\n\tat .<clinit>(<console>)\n\tat $print(<console>)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n{code}\n\n{code}\nscala> hql(\"select count(*) from src\")\n14/04/05 22:47:13 INFO ParseDriver: Parsing command: select count(*) from src\n14/04/05 22:47:13 INFO ParseDriver: Parse Completed\n14/04/05 22:47:13 INFO HiveMetaStore: 0: get_table : db=default tbl=src\n14/04/05 22:47:13 INFO audit: ugi=patrick\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=src\t\n14/04/05 22:47:13 INFO MemoryStore: ensureFreeSpace(147107) called with curMem=0, maxMem=308713881\n14/04/05 22:47:13 INFO MemoryStore: Block broadcast_0 stored as values to memory (estimated size 143.7 KB, free 294.3 MB)\n14/04/05 22:47:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n14/04/05 22:47:13 WARN LoadSnappy: Snappy native library not loaded\n14/04/05 22:47:13 INFO FileInputFormat: Total input paths to process : 1\n14/04/05 22:47:13 INFO SparkContext: Starting job: count at aggregates.scala:107\n14/04/05 22:47:13 INFO DAGScheduler: Got job 0 (count at aggregates.scala:107) with 2 output partitions (allowLocal=false)\n14/04/05 22:47:13 INFO DAGScheduler: Final stage: Stage 0 (count at aggregates.scala:107)\n14/04/05 22:47:13 INFO DAGScheduler: Parents of final stage: List()\n14/04/05 22:47:13 INFO DAGScheduler: Missing parents: List()\n14/04/05 22:47:13 INFO DAGScheduler: Submitting Stage 0 (MappedRDD[9] at map at aggregates.scala:94), which has no missing parents\n14/04/05 22:47:13 INFO DAGScheduler: Submitting 2 missing tasks from Stage 0 (MappedRDD[9] at map at aggregates.scala:94)\n14/04/05 22:47:13 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks\n14/04/05 22:47:13 INFO TaskSetManager: Starting task 0.0:0 as TID 0 on executor localhost: localhost (PROCESS_LOCAL)\n14/04/05 22:47:14 INFO TaskSetManager: Serialized task 0.0:0 as 3919 bytes in 323 ms\n14/04/05 22:47:14 INFO Executor: Running task ID 0\nException in thread \"Executor task launch worker-0\" java.lang.OutOfMemoryError: PermGen space\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:271)\n\tat org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:46)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:724)\n{code}", "comments": ["Fixed the toString issue here: https://github.com/apache/spark/pull/343\n\nCould not recreate the permgen problem, but I did run the examples by hand successfully."], "derived": {"summary": "{code}\nscala> hql(\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING)\")\n14/04/05 22:40:29 INFO ParseDriver: Parsing command: CREATE TABLE IF NOT EXISTS src (key INT, value STRING)\n14/04/05 22:40:30 INFO ParseDriver: Parse Completed\n14/04/05 22:40:30 INFO Driver: <PERFLOG method=Driver. run>\n14/04/05 22:40:30 INFO Driver: <PERFLOG method=TimeToSubmit>\n14/04/05 22:40:30 INFO Driver: <PERFLOG method=compile>\n14/04/05 22:40:30 INFO Driver: <PERFLOG method=parse>\n14/04/05 22:40:30 INFO ParseDriver: Parsing command: CREATE TABLE IF NOT EXISTS src (key INT, value STRING)\n14/04/05 22:40:30 INFO ParseDriver: Parse Completed\n14/04/05 22:40:30 INFO Driver: </PERFLOG method=parse start=1396762830162 end=1396762830163 duration=1>\n14/04/05 22:40:30 INFO Driver: <PERFLOG method=semanticAnalyze>\n14/04/05 22:40:30 INFO SemanticAnalyzer: Starting Semantic Analysis\n14/04/05 22:40:30 INFO SemanticAnalyzer: Creating table src position=27\n14/04/05 22:40:30 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "HQL Examples Don't Work - {code}\nscala> hql(\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING)\")\n14/04/05 22:40:29 INFO ParseDriver: Parsing command: CREATE TABLE IF NOT EXISTS src (key INT, value STRING)\n14/04/05 22:40:30 INFO ParseDriver: Parse Completed\n14/04/05 22:40:30 INFO Driver: <PERFLOG method=Driver. run>\n14/04/05 22:40:30 INFO Driver: <PERFLOG method=TimeToSubmit>\n14/04/05 22:40:30 INFO Driver: <PERFLOG method=compile>\n14/04/05 22:40:30 INFO Driver: <PERFLOG method=parse>\n14/04/05 22:40:30 INFO ParseDriver: Parsing command: CREATE TABLE IF NOT EXISTS src (key INT, value STRING)\n14/04/05 22:40:30 INFO ParseDriver: Parse Completed\n14/04/05 22:40:30 INFO Driver: </PERFLOG method=parse start=1396762830162 end=1396762830163 duration=1>\n14/04/05 22:40:30 INFO Driver: <PERFLOG method=semanticAnalyze>\n14/04/05 22:40:30 INFO SemanticAnalyzer: Starting Semantic Analysis\n14/04/05 22:40:30 INFO SemanticAnalyzer: Creating table src position=27\n14/04/05 22:40:30 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed the toString issue here: https://github.com/apache/spark/pull/343\n\nCould not recreate the permgen problem, but I did run the examples by hand successfully."}]}}
{"project": "SPARK", "issue_id": "SPARK-1428", "title": "MLlib should convert non-float64 NumPy arrays to float64 instead of complaining", "status": "Resolved", "priority": "Minor", "reporter": "Matei Alexandru Zaharia", "assignee": "Sandeep Singh", "labels": ["Starter"], "created": "2014-04-06T06:01:10.000+0000", "updated": "2016-07-03T04:55:04.000+0000", "description": "Pretty easy to fix, it would avoid spewing some scary task-failed errors. The place to fix this is _serialize_double_vector in _common.py.", "comments": ["this should work https://github.com/apache/spark/pull/356", "User 'techaddict' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/356"], "derived": {"summary": "Pretty easy to fix, it would avoid spewing some scary task-failed errors. The place to fix this is _serialize_double_vector in _common.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "MLlib should convert non-float64 NumPy arrays to float64 instead of complaining - Pretty easy to fix, it would avoid spewing some scary task-failed errors. The place to fix this is _serialize_double_vector in _common."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'techaddict' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/356"}]}}
{"project": "SPARK", "issue_id": "SPARK-1429", "title": "Spark shell fails to start after \"sbt clean assemble-deps package\"", "status": "Resolved", "priority": "Major", "reporter": "Cheng Lian", "assignee": "Cheng Lian", "labels": [], "created": "2014-04-06T08:44:46.000+0000", "updated": "2014-04-06T17:07:21.000+0000", "description": "After Hive assembly is added into account in compute-classpath.sh, Spark shell fails to start after \"sbt clean assemble-deps package\" and complains \"org.apache.spark.repl.Main\" is not found.\n\nThe fix is simple: just add a \"*\" in line 55 of compute-classpath.sh to make sure both \"spark-assembly-xxx-deps.jar\" and \"spark-hive-assembly-xxx-deps.jar\" are taken into account.", "comments": ["PR #337: https://github.com/apache/spark/pull/337"], "derived": {"summary": "After Hive assembly is added into account in compute-classpath. sh, Spark shell fails to start after \"sbt clean assemble-deps package\" and complains \"org.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Spark shell fails to start after \"sbt clean assemble-deps package\" - After Hive assembly is added into account in compute-classpath. sh, Spark shell fails to start after \"sbt clean assemble-deps package\" and complains \"org."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR #337: https://github.com/apache/spark/pull/337"}]}}
{"project": "SPARK", "issue_id": "SPARK-1430", "title": "Support sparse data in Python MLlib", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "labels": [], "created": "2014-04-06T23:25:37.000+0000", "updated": "2014-04-16T03:35:36.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support sparse data in Python MLlib"}]}}
{"project": "SPARK", "issue_id": "SPARK-1431", "title": "Allow merging pull requests that have conflicts", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "labels": [], "created": "2014-04-07T02:54:36.000+0000", "updated": "2014-04-07T04:05:16.000+0000", "description": "Sometimes if there is a small conflict it's nice to be able to just manually fix it up rather than have another RTT with the contributor.", "comments": [], "derived": {"summary": "Sometimes if there is a small conflict it's nice to be able to just manually fix it up rather than have another RTT with the contributor.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Allow merging pull requests that have conflicts - Sometimes if there is a small conflict it's nice to be able to just manually fix it up rather than have another RTT with the contributor."}]}}
{"project": "SPARK", "issue_id": "SPARK-1432", "title": "Potential memory leak in stageIdToExecutorSummaries in JobProgressTracker", "status": "Resolved", "priority": "Major", "reporter": "Davis Shepherd", "assignee": "Davis Shepherd", "labels": [], "created": "2014-04-07T16:01:07.000+0000", "updated": "2014-04-07T17:07:16.000+0000", "description": "JobProgressTracker continuously cleans up old metadata as per the spark.ui.retainedStages configuration parameter. It seems however that not all metadata maps are being cleaned, in particular stageIdToExecutorSummaries could grow in an unbounded manner in a long running application.", "comments": ["https://github.com/apache/spark/pull/338"], "derived": {"summary": "JobProgressTracker continuously cleans up old metadata as per the spark. ui.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Potential memory leak in stageIdToExecutorSummaries in JobProgressTracker - JobProgressTracker continuously cleans up old metadata as per the spark. ui."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/338"}]}}
{"project": "SPARK", "issue_id": "SPARK-1433", "title": "Upgrade Mesos dependency to 0.17.0", "status": "Resolved", "priority": "Minor", "reporter": "Sandeep Singh", "assignee": "Sandeep Singh", "labels": [], "created": "2014-04-07T16:46:05.000+0000", "updated": "2014-05-12T18:12:45.000+0000", "description": "Mesos 0.13.0 was released 6 months ago.\nUpgrade Mesos dependency to 0.17.0", "comments": ["You mean Mesos?", "Sorry a typo", "[INFO] Reactor Summary:\n[INFO]\n[INFO] Spark Project Parent POM .......................... SUCCESS [2.002s]\n[INFO] Spark Project Core ................................ SUCCESS [30.635s]\n[INFO] Spark Project Bagel ............................... SUCCESS [0.883s]\n[INFO] Spark Project GraphX .............................. SUCCESS [0.829s]\n[INFO] Spark Project ML Library .......................... SUCCESS [0.805s]\n[INFO] Spark Project Streaming ........................... SUCCESS [0.911s]\n[INFO] Spark Project Tools ............................... SUCCESS [0.645s]\n[INFO] Spark Project Catalyst ............................ SUCCESS [0.897s]\n[INFO] Spark Project SQL ................................. SUCCESS [1.193s]\n[INFO] Spark Project Hive ................................ SUCCESS [1.541s]\n[INFO] Spark Project REPL ................................ SUCCESS [1.164s]\n[INFO] Spark Project Assembly ............................ SUCCESS [1.729s]\n[INFO] Spark Project External Twitter .................... SUCCESS [0.809s]\n[INFO] Spark Project External Kafka ...................... SUCCESS [0.591s]\n[INFO] Spark Project External Flume ...................... SUCCESS [0.696s]\n[INFO] Spark Project External ZeroMQ ..................... SUCCESS [0.484s]\n[INFO] Spark Project External MQTT ....................... SUCCESS [0.543s]\n[INFO] Spark Project Examples ............................ SUCCESS [2.385s]", "Pull request https://github.com/apache/spark/pull/355", "I've reverted this due to an incompatiblity with some Hadoop versions.", "Likely want to aim higher at this point, perhaps 0.18.1", "I agree with [~tstclair].\n* Changes to use Mesos 0.18.1 with shaded protobufs can be found [here|https://github.com/berngp/spark/tree/feature/SPARK-1433]", "This is subsumed by SPARK-1806."], "derived": {"summary": "Mesos 0. 13.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Upgrade Mesos dependency to 0.17.0 - Mesos 0. 13."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is subsumed by SPARK-1806."}]}}
{"project": "SPARK", "issue_id": "SPARK-1434", "title": "Make labelParser Java friendly.", "status": "Resolved", "priority": "Minor", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-04-07T16:59:20.000+0000", "updated": "2014-04-09T03:39:09.000+0000", "description": "MLUtils#loadLibSVMData uses an anonymous function for the label parser. Java users won't like it. So I make a trait for LabelParser and provide two implementations: binary and multiclass.", "comments": [], "derived": {"summary": "MLUtils#loadLibSVMData uses an anonymous function for the label parser. Java users won't like it.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Make labelParser Java friendly. - MLUtils#loadLibSVMData uses an anonymous function for the label parser. Java users won't like it."}]}}
{"project": "SPARK", "issue_id": "SPARK-1435", "title": "Don't assume context class loader is set when creating classes via reflection", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "labels": [], "created": "2014-04-07T17:51:52.000+0000", "updated": "2014-04-13T03:51:38.000+0000", "description": null, "comments": ["SPARK-1403 provides a work around in the case of mesos, but in general we should just avoid making this assumption."], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Don't assume context class loader is set when creating classes via reflection"}, {"q": "What updates or decisions were made in the discussion?", "a": "SPARK-1403 provides a work around in the case of mesos, but in general we should just avoid making this assumption."}]}}
{"project": "SPARK", "issue_id": "SPARK-1436", "title": "Compression code broke in-memory store", "status": "Resolved", "priority": "Blocker", "reporter": "Reynold Xin", "assignee": "Cheng Lian", "labels": [], "created": "2014-04-07T19:11:03.000+0000", "updated": "2014-05-15T03:30:01.000+0000", "description": "Try run the following code:\n\n{code}\n\npackage org.apache.spark.sql\n\nimport org.apache.spark.sql.test.TestSQLContext._\nimport org.apache.spark.sql.catalyst.util._\n\ncase class Data(a: Int, b: Long)\n\nobject AggregationBenchmark {\n  def main(args: Array[String]): Unit = {\n    val rdd =\n      sparkContext.parallelize(1 to 20).flatMap(_ => (1 to 500000).map(i => Data(i % 100, i)))\n    rdd.registerAsTable(\"data\")\n    cacheTable(\"data\")\n\n    (1 to 10).foreach { i =>\n      println(s\"=== ITERATION $i ===\")\n\n      benchmark { println(\"SELECT COUNT() FROM data:\" + sql(\"SELECT COUNT(*) FROM data\").collect().head) }\n\n      println(\"SELECT a, SUM(b) FROM data GROUP BY a\")\n      benchmark { sql(\"SELECT a, SUM(b) FROM data GROUP BY a\").count() }\n\n      println(\"SELECT SUM(b) FROM data\")\n      benchmark { sql(\"SELECT SUM(b) FROM data\").count() }\n    }\n  }\n}\n{code}\n\nThe following exception is thrown:\n{code}\njava.nio.BufferUnderflowException\n\tat java.nio.Buffer.nextGetIndex(Buffer.java:498)\n\tat java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:355)\n\tat org.apache.spark.sql.columnar.ColumnAccessor$.apply(ColumnAccessor.scala:103)\n\tat org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1$$anon$1$$anonfun$3.apply(InMemoryColumnarTableScan.scala:61)\n\tat org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1$$anon$1$$anonfun$3.apply(InMemoryColumnarTableScan.scala:61)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)\n\tat org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1$$anon$1.<init>(InMemoryColumnarTableScan.scala:61)\n\tat org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1.apply(InMemoryColumnarTableScan.scala:60)\n\tat org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1.apply(InMemoryColumnarTableScan.scala:56)\n\tat org.apache.spark.rdd.RDD$$anonfun$3.apply(RDD.scala:504)\n\tat org.apache.spark.rdd.RDD$$anonfun$3.apply(RDD.scala:504)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:220)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:220)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:220)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:161)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:52)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)\n\tat org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:46)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\n14/04/07 12:07:38 WARN TaskSetManager: Lost TID 3 (task 4.0:0)\n14/04/07 12:07:38 WARN TaskSetManager: Loss was due to java.nio.BufferUnderflowException\njava.nio.BufferUnderflowException\n\tat java.nio.Buffer.nextGetIndex(Buffer.java:498)\n\tat java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:355)\n\tat org.apache.spark.sql.columnar.ColumnAccessor$.apply(ColumnAccessor.scala:103)\n\tat org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1$$anon$1$$anonfun$3.apply(InMemoryColumnarTableScan.scala:61)\n\tat org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1$$anon$1$$anonfun$3.apply(InMemoryColumnarTableScan.scala:61)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)\n\tat org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1$$anon$1.<init>(InMemoryColumnarTableScan.scala:61)\n\tat org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1.apply(InMemoryColumnarTableScan.scala:60)\n\tat org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1.apply(InMemoryColumnarTableScan.scala:56)\n\tat org.apache.spark.rdd.RDD$$anonfun$3.apply(RDD.scala:504)\n\tat org.apache.spark.rdd.RDD$$anonfun$3.apply(RDD.scala:504)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:220)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:220)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:220)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:161)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:52)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)\n\tat org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:46)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\n\n{code}", "comments": ["Try run the following code:\n\n{code}\n\npackage org.apache.spark.sql\n\nimport org.apache.spark.sql.test.TestSQLContext._\nimport org.apache.spark.sql.catalyst.util._\n\ncase class Data(a: Int, b: Long)\n\nobject AggregationBenchmark {\n  def main(args: Array[String]): Unit = {\n    val rdd =\n      sparkContext.parallelize(1 to 20).flatMap(_ => (1 to 500000).map(i => Data(i % 100, i)))\n    rdd.registerAsTable(\"data\")\n    cacheTable(\"data\")\n\n    (1 to 10).foreach { i =>\n      println(s\"=== ITERATION $i ===\")\n\n      benchmark { println(\"SELECT COUNT() FROM data:\" + sql(\"SELECT COUNT(*) FROM data\").collect().head) }\n\n      println(\"SELECT a, SUM(b) FROM data GROUP BY a\")\n      benchmark { sql(\"SELECT a, SUM(b) FROM data GROUP BY a\").count() }\n\n      println(\"SELECT SUM(b) FROM data\")\n      benchmark { sql(\"SELECT SUM(b) FROM data\").count() }\n    }\n  }\n}\n{code}\n\nThe following exception is thrown:\n{code}\njava.nio.BufferUnderflowException\n\tat java.nio.Buffer.nextGetIndex(Buffer.java:498)\n\tat java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:355)\n\tat org.apache.spark.sql.columnar.ColumnAccessor$.apply(ColumnAccessor.scala:103)\n\tat org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1$$anon$1$$anonfun$3.apply(InMemoryColumnarTableScan.scala:61)\n\tat org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1$$anon$1$$anonfun$3.apply(InMemoryColumnarTableScan.scala:61)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)\n\tat org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1$$anon$1.<init>(InMemoryColumnarTableScan.scala:61)\n\tat org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1.apply(InMemoryColumnarTableScan.scala:60)\n\tat org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1.apply(InMemoryColumnarTableScan.scala:56)\n\tat org.apache.spark.rdd.RDD$$anonfun$3.apply(RDD.scala:504)\n\tat org.apache.spark.rdd.RDD$$anonfun$3.apply(RDD.scala:504)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:220)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:220)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:220)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:161)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:52)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)\n\tat org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:46)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\n14/04/07 12:07:38 WARN TaskSetManager: Lost TID 3 (task 4.0:0)\n14/04/07 12:07:38 WARN TaskSetManager: Loss was due to java.nio.BufferUnderflowException\njava.nio.BufferUnderflowException\n\tat java.nio.Buffer.nextGetIndex(Buffer.java:498)\n\tat java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:355)\n\tat org.apache.spark.sql.columnar.ColumnAccessor$.apply(ColumnAccessor.scala:103)\n\tat org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1$$anon$1$$anonfun$3.apply(InMemoryColumnarTableScan.scala:61)\n\tat org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1$$anon$1$$anonfun$3.apply(InMemoryColumnarTableScan.scala:61)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)\n\tat org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1$$anon$1.<init>(InMemoryColumnarTableScan.scala:61)\n\tat org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1.apply(InMemoryColumnarTableScan.scala:60)\n\tat org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$execute$1.apply(InMemoryColumnarTableScan.scala:56)\n\tat org.apache.spark.rdd.RDD$$anonfun$3.apply(RDD.scala:504)\n\tat org.apache.spark.rdd.RDD$$anonfun$3.apply(RDD.scala:504)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:220)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:220)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:220)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:161)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:52)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)\n\tat org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:46)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\n\n{code}", "Sorry, forgot to duplicate the in-memory column byte buffer when creating new {{ColumnAccessor}}'s, so that when the column byte buffer is accessed multiple times, the position is not reset to 0. Will fix this in PR [#330|https://github.com/apache/spark/pull/330] with regression test.", "Fixed in [this commit|https://github.com/liancheng/spark/commit/1d037b83191099da961c247a57ef686cb508c447] of PR [#330|https://github.com/apache/spark/pull/330]"], "derived": {"summary": "Try run the following code:\n\n{code}\n\npackage org. apache.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Compression code broke in-memory store - Try run the following code:\n\n{code}\n\npackage org. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed in [this commit|https://github.com/liancheng/spark/commit/1d037b83191099da961c247a57ef686cb508c447] of PR [#330|https://github.com/apache/spark/pull/330]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1437", "title": "Jenkins should build with Java 6", "status": "Resolved", "priority": "Minor", "reporter": "Sean R. Owen", "assignee": "Shane Knapp", "labels": ["javac", "jenkins"], "created": "2014-04-07T22:01:44.000+0000", "updated": "2015-05-06T17:39:41.000+0000", "description": "Apologies if this was already on someone's to-do list, but I wanted to track this, as it bit two commits in the last few weeks.\n\nSpark is intended to work with Java 6, and so compiles with source/target 1.6. Java 7 can correctly enforce Java 6 language rules and emit Java 6 bytecode. However, unless otherwise configured with -bootclasspath, javac will use its own (Java 7) library classes. This means code that uses classes in Java 7 will be allowed to compile, but the result will fail when run on Java 6.\n\nThis is why you get warnings like ...\n\nUsing /usr/java/jdk1.7.0_51 as default JAVA_HOME.\n...\n[warn] warning: [options] bootstrap class path not set in conjunction with -source 1.6\n\nThe solution is just to tell Jenkins to use Java 6. This may be stating the obvious, but it should just be a setting under \"Configure\" for SparkPullRequestBuilder. In our Jenkinses, JDK 6/7/8 are set up; if it's not an option already I'm guessing it's not too hard to get Java 6 configured on the Amplab machines.", "comments": ["Pardon my boldness in pushing this onto your plate pwendell, but might be a very quick fix in Jenkins.\n\nIf Travis CI is going to be activated, it can definitely be configured to build with Java 6 and 7 both.", "Re-opening this, since it's now relevant for Project Tungsten (we hit a Java 6 build break due to a sun.misc.Unsafe API that wasn't present in that version of Java).", "..be good for the pull request test runs to use java6 too; I've already had to purge some java7-isms that didn't get picked up", "Hey [~shaneknapp] we had punted on this one a while back but it's now pretty important to day-to-day development for Spark. Any idea how much effort it would be to get this going on Jenkins?", "shouldn't be too much work...  i can get a 1.6 dist downloaded and synced out to the workers in no time and then you can set a specific JAVA_HOME for the java 6 builds.\n\nin a perfect world, i'd use the built-in JDK management, but that doesn't allow us to specify java versions for matrix projects.  :(\n\nanyways, let me get the JDK installed and deployment scripts updated.", "this is done.  to use java6 on the workers, you'll need to configure the jobs in two ways:\n\n1) set JAVA_HOME=/usr/java/jdk1.6.0_45\n2) prepend /usr/java/jdk1.6.0_45/bin to the PATH variable", "Heh, so now that we're all going to Java 7, I don't think we need to actually implement this, except possibly in builds for 1.4 and earlier? master builds and PRs will now (continue to) use Java 7", "that's my understanding:  all pre-1.4 builds will also be tested against\njava6...\n\n...FOR NOW?  @patrickwendell confirm pls?\n\n\n"], "derived": {"summary": "Apologies if this was already on someone's to-do list, but I wanted to track this, as it bit two commits in the last few weeks. Spark is intended to work with Java 6, and so compiles with source/target 1.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Jenkins should build with Java 6 - Apologies if this was already on someone's to-do list, but I wanted to track this, as it bit two commits in the last few weeks. Spark is intended to work with Java 6, and so compiles with source/target 1."}, {"q": "What updates or decisions were made in the discussion?", "a": "that's my understanding:  all pre-1.4 builds will also be tested against\njava6...\n\n...FOR NOW?  @patrickwendell confirm pls?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1438", "title": "Update RDD.sample() API to make seed parameter optional", "status": "Resolved", "priority": "Blocker", "reporter": "Matei Alexandru Zaharia", "assignee": "Arun Ramakrishnan", "labels": ["Starter"], "created": "2014-04-07T22:04:32.000+0000", "updated": "2014-04-25T00:29:26.000+0000", "description": "When a seed is not given, it should pick one based on Math.random().\n\nThis needs to be done in Java and Python as well.", "comments": ["You mean we should create a different method without seeds param or check for value of seed(is there a  default values for seed when not set) ?", "PartitionwiseSampledRDD already has the seed as an optional argument, using System.nanoTime as the default value. This seems reasonable, as Math.random() does the same thing (the first time). System.nanoTime is also usually high enough resolution that collisions are unlikely.\n\nScala and probably Python can use default arguments, Java will need an overloaded method.", "I see that org.apache.spark.sql.SchemaRDD also defines sample(), is there a reason why it must be overloaded?", "pull request at https://github.com/apache/spark/pull/462", "NEW PR at https://github.com/apache/spark/pull/477"], "derived": {"summary": "When a seed is not given, it should pick one based on Math. random().", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Update RDD.sample() API to make seed parameter optional - When a seed is not given, it should pick one based on Math. random()."}, {"q": "What updates or decisions were made in the discussion?", "a": "NEW PR at https://github.com/apache/spark/pull/477"}]}}
{"project": "SPARK", "issue_id": "SPARK-1439", "title": "Aggregate Scaladocs across projects", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "labels": [], "created": "2014-04-08T00:09:11.000+0000", "updated": "2014-04-22T05:02:48.000+0000", "description": "Apparently there's a \"Unidoc\" plugin to put together ScalaDocs across modules: https://github.com/akka/akka/blob/master/project/Unidoc.scala", "comments": ["I had a run at this today. First I tried Maven-based formulas, but didn't quite do the trick. I made some progress with unidoc although not all the way. Maybe an SBT expert can help me figure how to finish it.\n\n*Maven*\n\nhttp://stackoverflow.com/questions/12301620/how-to-generate-an-aggregated-scaladoc-for-a-maven-site\n\nThis works, but, generates *javadoc* for everything, including Scala source. The resulting javadoc is not so helpful. It also complains a lot about not finding references since javadoc doesn't quite understand links in the same way.\n\n*Maven #2*\n\nYou can also invoke the scala-maven-plugin 'doc' goal as part of the site generation:\n\n{code:xml}\n  <reporting>\n    <plugins>\n      ...\n      <plugin>\n        <groupId>net.alchim31.maven</groupId>\n        <artifactId>scala-maven-plugin</artifactId>\n        <reportSets>\n          <reportSet>\n            <reports>\n              <report>doc</report>\n            </reports>\n          </reportSet>\n        </reportSets>\n      </plugin>\n    </plugins>\n  </reporting>\n{code}\n\nIt lacks a goal like \"aggregate\" that the javadoc plugin has, which takes care of combining everything into one set of docs. This only generates scaladoc in each module in exploded format.\n\n*Unidoc / SBT*\n\nIt is almost as easy as:\n\n- adding the plugin to plugins.sbt: {{addSbtPlugin(\"com.eed3si9n\" % \"sbt-unidoc\" % \"0.3.0\")}}\n- {{import sbtunidoc.Plugin.\\_}} and {{UnidocKeys.\\_}} in SparkBuild.scala\n- adding \"++ unidocSettings\" to rootSettings in SparkBuild.scala\n\nbut it was also necessary to:\n\n- {{SPARK_YARN=true}} and {{SPARK_HADOOP_VERSION=2.2.0}}, for example, to make YARN scaladoc work\n- Exclude {{yarn-alpha}} since scaladoc doesn't like the collision of class names:\n\n{code}\n  def rootSettings = sharedSettings ++ unidocSettings ++ Seq(\n    unidocProjectFilter in (ScalaUnidoc, unidoc) := inAnyProject -- inProjects(yarnAlpha),\n    publish := {}\n  )\n{code}\n\nI still get SBT errors since I think this is not quite correctly finessing the build. But it seems almost there.\n", "Thanks for looking into this, Sean. Instead of messing around with YARN I got it working by just excluding the YARN project from unidoc, since none of the classes there are public APIs. I also got Javadoc to work after doing this. Check out https://github.com/apache/spark/pull/457."], "derived": {"summary": "Apparently there's a \"Unidoc\" plugin to put together ScalaDocs across modules: https://github. com/akka/akka/blob/master/project/Unidoc.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Aggregate Scaladocs across projects - Apparently there's a \"Unidoc\" plugin to put together ScalaDocs across modules: https://github. com/akka/akka/blob/master/project/Unidoc."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for looking into this, Sean. Instead of messing around with YARN I got it working by just excluding the YARN project from unidoc, since none of the classes there are public APIs. I also got Javadoc to work after doing this. Check out https://github.com/apache/spark/pull/457."}]}}
{"project": "SPARK", "issue_id": "SPARK-1440", "title": "Generate JavaDoc instead of ScalaDoc for Java API", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "labels": [], "created": "2014-04-08T00:10:03.000+0000", "updated": "2014-04-22T05:03:00.000+0000", "description": "It may be possible to use this plugin:  https://github.com/typesafehub/genjavadoc", "comments": [], "derived": {"summary": "It may be possible to use this plugin:  https://github. com/typesafehub/genjavadoc.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Generate JavaDoc instead of ScalaDoc for Java API - It may be possible to use this plugin:  https://github. com/typesafehub/genjavadoc."}]}}
{"project": "SPARK", "issue_id": "SPARK-1441", "title": "Compile Spark Core error with Hadoop 0.23.x when not using YARN", "status": "Resolved", "priority": "Major", "reporter": "Guoqiang Li", "assignee": "Guoqiang Li", "labels": [], "created": "2014-04-08T04:01:40.000+0000", "updated": "2014-04-29T06:18:39.000+0000", "description": null, "comments": ["{code}\n./make-distribution.sh --hadoop 0.23.9  > sbt.log\nmvn -Dhadoop.version=0.23.9 -DskipTests package -X > mvn.log\n{code}", "You're not enabling the yarn-alpha build profile here, and Hadoop 0.23.x is what it's for. Similarity for SBT although the \"profile\" is enabled by the SPARK_YARN env variable. Your change just forced the config intended for yarn-alpha into the rest of the build, which seems incorrect.", "If someone compile spark without yarn support,like this\n {code}./make-distribution.sh --hadoop 0.23.9{code}\nHow do we ensure compile?", "It sounds like the avro changes we put into the poms for yarn-alpha needs to be tied to another independent profile then so that someone building hadoop 0.23 without yarn can also pick them up.  (like -Pinclude-avro)  This is what was originally proposed but we changed it to tie to yarn-alpha profile because we didn't realize it affected this build. ", "I see, I had taken the \"yarn-alpha\" profile to be the slightly-misnamed profile you would use when building the whole project with 0.23, and building this distro means building everything. At least, that's a fairly fine solution, no? you should set yarn.version to 0.23.9 too.", "The yarn-alpha profile is tied to meaning included the spark on yarn support for hadoop 0.23.   If you don't need the spark on yarn support you should be able to just compile spark for hadoop 0.23 so that you can talk to HDFS on a hadoop 0.23 cluster, but you are running spark separately (standalone cluster or mesos).  I think the latter is what is trying to be done here.      It doesn't hurt anything to included the yarn pieces (other then slightly larger assembly jar) so that is atleast a work around for now. ", "Got it. Maybe the profile can be switched around to activate based on hadoop.version and/or yarn.version being 0.23.x. This sort of thing is possible in Maven, although I'm not 100% sure you can express a range constraint on a user property. ", "How can use multiple property to activate profile?\n{code}\n<activation>\n   <property><name>hadoop.version</name><value>0.23.9</value></property>\n</activation>\n{code} \nThis configuration is only activated when hadoop.version = 0.23.9"], "derived": {"summary": "", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Compile Spark Core error with Hadoop 0.23.x when not using YARN"}, {"q": "What updates or decisions were made in the discussion?", "a": "How can use multiple property to activate profile?\n{code}\n<activation>\n   <property><name>hadoop.version</name><value>0.23.9</value></property>\n</activation>\n{code} \nThis configuration is only activated when hadoop.version = 0.23.9"}]}}
{"project": "SPARK", "issue_id": "SPARK-1442", "title": "Add Window function support", "status": "Resolved", "priority": "Blocker", "reporter": "Chengxiang Li", "assignee": "guowei", "labels": [], "created": "2014-04-08T09:11:12.000+0000", "updated": "2016-08-31T07:34:20.000+0000", "description": "similiar to Hive, add window function support for catalyst.\nhttps://issues.apache.org/jira/browse/HIVE-4197\nhttps://issues.apache.org/jira/browse/HIVE-896", "comments": ["Does the Spark SQLContext support windowing functions with the support added into Hive?", "User 'guowei2' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/2953", "User 'liancheng' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/3703", "why the two PR both closed?", "I am setting the target version to 1.4. If we cannot make it, we will bump the version then.", "User 'guowei2' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/5604", "Issue resolved by pull request 5604\n[https://github.com/apache/spark/pull/5604]", "User 'yhuai' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/5945"], "derived": {"summary": "similiar to Hive, add window function support for catalyst. https://issues.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add Window function support - similiar to Hive, add window function support for catalyst. https://issues."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'yhuai' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/5945"}]}}
{"project": "SPARK", "issue_id": "SPARK-1443", "title": "Unable to Access MongoDB GridFS data with Spark using mongo-hadoop API", "status": "Resolved", "priority": "Critical", "reporter": "Pavan Kumar Varma", "assignee": null, "labels": ["GridFS", "MongoDB", "Spark", "hadoop2", "java"], "created": "2014-04-08T13:00:26.000+0000", "updated": "2014-09-21T14:00:27.000+0000", "description": "I saved a 2GB pdf file into MongoDB using GridFS. now i want process those GridFS collection data using Java Spark Mapreduce API. previously i have successfully processed mongoDB collections with Apache spark using Mongo-Hadoop connector. now i'm unable to GridFS collections with the following code.\n\nMongoConfigUtil.setInputURI(config, \"mongodb://localhost:27017/pdfbooks.fs.chunks\" );\n MongoConfigUtil.setOutputURI(config,\"mongodb://localhost:27017/\"+output );\n JavaPairRDD<Object, BSONObject> mongoRDD = sc.newAPIHadoopRDD(config,\n            com.mongodb.hadoop.MongoInputFormat.class, Object.class,\n            BSONObject.class);\n JavaRDD<String> words = mongoRDD.flatMap(new FlatMapFunction<Tuple2<Object,BSONObject>,\n   String>() {                                \n   @Override\n   public Iterable<String> call(Tuple2<Object, BSONObject> arg) {   \n   System.out.println(arg._2.toString());\n   ...\nPlease suggest/provide  better API methods to access MongoDB GridFS data.\n\n", "comments": ["working fine with spark when accessing normal mongodb collections.but while accessing GridFS collections i'm unable to predict the content inside that collection. I need help to access GridFS Data....Thank you!!", "[~PavanKumarVarma] i hope you've been able to resolve your issue over the past 5 months. since you'll get a better response asking on the spark user list than in jira, see http://spark.apache.org/community.html, i'm going to close this out."], "derived": {"summary": "I saved a 2GB pdf file into MongoDB using GridFS. now i want process those GridFS collection data using Java Spark Mapreduce API.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Unable to Access MongoDB GridFS data with Spark using mongo-hadoop API - I saved a 2GB pdf file into MongoDB using GridFS. now i want process those GridFS collection data using Java Spark Mapreduce API."}, {"q": "What updates or decisions were made in the discussion?", "a": "[~PavanKumarVarma] i hope you've been able to resolve your issue over the past 5 months. since you'll get a better response asking on the spark user list than in jira, see http://spark.apache.org/community.html, i'm going to close this out."}]}}
{"project": "SPARK", "issue_id": "SPARK-1444", "title": "Update branch-0.9's SBT to 0.13.1 so that it works with Java 8", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "2014-04-08T18:11:27.000+0000", "updated": "2015-03-08T16:34:07.000+0000", "description": "Apparently the older versions have problems if you compile on Java 8.", "comments": ["FWIW I quickly tried updating branch 0.9 this way, and I get errors claiming that net.virtual-void:sbt-dependency-graph:0.7.3 doesn't exist. Maven Central says it does, but there are merely lots of empty entries for all versions: http://search.maven.org/#search%7Cga%7C1%7Csbt-dependency-graph\nUpdating to SBT 0.13.6 didn't even work. I found published artifacts for this project, but only version 0.7.4. I don't know what to make of it, but abandoned trying on the theory that I'm missing something.", "I suggest we call this WontFix, as 0.9 is now 4 minor releases behind, SBT isn't the primary or only build, and the straightforward way to address this does not seem to work."], "derived": {"summary": "Apparently the older versions have problems if you compile on Java 8.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Update branch-0.9's SBT to 0.13.1 so that it works with Java 8 - Apparently the older versions have problems if you compile on Java 8."}, {"q": "What updates or decisions were made in the discussion?", "a": "I suggest we call this WontFix, as 0.9 is now 4 minor releases behind, SBT isn't the primary or only build, and the straightforward way to address this does not seem to work."}]}}
{"project": "SPARK", "issue_id": "SPARK-1445", "title": "compute-classpath.sh should not print error if lib_managed not found", "status": "Resolved", "priority": "Minor", "reporter": "Aaron Davidson", "assignee": "Aaron Davidson", "labels": [], "created": "2014-04-08T18:23:30.000+0000", "updated": "2014-04-08T21:40:35.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "compute-classpath.sh should not print error if lib_managed not found"}]}}
{"project": "SPARK", "issue_id": "SPARK-1446", "title": "Spark examples should not do a System.exit", "status": "Resolved", "priority": "Major", "reporter": "Thomas Graves", "assignee": "Sandeep Singh", "labels": [], "created": "2014-04-08T21:18:15.000+0000", "updated": "2014-04-10T07:37:55.000+0000", "description": "The spark examples should exit nice (sparkcontext.stop()) rather then doing a System.exit. The System.exit can cause issues like in SPARK-1407.  SparkHdfsLR and JavaWordCount both do the System.exit. We should look through all the examples.", "comments": ["I want to work on this.\nCalling sparkcontext.stop() before System.exit(0) will resolve this, or should we remove System.exit(0) and use just sparkcontext.stop()?", "https://github.com/apache/spark/pull/370"], "derived": {"summary": "The spark examples should exit nice (sparkcontext. stop()) rather then doing a System.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Spark examples should not do a System.exit - The spark examples should exit nice (sparkcontext. stop()) rather then doing a System."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/370"}]}}
{"project": "SPARK", "issue_id": "SPARK-1447", "title": "Update Java programming guide to explain Java 8 syntax", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "2014-04-08T21:35:18.000+0000", "updated": "2014-04-22T04:51:54.000+0000", "description": null, "comments": ["This is already addressed in the programming guide."], "derived": {"summary": "", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Update Java programming guide to explain Java 8 syntax"}, {"q": "What updates or decisions were made in the discussion?", "a": "This is already addressed in the programming guide."}]}}
{"project": "SPARK", "issue_id": "SPARK-1448", "title": "KEYS file to be added to dist directory", "status": "Closed", "priority": "Major", "reporter": "Sebb", "assignee": null, "labels": [], "created": "2014-04-09T00:08:42.000+0000", "updated": "2014-06-12T12:14:06.000+0000", "description": "In order to validate signature files, users must have access to the KEYS file.\n\nThis is present at\n\nhttp://www.apache.org/dist/incubator/spark/KEYS\n\nbut needs to be copied to the TLP directory at\n\nhttps://dist.apache.org/repos/dist/release/spark/\n\nso it becomes avaliable for the 0.9.1 release", "comments": [], "derived": {"summary": "In order to validate signature files, users must have access to the KEYS file. This is present at\n\nhttp://www.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "KEYS file to be added to dist directory - In order to validate signature files, users must have access to the KEYS file. This is present at\n\nhttp://www."}]}}
{"project": "SPARK", "issue_id": "SPARK-1449", "title": "Please delete old releases from mirroring system", "status": "Resolved", "priority": "Major", "reporter": "Sebb", "assignee": "Sean R. Owen", "labels": [], "created": "2014-04-09T00:10:11.000+0000", "updated": "2018-12-05T10:15:55.000+0000", "description": "To reduce the load on the ASF mirrors, projects are required to delete old releases [1]\n\nPlease can you remove all non-current releases?\nThanks!\n[Note that older releases are always available from the ASF archive server]\n\nAny links to older releases on download pages should first be adjusted to point to the archive server.\n\n[1] http://www.apache.org/dev/release.html#when-to-archive\n", "comments": ["The Spark project uses svnpubsub, so any PMC member should be able to clear up the superseded releases\n\nhttps://dist.apache.org/repos/dist/release/spark/\n\nNote that older releases are always available from the archive server.", "PING - is there anybody there?", "Sebb, is this just a matter of \"svn co https://dist.apache.org/repos/dist/release/spark/\" and svn rm'ing the 0.9.1 and 1.0.0 releases?\nI'd do it but I don't have access. I think.\n\n[~pwendell] maybe this can be a step in the release process if not already? It may well be and these older ones were just missed last time.", "No need to check out the directory tree (which is large), you can remove files directly from SVN using \n\"svn delete (del, remove, rm)\"\n\nBy default all members of the Spark PMC [1] will have karma to update the dist/release/spark tree.\nIn particular whoever uploaded the last release should have ensured that previous releases were tidied up a few days after uploading the latest release ...\n\nThe PMC can vote to ask Infra if they wish the dist/release/spark tree to be updateable by non-PMC members as well.\n\n[1] http://people.apache.org/committers-by-project.html#spark-pmc", "[~pwendell] can you or someone else on the PMC zap this one? should be straightforward.", "Is no-one able to deal with this please?", "Hey folks, sorry for the delay -- will look into this soon.", "I've left 1.0.2 and 1.1.0, since 1.1.0 is an unstable release. I've also updated our release process documentation to make sure we do this in the future every time we make a release.", "The mirror system includes the following:\n\nspark-1.6.2\nspark-1.6.3\nspark-2.0.1\nspark-2.0.2\nspark-2.1.0\nspark-2.1.1\n\nAt least half of these are clearly superseded versions which should please be deleted.", "[~marmbrus] I think this step got missed in the last few releases. I will zap these files, after I update the downloads site to point to the archive for the removed releases, and after that change syncs to the main spark.apache.org site.", "The web site is updated to point to archive as appropriate, and old releases have been removed (again)."], "derived": {"summary": "To reduce the load on the ASF mirrors, projects are required to delete old releases [1]\n\nPlease can you remove all non-current releases?\nThanks!\n[Note that older releases are always available from the ASF archive server]\n\nAny links to older releases on download pages should first be adjusted to point to the archive server. [1] http://www.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Please delete old releases from mirroring system - To reduce the load on the ASF mirrors, projects are required to delete old releases [1]\n\nPlease can you remove all non-current releases?\nThanks!\n[Note that older releases are always available from the ASF archive server]\n\nAny links to older releases on download pages should first be adjusted to point to the archive server. [1] http://www."}, {"q": "What updates or decisions were made in the discussion?", "a": "The web site is updated to point to archive as appropriate, and old releases have been removed (again)."}]}}
{"project": "SPARK", "issue_id": "SPARK-1450", "title": "Specify the default zone in the EC2 script help", "status": "Resolved", "priority": "Minor", "reporter": "Tathagata Das", "assignee": "Sean R. Owen", "labels": [], "created": "2014-04-09T00:24:41.000+0000", "updated": "2014-11-28T22:44:10.000+0000", "description": null, "comments": ["User 'srowen' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/3454"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Specify the default zone in the EC2 script help"}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'srowen' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/3454"}]}}
{"project": "SPARK", "issue_id": "SPARK-1451", "title": "Multinomial Logistic Regression Support", "status": "Closed", "priority": "Major", "reporter": "DB Tsai", "assignee": null, "labels": ["features"], "created": "2014-04-09T05:03:28.000+0000", "updated": "2014-07-16T01:45:34.000+0000", "description": "Logistic Regression can not only be used for modeling binary outcomes but also multinomial outcome with some extension. In this work, we're going to add costFun for Multinomial Logistic Regression to support multinomial outcome.", "comments": ["Duplicate of SPARK-2309"], "derived": {"summary": "Logistic Regression can not only be used for modeling binary outcomes but also multinomial outcome with some extension. In this work, we're going to add costFun for Multinomial Logistic Regression to support multinomial outcome.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Multinomial Logistic Regression Support - Logistic Regression can not only be used for modeling binary outcomes but also multinomial outcome with some extension. In this work, we're going to add costFun for Multinomial Logistic Regression to support multinomial outcome."}, {"q": "What updates or decisions were made in the discussion?", "a": "Duplicate of SPARK-2309"}]}}
{"project": "SPARK", "issue_id": "SPARK-1452", "title": "dynamic partition creation not working on cached table", "status": "Resolved", "priority": "Major", "reporter": "Jai Kumar Singh", "assignee": null, "labels": ["Shark"], "created": "2014-04-09T09:17:47.000+0000", "updated": "2015-02-07T17:26:37.000+0000", "description": "dynamic partition creation via shark QL command not working with cached table. Though it works fine with non-cached tables.\nAlso static partition is working fine with cached table. \n\n\nshark> desc sample;\nOK\ncid                     string                  None                \nhost                    string                  None                \nurl                     string                  None                \nbytes                   int                     None                \npckts                   int                     None                \napp                     string                  None                \ncat                     string                  None                \nTime taken: 0.149 seconds\nshark> \n\nshark> desc sample_cached;\nOK\ncat                     string                  from deserializer   \nhost                    string                  from deserializer   \ncid                     string                  None                \n                 \n# Partition Information          \n# col_name              data_type               comment             \n                 \ncid                     string                  None                \nTime taken: 0.15 seconds\nshark> \n\nshark> insert into table sample_cached partition(cid) select cat,host,cid from sample;\nFAILED: Hive Internal Error: java.lang.NullPointerException(null)\nshark> \n\nshark> insert into table sample_cached partition(cid=\"my-cid\") select cat,host from sample limit 20;\njava.lang.InstantiationException: scala.Some\nContinuing ...\njava.lang.RuntimeException: failed to evaluate: <unbound>=Class.new();\nContinuing ...\nLoading data to table default.sample_cached partition (cid=my-cid)\nOK\nTime taken: 64.268 seconds\n\n\n\nI am logging this issue over here because https://spark-project.atlassian.net/browse/SHARK/?selectedTab=com.atlassian.jira.jira-projects-plugin:issues-panel not allowing me to log the issue there.", "comments": ["If this is a Shark issue then I believe it's WontFix, as Shark isn't developed anymore."], "derived": {"summary": "dynamic partition creation via shark QL command not working with cached table. Though it works fine with non-cached tables.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "dynamic partition creation not working on cached table - dynamic partition creation via shark QL command not working with cached table. Though it works fine with non-cached tables."}, {"q": "What updates or decisions were made in the discussion?", "a": "If this is a Shark issue then I believe it's WontFix, as Shark isn't developed anymore."}]}}
{"project": "SPARK", "issue_id": "SPARK-1453", "title": "Improve the way Spark on Yarn waits for executors before starting", "status": "Resolved", "priority": "Major", "reporter": "Thomas Graves", "assignee": "Thomas Graves", "labels": [], "created": "2014-04-09T15:17:09.000+0000", "updated": "2014-07-14T20:32:34.000+0000", "description": "Currently Spark on Yarn just delays a few seconds between when the spark context is initialized and when it allows the job to start.  If you are on a busy hadoop cluster is might take longer to get the number of executors. \n\nIn the very least we could make this timeout a configurable value.  Its currently hardcoded to 3 seconds.  \nBetter yet would be to allow user to give a minimum number of executors it wants to wait for, but that looks much more complex. \n", "comments": ["The timeout gets hit only when we dont get requested executors, right ? So it is more like max timeout (controlled by number of times we loop iirc).\nThe reason for keeping it stupid was simply because we have no gaurantees of number of containers which might be available to spark in a busy cluster : at times, it might not be practically possible to even get a fraction of the requested nodes (either due to busy cluster, or because of lack of resources - so infinite wait).\n\nIdeally, I should have exposed the number of containers allocated - so that atleast user code could use it as spi and decide how to proceed for more complex cases. Missed out on this one.\n\nI am not sure which usecases make sense.\na) Wait for X seconds or requested containers allocated.\nb) Wait until minimum of Y containers allocated (out of X requested).\nc) (b) with (a) - that is min containers and timeout on that.\nd) (c) with exit if min containers not allocated ?\n\n(d) is something which I keep hitting into (if I dont get my required minimum nodes, and job proceeds, I usually end up bringing down those nodes :-( )", "Actually there are 2 timeouts. The one you mention which is  a max.  Then one in YarnClusterScheduler which I think is basically always another 3 seconds.\n\nIdeally I think this should be b) by default with possibly the option for c), where c means I want x% but if I don't get it within a certain amount of time go ahead and run because I know my application will run ok with less resources (just not run optimally). \n\nI don't see a reason to do d).  If you have submitted your application then you want something to run.  If it exits then you have wasted all that time waiting.  I would rather the user just kill it if they have that tight of sla's.  Or they should get their own queue or reconfigure their queue.\n\nI'd be ok with adding the option for d if some power users want it.  I think for most normal users b is the best default behavior though.  If possible we should tell the user why its waiting too.", "\n(d) becomes relevant in case of headless/cron'ed jobs.\nIf the job is user initiated, then I agree, the user would typically kill and restart the job.", "duplicate of https://issues.apache.org/jira/browse/SPARK-1946"], "derived": {"summary": "Currently Spark on Yarn just delays a few seconds between when the spark context is initialized and when it allows the job to start. If you are on a busy hadoop cluster is might take longer to get the number of executors.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Improve the way Spark on Yarn waits for executors before starting - Currently Spark on Yarn just delays a few seconds between when the spark context is initialized and when it allows the job to start. If you are on a busy hadoop cluster is might take longer to get the number of executors."}, {"q": "What updates or decisions were made in the discussion?", "a": "duplicate of https://issues.apache.org/jira/browse/SPARK-1946"}]}}
{"project": "SPARK", "issue_id": "SPARK-1454", "title": "PySpark accumulators fail to update when runJob takes serialized/captured closures", "status": "Resolved", "priority": "Minor", "reporter": "William Benton", "assignee": null, "labels": [], "created": "2014-04-09T18:35:55.000+0000", "updated": "2016-01-04T14:42:29.000+0000", "description": "My patch for SPARK-729 optionally serializes closures when they are cleaned (in order to capture the values of mutable free variables at declaration time rather than at execution time).  This behavior is currently disabled for the closure argument to SparkContext.runJob, because enabling it there causes Python accumulators to fail to update.\n\nThe purpose of this JIRA is to note this issue and fix whatever is causing Python accumulators to behave this way so that closures passed to runJob can be captured in general.", "comments": ["Can someone please assign this to me?"], "derived": {"summary": "My patch for SPARK-729 optionally serializes closures when they are cleaned (in order to capture the values of mutable free variables at declaration time rather than at execution time). This behavior is currently disabled for the closure argument to SparkContext.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "PySpark accumulators fail to update when runJob takes serialized/captured closures - My patch for SPARK-729 optionally serializes closures when they are cleaned (in order to capture the values of mutable free variables at declaration time rather than at execution time). This behavior is currently disabled for the closure argument to SparkContext."}, {"q": "What updates or decisions were made in the discussion?", "a": "Can someone please assign this to me?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1455", "title": "Determine which test suites to run based on code changes", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": null, "labels": [], "created": "2014-04-09T19:13:27.000+0000", "updated": "2014-09-18T23:49:57.000+0000", "description": "Right now we run the entire set of tests for every change. This means the tests take a long time. Our pull request builder checks out the merge branch from git, so we could do a diff and figure out what source files were changed, and run a more isolated set of tests. We should just run tests in a way that reflects the inter-dependencies of the project. E.g:\n\n- If Spark core is modified, we should run all tests\n- If just SQL is modified, we should run only the SQL tests\n- If just Streaming is modified, we should run only the streaming tests\n- If just Pyspark is modified, we only run the PySpark tests.\n\nAnd so on. I think this would reduce the RTT of the tests a lot and it should be pretty easy to accomplish with some scripting foo.", "comments": ["This is a great idea.  One possible modification: If there are changes in Spark Core but not in Spark SQL, it is probably safe to skip running some of the more expensive test cases, specifically all of the hive compatibility query tests.  Since all the query operators just use mapPartitions, I think the other query tests would find changes to Spark core that are going to break Spark SQL.", "This is partially dealt with in https://github.com/apache/spark/pull/420, but only for the hive tests.", "I think we could reuse some of [the logic here|https://github.com/databricks/spark-pr-dashboard/blob/bf289c15ffd959eed0911f12591bb1b08672b2dd/sparkprs/models.py#L71] to determine what tests to run from the files that were changed.", "Which approach do y'all prefer?\n# Have {{dev/run-tests-jenkins}} determine which files changed and pass an argument to {{dev/run-tests}}, which will in turn run the appropriate tests. This allows Jenkins to report back to GitHub that, for example, only SQL tests are being run.\n# Have {{dev/run-tests}} determine which files changed and which tests to run. Let lets developers testing locally take advantage of the selective testing, but makes it difficult for Jenkins to report on GitHub what tests are being run. You'd most likely have to check the console output.", "I'd prefer the first.  For testing of individual projects its probably faster to use \"sbt/sbt <project>/test\" or \"sbt/sbt <project>/test-only <testsuite>\" anyway.", "User 'nchammas' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/2420", "This is mostly fixed by changes from [~nchammas] so I'll close this for now.", "There is still some work to be done to really run tests selectively, since right now we only do this for SQL tests. But if you want to track that in a separate JIRA issue we can do that."], "derived": {"summary": "Right now we run the entire set of tests for every change. This means the tests take a long time.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Determine which test suites to run based on code changes - Right now we run the entire set of tests for every change. This means the tests take a long time."}, {"q": "What updates or decisions were made in the discussion?", "a": "There is still some work to be done to really run tests selectively, since right now we only do this for SQL tests. But if you want to track that in a separate JIRA issue we can do that."}]}}
{"project": "SPARK", "issue_id": "SPARK-1456", "title": "Clean up use of Ordered/Ordering in OrderedRDDFunctions", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Michael Armbrust", "labels": [], "created": "2014-04-09T19:56:20.000+0000", "updated": "2014-04-21T17:52:15.000+0000", "description": "Need to discuss more with Reynold, but we should clean this up in case we need to slightly change APIs.", "comments": [], "derived": {"summary": "Need to discuss more with Reynold, but we should clean this up in case we need to slightly change APIs.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Clean up use of Ordered/Ordering in OrderedRDDFunctions - Need to discuss more with Reynold, but we should clean this up in case we need to slightly change APIs."}]}}
{"project": "SPARK", "issue_id": "SPARK-1457", "title": "Change APIs for training algorithms to take optimizer as parameter ", "status": "Resolved", "priority": "Minor", "reporter": "DB Tsai", "assignee": null, "labels": [], "created": "2014-04-09T21:20:35.000+0000", "updated": "2016-11-07T18:03:28.000+0000", "description": "Currently, the training api has signature like LogisticRegressionWithSGD. \n\nIf we want to use another optimizer, we've two options, either adding new api like LogisticRegressionWithNewOptimizer which causes 99% of the code duplication, or we can re-factorize the api to take the optimizer as an option like the following. \n\nclass LogisticRegression private (\n    var optimizer: Optimizer)\n  extends GeneralizedLinearAlgorithm[LogisticRegressionModel]\n\n", "comments": ["In this case, LogisticRegressionWithSGD hard-codes a gradient descent optimizer since it's, well, \"WithSGD\". You can still partially configure this optimizer.\nYou can of course always subclass GeneralizedLinearAlgorithm. It does make you duplicate a few lines of code, though it's not too bad.\n\nIt makes some sense to add a generic superclass in the middle here. What optimizer do you want to put in place though? there is already an implementation for L-BFGS too, which requires more than swapping out the optimizer.\n\nThat is I'm wondering whether it helps much in practice.", "I think this is not a bad idea but think this might have timed out, and will be superseded by the new ML API anyway."], "derived": {"summary": "Currently, the training api has signature like LogisticRegressionWithSGD. If we want to use another optimizer, we've two options, either adding new api like LogisticRegressionWithNewOptimizer which causes 99% of the code duplication, or we can re-factorize the api to take the optimizer as an option like the following.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Change APIs for training algorithms to take optimizer as parameter  - Currently, the training api has signature like LogisticRegressionWithSGD. If we want to use another optimizer, we've two options, either adding new api like LogisticRegressionWithNewOptimizer which causes 99% of the code duplication, or we can re-factorize the api to take the optimizer as an option like the following."}, {"q": "What updates or decisions were made in the discussion?", "a": "I think this is not a bad idea but think this might have timed out, and will be superseded by the new ML API anyway."}]}}
{"project": "SPARK", "issue_id": "SPARK-1458", "title": "Expose sc.version in PySpark", "status": "Resolved", "priority": "Minor", "reporter": "Nicholas Chammas", "assignee": "Josh Rosen", "labels": [], "created": "2014-04-09T22:22:08.000+0000", "updated": "2014-07-26T07:54:44.000+0000", "description": "As discussed [here|http://apache-spark-user-list.1001560.n3.nabble.com/programmatic-way-to-tell-Spark-version-td1929.html], I think it would be nice if there was a way to programmatically determine what version of Spark you are running. \n\nThe potential use cases are not that important, but they include:\n# Branching your code based on what version of Spark is running.\n# Checking your version without having to quit and restart the Spark shell.\n\nRight now in PySpark, I believe the only way to determine your version is by firing up the Spark shell and looking at the startup banner.", "comments": ["This issue appears to be resolved in [this merge|https://github.com/apache/spark/pull/204/files#diff-364713d7776956cb8b0a771e9b62f82dR779].", "Reopening this ticket. \n\nIt seems that, while the Scala shell exposes the Spark version via {{sc.version}}, this does not appear to be available in PySpark.", "[~nchammas] I updated the JIRA title to reflect the scope. We should just add this in PySpark, should be an easy fix!", "[~pwendell] if you're ok with having another version number to update in the code for release, i'm happy to close this gap between scala and python.", "Perhaps that could also be some kind of unit test: Check that certain hand-inputted values are identical across Scala and Python, like the shell banner and {{sc.version}}.", "Isn't it possible to just have the python function call the Java/Scala one? That way there is no new version number to be updated anywhere.", "User 'JoshRosen' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1596", "Derp, I guess we [overcomplicated|https://github.com/apache/spark/pull/1596/files] this. Expose the version in the Java context, and then call that from the Python one. Thanks [~joshrosen] for taking care of it!"], "derived": {"summary": "As discussed [here|http://apache-spark-user-list. 1001560.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Expose sc.version in PySpark - As discussed [here|http://apache-spark-user-list. 1001560."}, {"q": "What updates or decisions were made in the discussion?", "a": "Derp, I guess we [overcomplicated|https://github.com/apache/spark/pull/1596/files] this. Expose the version in the Java context, and then call that from the Python one. Thanks [~joshrosen] for taking care of it!"}]}}
{"project": "SPARK", "issue_id": "SPARK-1459", "title": "EventLoggingListener does not work with \"file://\" target dir", "status": "Resolved", "priority": "Major", "reporter": "Marcelo Masiero Vanzin", "assignee": "Marcelo Masiero Vanzin", "labels": [], "created": "2014-04-09T23:03:51.000+0000", "updated": "2014-04-22T06:12:17.000+0000", "description": "Bug is simple; FileLogger tries to pass a URL to FileOutputStream's constructor, and that fails. I'll upload a PR soon.", "comments": ["PR #375 (https://github.com/apache/spark/pull/375)", "This was commit 69047506. (If someone with permissions could set me as the assignee that would be great.)", "Sorry, got confused. This PR is still pending."], "derived": {"summary": "Bug is simple; FileLogger tries to pass a URL to FileOutputStream's constructor, and that fails. I'll upload a PR soon.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "EventLoggingListener does not work with \"file://\" target dir - Bug is simple; FileLogger tries to pass a URL to FileOutputStream's constructor, and that fails. I'll upload a PR soon."}, {"q": "What updates or decisions were made in the discussion?", "a": "Sorry, got confused. This PR is still pending."}]}}
{"project": "SPARK", "issue_id": "SPARK-1460", "title": "Set operations on SchemaRDDs are needlessly destructive of schema information.", "status": "Resolved", "priority": "Major", "reporter": "Michael Armbrust", "assignee": "Kan Zhang", "labels": [], "created": "2014-04-09T23:30:51.000+0000", "updated": "2014-05-07T17:37:11.000+0000", "description": "When you do a distinct of a subtract, you get back a normal RDD instead of a schema RDD, even though the schema is unchanged.", "comments": ["Resolved by: https://github.com/apache/spark/pull/448"], "derived": {"summary": "When you do a distinct of a subtract, you get back a normal RDD instead of a schema RDD, even though the schema is unchanged.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Set operations on SchemaRDDs are needlessly destructive of schema information. - When you do a distinct of a subtract, you get back a normal RDD instead of a schema RDD, even though the schema is unchanged."}, {"q": "What updates or decisions were made in the discussion?", "a": "Resolved by: https://github.com/apache/spark/pull/448"}]}}
{"project": "SPARK", "issue_id": "SPARK-1461", "title": "Support Short-circuit Expression Evaluation", "status": "Resolved", "priority": "Major", "reporter": "Cheng Hao", "assignee": "Cheng Hao", "labels": [], "created": "2014-04-10T06:43:21.000+0000", "updated": "2014-06-03T05:59:25.000+0000", "description": "Short-circuit expression evaluation impacts the performance significantly.\n\ne.g.\nIn expression: (a>b) && (c>d), if a>b equals false or null, the expression value is false or null definitely without considering the value of sub expression c>d.\n\nHowever, if c or d contains a stateful UDF  (for example, the UDF row_sequence) as its child expression, we have to evaluate the stateful expression but ignore its result.", "comments": ["The PR https://github.com/apache/spark/pull/446"], "derived": {"summary": "Short-circuit expression evaluation impacts the performance significantly. e.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support Short-circuit Expression Evaluation - Short-circuit expression evaluation impacts the performance significantly. e."}, {"q": "What updates or decisions were made in the discussion?", "a": "The PR https://github.com/apache/spark/pull/446"}]}}
{"project": "SPARK", "issue_id": "SPARK-1462", "title": "Examples of ML algorithms are using deprecated APIs", "status": "Resolved", "priority": "Major", "reporter": "Nan Zhu", "assignee": "Sandeep Singh", "labels": [], "created": "2014-04-10T11:55:41.000+0000", "updated": "2014-04-17T22:27:13.000+0000", "description": "mainly Vector, better to be Vectors.dense", "comments": ["mainly Vector, better to be Vector.dense", "I'm working on this one", "https://github.com/apache/spark/pull/416"], "derived": {"summary": "mainly Vector, better to be Vectors. dense.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Examples of ML algorithms are using deprecated APIs - mainly Vector, better to be Vectors. dense."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/416"}]}}
{"project": "SPARK", "issue_id": "SPARK-1463", "title": "cleanup unnecessary dependency jars in the spark assembly jars", "status": "Resolved", "priority": "Minor", "reporter": "Jenny MA", "assignee": null, "labels": ["easyfix"], "created": "2014-04-10T17:23:16.000+0000", "updated": "2014-11-11T14:15:57.000+0000", "description": "there are couple GPL/LGPL based dependencies which are included in the final assembly jar, which are not used by spark runtime.  identified the following libraries. we can provide a fix in assembly/pom.xml. \n\n<exclude>com.google.code.findbugs:*</exclude>\n<exclude>org.acplt:oncrpc:*</exclude>\n<exclude>glassfish:*</exclude>\n<exclude>com.cenqua.clover:clover:*</exclude>\n<exclude>org.glassfish:*</exclude>\n<exclude>org.glassfish.grizzly:*</exclude>\n<exclude>org.glassfish.gmbal:*</exclude> <exclude>org.glassfish.external:*</exclude>\n \n", "comments": ["FWIW I do not see these packages in the final assembly JAR anymore. This may be obsolete?", "Fixed at some point, it seems. No longer in the project."], "derived": {"summary": "there are couple GPL/LGPL based dependencies which are included in the final assembly jar, which are not used by spark runtime. identified the following libraries.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "cleanup unnecessary dependency jars in the spark assembly jars - there are couple GPL/LGPL based dependencies which are included in the final assembly jar, which are not used by spark runtime. identified the following libraries."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed at some point, it seems. No longer in the project."}]}}
{"project": "SPARK", "issue_id": "SPARK-1464", "title": "Update MLLib Examples to Use Breeze", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-04-10T20:10:58.000+0000", "updated": "2014-04-17T22:27:13.000+0000", "description": "If we want to deprecate the vector class we need to update all of the examples to use Breeze.", "comments": ["https://github.com/apache/spark/pull/416", "This is a duplicate of https://issues.apache.org/jira/browse/SPARK-1462 which is resolved now."], "derived": {"summary": "If we want to deprecate the vector class we need to update all of the examples to use Breeze.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Update MLLib Examples to Use Breeze - If we want to deprecate the vector class we need to update all of the examples to use Breeze."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is a duplicate of https://issues.apache.org/jira/browse/SPARK-1462 which is resolved now."}]}}
{"project": "SPARK", "issue_id": "SPARK-1465", "title": "Spark compilation is broken with the latest hadoop-2.4.0 release", "status": "Resolved", "priority": "Blocker", "reporter": "Xuan Gong", "assignee": null, "labels": [], "created": "2014-04-10T21:10:57.000+0000", "updated": "2014-04-16T19:46:53.000+0000", "description": "Building spark with the latest 2.4.0 version of yarn, appears to be broken\n{code}\n[ERROR]       Apps.addToEnvironment(env, Environment.CLASSPATH.name, Environment.PWD.$() +\n[ERROR]                            ^\n[ERROR] /yarn/common/src/main/scala/org/apache/spark/deploy/yarn/ClientBase.scala:441: not enough arguments for method addToEnvironment: (x$1: java.util.Map[String,String], x$2: String, x$3: String, x$4: String)Unit.\nUnspecified value parameter x$4.\n[ERROR]     Apps.addToEnvironment(env, Environment.CLASSPATH.name, Environment.PWD.$() +\n[ERROR]                          ^\n[ERROR] /yarn/common/src/main/scala/org/apache/spark/deploy/yarn/ClientBase.scala:446: not enough arguments for method addToEnvironment: (x$1: java.util.Map[String,String], x$2: String, x$3: String, x$4: String)Unit.\nUnspecified value parameter x$4.\n[ERROR]       Apps.addToEnvironment(env, Environment.CLASSPATH.name, Environment.PWD.$() +\n[ERROR]                            ^\n[ERROR] /yarn/common/src/main/scala/org/apache/spark/deploy/yarn/ClientBase.scala:449: not enough arguments for method addToEnvironment: (x$1: java.util.Map[String,String], x$2: String, x$3: String, x$4: String)Unit.\nUnspecified value parameter x$4.\n[ERROR]     Apps.addToEnvironment(env, Environment.CLASSPATH.name, Environment.PWD.$() +\n[ERROR]                          ^\n[ERROR] /yarn/common/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnableUtil.scala:170: not enough arguments for method setEnvFromInputString: (x$1: java.util.Map[String,String], x$2: String, x$3: String)Unit.\nUnspecified value parameter x$3.\n[ERROR]     Apps.setEnvFromInputString(env, System.getenv(\"SPARK_YARN_USER_ENV\"))\n{code}", "comments": ["YARN-1824 changes the APIs in Apps, which causes the spark build to break if built against a version 2.4.0.\n", "I am starting to fix this.", "You can't just update spark to the new api as it will break compiling against 2.3 and other 2.x versions.    I will file a bug against Hadoop for this and it should be fixed in a 2.4.1 release.  Unfortunately that doesn't help people using 2.4.  \n\nFor Spark I think we are better off just to make our own functions that do that functionality.", "https://issues.apache.org/jira/browse/YARN-1931 filed for this breakage in api.", "[~tgraves] \nbq. For Spark I think we are better off just to make our own functions that do that functionality.\n\nYes, that is what I am doing right now...", "https://github.com/apache/spark/pull/396"], "derived": {"summary": "Building spark with the latest 2. 4.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Spark compilation is broken with the latest hadoop-2.4.0 release - Building spark with the latest 2. 4."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/396"}]}}
{"project": "SPARK", "issue_id": "SPARK-1466", "title": "Pyspark doesn't check if gateway process launches correctly", "status": "Resolved", "priority": "Blocker", "reporter": "Kay Ousterhout", "assignee": "Kay Ousterhout", "labels": [], "created": "2014-04-10T21:17:13.000+0000", "updated": "2014-06-18T20:23:22.000+0000", "description": "If the gateway process fails to start correctly (e.g., because JAVA_HOME isn't set correctly, there's no Spark jar, etc.), right now pyspark fails because of a very difficult-to-understand error, where we try to parse stdout to get the port where Spark started and there's nothing there.  We should properly catch the error, print it to the user, and exit.", "comments": ["https://github.com/apache/spark/pull/383", "[~pwendell] I didn't merge this into the 1.0 branch since it's pretty minor and I don't know if anyone besides the students in the course where we used iPython notebook have noticed this. I know it was marked as a blocker but I think that was an inflated priority -- anyway let me know if you think it should go in for 1.0.1."], "derived": {"summary": "If the gateway process fails to start correctly (e. g.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Pyspark doesn't check if gateway process launches correctly - If the gateway process fails to start correctly (e. g."}, {"q": "What updates or decisions were made in the discussion?", "a": "[~pwendell] I didn't merge this into the 1.0 branch since it's pretty minor and I don't know if anyone besides the students in the course where we used iPython notebook have noticed this. I know it was marked as a blocker but I think that was an inflated priority -- anyway let me know if you think it should go in for 1.0.1."}]}}
{"project": "SPARK", "issue_id": "SPARK-1467", "title": "Make StorageLevel.apply() factory methods developer API's", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Sandeep Singh", "labels": [], "created": "2014-04-10T22:20:51.000+0000", "updated": "2014-04-27T02:05:02.000+0000", "description": "We may want to evolve these in the future to add things like SSDs, so let's mark them as experimental for now. Long-term the right solution might be some kind of builder. The stable API should be the existing StorageLevel constants.", "comments": ["https://github.com/apache/spark/pull/551"], "derived": {"summary": "We may want to evolve these in the future to add things like SSDs, so let's mark them as experimental for now. Long-term the right solution might be some kind of builder.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Make StorageLevel.apply() factory methods developer API's - We may want to evolve these in the future to add things like SSDs, so let's mark them as experimental for now. Long-term the right solution might be some kind of builder."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/551"}]}}
{"project": "SPARK", "issue_id": "SPARK-1468", "title": "The hash method used by partitionBy in Pyspark doesn't deal with None correctly.", "status": "Resolved", "priority": "Major", "reporter": "Erik Selin", "assignee": "Erik Selin", "labels": [], "created": "2014-04-10T22:57:28.000+0000", "updated": "2014-06-03T20:35:33.000+0000", "description": "In python the default hash method uses the memory address of objects. Since None is an object None will get partitioned into different partitions depending on which python process it is run in. This causes some really odd results when None key's are used in the partitionBy.\n\nI've created a fix using a consistent hashing method that sends None to 0. That pr lives at https://github.com/apache/spark/pull/371", "comments": [], "derived": {"summary": "In python the default hash method uses the memory address of objects. Since None is an object None will get partitioned into different partitions depending on which python process it is run in.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "The hash method used by partitionBy in Pyspark doesn't deal with None correctly. - In python the default hash method uses the memory address of objects. Since None is an object None will get partitioned into different partitions depending on which python process it is run in."}]}}
{"project": "SPARK", "issue_id": "SPARK-1469", "title": "Scheduler mode should accept lower-case definitions and have nicer error messages", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Sandeep Singh", "labels": ["starter"], "created": "2014-04-10T23:26:47.000+0000", "updated": "2014-04-16T16:59:24.000+0000", "description": "I tried setting spark.scheduler.mode=fair and I got the following nasty exception:\n\n{code}\njava.util.NoSuchElementException: None.get\n\tat scala.None$.get(Option.scala:313)\n\tat scala.None$.get(Option.scala:311)\n\tat scala.Enumeration.withName(Enumeration.scala:132)\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.<init>(TaskSchedulerImpl.scala:101)\n\tat org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:1338)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:230)\n\tat org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:956)\n\tat $iwC$$iwC.<init>(<console>:8)\n\tat $iwC.<init>(<console>:14)\n\tat <init>(<console>:16)\n\tat .<init>(<console>:20)\n\tat .<clinit>(<console>)\n{code}\n\nWe should do two improvements:\n1. We should make the built in ones case insensitive (fair/FAIR, fifo/FIFO).\n2. If an invalid mode is given we should print a better error message.", "comments": ["https://github.com/apache/spark/pull/388"], "derived": {"summary": "I tried setting spark. scheduler.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Scheduler mode should accept lower-case definitions and have nicer error messages - I tried setting spark. scheduler."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/388"}]}}
{"project": "SPARK", "issue_id": "SPARK-1470", "title": "Use the scala-logging wrapper instead of the directly sfl4j api", "status": "Resolved", "priority": "Major", "reporter": "Guoqiang Li", "assignee": "Guoqiang Li", "labels": [], "created": "2014-04-11T02:07:26.000+0000", "updated": "2014-08-02T20:53:52.000+0000", "description": "Now the Spark Catalyst using scalalogging-slf4j, but the Spark Core to use slf4j-api\nWe should use the scalalogging-slf4 wrapper instead of the directly sfl4j-api", "comments": ["Closed in favor of SPARK-2804"], "derived": {"summary": "Now the Spark Catalyst using scalalogging-slf4j, but the Spark Core to use slf4j-api\nWe should use the scalalogging-slf4 wrapper instead of the directly sfl4j-api.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Use the scala-logging wrapper instead of the directly sfl4j api - Now the Spark Catalyst using scalalogging-slf4j, but the Spark Core to use slf4j-api\nWe should use the scalalogging-slf4 wrapper instead of the directly sfl4j-api."}, {"q": "What updates or decisions were made in the discussion?", "a": "Closed in favor of SPARK-2804"}]}}
{"project": "SPARK", "issue_id": "SPARK-1471", "title": "Worker not  recognize Driver state at standalone mode", "status": "Resolved", "priority": "Major", "reporter": "shenh062326", "assignee": null, "labels": [], "created": "2014-04-11T03:25:09.000+0000", "updated": "2014-06-18T12:20:09.000+0000", "description": "When I run a spark job in standalone,\n./bin/spark-class org.apache.spark.deploy.Client  launch spark://v125050024.bja:7077 file:///home/yuling.sh/spark-0.9.0-incubating/examples/target/spark-examples_2.10-0.9.0-incubating.jar org.apache.spark.examples.SparkPi\n\n Here is the Worker log.\n14/04/11 11:15:04 ERROR OneForOneStrategy: FAILED (of class scala.Enumeration$Val)\nscala.MatchError: FAILED (of class scala.Enumeration$Val)\n        at org.apache.spark.deploy.worker.Worker$$anonfun$receive$1.applyOrElse(Worker.scala:277)\n        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)\n        at akka.actor.ActorCell.invoke(ActorCell.scala:456)\n        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)\n        at akka.dispatch.Mailbox.run(Mailbox.scala:219)\n        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)\n        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n\n\n", "comments": ["The reason is when Worker receive DriverStateChanged, it only match  ERROR, FINISHED and KILLED. it should add a case like :case _ =>\n", "I see, the new FAILED status is introduced in https://github.com/apache/spark/commit/0f9d2ace6baefeacb1abf9d51a457644b67f2f8d, it should be a pretty easy fix", "Hello,\nI'm facing the same issue in version 1.0.0 (built from the sources distribution using {{make-distribution.sh --hadoop 2.0.0-cdh4.7.0}}).\n\nI'm running a job using the new {{bin/spark-submit}} script. When the job fails, one of the worker dies with the following error:\n\n{code}\n2014-06-17 17:00:04,675 [sparkWorker-akka.actor.default-dispatcher-3] ERROR akka.actor.OneForOneStrategy - FAILED (of class scala.Enumeration$Val)\nscala.MatchError: FAILED (of class scala.Enumeration$Val)\n        at org.apache.spark.deploy.worker.Worker$$anonfun$receive$1.applyOrElse(Worker.scala:317)\n        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)\n        at akka.actor.ActorCell.invoke(ActorCell.scala:456)\n        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)\n        at akka.dispatch.Mailbox.run(Mailbox.scala:219)\n        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)\n        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n{code}", "I will fix it right now", "this has been fixed by https://github.com/apache/spark/commit/95e4c9c6fb153b7f0aa4c442c4bdb6552d326640"], "derived": {"summary": "When I run a spark job in standalone,. /bin/spark-class org.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Worker not  recognize Driver state at standalone mode - When I run a spark job in standalone,. /bin/spark-class org."}, {"q": "What updates or decisions were made in the discussion?", "a": "this has been fixed by https://github.com/apache/spark/commit/95e4c9c6fb153b7f0aa4c442c4bdb6552d326640"}]}}
{"project": "SPARK", "issue_id": "SPARK-1472", "title": "Go through YARN api used in Spark to make sure we aren't using Private Apis", "status": "Resolved", "priority": "Major", "reporter": "Thomas Graves", "assignee": "Thomas Graves", "labels": [], "created": "2014-04-11T20:38:10.000+0000", "updated": "2016-01-08T12:04:09.000+0000", "description": "We need to look through all the yarn api's we are using to make sure they aren't now Private.  If they are private change to not use those api's.", "comments": ["So it looks like its currently impossible to use all public interfaces with Hadoop. There are some that are limitedprivate that we will have to use.\n\nI filed several jira in Hadoop land to add public interfaces for various things that we either need or would be handy for all types of applications. https://issues.apache.org/jira/browse/YARN-1953\nI should file a couple more jira also as things like UserGroupInformation is marked limitedprivate also.\n\nIn this jira I will clean up as much as possible mostly in the \"yarn stable\" code since that is where api's changed scope.", "[~tgraves] Is this as resolved as it can be? now that YARN alpha support is out too?", "No there are a bunch of others we can get rid of. I had started a patch a long time back but haven't had time to get back to it."], "derived": {"summary": "We need to look through all the yarn api's we are using to make sure they aren't now Private. If they are private change to not use those api's.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Go through YARN api used in Spark to make sure we aren't using Private Apis - We need to look through all the yarn api's we are using to make sure they aren't now Private. If they are private change to not use those api's."}, {"q": "What updates or decisions were made in the discussion?", "a": "No there are a bunch of others we can get rid of. I had started a patch a long time back but haven't had time to get back to it."}]}}
{"project": "SPARK", "issue_id": "SPARK-1473", "title": "Feature selection for high dimensional datasets", "status": "Resolved", "priority": "Minor", "reporter": "Ignacio Zendejas", "assignee": null, "labels": ["features"], "created": "2014-04-11T20:57:42.000+0000", "updated": "2015-11-16T09:51:05.000+0000", "description": "For classification tasks involving large feature spaces in the order of tens of thousands or higher (e.g., text classification with n-grams, where n > 1), it is often useful to rank and filter features that are irrelevant thereby reducing the feature space by at least one or two orders of magnitude without impacting performance on key evaluation metrics (accuracy/precision/recall).\n\nA feature evaluation interface which is flexible needs to be designed and at least two methods should be implemented with Information Gain being a priority as it has been shown to be amongst the most reliable.\n\nSpecial consideration should be taken in the design to account for wrapper methods (see research papers below) which are more practical for lower dimensional data.\n\nRelevant research:\n* Brown, G., Pocock, A., Zhao, M. J., & Lujn, M. (2012). Conditional\nlikelihood maximisation: a unifying framework for information theoretic\nfeature selection.*The Journal of Machine Learning Research*, *13*, 27-66.\n* Forman, George. \"An extensive empirical study of feature selection metrics for text classification.\" The Journal of machine learning research 3 (2003): 1289-1305.", "comments": ["MLlib already implements the Lasso (l1-regularized least squares regression), which is widely used for feature selection, and scales relatively well. Just as a baseline method to keep in mind when comparing quality and scalability.", "Thanks for pointing this out Martin. We'll definitely take this into consideration.", "I'm fairly new to Spark, and hopefully what follows isn't old news...\n\nFeature subsetting ought (imo) to be considered as part of a larger picture that involves various ETL-like tasks such as\n\n*) data assessment -- examining data columns to assess data types (real, integer, categorical/binary), identify noise in data (empty/missing values, bad values), suggest possible quantizations\n\n*) data quantization -- mapping values into byte encodings, sparse binary, etc\n\n*) dataset transposition -- moving from sample-wise to feature-wise orientation (e.g. decision tree training can work more efficiently when data can be traversed by feature)\n\n*) feature extraction, augmentation, reduction\n\nI don't yet have a strong feel for how these tasks should best work in spark, but in my previous lives I've found they are common and closely-integrated tasks when preparing for the care and feeding of ML models.\n\n", "I believe these types of thing were more the goals of the MLI and MLbase projects rather than MLlib? I don't know the status of those. For what it's worth I think these are very useful things but in a separate 'layer' above something like MLlib.", "+1 Info Gain / KL / Mutual Information, and all that, certainly are the most reliable means of feature selection.  This is due to entropy based measures having a strong justification in the form of characterization theorems.  I recommend \"Concepts in Statistical Mechanics\" - Hobson, \"The Mathematical Theory of Communication\", \"Information Theory\", \"An Uncertain Reasoners Companion\", and \"Elements of Information Theory\" for detailed proofs and generally good books.\n\nAnd unlike a lot of dimensionality reduction algorithms like PCA, Information Theoretic approaches are 1-pass, i.e. O(N).", "Does anybody work on this issue?", "I've implemented Chi-Squared and added a pull request https://github.com/apache/spark/pull/1484", "Hi you all,\n\nI am Dr. David Martinez and this is my first comment of this project. We implemented all feature selection methods included in\nBrown, G., Pocock, A., Zhao, M. J., & Lujn, M. (2012). Conditional\n likelihood maximisation: a unifying framework for information theoretic\n feature selection.The Journal of Machine Learning Research, 13, 27-66\n\nincluded more optimizations and left the framework open to include more criteria. We opened a pull request in the past but did not finished it. You can \nhave a look in our github\nhttps://github.com/LIDIAgroup/SparkFeatureSelection\nWe would like to finish our pull request", "Good paper, the theory is very solid. My only concern is that the paper does not explicitly tackle the problem of probability estimation for high dimensionality, which for sparse data will be even worse. It just touches on the problem, saying:\n\n\"This in turn causes increasingly poor judgements for the in- clusion/exclusion of features. For precisely this reason, the research community have developed various low-dimensional approximations to (9). In the following sections, we will investigate the implicit statistical assumptions and empirical effects of these approximations\"\n\nThose mentioned sections do not go into theoretical detail, and therefore I disagree that the paper provides a \"single unified information theoretic framework for feature selection\" as it basically leaves the problem of probability estimation to the readers choice, and merely suggests the reader assumes some level of independence between features in order to implement an algorithm.\n\n [~dmborque]  Do you know of any literature that does approach the problem of probability estimation in an information theoretic and philosophically justified way?? \n\nAnyway despite my concerns, this paper is still by far the best treatment of feature selection I have seen.", "[~dmmata@gmail.com]  mentioning also (i cant work which david is the one that posted above)", "Sorry for having my name incomplete when I first posted. I am David Martinez currently at\nUCL in London. We had this project abandoned for some time but we will restart a pull request\nshortly. You can see the current version of the code at https://github.com/LIDIAgroup/SparkFeatureSelection.\n\nIn response to a past post, the framework that Dr Gavin Brown presents in a single unified framework because it does\nnot make any assumptions when stating the basic probabilistic model for the problem of FS. The problem of probability estimation that he mentions is not a philosophical question and is not only a shortcoming of feature selection. The problem is that the number of parameters that you need to estimate is exponentially proportional to the number of variables if you do not make any independence assumption (all possible events). So, to have good estimations of these parameters (probabilities of events), you need an \nexponential number of samples (to observe all possible events you need and exponential number of observations). That is why you need to make independence assumptions and follow a greedy strategy to be able to draw some conclusions.", "[~torito1984] Thank you for the response, and apologies for my delay in responding.\n\nYes the problems of trying to estimate probabilities when independence assumptions are not made indeed make it necessary to consider some features independent.  My question is *how* should we do this? Is there any literature that has attempted to **formalize the way we introduce independence** in *information theoretic* terms.  Moreover I see this problem, and feature selection in general, as problems that are tightly coupled with the way probability estimation is performed.\n\nSuppose in the simplest case we wish to decide whether features F_1 and F_2 are dependent (we could consider arbitrary conjunctions too). Then the Information Theorist would want to consider the Mutual Information, i.e. the KL between the joint and product of marginals:\n\nKL( p(F_1, F_2) || p(F_1) * p(F_2) )\n\nThen use a threshold or rank on feature pairs to determine whether to consider them dependent. \n\nNow this is where we are tightly coupled with the means by which we estimate the probabilities p(F_1, F_2), p(F_1) and p(F_2).  We could use Maximum Liklihood with Laplace Smoothing, MAP / Regularization, etc, or the much lesser known Carnap's Continuum of Inductive Methods.  Which method we choose along with the usual arbitrary choice of some constant (e.g. alpha in Laplace/Additive Smoothing) will determine p(F_1, F_2), p(F_1) and p(F_2) and therefore determine whether or not F_1 & F_2 are to be considered dependent.\n\nThe current practice in Machine Learning has been to choose a method of estimation based off x-validation results rather than some deep philosophical justification.  Prof' Jeff Paris's work and his colleagues is the only work I've seen that attempts to use Information Theoretic principles to estimate probabilities.  Unfortunately the work is a little incomplete with regard to practical application.\n\nTo summarize, although I like the paper, especially it's principled approach (vs the \"just test and see\" commonly seen in Data Science), how independence is to be assumed (to solve the exponential sparsity problem) is left as arbitrary, and so is the choice of probability estimation, and therefore it is not fully principled nor fully foundational.\n\nPlease do not interpret this comment as a rejection/attack on the paper, rather I consider it a little incomplete and was hoping someone may have found a line of research more successful than my own to fill in the gaps.", "Hello, I am the first author of the paper being discussed.\n\nOur paper did indeed separate the two tasks of (1) estimating the probability distributions, and (2) the process/dynamics of selecting features once you have those probabilities.  So as Sam says, yes it is entirely possible that bad estimation of those probabilities could lead to bad choices of features.\n\nThe task of estimating those probabilities is an unsolved problem in general, but is known as \"entropy estimation\".  Sam rightly points out that you could use LaPlace, or other smoothing methods.  Which one will work best on any arbitrary dataset is unknown in general.  However many of these smoothing methods all perform identically once you get a reasonable number of datapoints per feature.  Small sample feature selection using information theoretic methods is an open problem - but one could use good heuristics like AIC or BIC to regularize.\n\nIn answer to the question by Sam - \"Is there any literature that has attempted to *formalize the way we introduce independence*\" ... no, our paper is the only one I know of.\n\nI think of these information theoretic filter methods as a way to *explore* large data. If you wanted to build the best possible classifier, with no limit on computation, you'd use a wrapper method around it.  Filter methods by nature assume independence between the feature selection stage and the classifier building, which is clearly false, but works well in practice with very large datasets that cannot have unlimited compute time.  If you want to explore data, a filter is a good heuristic - and the information theoretic ones are the most theoretically grounded \"heuristics\" I know of.\n\nHappy to answer further questions or give my perspectives.    Everybody - thanks for the attention to the paper - very happy to see it is useful.", "Sorry, one extra point I didn't notice before....\n\nSam said \"how independence is to be assumed (to solve the exponential sparsity problem) is left as arbitrary, and so is the choice of probability estimation\" .... that's not quite true.  The latter is, we did not specify how to estimate probabilities. However the former, we conducted a major empirical study, judging which independence assumptions had the best properties in terms of accuracy and stability with small/large samples, as well as which independence assumption permitted the most efficient implementations, across a wide range of data sets (26 sets if memory serves).\n\nOur conclusions were empirical, but really in this case there can be no other way. As we stated, we reached the limit of what is possible in theory terms, and had to continue with empirical studies. To identify what independence holds in any particular data set is to tackle the model fitting problem itself. Therefore all that can be done is to use the empirical study results, which recommend the JMI and CMIM criteria without a doubt. Certain other criteria (eg MRMR) we find are misleading and dangerous to use.", "[~gbrown@cs.man.ac.uk] Thanks for taking the time to respond to my questions, and I thank you again for writing the paper as I always enjoy reading foundational (i.e. information theoretic) approaches to Machine Learning.\n\nRegarding your final point about empiricism, yes this is better than \"arbitrary\" and so my original comment was too strong. I guess I was hoping for the same kind of foundational approach used to define the feature selection, and I am optimistic that there does exist a principled approach to how to define independence (which I think would also link with estimation).\n\nI notice that your email address indicates that you are at Manchester University (I must have overlooked this when reading the paper - typical mathematician).  This is where I learnt about Information Theory - in the maths department; Jeff Paris, George Wilmers, Vencovska, etc have all done sterling work.\n\nDo you ever come to London? Do you have any interest in applications? We have a Spark Meetup in London and it would be great if you could attend - much easier to share ideas in person. Perhaps yourself and [~torito1984] may even be willing to give a talk on \"Information Theoretic Feature Selection with Implementation in Spark\"?", "Dear Sam, \n\nThank you for the invitation. Funny enough, I am a usual at the meet ups and I have been already invited by Martin Goodson to do a talk about ... \"selected topics on ML in Big Data\". Currently I have a lab in Spain polishing the code and deploying it on a cluster to prove its performance (and support a future pull request). Dr. Brown has suggested me a couple of improvements using semi-supervised data. When we have solid results, at least on my side, I would love to share them with the community.", "Thanks Sam. I'm not due in London anytime soon, but as David says, perhaps he could deliver something - I could participate remotely.  I don't generally do applications, don't really have the engineering skillset that you guys have. But very very happy to see our paper has been noticed by the Spark community. As David says too - we have extensions of this work now - for missing and semi supervised data problems.", "User 'sramirez' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/5169", "Hi all,\n\nThe related link SPARK-6531 has been unassigned for a long time. Any clue about that?\n\nThanks", "I'm resolving this umbrella as \"wont fix\" but leaving the child open for a bit. It doesn't look like this is going to proceed though."], "derived": {"summary": "For classification tasks involving large feature spaces in the order of tens of thousands or higher (e. g.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Feature selection for high dimensional datasets - For classification tasks involving large feature spaces in the order of tens of thousands or higher (e. g."}, {"q": "What updates or decisions were made in the discussion?", "a": "I'm resolving this umbrella as \"wont fix\" but leaving the child open for a bit. It doesn't look like this is going to proceed though."}]}}
{"project": "SPARK", "issue_id": "SPARK-1474", "title": "Spark on yarn assembly doesn't include org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter", "status": "Resolved", "priority": "Major", "reporter": "Xuan Gong", "assignee": "Thomas Graves", "labels": [], "created": "2014-04-11T22:19:30.000+0000", "updated": "2014-05-06T19:00:35.000+0000", "description": "part of the error logs \n{code}\n14/04/11 15:08:06 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n14/04/11 15:08:06 WARN Holder: \njava.lang.ClassNotFoundException: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:202)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:190)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:306)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:247)\n\tat org.eclipse.jetty.util.Loader.loadClass(Loader.java:100)\n\tat org.eclipse.jetty.util.Loader.loadClass(Loader.java:79)\n\tat org.eclipse.jetty.servlet.Holder.doStart(Holder.java:107)\n\tat org.eclipse.jetty.servlet.FilterHolder.doStart(FilterHolder.java:90)\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)\n\tat org.eclipse.jetty.servlet.ServletHandler.initialize(ServletHandler.java:768)\n\tat org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:265)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:717)\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)\n\tat org.eclipse.jetty.server.handler.HandlerCollection.doStart(HandlerCollection.java:229)\n\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.doStart(ContextHandlerCollection.java:172)\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.doStart(HandlerWrapper.java:95)\n\tat org.eclipse.jetty.server.Server.doStart(Server.java:282)\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)\n\tat org.apache.spark.ui.JettyUtils$$anonfun$1.apply$mcV$sp(JettyUtils.scala:189)\n\tat org.apache.spark.ui.JettyUtils$$anonfun$1.apply(JettyUtils.scala:189)\n\tat org.apache.spark.ui.JettyUtils$$anonfun$1.apply(JettyUtils.scala:189)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat org.apache.spark.ui.JettyUtils$.connect$1(JettyUtils.scala:188)\n\tat org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:201)\n\tat org.apache.spark.ui.SparkUI.bind(SparkUI.scala:101)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:215)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:110)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:147)\n\tat org.apache.spark.examples.SparkPi$.main(SparkPi.scala:31)\n\tat org.apache.spark.examples.SparkPi.main(SparkPi.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2$$anonfun$run$1.apply$mcV$sp(ApplicationMaster.scala:184)\n\tat org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:43)\n\tat org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:394)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)\n\tat org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:42)\n\tat org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:178)\n14/04/11 15:08:06 WARN AbstractLifeCycle: FAILED org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-364e50ee: javax.servlet.UnavailableException: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\njavax.servlet.UnavailableException: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n\tat org.eclipse.jetty.servlet.Holder.doStart(Holder.java:114)\n\tat org.eclipse.jetty.servlet.FilterHolder.doStart(FilterHolder.java:90)\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)\n\tat org.eclipse.jetty.servlet.ServletHandler.initialize(ServletHandler.java:768)\n\tat org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:265)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:717)\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)\n\tat org.eclipse.jetty.server.handler.HandlerCollection.doStart(HandlerCollection.java:229)\n\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.doStart(ContextHandlerCollection.java:172)\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.doStart(HandlerWrapper.java:95)\n\tat org.eclipse.jetty.server.Server.doStart(Server.java:282)\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)\n\tat org.apache.spark.ui.JettyUtils$$anonfun$1.apply$mcV$sp(JettyUtils.scala:189)\n\tat org.apache.spark.ui.JettyUtils$$anonfun$1.apply(JettyUtils.scala:189)\n\tat org.apache.spark.ui.JettyUtils$$anonfun$1.apply(JettyUtils.scala:189)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat org.apache.spark.ui.JettyUtils$.connect$1(JettyUtils.scala:188)\n\tat org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:201)\n\tat org.apache.spark.ui.SparkUI.bind(SparkUI.scala:101)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:215)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:110)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:147)\n\tat org.apache.spark.examples.SparkPi$.main(SparkPi.scala:31)\n\tat org.apache.spark.examples.SparkPi.main(SparkPi.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2$$anonfun$run$1.apply$mcV$sp(ApplicationMaster.scala:184)\n\tat org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:43)\n\tat org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:394)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)\n\tat org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:42)\n\tat org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:178)\n{code}", "comments": ["use mvn to Compile the spark code from spark master branch with hadoop-2.2.0:\n{code}\nmvn -Pyarn -Dhadoop.version=2.2.0 -Dyarn.version=2.2.0 -DskipTests clean package\n{code}\n\nAnd launch spark application with yarn-standalone mode \n{code}\nSPARK_JAR=assembly/target/scala-2.10/spark-assembly_2.10-1.0.0-SNAPSHOT-hadoop2.2.0.jar ./bin/spark-class org.apache.spark.deploy.yarn.Client --jar examples/target/scala-2.10/spark-examples_2.10-assembly-1.0.0-SNAPSHOT.jar --class org.apache.spark.examples.SparkPi --arg yarn-standalone --num-executors 3 --driver-memory 512m --executor-memory 512m --executor-cores 1\n{code}\n", "Verified that spark application *works* in yarn-client mode.\n{code}\nSPARK_JAR=assembly/target/scala-2.10/spark-assembly_2.10-1.0.0-SNAPSHOT-hadoop2.2.0.jar SPARK_YARN_APP=examples/target/scala-2.10/spark-examples_2.10-assembly-1.0.0-SNAPSHOT.jar ./bin/run-example org.apache.spark.examples.SparkPi yarn-client\n{code}", "are you including the yarn jars in the classpath when launching on yarn?  \n\nThe error is ClassNotFoundException: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.\n\nThe yarn-client mode doesn't hook into the yarn UI filter since its running on the client and not on the cluster which is why it works there.\n\nWe may want to change it so the assembly includes that class also.", "[~tgraves]\nI think so, I did add YARN_HOME into class path\n{code}\nYARN_HOME=\"/Users/xuan/dep/hadoop-yarn-project-2.4.1-SNAPSHOT/share/hadoop/yarn\"\n{code} which will includes all the yarn jars", "do you have a /* at the end to pick up the jars?  Where are you setting that?\n\nYARN has the default classpath set for all applications by the config: yarn.application.classpath.  That generally default to including all the yarn/mapreduce/hdfs stuff so I would expect this to work by default.  \n\nNote that it is a bug that we don't package it in the assembly so fixing that, but you should be able to work around just by adding it to the classpath.", "Thanks, [~tgraves]. \nI messed up the configurations....", "https://github.com/apache/spark/pull/406", "Issue resolved by pull request 406\n[https://github.com/apache/spark/pull/406]"], "derived": {"summary": "part of the error logs \n{code}\n14/04/11 15:08:06 INFO JettyUtils: Adding filter: org. apache.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Spark on yarn assembly doesn't include org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter - part of the error logs \n{code}\n14/04/11 15:08:06 INFO JettyUtils: Adding filter: org. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 406\n[https://github.com/apache/spark/pull/406]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1475", "title": "Drain event logging queue before stopping event logger", "status": "Resolved", "priority": "Blocker", "reporter": "Kan Zhang", "assignee": "Kan Zhang", "labels": [], "created": "2014-04-11T23:00:28.000+0000", "updated": "2014-09-22T17:27:00.000+0000", "description": "When stopping SparkListenerBus, its event queue needs to be drained. And this needs to happen before event logger is stopped. Otherwise, any event still waiting to be processed in the queue may be lost and consequently event log file may be incomplete. ", "comments": ["When event queue is not drained, users may observe similar issues as those reported in SPARK-1407 (when sc.stop() is not called). \n\nhttps://github.com/apache/spark/pull/366\n\nThe above PR fixes this issue. It does require applications to call sc.stop() to properly stop SparkListenerBus and event logger.", "A second PR that fixes the unit test introduced above.\n\nhttps://github.com/apache/spark/pull/401"], "derived": {"summary": "When stopping SparkListenerBus, its event queue needs to be drained. And this needs to happen before event logger is stopped.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Drain event logging queue before stopping event logger - When stopping SparkListenerBus, its event queue needs to be drained. And this needs to happen before event logger is stopped."}, {"q": "What updates or decisions were made in the discussion?", "a": "A second PR that fixes the unit test introduced above.\n\nhttps://github.com/apache/spark/pull/401"}]}}
{"project": "SPARK", "issue_id": "SPARK-1476", "title": "2GB limit in spark for blocks", "status": "Closed", "priority": "Critical", "reporter": "Mridul Muralidharan", "assignee": null, "labels": [], "created": "2014-04-12T06:29:26.000+0000", "updated": "2019-12-18T02:38:18.000+0000", "description": "The underlying abstraction for blocks in spark is a ByteBuffer : which limits the size of the block to 2GB.\nThis has implication not just for managed blocks in use, but also for shuffle blocks (memory mapped blocks are limited to 2gig, even though the api allows for long), ser-deser via byte array backed outstreams (SPARK-1391), etc.\n\nThis is a severe limitation for use of spark when used on non trivial datasets.", "comments": ["WIP Proposal:\n\n- All references to ByteBuffer will need to be replaced with Seq[ByteBuffer].\n    This applies to definition of a block, memory mapped file segments for a shuffle block, etc.\n- All use of byte array backed outputstream will need to be replaced with a aggregating outputstream which writes to multiple boas as and when array limits are hit.\n", "This says it's a \"severe limitation\" - but why not just use more, smaller blocks? I think Spark's design assumes in various places that block's are not extremely large. Think of it like e.g. the HDFS block size... it can't be arbitrary large. The answer here might be to use multiple blocks in the case of something like a shuffle where the size can get really large.", "I agree, would be good to understand what kind of operations this arises in. Do you have cached RDD partitions that are this large, or is it shuffle blocks? Is it skew in the shuffle data?\n\nThe main concern I see with this is that it would complicate the deserializer and block sender code paths, but maybe it's worth it.", "There are multiple issues at play here :\n\na) If a block goes beyond 2G, everything fails - this is the case for shuffle, cached and 'normal' blocks.\nIrrespective of storage level and/or other config.\nIn a lot of cases, particularly when data generated 'increases' after a map/flatMap/etc, the user has no control over the size increase in the output block for a given input block (think iterations over datasets).\n\nb) Increasing number of partitions is not always an option (for the subset of usecases where it can be done) :\n1) It has an impact on number of intermediate files created while doing a shuffle. (ulimit constraints, IO performance issues, etc).\n2) It does not help when there is skew anyway.\n\nc) Compression codec, serializer used, etc have an impact.\n\nd) 2G is extremely low limit to have in modern hardware : and this is particularly a severe limitation when we have nodes running on 32G to 64G ram and TB's of disk space available for spark.\n\n\nTo address specific points raised above :\n\nA) [~pwendell] Mapreduce jobs dont fail in case the block size of files increases - it might be inefficient, it still runs (not that I know of any case where it does actually, but theoretically I guess it can become inefficient).\nSo analogy does not apply.\nAlso to add, 2G is not really an unlimited increase in block size - and in MR, output of a map can easily go a couple of orders above 2G : whether it is followed by a reduce or not.\n\nB) [~matei] In the specific cases it was failing, the users were not caching the data but directly going to shuffle.\nThere was no skew from what I see : just the data size per key is high; and there are a lot of keys too btw (as iterations increase and nnz increases).\nNote that it was an impl detail that it was not being cached - it could have been too.\nAdditionally, compression and/or serialization also apply implicitly in this case, since it was impacting shuffle - the 2G limit was observed at both the map and reduce side (in two different jobs).\n\n\nIn general, our effort is to make spark as a drop in replacement for most usecases which are currently being done via MR/Pig/etc.\nLimitations of this sort make it difficult to position spark as a credible alternative.\n\n\nCurrent approach we are exploring is to remove all direct references to ByteBuffer from spark (except for ConnectionManager, etc parts); and rely on a BlockData or similar datastructure which encapsulate the data corresponding to a block. By default, a single ByteBuffer should suffice but in case it does not, the class will automatically take care of splitting across blocks.\nSimilarly, all references to byte array backed streams will need to be replaced with a wrapper stream which multiplexes over byte array streams.\nThe performance impact for all 'normal' usecases should be the minimal, while allowing for spark to be used in cases where 2G limit is being hit.\n\nThe only unknown here is tachyon integration : where the interface is a ByteBuffer - and I am not knowledgable enough to comment on what the issues there would be.", "[~mridulm@yahoo-inc.com] I think the proposed change would benefit from a design doc to explain exactly the cases we want to fix and what trade-offs we are willing to make in terms of complexity.\n\nAgreed that there is definitely room for improvement in the out-of-the-box behavior here.\n\nRight now the limits as I understand them are (a) the shuffle output from one mapper to one reducer cannot be more than 2GB. (b) partitions of an RDD cannot exceed 2GB.\n\nI see (a) as the bigger of the two issues. It would be helpful to have specific examples of workloads where this causes a problem and the associated data sizes, etc. For instance, say I want to do a 1 Terabyte shuffle. Right now number of (mappers * reducers) needs to be > ~1000 for this to work (e.g. 100 mappers and 10 reducers) assuming uniform partitioning. That doesn't seem too crazy of an assumption, but if you have skew this would be a much bigger problem. \n\nWould it be possible to improve (a) but not (b) with a much simpler design? I'm not sure (maybe they reduce to the same problem), but it's something a design doc could help flesh out.\n\nPopping up a bit - I think our goal should be to handle reasonable workloads and not to be 100% compliant with the semantics of Hadoop MapReduce. After all, in-memory RDD's are not even a concept in MapReduce. And keep in mind that MapReduce became so bloated/complex of a project that it is today no longer possible to make substantial changes to it. That's something we definitely want to avoid by keeping Spark internals as simple as possible.", "Hey Mridul, the one thing I'd add as an alternative is whether we could have splitting happen at a higher level than the block manager. For example, maybe a map task is allowed to create 2 output blocks for a given reducer, or maybe a cached RDD partition gets stored as 2 blocks. This might be slightly easier to implement than replacing all instances of ByteBuffers. But I agree that this should be addressed somehow, since 2 GB will become more and more limiting over time. Anyway, I'd love to see a more detailed design. I think even the replace-ByteBuffers approach you proposed can be made to work with Tachyon.", "[~pwendell] IMO both are limitations (shuffle block < 2G vs any rdd block < 2G) though the former is slightly more trickier to work around at times (when there is skew, etc).\nNote that trying to work around latter will mean more partitions which also has an impact on functionality and shuffle performance (ulimit limits enforced will mean job fails again).\nIdeally though, I would prefer if we did not need to work around these issues actually.\n\nThis is one of two jira's we want to work on for the next release - the other being improving shuffle performance, particularly in context of number of files which are created and/or opened.\nSince that is a more involved work, [~tgraves] and I will file that later.\n\n\nAgree about need for a design document - unfortunately, we are flying blind right now since we dont know what all will need to change (tachyon integration I mentioned above was something we hit recently). Basic idea is known - but not sure what other things this will impact.\nCurrent plan is to do a POC, iron out the issues seen and run it on one of the jobs which is failing right now : address performance and/or functionality issues seen - and then submit a PR based on the conclusions.\nI filed this JIRA to get basic inputs from other developers and/or users; and to use it as a sounding board in case we hit particularly thorny issues we cant resolve due to lack of sufficient context - crowdsourcing design/implementation issues :-) \n\n[~matei] Interesting that you should mention about splitting output of a map into multiple blocks.\nWe are actually thinking about that in a different context - akin to MultiOutputs in hadoop or SPLIT in pig : without needing to cache the intermediate output; but directly emit values to different blocks/rdd's based on the output of a map or some such.\nIn the context of splitting the output into multiple blocks : I was not so sure - particularly given the need for compression, custom serialization, etc.\nAlso, we have degenerate cases where the value for a key actually becomes > 2G (list of sparse vectors which become denser as iterations increase, etc)", "Okay sounds good - a POC like that would be really helpful. We've run some very large shuffles recently and couldn't isolate any problems except for SPARK-1239, which should only be a small change.\n\nIf there are some smaller fixes you run into then by all means submit them directly. If it requires large architectural changes I'd recommend having a design doc before submitting a pull request, because people will want to discuss the overall approach. I.e. we should avoid being \"break-fix\" and think about the long term design implications of changes.", "\n[~matei] We are having some issue porting the netty shuffle copier code to support > 2G since only ByteBuf seems to be exposed.\nBefore I dig into netty more, wanted to know if you or someone else from among spark developers knew how to add support for large buffers in our netty code. Thanks !", "Proposal detailing the work we have done on this effort.\nLooking forward to feedback before a PR is submitted based on this..", "[~mridulm80] can you post an update on this?\n\nI think it is a great idea to provide a buffer abstraction that can be backed by various buffer implementations (nio.ByteBuffer, Netty ByteBuf, on disk file region). I would like to make this happen for 1.2.\n", "Based on discussions we had with others, apparently 1.1 was not a good vehicle for this proposal.\nFurther, since there was no interest in this jira/comments on the proposal, we put the effort on the backburner.\n\nWe plan to push atleast some of the bugs fixed as part of this effort - consolidated shuffle did get resolved in 1.1 and probably a few more might be contributed back in 1.2 time permitting (disk backed map output tracking for example looks like a good candidate).\nBut bulk of the change is pervasive and at times a bit invasive and at odds with some of the other changes (for example, zero-copy); shepherding it might be a bit time consuming for me given other deliverables.\n\nIf there is renewed interest in this to get it integrated into a spark release, I can try to push for it to be resurrected and submitted.", "Let's work together to get something for 1.2 or 1.3.  At the very least, I would like to have a buffer abstraction layer that can support this in the future.", "WIP version pushed to https://github.com/mridulm/spark/tree/2g_fix - about 2 weeks before feature freeze in 1.1 iirc. \n\nNote that the 2g fixes are functionally complete, but this branch also includes a large number of other fixes.\nSome of these have been pushed to master; while others have not yet done : for alleviating memory pressure primarily, and fixing resource leaks.\n\nThis branch has been shared for reference purpose - and is not meant to be actively worked on for merging into master.\nWe will need to cherry pick the changes and do that manually.", "Based on discussion on the dev list, [~mridulm80] isn't actively working on this.   I'd like to start on it, with the following very minimal goals:\n\n1. Make it *possible* for blocks to be bigger than 2GB\n2. Maintain performance on smaller blocks\n\nie., I'm not going to try to do anything fancy to optimize performance of the large blocks.  To that end, my plan is to\n\n1. create a {{LargeByteBuffer}} interface, which just has the same methods we use on {{ByteBuffer}}\n2. have one implementation that just wraps one {{ByteBuffer}}, and another which wraps a completely static set of {{ByteBuffer}}s (eg., if you map a 3 GB file, it'll just immediately map it to 2 {{ByteBuffer}}s, nothing fancy with only mapping the first half of the file until the second is needed etc. etc.)\n3. change {{ByteBuffer}} to {{LargeByteBuffer}} in {{ShuffleBlockManager}} and {{BlockStore}}\n\nI see that about a year back there was a lot of discussion on this, and some alternate proposals.  I'd like to push forward with a POC to try to move the discussion along again.  I know there was some discussion about how important this is, and whether or not we want to support it.  IMO this is a big limitation and results in a lot of frustration for the users, we really need a solution for this.", "Hi [~irashid],\n\nApproach sounds good. It would be nice to measure whether the optimization for smaller blocks actually makes a difference; from what I can tell, supporting multiple ByteBuffer instances just means having an array and picking the right ByteBuffer based on an offset, both of which should be pretty cheap.", "I spent a little time with [~sandyr] on this today, and I realized that the shuffle limit and the cache limit are actually quite distinct.  (Sorry if this was already obvious to everyone else.)  I've made another issue SPARK-5928 to deal w/ the shuffle issue.  Then I say we make SPARK-1391 focus more on the cache limit (and broadcast limit etc.).  I'm going to make this issue require both of those.\n\nI'm going to pursue a solution to *only* SPARK-1391 (basically what I outlined above), I'll move further discussion of the particular of what I'm doing over there.", "we have a lot of jira about the 2G limit.  I'm going to dup this to the umbrella jira https://issues.apache.org/jira/browse/SPARK-6235\n\nIf someone things something is missing from that, lets add another item there.", "This is an old chain that I happen to land on to. I am interested in the following points mentioned by [~mridulm80]. Did anyone ever get to implementing MultiOutputs map without needing to use cache? If not, can anyone give me a pointer on how to get started.\r\n\r\n_\"_[~matei]_Interesting that you should mention about splitting output of a map into multiple blocks__._\r\n\r\n_We are actually thinking about that in a different context - akin to MultiOutputs in hadoop or SPLIT in pig : without needing to cache the intermediate output; but directly emit values to different blocks/rdd's based on the output of a map or some such.\"_\r\n\r\n"], "derived": {"summary": "The underlying abstraction for blocks in spark is a ByteBuffer : which limits the size of the block to 2GB. This has implication not just for managed blocks in use, but also for shuffle blocks (memory mapped blocks are limited to 2gig, even though the api allows for long), ser-deser via byte array backed outstreams (SPARK-1391), etc.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "2GB limit in spark for blocks - The underlying abstraction for blocks in spark is a ByteBuffer : which limits the size of the block to 2GB. This has implication not just for managed blocks in use, but also for shuffle blocks (memory mapped blocks are limited to 2gig, even though the api allows for long), ser-deser via byte array backed outstreams (SPARK-1391), etc."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is an old chain that I happen to land on to. I am interested in the following points mentioned by [~mridulm80]. Did anyone ever get to implementing MultiOutputs map without needing to use cache? If not, can anyone give me a pointer on how to get started.\r\n\r\n_\"_[~matei]_Interesting that you should mention about splitting output of a map into multiple blocks__._\r\n\r\n_We are actually thinking about that in a different context - akin to MultiOutputs in hadoop or SPLIT in pig : without needing to cache the intermediate output; but directly emit values to different blocks/rdd's based on the output of a map or some such.\"_"}]}}
{"project": "SPARK", "issue_id": "SPARK-1477", "title": "Add the lifecycle interface", "status": "Resolved", "priority": "Major", "reporter": "Guoqiang Li", "assignee": "Guoqiang Li", "labels": [], "created": "2014-04-12T12:22:46.000+0000", "updated": "2014-09-18T17:34:35.000+0000", "description": "Now the Spark in the code, there are a lot of interface or class  defines the stop and start method,eg:[SchedulerBackend|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/SchedulerBackend.scala],[HttpServer|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/HttpServer.scala],[ContextCleaner|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/ContextCleaner.scala] . we should use a life cycle interface improve the code", "comments": ["Retargeting this to 1.2.0.", "Unless we are planning to interact with these components in a generic way, there is not a good reason to add this interface (see relevant discussion in github)."], "derived": {"summary": "Now the Spark in the code, there are a lot of interface or class  defines the stop and start method,eg:[SchedulerBackend|https://github. com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/SchedulerBackend.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add the lifecycle interface - Now the Spark in the code, there are a lot of interface or class  defines the stop and start method,eg:[SchedulerBackend|https://github. com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/SchedulerBackend."}, {"q": "What updates or decisions were made in the discussion?", "a": "Unless we are planning to interact with these components in a generic way, there is not a good reason to add this interface (see relevant discussion in github)."}]}}
{"project": "SPARK", "issue_id": "SPARK-1478", "title": "Upgrade FlumeInputDStream's FlumeReceiver to support FLUME-1915", "status": "Resolved", "priority": "Minor", "reporter": "Theodore michael Malaska", "assignee": "Theodore michael Malaska", "labels": [], "created": "2014-04-12T12:57:34.000+0000", "updated": "2014-07-31T00:15:08.000+0000", "description": "Flume-1915 added support for compression over the wire from avro sink to avro source.  I would like to add this functionality to the FlumeReceiver.\n\n", "comments": ["After reviewing the code and I found that Spark Streaming is focused on Flume 1.2.0 and Avro-ipc 1.6.3. \n\nCloudera, Hortenworks, and even MapR are using Flume 1.4.0 in their current releases and that version of Flume uses  Avro-ipc 1.7.3 which allows for us to apply our own ChannelPipeLine, which allows us to use compression and encryption.\n\nI don't think I can complete do this Jira until I get permission to up the versions of Flume in the pom file from 1.2.0 to at least 1.3.0.  Flume version 1.3 is more then 2 years old so that should be ok for Spark Users and Flume 1.3.0 uses Avro-ipc 1.7.3, which is what we need.\n\nAs for now, I will continue as if I had the permission, and I will do a pull request with the change from Flume from 1.2.0 to 1.3.0. \n\nThanks\n\n", "I sent the initial patch.  Here is the link.\n\nhttps://github.com/apache/spark/pull/405/files\n\nLet me know what I need to do next.  This is my first time.", "Chated with tdas.  He reviewed the code, but he has a patch coming with over lapping changes.  \n\nI will wait until he submits that patch then I will resync my changes and resubmit.\n\nThanks again.", "Spark-1584 is done and so is PR #300.  So final we are ready for this Jira.  I will start development today.", "Having some issues\n\n\"Executor has not been attached to this receiver\" error being thrown.  Looks like new code added to Spark.  Going to see how to populate this now.", "I re cloned the code and ran a test.  There is a bug with the current Github branch.  I'm looking into it now.", "never mind it is working now.  ", "Thats definitely new code. Came in PR #300. What was the situation that led to that exception? I find it alarming, that should never happen normally!", "No worries that error was caused by me.  Still learning Scala.  It was the difference between using a lazy val and a var.\n\nI have all three test cases working now and I will do one last review before submitting it tomorrow.\n\nNow there is also one more odd thing going on that I haven't figured out yet.  Sometime (seeming randomly) my tests will fail with the following exception:\n\n[info] - flume input stream *** FAILED *** (10 seconds, 332 milliseconds)\n[info]   0 did not equal 1 (FlumeStreamSuite.scala:104)\n[info]   org.scalatest.exceptions.TestFailedException:\n\nThen I will rerun the test with no code changes and they will success.  It feels very much like a race condition.  Note I found this so odd that I did a fresh git clone and tested the latest branch and I also was able to get the exception.\n\nI will look into this tomorrow.  I would assume at this point that something is odd in my environment until I find evidence of it being anything else.\n\nThank you again for the help.", "Haha, yeah, lazy vals are super useful in difficult situations but can lead to difficult situations themselves if not careful. :)\n\nI am not sure what the flakiness is coming from, but that really needs to be solved. Flakiness can really be a major headache in our automated tests in Jenkins, etc. Suffering from flakiness myself in two PRs. :(\n\nLet me know how I can help in this. ", "OK I submitted the pull request: https://github.com/apache/spark/pull/566\n\nAlso I was able to recreate the race connection every 4-5 times I ran the tests.  It felt like it would happen when my compute was busy doing other things.  \n\nI reviewed the Flume test code and didn't see anything majorly different then the Spark version.  I did add an additional 1000 wait and I was unable to get the exception after that.  I know that that isn't a solution but at least it stopped the problem for now.\n\n", "OK I have made the changes requested.  But I had to do it in a different pull request.  Here is the new pull request link\n\nhttps://github.com/apache/spark/pull/1168", "After much hoops, this was the PR that merged this feature.\n\nhttps://github.com/apache/spark/pull/1347"], "derived": {"summary": "Flume-1915 added support for compression over the wire from avro sink to avro source. I would like to add this functionality to the FlumeReceiver.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Upgrade FlumeInputDStream's FlumeReceiver to support FLUME-1915 - Flume-1915 added support for compression over the wire from avro sink to avro source. I would like to add this functionality to the FlumeReceiver."}, {"q": "What updates or decisions were made in the discussion?", "a": "After much hoops, this was the PR that merged this feature.\n\nhttps://github.com/apache/spark/pull/1347"}]}}
{"project": "SPARK", "issue_id": "SPARK-1479", "title": "building spark on 2.0.0-cdh4.4.0 failed", "status": "Resolved", "priority": "Major", "reporter": "jackielihf", "assignee": null, "labels": [], "created": "2014-04-13T03:05:03.000+0000", "updated": "2014-10-16T08:51:40.000+0000", "description": "[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal net.alchim31.maven:scala-maven-plugin:3.1.5:compile (scala-compile-first) on project spark-yarn-alpha_2.10: Execution scala-compile-first of goal net.alchim31.maven:scala-maven-plugin:3.1.5:compile failed. CompileFailed -> [Help 1]\norg.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal net.alchim31.maven:scala-maven-plugin:3.1.5:compile (scala-compile-first) on project spark-yarn-alpha_2.10: Execution scala-compile-first of goal net.alchim31.maven:scala-maven-plugin:3.1.5:compile failed.\n\tat org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:225)\n\tat org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)\n\tat org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)\n\tat org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:84)\n\tat org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:59)\n\tat org.apache.maven.lifecycle.internal.LifecycleStarter.singleThreadedBuild(LifecycleStarter.java:183)\n\tat org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:161)\n\tat org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:320)\n\tat org.apache.maven.DefaultMaven.execute(DefaultMaven.java:156)\n\tat org.apache.maven.cli.MavenCli.execute(MavenCli.java:537)\n\tat org.apache.maven.cli.MavenCli.doMain(MavenCli.java:196)\n\tat org.apache.maven.cli.MavenCli.main(MavenCli.java:141)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:290)\n\tat org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:230)\n\tat org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:409)\n\tat org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:352)\nCaused by: org.apache.maven.plugin.PluginExecutionException: Execution scala-compile-first of goal net.alchim31.maven:scala-maven-plugin:3.1.5:compile failed.\n\tat org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:110)\n\tat org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:209)\n\t... 19 more\nCaused by: Compilation failed\n\tat sbt.compiler.AnalyzingCompiler.call(AnalyzingCompiler.scala:76)\n\tat sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:35)\n\tat sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:29)\n\tat sbt.compiler.AggressiveCompile$$anonfun$4$$anonfun$compileScala$1$1.apply$mcV$sp(AggressiveCompile.scala:71)\n\tat sbt.compiler.AggressiveCompile$$anonfun$4$$anonfun$compileScala$1$1.apply(AggressiveCompile.scala:71)\n\tat sbt.compiler.AggressiveCompile$$anonfun$4$$anonfun$compileScala$1$1.apply(AggressiveCompile.scala:71)\n\tat sbt.compiler.AggressiveCompile.sbt$compiler$AggressiveCompile$$timed(AggressiveCompile.scala:101)\n\tat sbt.compiler.AggressiveCompile$$anonfun$4.compileScala$1(AggressiveCompile.scala:70)\n\tat sbt.compiler.AggressiveCompile$$anonfun$4.apply(AggressiveCompile.scala:88)\n\tat sbt.compiler.AggressiveCompile$$anonfun$4.apply(AggressiveCompile.scala:60)\n\tat sbt.inc.IncrementalCompile$$anonfun$doCompile$1.apply(Compile.scala:24)\n\tat sbt.inc.IncrementalCompile$$anonfun$doCompile$1.apply(Compile.scala:22)\n\tat sbt.inc.Incremental$.cycle(Incremental.scala:40)\n\tat sbt.inc.Incremental$.compile(Incremental.scala:25)\n\tat sbt.inc.IncrementalCompile$.apply(Compile.scala:20)\n\tat sbt.compiler.AggressiveCompile.compile2(AggressiveCompile.scala:96)\n\tat sbt.compiler.AggressiveCompile.compile1(AggressiveCompile.scala:44)\n\tat com.typesafe.zinc.Compiler.compile(Compiler.scala:158)\n\tat com.typesafe.zinc.Compiler.compile(Compiler.scala:142)\n\tat sbt_inc.SbtIncrementalCompiler.compile(SbtIncrementalCompiler.java:77)\n\tat scala_maven.ScalaCompilerSupport.incrementalCompile(ScalaCompilerSupport.java:296)\n\tat scala_maven.ScalaCompilerSupport.compile(ScalaCompilerSupport.java:123)\n\tat scala_maven.ScalaCompilerSupport.doExecute(ScalaCompilerSupport.java:104)\n\tat scala_maven.ScalaMojoSupport.execute(ScalaMojoSupport.java:460)\n\tat org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:101)\n\t... 20 more\n[ERROR] \n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR] \n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException\n[ERROR] \n[ERROR] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn <goals> -rf :spark-yarn-alpha_2.10\n\n=============\nthks\n", "comments": ["You can give all of the logs?\n{code}\nmvn -Pyarn-alpha -Dhadoop.version=2.0.0-cdh4.4.0 -Dyarn.version=2.0.0-chd4.4.0 -DskipTests clean package -X > mvn.log\n{code}", "export MAVEN_OPTS=\"-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m\"\n\nmvn -Pyarn-alpha -Dhadoop.version=2.0.0-cdh4.4.0 -Dyarn.version=2.0.0-cdh4.4.0 -DskipTests clean package", "CDH4.4.0 yarn api has changed .Right now Spark doesn't support cdh4.4,cdh4.5,cdh4. However            Spark support cdh4.3  ", "\"yarn\" is the slightly more appropriate profile, but, read: https://github.com/apache/spark/pull/151\nWhat Spark doesn't quite support is \"YARN beta\" and that's what you've got on your hands here.\n\nFWIW I am in favor of the change in this PR to make it all work. Soon, support for YARN alpha/beta can just go away anyway.\n\nIf you are interested in CDH, the best thing is moving to CDH5, which already has Spark set up in standalone mode, and which has \"YARN stable\". It also works with CDH 4.6 in standalone mode as a parcel.", "yes, long-term solution should be migrating to stable api or CDH5(stable version just come out two weeks ago) or hadoop 2.3.\nfor many production environments, IMO, this PR make sense.", "What's the status about this issue, do we have decision whether to accept the PR, or have a timeline for totally removing all support of un-stable yarn api?", "Given discussion in SPARK-3445, I doubt anything more will be done for YARN alpha support, as it's on its way out.", " sh make-distribution.sh --tgz -Phadoop-provided -Pyarn -DskipTests -Dhadoop.version=2.3.0-cdh5.0.0 -Phive -Dhive.verison=0.13.1\n\nget the same error output"], "derived": {"summary": "[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal net. alchim31.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "building spark on 2.0.0-cdh4.4.0 failed - [INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal net. alchim31."}, {"q": "What updates or decisions were made in the discussion?", "a": "sh make-distribution.sh --tgz -Phadoop-provided -Pyarn -DskipTests -Dhadoop.version=2.3.0-cdh5.0.0 -Phive -Dhive.verison=0.13.1\n\nget the same error output"}]}}
{"project": "SPARK", "issue_id": "SPARK-1480", "title": "Choose classloader consistently inside of Spark codebase", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "labels": [], "created": "2014-04-13T03:50:24.000+0000", "updated": "2015-03-23T08:38:23.000+0000", "description": "The Spark codebase is not always consistent on which class loader it uses when classlaoders are explicitly passed to things like serializers. This caused SPARK-1403 and also causes a bug where when the driver has a modified context class loader it is not translated correctly in local mode to the (local) executor.\n\nIn most cases what we want is the following behavior:\n1. If there is a context classloader on the thread, use that.\n2. Otherwise use the classloader that loaded Spark.\n\nWe should just have a utility function for this and call that function whenever we need to get a classloader.\n\nNote that SPARK-1403 is a workaround for this exact problem (it sets the context class loader because downstream code assumes it is set). Once this gets fixed in a more general way SPARK-1403 can be reverted.\n\n", "comments": ["[~pwendell] do you mind posting a link to the PR? Thx.", "https://github.com/apache/spark/pull/398/files", "I meet this bug on spark 1.3.0  + mesos 0.21.1....\n100%..\n\nI0323 16:32:18.933440 14504 fetcher.cpp:64] Extracted resource '/home/mesos/work_dir/slaves/20150323-100710-1214949568-5050-3453-S4/frameworks/20150323-152848-1214949568-5050-21134-0009/executors/20150323-100710-1214949568-5050-3453-S4/runs/3d8f22f5-7fed-44ed-b5f9-98a219133911/spark-1.3.0-bin-2.4.0.tar.gz' into '/home/mesos/work_dir/slaves/20150323-100710-1214949568-5050-3453-S4/frameworks/20150323-152848-1214949568-5050-21134-0009/executors/20150323-100710-1214949568-5050-3453-S4/runs/3d8f22f5-7fed-44ed-b5f9-98a219133911'\nException in thread \"main\" java.lang.NoClassDefFoundError: org/apache/spark/executor/MesosExecutorBackend\nCaused by: java.lang.ClassNotFoundException: org.apache.spark.executor.MesosExecutorBackend\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:217)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:205)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:321)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:266)\nCould not find the main class: org.apache.spark.executor.MesosExecutorBackend", "same as https://issues.apache.org/jira/browse/SPARK-6461\n\n \"run-example SparkPi\" can reproduce this bug."], "derived": {"summary": "The Spark codebase is not always consistent on which class loader it uses when classlaoders are explicitly passed to things like serializers. This caused SPARK-1403 and also causes a bug where when the driver has a modified context class loader it is not translated correctly in local mode to the (local) executor.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Choose classloader consistently inside of Spark codebase - The Spark codebase is not always consistent on which class loader it uses when classlaoders are explicitly passed to things like serializers. This caused SPARK-1403 and also causes a bug where when the driver has a modified context class loader it is not translated correctly in local mode to the (local) executor."}, {"q": "What updates or decisions were made in the discussion?", "a": "same as https://issues.apache.org/jira/browse/SPARK-6461\n\n \"run-example SparkPi\" can reproduce this bug."}]}}
{"project": "SPARK", "issue_id": "SPARK-1481", "title": "Add Naive Bayes to MLlib documentation", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-04-13T04:59:36.000+0000", "updated": "2014-05-07T00:01:40.000+0000", "description": null, "comments": ["Added in https://github.com/apache/spark/pull/422"], "derived": {"summary": "", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Add Naive Bayes to MLlib documentation"}, {"q": "What updates or decisions were made in the discussion?", "a": "Added in https://github.com/apache/spark/pull/422"}]}}
{"project": "SPARK", "issue_id": "SPARK-1482", "title": "Potential resource leaks in saveAsHadoopDataset and saveAsNewAPIHadoopDataset", "status": "Resolved", "priority": "Minor", "reporter": "Shixiong Zhu", "assignee": "Shixiong Zhu", "labels": ["easyfix"], "created": "2014-04-13T10:32:09.000+0000", "updated": "2014-04-19T00:52:04.000+0000", "description": "\"writer.close\" should be put in the \"finally\" block to avoid potential resource leaks.", "comments": ["PR: https://github.com/apache/spark/pull/400", "https://github.com/apache/spark/pull/400"], "derived": {"summary": "\"writer. close\" should be put in the \"finally\" block to avoid potential resource leaks.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Potential resource leaks in saveAsHadoopDataset and saveAsNewAPIHadoopDataset - \"writer. close\" should be put in the \"finally\" block to avoid potential resource leaks."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/400"}]}}
{"project": "SPARK", "issue_id": "SPARK-1483", "title": "Rename minSplits to minPartitions in public APIs", "status": "Resolved", "priority": "Critical", "reporter": "Matei Alexandru Zaharia", "assignee": "Nan Zhu", "labels": [], "created": "2014-04-13T20:21:30.000+0000", "updated": "2014-04-18T17:26:56.000+0000", "description": "The parameter name is part of the public API in Scala and Python, since you can pass named parameters to a method, so we should name it to this more descriptive term. Everywhere else we refer to \"splits\" as partitions.", "comments": ["made the PR: https://github.com/apache/spark/pull/430"], "derived": {"summary": "The parameter name is part of the public API in Scala and Python, since you can pass named parameters to a method, so we should name it to this more descriptive term. Everywhere else we refer to \"splits\" as partitions.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Rename minSplits to minPartitions in public APIs - The parameter name is part of the public API in Scala and Python, since you can pass named parameters to a method, so we should name it to this more descriptive term. Everywhere else we refer to \"splits\" as partitions."}, {"q": "What updates or decisions were made in the discussion?", "a": "made the PR: https://github.com/apache/spark/pull/430"}]}}
{"project": "SPARK", "issue_id": "SPARK-1484", "title": "MLlib should warn if you are using an iterative algorithm on non-cached data", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Aaron Staple", "labels": [], "created": "2014-04-13T20:46:32.000+0000", "updated": "2014-09-25T23:13:08.000+0000", "description": "Not sure what the best way to warn is, but even printing to the log is probably fine. We may want to print at the end of the training run as well as the beginning to make it more visible.", "comments": ["User 'staple' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/2347", "Issue resolved by pull request 2347\n[https://github.com/apache/spark/pull/2347]"], "derived": {"summary": "Not sure what the best way to warn is, but even printing to the log is probably fine. We may want to print at the end of the training run as well as the beginning to make it more visible.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "MLlib should warn if you are using an iterative algorithm on non-cached data - Not sure what the best way to warn is, but even printing to the log is probably fine. We may want to print at the end of the training run as well as the beginning to make it more visible."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 2347\n[https://github.com/apache/spark/pull/2347]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1485", "title": "Implement AllReduce", "status": "Resolved", "priority": "Critical", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-04-14T07:12:25.000+0000", "updated": "2018-06-04T19:54:19.000+0000", "description": "The current implementations of machine learning algorithms rely on the driver for some computation and data broadcasting. This will create a bottleneck at the driver for both computation and communication, especially in multi-model training. An efficient implementation of AllReduce (or AllAggregate) can help free the driver:\n\nallReduce(RDD[T], (T, T) => T): RDD[T]\n\nThis JIRA is created for discussing how to implement AllReduce efficiently and possible alternatives.", "comments": ["PR: https://github.com/apache/spark/pull/506", "Close this in favor of SPARK-2174.", "User 'mengxr' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/506"], "derived": {"summary": "The current implementations of machine learning algorithms rely on the driver for some computation and data broadcasting. This will create a bottleneck at the driver for both computation and communication, especially in multi-model training.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Implement AllReduce - The current implementations of machine learning algorithms rely on the driver for some computation and data broadcasting. This will create a bottleneck at the driver for both computation and communication, especially in multi-model training."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'mengxr' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/506"}]}}
{"project": "SPARK", "issue_id": "SPARK-1486", "title": "Support multi-model training in MLlib", "status": "Resolved", "priority": "Critical", "reporter": "Xiangrui Meng", "assignee": "Burak Yavuz", "labels": ["bulk-closed"], "created": "2014-04-14T07:25:30.000+0000", "updated": "2019-06-06T13:57:37.000+0000", "description": "It is rare in practice to train just one model with a given set of parameters. Usually, this is done by training multiple models with different sets of parameters and then select the best based on their performance on the validation set. MLlib should provide native support for multi-model training/scoring. It requires decoupling of concepts like problem, formulation, algorithm, parameter set, and model, which are missing in MLlib now. MLI implements similar concepts, which we can borrow. There are different approaches for multi-model training:\n\n0) Keep one copy of the data, and train models one after another (or maybe in parallel, depending on the scheduler).\n\n1) Keep one copy of the data, and train multiple models at the same time (similar to `runs` in KMeans).\n\n2) Make multiple copies of the data (still stored distributively), and use more cores to distribute the work.\n\n3) Collect the data, make the entire dataset available on workers, and train one or more models on each worker.\n\nUsers should be able to choose which execution mode they want to use. Note that 3) could cover many use cases in practice when the training data is not huge, e.g., <1GB.\n\nThis task will be divided into sub-tasks and this JIRA is created to discuss the design and track the overall progress.", "comments": ["I would really like to see this feature too. Handling multiple models seems valuable", "In the case where you are using the same training/optimization procedure, you can group together the sets of samples using a key. There is an example of this in the pull request linked to SPARK-2372. This could be a very effective way to deal with simultaneously training all of the different folds from a kFolds split.", "Does the dev on this issue effectively subsume SPARK-1457  and/or  SPARK-1856 ?\n", "It would be helpful to get some feedback if the work being done for SPARK-2372 would help with this issue.", "That sounds very true and relevant. I am completely with you on this one.\n\nOn Tue, Sep 16, 2014 at 5:50 PM, Xiangrui Meng (JIRA) <jira@apache.org>\n\n", "User 'brkyvz' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/2451", "42 bulk-closed issues were still \"In Progress\" when they should be resolved."], "derived": {"summary": "It is rare in practice to train just one model with a given set of parameters. Usually, this is done by training multiple models with different sets of parameters and then select the best based on their performance on the validation set.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support multi-model training in MLlib - It is rare in practice to train just one model with a given set of parameters. Usually, this is done by training multiple models with different sets of parameters and then select the best based on their performance on the validation set."}, {"q": "What updates or decisions were made in the discussion?", "a": "42 bulk-closed issues were still \"In Progress\" when they should be resolved."}]}}
{"project": "SPARK", "issue_id": "SPARK-1487", "title": "Support record filtering via predicate pushdown in Parquet", "status": "Resolved", "priority": "Major", "reporter": "Andre Schumacher", "assignee": "Andre Schumacher", "labels": [], "created": "2014-04-14T13:20:18.000+0000", "updated": "2014-06-14T06:27:50.000+0000", "description": "Parquet has support for column filters, which can be used to avoid reading and de-serializing records that fail the column filter condition. This can lead to potentially large savings, depending on the number of columns filtered by and how many records actually pass the filter.", "comments": ["PR here: https://github.com/apache/spark/pull/511/"], "derived": {"summary": "Parquet has support for column filters, which can be used to avoid reading and de-serializing records that fail the column filter condition. This can lead to potentially large savings, depending on the number of columns filtered by and how many records actually pass the filter.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Support record filtering via predicate pushdown in Parquet - Parquet has support for column filters, which can be used to avoid reading and de-serializing records that fail the column filter condition. This can lead to potentially large savings, depending on the number of columns filtered by and how many records actually pass the filter."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR here: https://github.com/apache/spark/pull/511/"}]}}
{"project": "SPARK", "issue_id": "SPARK-1488", "title": "Resolve scalac feature warnings during build", "status": "Resolved", "priority": "Minor", "reporter": "Sean R. Owen", "assignee": "Sean R. Owen", "labels": [], "created": "2014-04-14T13:20:19.000+0000", "updated": "2014-04-15T02:56:24.000+0000", "description": "For your consideration: scalac currently notes a number of feature warnings during compilation:\n\n{code}\n[warn] there were 65 feature warning(s); re-run with -feature for details\n{code}\n\nWarnings are like:\n\n{code}\n[warn] /Users/srowen/Documents/spark/core/src/main/scala/org/apache/spark/SparkContext.scala:1261: implicit conversion method rddToPairRDDFunctions should be enabled\n[warn] by making the implicit value scala.language.implicitConversions visible.\n[warn] This can be achieved by adding the import clause 'import scala.language.implicitConversions'\n[warn] or by setting the compiler option -language:implicitConversions.\n[warn] See the Scala docs for value scala.language.implicitConversions for a discussion\n[warn] why the feature should be explicitly enabled.\n[warn]   implicit def rddToPairRDDFunctions[K: ClassTag, V: ClassTag](rdd: RDD[(K, V)]) =\n[warn]                ^\n{code}\n\nscalac is suggesting that it's just best practice to explicitly enable certain language features by importing them where used.\n\nThe accompanying PR simply adds the imports it suggests (and squashes one other Java warning along the way). This leaves just deprecation warnings in the build.", "comments": [], "derived": {"summary": "For your consideration: scalac currently notes a number of feature warnings during compilation:\n\n{code}\n[warn] there were 65 feature warning(s); re-run with -feature for details\n{code}\n\nWarnings are like:\n\n{code}\n[warn] /Users/srowen/Documents/spark/core/src/main/scala/org/apache/spark/SparkContext. scala:1261: implicit conversion method rddToPairRDDFunctions should be enabled\n[warn] by making the implicit value scala.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Resolve scalac feature warnings during build - For your consideration: scalac currently notes a number of feature warnings during compilation:\n\n{code}\n[warn] there were 65 feature warning(s); re-run with -feature for details\n{code}\n\nWarnings are like:\n\n{code}\n[warn] /Users/srowen/Documents/spark/core/src/main/scala/org/apache/spark/SparkContext. scala:1261: implicit conversion method rddToPairRDDFunctions should be enabled\n[warn] by making the implicit value scala."}]}}
{"project": "SPARK", "issue_id": "SPARK-1489", "title": "Fix the HistoryServer view acls", "status": "Resolved", "priority": "Major", "reporter": "Thomas Graves", "assignee": "Thomas Graves", "labels": [], "created": "2014-04-14T15:38:27.000+0000", "updated": "2014-04-25T01:39:07.000+0000", "description": "If you are running the historyServer with view acls enabled (and a filter added to do auth), the application acls don't work properly.  It is looking at the user running the history server and not the user who ran the actual application, so basically no one other then the user running the history server can see anything.\n\nWe also need a way to allow all users to see the front page of the history server and only do authorization on the particular applications.", "comments": ["https://github.com/apache/spark/pull/509"], "derived": {"summary": "If you are running the historyServer with view acls enabled (and a filter added to do auth), the application acls don't work properly. It is looking at the user running the history server and not the user who ran the actual application, so basically no one other then the user running the history server can see anything.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Fix the HistoryServer view acls - If you are running the historyServer with view acls enabled (and a filter added to do auth), the application acls don't work properly. It is looking at the user running the history server and not the user who ran the actual application, so basically no one other then the user running the history server can see anything."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/509"}]}}
{"project": "SPARK", "issue_id": "SPARK-1490", "title": "Add kerberos support to the HistoryServer", "status": "Resolved", "priority": "Major", "reporter": "Thomas Graves", "assignee": "Thomas Graves", "labels": [], "created": "2014-04-14T16:27:02.000+0000", "updated": "2014-04-24T21:21:18.000+0000", "description": "Now that we have a history server that works on yarn and mesos we should add the ability for it to authenticate via kerberos so that it can read HDFS files without having to be restarted every 24 hours. \n\nOne solution to this is to have the history server read a keytab file.  The Hadoop UserGroupInformation class has that functionality built in and as long as its using rpc to talk to hdfs it will automatically relogin when it needs to.   If the history server isn't using rpc to talk to hdfs then we would have to add some functionality to relogin approximately every 24 hours (configurable time).", "comments": ["https://github.com/apache/spark/pull/513"], "derived": {"summary": "Now that we have a history server that works on yarn and mesos we should add the ability for it to authenticate via kerberos so that it can read HDFS files without having to be restarted every 24 hours. One solution to this is to have the history server read a keytab file.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add kerberos support to the HistoryServer - Now that we have a history server that works on yarn and mesos we should add the ability for it to authenticate via kerberos so that it can read HDFS files without having to be restarted every 24 hours. One solution to this is to have the history server read a keytab file."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/513"}]}}
{"project": "SPARK", "issue_id": "SPARK-1491", "title": "maven hadoop-provided profile fails to build", "status": "Resolved", "priority": "Major", "reporter": "Thomas Graves", "assignee": "Guoqiang Li", "labels": [], "created": "2014-04-14T17:51:10.000+0000", "updated": "2014-04-29T06:28:06.000+0000", "description": "I tried to use the -Phadoop-profile with maven to build the assembly without the yarn/hadoop pieces that are already on the cluster, but it fails with:\n\n[ERROR]   The project org.apache.spark:spark-parent:1.0.0-SNAPSHOT (/home/tgraves/tgravescs-spark/pom.xml) has 2 errors\n[ERROR]     'dependencies.dependency.version' for org.apache.avro:avro-ipc:jar is missing. @ line 884, column 21\n[ERROR]     'dependencies.dependency.version' for org.apache.zookeeper:zookeeper:jar is missing. @ line 889, column 21\n[ERROR] \n\n\nThis means avro-ipc and zookeeper don't have versions in the dependencyManagement section.\nI tried on both hadoop 0.23 and hadoop 2.3.0.  build command like:\n\n mvn  -Dyarn.version=0.23.9 -Dhadoop.version=0.23.9  -Pyarn-alpha -Phadoop-provided package -DskipTests", "comments": ["Fixed by :\nhttps://github.com/apache/spark/pull/480\n\nI tested the build with this exact configuration and it worked."], "derived": {"summary": "I tried to use the -Phadoop-profile with maven to build the assembly without the yarn/hadoop pieces that are already on the cluster, but it fails with:\n\n[ERROR]   The project org. apache.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "maven hadoop-provided profile fails to build - I tried to use the -Phadoop-profile with maven to build the assembly without the yarn/hadoop pieces that are already on the cluster, but it fails with:\n\n[ERROR]   The project org. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed by :\nhttps://github.com/apache/spark/pull/480\n\nI tested the build with this exact configuration and it worked."}]}}
{"project": "SPARK", "issue_id": "SPARK-1492", "title": "running-on-yarn doc should use spark-submit script for examples", "status": "Resolved", "priority": "Blocker", "reporter": "Thomas Graves", "assignee": "Sandy Ryza", "labels": [], "created": "2014-04-14T21:44:48.000+0000", "updated": "2014-05-03T04:43:38.000+0000", "description": "the spark-class script puts out lots of warnings telling users to use spark-submit script with new options.  We should update the running-on-yarn.md docs to have examples using the spark-submit script rather then spark-class. ", "comments": ["https://github.com/apache/spark/pull/601", "Issue resolved by pull request 601\n[https://github.com/apache/spark/pull/601]"], "derived": {"summary": "the spark-class script puts out lots of warnings telling users to use spark-submit script with new options. We should update the running-on-yarn.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "running-on-yarn doc should use spark-submit script for examples - the spark-class script puts out lots of warnings telling users to use spark-submit script with new options. We should update the running-on-yarn."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 601\n[https://github.com/apache/spark/pull/601]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1493", "title": "Apache RAT excludes don't work with file path (instead of file name)", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": null, "labels": ["starter"], "created": "2014-04-14T22:42:28.000+0000", "updated": "2015-09-29T16:58:52.000+0000", "description": "Right now the way we do RAT checks, it doesn't work if you try to exclude:\n\n/path/to/file.ext\n\nyou have to just exclude\n\nfile.ext", "comments": ["RAT itself appears to preclude exclusion using a \"/path/to/file.ext\" regex because it traverses the directory tree and applies its exclusion filter only to individual file names.  The filter never sees an entire path \"path/to/file.ext\", only \"path\", \"to\", and \"file.ext\"\n\nhttps://github.com/apache/rat/blob/incubator-site-import/rat/rat-core/src/main/java/org/apache/rat/DirectoryWalker.java#L127\n\nEither RAT needs a new filtering feature that can see an entire path, or the report it generates has to be filtered post-hoc.\n\nFiled an RFE against RAT:  RAT-161", "Thanks for looking into this Erik. It seems like maybe there isn't a good way to do unless we want to implement filtering post-hoc (and it might be tricky to support e.g. globbing in that case).", "I submitted a proposal patch for RAT-161, which allows one to request path-spanning patterns by including a leading '/'\n\nIf '--dir' argument is /path/to/repo, and contents of '-E' file includes:\n/subpath/to/.*ext\n\nthen the pattern induced is:\n/path/to/repo + /subpath/to/.*ex t --> /path/to/repo/subpath/to/.*ext\n", "Looks like this was fixed? There are no paths of this form in .rat-excludes now.", "Erik's proposal would certainly help us in the Trafodion project. We have a high level directory sturcture on which we'd like to run RAT. We have several subdirectories and want to include only a subset of files in each of those directories. Using filename patterns without /path would make the exclusions imprecise. There may be the same file it 2 different subdirectories and both would get excluded which we don't really intend to do.We want to exclude specific files in specific directories with a regexp. Unfortunately  it isn't working.  \n"], "derived": {"summary": "Right now the way we do RAT checks, it doesn't work if you try to exclude:\n\n/path/to/file. ext\n\nyou have to just exclude\n\nfile.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Apache RAT excludes don't work with file path (instead of file name) - Right now the way we do RAT checks, it doesn't work if you try to exclude:\n\n/path/to/file. ext\n\nyou have to just exclude\n\nfile."}, {"q": "What updates or decisions were made in the discussion?", "a": "Erik's proposal would certainly help us in the Trafodion project. We have a high level directory sturcture on which we'd like to run RAT. We have several subdirectories and want to include only a subset of files in each of those directories. Using filename patterns without /path would make the exclusions imprecise. There may be the same file it 2 different subdirectories and both would get excluded which we don't really intend to do.We want to exclude specific files in specific directories with a regexp. Unfortunately  it isn't working."}]}}
{"project": "SPARK", "issue_id": "SPARK-1494", "title": "Hive Dependencies being checked by MIMA", "status": "Resolved", "priority": "Minor", "reporter": "Ahir Reddy", "assignee": "Michael Armbrust", "labels": [], "created": "2014-04-15T00:00:38.000+0000", "updated": "2014-05-09T21:01:36.000+0000", "description": "It looks like code in companion objects is being invoked by the MIMA checker, as it uses Scala reflection to check all of the interfaces. As a result it's starting a Spark context and eventually out of memory errors. As a temporary fix all classes that contain \"hive\" or \"Hive\" are excluded from the check.", "comments": ["We could change the object to contain a lazy val instead of being a TestHiveContext itself.  That should fix the issues.", "I think this is fixed now that MIMA loads classes passing false to \"initialize\"."], "derived": {"summary": "It looks like code in companion objects is being invoked by the MIMA checker, as it uses Scala reflection to check all of the interfaces. As a result it's starting a Spark context and eventually out of memory errors.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Hive Dependencies being checked by MIMA - It looks like code in companion objects is being invoked by the MIMA checker, as it uses Scala reflection to check all of the interfaces. As a result it's starting a Spark context and eventually out of memory errors."}, {"q": "What updates or decisions were made in the discussion?", "a": "I think this is fixed now that MIMA loads classes passing false to \"initialize\"."}]}}
{"project": "SPARK", "issue_id": "SPARK-1495", "title": "support leftsemijoin for sparkSQL", "status": "Resolved", "priority": "Major", "reporter": "Adrian Wang", "assignee": "Adrian Wang", "labels": [], "created": "2014-04-15T01:51:09.000+0000", "updated": "2014-06-09T18:36:48.000+0000", "description": "I created Github PR #395 for this issue.[https://github.com/apache/spark/pull/395]\nAs marmbrus comments there, one design question is which of the following is better:\n1. multiple operators that handle different kinds of joins, letting the planner pick the correct one\n2. putting the switching logic inside of the operator as is done here", "comments": ["What do you think of this? In addition to the change in BroadcastNestedLoopJoin, we also to build a special operator for left semi join where there are at least some equality predicates.  It will be similar to hash join but make the following optimization: dedup the right side and just build a hash set instead of a hash map.", "[~marmbrus] Yes your are right, I'll try to refine the patch accordingly.", "This should make adding the new operator to the planner easier: https://github.com/apache/spark/pull/418", "Another PR [https://github.com/apache/spark/pull/837] submitted.", "Resolved by: https://github.com/apache/spark/pull/837"], "derived": {"summary": "I created Github PR #395 for this issue. [https://github.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "support leftsemijoin for sparkSQL - I created Github PR #395 for this issue. [https://github."}, {"q": "What updates or decisions were made in the discussion?", "a": "Resolved by: https://github.com/apache/spark/pull/837"}]}}
{"project": "SPARK", "issue_id": "SPARK-1496", "title": "SparkContext.jarOfClass should return Option instead of a sequence", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "labels": ["api-change"], "created": "2014-04-15T02:53:10.000+0000", "updated": "2014-04-29T00:36:54.000+0000", "description": "This is pretty confusing, especially since addJar expects to take a single jar.", "comments": ["Is it should return Option[Seq[String]]? Maybe I could help you fix this issue. :-)", "Hey I'm, just gonna fix this quicky right now... appreciate the offer though."], "derived": {"summary": "This is pretty confusing, especially since addJar expects to take a single jar.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "SparkContext.jarOfClass should return Option instead of a sequence - This is pretty confusing, especially since addJar expects to take a single jar."}, {"q": "What updates or decisions were made in the discussion?", "a": "Hey I'm, just gonna fix this quicky right now... appreciate the offer though."}]}}
{"project": "SPARK", "issue_id": "SPARK-1497", "title": "Spark YARN code isn't checked with Scalastyle and has many style violations", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Sean R. Owen", "labels": [], "created": "2014-04-15T05:08:12.000+0000", "updated": "2014-04-16T16:37:10.000+0000", "description": "We should just set SPARK_YARN=true when running scalatstyle. Also we should clean up the existing style issues.", "comments": ["FWIW: https://github.com/apache/spark/pull/413"], "derived": {"summary": "We should just set SPARK_YARN=true when running scalatstyle. Also we should clean up the existing style issues.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Spark YARN code isn't checked with Scalastyle and has many style violations - We should just set SPARK_YARN=true when running scalatstyle. Also we should clean up the existing style issues."}, {"q": "What updates or decisions were made in the discussion?", "a": "FWIW: https://github.com/apache/spark/pull/413"}]}}
{"project": "SPARK", "issue_id": "SPARK-1498", "title": "Spark can hang if pyspark tasks fail", "status": "Resolved", "priority": "Major", "reporter": "Kay Ousterhout", "assignee": null, "labels": [], "created": "2014-04-15T05:10:20.000+0000", "updated": "2014-11-06T07:59:28.000+0000", "description": "In pyspark, when some kinds of jobs fail, Spark hangs rather than returning an error.  This is partially a scheduler problem -- the scheduler sometimes thinks failed tasks succeed, even though they have a stack trace and exception.\n\nYou can reproduce this problem with:\nardd = sc.parallelize([(1,2,3), (4,5,6)])\nbrdd = sc.parallelize([(1,2,6), (4,5,9)])\nardd.join(brdd).count()\n\nThe last line will run forever (the problem in this code is that the RDD entries have 3 values instead of the expected 2).  I haven't verified if this is a problem for 1.0 as well as 0.9.\n\nThanks to Shivaram for helping diagnose this issue!", "comments": ["I see this problem on the master branch. To add more details I get a log like \n\n14/04/14 22:07:18 INFO TaskSetManager: Starting task 1.0:4 using 1 cores as TID 4 on executor localhost: localhost (PROCESS_LOCAL)\n14/04/14 22:07:18 INFO TaskSetManager: Serialized task 1.0:4 as 6651 bytes in 1 ms\n14/04/14 22:07:18 INFO Executor: Running task ID 4\n14/04/14 22:07:18 INFO DAGScheduler: Completed ShuffleMapTask(1, 2)\n14/04/14 22:07:18 INFO TaskSetManager: Finished TID 2 in 132 ms on localhost (progress: 1/8)\n14/04/14 22:07:18 INFO TaskSetManager: Starting task 1.0:5 using 1 cores as TID 5 on executor localhost: localhost (PROCESS_LOCAL)\n14/04/14 22:07:18 INFO TaskSetManager: Serialized task 1.0:5 as 6673 bytes in 1 ms\n14/04/14 22:07:18 INFO Executor: Running task ID 5\n14/04/14 22:07:18 INFO DAGScheduler: Completed ShuffleMapTask(1, 0)\n14/04/14 22:07:18 INFO TaskSetManager: Finished TID 0 in 150 ms on localhost (progress: 2/8)\n14/04/14 22:07:18 INFO PythonRDD: Times: total = 2, boot = 1, init = 1, finish = 0\n14/04/14 22:07:18 INFO PythonRDD: Times: total = 6, boot = 1, init = 5, finish = 0\n14/04/14 22:07:18 INFO Executor: Serialized size of result for 4 is 847\n14/04/14 22:07:18 INFO Executor: Sending result for 4 directly to driver\n14/04/14 22:07:18 INFO Executor: Finished task ID 4\n14/04/14 22:07:18 INFO TaskSetManager: Starting task 1.0:6 using 1 cores as TID 6 on executor localhost: localhost (PROCESS_LOCAL)\n14/04/14 22:07:18 INFO TaskSetManager: Serialized task 1.0:6 as 6651 bytes in 1 ms\n14/04/14 22:07:18 INFO Executor: Running task ID 6\n14/04/14 22:07:18 INFO DAGScheduler: Completed ShuffleMapTask(1, 4)\n14/04/14 22:07:18 INFO TaskSetManager: Finished TID 4 in 23 ms on localhost (progress: 3/8)\nPySpark worker failed with exception:\nTraceback (most recent call last):\n  File \"/home/shivaram/projects/spark/python/pyspark/worker.py\", line 77, in main\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/shivaram/projects/spark/python/pyspark/serializers.py\", line 191, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/home/shivaram/projects/spark/python/pyspark/serializers.py\", line 123, in dump_stream\n    for obj in iterator:\n  File \"/home/shivaram/projects/spark/python/pyspark/serializers.py\", line 180, in _batched\n    for item in iterator:\n  File \"/home/shivaram/projects/spark/python/pyspark/join.py\", line 38, in <lambda>\n    ws = other.map(lambda (k, v): (k, (2, v)))\nValueError: too many values to unpack\n\nBut the web ui shows 4 out of 8 tasks succeeded and 4 tasks keep running forever", "This is still a problem as of 0.9.2 but it's fixed in 1.0+.", "I closed this since 0.9 seems pretty ancient now."], "derived": {"summary": "In pyspark, when some kinds of jobs fail, Spark hangs rather than returning an error. This is partially a scheduler problem -- the scheduler sometimes thinks failed tasks succeed, even though they have a stack trace and exception.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Spark can hang if pyspark tasks fail - In pyspark, when some kinds of jobs fail, Spark hangs rather than returning an error. This is partially a scheduler problem -- the scheduler sometimes thinks failed tasks succeed, even though they have a stack trace and exception."}, {"q": "What updates or decisions were made in the discussion?", "a": "I closed this since 0.9 seems pretty ancient now."}]}}
{"project": "SPARK", "issue_id": "SPARK-1499", "title": "Workers continuously produce failing executors", "status": "Resolved", "priority": "Major", "reporter": "Aaron Davidson", "assignee": null, "labels": [], "created": "2014-04-15T06:25:18.000+0000", "updated": "2015-04-08T10:55:46.000+0000", "description": "If a node is in a bad state, such that newly started executors fail on startup or first use, the Standalone Cluster Worker will happily keep spawning new ones. A better behavior would be for a Worker to mark itself as dead if it has had a history of continuously producing erroneous executors, or else to somehow prevent a driver from re-registering executors from the same machine repeatedly.\n\nReported on mailing list: http://mail-archives.apache.org/mod_mbox/spark-user/201404.mbox/%3CCAL8t0BqJFgtf-Vbzjq6Yj7CKBL_9P9S0tRVEW2MVG6ZBNgxY2g@mail.gmail.com%3E\n\nRelevant logs: \n\n{noformat}\n14/04/11 19:06:52 INFO client.AppClient$ClientActor: Executor updated: app-20140411190649-0008/4 is now FAILED (Command exited with code 53)\n14/04/11 19:06:52 INFO cluster.SparkDeploySchedulerBackend: Executor app-20140411190649-0008/4 removed: Command exited with code 53\n14/04/11 19:06:52 INFO cluster.SparkDeploySchedulerBackend: Executor 4 disconnected, so removing it\n14/04/11 19:06:52 ERROR scheduler.TaskSchedulerImpl: Lost an executor 4 (already removed): Failed to create local directory (bad spark.local.dir?)\n14/04/11 19:06:52 INFO client.AppClient$ClientActor: Executor added: app-20140411190649-0008/27 on worker-20140409212012-ip-172-31-19-11.us-west-1.compute.internal-58614 (ip-172-31-19-11.us-west-1.compute.internal:58614) with 8 cores\n14/04/11 19:06:52 INFO cluster.SparkDeploySchedulerBackend: Granted executor ID app-20140411190649-0008/27 on hostPort ip-172-31-19-11.us-west-1.compute.internal:58614 with 8 cores, 56.9 GB RAM\n14/04/11 19:06:52 INFO client.AppClient$ClientActor: Executor updated: app-20140411190649-0008/27 is now RUNNING\n14/04/11 19:06:52 INFO storage.BlockManagerMasterActor$BlockManagerInfo: Registering block manager ip-172-31-24-76.us-west-1.compute.internal:50256 with 32.7 GB RAM\n14/04/11 19:06:52 INFO metastore.HiveMetaStore: 0: get_table : db=default tbl=wikistats_pd\n14/04/11 19:06:52 INFO HiveMetaStore.audit: ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=wikistats_pd\t\n14/04/11 19:06:53 DEBUG hive.log: DDL: struct wikistats_pd { string projectcode, string pagename, i32 pageviews, i32 bytes}\n14/04/11 19:06:53 DEBUG lazy.LazySimpleSerDe: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[projectcode, pagename, pageviews, bytes] columnTypes=[string, string, int, int] separator=[[B@29a81175] nullstring=\\N lastColumnTakesRest=false\nshark> 14/04/11 19:06:55 INFO cluster.SparkDeploySchedulerBackend: Registered executor: Actor[akka.tcp://sparkExecutor@ip-172-31-19-11.us-west-1.compute.internal:45248/user/Executor#-1002203295] with ID 27\nshow 14/04/11 19:06:56 INFO cluster.SparkDeploySchedulerBackend: Executor 27 disconnected, so removing it\n14/04/11 19:06:56 ERROR scheduler.TaskSchedulerImpl: Lost an executor 27 (already removed): remote Akka client disassociated\n14/04/11 19:06:56 INFO client.AppClient$ClientActor: Executor updated: app-20140411190649-0008/27 is now FAILED (Command exited with code 53)\n14/04/11 19:06:56 INFO cluster.SparkDeploySchedulerBackend: Executor app-20140411190649-0008/27 removed: Command exited with code 53\n14/04/11 19:06:56 INFO client.AppClient$ClientActor: Executor added: app-20140411190649-0008/28 on worker-20140409212012-ip-172-31-19-11.us-west-1.compute.internal-58614 (ip-172-31-19-11.us-west-1.compute.internal:58614) with 8 cores\n14/04/11 19:06:56 INFO cluster.SparkDeploySchedulerBackend: Granted executor ID app-20140411190649-0008/28 on hostPort ip-172-31-19-11.us-west-1.compute.internal:58614 with 8 cores, 56.9 GB RAM\n14/04/11 19:06:56 INFO client.AppClient$ClientActor: Executor updated: app-20140411190649-0008/28 is now RUNNING\ntables;\n{noformat}", "comments": ["Have you looked into the log of the failing worker?\nI think there must be a lot of lines like\n\"\nERROR EndpointWriter: AssociationError [akka.tcp://sparkWorker@slave1:45324] -> [akka.tcp://sparkExecutor@slave1:59294]: Error [Association failed with [akka.tcp://sparkExecutor@slave1:59294]] [\nakka.remote.EndpointAssociationException: Association failed with [akka.tcp://sparkExecutor@slave1:59294]\nCaused by: akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2: Connection refused: slave1/192.168.1.2:59294\n\"", "I also encounter the same problem. Is there any workaround/solution for this issue ?\nI am runny the spark cluster using a vagrant setup with 4 machines. All the nodes behave this way.\n\n[~adrian-wang] I can confirm that the worker nodes log the messages posted by you.", "Since it looks like exactly the same issue, and the new JIRA has a PR, let's resolve this as a duplicate.", "How to solve this problem I have encountered this issue."], "derived": {"summary": "If a node is in a bad state, such that newly started executors fail on startup or first use, the Standalone Cluster Worker will happily keep spawning new ones. A better behavior would be for a Worker to mark itself as dead if it has had a history of continuously producing erroneous executors, or else to somehow prevent a driver from re-registering executors from the same machine repeatedly.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Workers continuously produce failing executors - If a node is in a bad state, such that newly started executors fail on startup or first use, the Standalone Cluster Worker will happily keep spawning new ones. A better behavior would be for a Worker to mark itself as dead if it has had a history of continuously producing erroneous executors, or else to somehow prevent a driver from re-registering executors from the same machine repeatedly."}, {"q": "What updates or decisions were made in the discussion?", "a": "How to solve this problem I have encountered this issue."}]}}
{"project": "SPARK", "issue_id": "SPARK-1500", "title": " add with-hive argument to make-distribution.sh", "status": "Resolved", "priority": "Major", "reporter": "Guoqiang Li", "assignee": null, "labels": [], "created": "2014-04-15T09:56:21.000+0000", "updated": "2014-05-10T03:52:29.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "add with-hive argument to make-distribution.sh"}]}}
{"project": "SPARK", "issue_id": "SPARK-1501", "title": "Assertions in Graph.apply test are never executed", "status": "Resolved", "priority": "Minor", "reporter": "William Benton", "assignee": "William Benton", "labels": ["test"], "created": "2014-04-15T14:34:56.000+0000", "updated": "2014-05-28T22:02:55.000+0000", "description": "The current Graph.apply test in GraphSuite contains assertions within an RDD transformation.  These never execute because the transformation never executes.  I have a (trivial) patch to fix this by collecting the graph triplets first.", "comments": ["Here's the PR:  https://github.com/apache/spark/pull/415"], "derived": {"summary": "The current Graph. apply test in GraphSuite contains assertions within an RDD transformation.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Assertions in Graph.apply test are never executed - The current Graph. apply test in GraphSuite contains assertions within an RDD transformation."}, {"q": "What updates or decisions were made in the discussion?", "a": "Here's the PR:  https://github.com/apache/spark/pull/415"}]}}
{"project": "SPARK", "issue_id": "SPARK-1502", "title": "Spark on Yarn: add config option to not include yarn/mapred cluster classpath", "status": "Resolved", "priority": "Major", "reporter": "Thomas Graves", "assignee": null, "labels": [], "created": "2014-04-15T15:40:10.000+0000", "updated": "2019-02-12T21:45:15.000+0000", "description": "Since we have a spark assembly that is including all the yarn and other dependencies we need, we should add an option to allow the user to not include the cluster default yarn/mapreduce application classpaths when running spark on yarn.\n\nClientBase.populateHadoopClasspath is function that populates that.", "comments": ["User 'Sephiroth-Lin' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/5294", "I don't think a new setting is worth it. You can use the {{spark.hadoop.blah}} to set the Hadoop config to an empty string and achieve the same result."], "derived": {"summary": "Since we have a spark assembly that is including all the yarn and other dependencies we need, we should add an option to allow the user to not include the cluster default yarn/mapreduce application classpaths when running spark on yarn. ClientBase.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Spark on Yarn: add config option to not include yarn/mapred cluster classpath - Since we have a spark assembly that is including all the yarn and other dependencies we need, we should add an option to allow the user to not include the cluster default yarn/mapreduce application classpaths when running spark on yarn. ClientBase."}, {"q": "What updates or decisions were made in the discussion?", "a": "I don't think a new setting is worth it. You can use the {{spark.hadoop.blah}} to set the Hadoop config to an empty string and achieve the same result."}]}}
{"project": "SPARK", "issue_id": "SPARK-1503", "title": "Implement Nesterov's accelerated first-order method", "status": "Resolved", "priority": "Major", "reporter": "Xiangrui Meng", "assignee": "Aaron Staple", "labels": ["bulk-closed"], "created": "2014-04-15T16:56:55.000+0000", "updated": "2019-05-21T04:16:17.000+0000", "description": "Nesterov's accelerated first-order method is a drop-in replacement for steepest descent but it converges much faster. We should implement this method and compare its performance with existing algorithms, including SGD and L-BFGS.\n\nTFOCS (http://cvxr.com/tfocs/) is a reference implementation of Nesterov's method and its variants on composite objectives.", "comments": ["Hi, Id like to try working on this ticket. If youd like to assign it to me, I can write a short spec and then work on a PR.", "[~staple] Thanks for picking up this JIRA! TFOCS is a good place to start. We can support AT (Auslender and Teboulle) update, line search, and restart in the first version. It would be nice to take generic composite objective functions.\n\nPlease note that this could become a big task. We definitely need to go through the design first.", "[~mengxr] Thanks for the heads up! Ill definitely go through TFOCS and am happy to work carefully and collaboratively on design.", "[~mengxr] Sorry for the delay. I wrote up a design proposal for the initial implementation. Let me know what you think, and if you'd like me to clarify anything.\n\nUPDATE: Ok, here's the document:\nhttps://docs.google.com/document/d/1L50O66LnBfVopFjptbet2ZTQRzriZTjKvlIILZwKsno/edit?usp=sharing", "[~staple] Thanks for working on the design doc! [~rezazadeh] will make a pass.", "[~mengxr] [~rezazadeh] Ok, thanks for the heads up. Let me know if theres anything about the spec that should be handled differently. I covered most of the mathematics informally (the details are already covered formally in the references). And in addition, the proposal describes a method of implementing TFOCS functionality distributively but does not investigate existing distributed optimization systems.", "Thanks for this design doc Aaron. \n\nIt looks good for the first implementation to support composite objectives, A&T updates, but I'm not sure about backtracking.\n\nHave you thought about how many passes through the data backtracking can require? As you mention: per backtracking inner loop iteration, we need 2 shuffles. But how many iterations of the inner backtracking loop can be typical? Could it be better in a distributed environment to avoid backtracking and use a constant step size? Especially for the well-behaved objectives we have (e.g. logistic regression). If a constant step size works fast enough, we should do that first - what do you think?\n\nPlease try a constant step size first - if it works, that will bring down the communication cost greatly.\n\nIts fine that the initial implementation will not include the linear operator optimizations present in TFOCS. Thats a good call. In general lets try to keep the first PR as simple as possible.\n\nPlease make sure your code adheres to this example for LBFGS, so we can swap out the Optimizer with your contribution:\nhttp://spark.apache.org/docs/latest/mllib-optimization.html#l-bfgs", "[~rezazadeh] Thanks for your feedback.\n\nYour point about the communication cost of backtracking is well taken. Just to explain where I was coming from with the design proposal: As I was looking to come up to speed on accelerated gradient descent, I came across some scattered comments online suggesting that acceleration was difficult to implement well, was finicky, etc - especially when compared with standard gradient descent for machine learning applications. So, wary of these sorts of comments, I wrote up the proposal with the intention of duplicating TFOCS as closely as possible to start with, with the possibility of making changes from there based on performance. In addition, Id interpreted an earlier comment in this ticket as suggesting that line search be implemented in the same manner as in TFOCS.\n\nI am happy to implement a constant step size first, though. It may also be informative to run some performance tests in spark both with and without backtracking. One (basic, not conclusive) data point I have now is that, if I run TFOCS test_LASSO example it triggers 97 iterations of the outer AT loop and 106 iterations of the inner backtracking loop. For this one example, the backtracking iteration overhead is only about 10%. Though keep in mind that in spark if we removed backtracking entirely it would mean only one distributed aggregation per iteration rather than two - so a huge improvement in communication cost assuming there is still a good convergence rate.\n\nIncidentally, are there any specific learning benchmarks for spark that you would recommend?\n\nIll do a bit of research to identify the best ways to manage the lipschitz estimate / step size in the absence of backtracking (for our objective functions in particular). Ive also noticed some references online to distributed implementations of accelerated methods. It may be informative to learn more about them - if you happen to have heard of any particularly good distributed optimizers using acceleration, please let me know.\n\nThanks,\nAaron\n\nPS Yes, Ill make sure to follow the lbfgs example so the accelerated implementation can be easily substituted.", "Thanks Aaron. From an implementation perspective, it's probably easier to implement a constant step size first. From there you can see if there is any finicky behavior and compare to the unaccelerated proximal gradient already in Spark. If it works well enough, we should commit the first PR without backtracking, and then experiment with backtracking, otherwise if you see strange behavior then you can decide if backtracking would solve it. ", "User 'staple' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/4934", "I recently created a PR for an implementation of accelerated gradient descent without backtracking, as discussed above.\n\nI also ran some simple, small scale (single server) benchmarks to compare different optimization methods. (The benchmark result graphs are provided as images attached to this ticket.) The optimization methods tested were:\n\n- gra: existing gradient descent implementation (using full batch)\n- acc: accelerated descent (as implemented in the PR), but without automatic restart\n- acc_r: accelerated descent, with automatic restart\n- acc_b: accelerated descent, with backtracking, without automatic restart\n- acc_rb: accelerated descent, with backtracking, with automatic restart\n- lbfgs: existing lbfgs implementation\n\n(Note that backtracking is not part of the PR, and was just tested as a point of comparison.)\n\nThe x axis shows the number of outer loop iterations of the optimization algorithm. (Note that for backtracking implementations, the full cost of backtracking is not represented in this outer loop count. For non backtracking implementations, the number of outer loop iterations is the same as the number of spark map reduce jobs). The y axis is the log of the difference from best determined optimized value.\n\nThe optimization test runs were:\n\n- linear: A scaled up version of the test data from TFOCSs test_LASSO.m example, with 10000 observations on 1024 features. 512 of the features are actually correlated with result. Unregularized linear regression was used. (The scala acceleration implementation was observed to be consistent with the TFOCS implementation on this dataset.)\n- linear l1: The same as linear, but with l1 regularization\n- logistic: Each feature of each observation is generated by summing a feature gaussian specific to the observations binary category with a noise gaussian. 10000 observations on 250 features. Unregularized logistic regression was used.\n- logistic l2: Same as logistic, but using l2 regularization\n\nNote that for each test run, all optimization methods were given the same initial step size.\n\nObservations:\n- Acceleration consistently converges more quickly than standard gradient descent, given the same initial step size.\n- Automatic restart is helpful for acceleration convergence\n- Backtracking can significantly boost convergence rates in some cases (measured in terms of outer loop iterations). But the full cost of backtracking was not measured in these runs.\n- lbfgs generally outperformed accelerated gradient descent in these test runs. Accelerated gradient descent was competitive with lbfgs for linear l1 (lasso) regression. However as noted in the documentation, the L1Updater will not work for the lbfgs implementation. It seems l1 regularization is currently a weak spot for the lbfgs implementation.", "Optimization benchmarks, uploaded 2015-03-06.", "I was just today reminded of this other issue, SPARK-3942, which takes issue with shrinking step size instead of a more principled line search in L-BFGS. Worth linking this up.", "[~staple] [~lewuathe] Can you please coordinate about how convergence is measured, as in [SPARK-3382]?  The current implementation for [SPARK-3382] uses relative convergence w.r.t. the weight vector, where it measures relative to the weight vector from the previous iteration.  I figure we should use the same convergence criterion for both accelerated and non-accelerated gradient descent.", "[~josephkb] and [~lewuathe] Sure, happy to coordinate. So far I have just been duplicating the the convergence tolerance check in TFOCS, the matlab package on which the accelerated gradient descent implementation is based. TFOCS also tests for convergence by checking if the relative change in the weight vector is below a specified threshold. But there are some differences from the SPARK-3382 implementation. For example in TFOCS the relative difference between new and old weight vectors is measured with respect to the new weight vector instead of the old. And if the new weight vector is smaller than the unit vector the convergence test is changed to be an absolute rather than relative difference between successive weight vectors. I am just describing the implementation here, happy to discuss further and potentially look at making changes.\n\nHere is the relevant code if you are interested (there is also a separate condition when the weight vector does not change between iterations):\nhttps://github.com/cvxr/TFOCS/blob/e34c0daeb136935d23b8df506de8b7b191f6b0a3/private/tfocs_iterate.m#L19-L24", "Thanks!  [~lewuathe] I probably should have looked into this more carefully before your PR.  It would be good to understand the best criterion.  Regardless, I believe most of your PR's changes should carry over.\n\n[~staple]  [~mengxr]  Do you know what the basis of the TFOCS stopping criterion is?  I don't see that detail described in the paper.\n\nBarring a better understanding...I guess I'd vote to mimic TFOCS.", "I believe this stopping criteria was added after the paper was written. It is documented on page 8 of the userguide (https://github.com/cvxr/TFOCS/raw/master/userguide.pdf) but unfortunately no explanation is provided. (The userguide also documents this as a <= test, while the current code uses <.) And unfortunately I couldnt find an explanation in the code or git history.\n\nI think the switch to absolute tolerance may be because a relative difference measurement could be less useful when the weights are extremely small, and 1 is a convenient cutoff point. (Using 1, the equation is simple and the interpretation is clear.) I believe [~mengxr] alluded to switching to an absolute tolerance at 1 already (https://github.com/apache/spark/pull/3636#discussion_r22078041) so he might be able to provide more information.\n\nWith regard to using the new weight norms as the basis for measuring relative weight difference, I think that if the convergence test passes using either the old or new weight norms, then the old and new norms are going to be very similar. It may not make a significant difference which test is used. (It may also be worth pointing out that in cases where the tolerance tests with respect to different old/new weights return different results, if the tolerance wrt new weights is met (and wrt old weights is not) then the weight norm increased slightly; if the tolerance wrt the old weights is met (and wrt new weights not) then we weight norm decreased slightly.)\n\nFinally, TFOCS adopts a policy of skipping the convergence test on the first iteration if the weights are unchanged. I believe this condition is based on implementation specific behavior and does not need to be adopted generally.\n", "Switching to absolute tolerance sounds reasonable.\n\nI agree about old vs. new weight norms not making much difference.  Let's go with what TFOCS does to facilitate your comparisons with the existing implementation.\n", "[~staple] [~josephkb] Thank you for pinging and inspiring information! I'll rewrite current patch based on your logic and codes. Thanks a lot.\n", "I appreciate it!", "There are couple stop criteria in TFOCS (implemented in https://github.com/cvxr/TFOCS/blob/master/private/tfocs_iterate.m and documented in section 2.4.4 of https://github.com/cvxr/TFOCS/raw/master/userguide.pdf)\n\nCopied from https://github.com/cvxr/TFOCS/blob/e34c0daeb136935d23b8df506de8b7b191f6b0a3/userguide.tex#L536:\n\n{code}\n\\subsubsection{Stopping criteria}\n\nThere are a variety of ways to decide when the algorithm should terminate:\n\\begin{trivlist}\n\\item \\texttt{tol}: TFOCS terminates when the iterates satisfy\n$\\|x_{k+1}-x_k\\|/\\max\\{1,\\|x_{k+1}\\|\\}\\leq\\texttt{tol}$.\nThe default value is $10^{-8}$; if set to zero or a negative value,\nthis criterion will never be engaged.\n\\item \\texttt{maxIts}: The maximum number of iterations the algorithm\nshould take; defaults to \\verb@Inf@.\n\\item \\texttt{maxCounts}: This option causes termination after a certain\nnumber of function calls or linear operations are made; see \n\\S\\ref{sec:opcounts}\nfor details. It defaults to \\verb@Inf@.\n\\item \\texttt{stopCrit}: Choose from one of several stopping criteria.\n    By default, \\texttt{stopCrit} is 1, which is our recommended stopping criteria\n    when not using the SCD model.\n    Setting this to 3 will use a stopping criteria applied to the dual value\n    (so this is only available in SCD models, where the dual is really the primal),\n    and setting this to 4 is similar but uses a relative error tolerance.\n    A value of 4 is recommended when using the SCD model with continuation.\n    For details, see the code in \\verb@private/tfocs_iterate.m@.\n\\item \\texttt{stopFcn}: This option allows you to supply one or more\nstopping criteria of your own design. To use it, set \\verb@stopFcn@\nmust be a function handle or a cell array of function handles. For\n\\verb@tfocs.m@, these function handles will be called as follows:\n\\begin{code_}\n\tstop = stopFcn( f, x );\n\\end{code_}\nwhere \\verb@f@ is the function value and \\verb@x@ is the current point.\n\\begin{code_}\n\tstop = stopFcn( f, z, x );\n\\end{code_}\nwhere \\verb@f@ is the current \\emph{dual} function value, \\verb@z@ is\nthe current dual point, and  \\verb@x@ is the current primal point.\nThe output should either be  \\verb@true@ or \\verb@false@; if\n\\verb@true@, the algorithm will stop.\n\nNote that the standard stopping criteria still apply, so the algorithm will halt\nwhen any of the stopping criteria are reached.  To ignore the standard stopping criteria,\nset \\texttt{stopCrit} to $\\infty$.\n\\end{trivlist}\n{code}", "I think it's safe to say this won't go into Spark core, especially as there is (at least some) support in an external package now. Shall we close this off?", "I'm fine with us closing it since there isn't momentum on it, but I could definitely see us reviving it in the future.  It could be worth exploring as a lightweight alternative to LBFGS."], "derived": {"summary": "Nesterov's accelerated first-order method is a drop-in replacement for steepest descent but it converges much faster. We should implement this method and compare its performance with existing algorithms, including SGD and L-BFGS.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "Implement Nesterov's accelerated first-order method - Nesterov's accelerated first-order method is a drop-in replacement for steepest descent but it converges much faster. We should implement this method and compare its performance with existing algorithms, including SGD and L-BFGS."}, {"q": "What updates or decisions were made in the discussion?", "a": "I'm fine with us closing it since there isn't momentum on it, but I could definitely see us reviving it in the future.  It could be worth exploring as a lightweight alternative to LBFGS."}]}}
{"project": "SPARK", "issue_id": "SPARK-1504", "title": "[streaming] Add deployment subsection to streaming", "status": "Resolved", "priority": "Blocker", "reporter": "Tathagata Das", "assignee": "Tathagata Das", "labels": [], "created": "2014-04-15T17:29:31.000+0000", "updated": "2014-05-05T22:29:46.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "[streaming] Add deployment subsection to streaming"}]}}
{"project": "SPARK", "issue_id": "SPARK-1505", "title": "[streaming] Add 0.9 to 1.0 migration guide for streaming receiver", "status": "Resolved", "priority": "Major", "reporter": "Tathagata Das", "assignee": "Tathagata Das", "labels": [], "created": "2014-04-15T17:29:58.000+0000", "updated": "2014-05-05T22:29:55.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "[streaming] Add 0.9 to 1.0 migration guide for streaming receiver"}]}}
{"project": "SPARK", "issue_id": "SPARK-1506", "title": "Documentation improvements for MLlib 1.0", "status": "Resolved", "priority": "Blocker", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-04-15T18:56:27.000+0000", "updated": "2014-05-07T00:01:59.000+0000", "description": "Proposed TOC:\n\nLinear algebra\n\n* vector and matrix\n* distributed matrix\n\nClassification and regression\n\n* generalized linear models (GLMs)\n* support vector machine (SVM)\n* decision tree\n* naive Bayes\n\nCollaborative filtering\n\n* alternating least squares (ALS)\n\nClustering\n\n* k-means\n\nDimension reduction\n\n* principal component analysis (PCA)\n\nOptimization\n\n* stochastic gradient descent\n* limited-memory BFGS (L-BFGS)\n\nMigration guide\n\n* from v0.9.x to v1.0", "comments": ["https://github.com/apache/spark/pull/422"], "derived": {"summary": "Proposed TOC:\n\nLinear algebra\n\n* vector and matrix\n* distributed matrix\n\nClassification and regression\n\n* generalized linear models (GLMs)\n* support vector machine (SVM)\n* decision tree\n* naive Bayes\n\nCollaborative filtering\n\n* alternating least squares (ALS)\n\nClustering\n\n* k-means\n\nDimension reduction\n\n* principal component analysis (PCA)\n\nOptimization\n\n* stochastic gradient descent\n* limited-memory BFGS (L-BFGS)\n\nMigration guide\n\n* from v0. 9.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Documentation improvements for MLlib 1.0 - Proposed TOC:\n\nLinear algebra\n\n* vector and matrix\n* distributed matrix\n\nClassification and regression\n\n* generalized linear models (GLMs)\n* support vector machine (SVM)\n* decision tree\n* naive Bayes\n\nCollaborative filtering\n\n* alternating least squares (ALS)\n\nClustering\n\n* k-means\n\nDimension reduction\n\n* principal component analysis (PCA)\n\nOptimization\n\n* stochastic gradient descent\n* limited-memory BFGS (L-BFGS)\n\nMigration guide\n\n* from v0. 9."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/422"}]}}
{"project": "SPARK", "issue_id": "SPARK-1507", "title": "Spark on Yarn: Add support for user to specify # cores for ApplicationMaster", "status": "Closed", "priority": "Major", "reporter": "Thomas Graves", "assignee": "Tao Wang", "labels": [], "created": "2014-04-15T19:23:28.000+0000", "updated": "2015-01-16T17:17:41.000+0000", "description": "Now that Hadoop 2.x can schedule cores as a resource we should allow the user to specify the # of cores for the ApplicationMaster.", "comments": ["User 'XuTingjun' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/3686", "User 'XuTingjun' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/3799", "User 'XuTingjun' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/3806", "User 'WangTaoTheTonic' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/4018"], "derived": {"summary": "Now that Hadoop 2. x can schedule cores as a resource we should allow the user to specify the # of cores for the ApplicationMaster.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Spark on Yarn: Add support for user to specify # cores for ApplicationMaster - Now that Hadoop 2. x can schedule cores as a resource we should allow the user to specify the # of cores for the ApplicationMaster."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'WangTaoTheTonic' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/4018"}]}}
{"project": "SPARK", "issue_id": "SPARK-1508", "title": "Add support for reading from SparkConf", "status": "Resolved", "priority": "Major", "reporter": "Michael Armbrust", "assignee": "Zongheng Yang", "labels": [], "created": "2014-04-15T22:28:54.000+0000", "updated": "2014-06-10T07:52:20.000+0000", "description": "Right now we have no ability to configure things in Spark SQL.  A good start would be passing a SparkConf though the planner such that users could override the number of partitions used during an Exchange.\n\nNote that while current spark confs are immutable after the context is created, we want some ability to change settings on a per query basis.", "comments": ["WIP PR: https://github.com/apache/spark/pull/956\n\nWe'd want to support:\n\n(1) API calls on SQLConf objects to get/set properties.\n(2) Support SQL/HiveQL SET commands of various kinds, e.g. \"SET key=val\", \"SET\", \"SET key\" in the sense that these should be reflected in / go through SQLConf objects.\n(3) Make sql(\"SET ...\").collect() (or perhaps also some other operations; also for hql()) return expected results, i.e. the key/val pairs. To do this there are some necessary refactorings for the QueryExecution pipeline.", "It is likely we will fix a related issue regarding calling collect on NativeCommands through the solution here."], "derived": {"summary": "Right now we have no ability to configure things in Spark SQL. A good start would be passing a SparkConf though the planner such that users could override the number of partitions used during an Exchange.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add support for reading from SparkConf - Right now we have no ability to configure things in Spark SQL. A good start would be passing a SparkConf though the planner such that users could override the number of partitions used during an Exchange."}, {"q": "What updates or decisions were made in the discussion?", "a": "It is likely we will fix a related issue regarding calling collect on NativeCommands through the solution here."}]}}
{"project": "SPARK", "issue_id": "SPARK-1509", "title": "add zipWithIndex zipWithUniqueId methods to java api", "status": "Resolved", "priority": "Minor", "reporter": "Guoqiang Li", "assignee": null, "labels": [], "created": "2014-04-16T05:00:15.000+0000", "updated": "2014-11-08T11:52:14.000+0000", "description": null, "comments": ["This was actually fixed in about 1.1"], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "add zipWithIndex zipWithUniqueId methods to java api"}, {"q": "What updates or decisions were made in the discussion?", "a": "This was actually fixed in about 1.1"}]}}
{"project": "SPARK", "issue_id": "SPARK-1510", "title": "Add Spark Streaming metrics source for metrics system", "status": "Resolved", "priority": "Major", "reporter": "Saisai Shao", "assignee": null, "labels": [], "created": "2014-04-16T08:29:47.000+0000", "updated": "2014-04-25T04:38:54.000+0000", "description": "Since Spark Streaming application is a long-run application, it is more important to monitor the current running status of this application. Now we have Streaming UI which can directly view the status of app, it is also necessary to export some of these metrics to metrics system, so that external tools can connect and monitor the status of app.", "comments": ["https://github.com/apache/spark/pull/545"], "derived": {"summary": "Since Spark Streaming application is a long-run application, it is more important to monitor the current running status of this application. Now we have Streaming UI which can directly view the status of app, it is also necessary to export some of these metrics to metrics system, so that external tools can connect and monitor the status of app.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add Spark Streaming metrics source for metrics system - Since Spark Streaming application is a long-run application, it is more important to monitor the current running status of this application. Now we have Streaming UI which can directly view the status of app, it is also necessary to export some of these metrics to metrics system, so that external tools can connect and monitor the status of app."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/545"}]}}
{"project": "SPARK", "issue_id": "SPARK-1511", "title": "Update TestUtils.createCompiledClass() API to work with creating class file on different filesystem", "status": "Closed", "priority": "Minor", "reporter": "YE", "assignee": null, "labels": ["starter"], "created": "2014-04-16T13:21:42.000+0000", "updated": "2014-04-17T14:07:52.000+0000", "description": "The createCompliedClass method uses java File.renameTo method to rename source file to destination file, which will fail if source and destination files are on different disks (or partitions).\n\nsee http://apache-spark-developers-list.1001551.n3.nabble.com/Tests-failed-after-assembling-the-latest-code-from-github-td6315.html for more details.\n\nUse com.google.common.io.Files.move instead of renameTo will solve this issue.", "comments": ["Close this issue.\npr https://github.com/apache/spark/pull/427 solves it."], "derived": {"summary": "The createCompliedClass method uses java File. renameTo method to rename source file to destination file, which will fail if source and destination files are on different disks (or partitions).", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Update TestUtils.createCompiledClass() API to work with creating class file on different filesystem - The createCompliedClass method uses java File. renameTo method to rename source file to destination file, which will fail if source and destination files are on different disks (or partitions)."}, {"q": "What updates or decisions were made in the discussion?", "a": "Close this issue.\npr https://github.com/apache/spark/pull/427 solves it."}]}}
{"project": "SPARK", "issue_id": "SPARK-1512", "title": "improve spark sql to support table with more than 22 fields", "status": "Resolved", "priority": "Major", "reporter": "Fei Wang", "assignee": null, "labels": [], "created": "2014-04-16T15:48:11.000+0000", "updated": "2014-04-29T22:45:33.000+0000", "description": "spark sql use case class to define a table, so 22 fields limit in case classes lead to spark sql not support wide(more than 22 fields) tables. wide table is  common in many cases", "comments": ["Now that we have updated the docs to talk about creating custom Product classes, I'm going to mark this as resolved."], "derived": {"summary": "spark sql use case class to define a table, so 22 fields limit in case classes lead to spark sql not support wide(more than 22 fields) tables. wide table is  common in many cases.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "improve spark sql to support table with more than 22 fields - spark sql use case class to define a table, so 22 fields limit in case classes lead to spark sql not support wide(more than 22 fields) tables. wide table is  common in many cases."}, {"q": "What updates or decisions were made in the discussion?", "a": "Now that we have updated the docs to talk about creating custom Product classes, I'm going to mark this as resolved."}]}}
{"project": "SPARK", "issue_id": "SPARK-1513", "title": "Specialized ColumnType for Timestamp", "status": "Resolved", "priority": "Major", "reporter": "Cheng Lian", "assignee": null, "labels": ["compression"], "created": "2014-04-16T16:38:53.000+0000", "updated": "2014-06-26T21:18:39.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Specialized ColumnType for Timestamp"}]}}
{"project": "SPARK", "issue_id": "SPARK-1514", "title": "Standardize process for creating Spark packages", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "labels": [], "created": "2014-04-16T16:50:42.000+0000", "updated": "2014-05-21T23:17:04.000+0000", "description": "Over time we've got (a) make-distribution.sh (b) maven distribution targets (c) create-release.sh script in /dev. This is pretty confusing for downstream packagers.\n\nWe should have a single way to package releases, probably using a modified maven distribution.", "comments": ["What was the resolution here?  is there a pull request that went in?", "+1 [~tgraves] - what occurred in the branch-1.0 branch to move this to Resolved?\n\nWorking with make-distribution.sh now and wondering if there will be a unification at some point in the future of building Spark{,-Mesos} artifacts for distribution", "The create-release script and the make-distribution script have been consolidated (the former now calls the latter) which was the main point of this JIRA. This was fixed in master and 1.0 a while back. Here is an example:\n\nhttps://github.com/apache/spark/blob/master/dev/create-release/create-release.sh#L98\n\nSo now make-distribution is the official way of making binary distributions. This is what we used for all of the 1.0 release candidates and has been extensively tested."], "derived": {"summary": "Over time we've got (a) make-distribution. sh (b) maven distribution targets (c) create-release.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Standardize process for creating Spark packages - Over time we've got (a) make-distribution. sh (b) maven distribution targets (c) create-release."}, {"q": "What updates or decisions were made in the discussion?", "a": "The create-release script and the make-distribution script have been consolidated (the former now calls the latter) which was the main point of this JIRA. This was fixed in master and 1.0 a while back. Here is an example:\n\nhttps://github.com/apache/spark/blob/master/dev/create-release/create-release.sh#L98\n\nSo now make-distribution is the official way of making binary distributions. This is what we used for all of the 1.0 release candidates and has been extensively tested."}]}}
{"project": "SPARK", "issue_id": "SPARK-1515", "title": "Specialized ColumnTypes for Array, Map and Struct", "status": "Resolved", "priority": "Major", "reporter": "Cheng Lian", "assignee": null, "labels": ["compression"], "created": "2014-04-16T16:51:41.000+0000", "updated": "2016-01-12T13:43:45.000+0000", "description": null, "comments": ["Assuming this is obsolete"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Specialized ColumnTypes for Array, Map and Struct"}, {"q": "What updates or decisions were made in the discussion?", "a": "Assuming this is obsolete"}]}}
{"project": "SPARK", "issue_id": "SPARK-1516", "title": "Yarn Client should not call System.exit, should throw exception instead.", "status": "Resolved", "priority": "Major", "reporter": "DB Tsai", "assignee": "John Zhao", "labels": [], "created": "2014-04-16T17:40:20.000+0000", "updated": "2014-07-23T07:10:43.000+0000", "description": "People submit spark job inside their application to yarn cluster using spark yarn client, and it's not desirable to call System.exit in yarn client which will terminate the parent application as well.\n\nWe should throw exception instead, and people can determine which action they want to take given the exception.", "comments": ["I am working on this. ", "PR: https://github.com/apache/spark/pull/490", "PR for branch-0.9: https://github.com/apache/spark/pull/1099"], "derived": {"summary": "People submit spark job inside their application to yarn cluster using spark yarn client, and it's not desirable to call System. exit in yarn client which will terminate the parent application as well.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Yarn Client should not call System.exit, should throw exception instead. - People submit spark job inside their application to yarn cluster using spark yarn client, and it's not desirable to call System. exit in yarn client which will terminate the parent application as well."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR for branch-0.9: https://github.com/apache/spark/pull/1099"}]}}
{"project": "SPARK", "issue_id": "SPARK-1517", "title": "Publish nightly snapshots of documentation, maven artifacts, and binary builds", "status": "Resolved", "priority": "Critical", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "labels": [], "created": "2014-04-16T19:07:40.000+0000", "updated": "2018-11-16T16:35:25.000+0000", "description": "Should be pretty easy to do with Jenkins. The only thing I can think of that would be tricky is to set up credentials so that jenkins can publish this stuff somewhere on apache infra.\n\nIdeally we don't want to have to put a private key on every jenkins box (since they are otherwise pretty stateless). One idea is to encrypt these credentials with a passphrase and post them somewhere publicly visible. Then the jenkins build can download the credentials provided we set a passphrase in an environment variable in jenkins. There may be simpler solutions as well.", "comments": ["We should revisit this for the 1.2.0 release cycle, since this would have solved the issue that we ran into with bumping the SNAPSHOT version before the 1.1 artifacts were published on Maven.", "Hey [~pwendell], any updates here? The disconnect between the content of the github README and the \"/latest/\" published docs leading up to the 1.2.0 release continues to cast a shadow and new divergence is set to begin as we move further from having just cut a release.\n\nAs was [recently pointed out on the dev list|http://apache-spark-developers-list.1001551.n3.nabble.com/Starting-with-Spark-tp9908p9925.html], [my|https://github.com/apache/spark/commit/4ceb048b38949dd0a909d2ee6777607341c9c93a#diff-04c6e90faac2675aa89e2176d2eec7d8] and [Reynold's|https://github.com/apache/spark/commit/342b57db66e379c475daf5399baf680ff42b87c2#diff-04c6e90faac2675aa89e2176d2eec7d8] \"fixes\" to previously-broken links in the README became broken links when the 1.2.0 docs were cut, as [~srowen] [warned would happen|https://github.com/apache/spark/commit/342b57db66e379c475daf5399baf680ff42b87c2#commitcomment-8250912] (one is fixed [here|https://github.com/apache/spark/pull/3802/files], the other remains broken on the README today).\n\nI still believe that the correct fix is to have the README point at docs that are published with each Jenkins build, per this JIRA and [our previous discussion about it|http://apache-spark-developers-list.1001551.n3.nabble.com/Spurious-test-failures-testing-best-practices-tt9560.html#a9568].\n\nEven better would be to publish nightly docs *and* remove any pretense that the github README is a canonical source of documentation, in favor of just linking to the /latest/ published docs. Let me know if you want me to file that as a sub-task here.", "Recap: old URL was \"building-with-maven.html\", new URL is \"building-spark.html\" to match a rename and content change of the page itself a few months ago. There should be a redirect from the former to latter. Until the 1.2.0 site was published, there was no building-spark.html page live on the site. So README.md had to link to building-with-maven.html, with the intent that after 1.2.0 this would just redirect to building-spark.html.\n\nI'm not sure why, but the redirect isn't working. It redirects to http://spark.apache.org/building-spark.html . It seems like this is some default mechanism, and the redirector that the plugin is supposed to generate isn't present or something. Could somehow be my mistake but I certainly recall it worked on my local build of the site or else I never would have proposed it.\n\nSo yes one direct hotfix is to change links to the old page to links to the new page. Only one of two links in README.md was updated. It's easy to fix the other.\n\nThe README.md that you see on github.com is always going to be from master, but people are going to encounter the page and sometimes expect it corresponds to a latest stable release. (You can always view README.md from the branch you want of course, if you know what you're doing.) Yes, for this reason I agree that it's best to make it mostly pointers to other information, and I think that was already the intent of changes that included the renaming I alluded to above. IIRC there was a desire to not strip down README.md further and leave some minimal, hopefully fairly unchanging, info there.\n\nWhether there should be nightly builds of the site is a different question. If you linked to \"nightly\" instead of \"latest\" I suppose you'd have more of the same problem, no? people finding the github site and perhaps thinking they are seeing latest stable docs? On the other hand, it would at least be more internally consistent. On the other other hand, would you have to change the links to the stable URLs for release and then back as part of the release process? I had thought just linking to latest stable release docs was simple and fine.", "Agreed that the redirect you speak of should exist / be fixed; a separate JIRA should be filed for that.\n\nbq. Whether there should be nightly builds of the site is a different question.\n\nMy understanding is that there has been consensus at a few points that this is a good idea. The main concern you've voiced is the risk that people will land on the github README when looking for stable/release docs, and:\n# find \"nightly\" info directly in the README content (and not understand it to be incorrect (too up-to-date) for their purposes), or\n# inadvertently follow a link to published \"nightly\" docs.\n\nre: 1, in my last post I suggested doing away with the pretense that the github README will directly contain Spark documentation, and replacing its current content with links to the relevant published docs, potentially *both* nightly and stable.\n\nre: 2, as long as the README's links to \"nightly\" and \"stable\" docs sites are clearly marked, this should not be a problem. Users already must have a minimal level of understanding of what version of Spark docs they want to look at.\n", "Does this handle SparkR doc build (Which requires R)", "[~felixcheung], nightly doc builds are already being published at https://people.apache.org/~pwendell/spark-nightly/spark-master-docs/latest/ and these include the R docs.", "User 'pwendell' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/7411", "Hey [~pwendell], thanks for continuing to push on this. \n\nA workflow I'd like to see supported (and maybe it already is; please let me know if so) is to more easily fetch these artifacts (both [Maven snapshots|https://repository.apache.org/content/repositories/snapshots/org/apache/spark/] and [bundled release {{.tgz}} files|https://people.apache.org/~pwendell/spark-nightly/]) by their git SHAs. \n\nFor the Maven snapshots, I'd like to be able to just change the Spark version in a downstream project's POM to a git SHA and have Maven fetch the Spark JARs for that SHA (assuming it's one that has been built by the tools here); I'm fine with the (presumably necessary) step on my end of adding a Maven repository to make this work, either per-project or globally.\n\nToday, the Maven snapshots at e.g. https://repository.apache.org/content/repositories/snapshots/org/apache/spark/spark-core_2.10/1.5.0-SNAPSHOT/ all seem to be uniquely ID'd by timestamps that I don't know how to get useful information out of, which has precluded my using them.\n\nOn the bundled releases front, I see that the git SHA is being added to the folders at https://people.apache.org/~pwendell/spark-nightly/spark-master-bin/:\n\n!http://cl.ly/image/0o111a1o0U2N/Screen%20Shot%202015-08-06%20at%201.08.18%20PM.png!\n\nbut those don't seem to stick around more than a day or so? Additionally, as that screenshot shows, there are 3 copies of one SHA there right now, and only 2 SHAs total.\n\nI rolled some of my own scripts for cloning, building, and selecting specific Spark versions locally at [ryan-williams/spark-helpers|https://github.com/ryan-williams/spark-helpers], which currently fetches release {{.tgz}} files for released Spark versions, but for arbitrary Spark SHAs there doesn't seem to be an easy way to download a pre-built Spark, so I am just cloning them and running {{mvn package}}.\n\nLet me know if you have thoughts about exposing built artifacts for more SHAs, the workflows I've described here, etc. Thanks again!\n", "Hey Ryan,\n\nFor the maven snapshot releases - unfortunately we are constrained by maven's own SNAPSHOT version format which doesn't allow encoding anything other than the timestamp. It's just not supported in their SNAPSHOT mechanism. However, one thing we could see is whether we can align the timestamp with the time of the actual spark commit, rather than the time of publication of the SNAPSHOT release. I'm not sure if maven lets you provide a custom timestamp when publishing. If we had that feature users could look at the Spark commit log and do some manual association.\n\nFor the binaries, the reason why the same commit appears multiple times is that we do the build every four hours and always publish the latest one even if it's a duplicate. However, this could be modified pretty easily to just avoid double-publishing the same commit if there hasn't been any code change. Maybe create a JIRA for this?\n\nIn terms of how many older versions are available, the scripts we use for this have a tunable retention window. Right now I'm only keeping the last 4 builds, we could probably extend it to something like 10 builds. However, at some point I'm likely to blow out of space in my ASF user account. Since the binaries are quite large, I don't think at least using ASF infrastructure it's feasible to keep all past builds. We have 3000 commits in a typical Spark release, and it's a few gigs for each binary build.", "h3. Maven snapshots\n\nI hear your point that idiomatic Maven-snapshot workflows are not well suited to this task. Something I've been doing instead is running commands like this from within a Spark repo:\n\n{code}\n$ sha=$(git --no-pager log --no-walk --format=\"%h\" HEAD)\n$ mvn versions:set -DgenerateBackupPoms=false -DnewVersion=$sha\n$ mvn install -DskipTests\n{code}\n\nThis renames the version in all POMs to the abbreviated SHA of {{HEAD}}, builds Spark, and installs the SHA-namespaced artifacts in my local Maven cache, at e.g. {{~/.m2/repository/org/apache/spark/spark-core_2.10/901dbd0}}.\n\nThen I just put {{901dbd0}} as the version in some other project and, voila, I can link against arbitrary Spark SHAs, have many co-exist in my local Maven cache without them all being named {{1.x.y-SNAPSHOT}}, etc. [Here's an example|https://github.com/hammerlab/pageant/blob/56bff88f426dd69083424a91cc35099a2a157f10/pom.xml#L30] where I needed a patched Spark before {{1.4.1}} was released with the fix I needed.\n\nCould any existing continuous build infrastructure be modified to run the {{mvn versions:set}} command above and publish artifacts to some Maven repository, ID'd by SHA?\n\nh3. Binaries\nIt also makes sense that your ASF user account will not scale for this purpose :) OTOH, it should be possible to store these cheaply somewhere. {{spark-1.4.1-bin-hadoop2.4.tgz}} is ~234MB and there are ~4000 SHAs from 1.2.0 to 1.5.0, so hosting every single SHA in that range would be a few TB, afaict. \n\nAnalogous to my previous question: could any existing continuous build infrastructure be modified to run the {{mvn versions:set}} command above and send upload binaries somewhere that could hold more than just the last few? These binaries are apparently already being generated, and mostly deleted in ~24hrs as your ASF userdir runs out of space?", "Hey Ryan,\n\nIIRC - the Apache snapshot repository won't let us publish binaries that do not have SNAPSHOT in the version number. The reason is it expects to see timestamped snapshots so its garbage collection mechanism can work. We could look at adding sha1 hashes, before SNAPSHOT, but I think there is some chance this would break their cleanup.\n\nIn terms of posting more binaries - I can look at whether Databricks or Berkeley might be able to donate S3 resources for this, but it would have to be clearly maintained by those organizations and not branded as official Apache releases or anything like that.", "That all makes sense, thanks.\n\nI guess I was imagining \"we\" could publish the SHA'd \"snapshots\" to a Maven repository other than the Apache snapshot repository, especially if the latter has rules that make this inconvenient.\n\nUnderstood that the binaries (and Maven artifacts) would have to be clearly branded as not official Apache releases.\n\nIf I came up with a URL that binaries could be uploaded to, what would have to change to make it happen? Likewise if I found a Maven repository that could host these artifacts?", "Ancient JIRA here ... I think we did publish these and don't know that we do anymore. Partly there was concern in the past about 'featuring' unreleased code. I don't think we're going to do anything more here."], "derived": {"summary": "Should be pretty easy to do with Jenkins. The only thing I can think of that would be tricky is to set up credentials so that jenkins can publish this stuff somewhere on apache infra.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Publish nightly snapshots of documentation, maven artifacts, and binary builds - Should be pretty easy to do with Jenkins. The only thing I can think of that would be tricky is to set up credentials so that jenkins can publish this stuff somewhere on apache infra."}, {"q": "What updates or decisions were made in the discussion?", "a": "Ancient JIRA here ... I think we did publish these and don't know that we do anymore. Partly there was concern in the past about 'featuring' unreleased code. I don't think we're going to do anything more here."}]}}
{"project": "SPARK", "issue_id": "SPARK-1518", "title": "Spark master doesn't compile against hadoop-common trunk", "status": "Resolved", "priority": "Critical", "reporter": "Marcelo Masiero Vanzin", "assignee": "Colin McCabe", "labels": [], "created": "2014-04-16T20:23:54.000+0000", "updated": "2014-06-04T22:56:55.000+0000", "description": "FSDataOutputStream::sync() has disappeared from trunk in Hadoop; FileLogger.scala is calling it.\n\nI've changed it locally to hsync() so I can compile the code, but haven't checked yet whether those are equivalent. hsync() seems to have been there forever, so it hopefully works with all versions Spark cares about.", "comments": ["As the hadoop API changes, some methods have been removed.\nThe hadoop related in spark core Independence to new modules. As in the case of yarn.", "{{FSDataOutputStream::sync()}} has been deprecated for a while, and finally got removed in HADOOP-8124.  It was a synonym for {{hflush}}, so replacing it with that function would probably be more appropriate.  You usually don't want {{hsync}}, since it forces an {{fsync}} on each datanode that contains the file.\n\nIt looks like there's some other YARN-related stuff that still fails to build once I make this fix, though...", "[~pwendell], I can take a look at this.  Can you assign it to me?", "Hey [~cmccabe], what is the oldest version of hadooop that contains hflush? /cc [~andrewor] who IIRC looked into this a bunch when writing the logger.", "bq. Hey Colin Patrick McCabe, what is the oldest version of hadooop that contains hflush? /cc Andrew Or who IIRC looked into this a bunch when writing the logger.\n\nThe oldest Apache release I know about with hflush is Hadoop 0.21.", "Ah okay. I'm not sure what the oldest version Hadoop that spark Spark compiles against pre 0.21, but it's worth knowing whether this change would cause us to drop support for some of the older versions.", "I think it's very, very, very unlikely that anyone will want to run Hadoop 0.20 against Spark.  Why don't we:\n* fix the compile against Hadoop trunk\n* wait for someone to show up who wants compatibility with hadoop 0.20 before we work on it?\n\nIt seems like if there is interest in Spark on Hadoop 0.20, there will quickly be a patch submitted to get it compiling there.  If there is no such interest, then we'll be done here without doing a lot of work up front.\n\nIf you agree then I'll create a pull req.  I have verified that fixing the flush thing un-breaks the compile on master.", "0.20.x stopped in early 2010. It is ancient.", "I wonder if we should have a discussion on the mailing list about the oldest version of Hadoop we should support.  I would argue that it should be 0.23.  Yahoo! is still using that version.  Perhaps other people have more information than I do, though.\n\nIf we decide to support 0.20, I will create a patch that does this using reflection.  But I'd rather get your guys' opinion on whether that make sense first.", "Hmm, may I suggest a different approach?\n\nAndrew, who wrote the code, might have more info. But from my understanding, the flushes were needed because the history server might read logs from applications that were not yet finished. So the flush was a best-effort to avoid having the HS read files that contained partial JSON objects (and fail to parse them).\n\nBut since then the HS was changed to only read logs from finished applications. I think it's safe to assume that finished applications are not writing to the event log anymore, so the above scenario doesn't exist.\n\nSo could we just get rid of the explicit flush instead?", "RE: Hadoop versions, in my reckoning of the twisted world of Hadoop versions, the 0.23.x branch is still active and so is kind of later than 1.0.x. It may be easier to retain 0.23 compatibility than 1.0.x for example.", "Re: versioning - I was just asking whether this changes the oldest version we are compatible with. That's a question we should ask of all Hadoop patches. I just tested Spark and it doesn't compile against 0.20.X, so this is a no-op in terms of compatibility anyways.\n\nIt would be good to list the oldest upstream Hadoop version we support. Many people still run Spark against Hadoop 1.X/CDH3 variants. We get high download rates for those pre-built packages. I think this is a little different than e.g. CDH where people upgrade all components at the same time... people download newer versions of Spark and run it with old filesystems very often.", "bq. It would be good to list the oldest upstream Hadoop version we support. Many people still run Spark against Hadoop 1.X/CDH3 variants. We get high download rates for those pre-built packages. I think this is a little different than e.g. CDH where people upgrade all components at the same time... people download newer versions of Spark and run it with old filesystems very often.\n\nThanks, Patrick.  This is useful info... I didn't realize there was still interest in running Spark against CDH3.  Certainly we'll never support it directly, since CDH3 was end-of-lifed last year.  So we don't really support doing anything on CDH3... except upgrading it to CDH4 (or hopefully, 5 which has Spark. :)\n\nbq. Re: versioning - I was just asking whether this changes the oldest version we are compatible with. That's a question we should ask of all Hadoop patches. I just tested Spark and it doesn't compile against 0.20.X, so this is a no-op in terms of compatibility anyways.\n\nIt sounds like we're good to go on replacing the {{flush}} with {{hsync}} then?  I notice you marked this as \"critical\" recently; do you think it's important to 1.0?", "bq. Thanks, Patrick. This is useful info... I didn't realize there was still interest in running Spark against CDH3. Certainly we'll never support it directly, since CDH3 was end-of-lifed last year. So we don't really support doing anything on CDH3... except upgrading it to CDH4 (or hopefully, 5 which has Spark. \n\nYeah, the upstream project tries pretty hard to maintain compatibility with as many Hadoop versions as possible since we see many people running even fairly old ones in the wild. Of course, I'm sure Cloudera will commercially support only the most recent ones.\n\nbq. Re: versioning - I was just asking whether this changes the oldest version we are compatible with. That's a question we should ask of all Hadoop patches. I just tested Spark and it doesn't compile against 0.20.X, so this is a no-op in terms of compatibility anyways.\n\nIt won't make the 1.0.0 window but we should get it into a 1.0.X release. My concern is that once newer Hadoop versions come out, I want people to be able to compile Spark against them. Again, since Spark is distributed independently from HDFS, this is something that happens a lot, people try to compile older Spark releases against newer Hadoop releases.", "Sounds good.  https://github.com/apache/spark/pull/898", "Re: versioning one more time, really supporting a bunch of versions may get costly. It's already tricky to manage two builds times YARN-or-not, Hive-or-not, times 4 flavors of Hadoop. I doubt the assemblies are yet problem-free in all cases. \n\nIn practice it look like one generic Hadoop 1, Hadoop 2, and CDH 4 release is produced, and 1 set of Maven artifact. (PS again I am not sure Spark should contain a CDH-specific distribution? realizing it's really a proxy for a particular Hadoop combo. Same goes for a MapR profile, which is really for vendors to maintain) That means right now you can't build a Spark app for anything but Hadoop 1.x with Maven, without installing it yourself, and there's not an official distro for anything but two major Hadoop versions. Support for niche versions isn't really there or promised anyway, and fleshing out \"support\" may make doing so pretty burdensome. \n\nThere is no suggested action here; if anything I suggest that the right thing is to add Maven artifacts with classifiers, add a few binary artifacts, subtract a few vendor artifacts, but this is a different action.", "bq. Re: versioning one more time, really supporting a bunch of versions may get costly. It's already tricky to manage two builds times YARN-or-not, Hive-or-not, times 4 flavors of Hadoop. I doubt the assemblies are yet problem-free in all cases.\n\nI think in this particular case, we can use reflection to support both Hadoop 1.X and newer stuff.\n\nbq. I am not sure Spark should contain a CDH-specific distribution? realizing it's really a proxy for a particular Hadoop combo. Same goes for a MapR profile, which is really for vendors to maintain)\n\nI agree 100%.  We should keep vendor stuff out of the Apache release.  Vendors can create their own build setups (that's what they get paid to do, after all.)\n\nbq. There is no suggested action here; if anything I suggest that the right thing is to add Maven artifacts with classifiers, add a few binary artifacts, subtract a few vendor artifacts, but this is a different action.\n\nIf you have some ideas for how to improve the Maven build, it could be worth creating a JIRA.  I think you're right that we need to make it more flexible so that people can build against more versions without editing the pom.  It might be helpful to look at how HBase handles this in its {{pom.xml}} files.", "bq. In practice it look like one generic Hadoop 1, Hadoop 2, and CDH 4 release is produced, and 1 set of Maven artifact. (PS again I am not sure Spark should contain a CDH-specific distribution? realizing it's really a proxy for a particular Hadoop combo. Same goes for a MapR profile, which is really for vendors to maintain) That means right now you can't build a Spark app for anything but Hadoop 1.x with Maven, without installing it yourself, and there's not an official distro for anything but two major Hadoop versions. Support for niche versions isn't really there or promised anyway, and fleshing out \"support\" may make doing so pretty burdensome.\n\nWe need to update the list of binary builds for Spark... some are getting outdated. The workflow for people building Spark apps is that they write their app against the Spark API's in Maven central (they can do this no matter which cluster they want to run on). To run the app, If they just want to run it locally they can spark-submit from any compiled package of Spark, or they can use their build tool to just run it. If they want to submit it to a cluster, users need to have a Spark package compiled for the Hadoop version on the cluster. Because of this we distribute pre-compiled builds to allow people to avoid ever having to compile Spark.\n\nIn terms of vendor-specific builds, we've done this because users asked for it. It's useful if, e.g. a user wants to submit a Spark job to a CDH or MapR cluster. Or run spark-shell locally and read data from a CDH HDFS cluster. That's the main use case we want to support.\n\nI don't know what it means that you \"can't build a Spark app\" for Hadoop 2.X. Building a Spark app is intentionally decoupled from the process of submitting an app to a cluster. We want users to be able to build Spark apps that they can run on e.g. different versions of Hadoop.", "\"they write their app against the Spark API's in Maven central (they can do this no matter which cluster they want to run on)\" \n\nYeah this is the issue. OK, if I compile against Spark artifacts as a runtime dependency and submit an app to the cluster, it should be OK no matter what build of Spark is running. The binding from Spark to Hadoop is hidden from the app.\n\nI am thinking of the case where I want to build an app that is a client of Spark -- embedding it. Then I am including the client of Hadoop for example. I have to match my cluster than and there is no Hadoop 2 Spark artifact.\n\nAm I missing something big here? that's my premise about why there would ever be a need for different artifacts. It's the same use case as in Sandy's blog: http://blog.cloudera.com/blog/2014/04/how-to-run-a-simple-apache-spark-app-in-cdh-5/", "Sean, the model for linking to Hadoop has been that users also add a dependency on hadoop-client if they want to access HDFS for the past few releases. See http://spark.apache.org/docs/latest/scala-programming-guide.html#linking-with-spark for example. This model is there because Hadoop itself has decided to create the hadoop-client Maven artifact as a way to get apps to link to it. It works for all the recent versions of Hadoop as far as I know -- users don't have to link against a custom-built Spark for their distro.\n\nRegarding binary builds on apache.org, we want users to be able to start using Spark as conveniently as possible on any distribution. It is the goal of the Apache project to have people use Apache Spark as easily as possible.", "Yes Matei that's what I'm getting at. Spark is a client of Hadoop, so if I use Spark, and Spark uses Hadoop, then I have to match the Hadoop that Spark uses to the cluster. It's not just if my app uses HDFS directly. I can manually override hadoop-client, although, I'd have to reproduce a lot of the dependency-graph manipulation in Spark's build to make it work.\n\nIn Sandy's blog post example he's just running the code on the cluster and pointing at the matched Spark/Hadoop jars already there. That's also a solution that will work for a lot of use cases. I accept that the use case I have in mind, which is adding Spark to a larger stand-alone app, is not everyone's use case, although it's not crazy. It doesn't work out if instead the Spark/Hadoop jars are packaged together into an assembly and run that way.\n\nI agree overriding the Hadoop dependency is a solution, and accept that Spark shouldn't necessarily bend over backwards for these Hadoop issues, but this does go back to your point about accessibility. Right now I think anyone that wants to do what I'm doing for any Hadoop 2 app, and doesn't want to make a custom build or manually override dependencies, will just point at Cloudera's \"0.9.0-cdh5.0.1\" even if not using CDH. That felt funny.\n\nApologies if I have somehow totally missed something. I've talked too much, thanks for hearing out the use case. Maybe best to see if this is actually an issue anyone shares.", "Sorry, I'm still not sure I understand what you're asking for -- maybe I missed it above. Are you worried that the Spark assembly on the cluster has to be pre-built against Hadoop? We could perhaps make it find stuff out of HADOOP_HOME, but then it wouldn't work for users that don't have a Hadoop installation, which is a lot of users. For client apps, it's really enough to add that hadoop-client dependency. No other manipulation is needed.\n\nIf you want to build a client app that automatically works with multiple versions of Hadoop, you can also package it with Spark and hadoop-client marked as \"provided\" and use spark-submit to put the Spark assembly on your cluster in the classpath. Then it will work with whatever version that was built against. But you need to specify hadoop-client when you run without spark-submit if you want to talk to the version of HDFS in your cluster (e.g. you're testing the app on your laptop and trying to make it read from HDFS).", "Heh, I think the essence is: at least one more separate Maven artifact, under a different classifier, for Hadoop 2.x builds. If you package that, you get Spark and everything it needs to work against a Hadoop 2 cluster. Yeah I see that you're suggesting various ways to push the app to the cluster, where it can bind to the right version of things, and that may be the right-est way to think about this. I had envisioned running a stand-alone app on a machine that is not part of the cluster, that is a client of it, and this means packaging in the right Hadoop client dependencies, and Spark already declares how it wants to include these various Hadoop client versions -- it's more than just including hadoop-client -- so wanted to leverage that. Let's see if this actually turns out to be a broader request though.", "Okay, got it. But this only applies to you running the job on your laptop, right? Because otherwise you'll get the right Hadoop via the installation on the cluster.\n\nFor this use case I still think it's fine to require use of hadoop-client. It's been like that for the past 2 releases and nobody has asked questions about it. It's just one more entry to add to your pom.xml.\n\nThe concrete problem is that Hadoop has been extremely fickle with compatibility even within a major release series (1.x or 2.x). HDFS protocol versions change and you can't access the cluster, YARN versions change, etc. I don't think there's a single release I'd call \"Hadoop 2\", and it would be confusing to users to link to the \"Hadoop 2\" artifact and not have it run on their cluster.", "BTW one other thing is that in 1.0, you can also use spark-submit in local mode to get your locally installed Spark. So people will be able to yum install spark-from-their-vendor, build their app with just spark-core, and then run it with the spark-submit on their PATH.", "bq. The concrete problem is that Hadoop has been extremely fickle with compatibility even within a major release series (1.x or 2.x). HDFS protocol versions change and you can't access the cluster, YARN versions change, etc. I don't think there's a single release I'd call \"Hadoop 2\", and it would be confusing to users to link to the \"Hadoop 2\" artifact and not have it run on their cluster.\n\nI know that there was an RPC compatibility break between {{2.1.1-beta}} and {{2.1.0-beta}}.  Around the 2.3 time-frame, Hadoop decided to freeze the RPC format at version 9, and try to maintain compatibility going forward.  You are right that bundling the appropriate version of the Hadoop client is the usual approach that projects which depend on Hadoop take, exactly to avoid these kinds of worries.", "bq. I don't think there's a single release I'd call \"Hadoop 2\", and it would be confusing to users to link to the \"Hadoop 2\" artifact and not have it run on their cluster.\n\nWhile Hadoop releases have not historically been amazing at maintaining compatibility, I think this a bit of an overstatement.  There is a definitive Hadoop 2, which became GA starting at 2.2.  It has a set of public/stable APIs that have not been broken since then, a promise not to break them for the remainder of 2.x, and a comprehensive compatibility guide that describes exactly what \"break\" means - http://hadoop.apache.org/docs/r2.3.0/hadoop-project-dist/hadoop-common/Compatibility.html.  All the major distributions (CDH, Pivotal, HDP, I think MapR?) support these APIs.  The Hadoop 2 releases that preceded 2.2 were labeled alpha and beta, and did not come with these same guarantees.\n\nWhile, with my Cloudera hat on, I'd love for the CDH5 Spark artifacts to become the canonical Spark Hadoop 2 artifacts, with my Apache hat on, I do see some value in publishing Spark Hadoop 2 artifacts.  Though, as Matei and Patrick pointed out, these only matter when bundling Spark inside your own application.  In most cases, it's better to point to Spark jars installed on one's laptop or cluster.\n", "Sorry for one more message here to reply to Matei -- yes it's the \"laptop\" use case except I'd describe that as a not-uncommon production deployment! it's the embedded-client scenario. It is more than adding one hadoop-client dependency, because you need to emulate the excludes, etc that Spark has to. (But yeah then it works.) I agree supporting a bunch of Hadoop versions gets painful, as a result. This was why I was suggesting way up top that supporting old versions may become more trouble than its worht.", "Got it, the excludes have indeed gotten more painful, and I can see that being a problem. Maybe the solution would be to publish some kind of \"spark-core-hadoopX\" for each version of Hadoop, which depends on hadoop-client and spark-core. But then we'll need a list of supported versions to publish for. By the way AFAIK Maven classifiers do not solve this issue, as versions of an artifact with different classifiers must have the same dependency tree (they can differ in other things, e.g. maybe they're compiled on different Java versions).\n\nBTW, in terms of the Hadoop 2 thing, I just meant that there was a lot of variability before the community decided to go GA, and unfortunately a lot of users are on older versions. (Ironically sometimes because of this volatility). I definitely appreciate the move to GA and the compatibility policies. When I said Hadoop 2, I meant that, for example, do you consider 0.23 to be Hadoop 2? It's YARN-based and it was (and still is AFAIK) widely used at Yahoo. What about 2.0.x? Some users are on that too. What about CDH4? Its version number is 2.0.0-something. In any case we will be sensible about old versions, but my philosophy is always to support the broadest range possible, and from everything I've seen it's paid off -- users appreciate when you do not force their hand to upgrade. This is why Yahoo for example continues to be our biggest contributor on YARN support, even though their YARN is pretty different.", "It seems reasonable to have a list of supported versions in the Maven build.  That wouldn't exclude people from building against other versions, of course, but they might have to supply a maven definition via {{\\-D}} or something.\n\nbq. For example, do you consider 0.23 to be Hadoop 2? It's YARN-based and it was (and still is AFAIK) widely used at Yahoo\n\n0.23 is not Hadoop 2.  It's a branch that Yahoo! uses internally.  Everyone else has moved on to branch-2 (Hortonworks, Cloudera, WANDisco, Intel, etc. etc.)  Yahoo! also has some clusters running on branch-2, and that is their future too.  More info here: http://osdir.com/ml/general-hadoop-apache/2012-04/msg00000.html\n\nbq. What about CDH4? Its version number is 2.0.0-something\n\nTechnically CDH4 is \"Cloudera's distribution of Hadoop including Apache Hadoop 2.0.0.\"  Its evolution didn't stop with 2.0.0, though.  We still are going to make another release in the cdh4 line where we backport some things.  CDH5 is where the focus is now, though.\n\nbq. In any case we will be sensible about old versions, but my philosophy is always to support the broadest range possible, and from everything I've seen it's paid off  users appreciate when you do not force their hand to upgrade. This is why Yahoo for example continues to be our biggest contributor on YARN support, even though their YARN is pretty different.\n\nAgree.", "Issue resolved by pull request 898\n[https://github.com/apache/spark/pull/898]"], "derived": {"summary": "FSDataOutputStream::sync() has disappeared from trunk in Hadoop; FileLogger. scala is calling it.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Spark master doesn't compile against hadoop-common trunk - FSDataOutputStream::sync() has disappeared from trunk in Hadoop; FileLogger. scala is calling it."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 898\n[https://github.com/apache/spark/pull/898]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1519", "title": "support minPartitions parameter of wholeTextFiles() in pyspark", "status": "Resolved", "priority": "Major", "reporter": "Nan Zhu", "assignee": "Kan Zhang", "labels": [], "created": "2014-04-16T21:01:50.000+0000", "updated": "2014-05-21T20:38:58.000+0000", "description": "though Scala implementation provides the parameter of minPartitions in wholeTextFiles, PySpark hasn't support it, \n\nshould be easy to add in context.py", "comments": ["PR: https://github.com/apache/spark/pull/697"], "derived": {"summary": "though Scala implementation provides the parameter of minPartitions in wholeTextFiles, PySpark hasn't support it, \n\nshould be easy to add in context. py.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "support minPartitions parameter of wholeTextFiles() in pyspark - though Scala implementation provides the parameter of minPartitions in wholeTextFiles, PySpark hasn't support it, \n\nshould be easy to add in context. py."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR: https://github.com/apache/spark/pull/697"}]}}
{"project": "SPARK", "issue_id": "SPARK-1520", "title": "Assembly Jar with more than 65536 files won't work when compiled on  JDK7 and run on JDK6", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-04-17T06:49:28.000+0000", "updated": "2015-04-20T09:54:48.000+0000", "description": "This is a real doozie - when compiling a Spark assembly with JDK7, the produced jar does not work well with JRE6. I confirmed the byte code being produced is JDK 6 compatible (major version 50). What happens is that, silently, the JRE will not load any class files from the assembled jar.\n\n{code}\n$> sbt/sbt assembly/assembly\n\n$> /usr/lib/jvm/java-1.7.0-openjdk-amd64/bin/java -cp /home/patrick/Documents/spark/assembly/target/scala-2.10/spark-assembly-1.0.0-SNAPSHOT-hadoop1.0.4.jar org.apache.spark.ui.UIWorkloadGenerator\nusage: ./bin/spark-class org.apache.spark.ui.UIWorkloadGenerator [master] [FIFO|FAIR]\n\n$> /usr/lib/jvm/java-1.6.0-openjdk-amd64/bin/java -cp /home/patrick/Documents/spark/assembly/target/scala-2.10/spark-assembly-1.0.0-SNAPSHOT-hadoop1.0.4.jar org.apache.spark.ui.UIWorkloadGenerator\nException in thread \"main\" java.lang.NoClassDefFoundError: org/apache/spark/ui/UIWorkloadGenerator\nCaused by: java.lang.ClassNotFoundException: org.apache.spark.ui.UIWorkloadGenerator\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:217)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:205)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:323)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:268)\nCould not find the main class: org.apache.spark.ui.UIWorkloadGenerator. Program will exit.\n\n{code}\n\nI also noticed that if the jar is unzipped, and the classpath set to the currently directory, it \"just works\". Finally, if the assembly jar is compiled with JDK6, it also works. The error is seen with any class, not just the UIWorkloadGenerator. Also, this error doesn't exist in branch 0.9, only in master.\n\nh1. Isolation and Cause\n\nThe package-time behavior of Java 6 and 7 differ with respect to the format used for jar files:\n||Number of entries||JDK 6||JDK 7||\n|<= 65536|zip|zip|\n|> 65536|zip*|zip64|\n\nzip* is a workaround for the original zip format that [described in JDK-6828461|https://bugs.openjdk.java.net/browse/JDK-4828461] that allows some versions of Java 6 to support larger assembly jars.\n\nThe Scala libraries we depend on have added a large number of classes which bumped us over the limit. This causes the Java 7 packaging to not work with Java 6. We can probably go back under the limit by clearing out some accidental inclusion of FastUtil, but eventually we'll go over again.\n\nThe real answer is to force people to build with JDK 6 if they want to run Spark on JRE 6.\n\n-I've found that if I just unpack and re-pack the jar (using `jar`) it always works:-\n\n{code}\n$ cd assembly/target/scala-2.10/\n$ /usr/lib/jvm/java-1.6.0-openjdk-amd64/bin/java -cp ./spark-assembly-1.0.0-SNAPSHOT-hadoop1.0.4.jar org.apache.spark.ui.UIWorkloadGenerator # fails\n$ jar xvf spark-assembly-1.0.0-SNAPSHOT-hadoop1.0.4.jar\n$ jar cvf spark-assembly-1.0.0-SNAPSHOT-hadoop1.0.4.jar *\n$ /usr/lib/jvm/java-1.6.0-openjdk-amd64/bin/java -cp ./spark-assembly-1.0.0-SNAPSHOT-hadoop1.0.4.jar org.apache.spark.ui.UIWorkloadGenerator # succeeds\n{code}\n\n-I also noticed something of note. The Breeze package contains single directories that have huge numbers of files in them (e.g. 2000+ class files in one directory). It's possible we are hitting some weird bugs/corner cases with compatibility of the internal storage format of the jar itself.-\n\n-I narrowed this down specifically to the inclusion of the breeze library. Just adding breeze to an older (unaffected) build triggered the issue.-\n\n-I ran a git bisection and this appeared after the MLLib sparse vector patch was merged:-\nhttps://github.com/apache/spark/commit/80c29689ae3b589254a571da3ddb5f9c866ae534\nSPARK-1212", "comments": ["Madness. One wild guess is that the breeze .jar files have something in META-INF that, when merged together into the assembly jar, conflicts with other META-INF items. In particular I'm thinking of MANIFEST.MF entries. It's worth diffing those if you can from before and after. However this would still require that Java 7 and 6 behave differently with respect to the entries, to explain your findings. It's possible.\n\nYour last comment however suggests it's something strange with the byte code that gets output for a few classes. Java 7 is stricter about byte code. For example: https://weblogs.java.net/blog/fabriziogiudici/archive/2012/05/07/understanding-subtle-new-behaviours-jdk-7\nHowever I would think these would manifest as quite different errors.\n\nWhat about running with -verbose:class to print classloading messages? it might point directly to what's failing to load, if that's it.\n\nOf course you can always build with Java 6 since that's supposed to be all that's supported/required now (see my other JIRA about making Jenkins do this), although I agree that it would be nice to get to the bottom of this, as there is no obvious reason this shouldn't work.", "Regarding large numbers of files: are there INDEX.LST files used anywhere in the jars? If this gets munged or truncated while building the assembly jar, that might cause all kinds of havoc. It could be omitted.\n\nhttp://docs.oracle.com/javase/7/docs/technotes/guides/jar/jar.html#Index_File_Specification", "[~srowen] heading to bed for the night... but would welcome help with this. I looked earlier and I don't think breeze is doing anything fancy with their manifest or meta-inf directories. I did a diff on the breeze directory itself between java 6 and java 7 compiled jars and they were identical. The classloading messages don't provide any useful output.\n\nMy best guess at present is we are hitting corner cases in the compatibility of the jar format itself due to having individual directories with thousands of class files. And these are causing the Java 6 RE to silently find the jar corrupt. I have no evidence to support that claim, however.", "Java 6 had a limit of 65536 files per jar in total, but the limit is much higher in Java 7:\nhttp://stackoverflow.com/questions/9616250/what-is-the-maximum-number-of-files-per-jar\nhttps://blogs.oracle.com/xuemingshen/entry/zip64_support_for_4g_zipfile\n\nWhen I build the assembly I find that it has 70948 files. I think you are certainly onto something.\n\nI see the same behavior as you, and am using the latest Java 6/7. I also note that \"unzip -l\" succeeds for the Java 6 version, but fails with the following on the Java 7 version:\n\n{code}\nerror:  expected central file header signature not found (file #70949).\n  (please check that you have transferred or created the zipfile in the\n  appropriate BINARY mode and that you have compiled UnZip properly)\n{code}\n\nThis might not be Java's fault. It could be something to do with how SBT handles merging the zip files, and not handling Java 7's output (which is zip64) correctly.\n\nAs a short-term solution, I note that we can probably slim down the assembly jar. For example, fastutil is still in there for some reason, and accounts for 10,666 files. It shouldn't be there.\n\nYou can get a quick view into where the files are with:\n\n{code}\njar tf spark-assembly-1.0.0-SNAPSHOT-hadoop1.0.4.jar | grep -oE \"(.+/)+\" | uniq -c | sort -rn | head -100\n{code}\n\n{code}\n2883 breeze/linalg/operators/\n2034 it/unimi/dsi/fastutil/objects/\n1396 spire/std/\n1379 scala/tools/nsc/typechecker/\n1351 breeze/linalg/\n1215 it/unimi/dsi/fastutil/longs/\n1214 it/unimi/dsi/fastutil/ints/\n1213 it/unimi/dsi/fastutil/doubles/\n1211 it/unimi/dsi/fastutil/floats/\n1210 it/unimi/dsi/fastutil/shorts/\n1209 it/unimi/dsi/fastutil/chars/\n1209 it/unimi/dsi/fastutil/bytes/\n1187 scala/reflect/internal/\n 896 com/google/common/collect/\n 894 tachyon/thrift/\n 886 spire/algebra/\n 797 scala/tools/nsc/transform/\n 749 scala/tools/nsc/interpreter/\n 723 org/netlib/lapack/\n 677 spire/math/\n...\n{code}", "I'm using Java 6 JDK located at /System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home on a mac. It can create a jar with more than 65536 files. I also found this JIRA:\n\nhttps://bugs.openjdk.java.net/browse/JDK-4828461 (Support Zip files with more than 64k entries)\n\nwhich was fixed in version 6. Note that this is for openjdk.\n\nI'm going to check the headers of assembly jars created by java 6 and 7.", "When I try to use jar-1.6 to untar the assembly jar created by java 7:\n\n~~~\njava.util.zip.ZipException: invalid CEN header (bad signature)\n\tat java.util.zip.ZipFile.open(Native Method)\n\tat java.util.zip.ZipFile.<init>(ZipFile.java:128)\n\tat java.util.zip.ZipFile.<init>(ZipFile.java:89)\n\tat sun.tools.jar.Main.list(Main.java:977)\n\tat sun.tools.jar.Main.run(Main.java:222)\n\tat sun.tools.jar.Main.main(Main.java:1147)\n~~~\n\n7z shows:\n\n~~~\nPath = spark-assembly-1.6.jar\nType = zip\nPhysical Size = 119682511\n\nPath = spark-assembly-1.7.jar\nType = zip\n64-bit = +\nPhysical Size = 119682587\n~~~\n\nI think the number of files limit is already increased in Java 6 (at least in the latest update), but Java 7 will use zip64 format for more than 64k  files, and this format cannot be recognized by Java 6.", "The quick fix may be removing fastutil, so Java 7 still generates the assembly jar in zip format instead of zip64.\n\nIn RDD#countApproxDistinct, we use HyperLogLog from com.clearspring.analytics:stream, which depends on fastutil. If this is the only place that introduces fastutil dependency, we should implement HyperLogLog and remove fastutil completely from Spark's dependencies.", "It seems HyperLogLog doesn't need fastutil, so we can exclude fastutil directly. Will send a patch.", "this one is a headache because i have not been able to actually make the unit tests pass with sbt and java 6 in a long time now, so i resorted to java 7 for the build process assuming the resulting jar could be run by java 6.\n", "There is no known resolution for this problem. We've updated the build scripts to enforce JDK6 and added better error messages when this happens.", "[~pwendell] On this note, I wonder if it's also best to make Jenkins build with Java 6? I'm not quite sure if it catches things like this, but catches things with similar roots. I had a request open at https://issues.apache.org/jira/browse/SPARK-1437 but it's a Jenkins change rather than a code change.", "Koert, which JDK6 did you use? This problem was fixed in a later version of openjdk-6. If you are using openjdk-6, you can try upgrading it to the latest version and see whether the problem still exists.", "Xiangrui,\nI have to stick to sun java. Currently on java version \"1.6.0_43\"\n\n\n\n\n\n\n", "The latest version of Sun JDK-6 is 1.6.0_45, though I didn't see any relevant changes between 43 and 45. After we remove fastutil from the dependencies, the assembly jar should have less than 65536 files. Did you try with the latest master?", "I will try latest master. thanks\n\n"], "derived": {"summary": "This is a real doozie - when compiling a Spark assembly with JDK7, the produced jar does not work well with JRE6. I confirmed the byte code being produced is JDK 6 compatible (major version 50).", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Assembly Jar with more than 65536 files won't work when compiled on  JDK7 and run on JDK6 - This is a real doozie - when compiling a Spark assembly with JDK7, the produced jar does not work well with JRE6. I confirmed the byte code being produced is JDK 6 compatible (major version 50)."}, {"q": "What updates or decisions were made in the discussion?", "a": "I will try latest master. thanks"}]}}
{"project": "SPARK", "issue_id": "SPARK-1521", "title": "Take character set size into account when compressing in-memory string columns", "status": "Resolved", "priority": "Major", "reporter": "Cheng Lian", "assignee": null, "labels": ["compression"], "created": "2014-04-17T08:07:30.000+0000", "updated": "2016-01-12T13:43:22.000+0000", "description": "Quoted from [a blog post|https://code.facebook.com/posts/229861827208629/scaling-the-facebook-data-warehouse-to-300-pb/] from Facebook:\n\nbq. Strings dominate the largest tables in our warehouse and make up about 80% of the columns across the warehouse, so optimizing compression for string columns was important. By using a threshold on observed number of distinct column values per stripe, we modified the ORCFile writer to apply dictionary encoding to a stripe only when beneficial. Additionally, we sample the column values and take the character set of the column into account, since a small character set can be leveraged by codecs like Zlib for good compression and dictionary encoding then becomes unnecessary or sometimes even detrimental if applied.", "comments": ["I assume this is obsolete or else already implemented in some sense by tungsten"], "derived": {"summary": "Quoted from [a blog post|https://code. facebook.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Take character set size into account when compressing in-memory string columns - Quoted from [a blog post|https://code. facebook."}, {"q": "What updates or decisions were made in the discussion?", "a": "I assume this is obsolete or else already implemented in some sense by tungsten"}]}}
{"project": "SPARK", "issue_id": "SPARK-1522", "title": "YARN ClientBase will throw a NPE if there is no YARN application specific classpath.", "status": "Resolved", "priority": "Critical", "reporter": "Bernardo Gomez Palacio", "assignee": "Bernardo Gomez Palacio", "labels": ["YARN"], "created": "2014-04-17T09:04:52.000+0000", "updated": "2014-06-09T21:18:35.000+0000", "description": "The current implementation of ClientBase.getDefaultYarnApplicationClasspath inspects the MRJobConfig class for the field DEFAULT_YARN_APPLICATION_CLASSPATH when it should be really looking into YarnConfiguration.\n\nIf the Application Configuration has no yarn.application.classpath defined a NPE exception will be thrown.", "comments": ["https://github.com/apache/spark/pull/433"], "derived": {"summary": "The current implementation of ClientBase. getDefaultYarnApplicationClasspath inspects the MRJobConfig class for the field DEFAULT_YARN_APPLICATION_CLASSPATH when it should be really looking into YarnConfiguration.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "YARN ClientBase will throw a NPE if there is no YARN application specific classpath. - The current implementation of ClientBase. getDefaultYarnApplicationClasspath inspects the MRJobConfig class for the field DEFAULT_YARN_APPLICATION_CLASSPATH when it should be really looking into YarnConfiguration."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/433"}]}}
{"project": "SPARK", "issue_id": "SPARK-1523", "title": "improve the readability of code in AkkaUtil ", "status": "Resolved", "priority": "Trivial", "reporter": "Nan Zhu", "assignee": "Nan Zhu", "labels": [], "created": "2014-04-17T11:38:00.000+0000", "updated": "2014-04-18T17:27:44.000+0000", "description": "Actually it is separated from https://github.com/apache/spark/pull/85 as suggested by Reynold\n\ncompare https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/AkkaUtils.scala#L122 and https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/AkkaUtils.scala#L117\n\nthe first one use get and then toLong, the second one getLong....better to make them consistent\n\n\nvery very small fix........", "comments": [], "derived": {"summary": "Actually it is separated from https://github. com/apache/spark/pull/85 as suggested by Reynold\n\ncompare https://github.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "improve the readability of code in AkkaUtil  - Actually it is separated from https://github. com/apache/spark/pull/85 as suggested by Reynold\n\ncompare https://github."}]}}
{"project": "SPARK", "issue_id": "SPARK-1524", "title": "TaskSetManager'd better not schedule tasks which has no preferred executorId using PROCESS_LOCAL in the first search process", "status": "Resolved", "priority": "Minor", "reporter": "YanTang Zhai", "assignee": null, "labels": [], "created": "2014-04-17T11:54:06.000+0000", "updated": "2016-01-04T14:42:45.000+0000", "description": "ShuffleMapTask is constructed with TaskLocation which has only host not (host, executorID) pair in DAGScheduler.\nWhen TaskSetManager schedules ShuffleMapTask which has no preferred executorId using specific execId host and PROCESS_LOCAL locality level, no tasks match the given locality constraint in the first search process.\nWe also find that the host used by Scheduler is hostname while the host used by TaskLocation is IP in our cluster. The tow hosts do not match, that makes pendingTasksForHost HashMap empty and the finding task process against our expectation.", "comments": ["The expectation is to fallback to a previous schedule type in case the higher level is not valid : though this is tricky in general case.\nWill need to take a look at it - though given that I am tied up with other things, if someone else wants to take a crack, please feel free to do so !\n\nBtw, use of IP's and multiple hostnames for a host is not supported in spark - so that is something that will need to be resolved at the deployment end."], "derived": {"summary": "ShuffleMapTask is constructed with TaskLocation which has only host not (host, executorID) pair in DAGScheduler. When TaskSetManager schedules ShuffleMapTask which has no preferred executorId using specific execId host and PROCESS_LOCAL locality level, no tasks match the given locality constraint in the first search process.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "TaskSetManager'd better not schedule tasks which has no preferred executorId using PROCESS_LOCAL in the first search process - ShuffleMapTask is constructed with TaskLocation which has only host not (host, executorID) pair in DAGScheduler. When TaskSetManager schedules ShuffleMapTask which has no preferred executorId using specific execId host and PROCESS_LOCAL locality level, no tasks match the given locality constraint in the first search process."}, {"q": "What updates or decisions were made in the discussion?", "a": "The expectation is to fallback to a previous schedule type in case the higher level is not valid : though this is tricky in general case.\nWill need to take a look at it - though given that I am tied up with other things, if someone else wants to take a crack, please feel free to do so !\n\nBtw, use of IP's and multiple hostnames for a host is not supported in spark - so that is something that will need to be resolved at the deployment end."}]}}
{"project": "SPARK", "issue_id": "SPARK-1525", "title": "TaskSchedulerImpl should decrease availableCpus by spark.task.cpus not 1", "status": "Closed", "priority": "Minor", "reporter": "YanTang Zhai", "assignee": null, "labels": [], "created": "2014-04-17T13:07:57.000+0000", "updated": "2014-07-02T06:25:57.000+0000", "description": "TaskSchedulerImpl decreases availableCpus by 1 in resourceOffers process always even though spark.task.cpus is more than 1, which will schedule more tasks to some node when spark.task.cpus is more than 1.", "comments": ["The latest code already fix the bug.", "this is fixed by https://github.com/apache/spark/commit/f8111eaeb0e35f6aa9b1e3ec1173fff207174155"], "derived": {"summary": "TaskSchedulerImpl decreases availableCpus by 1 in resourceOffers process always even though spark. task.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "TaskSchedulerImpl should decrease availableCpus by spark.task.cpus not 1 - TaskSchedulerImpl decreases availableCpus by 1 in resourceOffers process always even though spark. task."}, {"q": "What updates or decisions were made in the discussion?", "a": "this is fixed by https://github.com/apache/spark/commit/f8111eaeb0e35f6aa9b1e3ec1173fff207174155"}]}}
{"project": "SPARK", "issue_id": "SPARK-1526", "title": "Running spark driver program from my local machine", "status": "Resolved", "priority": "Major", "reporter": "Idan Zalzberg", "assignee": null, "labels": [], "created": "2014-04-17T14:24:33.000+0000", "updated": "2015-01-23T12:36:14.000+0000", "description": "Currently it seems that the design choice is that the driver program should be close network-wise to the worker and allow connections to be created from either side.\n\nThis makes using Spark somewhat harder since when I develop locally I not only to package all my program, but also all it's local dependencies.\nlet's say I have a local DB with names of files in HADOOP that I want to process with spark, now I need my local DB to be accessible from the cluster so it can fetch the file names in runtime.\n\nThe driver program is an awesome thing, but it loses some of it's strength if you can't really run it anywhere.\n\nIt seems to me that the problem is with the DAGScheduler that needs to be close to the worker, maybe it shouldn't be embedded in the driver then?", "comments": ["This may be a little bold in closing, but there's been no activity and I do not see an actionable change here, but I think there is a fine workaround for this case. Yes it's a pretty fundamental property of Spark that the driver communicates a lot with the executors, and I can't see that changing. You can of course run the driver remotely; it's a matter of network config, and having enough network bandwidth to support however much communication your driver/executors need, which is not necessarily a lot. Finally, you can access resources like DBs from your executors too, of course. In fact that is probably more sensible than loading to the driver, then copying again to executors."], "derived": {"summary": "Currently it seems that the design choice is that the driver program should be close network-wise to the worker and allow connections to be created from either side. This makes using Spark somewhat harder since when I develop locally I not only to package all my program, but also all it's local dependencies.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Running spark driver program from my local machine - Currently it seems that the design choice is that the driver program should be close network-wise to the worker and allow connections to be created from either side. This makes using Spark somewhat harder since when I develop locally I not only to package all my program, but also all it's local dependencies."}, {"q": "What updates or decisions were made in the discussion?", "a": "This may be a little bold in closing, but there's been no activity and I do not see an actionable change here, but I think there is a fine workaround for this case. Yes it's a pretty fundamental property of Spark that the driver communicates a lot with the executors, and I can't see that changing. You can of course run the driver remotely; it's a matter of network config, and having enough network bandwidth to support however much communication your driver/executors need, which is not necessarily a lot. Finally, you can access resources like DBs from your executors too, of course. In fact that is probably more sensible than loading to the driver, then copying again to executors."}]}}
{"project": "SPARK", "issue_id": "SPARK-1527", "title": "rootDirs in DiskBlockManagerSuite doesn't get full path from rootDir0, rootDir1", "status": "Resolved", "priority": "Minor", "reporter": "YE", "assignee": "Niraj Suthar", "labels": ["starter"], "created": "2014-04-17T15:21:24.000+0000", "updated": "2014-05-14T02:04:45.000+0000", "description": "In core/src/test/scala/org/apache/storage/DiskBlockManagerSuite.scala\n\n  val rootDir0 = Files.createTempDir()\n  rootDir0.deleteOnExit()\n  val rootDir1 = Files.createTempDir()\n  rootDir1.deleteOnExit()\n  val rootDirs = rootDir0.getName + \",\" + rootDir1.getName\n\nrootDir0 and rootDir1 are in system's temporary directory. \nrootDir0.getName will not get the full path of the directory but the last component of the directory. When passing to DiskBlockManage constructor, the DiskBlockerManger creates directories in pwd not the temporary directory.\n\nrootDir0.toString will fix this issue.", "comments": ["{{toString()}} returns {{getPath()}} which may still be relative. {{getAbsolutePath()}} is better, but even {{getCanonicalPath()}} may be better still.", "Yes. You are right. toString() may give relative path. And since it's determined by java.io.tmpdir system property. see https://code.google.com/p/guava-libraries/source/browse/guava/src/com/google/common/io/Files.java line 591. It's possible that the DiskBlockManager will create different directories than the original temp dir when java.io.tmpdir is a relative path. \n\nso use getAbsolutePath since I use this method in my last pr?\n\nBut, I saw toString() was called other places! Should we do something about that?", "If the paths are only used locally, then an absolute path never hurts (except to be a bit longer). I assume that since these are references to a temp directory that is by definition only valid locally, that absolute path is the right thing to use.\n\nIn other cases, similar logic may apply. I could imagine in some cases the right thing to do is transmit a relative path. ", "Yes, of course, sometimes we want absolute path, sometimes we want to transmit a relative path. It depends on logic. \nBut I think maybe we should review these usages so that we can make sure absolute paths or relative paths are used appropriately.\n\nI may have time to review it after I finish another JIRA issue. If you want to take it over, please!\n\nAnyway, thanks for your comments and help.\n", "There are a number of other uses of File.getName(), but a quick glance suggests all the others are appropriate.\n\nThere are a number of other uses of File.toString(), almost all in tests. I suspect the Files in question already have absolute paths, and that even relative paths happen to work fine in a test since the working dir doesn't change. So those could change, but are probably not a concern.\n\nThe only one that gave me pause was the use in HttpBroadcast.scala, though I suspect it turns out to work fine for similar reasons.\n\nIf reviewers are interested in changing the toString()s I'll test and submit a PR for that.", "hi, [~nirajsuthar]\nThis is my pr. https://github.com/apache/spark/pull/436. As [~srowen] said, if someone interested in changing the toString()s, please leave a comment.\n\n[~rxin] what do you think?", "Sure Ye Xianjin,\n\nI am more thn happy to do so. after reading the comments I looked at the HttpBroadcast.scala and will update it appropriately.\n\nif you guys have any suggestions here..please let me know. \n\nThank you,\nNiraj", "Issue resolved by pull request 436\n[https://github.com/apache/spark/pull/436]"], "derived": {"summary": "In core/src/test/scala/org/apache/storage/DiskBlockManagerSuite. scala\n\n  val rootDir0 = Files.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "rootDirs in DiskBlockManagerSuite doesn't get full path from rootDir0, rootDir1 - In core/src/test/scala/org/apache/storage/DiskBlockManagerSuite. scala\n\n  val rootDir0 = Files."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 436\n[https://github.com/apache/spark/pull/436]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1528", "title": "Spark on Yarn: Add option for user to specify additional namenodes to get tokens from", "status": "Resolved", "priority": "Major", "reporter": "Thomas Graves", "assignee": "Thomas Graves", "labels": [], "created": "2014-04-17T15:56:01.000+0000", "updated": "2014-08-05T17:51:16.000+0000", "description": "Some users running spark on yarn may wish to contact other Hdfs clusters then the one they are running on.  We should add in an option for them to specify those namenodes so that we can get the credentials needed for the application to contact them.  ", "comments": ["https://github.com/apache/spark/pull/1159"], "derived": {"summary": "Some users running spark on yarn may wish to contact other Hdfs clusters then the one they are running on. We should add in an option for them to specify those namenodes so that we can get the credentials needed for the application to contact them.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Spark on Yarn: Add option for user to specify additional namenodes to get tokens from - Some users running spark on yarn may wish to contact other Hdfs clusters then the one they are running on. We should add in an option for them to specify those namenodes so that we can get the credentials needed for the application to contact them."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/1159"}]}}
{"project": "SPARK", "issue_id": "SPARK-1529", "title": "Support DFS based shuffle in addition to Netty shuffle", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Kannan Rajah", "labels": [], "created": "2014-04-17T20:39:28.000+0000", "updated": "2019-12-02T14:42:01.000+0000", "description": "In some environments, like with MapR, local volumes are accessed through the Hadoop filesystem interface. Shuffle is implemented by writing intermediate data to local disk and serving it to remote node using Netty as a transport mechanism. We want to provide an HDFS based shuffle such that data can be written to HDFS (instead of local disk) and served using HDFS API on the remote nodes. This could involve exposing a file system abstraction to Spark shuffle and have 2 modes of running it. In default mode, it will write to local disk and in the DFS mode, it will write to HDFS.", "comments": ["After some investigation, I came to the conclusion that, unlike adding Tachyon support, to allow setting {{spark.local.dir}} to a Hadoop FS location, instead of adding something like {{HDFSBlockManager}} / {{HDFSStore}}, we have to refactor related local FS access code to leverage HDFS interfaces. And it seems hard to make this change incremental. Besides writing shuffle map output, at least two places reference {{spark.local.dir}}:\n\n# HTTP broadcasting uses {{spark.local.dir}} as resource root, and access local FS with `java.io.File`\n# {{FileServerHandler}} accesses {{spark.local.dir}} via {{DiskBlockManager}} and reads local file with {{FileSegment}} and {{java.io.File}}\n\nAdding new block manager / store for HDFS can't fix these places. I'm currently working on this issue by:\n\n# Refactoring {{FileSegment.file}} from {{java.io.File}} to {{org.apache.hadoop.fs.Path}},\n# Refactoring {{DiskBlockManager}}, {{DiskStore}}, {{HttpBroadcast}} & {{FileServerHandler}}  to leverage HDFS interfaces.\n\nPlease leave comments if I missed anything or there exist simpler ways to workaround this.\n\n(PS: We should definitely refactor block manager related code to reduce duplicate code and encapsulate more details. Maybe the public interface of block manager should only communicate with other component with block IDs and storage levels.)", "[~liancheng] Hey Cheng, the tricky thing here is want to avoid _always_ going through the HDFS filesystem interface when people are actually using local files. We might need to add an intermediate abstraction to deal with this. We already do this elsehwere in the code base, for instance the JobLogger will load an output stream either directly form a file or from a hadoop file.\n\nOne thing to note is that the requirement here is really only for the shuffle files, not for the other uses. But I realize we currently conflate these inside of Spark so that might not buy us much. I'll look into this a bit more later.", "One idea proposed by [~adav] was to always use the Hadoop filesystem API, but to potentially implement our own version of the local filesystem if we find the Hadoop version has performance drawbacks. Not proposing this for now, but it could be interesting to look at later.\n\nAnother issue is that we use FileChannel objects directly in the `DiskBlockObjectWriter`. After looking through this a bit, the functionality there to commit and rewind writes is not actually used anywhere, we could probably just remove it.\n\n[~liancheng] I think it would be worth it to look at a version where we just take all of the File API's and replace them with Hadoop equivalents. I.e. your proposal. ", "Hey, looking through the code a little more in depth reveals that the rewind and truncate functionality of DiskBlockObjectWriter is actually used during shuffle file consolidation. The issue is that we store the metadata for each consolidated shuffle file as a consecutive set of offsets into the file (an \"offset table\"). That is, if we have 3 blocks stored in the same file, rather than storing 3 pairs of (offset, length), we simply store the offsets and use the fact that they're laid out consecutively to reconstruct the lengths. This means we can't suffer \"holes\" in the data structure of partial writes, and thus rely on the fact that partial writes (which are not included in the data structure right now) are always of size 0.\n\nI think getting around this is pretty straightforward, however: We can simply store the offsets of all partial writes in the offset table, and just avoid storing them in the index we build to look up the positions of particular map tasks in the offset table. This will mean we can reconstruct the lengths properly, but most importantly it means we will not think that our failed map tasks were successful (because index lookups for them will still fail, even though they're in the offset table).\n\nThis seems like a pretty clean solution that wraps up our usage of FileChannels, save where we mmap files back into memory. We will likely want to special-case the blocks to make sure we can mmap them directly when reading from the local file system.", "This may be a dumb question on an old issue, but: you can already access local filesystems through HDFS's FileSystem. Of course you can always access local filesystems directly in general.\n\nThe FileSystem abstraction won't provide everything that java.nio does because it is not a local filesystem.\n\nBut why would you want to put shuffle and temp files in HDFS? The MapR comment confuses me more since its main trick is exposing HDFS as more like an NFS mount point. But if anything that makes it already usable as is for this purpose. \n\n", "[~lian cheng] [~pwendell] I want to work on this JIRA. It's been a while since there has been any update. So can you please share what the current status is? Has there been a consensus on replacing the file API with a HDFS kind of interface and plugging in the right implementation? I will be looking at the code base in the mean time.", "Hi [~srowen], first of all we are not trying to put shuffle and temp files in HDFS. At the time this ticket was created, the initial motivation was to support MapR, because MapR only exposes local file system via MapR volume and HDFS {{FileSystem}} interface. However, later on this issue was worked around with NFS. And this ticket wasn't solved because of lacking enough capacity.\n\n[~rkannan82] Thanks for looking into this! Several months ago, I had once implemented a prototype by simply replacing Java NIO file system operations with corresponding HDFS {{FileSystem}} version. According to prior benchmark done with {{spark-perf}}, this introduces ~15% performance penalty for shuffling. Thus we had once planned to write a specialized {{FileSystem}} implementation which simply wraps normal Java NIO operations to avoid the performance penalty as much as possible, and then replace all local file system access with this specialized {{FileSystem}} implementation.", "Hm, how do these APIs preclude the direct use of java.io? Is this actually disabled in MapR? If there is a workaround what is the remaining motivation? ", "[~lian cheng] Can you upload this prototype patch so that I can reuse it? What branch was it based off? When I start making new changes, I suppose I can do it against master branch, right?", "It cannot preclude the use of java.io completely. If there are java.io. APIs that are needed for some use case, then you cannot use the HDFS API. But  that is the case normally.\n\nThe NFS mount based workaround is not as efficient as accessing it through the HDFS interface. Hence the need.", "So Spark needs to read and write local files for things like shuffle. It uses java.io. this continues to work everywhere. I am still missing why something has to go through HDFS or NFS here. These files should not be on NFS either. ", "In a MapR distribution, these files need to go into MapR volume and not local disk. This MapR volume is local to the node though, but its part of the distributed file system. This can be achieved in 2 ways:\n1. Expose the MapR file system as a NFS mount point. Now, you can use the normal java.io API and data will still get written to MapR volume instead of local disk.\n\n2. Use HDFS API (with underlying MapR implementation) instead of java.io to make the IO go to MapR volume.", "OK, but, why do these files *have* to go on a non-local disk? It sounds like you're saying Spark doesn't work at all on MapR right now, but that can't be the case. They *can* go on a non-local disk, I'm sure. What's the value of that, given that Spark is transporting the files itself?\n\nStill, as you say, this proprietary setup works already through the java.io+NFS and HDFS APIs, with no change. If it's just not as fast, is that a problem that Spark should be solving? Just don't do it. Or it's up to the vendor to optimize.", "Hey Sean,\n\nFrom what I remember of this, the issue is that MapR clusters are not typically provisioned with much local disk space available, because the MapRFS supports accessing \"local\" volumes in its API, unlike the HDFS API. So in general the expectation is that large amounts of local data should be written through MapR's API to its local filesystem. They have an NFS mount you can use as a work around to provide POSIX API's, and I think most MapR users set this mount up and then have Spark write shuffle data there.\n\nOption 2 which [~rkannan82] mentions is not actually feasible in Spark right now. We don't support writing shuffle data through the Hadoop API's right now and I think Cheng's patch was only a prototype of how we might do that...", "BTW - I think if MapR wants to have a customized shuffle, the direction proposed in this patch is probably not the best way to do it. It would make more sense to implement a DFS-based shuffle using the new pluggable shuffle API. I.e. a shuffle that communicates through the filesystem rather than doing transfers through Spark.", "[~pwendell] Gotcha, that begins to make sense. I assume the cluster can be provisioned with as much local disk as desired, regardless of defaults. The alternative, to write temp files across the network and read them back in order to then broadcast them back over the network, seems a lot worse than just setting up the right amount of local disk. But if it works well enough and is easier in some situations, sounds like that's also an option. I suppose I'm asking / questioning why the project would want to encourage remote shuffle files by trying to not just use the HDFS APIs, but even maintain a specialized version of it, just to make a third workaround for a vendor config issue? Surely MapR should just set up clusters that are provisioned with Spark more how Spark needs them.", "[~pwendell] I would like us to consider the option of reusing the write code path of existing shuffle implementation instead of implementing from scratch. This will allow us to take advantage of all the optimizations that are already done and will be done in future. Only the read code path needs to be reimplemented fully as we don't need all the shuffle server logic. There are a handful of shuffle classes that need to use the HDFS abstractions in order to achieve this. I have attached a high level proposal. Let me know your thoughts.\n\nWrite\nIndexShufflleBlockManager, SortShuffleWriter, ExternalSorter, BlockObjectWriter.\n\nRead\nBlockStoreShuffleFetcher, HashShuffleReader", "Attached: High level proposal of the changes to make shuffle use Distributed File System instead of local file system.", "Can someone assign this bug to me? I am working on a patch.", "I have pushed the first round of commits to my repo. I would like to get some early feedback on the overall design.\nhttps://github.com/rkannan82/spark/commits/dfs_shuffle\n\nCommits:\nhttps://github.com/rkannan82/spark/commit/ce8b430512b31e932ffdab6e0a2c1a6a1768ffbf\nhttps://github.com/rkannan82/spark/commit/8f5415c248c0a9ca5ad3ec9f48f839b24c259813\nhttps://github.com/rkannan82/spark/commit/d9d179ba6c685cc8eb181f442e9bd6ad91cc4290", "[~liancheng] Will you be able to take a look at the code and provide some feedback?", "[~kannan] I haven't being working on Spark Core for a while, but I'll take a look at this. Thanks for working on this!\n\nAlso cc [~pwendell] [~andrewor14].", "Thanks. FYI, I have pushed few more commits to my repo to handle all the TODOs and bug fixes. So you can track this branch for all the changes: https://github.com/rkannan82/spark/commits/dfs_shuffle", "Hey Kannan,\n\nWe originally considered doing something like you are proposing, where we would change our filesystem interactions to all use a Hadoop FileSystem class and then we'd use Hadoop's LocalFileSystem. However, there were two issues:\n\n1. We used POSIX API's that are not present in Hadoop. For instance, we use memory mapping in various places, FileChannel in the BlockObjectWriter, etc.\n2. Using LocalFileSystem has a substantial performance overheads compared with our current code. So we'd have to write our own implementation of a Local filesystem.\n\nFor this reason, we decided that our current shuffle machinery was fundamentally not usable for non-POSIX environments. So we decided that instead, we'd let people customize shuffle behavior at a higher level and we implemented the pluggable shuffle components. So you can create a shuffle manager that is specifically optimized for a particular environment (e.g. MapR).\n\nDid you consider implementing a MapR shuffle using that mechanism instead? You'd have to operate at a higher level, where you reason about shuffle records, etc. But you'd have a lot of flexibility to optimize within that.", "[~pwendell] The default code path still uses the FileChannel, memory mapping techniques. I just provided an abstraction called FileSystem.scala (not Hadoop's FileSystem.java). LocalFileSystem.scala delegates the call to existing Spark code path that uses FileChannel. I am using Hadoop's RawLocalFileSystem class just to get an InputStream, OutputStream. And this internally also uses FileChannel. Please see RawLocalFileSystem.LocalFSFileInputStream. It is just a wrapper on java.io.FileInputStream.\n\nGoing back to why I considered this approach. It will allow us to reuse all the logic currently used by SortShuffle code path. We would have to implement pretty much everything that's been done by Spark to do the shuffle on HDFS. We are in the processing of running some performance tests to understand the impact of the change. One of the main things we will be verifying is if there is any performance degradation introduced in the default code path and fix if there is any. Is this acceptable?", "(Sorry if this double-posts.)\n\nIs there a good way to see the whole diff at the moment? I know there's a branch with individual commits. Maybe I am missing something basic.\n\nThis puts a new abstraction on top of a Hadoop FileSystem on top of the underlying file system abstraction. That's getting heavy. If it's only abstracting access to an InputStream / OutputStream, why is it needed? that's already directly available from, say, Hadoop's FileSystem.\n\nWhat would be the performance gain if this is the bit being swapped out? This is my original question -- you shuffle to HDFS, then read it back to send it again via the existing shuffle? It kind of made sense when the idea was to swap the whole shuffle to replace its transport.", "You can use the Compare functionality to see a single page of diffs across commits. Here is the link: https://github.com/rkannan82/spark/compare/4aaf48d46d13129f0f9bdafd771dd80fe568a7dc...rkannan82:7195353a31f7cfb087ec804b597b01fb362bc3f6\n\nA few clarifications.\n1. There are 2 reasons for introducing a FileSystem abstraction in Spark instead of directly using Hadoop FileSystem.\n  - There are Spark shuffle specific APIs that needed abstraction. Please take a look at this code:\nhttps://github.com/rkannan82/spark/blob/dfs_shuffle/core/src/main/scala/org/apache/spark/storage/FileSystem.scala\n\n  - For local file system access, we can choose to circumvent using Hadoop's local file system implementation if its not efficient. If you look at LocalFileSystem.scala, for most APIs, it just delegates to the old code of using Spark's disk block manager, etc. In fact, we can just look at this single class and determine if we will hit any performance degradation for the default Apache shuffle code path.\nhttps://github.com/rkannan82/spark/blob/dfs_shuffle/core/src/main/scala/org/apache/spark/storage/LocalFileSystem.scala\n\n2. During the write phase, we shuffle to HDFS instead of local file system. While reading back, we don't use the Netty based transport that Apache shuffle uses. Instead we have a new implementation called DFSShuffleClient that reads from HDFS. That is the main difference.\nhttps://github.com/rkannan82/spark/blob/dfs_shuffle/network/shuffle/src/main/java/org/apache/spark/network/shuffle/DFSShuffleClient.java", "Just wanted to check if folks got a chance to review the changes. If you have any concerns, I will be happy to address them.", "Hi,\nare there any updates for this improvement?\n\nWe are running Spark on YARN with a MapR distribution hadoop cluster with small local disks. Small disk setup is not uncommon. Requiring local disk space just for Spark scratch space seems like a waste of disk space which can be part of hdfs. What should be the size of local disk anyway? Should it be dependent on the size of datasets we are processing? Doesn't a multi-user environment make the problem even worse?\n\nAnother issue is with configuration of {{spark.local.dir}} for Spark on YARN: {quote}In Spark 1.0 and later this will be overriden by SPARK_LOCAL_DIRS (Standalone, Mesos) or LOCAL_DIRS (YARN) environment variables set by the cluster manager. {quote}\nSpark uses now the same local directory as Node Manager. If we use the workaround with NFS mount, it requires us to move Node Manager directory to NFS too, which seems obscure (should we create a separate ticket for that?).\n\nWe would really appreciate if you implement a solution for this ticket. We can also see  a lot of questions on forums such as [StackOverflow|http://stackoverflow.com/q/31303568/878613] related to Spark running out of space for scratch dir, so the community would surely appreciate it too.\n\nThanks.", "Hello [~rkannan82]\r\n\r\nAre there any updates for this feature? I was looking for something similar as well. Do you happen to have any comparisons between using hdfs to read / write shuffle data vs using local disk + netty ?\r\n\r\nThanks.", "For huge Spark jobs, we are seeing frequent failures due to the Shuffle phase: local disk being full, unable to connect to External Shuffle Service.\r\n\r\nThe frustrating fact is that for these huge jobs, even a single task retry sometimes takes a very long time.\r\n\r\nIt will be great to have the option to write the temporary shuffle data to a reliable storage, e.g. HDFS but also others.\r\n\r\nMaybe it should be a different Shuffle Manager implementation altogether.\r\n\r\nAny thoughts?"], "derived": {"summary": "In some environments, like with MapR, local volumes are accessed through the Hadoop filesystem interface. Shuffle is implemented by writing intermediate data to local disk and serving it to remote node using Netty as a transport mechanism.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support DFS based shuffle in addition to Netty shuffle - In some environments, like with MapR, local volumes are accessed through the Hadoop filesystem interface. Shuffle is implemented by writing intermediate data to local disk and serving it to remote node using Netty as a transport mechanism."}, {"q": "What updates or decisions were made in the discussion?", "a": "For huge Spark jobs, we are seeing frequent failures due to the Shuffle phase: local disk being full, unable to connect to External Shuffle Service.\r\n\r\nThe frustrating fact is that for these huge jobs, even a single task retry sometimes takes a very long time.\r\n\r\nIt will be great to have the option to write the temporary shuffle data to a reliable storage, e.g. HDFS but also others.\r\n\r\nMaybe it should be a different Shuffle Manager implementation altogether.\r\n\r\nAny thoughts?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1530", "title": "Streaming UI test can hang indefinitely", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Tathagata Das", "labels": [], "created": "2014-04-17T23:50:56.000+0000", "updated": "2014-11-08T11:53:37.000+0000", "description": "This has been causing Jenkins to hang recently:\n\n{code}\n\"pool-1-thread-1\" prio=10 tid=0x00007f4b9449f000 nid=0x6c37 runnable [0x00007f4b8a26c000]\n   java.lang.Thread.State: RUNNABLE\n        at java.net.SocketInputStream.socketRead0(Native Method)\n        at java.net.SocketInputStream.read(SocketInputStream.java:152)\n        at java.net.SocketInputStream.read(SocketInputStream.java:122)\n        at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)\n        at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)\n        at java.io.BufferedInputStream.read(BufferedInputStream.java:334)\n        - locked <0x00000007cad700d0> (a java.io.BufferedInputStream)\n        at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:687)\n        at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:633)\n        at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1323)\n        - locked <0x00000007cad662b8> (a sun.net.www.protocol.http.HttpURLConnection)\n        at java.net.URL.openStream(URL.java:1037)\n        at scala.io.Source$.fromURL(Source.scala:140)\n        at scala.io.Source$.fromURL(Source.scala:130)\n        at org.apache.spark.ui.UISuite$$anonfun$2$$anonfun$apply$mcV$sp$2$$anonfun$apply$2.apply$mcV$sp(UISuite.scala:57)\n        at org.apache.spark.ui.UISuite$$anonfun$2$$anonfun$apply$mcV$sp$2$$anonfun$apply$2.apply(UISuite.scala:56)\n        at org.apache.spark.ui.UISuite$$anonfun$2$$anonfun$apply$mcV$sp$2$$anonfun$apply$2.apply(UISuite.scala:56)\n        at org.scalatest.concurrent.Eventually$class.makeAValiantAttempt$1(Eventually.scala:394)\n        at org.scalatest.concurrent.Eventually$class.tryTryAgain$1(Eventually.scala:408)\n        at org.scalatest.concurrent.Eventually$class.eventually(Eventually.scala:437)\n        at org.scalatest.concurrent.Eventually$.eventually(Eventually.scala:477)\n        at org.scalatest.concurrent.Eventually$class.eventually(Eventually.scala:307)\n        at org.scalatest.concurrent.Eventually$.eventually(Eventually.scala:477)\n        at org.apache.spark.ui.UISuite$$anonfun$2$$anonfun$apply$mcV$sp$2.apply(UISuite.scala:56)\n        at org.apache.spark.ui.UISuite$$anonfun$2$$anonfun$apply$mcV$sp$2.apply(UISuite.scala:54)\n        at org.apache.spark.LocalSparkContext$.withSpark(LocalSparkContext.scala:60)\n        at org.apache.spark.ui.UISuite$$anonfun$2.apply$mcV$sp(UISuite.scala:54)\n        at org.apache.spark.ui.UISuite$$anonfun$2.apply(UISuite.scala:54)\n        at org.apache.spark.ui.UISuite$$anonfun$2.apply(UISuite.scala:54)\n        at org.scalatest.FunSuite$$anon$1.apply(FunSuite.scala:1265)\n        at org.scalatest.Suite$class.withFixture(Suite.scala:1974)\n        at org.apache.spark.ui.UISuite.withFixture(UISuite.scala:37)\n        at org.scalatest.FunSuite$class.invokeWithFixture$1(FunSuite.scala:1262)\n        at org.scalatest.FunSuite$$anonfun$runTest$1.apply(FunSuite.scala:1271)\n        at org.scalatest.FunSuite$$anonfun$runTest$1.apply(FunSuite.scala:1271)\n        at org.scalatest.SuperEngine.runTestImpl(Engine.scala:198)\n        at org.scalatest.FunSuite$class.runTest(FunSuite.scala:1271)\n        at org.apache.spark.ui.UISuite.runTest(UISuite.scala:37)\n        at org.scalatest.FunSuite$$anonfun$runTests$1.apply(FunSuite.scala:1304)\n        at org.scalatest.FunSuite$$anonfun$runTests$1.apply(FunSuite.scala:1304)\n        at org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:260)\n        at org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:249)\n        at scala.collection.immutable.List.foreach(List.scala:318)\n        at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:249)\n        at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:326)\n        at org.scalatest.FunSuite$class.runTests(FunSuite.scala:1304)\n        at org.apache.spark.ui.UISuite.runTests(UISuite.scala:37)\n        at org.scalatest.Suite$class.run(Suite.scala:2303)\n        at org.apache.spark.ui.UISuite.org$scalatest$FunSuite$$super$run(UISuite.scala:37)\n        at org.scalatest.FunSuite$$anonfun$run$1.apply(FunSuite.scala:1310)\n        at org.scalatest.FunSuite$$anonfun$run$1.apply(FunSuite.scala:1310)\n        at org.scalatest.SuperEngine.runImpl(Engine.scala:362)\n        at org.scalatest.FunSuite$class.run(FunSuite.scala:1310)\n        at org.apache.spark.ui.UISuite.run(UISuite.scala:37)\n        at org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:214)\n        at sbt.RunnerWrapper$1.runRunner2(FrameworkWrapper.java:223)\n        at sbt.RunnerWrapper$1.execute(FrameworkWrapper.java:236)\n        at sbt.ForkMain$Run$2.call(ForkMain.java:294)\n        at sbt.ForkMain$Run$2.call(ForkMain.java:284)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:744)\n\n{code}", "comments": ["I wonder what about Jenkins environment is causing this. \nAnd how frequently does this happen?\n", "I assume this got fixed along the way, either by [~tdas]'s changes or [~shaneknapp]'s changes to Jenkins?"], "derived": {"summary": "This has been causing Jenkins to hang recently:\n\n{code}\n\"pool-1-thread-1\" prio=10 tid=0x00007f4b9449f000 nid=0x6c37 runnable [0x00007f4b8a26c000]\n   java. lang.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Streaming UI test can hang indefinitely - This has been causing Jenkins to hang recently:\n\n{code}\n\"pool-1-thread-1\" prio=10 tid=0x00007f4b9449f000 nid=0x6c37 runnable [0x00007f4b8a26c000]\n   java. lang."}, {"q": "What updates or decisions were made in the discussion?", "a": "I assume this got fixed along the way, either by [~tdas]'s changes or [~shaneknapp]'s changes to Jenkins?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1531", "title": "GraphX should have messageRDD to enable OutOfCore messages", "status": "Resolved", "priority": "Major", "reporter": "Jianfeng Jia", "assignee": null, "labels": [], "created": "2014-04-18T05:46:21.000+0000", "updated": "2016-01-04T14:42:57.000+0000", "description": "There is no such `messageRDD` in Pregel function.\nMost of the sendMessage is directly sent one Scala Iterator. Like the below one in staticPageRank:\n```\ndef sendMessage(edge: EdgeTriplet[Double, Double]) =\n      Iterator((edge.dstId, edge.srcAttr * edge.attr))\n```\nFor some message intensively computation on some bigger graph, it will throw OOM exceptions. If we have some more general messageRDD, at lease we can set MessageRDD.persist(DISK) to enable it flush onto the disk.\n", "comments": [], "derived": {"summary": "There is no such `messageRDD` in Pregel function. Most of the sendMessage is directly sent one Scala Iterator.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "GraphX should have messageRDD to enable OutOfCore messages - There is no such `messageRDD` in Pregel function. Most of the sendMessage is directly sent one Scala Iterator."}]}}
{"project": "SPARK", "issue_id": "SPARK-1532", "title": "provide option for more restrictive firewall rule in ec2/spark_ec2.py", "status": "Resolved", "priority": "Minor", "reporter": "Art Peel", "assignee": null, "labels": [], "created": "2014-04-18T06:14:18.000+0000", "updated": "2015-01-19T03:15:16.000+0000", "description": "When ec2/spark_ec2.py sets up firewall rules for various ports, it uses an extremely lenient hard-coded value for allowed IP addresses: '0.0.0.0/0'\n\nIt would be very useful for deployments to allow specifying the allowed IP addresses as a command-line option to ec2/spark_ec2.py.  This new configuration parameter should have as its default the current hard-coded value, '0.0.0.0/0', so the functionality of ec2/spark_ec2.py will change only for those users who specify the new option.\n", "comments": ["I'll be submitting a pull request shortly.", "https://github.com/apache/spark/pull/445 \n\n(subsequently closed and replaced by https://github.com/apache/spark/pull/453 ) ", "the original pull request failed on Travis-CI due to a timeout compiling scala code.  That seems extremely unlikely to have resulted from my changes to ec2/spark_ec2.py so I have generated a new pull request: https://github.com/apache/spark/pull/453", "[~foundart] Is this abandoned? your second PR was ready to merge but needed rebasing, then got closed. Looks like it was a good change that can be revived.", "I believe this capability is now present (at least in {{master}}) under the following option:\n\n{code}\n  --authorized-address=AUTHORIZED_ADDRESS\n                        Address to authorize on created security groups\n                        (default: 0.0.0.0/0)\n{code}\n\n[~foundart] - Can  you confirm this solves your problem?", "I'm resolving this as Fixed since, as far as I can tell, the requested functionality now exists. Feel free to reopen with clarification if not."], "derived": {"summary": "When ec2/spark_ec2. py sets up firewall rules for various ports, it uses an extremely lenient hard-coded value for allowed IP addresses: '0.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "provide option for more restrictive firewall rule in ec2/spark_ec2.py - When ec2/spark_ec2. py sets up firewall rules for various ports, it uses an extremely lenient hard-coded value for allowed IP addresses: '0."}, {"q": "What updates or decisions were made in the discussion?", "a": "I'm resolving this as Fixed since, as far as I can tell, the requested functionality now exists. Feel free to reopen with clarification if not."}]}}
{"project": "SPARK", "issue_id": "SPARK-1533", "title": "The (kill) button in the web UI is visible to everyone.", "status": "Resolved", "priority": "Blocker", "reporter": "Xiangrui Meng", "assignee": null, "labels": [], "created": "2014-04-18T08:36:44.000+0000", "updated": "2014-05-20T14:21:29.000+0000", "description": "We can kill jobs from web UI now, which is great. But there is no authentication in the standalone mode, e.g., clusters created by spark-ec2.\nThen everyone can visit a standalone server and kill jobs.", "comments": ["We'd like to keep it visible and let security conscious people disable it.", "I think we should add another set of acls for this.  There is authentication in the UI if you create a servlet filter for it.  Currently there are view acls and we should add something like modify acls that would allow certain users to modify the running job.  \n\nI will file a separate jira for that and people can comment on that idea.  https://issues.apache.org/jira/browse/SPARK-1890"], "derived": {"summary": "We can kill jobs from web UI now, which is great. But there is no authentication in the standalone mode, e.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "The (kill) button in the web UI is visible to everyone. - We can kill jobs from web UI now, which is great. But there is no authentication in the standalone mode, e."}, {"q": "What updates or decisions were made in the discussion?", "a": "I think we should add another set of acls for this.  There is authentication in the UI if you create a servlet filter for it.  Currently there are view acls and we should add something like modify acls that would allow certain users to modify the running job.  \n\nI will file a separate jira for that and people can comment on that idea.  https://issues.apache.org/jira/browse/SPARK-1890"}]}}
{"project": "SPARK", "issue_id": "SPARK-1534", "title": "spark-submit for yarn prints warnings even though calling as expected ", "status": "Resolved", "priority": "Major", "reporter": "Thomas Graves", "assignee": "Patrick Wendell", "labels": [], "created": "2014-04-18T16:03:35.000+0000", "updated": "2014-04-29T01:16:57.000+0000", "description": "I am calling spark-submit to submit application to spark on yarn (cluster mode) and it is still printing warnings:\n\n$ ./bin/spark-submit  examples/target/scala-2.10/spark-examples_2.10-assembly-1.0.0-SNAPSHOT.jar  --master yarn --deploy-mode cluster --class org.apache.spark.examples.SparkPi --arg yarn-cluster --properties-file ./spark-conf.properties \nWARNING: This client is deprecated and will be removed in a future version of Spark.\nUse ./bin/spark-submit with \"--master yarn\"\n--args is deprecated. Use --arg instead.\n\n\nThe --args is deprecated is coming out because SparkSubmit itself needs to be updated to --arg. \n\nSimilarly I think the Client.scala class for yarn needs to have the \"Use ./bin/spark-submit with \"--master yarn\"\" warning removed since SparkSubmit also calls it directly.\n\nI think the last one was supposed to warn users using spark-class directly. ", "comments": [], "derived": {"summary": "I am calling spark-submit to submit application to spark on yarn (cluster mode) and it is still printing warnings:\n\n$. /bin/spark-submit  examples/target/scala-2.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "spark-submit for yarn prints warnings even though calling as expected  - I am calling spark-submit to submit application to spark on yarn (cluster mode) and it is still printing warnings:\n\n$. /bin/spark-submit  examples/target/scala-2."}]}}
{"project": "SPARK", "issue_id": "SPARK-1535", "title": "jblas's DoubleMatrix(double[]) ctor creates garbage; avoid", "status": "Resolved", "priority": "Trivial", "reporter": "Tor Myklebust", "assignee": "Tor Myklebust", "labels": [], "created": "2014-04-18T17:27:44.000+0000", "updated": "2014-04-19T22:11:56.000+0000", "description": "The DoubleMatrix constructor that wraps a double[] and presents it as a row vector in jblas-1.2.3 new's a double[] and then immediately discards it.  It is straightforward to replace uses of this constructor with the (int, int, double...) constructor; perhaps this should be done until jblas-1.2.4 is released.", "comments": [], "derived": {"summary": "The DoubleMatrix constructor that wraps a double[] and presents it as a row vector in jblas-1. 2.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "jblas's DoubleMatrix(double[]) ctor creates garbage; avoid - The DoubleMatrix constructor that wraps a double[] and presents it as a row vector in jblas-1. 2."}]}}
{"project": "SPARK", "issue_id": "SPARK-1536", "title": "Add multiclass classification tree support to MLlib", "status": "Resolved", "priority": "Critical", "reporter": "Manish Amde", "assignee": "Manish Amde", "labels": [], "created": "2014-04-18T21:45:33.000+0000", "updated": "2014-07-18T21:00:30.000+0000", "description": "The current decision tree implementation in MLlib only supports binary classification. This task involves adding multiclass classification support to the decision tree implementation.\n\nThe tasks involves:\n- Choosing a good strategy for multiclass classification among multiple options:\n  -- add multi class support to impurity but it won't work well with the categorical features since the centriod-based ordering assumptions won't hold true\n  -- error-correcting output codes\n  -- one-vs-all\n- Code implementation\n- Unit tests\n- Functional tests\n- Performance tests\n- Documentation\n", "comments": ["Issue resolved by pull request 886\n[https://github.com/apache/spark/pull/886]"], "derived": {"summary": "The current decision tree implementation in MLlib only supports binary classification. This task involves adding multiclass classification support to the decision tree implementation.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add multiclass classification tree support to MLlib - The current decision tree implementation in MLlib only supports binary classification. This task involves adding multiclass classification support to the decision tree implementation."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 886\n[https://github.com/apache/spark/pull/886]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1537", "title": "Add integration with Yarn's Application Timeline Server", "status": "Resolved", "priority": "Major", "reporter": "Marcelo Masiero Vanzin", "assignee": null, "labels": [], "created": "2014-04-19T00:02:02.000+0000", "updated": "2020-04-22T01:03:53.000+0000", "description": "It would be nice to have Spark integrate with Yarn's Application Timeline Server (see YARN-321, YARN-1530). This would allow users running Spark on Yarn to have a single place to go for all their history needs, and avoid having to manage a separate service (Spark's built-in server).\n\nAt the moment, there's a working version of the ATS in the Hadoop 2.4 branch, although there is still some ongoing work. But the basics are there, and I wouldn't expect them to change (much) at this point.", "comments": ["I'm working on this but this all sort of depends on progress being made on the Yarn side, so at this moment I'm not yet ready to send any PRs.", "I am also interested in it and trying to integrate spark to yarn timeline server. Do you have any concrete plan in mind? I can start prototype it and then we can work together on this topic.  How do you think?", "I have a prototype ready. But I'm still investigating some issues with the Yarn side of things (mostly around security and scalability). Given that I have some code pretty much ready, you're welcome to spend time on it, but you'd be duplicating work I already have done.", "Do you mind sharing your thoughts, design document or prototype code?\n\nThanks.", "Currently busy with other more urgent tasks, but I'll push to my repo and post a link when I get some time.", "Current code is here:\nhttps://github.com/vanzin/spark/tree/yarn-timeline\n\nVery much WIP at this point.", "Thanks for sharing this. Do you have concrete plan or timeline for this Jira?", "No concrete timeline at the moment. I'm just starting to look at the 2.5.0 version of ATS so I can incorporate things into my patch.", "Do you have any update on this, or any schedule in your mind yet?", "No set schedule as of now. The current code \"works\", but it's blocked by at least one bug I filed against Yarn (YARN-2444).\n\nAlso, I'm not comfortable with the current ATS design. There's discussion on YARN-1530 about making it better and I want to wait until that work at least starts, in case it causes changes in the API. While it's possible to submit the code without the Yarn changes in, I'm loth to add support for something that just isn't production-ready yet.", "Hi Marcelo,\n\nDo you have update on this? If you don't mind, I can work on your branch to get this done asap. Please let me know how do you think?\n", "Hi Zhan,\n\nAs I mentioned, I'm waiting for issues being discussed in YARN-1530 to be resolved first. The current plans, as far as I am aware, would result in incompatible API changes in the timeline server API, so I'd rather wait for that before pushing any solution in Spark.\n\nYou're free to come up with your own solution if you want, but I would seriously recommend waiting for the timeline server to actually reach production-level quality before going with integration, especially as far as its API goes.", "[~vanzin], thanks for introducing YARN timeline server to Spark. Let me briefly summarize the current status of the timeline server and answer some concerns here. Spark folks who are interested in this monitoring service offered by YARN can go ahead to YARN-1530 to read the design doc and watch the latest progress.\n\n1. The essential functions or the timeline service have been available since Hadoop 2.4. Basically, the user can organize the app's history or metrics according to timeline data model and post it the the timeline server. Later on, user or admin can come back to query this information to analyze how the app was going. The essential APIs keep unchanged from 2.4 to the coming 2.6. There should *NOT* be any incompatible API changes that will block this work. Moreover, Keeping compatible is always in our consideration when coming up with new features in the following Hadoop releases.\n\n2. It's *NOT* exactly that the timeline server is not production-ready. In fact, Apache Tez has already integrated the timeline server for logging the history information. In the coming Hadoop 2.6, MapReduce is also enabled to publish the history information to the timeline server, too. Moreover, within the scope of YARN, a built-in generic history service on top of the timeline service is available to YARN users to watch all kinds of apps. Hence, with several successful pioneer, Spark should be confident enough to take the new merit of YARN.\n\n3. While YARN community is progressing quickly to improve the timeline server in terms of security (coming 2.6), high availability, scalability, better client libs and so on, it should not disturb the initial attempt for Spark to embrace the timeline server, but will offer better experience if Spark is riding on it.\n\nIf you have other issue of high priority to work on, I think [~zhazhan] will be able to help this integration. Thanks!", "bq. ...security (coming 2.6), high availability, scalability, better client libs and so on...\n\nThat's exactly my point about the ATS not being production-level quality yet. The current plans I'm aware of would require changes in the ATS API. Since Spark does not support the ATS at the moment, I'd rather have it support the new-and-secure-and-scalable-and-available API than the current one. Otherwise you'll get into the mess of having to conditionally compile code for both APIs, or implement part of those features into your own client code (something I've done in my proof-of-concept but I'd really like to avoid, because it's really just trying to work around limitations in the current ATS design).\n\nSo, short version of what I'm trying to say: yes, you can build something that talks to the current ATS. But given that it currently has shortcomings, and the fix for those will, as far as I know, affect the client API, I don't see the point in trying to push that integration at this moment when Spark already has a working solution for job history, just so that you'll ship code that will be immediately deprecated by the new ATS...", "bq. That's exactly my point about the ATS not being production-level quality yet. The current plans I'm aware of would require changes in the ATS API.\n\nNot to mention the definition of production ready (which differs from community to community, such as Tez and MapReduce), I'm curious about the required API changes of the timeline server. Please elaborate the *changes* in case I've missed some discussion. On the other side, according to my understanding of the timeline server, the ongoing and the future improvement is: \n\n1) Security is coming with Hadoop 2.6, which doesn't affect the usage of the existing APIs in a insecure mode. AFAIK, Spark is working with Hadoop 2.3(4). It should be okay to ride on the timeline server in insecure mode. Whenever upgrading to Hadoop 2.6, you just need to turn on the security switch.\n\n2) Timeline availability and scalability is going to be a server side improvement, but doesn't affect user-faced API. In the scope of YARN, we have already successfully enhance RM with the HA feature while making it transparent to the user. I'm not aware of the major blocker that prevents the timeline server to achieve the same goal.\n\n3) For the client libs, we're trying to help to users to utilize the timeline service more easily (e.g., YARN-2517, YARN-2673), which are either transparent or additions. As I've mentioned before, we're careful about any proposed changes that will break the incompatibility.\n\nI'm commenting on this Jira to share more insights about the timeline server to Spark folks in case the folks interested in this YARN offer. It's up to Spark folks to decide whether they want to make use of it or when they make use of it.", "bq. Please elaborate the changes in case I've missed some discussion\n\nThat's part of why I'm waiting on SPARK-1530. There's been no activity in a while; I've been told there have been offline discussions but I don't see any updates on the actual issue itself, so that's the main reason why I've been holding off on this work: I don't feel it's a good investment of time to go forward with something that might change in the near future.\n\nIt would be great if you could update that bug with a concrete plan for the post-2.6 updates related to reliability and other features. If they really don't affect the client API, then great, I can continue my Spark-side work without worries. But again, I've mainly been waiting because of the radio silence from the ATS side w.r.t. the issues that I think are important to Spark.", "BTW, if you want a list of things I think are important for Spark, here are some quick ones:\n\n* YARN-2521 (I've sort of implemented this in my code, but would really like to not have to care about it)\n* YARN-2423 (note how this is a new API)\n* YARN-2444\n\nYARN-2521 might be the same as YARN-2673, no? YARN-2513 is sort of interesting but not necessary.", "Yarn-2521 can make client easier to use, but not critical. Some application logic make the client cache difficult to be generic.\nYarn-2444 may be already obsolete. ", "I think it's pretty critical when you can't upload your data because the server is down; it means we can't really recommend using the current ATS because it's not reliable. I understand it doesn't affect the client API and we can still have the code in, but it's an important feature that seems to be missing.\n\nYARN-2423, though, is really something that can't be done today without poking into private Yarn classes or writing a bunch of extra code. I really wouldn't want to have to support any of those two options in Spark.", "bq. BTW, if you want a list of things I think are important for Spark, here are some quick ones:\n\nThanks for sharing the details, which are more helpful to clean up the puzzles than some big but vague statement. Let me go through the aforementioned Jiras:\n\n* YARN-2521: I'd like to keep it open for some further client improvement, such as local timeline data caching, while YARN-2673 already made the client retry when the server temporally doesn't respond. Please note that \"I think it's pretty critical when you can't upload your data because the server is down\" is *no longer true* after YARN-2673. On the other side, At the point of view of the API, it should keep stable.\n\n* YARN-2423: This is proposed to improve the Java libs by adding GET APIs. They are used to query data, NOT to put data. We do this to help the use case that the developers write Java code to implement the UI to analyze the timeline data. Framework integration mainly deals with PUT APIs, and the Java client libs are already there. Take one step back, apart from the client libs, the RESTful APIs are always there, which is programming language neutral, and useful to non-Java developers.\n\n* YARN-2444: It's may be a bug or an improper use case. According to the exception, the user doesn't pass the authorization for some reason. It is reported for 2.5, and is probably no longer valid after we fixed a bunch of security issues for 2.6. We need to do more validation for this issue before a conclusion. Anyway, it's obviously an internal issue happening in secure mode only, which should not the API CHANGES.\n\nbq. I understand it doesn't affect the client API and we can still have the code in,\n\nIt seems that we have the agreement that the current timeline service offering is not blocking the Spark integration work.\n", "bq. This is proposed to improve the Java libs by adding GET APIs. They are used to query data, NOT to put data.\n\nSpark needs both to put and read data, otherwise the ATS is useless for Spark. The current goal of Spark is to use the ATS as a store for its history data, since the data itself is not considered public and stable itself.\n\nSo there is no point in integration if you can only write data. (I know you can read data through other means, but I don't want to write a custom REST client just to get ATS support in.)\n\nbq.  It is reported for 2.5, and is probably no longer valid after we fixed a bunch of security issues for 2.6.\n\nI'm not sure why you say it's security-related since there nothing security-related in the example code I posted. And if something doesn't work in 2.5 but works in 2.6, it means we (and by that I mean Spark) have to restrict our support to the versions where things work - even if the underlying API is exactly the same.", "bq. Spark needs both to put and read data\n\nIt's again a vague statement. Can you share your design detail, such that we can evaluate it is really necessary? \nAnd what is the actual way of visualizing data? And integration work is not just single bug fix patch, we can divide work into a sequent of sub tasks, and the first step is to enable Spark job to be able to putting the data into the timeline server. By doing this, not only Spark's only web front can visualize job history, it also enable the third-party tools to do Spark job analysis too.\n\nbq. I'm not sure why you say it's security-related since there nothing security-related in the example code I posted. \n\nI said \"According to the exception, the user doesn't pass the authorization for some reason.\" If you don't agree on it, please post your investigation on YARN-2444, YARN folks will help you on this issue.\n\nbq. if something doesn't work in 2.5 but works in 2.6,\n\nNo matter the integration with timeline service, Spark on YARN is picking Hadoop versions now. It doesn't make sense to ask for a feature by using an early version that hasn't it.\n", "bq. It's again a vague statement. \n\nI don't know what is vague about wanting to read the data you write.\n\nbq. Can you share your design detail\n\nI already did way better than that, way earlier in this bug: I shared the actual code. For this particular question, here it is:\nhttps://github.com/vanzin/spark/blob/yarn-timeline/yarn/timeline/src/main/scala/org/apache/spark/deploy/yarn/timeline/YarnTimelineProvider.scala\n\nSee how it reads data from the ATS? It feeds it into the Spark history server, where the data can be visualized. It's using Yarn internal APIs, which is generally bad practice.\n\nbq.  If you don't agree on it, please post your investigation on YARN-2444, YARN folks will help you on this issue.\n\nI posted the error and the code to reproduce it. I don't know what else do you expect from me. If you think it's an authorization issue, test it with 2.6 and close the bug if you believe it's fixed.\n\nbq. No matter the integration with timeline service, Spark on YARN is picking Hadoop versions now. It doesn't make sense to ask for a feature by using an early version that hasn't it.\n\nI'm not sure I really understood what you're trying to say here. Yes, we have to pick versions. We need a version that supports the features we need. Even if the API in 2.5 didn't change in 2.6, it seems to have bugs that prevent my current code from working, so there is no point in trying to integrate with 2.5 as far as I'm concerned. And as far as I know, 2.6 hasn't been released yet. (BTW, my code used to work with 2.4.)", "I believe with YARN-2033 and YARN-2423 I can work around YARN-2444 even if it's still an issue, so I'll add the dependency accordingly.", "I have sent a PR with WIP for people who are interested. \nhttps://github.com/apache/spark/pull/4683/files", "User 'zhzhan' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/4683", "Patch against v1.2.1", "High level design doc for spark ATS integration.", "Hi [~zzhan], thanks for uploading the document.\n\nReading through it, I don't see anything that is really that much different from my initial proof-of-concept. The points I'd like to highlight are:\n\n- It still depends on YARN-2423, or at least on some effort to write a REST client that does not depend on internal Yarn classes.\n- What about overhead of the read code? Large jobs with lots of tasks, or really long jobs such as Spark Streaming jobs, will have a really large amount of events. Fetching them all in one batch would require a lot of memory for serializing the data on both sides (ATS and History Server).\n- Any security considerations? I haven't really kept up-to-date with the security changes in the ATS after I ran into issues with my p.o.c.; but mainly, does the Spark job need any special tokens to talk to the ATS when security is enabled? Does the ATS guarantee that only the job itself (or someone with the right credentials) can add events to its timeline? Or is that all handled transparently, somehow, by the client library?\n- Does YARN-2928 affect the design in any way? I took a quick look at the data model, so hopefully they'll keep things backwards compatible. But it would kinda suck to add support for an API with a limited shelf life if that's not the case.\n", "[~vanzin] Thanks for the comments.  I  don't understand you keep saying \"my code does not have many differences form your code.\"   We are working for apache project, and we all follow apache policy. Here is the link for apache license details:\nhttp://www.apache.org/licenses/LICENSE-2.0.\n\nSince you think your prototype is ready half year ago, as I request several times, why not post your workable patch and design and move forward. I will explain to you clearly \"what's the major difference of  the core design of my code from yours\" . The patch size is small, and the design is not so complicated, but I am sure to show you where those core design come from.\n\nAfter you post your design and code, we can start from there.\n\nThanks.\n\nZhan Zhang\n", "Hi [~zzhan],\n\nI already posted the link to my code in this bug several times. The reason why I haven't sent a PR is the exact reason I raised about your spec and your patch: it uses private Yarn APIs. I've said this several times, and I really don't understand what part of it you don't understand. Pardon me if I haven't been clear about it.\n\nAlso note how there's Yarn bug in the list of blocker bugs for this one. That's because my p.o.c. code depends on that bug to be fixed before it can move forward. If you have a design that is not blocked by that code, and does not use internal APIs, feel free to remove the link and post it.\n\nHere's the link to the comment with the link to my code, dated August '14:\nhttps://issues.apache.org/jira/browse/SPARK-1537?focusedCommentId=14088438&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14088438\n\nA link you have already seen, since you used parts of that code in your patch.\n\nSo please, can you reply to my actual comments instead of keep going back to this issue? My comments have nothing to do with the fact that I've written a p.o.c. for this feature. They're issues that exist in your spec and your code independent of anything I've done.", "[~vanzin] If you don't have bandwidth, or don't know how to move forward with this JIRA after a long time. I don't mind to take it over.", "[~vanzin] I declare \"integrate your code\" from the first submission of PR. Do you want to count how many times you keeping saying this? \n\n \"Here's the link to the comment with the link to my code, dated August '14\".  Now spark is under the vote for 1.3, and today is 2/20/2015.  Is it so difficult submit a workable patch and design doc? ", "It's impossible to submit a patch when the implementation is currently blocked on a feature that doesn't exist in Yarn. Please check the \"is blocked by\" link at the top of this bug.\n\nIf you're willing to write the code to work around that missing feature, please include that in your spec and patch. I am not and would rather wait for Yarn instead.", "[~zzhan] I also can't figure out what you are suggesting here. You have proposed a patch, and you've been given feedback with specific reasons it shouldn't be committed to Spark. I agree with those, FWIW, thought I think they can be overcome soon. I assume others agree, given the silence (?). You haven't responded to these specific points. As it stands I think that's your answer: these YARN issues need to be addressed -- either fixed or agreed to be not an issue.\n\nNobody needs to 'take over'. I'm not clear why you think you have been waiting on something or someone to give you code. Right now the only thing this is waiting on is for you or [~zjshen] or anyone to address the YARN API issues. Rather than keep the broken record going, why not address the YARN API issues highlighted here? sorry, the answer may be that you can't commit this patch you want to by yourself but that's just how OSS works.", "[~sowen] From the whole context, I believe you understand what happened here. Let's be professional. \n\nMy request is \"if someone want to try this alpha feature, we can provide a patch at least so that people can give it a try. Even if it cannot go upstream due to various reasons.\" \n\nDue to Yarn block, we should discuss with the yarn community, instead of filing a bug and wait forever. ", "[~zzhan] You have provided a patch as a PR right? anyone can try it. Request granted.\n\nGiven the YARN JIRAs already referenced here, some of which have patches ready to go too, I think it has been discussed in YARN too? What isn't happening with YARN that should be, and, can you help with it? I'm not sure if that's where you are saying the waiting is. That is: hasn't this been blocked on YARN changes for a long time?\n\nI get it, one person's 'outstanding bug' is another's 'will not fix' but that's the give and take of OSS. If you want this feature in Spark, and people are asking that it should depend on some YARN changes -- then what do you think about lobbying for those YARN changes? or do you disagree that they're necessary, and can you argue that here please?\n\nI don't understand your second reply. Yes, it sounds like two people have a similar solution with a similar problem with YARN APIs. You say you're not waiting on code now, but have repeatedly asked Marcelo to share some (other?) code. It's odd since, yes, it's very clear you acknowledge you've already seen his code and reused a bit, which is entirely fine. I hope we're done with that exchange.\n\nI sense some insinuation that code is being 'hidden' in bad faith, but I can't figure out the conspiracy. I see every willingness to make *your* change alone here, if you propose something that addresses the YARN issues raised here. You are *not* blocked on anyone else's patch. However all of us are 'blocked' on the consensus of community / committers that care about this issue, and it looks like the response is clear so far: not until YARN API stuff is sorted out one way or the other.\n\nAre you suggesting this patch should be committed without the YARN changes? or that you're working on the YARN changes? what do you want to take over and do next?", "[~sowen] In JIRA, we share the code so that other people can comment and review. I am not waiting for patch. But It is hard to comment or review patch given a hyper-link. \n\nI never think to make my change alone. Actually from the beginning I acknowledge his contribution, and don't mind closing my PR and help to review his at all if you follow the PR record.  Do you agree?\n\nYou mention you sense some insinuation and conspiracy. I didn't sense it. Can you please educate me if you figure it out?\n\nLet's go back to technical: Overall, it is early adoption for timeline service. It is alpha feature, but most functionality is working although with some walkaround.\n\nREST client: Currently Timeline client does not provide retrieve API. So we walk around with the similar approach to the timeclient its own implementation.  This needs to be changed after timeline component provide more mature API.\n\nRead overhead and scalability: The effort is in the roadmap in yarn timeline service.  This is a critical  feature to use timeline service. Current HDFS approach in spark may not scalable due to similar reason (point me out if I am wrong), and timeline service may be more promising, although it is not there yet.\n\nSecurity: The security is handled transparently in timeline client.   \n\nACL: Timeline has ACL control as in hadoop-2.6, and client can create and set domain with R/W so that control the permission. \n\n\n\n", "Hi [~zhzhan],\n\nbq. But It is hard to comment or review patch given a hyper-link. \n\nPerhaps you're not familiar with all of Github's features, but you can click on each individual commit and comment on the code right there, just like you can on a PR created from those commits. Even if that doesn't sound very appealing, it's not hard to copy & paste the code and comment here if you really want to. Or generate a downloadable diff from the commits (just add \".diff\" at the end of the commit URL, e.g. https://github.com/vanzin/spark/commit/c1365e0de264daa015c61a2248c80dfdea705786.diff).\n\nbq. REST client: Currently Timeline client does not provide retrieve API.\n\nThat's the main reason why this feature hasn't moved forward. Using internal APIs to achieve that is something we're not willing to do in Spark, because it exposes us to future breakages and makes compatibility harder to maintain (just look at what has been done for Hive). So we either need the new API in Yarn, or we need to invest time to create a client API that does not use Yarn's classes.\n\nbq. ACL: Timeline has ACL control as in hadoop-2.6\n\nI'll believe you here since I haven't looked at that code yet. But it seems like it requires work on the client side, which is not currently covered in your spec.\nbq. Read overhead and scalability: The effort is in the roadmap in yarn timeline service. This is a critical feature to use timeline service. Current HDFS approach in spark may not scalable due to similar reason\n\nI think we're talking about different things. What I'm referring to is that the current code that reads from the ATS reads all events of a particular entity at the same time. If that entity has a large number of events, that will require a lot of memory on the ATS side to serialize the data, and a lot of memory on the Spark History Server side to deserialize it. It's orthogonal to whether the backing store is scalable or not.", "[~vanzin]  We should centralized all comments and reviews in one place, instead of going to different links. Also, we want to the reviewed code is updated, instead of based on some old version.   \n\nLet's go to technical:\n\n1. We all agree on this one about timeline client, and this is why it is alpha feature. Hive is a good example, but nobody can deny its importance in spark.\n2. ACL is included in the patch, but not in the spec.\n3. I understand your question, but the scope of my respond may be too big. To solve this, more work is needed on the entity design.\n\nLet's keep an eye on these issues. \n", "# I've just tried to see where YARN-2444 stands; I can't replicate it in trunk but I've submitted the tests to verify that it isn't there.\n# for YARN-2423 Spark seems kind of trapped. It needs an api tagged as public/stable; Robert's patch has the API, except it's being rejected on the basis that \"ATSv2 will break it\". So it can't be tagged as stable. So there's no API for GET operations until some undefined time {{t1  > now()}} and then, only for Hadoop versions with it. Which implies it won't get picked up by Spark for a long time.\n\nI think we need to talk to the YARN dev team and see what can be done here. Even if there's no API client bundled into YARN, unless the v1 API and its paths beginning {{/ws/v1/timeline/}} are going to go away, then a REST client is possible; it may just have to be done spark-side, where at least it can be made resilient to hadoop versions. \n", "User 'steveloughran' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/5423", "HADOOP-11826 patches the hadoop compatibility document to add timeline server to the list of stable APIs.", "For people who've not been tracking the WiP\n\n# the timeline API is pretty thoroughly documented with examples; very close to going in\n[Latest TimelineServer.md|https://github.com/steveloughran/hadoop-trunk/blob/stevel/YARN-3539-ATS-compatibility/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/markdown/TimelineServer.md]\n# the timeline server integration is in sync with trunk, especially the SPARK-4705 changes\n# it has lots of tests. This includes: generating events from a spark context and verifying that they are served up by an an-VM timeline server instance, and retrievable by a REST client, bringing up a Spark History server and making GET requests against it to verifying it hooks up to the server, and other cross-system tests. That's about as much as you can do in a standalone unit test suite.\n# those tests all run happily on unix and windows, provided you set the {{-Phadoop-2.6 -Pyarn}} flags to request a Hadoop 2.6 profile. \n# and I've tested against hadoop 2.6.0, 2.7.0 &  branch-2; everything compiles and runs\n\nCan I get some reviews?", "+ YARN-3539 is resolved; the [v1 timeline |https://github.com/apache/hadoop/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/markdown/TimelineServer.md#Timeline_Server_REST_API_v1] is now defined and declared one of the supported REST APIs.\n\nI'm also removing YARN-2423 as a dependency; the latest patch does this itself", "User 'steveloughran' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/8744", "User 'steveloughran' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/9182", "User 'steveloughran' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/10545", "I don't think it makes sense to integrate with the ATS at this point.", "[~vanzin], is that because the community has invested instead in making SHS that central metrics store and UI?  What about clusters with mixed workloads?", "[~templedf] sorry forgot to reply.\r\n\r\nATSv1 wasn't a good match for this, and by the time ATSv2 was developed, interest in this feature had long lost traction in the Spark community. So this was closed.\r\n\r\nAlso you probably can do this without requiring the code to live in Spark.\r\n\r\nBut if you actually want to contribute the integration, there's nothing preventing you from opening a new bug and posting a PR.", "Thanks for the response, [~vanzin].  Yeah, I think we would be interested in exploring the work involved to do the integration.  We're in the process of introducing Spark into Hadoop clusters that primarily run Scalding today.  We're using ATSv2 as the store for all of the Scalding metrics, so it would make sense for us to do the same with Spark.\r\n\r\nAny required reading that we should do as we decide how best to tackle this?  Pointers, tips, tricks, potholes, or any other info would be welcome.  Thanks!", "Well, the only thing to start with is the existing SHS code. EventLoggingListener + FsHistoryProvider."], "derived": {"summary": "It would be nice to have Spark integrate with Yarn's Application Timeline Server (see YARN-321, YARN-1530). This would allow users running Spark on Yarn to have a single place to go for all their history needs, and avoid having to manage a separate service (Spark's built-in server).", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add integration with Yarn's Application Timeline Server - It would be nice to have Spark integrate with Yarn's Application Timeline Server (see YARN-321, YARN-1530). This would allow users running Spark on Yarn to have a single place to go for all their history needs, and avoid having to manage a separate service (Spark's built-in server)."}, {"q": "What updates or decisions were made in the discussion?", "a": "Well, the only thing to start with is the existing SHS code. EventLoggingListener + FsHistoryProvider."}]}}
{"project": "SPARK", "issue_id": "SPARK-1538", "title": "SparkUI forgets about all persisted RDD's not directly associated with the Stage", "status": "Resolved", "priority": "Blocker", "reporter": "Andrew Or", "assignee": "Andrew Or", "labels": [], "created": "2014-04-19T00:49:07.000+0000", "updated": "2014-11-05T10:45:27.000+0000", "description": "The following command creates two RDDs in one Stage:\n\nsc.parallelize(1 to 1000, 4).persist.map(_ + 1).count\n\nMore specifically, parallelize creates one, and map creates another. If we persist only the first one, it does not actually show up on the StorageTab of the SparkUI.\n\nThis is because StageInfo only keeps around information for the last RDD associated with the stage, but forgets about all of its parents. The proposal here is to have StageInfo climb the RDD dependency ladder to keep a list of all associated RDDInfos.", "comments": [], "derived": {"summary": "The following command creates two RDDs in one Stage:\n\nsc. parallelize(1 to 1000, 4).", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "SparkUI forgets about all persisted RDD's not directly associated with the Stage - The following command creates two RDDs in one Stage:\n\nsc. parallelize(1 to 1000, 4)."}]}}
{"project": "SPARK", "issue_id": "SPARK-1539", "title": "RDDPage.scala contains RddPage", "status": "Resolved", "priority": "Major", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-04-19T18:39:23.000+0000", "updated": "2014-04-21T21:28:57.000+0000", "description": "SPARK-1386 changed RDDPage to RddPage but didn't change the filename. I tried sbt/sbt publish-local. Inside the spark-core jar, the unit name is RDDPage.class and hence I got the following error:\n\n{code}\n[error] (run-main) java.lang.NoClassDefFoundError: org/apache/spark/ui/storage/RddPage\njava.lang.NoClassDefFoundError: org/apache/spark/ui/storage/RddPage\n\tat org.apache.spark.ui.SparkUI.initialize(SparkUI.scala:59)\n\tat org.apache.spark.ui.SparkUI.<init>(SparkUI.scala:52)\n\tat org.apache.spark.ui.SparkUI.<init>(SparkUI.scala:42)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:215)\n\tat MovieLensALS$.main(MovieLensALS.scala:38)\n\tat MovieLensALS.main(MovieLensALS.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\nCaused by: java.lang.ClassNotFoundException: org.apache.spark.ui.storage.RddPage\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:366)\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:358)\n\tat org.apache.spark.ui.SparkUI.initialize(SparkUI.scala:59)\n\tat org.apache.spark.ui.SparkUI.<init>(SparkUI.scala:52)\n\tat org.apache.spark.ui.SparkUI.<init>(SparkUI.scala:42)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:215)\n\tat MovieLensALS$.main(MovieLensALS.scala:38)\n\tat MovieLensALS.main(MovieLensALS.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n{code}\n\nThis can be fixed after renaming RddPage to RDDPage.", "comments": [], "derived": {"summary": "SPARK-1386 changed RDDPage to RddPage but didn't change the filename. I tried sbt/sbt publish-local.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "RDDPage.scala contains RddPage - SPARK-1386 changed RDDPage to RddPage but didn't change the filename. I tried sbt/sbt publish-local."}]}}
{"project": "SPARK", "issue_id": "SPARK-1540", "title": "Investigate whether we should require keys in PairRDD to be Comparable", "status": "Resolved", "priority": "Blocker", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "labels": [], "created": "2014-04-19T22:32:17.000+0000", "updated": "2014-04-24T07:31:48.000+0000", "description": "This is kind of a bigger change, but it would make it easier to do sort-based versions of external operations later. We might also get away without it. Note that sortByKey() already does require an Ordering or Comparables.", "comments": ["Resolved here: https://github.com/apache/spark/pull/487. We were able to add an implicit Ordering to most methods while giving it a default value of null, which will allow these to work on un-orderble types too using the current, less efficient code path.", "Note that it will remain to add this to the Java and Python APIs. But in those it can be done through new versions of methods, so it will be doable in a binary-compatible way."], "derived": {"summary": "This is kind of a bigger change, but it would make it easier to do sort-based versions of external operations later. We might also get away without it.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Investigate whether we should require keys in PairRDD to be Comparable - This is kind of a bigger change, but it would make it easier to do sort-based versions of external operations later. We might also get away without it."}, {"q": "What updates or decisions were made in the discussion?", "a": "Note that it will remain to add this to the Java and Python APIs. But in those it can be done through new versions of methods, so it will be doable in a binary-compatible way."}]}}
{"project": "SPARK", "issue_id": "SPARK-1541", "title": "sortByKey requires all data to fit in memory", "status": "Resolved", "priority": "Major", "reporter": "Sandy Ryza", "assignee": "Sandy Ryza", "labels": [], "created": "2014-04-19T22:56:11.000+0000", "updated": "2014-04-19T22:57:58.000+0000", "description": "This is a feature gap with respect to Hadoop MapReduce", "comments": ["Oops already exists"], "derived": {"summary": "This is a feature gap with respect to Hadoop MapReduce.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "sortByKey requires all data to fit in memory - This is a feature gap with respect to Hadoop MapReduce."}, {"q": "What updates or decisions were made in the discussion?", "a": "Oops already exists"}]}}
{"project": "SPARK", "issue_id": "SPARK-1542", "title": "Add ADMM for solving Lasso (and elastic net) problem", "status": "Closed", "priority": "Minor", "reporter": "Shuo Xiang", "assignee": null, "labels": ["features"], "created": "2014-04-20T21:48:51.000+0000", "updated": "2014-04-20T21:53:47.000+0000", "description": "This PR introduces the Alternating Direction Method of Multipliers (ADMM) for solving Lasso (elastic net, in fact) in mllib. \n\nADMM is capable of solving a class of composite minimization problems in a distributed way. Specifically for Lasso (if only L1-regularization) or elastic-net (both L1- and L2- regularization), it requires solving independent systems of linear equations on each partition and a soft-threholding operation on the driver. Unlike SGD, it is a deterministic algorithm (except for the random partition). Details can be found in the [S. Boyd's paper](http://www.stanford.edu/~boyd/papers/admm_distr_stats.html).\n\nThe linear algebra operations mainly rely on the Breeze library, particularly, it applies `breeze.linalg.cholesky` to perform cholesky decomposition on each partition to solve the linear system.\n\nI tried to follow the organization of existing Lasso implementation. However, as ADMM is also a good fit for similar optimization problems, e.g., (sparse) logistic regression, it may worth to re-organize and put ADMM into a separate section.\n\n", "comments": ["Duplicate with SPARK-1543 by mistake. "], "derived": {"summary": "This PR introduces the Alternating Direction Method of Multipliers (ADMM) for solving Lasso (elastic net, in fact) in mllib. ADMM is capable of solving a class of composite minimization problems in a distributed way.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add ADMM for solving Lasso (and elastic net) problem - This PR introduces the Alternating Direction Method of Multipliers (ADMM) for solving Lasso (elastic net, in fact) in mllib. ADMM is capable of solving a class of composite minimization problems in a distributed way."}, {"q": "What updates or decisions were made in the discussion?", "a": "Duplicate with SPARK-1543 by mistake."}]}}
{"project": "SPARK", "issue_id": "SPARK-1543", "title": "Add ADMM for solving Lasso (and elastic net) problem", "status": "Closed", "priority": "Minor", "reporter": "Shuo Xiang", "assignee": null, "labels": ["features"], "created": "2014-04-20T21:49:04.000+0000", "updated": "2015-02-21T00:04:31.000+0000", "description": "This PR introduces the Alternating Direction Method of Multipliers (ADMM) for solving Lasso (elastic net, in fact) in mllib. \n\nADMM is capable of solving a class of composite minimization problems in a distributed way. Specifically for Lasso (if only L1-regularization) or elastic-net (both L1- and L2- regularization), in each iteration, it requires solving independent systems of linear equations on each partition and a subsequent soft-threholding operation on the driver machine. Unlike SGD, it is a deterministic algorithm (except for the random partition). Details can be found in the [S. Boyd's paper](http://www.stanford.edu/~boyd/papers/admm_distr_stats.html).\n\nThe linear algebra operations mainly rely on the Breeze library, particularly, it applies `breeze.linalg.cholesky` to perform cholesky decomposition on each partition to solve the linear system.\n\nI tried to follow the organization of existing Lasso implementation. However, as ADMM is also a good fit for similar optimization problems, e.g., (sparse) logistic regression, it may be worth reorganizing and putting ADMM into a separate section.\n", "comments": ["close at this time for more design discussion"], "derived": {"summary": "This PR introduces the Alternating Direction Method of Multipliers (ADMM) for solving Lasso (elastic net, in fact) in mllib. ADMM is capable of solving a class of composite minimization problems in a distributed way.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add ADMM for solving Lasso (and elastic net) problem - This PR introduces the Alternating Direction Method of Multipliers (ADMM) for solving Lasso (elastic net, in fact) in mllib. ADMM is capable of solving a class of composite minimization problems in a distributed way."}, {"q": "What updates or decisions were made in the discussion?", "a": "close at this time for more design discussion"}]}}
{"project": "SPARK", "issue_id": "SPARK-1544", "title": "Add support for deep decision trees.", "status": "Closed", "priority": "Major", "reporter": "Manish Amde", "assignee": "Manish Amde", "labels": [], "created": "2014-04-21T00:00:43.000+0000", "updated": "2014-06-19T21:46:22.000+0000", "description": "The current tree implementation stores an Array[Double] of size O(#features \\ #splits * 2^maxDepth)* in memory for aggregating histograms over partitions. The current implementation might not scale to very deep trees since the memory requirement grows exponentially with tree depth. \n\nThis task enables construction of arbitrary deep trees.", "comments": ["PR here.\nhttps://github.com/apache/spark/pull/475", "The PR has been accepted."], "derived": {"summary": "The current tree implementation stores an Array[Double] of size O(#features \\ #splits * 2^maxDepth)* in memory for aggregating histograms over partitions. The current implementation might not scale to very deep trees since the memory requirement grows exponentially with tree depth.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add support for deep decision trees. - The current tree implementation stores an Array[Double] of size O(#features \\ #splits * 2^maxDepth)* in memory for aggregating histograms over partitions. The current implementation might not scale to very deep trees since the memory requirement grows exponentially with tree depth."}, {"q": "What updates or decisions were made in the discussion?", "a": "The PR has been accepted."}]}}
{"project": "SPARK", "issue_id": "SPARK-1545", "title": "Add Random Forest algorithm to MLlib", "status": "Resolved", "priority": "Major", "reporter": "Manish Amde", "assignee": "Joseph K. Bradley", "labels": [], "created": "2014-04-21T00:16:03.000+0000", "updated": "2014-09-29T04:45:30.000+0000", "description": "This task requires adding Random Forest support to Spark MLlib. The implementation needs to adapt the classic algorithm to the scalable tree implementation.\n\nThe tasks involves:\n- Comparing the various tradeoffs and finalizing the algorithm before implementation\n- Code implementation\n- Unit tests\n- Functional tests\n- Performance tests\n- Documentation", "comments": ["User 'jkbradley' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/2435", "Issue resolved by pull request 2435\n[https://github.com/apache/spark/pull/2435]"], "derived": {"summary": "This task requires adding Random Forest support to Spark MLlib. The implementation needs to adapt the classic algorithm to the scalable tree implementation.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add Random Forest algorithm to MLlib - This task requires adding Random Forest support to Spark MLlib. The implementation needs to adapt the classic algorithm to the scalable tree implementation."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 2435\n[https://github.com/apache/spark/pull/2435]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1546", "title": "Add AdaBoost algorithm to Spark MLlib", "status": "Resolved", "priority": "Major", "reporter": "Manish Amde", "assignee": "Manish Amde", "labels": ["bulk-closed"], "created": "2014-04-21T00:18:45.000+0000", "updated": "2019-06-06T13:57:46.000+0000", "description": "This task requires adding the AdaBoost algorithm to Spark MLlib. The implementation needs to adapt the classic AdaBoost algorithm to the scalable tree implementation.\n\nThe tasks involves:\n- Comparing the various tradeoffs and finalizing the algorithm before implementation\n- Code implementation\n- Unit tests\n- Functional tests\n- Performance tests\n- Documentation", "comments": ["This work was stalled due to decision tree optimizations and RF algo addition tasks. I will create a PR for this task as soon as we complete SPARK-1547.", "I'm curious about the status of this and other subtasks of SPARK-3703 -- I don't think this has been active for a while, and am also wondering if the new Pipelines API changes how something like boosting might fit in. I assume it can be fit in as an Estimator built on Estimators (not sure I have that right)?", "I haven't worked on it since we haven't heard a need for it post RF and GBT work. :-) \n\nThis might be best done after the API standardization work on https://issues.apache.org/jira/browse/SPARK-6113", "Just commenting: When the time comes to resume this, it will be nice to have it under Pipelines, taking generic weak learners.", "42 bulk-closed issues were still \"In Progress\" when they should be resolved."], "derived": {"summary": "This task requires adding the AdaBoost algorithm to Spark MLlib. The implementation needs to adapt the classic AdaBoost algorithm to the scalable tree implementation.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Add AdaBoost algorithm to Spark MLlib - This task requires adding the AdaBoost algorithm to Spark MLlib. The implementation needs to adapt the classic AdaBoost algorithm to the scalable tree implementation."}, {"q": "What updates or decisions were made in the discussion?", "a": "42 bulk-closed issues were still \"In Progress\" when they should be resolved."}]}}
{"project": "SPARK", "issue_id": "SPARK-1547", "title": "Add gradient boosting algorithm to MLlib", "status": "Resolved", "priority": "Major", "reporter": "Manish Amde", "assignee": "Manish Amde", "labels": [], "created": "2014-04-21T00:20:02.000+0000", "updated": "2015-01-05T21:40:05.000+0000", "description": "This task requires adding the gradient boosting algorithm to Spark MLlib. The implementation needs to adapt the gradient boosting algorithm to the scalable tree implementation.\n\nThe tasks involves:\n- Comparing the various tradeoffs and finalizing the algorithm before implementation\n- Code implementation\n- Unit tests\n- Functional tests\n- Performance tests\n- Documentation\n\n[Ensembles design document (Google doc) | https://docs.google.com/document/d/1J0Q6OP2Ggx0SOtlPgRUkwLASrAkUJw6m6EK12jRDSNg/]", "comments": ["Honestly trees are most useful when the feature vectors are dense. Any possibility that the solver can be decoupled from the tree part for dealing with sparse data?", "Yes, the loss function and the solver should ideally be independent of the tree algorithm and hence work with sparse data. Any particular non-tree algorithm you had in mind?\n\nI will definitely keep your suggestion in mind during the implementation (coming up soon)  but might postpone it for a later release if it involves a lot more work than implementing it for decision trees since the goal is to get ensembles built on top of decision trees ASAP.", "Just generic log loss with L1 regularization should suffice. Most of the work is in feature engineering anyway. It is no hurry at all, I already have several implementations not in MLLib that I am using. It would just be convenient to have another implementation to compare against.", "[~hector.yee] I strongly agree about keeping ensembles general enough to work with any weak learning algorithm.  This is difficult now because of the lack of a general class hierarchy, but that will be easier after the [current API redesign|https://issues.apache.org/jira/browse/SPARK-1856].  Starting with trees, and later generalizing once the new API is available, will be great.", "Given the interesting in boosting algorithms for the MLlib 1.2 release, I have revived the boosting work that Hirakendu Das and I worked on a few months ago but was stalled due to the decision tree optimization effort. \n\nA basic version of the GBT code with \"pluggable\" loss functions can be found here:\nhttps://github.com/manishamde/spark/compare/gbt?diff=unified\n\nI will put it up for a WIP PR in a few days if I don't hear any major concerns.\n\nHere are a few things that are left:\n1. Stochastic gradient boosting support -- I am waiting for the [RF ticket|https://issues.apache.org/jira/browse/SPARK-1545] to be closed so that I can re-use the BaggedPoint approach.\n2. Checkpointing -- This approach will avoid long lineage chains. Need Hirakendu's inputs on this especially his findings on large-scale experiments. Also need to conduct experiments of my own.\n3. Unit Tests -- I have done some basic test but I need to add unit tests.\n4. Classification support -- It should be straightforward to add\n5. Create public APIs\n6. Tests on multiple cluster sizes and datasets -- require help from the community on this front. \n\nFeedback will be appreciated.", "This will be great to have!  The WIP code and the list of to-do items look good to me.\n\nSmall comment: For the losses, it would be good to rename \"residual\" to either \"pseudoresidual\" (following Friedman's paper) or to \"lossGradient\" (which is more literal/accurate).  It would also be nice to have the loss classes compute the loss itself, so that we can compute that at the end (and later track it along the way).\n", "Sure. I like your naming suggestion. \n\nI will rebase from the latest master now that the RF PR has been accepted.  I will create a WIP PR soon after (with tests and docs) so that we can discuss the code in greater detail.", "Adding Hirakendu's feedback on checkpointing below.\n\n~~~~~~~~~~~~~\n\nHi Manish,\n\nJust looked at the JIRA, quite impressive progress. Will create a JIRA account and submit the Apache CLA, long time procrastinating :).\n\nAbout checkpointing, as I may have discussed before, I had a very crude solution to the problem with long lineage chains. I simply cache the residual dataset periodically after a certain number of iterations. I uncache the previous cached dataset prior to caching the current one. It does materialize the dataset, but apparently doesn't break the lineage graph. Nonetheless, in practice I see that speed improves to as if I started afresh. It's a crude form of checkpointing, but it would be good to do the real thing.\n\nAbout the choice of interval, obviously it depends on the dataset. Suppose there is a linear increase in time with iterations (the map operation to subtract the previous tree predictions), and say it takes time t for each such map operation. If we break the chain every B iterations, then the time for such map operations is t + 2t + ... + Bt ~= B^2 * t. Suppose the time taken to checkpoint/materialize is c, then the time for the period of B iterations is  B^2 * t + c. Thus, the time per iteration is (B^2 * t  + c)/B = B*t + c/B. Thus, B = sqrt(c/t). (Ignoring the -1s and factor of 2s in B^2 * t.)\n\nOf course, t and c depend on the dataset. So setting aside all calculations, in my implementation B is a user defined parameter. In practice, I run it once and see how much time it takes current iteration and when it slows down to the extent that it's same as the first iteration. It means might have as started afresh. Note that mathematically, for optimal B = sqrt(c/t), the last iteration takes time B^2 t = c, which is same as the checkpoint time.\n\nClearly, this can be automated by keeping track of iteration times, just subtract the actual computation time and leave some room for noisy running times. Nonetheless, would be good to have a user defined override by a optional parameter.\n\nI think a similar optional parameter may also be provided to override the automatically determined batch size for deep trees.\n\nBtw, the comment in the JIRA about support for sparse vectors is something nice to have and something I have been thinking about. Sparse datasets are now far too common and flexible, although the go-to solution is by linear models.\n\nThanks,\nHirakendu.", "User 'manishamde' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/2607", "Issue resolved by pull request 2607\n[https://github.com/apache/spark/pull/2607]"], "derived": {"summary": "This task requires adding the gradient boosting algorithm to Spark MLlib. The implementation needs to adapt the gradient boosting algorithm to the scalable tree implementation.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Add gradient boosting algorithm to MLlib - This task requires adding the gradient boosting algorithm to Spark MLlib. The implementation needs to adapt the gradient boosting algorithm to the scalable tree implementation."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 2607\n[https://github.com/apache/spark/pull/2607]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1548", "title": "Add Partial Random Forest algorithm to MLlib", "status": "Resolved", "priority": "Major", "reporter": "Manish Amde", "assignee": null, "labels": ["bulk-closed"], "created": "2014-04-21T00:23:25.000+0000", "updated": "2019-05-21T04:13:15.000+0000", "description": "This task involves creating an alternate approximate random forest implementation where each tree is constructed per partition.\n\nThe tasks involves:\n- Justifying with theory and experimental results why this algorithm is a good choice.\n- Comparing the various tradeoffs and finalizing the algorithm before implementation\n- Code implementation\n- Unit tests\n- Functional tests\n- Performance tests\n- Documentation", "comments": ["There was already some work done by Frank Dai that was waiting for the tree implementation to get accepted.", "Same question, curious about the status as there hasn't been activity in a while. It sounds like this is proposed as a separate algorithm, but isn't this just a change to the bootstrapping strategy for RDF? In which case, is this subsumed by the general idea of making bootstrapping available to a bunch of algorithms, as in https://issues.apache.org/jira/browse/SPARK-2516 ?", "I think the idea is to train a different tree on each worker, rather than using the distributed tree learning algorithm.  That could be generalized to any algorithm, but it's a bit different than the bootstrapping JIRA, which seems to be about a wrapper for algorithms.", "We should also leave this ticket unassigned for somebody else to pick up if/when interested.", "[~manishamde] [~sowen] [~josephkb] \nI have small experience in contributions on starter tasks in spark, and found this issue interesting. I was investigating regarding the partial implementation of RF, and found these resources:\n\nhttps://mahout.apache.org/users/classification/partial-implementation.html\nhttps://github.com/apache/mahout/blob/b5fe4aab22e7867ae057a6cdb1610cfa17555311/mr/src/main/java/org/apache/mahout/classifier/df/mapreduce/partial/package-info.java\n\nI thinks analyzing mahout implementation provides a good basis to start analyzing RF partial implementation in theory and practically. If this issue is still important to Spark, It would be great if I can start on it. I can start with creating analysis document for current mahout implementation to assess its performance", "[~srowen] [~josephkb] any updates on the possibility of proceeding with this issue ?"], "derived": {"summary": "This task involves creating an alternate approximate random forest implementation where each tree is constructed per partition. The tasks involves:\n- Justifying with theory and experimental results why this algorithm is a good choice.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Add Partial Random Forest algorithm to MLlib - This task involves creating an alternate approximate random forest implementation where each tree is constructed per partition. The tasks involves:\n- Justifying with theory and experimental results why this algorithm is a good choice."}, {"q": "What updates or decisions were made in the discussion?", "a": "[~srowen] [~josephkb] any updates on the possibility of proceeding with this issue ?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1549", "title": "Add python support to spark-submit script", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Matei Alexandru Zaharia", "labels": [], "created": "2014-04-21T05:03:54.000+0000", "updated": "2014-05-06T22:13:37.000+0000", "description": null, "comments": ["https://github.com/apache/spark/pull/664"], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add python support to spark-submit script"}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/664"}]}}
{"project": "SPARK", "issue_id": "SPARK-1550", "title": "Successive creation of spark context fails in pyspark, if the previous initialization of spark context had failed.", "status": "Resolved", "priority": "Major", "reporter": "Prabin Banka", "assignee": "Josh Rosen", "labels": ["pyspark", "sparkcontext"], "created": "2014-04-21T05:38:17.000+0000", "updated": "2014-07-28T05:55:29.000+0000", "description": "For example;-\nIn PySpark, if we try to initialize spark context with insufficient arguments, >>>sc=SparkContext('local')\nit fails with an exception \nException: An application name must be set in your configuration\n\nThis is all fine. \nHowever, any successive creation of spark context with correct arguments, also fails,\n>>>s1=SparkContext('local', 'test1')\nAttributeError: 'SparkContext' object has no attribute 'master'\n\n\n", "comments": ["this issue as reported is no longer present in spark 1.0, where defaults are provided for app name and master.\n\n{code}\n$ SPARK_HOME=dist PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.8.1-src.zip python\nPython 2.7.5 (default, Feb 19 2014, 13:47:28) \n[GCC 4.8.2 20131212 (Red Hat 4.8.2-7)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> from pyspark import SparkContext\n>>> sc=SparkContext('local')\n[successful creation of context]\n{code}\n\ni believe this should be closed as resolved. /cc: [~pwendell]", "Actually, there's still a similar problem in Spark 1.01:\n\n{code}\n>>> sc = SparkContext('sjkdlsjfsl')\n14/07/26 16:04:14 INFO SecurityManager: Changing view acls to: joshrosen\n14/07/26 16:04:14 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(joshrosen)\n14/07/26 16:04:14 INFO Slf4jLogger: Slf4jLogger started\n14/07/26 16:04:14 INFO Remoting: Starting remoting\n14/07/26 16:04:14 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://spark@localhost:57189]\n14/07/26 16:04:14 INFO Remoting: Remoting now listens on addresses: [akka.tcp://spark@localhost:57189]\n14/07/26 16:04:14 INFO SparkEnv: Registering MapOutputTracker\n14/07/26 16:04:14 INFO SparkEnv: Registering BlockManagerMaster\n14/07/26 16:04:14 INFO DiskBlockManager: Created local directory at /var/folders/d2/mt2f3tx14vgg3b38xq_3ydfw0000gp/T/spark-local-20140726160414-c9d9\n14/07/26 16:04:14 INFO ConnectionManager: Bound socket to port 57190 with id = ConnectionManagerId(localhost,57190)\n14/07/26 16:04:14 INFO MemoryStore: MemoryStore started with capacity 297.0 MB\n14/07/26 16:04:14 INFO BlockManagerMaster: Trying to register BlockManager\n14/07/26 16:04:14 INFO BlockManagerMasterActor: Registering block manager localhost:57190 with 297.0 MB RAM\n14/07/26 16:04:14 INFO BlockManagerMaster: Registered BlockManager\n14/07/26 16:04:14 INFO HttpFileServer: HTTP File server directory is /var/folders/d2/mt2f3tx14vgg3b38xq_3ydfw0000gp/T/spark-a7453bad-5278-43fd-aa43-efb33228c660\n14/07/26 16:04:14 INFO HttpServer: Starting HTTP Server\n14/07/26 16:04:14 INFO SparkUI: Started SparkUI at http://localhost:4040\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/Users/joshrosen/Documents/spark/spark/python/pyspark/context.py\", line 133, in __init__\n    self._jsc = self._initialize_context(self._conf._jconf)\n  File \"/Users/joshrosen/Documents/spark/spark/python/pyspark/context.py\", line 179, in _initialize_context\n    return self._jvm.JavaSparkContext(jconf)\n  File \"/Users/joshrosen/Documents/spark/spark/python/lib/py4j-0.8.1-src.zip/py4j/java_gateway.py\", line 669, in __call__\n\n  File \"/Users/joshrosen/Documents/spark/spark/python/lib/py4j-0.8.1-src.zip/py4j/protocol.py\", line 300, in get_return_value\npy4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Could not parse Master URL: 'sjkdlsjfsl'\n\tat org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:1594)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:309)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:53)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:525)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:214)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:722)\n\n>>> sc = SparkContext('local')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/Users/joshrosen/Documents/spark/spark/python/pyspark/context.py\", line 93, in __init__\n    SparkContext._ensure_initialized(self, gateway=gateway)\n  File \"/Users/joshrosen/Documents/spark/spark/python/pyspark/context.py\", line 206, in _ensure_initialized\n    callsite.function, callsite.file, callsite.linenum))\nValueError: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=sjkdlsjfsl) created by __init__ at <stdin>:1\n>>>\n{code}\n\nThe right solution is to clear {{active_spark_context}} if the SparkContext creation fails.  I'll submit a pull request for this in a little bit.", "User 'JoshRosen' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1606"], "derived": {"summary": "For example;-\nIn PySpark, if we try to initialize spark context with insufficient arguments, >>>sc=SparkContext('local')\nit fails with an exception \nException: An application name must be set in your configuration\n\nThis is all fine. However, any successive creation of spark context with correct arguments, also fails,\n>>>s1=SparkContext('local', 'test1')\nAttributeError: 'SparkContext' object has no attribute 'master'.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Successive creation of spark context fails in pyspark, if the previous initialization of spark context had failed. - For example;-\nIn PySpark, if we try to initialize spark context with insufficient arguments, >>>sc=SparkContext('local')\nit fails with an exception \nException: An application name must be set in your configuration\n\nThis is all fine. However, any successive creation of spark context with correct arguments, also fails,\n>>>s1=SparkContext('local', 'test1')\nAttributeError: 'SparkContext' object has no attribute 'master'."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'JoshRosen' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1606"}]}}
{"project": "SPARK", "issue_id": "SPARK-1551", "title": "Spark master does not build in sbt", "status": "Resolved", "priority": "Major", "reporter": "Holden Karau", "assignee": null, "labels": [], "created": "2014-04-21T05:43:42.000+0000", "updated": "2014-04-21T17:50:09.000+0000", "description": "metrics-ganglia is missing", "comments": ["metrics ganglia was removed a while ago from the default build due to a license problem. What command are you using to build spark? What is the error?", "Sorry about that, I had something dirty locally that added the requirement for gangilla."], "derived": {"summary": "metrics-ganglia is missing.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Spark master does not build in sbt - metrics-ganglia is missing."}, {"q": "What updates or decisions were made in the discussion?", "a": "Sorry about that, I had something dirty locally that added the requirement for gangilla."}]}}
{"project": "SPARK", "issue_id": "SPARK-1552", "title": "GraphX performs type comparison incorrectly", "status": "Resolved", "priority": "Major", "reporter": "Ankur Dave", "assignee": "Ankur Dave", "labels": [], "created": "2014-04-21T08:41:23.000+0000", "updated": "2014-06-06T06:33:28.000+0000", "description": "In GraphImpl, mapVertices and outerJoinVertices use a more efficient implementation when the map function preserves vertex attribute types. This is implemented by comparing the ClassTags of the old and new vertex attribute types. However, ClassTags store _erased_ types, so the comparison will return a false positive for types with different type parameters, such as Option[Int] and Option[Double].\n\nThanks to Pierre-Alexandre Fonta for reporting this bug on the [mailing list|http://apache-spark-user-list.1001560.n3.nabble.com/GraphX-Cast-error-when-comparing-a-vertex-attribute-after-its-type-has-changed-td4119.html].\n\nDemo in the Scala shell:\n\nscala> import scala.reflect.{classTag, ClassTag}\nscala> def typesEqual[A: ClassTag, B: ClassTag](a: A, b: B): Boolean = classTag[A] equals classTag[B]\nscala> typesEqual(Some(1), Some(2.0)) // should return false\nres2: Boolean = true\n\nWe can require richer TypeTags for these methods, or just take a flag from the caller specifying whether the types are equal.", "comments": ["Does it make sense to get rid of this optimization until richer TypeTags are available? ", "Alternatively, we could introduce type-preserving variants of these methods, maybe called mapVerticesSameType and outerJoinVerticesSameType.\n\nWe can't make it into 1.0.0, but it would be good to get a fix into 1.0.1.", "Proposed fix: https://github.com/apache/spark/pull/967"], "derived": {"summary": "In GraphImpl, mapVertices and outerJoinVertices use a more efficient implementation when the map function preserves vertex attribute types. This is implemented by comparing the ClassTags of the old and new vertex attribute types.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "GraphX performs type comparison incorrectly - In GraphImpl, mapVertices and outerJoinVertices use a more efficient implementation when the map function preserves vertex attribute types. This is implemented by comparing the ClassTags of the old and new vertex attribute types."}, {"q": "What updates or decisions were made in the discussion?", "a": "Proposed fix: https://github.com/apache/spark/pull/967"}]}}
{"project": "SPARK", "issue_id": "SPARK-1553", "title": "Support alternating nonnegative least-squares", "status": "Resolved", "priority": "Major", "reporter": "Tor Myklebust", "assignee": "Tor Myklebust", "labels": [], "created": "2014-04-21T16:18:18.000+0000", "updated": "2014-06-02T18:49:59.000+0000", "description": "There's already an ALS implementation.  It can be tweaked to support nonnegative least-squares by conditionally running a nonnegative least-squares solve instead of a least-squares solver.", "comments": ["PR: https://github.com/apache/spark/pull/460"], "derived": {"summary": "There's already an ALS implementation. It can be tweaked to support nonnegative least-squares by conditionally running a nonnegative least-squares solve instead of a least-squares solver.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support alternating nonnegative least-squares - There's already an ALS implementation. It can be tweaked to support nonnegative least-squares by conditionally running a nonnegative least-squares solve instead of a least-squares solver."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR: https://github.com/apache/spark/pull/460"}]}}
{"project": "SPARK", "issue_id": "SPARK-1554", "title": "Update doc overview page to not mention building if you get a pre-built distro", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Patrick Wendell", "labels": [], "created": "2014-04-21T17:35:27.000+0000", "updated": "2014-05-13T01:10:38.000+0000", "description": "SBT assembly takes a long time and we should tell people to skip it if they got a binary build (which will likely be the most common case).", "comments": ["This is fixed in 1.0"], "derived": {"summary": "SBT assembly takes a long time and we should tell people to skip it if they got a binary build (which will likely be the most common case).", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Update doc overview page to not mention building if you get a pre-built distro - SBT assembly takes a long time and we should tell people to skip it if they got a binary build (which will likely be the most common case)."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is fixed in 1.0"}]}}
{"project": "SPARK", "issue_id": "SPARK-1555", "title": "enable ec2/spark_ec2.py to stop/delete cluster non-interactively", "status": "Resolved", "priority": "Minor", "reporter": "Art Peel", "assignee": null, "labels": [], "created": "2014-04-21T17:42:38.000+0000", "updated": "2016-01-09T12:52:56.000+0000", "description": "Currently ec2/spark_ec2.py asks for user input to confirm a request to stop/delete the cluster.\n\nThis prevents non-interactive use of the script.\n\nPlease add --assume-yes option with this behavior:\na. It defaults to false so the current behavior is maintained by default.\nb. If set to true, the script does not ask for user input and instead assumes the answer is 'y'.\n", "comments": ["Why not use the {{yes}} unix utility for this?\n\n{code}\nyes | ./bin/spark-ec2 stop [...]\n{code}", "[~joshrosen]'s workaround is precisely what I use when I want to destroy a cluster non-interactively, but it might be worth eventually adding an {{--assume-yes}} option or something similar to guarantee non-interactive execution regardless of the requested action.", "The other thing to consider, if we support running {{spark-ec2}} on Windows, is that {{yes}} is not available on all platforms that Spark supports.", "I'm guessing this is WontFix given the lack of activity, and that EC2 support is finally moving out of Spark"], "derived": {"summary": "Currently ec2/spark_ec2. py asks for user input to confirm a request to stop/delete the cluster.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "enable ec2/spark_ec2.py to stop/delete cluster non-interactively - Currently ec2/spark_ec2. py asks for user input to confirm a request to stop/delete the cluster."}, {"q": "What updates or decisions were made in the discussion?", "a": "I'm guessing this is WontFix given the lack of activity, and that EC2 support is finally moving out of Spark"}]}}
{"project": "SPARK", "issue_id": "SPARK-1556", "title": "jets3t dep doesn't update properly with newer Hadoop versions", "status": "Resolved", "priority": "Blocker", "reporter": "Nan Zhu", "assignee": "Sean R. Owen", "labels": [], "created": "2014-04-21T19:52:05.000+0000", "updated": "2015-04-27T06:47:51.000+0000", "description": "In Hadoop 2.2.x or newer, Jet3st 0.9.0 which defines S3ServiceException/ServiceException is introduced, however, Spark still relies on Jet3st 0.7.x which has no definition of these classes\n\nWhat I met is that \n\n[code]\n\n14/04/21 19:30:53 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n14/04/21 19:30:53 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n14/04/21 19:30:53 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n14/04/21 19:30:53 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n14/04/21 19:30:53 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\njava.lang.NoClassDefFoundError: org/jets3t/service/S3ServiceException\n\tat org.apache.hadoop.fs.s3native.NativeS3FileSystem.createDefaultStore(NativeS3FileSystem.java:280)\n\tat org.apache.hadoop.fs.s3native.NativeS3FileSystem.initialize(NativeS3FileSystem.java:270)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2316)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:90)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2350)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2332)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:369)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:221)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:270)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:140)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:207)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:205)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:205)\n\tat org.apache.spark.rdd.MappedRDD.getPartitions(MappedRDD.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:207)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:205)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:205)\n\tat org.apache.spark.rdd.MappedRDD.getPartitions(MappedRDD.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:207)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:205)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:205)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:891)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:741)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:692)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:574)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:900)\n\tat $iwC$$iwC$$iwC$$iwC.<init>(<console>:15)\n\tat $iwC$$iwC$$iwC.<init>(<console>:20)\n\tat $iwC$$iwC.<init>(<console>:22)\n\tat $iwC.<init>(<console>:24)\n\tat <init>(<console>:26)\n\tat .<init>(<console>:30)\n\tat .<clinit>(<console>)\n\tat .<init>(<console>:7)\n\tat .<clinit>(<console>)\n\tat $print(<console>)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:772)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1040)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:609)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:640)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:604)\n\tat org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:793)\n\tat org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:838)\n\tat org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:750)\n\tat org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:598)\n\tat org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:605)\n\tat org.apache.spark.repl.SparkILoop.loop(SparkILoop.scala:608)\n\tat org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:931)\n\tat org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:881)\n\tat org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:881)\n\tat scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)\n\tat org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:881)\n\tat org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:973)\n\tat org.apache.spark.repl.Main$.main(Main.scala:31)\n\tat org.apache.spark.repl.Main.main(Main.scala)\nCaused by: java.lang.ClassNotFoundException: org.jets3t.service.S3ServiceException\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:366)\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:358)\n\t... 63 more\n\n[/code]\n\n", "comments": ["Issue resolved by pull request 629\n[https://github.com/apache/spark/pull/629]", "If you're running spark on cdh5 there is a dirty hotfix:\n{code}\ncd /usr/lib/spark/assembly/lib/ && ln -s /usr/lib/hadoop/lib/jets3t-0.9.0.jar\n{code}\n\nYou have to do this on all nodes, this will put jets3t-0.9.0 before spark_assembly .jar on a classpath\nFortunately \"j\" is before \"s\" in the alphabet :)\n", "Good tip!!! Many thanks! Will give that a shot.", "Thanks again for the tip.  I didn't seem to have the /usr/lib/spark/assembly/lib directory in my installation, but adding the symlink in /usr/lib/shark/lib did the trick as well.", "[~srowen] I saw you set jets3t's scope to runtime. Any particular reason for that setting? Now sbt reads deps info from pom. The assembly jar won't include jets3t if its scope is runtime only. ", "Yeah see PR comments -- runtime is correct because Spark code should not compile against it. Ideally sbt assembly should include runtime dependencies since they are needed exactly at, well, runtime!", "User 'CodingCat' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/468"], "derived": {"summary": "In Hadoop 2. 2.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "jets3t dep doesn't update properly with newer Hadoop versions - In Hadoop 2. 2."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'CodingCat' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/468"}]}}
{"project": "SPARK", "issue_id": "SPARK-1557", "title": "Set permissions on event log files/directories", "status": "Resolved", "priority": "Major", "reporter": "Thomas Graves", "assignee": "Thomas Graves", "labels": [], "created": "2014-04-21T20:08:52.000+0000", "updated": "2014-04-29T14:21:27.000+0000", "description": "We should set the permissions on the event log directories and files so that it restricts access to only those users who own them, but could also allow a super user to read them so that they could be displayed by the history server in a multi-tenant secure environment. \n\n", "comments": ["https://github.com/apache/spark/pull/538"], "derived": {"summary": "We should set the permissions on the event log directories and files so that it restricts access to only those users who own them, but could also allow a super user to read them so that they could be displayed by the history server in a multi-tenant secure environment.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Set permissions on event log files/directories - We should set the permissions on the event log directories and files so that it restricts access to only those users who own them, but could also allow a super user to read them so that they could be displayed by the history server in a multi-tenant secure environment."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/538"}]}}
{"project": "SPARK", "issue_id": "SPARK-1558", "title": "[streaming] Update custom receiver guide with new Receiver API", "status": "Resolved", "priority": "Blocker", "reporter": "Tathagata Das", "assignee": "Tathagata Das", "labels": [], "created": "2014-04-21T22:00:18.000+0000", "updated": "2014-05-05T22:30:40.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "[streaming] Update custom receiver guide with new Receiver API"}]}}
{"project": "SPARK", "issue_id": "SPARK-1559", "title": "Add conf dir to CLASSPATH in compute-classpath.sh dependent on whether SPARK_CONF_DIR is set", "status": "Resolved", "priority": "Minor", "reporter": "Albert Chu", "assignee": null, "labels": [], "created": "2014-04-21T23:32:53.000+0000", "updated": "2014-12-11T20:24:19.000+0000", "description": "bin/load-spark-env.sh loads spark-env.sh from SPARK_CONF_DIR if it is set, or from \"$parent_dir/conf\" if it is not set.\n\nHowever, in compute-classpath.sh, the CLASSPATH adds \"$FWDIR/conf\" to the CLASSPATH regardless if SPARK_CONF_DIR is set.\n\nAttached patch fixes this.  Pull request on github will also be sent.\n\n", "comments": ["The PR discussion suggests it was duplicated by the PR for SPARK-2058."], "derived": {"summary": "bin/load-spark-env. sh loads spark-env.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add conf dir to CLASSPATH in compute-classpath.sh dependent on whether SPARK_CONF_DIR is set - bin/load-spark-env. sh loads spark-env."}, {"q": "What updates or decisions were made in the discussion?", "a": "The PR discussion suggests it was duplicated by the PR for SPARK-2058."}]}}
{"project": "SPARK", "issue_id": "SPARK-1560", "title": "PySpark SQL depends on Java 7 only jars", "status": "Resolved", "priority": "Blocker", "reporter": "Michael Armbrust", "assignee": "Ahir Reddy", "labels": [], "created": "2014-04-22T02:00:09.000+0000", "updated": "2014-04-22T16:45:52.000+0000", "description": "We need to republish the pickler built with java 7. Details below:\n\n{code}\n14/04/19 12:31:29 INFO rdd.HadoopRDD: Input split: file:/Users/ceteri/opt/spark-branch-1.0/examples/src/main/resources/people.txt:0+16\nException in thread \"Local computation of job 1\" java.lang.UnsupportedClassVersionError: net/razorvine/pickle/Unpickler : Unsupported major.minor version 51.0\n\tat java.lang.ClassLoader.defineClass1(Native Method)\n\tat java.lang.ClassLoader.defineClassCond(ClassLoader.java:637)\n\tat java.lang.ClassLoader.defineClass(ClassLoader.java:621)\n\tat java.security.SecureClassLoader.defineClass(SecureClassLoader.java:141)\n\tat java.net.URLClassLoader.defineClass(URLClassLoader.java:283)\n\tat java.net.URLClassLoader.access$000(URLClassLoader.java:58)\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:197)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:190)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:306)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:247)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$pythonToJavaMap$1.apply(PythonRDD.scala:295)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$pythonToJavaMap$1.apply(PythonRDD.scala:294)\n\tat org.apache.spark.rdd.RDD$$anonfun$3.apply(RDD.scala:518)\n\tat org.apache.spark.rdd.RDD$$anonfun$3.apply(RDD.scala:518)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:243)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:234)\n\tat org.apache.spark.scheduler.DAGScheduler.runLocallyWithinThread(DAGScheduler.scala:700)\n\tat org.apache.spark.scheduler.DAGScheduler$$anon$1.run(DAGScheduler.scala:685)\n{code}", "comments": [], "derived": {"summary": "We need to republish the pickler built with java 7. Details below:\n\n{code}\n14/04/19 12:31:29 INFO rdd.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "PySpark SQL depends on Java 7 only jars - We need to republish the pickler built with java 7. Details below:\n\n{code}\n14/04/19 12:31:29 INFO rdd."}]}}
{"project": "SPARK", "issue_id": "SPARK-1561", "title": "sbt/sbt assembly generates too many local files", "status": "Resolved", "priority": "Major", "reporter": "Xiangrui Meng", "assignee": null, "labels": [], "created": "2014-04-22T02:47:02.000+0000", "updated": "2014-10-16T03:27:50.000+0000", "description": "Running `find ./ | wc -l` after `sbt/sbt assembly` returned \n\n564365\n\nThis hits the default limit of #INode of an 8GB EXT FS (the default volume size for an EC2 instance), which means you can do nothing after 'sbt/sbt assembly` on such a partition.\n\nMost of the small files are under assembly/target/streams and the same folder under examples/.\n", "comments": ["Tried adding \n\n{code}\nassemblyOption in assembly ~= { _.copy(cacheUnzip = false) },\nassemblyOption in assembly ~= { _.copy(cacheOutput = false) },\n{code}\n\nto SparkBuild.scala, but it didn't help reduce the number of files.", "Tried with a clean repo, the number is 254924, which is probably fine.", "FWIW I see 194188 when I run it now, even lower. OK to resolve this?", "Does not seem to be an issue any more - resolving per comment from Sean."], "derived": {"summary": "Running `find. / | wc -l` after `sbt/sbt assembly` returned \n\n564365\n\nThis hits the default limit of #INode of an 8GB EXT FS (the default volume size for an EC2 instance), which means you can do nothing after 'sbt/sbt assembly` on such a partition.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "sbt/sbt assembly generates too many local files - Running `find. / | wc -l` after `sbt/sbt assembly` returned \n\n564365\n\nThis hits the default limit of #INode of an 8GB EXT FS (the default volume size for an EC2 instance), which means you can do nothing after 'sbt/sbt assembly` on such a partition."}, {"q": "What updates or decisions were made in the discussion?", "a": "Does not seem to be an issue any more - resolving per comment from Sean."}]}}
{"project": "SPARK", "issue_id": "SPARK-1562", "title": "Exclude internal catalyst classes from scaladoc, or make them package private", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Michael Armbrust", "labels": [], "created": "2014-04-22T04:53:49.000+0000", "updated": "2014-04-23T03:03:51.000+0000", "description": "Michael - this is up to you. But I noticed there are a ton of internal catalyst types that show up in our scaladoc. I'm not sure if you mean these to be user-facing API's. If not, it might be good to hide them from the docs or make them package private.", "comments": ["Thanks for taking a look at this!"], "derived": {"summary": "Michael - this is up to you. But I noticed there are a ton of internal catalyst types that show up in our scaladoc.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Exclude internal catalyst classes from scaladoc, or make them package private - Michael - this is up to you. But I noticed there are a ton of internal catalyst types that show up in our scaladoc."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for taking a look at this!"}]}}
{"project": "SPARK", "issue_id": "SPARK-1563", "title": "Add package-info.java and package.scala files for all packages that appear in docs", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Prashant Sharma", "labels": ["Starter"], "created": "2014-04-22T06:06:00.000+0000", "updated": "2015-12-10T15:05:58.000+0000", "description": null, "comments": ["So this applies to only those packages which appear in scaladocs for all modules. Including the new sql module.", "User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/599"], "derived": {"summary": "", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Add package-info.java and package.scala files for all packages that appear in docs"}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/599"}]}}
{"project": "SPARK", "issue_id": "SPARK-1564", "title": "Add JavaScript into Javadoc to turn ::Experimental:: and such into badges", "status": "Resolved", "priority": "Minor", "reporter": "Matei Alexandru Zaharia", "assignee": "Jon Deron Eriksson", "labels": [], "created": "2014-04-22T06:06:30.000+0000", "updated": "2015-08-30T05:02:15.000+0000", "description": null, "comments": ["This is one of two issues left under the stale-sounding https://issues.apache.org/jira/browse/SPARK-1351 . Can this be turned loose into a floating issue for 1.3+ or marked WontFix?", "This is still a valid issue AFAIK, isn't it? These things still show up badly in Javadoc. So we could change the parent issue or something but I'd like to see it fixed.", "Yeah that's what I did, just made it not tied to the old 1.0 parent issue.", "I'm working on this one. I believe this is a duplicate of https://issues.apache.org/jira/browse/SPARK-1635", "User 'deroneriksson' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/7169", "https://github.com/apache/spark/pull/7169 has been merged and it is included in both 1.5.0-rc1 and 1.5.0-rc2. I am resolving this issue.", "[~andrewor14] seems I cannot change the assignee to [~deron]. Can you assign it to him?"], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add JavaScript into Javadoc to turn ::Experimental:: and such into badges"}, {"q": "What updates or decisions were made in the discussion?", "a": "[~andrewor14] seems I cannot change the assignee to [~deron]. Can you assign it to him?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1565", "title": "Spark examples should be changed given spark-submit", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Prashant Sharma", "labels": [], "created": "2014-04-22T07:58:24.000+0000", "updated": "2015-12-10T15:05:56.000+0000", "description": "Example: https://github.com/pwendell/spark/commit/3fdad8ada28c1cef9b3b367993057de22295b8ed\n\nNow that we have the spark-submit script we should make some changes to the examples:\n\n1. We should always create a SparkConf that sets the app name and use the SparkContext constructor that accepts a conf.\n2. We shouldn't set the master based on a command line argument, instead users can do this on their own with spark-submit.\n3. The examples projects should mark spark-core/streaming as a \"provided\" dependency, so that the examples assembly jar does not include spark.\n\nThen users can launch examples like this:\n\n{code}\n./bin/spark-submit examples/target/scala-2.10/spark-examples-assembly-1.0.0-SNAPSHOT.jar \\\n  --class org.apache.spark.examples.SparkPi \\\n  --arg 1000\n{code}", "comments": ["Issue resolved by pull request 552\n[https://github.com/apache/spark/pull/552]", "User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/552"], "derived": {"summary": "Example: https://github. com/pwendell/spark/commit/3fdad8ada28c1cef9b3b367993057de22295b8ed\n\nNow that we have the spark-submit script we should make some changes to the examples:\n\n1.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Spark examples should be changed given spark-submit - Example: https://github. com/pwendell/spark/commit/3fdad8ada28c1cef9b3b367993057de22295b8ed\n\nNow that we have the spark-submit script we should make some changes to the examples:\n\n1."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/552"}]}}
{"project": "SPARK", "issue_id": "SPARK-1566", "title": "Consolidate the Spark Programming Guide with tabs for all languages", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "labels": [], "created": "2014-04-22T17:23:16.000+0000", "updated": "2014-05-30T07:35:27.000+0000", "description": "Right now it's Scala-only and the other ones say \"look at the Scala programming guide first\".", "comments": ["https://github.com/apache/spark/pull/896", "https://github.com/apache/spark/pull/896"], "derived": {"summary": "Right now it's Scala-only and the other ones say \"look at the Scala programming guide first\".", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Consolidate the Spark Programming Guide with tabs for all languages - Right now it's Scala-only and the other ones say \"look at the Scala programming guide first\"."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/896"}]}}
{"project": "SPARK", "issue_id": "SPARK-1567", "title": "Add language tabs to quick start guide", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Patrick Wendell", "labels": [], "created": "2014-04-22T17:23:29.000+0000", "updated": "2014-04-22T17:28:07.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add language tabs to quick start guide"}]}}
{"project": "SPARK", "issue_id": "SPARK-1568", "title": "Spark 0.9.0 hangs reading s3", "status": "Resolved", "priority": "Major", "reporter": "sam", "assignee": null, "labels": [], "created": "2014-04-22T18:59:07.000+0000", "updated": "2014-11-14T15:23:55.000+0000", "description": "I've tried several jobs now and many of the tasks complete, then it get stuck and just hangs.  The exact same jobs function perfectly fine if I distcp to hdfs first and read from hdfs.\n\nMany thanks", "comments": ["Sam, did the other recent changes to S3 deps resolve this, do you think?", "When we upgrade to 1.0.0 I'll test this.\n\nThis particular problem was from quite a while back when our cluster was quite different from it is now.  At the moment we get the jets3 thing, which is supposed to go away in 1.0.0.", "[~sams] did you see an improvement when you upgraded to 1.0.0?\n\nI've noticed myself that reading from s3 can be slow, particularly in the scenario where there are many small files.  I think the hadoop S3 adapter makes many more API calls than is necessary, and that scales on the number of files you have.", "Sorry to have not updated. Yes no problems reading s3 with 1.0.0"], "derived": {"summary": "I've tried several jobs now and many of the tasks complete, then it get stuck and just hangs. The exact same jobs function perfectly fine if I distcp to hdfs first and read from hdfs.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Spark 0.9.0 hangs reading s3 - I've tried several jobs now and many of the tasks complete, then it get stuck and just hangs. The exact same jobs function perfectly fine if I distcp to hdfs first and read from hdfs."}, {"q": "What updates or decisions were made in the discussion?", "a": "Sorry to have not updated. Yes no problems reading s3 with 1.0.0"}]}}
{"project": "SPARK", "issue_id": "SPARK-1569", "title": "Spark on Yarn, authentication broken by pr299", "status": "Resolved", "priority": "Blocker", "reporter": "Thomas Graves", "assignee": "Thomas Graves", "labels": [], "created": "2014-04-22T19:03:35.000+0000", "updated": "2014-05-07T22:53:07.000+0000", "description": "https://github.com/apache/spark/pull/299 changed the way configuration was done and passed to the executors.  This breaks use of authentication as the executor needs to know that authentication is enabled before connecting to the driver.  ", "comments": ["{quote}\n@tgravescs ah I see, you're right. I think I assumed incorrectly that the executor launcher would bundle up the options and send them over, but I don't actually see that happening anywhere. So this part of the code is actually not used:\n\nhttps://github.com/apache/spark/blob/df6d81425bf3b8830988288069f6863de873aee2/yarn/common/src/main/scala/org/apache/spark/deploy/yarn/ClientBase.scala#L328\n\nWhat happens is the executor is just getting its configuration from the driver when the executor launches. And that works in most cases except for security, which it needs to know about before connecting. Is that right?\n{quote}\n\nThat is correct. It needs it before connecting.  The code you reference handles add it for the application master but not the executors. it looks like we need similar code in ExecutorRunnableUtil.prepareCommand. \n\nYes before we only had SPARK_JAVA_OPTS and that got put as -D on the command line so it was always set correctly when the executors launched.\n", "https://github.com/apache/spark/pull/649", "Fixed in this pr: https://github.com/apache/spark/pull/649"], "derived": {"summary": "https://github. com/apache/spark/pull/299 changed the way configuration was done and passed to the executors.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Spark on Yarn, authentication broken by pr299 - https://github. com/apache/spark/pull/299 changed the way configuration was done and passed to the executors."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed in this pr: https://github.com/apache/spark/pull/649"}]}}
{"project": "SPARK", "issue_id": "SPARK-1570", "title": "Class loading issue when using Spark SQL Java API", "status": "Resolved", "priority": "Blocker", "reporter": "Kan Zhang", "assignee": "Kan Zhang", "labels": [], "created": "2014-04-22T19:35:05.000+0000", "updated": "2014-04-22T23:19:49.000+0000", "description": "ClassNotFoundException in Executor when running JavaSparkSQL example using spark-submit in local mode.\n\n14/04/22 12:26:20 ERROR Executor: Exception in task ID 0\njava.lang.ClassNotFoundException: org.apache.spark.examples.sql.JavaSparkSQL.Person\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:366)\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:358)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:190)\n\tat org.apache.spark.sql.api.java.JavaSQLContext$$anonfun$1.apply(JavaSQLContext.scala:90)\n\tat org.apache.spark.sql.api.java.JavaSQLContext$$anonfun$1.apply(JavaSQLContext.scala:88)\n\tat org.apache.spark.rdd.RDD$$anonfun$3.apply(RDD.scala:512)\n\tat org.apache.spark.rdd.RDD$$anonfun$3.apply(RDD.scala:512)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\n", "comments": ["PR: https://github.com/apache/spark/pull/484"], "derived": {"summary": "ClassNotFoundException in Executor when running JavaSparkSQL example using spark-submit in local mode. 14/04/22 12:26:20 ERROR Executor: Exception in task ID 0\njava.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Class loading issue when using Spark SQL Java API - ClassNotFoundException in Executor when running JavaSparkSQL example using spark-submit in local mode. 14/04/22 12:26:20 ERROR Executor: Exception in task ID 0\njava."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR: https://github.com/apache/spark/pull/484"}]}}
{"project": "SPARK", "issue_id": "SPARK-1571", "title": "UnresolvedException when running JavaSparkSQL example", "status": "Resolved", "priority": "Blocker", "reporter": "Kan Zhang", "assignee": "Michael Armbrust", "labels": [], "created": "2014-04-22T19:53:10.000+0000", "updated": "2014-04-24T18:09:52.000+0000", "description": "When running JavaSparkSQL example using spark-submit in local mode (this happens after fixing the class loading issue in SPARK-1570).\n\n14/04/22 12:46:47 ERROR Executor: Exception in task ID 0\norg.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to dataType on unresolved object, tree: 'age\n\tat org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.dataType(unresolved.scala:49)\n\tat org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.dataType(unresolved.scala:47)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.c2(Expression.scala:203)\n\tat org.apache.spark.sql.catalyst.expressions.GreaterThanOrEqual.eval(predicates.scala:142)\n\tat org.apache.spark.sql.catalyst.expressions.And.eval(predicates.scala:84)\n\tat org.apache.spark.sql.execution.Filter$$anonfun$2$$anonfun$apply$1.apply(basicOperators.scala:43)\n\tat org.apache.spark.sql.execution.Filter$$anonfun$2$$anonfun$apply$1.apply(basicOperators.scala:43)\n", "comments": ["I think this is fixed by https://github.com/apache/spark/commit/39f85e0322cfecefbc30e7d5a30356cfab1e9640\n\nKan, please let me know if you have any further problems!", "Thanks, it worked."], "derived": {"summary": "When running JavaSparkSQL example using spark-submit in local mode (this happens after fixing the class loading issue in SPARK-1570). 14/04/22 12:46:47 ERROR Executor: Exception in task ID 0\norg.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "UnresolvedException when running JavaSparkSQL example - When running JavaSparkSQL example using spark-submit in local mode (this happens after fixing the class loading issue in SPARK-1570). 14/04/22 12:46:47 ERROR Executor: Exception in task ID 0\norg."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks, it worked."}]}}
{"project": "SPARK", "issue_id": "SPARK-1572", "title": "Uncaught IO exceptions in Pyspark kill Executor", "status": "Resolved", "priority": "Major", "reporter": "Aaron Davidson", "assignee": "Aaron Davidson", "labels": [], "created": "2014-04-22T20:29:55.000+0000", "updated": "2014-04-23T21:47:15.000+0000", "description": "If an exception is thrown in the Python \"stdin writer\" thread during this line:\n\n{code}\nPythonRDD.writeIteratorToStream(parent.iterator(split, context), dataOut)\n{code}\n\n(e.g., while reading from an HDFS source) then the exception will be handled by the default ThreadUncaughtExceptionHandler, which is set in Executor. The default behavior is, unfortunately, to call System.exit().\n\nIdeally, normal exceptions while running a task should not bring down all the executors of a Spark cluster.", "comments": [], "derived": {"summary": "If an exception is thrown in the Python \"stdin writer\" thread during this line:\n\n{code}\nPythonRDD. writeIteratorToStream(parent.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Uncaught IO exceptions in Pyspark kill Executor - If an exception is thrown in the Python \"stdin writer\" thread during this line:\n\n{code}\nPythonRDD. writeIteratorToStream(parent."}]}}
{"project": "SPARK", "issue_id": "SPARK-1573", "title": "slight modification with regards to sbt/sbt test", "status": "Resolved", "priority": "Major", "reporter": "Nishkam Ravi", "assignee": null, "labels": [], "created": "2014-04-22T21:14:19.000+0000", "updated": "2014-10-13T17:18:43.000+0000", "description": "When the sources are built against a certain Hadoop version with SPARK_YARN=true, the same settings seem necessary when running sbt/sbt test. \n\nFor example:\nSPARK_HADOOP_VERSION=2.3.0-cdh5.0.0 SPARK_YARN=true sbt/sbt assembly\n\nSPARK_HADOOP_VERSION=2.3.0-cdh5.0.0 SPARK_YARN=true sbt/sbt test\n\nOtherwise build errors and failing tests are seen.\n", "comments": ["I think it's to be expected that you have to run tests the same way you build. This is what https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark directs you to do in step 5 already.\n\nMaybe it could be completely explicit that you can do it in one command (sbt/sbt assembly test), meaning the properties must necessarily be the same. I don't have edit rights or I'd just adjust it.", "In the documentation at the bottom of this page: https://github.com/apache/spark\na clause can be added for sbt/sbt test in the section on \"Note about Hadoop versions\". \n\nMaybe you can assign it to someone who has edit rights?", "This has been resolved insofar as the main README.md no longer has this text."], "derived": {"summary": "When the sources are built against a certain Hadoop version with SPARK_YARN=true, the same settings seem necessary when running sbt/sbt test. For example:\nSPARK_HADOOP_VERSION=2.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "slight modification with regards to sbt/sbt test - When the sources are built against a certain Hadoop version with SPARK_YARN=true, the same settings seem necessary when running sbt/sbt test. For example:\nSPARK_HADOOP_VERSION=2."}, {"q": "What updates or decisions were made in the discussion?", "a": "This has been resolved insofar as the main README.md no longer has this text."}]}}
{"project": "SPARK", "issue_id": "SPARK-1574", "title": "ec2/spark_ec2.py should provide option to control number of attempts for ssh operations", "status": "Resolved", "priority": "Minor", "reporter": "Art Peel", "assignee": "Nicholas Chammas", "labels": [], "created": "2014-04-22T21:21:38.000+0000", "updated": "2014-12-13T22:19:59.000+0000", "description": "EC instances are sometimes slow to start up.  When this happens, generating the cluster ssh key or sending the generated cluster key to the slaves can fail due to an ssh timeout.\n\nThe script currently hard-codes the number of tries for ssh operations as 2.\n\nFor more flexibility, it should be possible to specify the number of tries with a command-line option, --num-ssh-tries, that defaults to 2 to keep the current behavior if not provided.\n\n", "comments": ["I think that SPARK-3398 should have fixed this in Spark 1.2.0, so I'm going to mark this issue as 'Fixed'."], "derived": {"summary": "EC instances are sometimes slow to start up. When this happens, generating the cluster ssh key or sending the generated cluster key to the slaves can fail due to an ssh timeout.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "ec2/spark_ec2.py should provide option to control number of attempts for ssh operations - EC instances are sometimes slow to start up. When this happens, generating the cluster ssh key or sending the generated cluster key to the slaves can fail due to an ssh timeout."}, {"q": "What updates or decisions were made in the discussion?", "a": "I think that SPARK-3398 should have fixed this in Spark 1.2.0, so I'm going to mark this issue as 'Fixed'."}]}}
{"project": "SPARK", "issue_id": "SPARK-1575", "title": "failing tests with master branch ", "status": "Resolved", "priority": "Minor", "reporter": "Nishkam Ravi", "assignee": null, "labels": [], "created": "2014-04-22T21:24:18.000+0000", "updated": "2014-05-19T18:51:50.000+0000", "description": "Built the master branch against Hadoop version 2.3.0-cdh5.0.0 with SPARK_YARN=true. sbt tests don't go through successfully (tried multiple runs).", "comments": ["I get the same result. Here are the key parts of the errors I see:\n\n{code}\n[error] Uncaught exception when running org.apache.spark.CacheManagerSuite: java.lang.NoClassDefFoundError: org/objectweb/asm/Type\nsbt.ForkMain$ForkError: org/objectweb/asm/Type\n\tat net.sf.cglib.core.TypeUtils.parseType(TypeUtils.java:180)\n\tat net.sf.cglib.core.KeyFactory.<clinit>(KeyFactory.java:66)\n\tat net.sf.cglib.proxy.Enhancer.<clinit>(Enhancer.java:69)\n\tat org.easymock.internal.ClassProxyFactory.createEnhancer(ClassProxyFactory.java:249)\n\tat org.easymock.internal.ClassProxyFactory.createProxy(ClassProxyFactory.java:159)\n\tat org.easymock.internal.MocksControl.createMock(MocksControl.java:59)\n\tat org.easymock.EasyMock.createMock(EasyMock.java:103)\n\tat org.scalatest.mock.EasyMockSugar$class.mock(EasyMockSugar.scala:265)\n\tat org.apache.spark.CacheManagerSuite.mock(CacheManagerSuite.scala:30)\n\tat org.apache.spark.CacheManagerSuite$$anonfun$1.apply$mcV$sp(CacheManagerSuite.scala:40)\n...\n{code}\n\n{code}\n[info] - handles YARN cluster mode *** FAILED *** (1 millisecond)\n[info]   java.lang.Exception: When running with master 'yarn' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.\n[info]   at org.apache.spark.deploy.SparkSubmitArguments.checkRequiredArguments(SparkSubmitArguments.scala:124)\n...\n{code}\n\n{code}\n[error] Uncaught exception when running org.apache.spark.util.random.RandomSamplerSuite: java.lang.NoClassDefFoundError: Could not initialize class org.easymock.internal.ClassProxyFactory$2\nsbt.ForkMain$ForkError: Could not initialize class org.easymock.internal.ClassProxyFactory$2\n\tat org.easymock.internal.ClassProxyFactory.createEnhancer(ClassProxyFactory.java:249)\n\tat org.easymock.internal.ClassProxyFactory.createProxy(ClassProxyFactory.java:159)\n\tat org.easymock.internal.MocksControl.createMock(MocksControl.java:59)\n\tat org.easymock.EasyMock.createMock(EasyMock.java:103)\n\tat org.scalatest.mock.EasyMockSugar$class.mock(EasyMockSugar.scala:265)\n\tat org.apache.spark.util.random.RandomSamplerSuite.mock(RandomSamplerSuite.scala:26)\n\tat org.apache.spark.util.random.RandomSamplerSuite$$anonfun$1.apply$mcV$sp(RandomSamplerSuite.scala:34)\n\tat org.apache.spark.util.random.RandomSamplerSuite$$anonfun$1.apply(RandomSamplerSuite.scala:33)\n\tat org.apache.spark.util.random.RandomSamplerSuite$$anonfun$1.apply(RandomSamplerSuite.scala:33)\n...\n{code}\n\n{code}\n[error] Test org.apache.spark.JavaAPISuite.wholeTextFiles failed: java.lang.AssertionError: expected:<spark is easy to use.\n[error] > but was:<null>\n[error]     at org.apache.spark.JavaAPISuite.wholeTextFiles(JavaAPISuite.java:633)\n[error]     ...\n{code}\n\n{code}\n[info] Test run finished: 1 failed, 0 ignored, 46 total, 68.055s\n[error] Error: Total 609, Failed 3, Errors 2, Passed 600, Skipped 4\n[error] Failed tests:\n[error] \torg.apache.spark.deploy.SparkSubmitSuite\n[error] \torg.apache.spark.JavaAPISuite\n[error] Error during tests:\n[error] \torg.apache.spark.util.random.RandomSamplerSuite\n[error] \torg.apache.spark.CacheManagerSuite\n{code}\n\n{code}\n[error] (core/test:test) sbt.TestsFailedException: Tests unsuccessful\n{code}", "I get similar results. Tests are failing for other Hadoop versions as well (e.g, SPARK_HADOOP_VERSION=2.2.0)", "For what it's worth, I no longer see this failure I believe this has been resolved by other changes along the way.", "I couldn't reproduce this issue, but if you can still reproduce it, still re-open."], "derived": {"summary": "Built the master branch against Hadoop version 2. 3.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "failing tests with master branch  - Built the master branch against Hadoop version 2. 3."}, {"q": "What updates or decisions were made in the discussion?", "a": "I couldn't reproduce this issue, but if you can still reproduce it, still re-open."}]}}
{"project": "SPARK", "issue_id": "SPARK-1576", "title": "Passing of JAVA_OPTS to YARN on command line", "status": "Resolved", "priority": "Major", "reporter": "Nishkam Ravi", "assignee": null, "labels": [], "created": "2014-04-22T21:30:37.000+0000", "updated": "2014-09-18T16:47:36.000+0000", "description": "JAVA_OPTS can be passed by using either env variables (i.e., SPARK_JAVA_OPTS) or as config vars (after Patrick's recent change). It would be good to allow the user to pass them on command line as well to restrict scope to single application invocation.", "comments": ["Patch attached.", "Hi [~nravi], Spark patches should be submitted as pull requests against the Spark github - https://github.com/apache/spark.", "Hi Sandy, I've initiated a pull request for this fix.", "Thanks, mind posting a link to it on this JIRA?", "https://github.com/apache/spark/pull/492", "Is this meant for the driver or the executors?  The spark-submit script has a command line option for the driver:  --driver-java-options.\nI believe the intent of https://github.com/apache/spark/pull/299 was to not expose SPARK_JAVA_OPTS to the user anymore.", "Bit of a race condition here it seems. Patrick made a few changes yesterday around the same time as I did (in ClientBase.scala):\n\n for ((k, v) <- sys.props.filterKeys(_.startsWith(\"spark\"))) {\n        JAVA_OPTS += \"-D\" + k + \"=\" + \"\\\\\\\"\" + v + \"\\\\\\\"\"\n }\n\nThis would allow JAVA_OPTS to be passed on the command line to the ApplicationMaster, and accomplishes the same things as creation of a new command line flag --spark-java-opts. ", "Thomas, --driver-java-options would not work with yarn client though (only spark-submit).", "There is a misunderstanding here - it is to pass SPARK_JAVA_OPTS : not JAVA_OPTS.\nDirectly passing JAVA_OPTS has beem removed", "We have been using SPARK_JAVA_OPTS and JAVA_OPTS interchangeably. SPARK_JAVA_OPTS are JAVA_OPTS :)", "With Sandy's recent patch (https://github.com/apache/spark/pull/1253) this should be easy to do (spark-submit --conf spark.executor.extraJavaOptions=blah).", "spark-submit already supports this with existing options."], "derived": {"summary": "JAVA_OPTS can be passed by using either env variables (i. e.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Passing of JAVA_OPTS to YARN on command line - JAVA_OPTS can be passed by using either env variables (i. e."}, {"q": "What updates or decisions were made in the discussion?", "a": "spark-submit already supports this with existing options."}]}}
{"project": "SPARK", "issue_id": "SPARK-1577", "title": "GraphX mapVertices with KryoSerialization", "status": "Resolved", "priority": "Major", "reporter": "Joseph E. Gonzalez", "assignee": "Ankur Dave", "labels": [], "created": "2014-04-23T00:02:29.000+0000", "updated": "2014-05-26T20:20:21.000+0000", "description": "If Kryo is enabled by setting:\n\n{code}\nSPARK_JAVA_OPTS+=\"-Dspark.serializer=org.apache.spark.serializer.KryoSerializer \"\nSPARK_JAVA_OPTS+=\"-Dspark.kryo.registrator=org.apache.spark.graphx.GraphKryoRegistrator  \"\n{code}\n\nin conf/spark_env.conf and running the following block of code in the shell:\n\n{code}\nimport org.apache.spark.graphx._\nimport org.apache.spark.graphx.lib._\nimport org.apache.spark.rdd.RDD\n\nval vertexArray = Array(\n  (1L, (\"Alice\", 28)),\n  (2L, (\"Bob\", 27)),\n  (3L, (\"Charlie\", 65)),\n  (4L, (\"David\", 42)),\n  (5L, (\"Ed\", 55)),\n  (6L, (\"Fran\", 50))\n  )\nval edgeArray = Array(\n  Edge(2L, 1L, 7),\n  Edge(2L, 4L, 2),\n  Edge(3L, 2L, 4),\n  Edge(3L, 6L, 3),\n  Edge(4L, 1L, 1),\n  Edge(5L, 2L, 2),\n  Edge(5L, 3L, 8),\n  Edge(5L, 6L, 3)\n  )\n\nval vertexRDD: RDD[(Long, (String, Int))] = sc.parallelize(vertexArray)\nval edgeRDD: RDD[Edge[Int]] = sc.parallelize(edgeArray)\n\nval graph: Graph[(String, Int), Int] = Graph(vertexRDD, edgeRDD)\n\n// Define a class to more clearly model the user property\ncase class User(name: String, age: Int, inDeg: Int, outDeg: Int)\n\n// Transform the graph\nval userGraph = graph.mapVertices{ case (id, (name, age)) => User(name, age, 0, 0) }\n{code}\n\nThe following block of code works:\n\n{code}\nuserGraph.vertices.count\n{code}\n\nand the following block of code generates a Kryo error:\n\n{code}\nuserGraph.vertices.collect\n{code}\n\nThere error:\n\n{code}\njava.lang.StackOverflowError\n\tat sun.reflect.UnsafeFieldAccessorImpl.ensureObj(UnsafeFieldAccessorImpl.java:54)\n\tat sun.reflect.UnsafeQualifiedObjectFieldAccessorImpl.get(UnsafeQualifiedObjectFieldAccessorImpl.java:38)\n\tat java.lang.reflect.Field.get(Field.java:379)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.write(FieldSerializer.java:552)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:213)\n\tat com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:501)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.write(FieldSerializer.java:564)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:213)\n\tat com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:501)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.write(FieldSerializer.java:564)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:213)\n\tat com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:501)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.write(FieldSerializer.java:564)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:213)\n\tat com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:501)\n{code}\n\n ", "comments": ["I have narrowed the issued down to (line 46 in GraphKryoRegistrator):\n\n{code} \nkryo.setReferences(false)\n{code}\n\nThis creates an issue in the Spark REPL which leads to cyclic references.  Removing this line addresses the issue.  I will submit a pull request with the fix. \n\nIn fact, I can reproduce the bug with the following much simpler block of code:\n\n{code}\nclass A(a: String) extends Serializable\nval x = sc.parallelize(Array.fill(10)(new A(\"hello\")))\nx.collect\n{code}\n\ntl;dr\n\nDisabling reference tracking in Kryo will break the Spark Shell.  \n", "Resolved by re-enabling Kryo reference tracking in #742: https://github.com/apache/spark/pull/742"], "derived": {"summary": "If Kryo is enabled by setting:\n\n{code}\nSPARK_JAVA_OPTS+=\"-Dspark. serializer=org.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "GraphX mapVertices with KryoSerialization - If Kryo is enabled by setting:\n\n{code}\nSPARK_JAVA_OPTS+=\"-Dspark. serializer=org."}, {"q": "What updates or decisions were made in the discussion?", "a": "Resolved by re-enabling Kryo reference tracking in #742: https://github.com/apache/spark/pull/742"}]}}
{"project": "SPARK", "issue_id": "SPARK-1578", "title": "Do not require setting of cleaner TTL when creating StreamingContext", "status": "Resolved", "priority": "Blocker", "reporter": "Tathagata Das", "assignee": "Tathagata Das", "labels": [], "created": "2014-04-23T00:25:24.000+0000", "updated": "2014-04-23T02:36:09.000+0000", "description": "Since shuffles and RDDs that are out of context are automatically cleaned by Spark core (using ContextCleaner) there is no need for setting the cleaner TTL in StreamingContext.", "comments": [], "derived": {"summary": "Since shuffles and RDDs that are out of context are automatically cleaned by Spark core (using ContextCleaner) there is no need for setting the cleaner TTL in StreamingContext.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Do not require setting of cleaner TTL when creating StreamingContext - Since shuffles and RDDs that are out of context are automatically cleaned by Spark core (using ContextCleaner) there is no need for setting the cleaner TTL in StreamingContext."}]}}
{"project": "SPARK", "issue_id": "SPARK-1579", "title": "PySpark should distinguish expected IOExceptions from unexpected ones in the worker", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Aaron Davidson", "labels": [], "created": "2014-04-23T01:49:37.000+0000", "updated": "2014-09-12T20:30:40.000+0000", "description": "I chatted with [~adav] a bit about this. Right now we drop IOExceptions because they are (in some cases) expected if a Python worker returns before consuming its entire input. The issue is this swallows legitimate IO exceptions when they occur.\n\nOne thought we had was to change the daemon.py file to, instead of closing the socket when the function is over, simply busy-wait on the socket being closed. We'd transfer the responsibility for closing the socket to the Java reader. The Java reader could, when it has finished consuming output form Python, set a flag on a volatile variable to indicate that Python has fully returned, and then close the socket. Then if an IOException is thrown in the write thread, it only swallows the exception if we are expecting it.\n\nThis would also let us remove the warning message right now.", "comments": ["Fixed by:\nhttps://github.com/apache/spark/pull/640"], "derived": {"summary": "I chatted with [~adav] a bit about this. Right now we drop IOExceptions because they are (in some cases) expected if a Python worker returns before consuming its entire input.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "PySpark should distinguish expected IOExceptions from unexpected ones in the worker - I chatted with [~adav] a bit about this. Right now we drop IOExceptions because they are (in some cases) expected if a Python worker returns before consuming its entire input."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed by:\nhttps://github.com/apache/spark/pull/640"}]}}
{"project": "SPARK", "issue_id": "SPARK-1580", "title": "[MLlib] ALS: Estimate communication and computation costs given a partitioner", "status": "Resolved", "priority": "Minor", "reporter": "Tor Myklebust", "assignee": "Tor Myklebust", "labels": [], "created": "2014-04-23T02:10:14.000+0000", "updated": "2014-08-02T04:25:48.000+0000", "description": "It would be nice to be able to estimate the amount of work needed to solve an ALS problem.  The chief components of this \"work\" are computation time---time spent forming and solving the least squares problems---and communication cost---the number of bytes sent across the network.  Communication cost depends heavily on how the users and products are partitioned.\n\nWe currently do not try to cluster users or products so that fewer feature vectors need to be communicated.  This is intended as a first step toward that end---we ought to be able to tell whether one partitioning is better than another.", "comments": ["User 'mengxr' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1731", "Issue resolved by pull request 1731\n[https://github.com/apache/spark/pull/1731]"], "derived": {"summary": "It would be nice to be able to estimate the amount of work needed to solve an ALS problem. The chief components of this \"work\" are computation time---time spent forming and solving the least squares problems---and communication cost---the number of bytes sent across the network.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "[MLlib] ALS: Estimate communication and computation costs given a partitioner - It would be nice to be able to estimate the amount of work needed to solve an ALS problem. The chief components of this \"work\" are computation time---time spent forming and solving the least squares problems---and communication cost---the number of bytes sent across the network."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 1731\n[https://github.com/apache/spark/pull/1731]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1581", "title": "Allow One Flume Avro RPC Server for Each Worker rather than Just One Worker", "status": "Resolved", "priority": "Minor", "reporter": "Christophe Clapp", "assignee": null, "labels": ["Flume"], "created": "2014-04-23T03:58:00.000+0000", "updated": "2014-12-11T20:21:10.000+0000", "description": null, "comments": ["A pull request has been posted for this issue:<br>Author: christopheclc<br>URL: https://github.com/apache/spark/pull/495", "No follow-up from OP explaining the change, and so the PR was closed already."], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Allow One Flume Avro RPC Server for Each Worker rather than Just One Worker"}, {"q": "What updates or decisions were made in the discussion?", "a": "No follow-up from OP explaining the change, and so the PR was closed already."}]}}
{"project": "SPARK", "issue_id": "SPARK-1582", "title": "Job cancellation does not interrupt threads", "status": "Resolved", "priority": "Major", "reporter": "Aaron Davidson", "assignee": "Aaron Davidson", "labels": [], "created": "2014-04-23T04:46:07.000+0000", "updated": "2014-04-25T06:25:20.000+0000", "description": "Cancelling Spark jobs is limited because executors that are blocked are not interrupted. In effect, the cancellation will succeed and the job will no longer be \"running\", but executor threads may still be tied up with the cancelled job and unable to do further work until complete. This is particularly problematic in the case of deadlock or unlimited/long timeouts.\n\nIt would be useful if cancelling a job would call Thread.interrupt() in order to interrupt blocking in most situations, such as Object monitors or IO. The one caveat is [HDFS-1208|https://issues.apache.org/jira/browse/HDFS-1208], where HDFS's DFSClient will not only swallow InterruptedException but may reinterpret them as IOException, causing HDFS to mark a node as permanently failed. Thus, this feature must be optional and probably off by default.", "comments": [], "derived": {"summary": "Cancelling Spark jobs is limited because executors that are blocked are not interrupted. In effect, the cancellation will succeed and the job will no longer be \"running\", but executor threads may still be tied up with the cancelled job and unable to do further work until complete.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Job cancellation does not interrupt threads - Cancelling Spark jobs is limited because executors that are blocked are not interrupted. In effect, the cancellation will succeed and the job will no longer be \"running\", but executor threads may still be tied up with the cancelled job and unable to do further work until complete."}]}}
{"project": "SPARK", "issue_id": "SPARK-1583", "title": "Use java.util.HashMap.remove by mistake in BlockManagerMasterActor.removeBlockManager", "status": "Resolved", "priority": "Major", "reporter": "Shixiong Zhu", "assignee": "Shixiong Zhu", "labels": ["easyfix"], "created": "2014-04-23T06:26:35.000+0000", "updated": "2014-04-24T12:54:22.000+0000", "description": "The following code in BlockManagerMasterActor.removeBlockManager uses a value to remove an entry from java.util.HashMap.\n\n      if (locations.size == 0) {\n        blockLocations.remove(locations)\n      }\n\nShould change to \"blockLocations.remove(blockId)\".", "comments": ["PR: https://github.com/apache/spark/pull/500"], "derived": {"summary": "The following code in BlockManagerMasterActor. removeBlockManager uses a value to remove an entry from java.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Use java.util.HashMap.remove by mistake in BlockManagerMasterActor.removeBlockManager - The following code in BlockManagerMasterActor. removeBlockManager uses a value to remove an entry from java."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR: https://github.com/apache/spark/pull/500"}]}}
{"project": "SPARK", "issue_id": "SPARK-1584", "title": "Upgrade Flume dependency to 1.4.0", "status": "Resolved", "priority": "Major", "reporter": "Sandy Ryza", "assignee": "Theodore michael Malaska", "labels": [], "created": "2014-04-23T06:43:25.000+0000", "updated": "2014-04-25T04:02:55.000+0000", "description": null, "comments": ["This will allow us to take advantage of features like compression and encryption in Flume channels.", "Hey Sandy,\n\nI hope you don't mind. I made a pull request for this Jira.\n\nSPARK-1584 #507 - https://github.com/apache/spark/pull/507/files\n\nLet me know if there is anything else I can do.\n\nThanks", "Never mind it does like the build issue I'm having is related to this change.  The pull request may be bad."], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Upgrade Flume dependency to 1.4.0"}, {"q": "What updates or decisions were made in the discussion?", "a": "Never mind it does like the build issue I'm having is related to this change.  The pull request may be bad."}]}}
{"project": "SPARK", "issue_id": "SPARK-1585", "title": "Not robust Lasso causes Infinity on weights and losses", "status": "Closed", "priority": "Major", "reporter": "Xusen Yin", "assignee": "Xusen Yin", "labels": [], "created": "2014-04-23T09:45:35.000+0000", "updated": "2014-05-19T06:00:59.000+0000", "description": "Lasso uses LeastSquaresGradient and L1Updater, but \n\ndiff = brzWeights.dot(brzData) - label\n\nin LeastSquaresGradient would cause too big diff, then will affect the L1Updater, which increases weights exponentially. Small shrinkage value cannot lasso weights back to zero then. Finally, the weights and losses reach Infinity.\n\nFor example, data = (0.5 repeats 10k times), weights = (0.6 repeats 10k times), then data.dot(weights) approximates 300+, the diff will be 300. Then L1Updater sets weights to approximate 300. In the next iteration, the weights will be set to approximate 30000, and so on.", "comments": ["I think the gradient should pull the weights back. If I'm wrong, could you create an example code to demonstrate the problem? -Xiangrui", "All relates to the step size.", "I see. I close it now.", "Parameter tuning is vital for LASSO, especially the step size. Large step size causes large updating value, then infinity occurs."], "derived": {"summary": "Lasso uses LeastSquaresGradient and L1Updater, but \n\ndiff = brzWeights. dot(brzData) - label\n\nin LeastSquaresGradient would cause too big diff, then will affect the L1Updater, which increases weights exponentially.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Not robust Lasso causes Infinity on weights and losses - Lasso uses LeastSquaresGradient and L1Updater, but \n\ndiff = brzWeights. dot(brzData) - label\n\nin LeastSquaresGradient would cause too big diff, then will affect the L1Updater, which increases weights exponentially."}, {"q": "What updates or decisions were made in the discussion?", "a": "Parameter tuning is vital for LASSO, especially the step size. Large step size causes large updating value, then infinity occurs."}]}}
{"project": "SPARK", "issue_id": "SPARK-1586", "title": "Fix issues with spark development under windows", "status": "Resolved", "priority": "Major", "reporter": "Mridul Muralidharan", "assignee": "Mridul Muralidharan", "labels": [], "created": "2014-04-23T09:49:47.000+0000", "updated": "2014-04-25T17:55:16.000+0000", "description": null, "comments": ["Immediate issues fixed though there are more hive tests failing due to path related issues. pr : https://github.com/apache/spark/pull/505"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Fix issues with spark development under windows"}, {"q": "What updates or decisions were made in the discussion?", "a": "Immediate issues fixed though there are more hive tests failing due to path related issues. pr : https://github.com/apache/spark/pull/505"}]}}
{"project": "SPARK", "issue_id": "SPARK-1587", "title": "Fix thread leak in spark", "status": "Resolved", "priority": "Major", "reporter": "Mridul Muralidharan", "assignee": "Mridul Muralidharan", "labels": [], "created": "2014-04-23T09:51:41.000+0000", "updated": "2014-04-25T17:54:24.000+0000", "description": "SparkContext.stop does not cause all threads to exit.\nWhen running tests via scalatest (which keeps reusing the same vm), over time, this causes too many threads to be created causing tests to fail due to inability to create more threads.\n", "comments": ["Fixed, https://github.com/apache/spark/pull/504"], "derived": {"summary": "SparkContext. stop does not cause all threads to exit.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Fix thread leak in spark - SparkContext. stop does not cause all threads to exit."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed, https://github.com/apache/spark/pull/504"}]}}
{"project": "SPARK", "issue_id": "SPARK-1588", "title": "SPARK_JAVA_OPTS and SPARK_YARN_USER_ENV are not getting propagated", "status": "Resolved", "priority": "Blocker", "reporter": "Mridul Muralidharan", "assignee": "Sandy Ryza", "labels": [], "created": "2014-04-23T11:31:34.000+0000", "updated": "2014-04-29T19:55:26.000+0000", "description": "We could use SPARK_JAVA_OPTS to pass JAVA_OPTS to be used in the master.\nThis is no longer working in current master.", "comments": ["Noticed this specifically in yarn-alpha - we use SPARK_JAVA_OPTS to propagate system properties : this is not working anymore in the master.", "The approach in spark-submit is to capture SPARK_JAVA_OPTS and convert them into spark.executor.javaOptions and spark.driver.javaOptions - I think we can just do the same thing in the yarn client to provide backwards compatiblity.", "Btw - SPARK_JAVA_OPTS is officially dropped in 1.0 and there are loud warnings printed if it's used. Unfortunately this was a brute force mechanism used all over the place that had tons of corner cases and bugs.\n\nBut I agree we should do our best to backwards support the old mechanisms. It's currently backwards-supported in the park-submit script but not the yarn client.", "Apparently, SPARK_YARN_USER_ENV is also broken", "Mridul put up pr: https://github.com/apache/spark/pull/514", "PR for an adapted version of Mridul's patch - https://github.com/apache/spark/pull/586"], "derived": {"summary": "We could use SPARK_JAVA_OPTS to pass JAVA_OPTS to be used in the master. This is no longer working in current master.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "SPARK_JAVA_OPTS and SPARK_YARN_USER_ENV are not getting propagated - We could use SPARK_JAVA_OPTS to pass JAVA_OPTS to be used in the master. This is no longer working in current master."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR for an adapted version of Mridul's patch - https://github.com/apache/spark/pull/586"}]}}
{"project": "SPARK", "issue_id": "SPARK-1589", "title": "Compare Option[Partitioner] and Partitioner directly", "status": "Resolved", "priority": "Major", "reporter": "Shixiong Zhu", "assignee": "Shixiong Zhu", "labels": [], "created": "2014-04-23T14:58:55.000+0000", "updated": "2014-04-24T12:54:39.000+0000", "description": "PairRDDFunctions.partitionBy compares Option[Partitioner] and Partitioner directly.\n\nCode: https://github.com/apache/spark/blob/39f85e0322cfecefbc30e7d5a30356cfab1e9640/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala#L294", "comments": ["PR: https://github.com/apache/spark/pull/508"], "derived": {"summary": "PairRDDFunctions. partitionBy compares Option[Partitioner] and Partitioner directly.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Compare Option[Partitioner] and Partitioner directly - PairRDDFunctions. partitionBy compares Option[Partitioner] and Partitioner directly."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR: https://github.com/apache/spark/pull/508"}]}}
{"project": "SPARK", "issue_id": "SPARK-1590", "title": "Recommend to use FindBugs", "status": "Resolved", "priority": "Minor", "reporter": "Shixiong Zhu", "assignee": "Shixiong Zhu", "labels": [], "created": "2014-04-23T15:14:10.000+0000", "updated": "2014-04-25T10:07:59.000+0000", "description": "FindBugs is an open source program created by Bill Pugh and David Hovemeyer which looks for bugs in Java code. It uses static analysis to identify hundreds of different potential types of errors in Java programs.\n\nAlthough Spark is a Scala project, FindBugs is still helpful. For example, I used it to find SPARK-1583 and SPARK-1589. However, the disadvantage is that the report generated by FindBugs usually contains many false alarms for a Scala project.\n\n\n\n", "comments": ["I attached a screenshot of FindBugs plugin in IDEA", "I love static analysis, and like FindBugs. I've submitted a few changes already that follow from static analysis. Over time I hope a lot more small fixes from inspections can be incorporated.\n\nIn general I find that IntelliJ's inspections are much better than FindBugs. And for Scala, IntelliJ already has a decent set of Scala inspections, whereas FindBugs isn't quite set up to make sense of Scala.\n\nI also find that incorporating static analysis into the build, which inevitably means trying to warn or fail on builds with a new 'error', doesn't work. Too many false positives.\n\nTherefore I suggest that there's not a particular action here, other than the idea that, when the dust settles from 1.0.0, we should run static analysis tools -- probably IntelliJ's really -- and submit patches that fix existing non-trivial issues. And repeat that process periodically.", "Agree that a periodical scanning is better then incorporating into the build, since there are so many false positives.", "I agree with this course of action. [~zsxwing] the issues you've been reporting are super helpful. I think the way to go here is to periodically look through these and contribute fixes.\n\n[~srowen] are you doing any custom configurations in intellij to enable these? Or do you just go with the defaults?", "[~zsxwing] I'm marking this as not an issue now, but if you feel we should re-consider adding this to the build please feel free to open it.", "To enable in IntelliJ, go to Preferences > Inspections and choose the inspection profile you have enabled for inspection. (At the bottom right of the editor window, click the small man icon and you can select a profile to use for continuously highlighting issues in your code in the editor.) Choose Scala from the dropdown. I enable Scala: General and Scala: Method signature.\n\nIt can flag some serious errors, but also small items to polish. For example it flags calls to Arrays.size instead of Arrays.length, which incur the overhead of a conversion. Or missing tail-recursion annotation, old format strings, etc.\n\nIt's easy to make a pass and fix these automatically; mostly a question of whether it would disruptive at this point as the changes tend to span many files."], "derived": {"summary": "FindBugs is an open source program created by Bill Pugh and David Hovemeyer which looks for bugs in Java code. It uses static analysis to identify hundreds of different potential types of errors in Java programs.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Recommend to use FindBugs - FindBugs is an open source program created by Bill Pugh and David Hovemeyer which looks for bugs in Java code. It uses static analysis to identify hundreds of different potential types of errors in Java programs."}, {"q": "What updates or decisions were made in the discussion?", "a": "To enable in IntelliJ, go to Preferences > Inspections and choose the inspection profile you have enabled for inspection. (At the bottom right of the editor window, click the small man icon and you can select a profile to use for continuously highlighting issues in your code in the editor.) Choose Scala from the dropdown. I enable Scala: General and Scala: Method signature.\n\nIt can flag some serious errors, but also small items to polish. For example it flags calls to Arrays.size instead of Arrays.length, which incur the overhead of a conversion. Or missing tail-recursion annotation, old format strings, etc.\n\nIt's easy to make a pass and fix these automatically; mostly a question of whether it would disruptive at this point as the changes tend to span many files."}]}}
{"project": "SPARK", "issue_id": "SPARK-1591", "title": "scala.MatchError executing custom UDTF", "status": "Resolved", "priority": "Minor", "reporter": "Ken Ellinwood", "assignee": null, "labels": [], "created": "2014-04-23T16:40:40.000+0000", "updated": "2015-07-20T12:48:53.000+0000", "description": "My custom UDTF fails to execute in Shark even though it runs fine in Hive.\n\nscala.MatchError: [orange, 1, Black, 419] (of class java.util.ArrayList)\n    at scala.runtime.ScalaRunTime$.array_clone(ScalaRunTime.scala:118)\n    at shark.execution.UDTFCollector.collect(UDTFOperator.scala:92)\n    at org.apache.hadoop.hive.ql.udf.generic.GenericUDTF.forward(GenericUDTF.java:91)\n    at com.mycompany.warehouse.hive.HiveUdtfColorTreeTable.process(HiveUdtfColorTreeTable.java:98)\n    at shark.execution.UDTFOperator.explode(UDTFOperator.scala:79)\n    at shark.execution.LateralViewJoinOperator$$anonfun$processPartition$1.apply(LateralViewJoinOperator.scala:141)\n\n\nThe code at UDTFOperator.scala, line 92 is making two assumptions which are not true in my case.  First, it claims to need to clone the row object.  Second, it assumes all rows objects are arrays.  In my case the row is represented by ArrayList and does not need to be cloned because my UDTF creates a new one for each row already.   The clone operation fails because my row is not an array.\n\nI changed my implementation to use an array, but we have a non-trivial number of custom UDFs that all work with Hive and I think they should work in Shark without modification.", "comments": ["Hi Ken,\n\nI'd be curious if your UDTF works with Spark SQL, which is a from scratch rewrite of Shark that will be included in Spark 1.0.  My guess is we are going to run into the same problem, but if we do I'd like to fix it.\n\nMichael"], "derived": {"summary": "My custom UDTF fails to execute in Shark even though it runs fine in Hive. scala.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "scala.MatchError executing custom UDTF - My custom UDTF fails to execute in Shark even though it runs fine in Hive. scala."}, {"q": "What updates or decisions were made in the discussion?", "a": "Hi Ken,\n\nI'd be curious if your UDTF works with Spark SQL, which is a from scratch rewrite of Shark that will be included in Spark 1.0.  My guess is we are going to run into the same problem, but if we do I'd like to fix it.\n\nMichael"}]}}
{"project": "SPARK", "issue_id": "SPARK-1592", "title": "Old streaming input blocks not removed automatically from the BlockManagers", "status": "Resolved", "priority": "Blocker", "reporter": "Tathagata Das", "assignee": "Tathagata Das", "labels": [], "created": "2014-04-23T19:17:31.000+0000", "updated": "2016-10-29T18:34:09.000+0000", "description": "The raw input data is stored as blocks in BlockManagers. Earlier they were cleared by cleaner ttl. Now since streaming does not require cleaner TTL to be set, the block would not get cleared. This increases up the Spark's memory usage, which is not even accounted and shown in the Spark storage UI. It may cause the data blocks to spill over to disk, which eventually slows down the receiving of data (persisting to memory become bottlenecked by writing to disk).\n\n", "comments": ["https://github.com/apache/spark/pull/512"], "derived": {"summary": "The raw input data is stored as blocks in BlockManagers. Earlier they were cleared by cleaner ttl.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "Old streaming input blocks not removed automatically from the BlockManagers - The raw input data is stored as blocks in BlockManagers. Earlier they were cleared by cleaner ttl."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/512"}]}}
{"project": "SPARK", "issue_id": "SPARK-1593", "title": "Add status command to Spark Daemons(master/worker)", "status": "Resolved", "priority": "Major", "reporter": "Pradeep Chanumolu", "assignee": null, "labels": ["patch"], "created": "2014-04-23T21:00:23.000+0000", "updated": "2016-01-09T13:00:30.000+0000", "description": "Currently we have only start and stop commands for spark\ndaemons(master/worker). So a status command can be added to spark-daemon.sh and spark-daemons.sh which tells if the master/worker is alive or not.", "comments": ["Patch for adding status command to spark daemons. ", "Make a pull request.", "[~pradeepbaji] if you want to take this forward, can you make a pull request instead? This would need to be revised to match the current script.\n\nIt makes some sense although this is only going to tell you if the process is running."], "derived": {"summary": "Currently we have only start and stop commands for spark\ndaemons(master/worker). So a status command can be added to spark-daemon.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add status command to Spark Daemons(master/worker) - Currently we have only start and stop commands for spark\ndaemons(master/worker). So a status command can be added to spark-daemon."}, {"q": "What updates or decisions were made in the discussion?", "a": "[~pradeepbaji] if you want to take this forward, can you make a pull request instead? This would need to be revised to match the current script.\n\nIt makes some sense although this is only going to tell you if the process is running."}]}}
{"project": "SPARK", "issue_id": "SPARK-1594", "title": "Cleaning up MLlib APIs and guide", "status": "Resolved", "priority": "Blocker", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-04-23T21:40:27.000+0000", "updated": "2014-05-06T05:18:39.000+0000", "description": "Final pass for the 1.0 release.", "comments": ["https://github.com/apache/spark/pull/524"], "derived": {"summary": "Final pass for the 1. 0 release.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Cleaning up MLlib APIs and guide - Final pass for the 1. 0 release."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/524"}]}}
{"project": "SPARK", "issue_id": "SPARK-1595", "title": "Remove VectorRDDs", "status": "Resolved", "priority": "Major", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-04-23T21:42:38.000+0000", "updated": "2014-05-06T05:17:39.000+0000", "description": "This is basically an RDD#map with Vectors.dense, maybe useful for Java users but it is simple to write one.", "comments": ["https://github.com/apache/spark/pull/524"], "derived": {"summary": "This is basically an RDD#map with Vectors. dense, maybe useful for Java users but it is simple to write one.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Remove VectorRDDs - This is basically an RDD#map with Vectors. dense, maybe useful for Java users but it is simple to write one."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/524"}]}}
{"project": "SPARK", "issue_id": "SPARK-1596", "title": "Re-arrange public methods in evaluation.", "status": "Resolved", "priority": "Major", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-04-23T21:44:26.000+0000", "updated": "2014-05-06T05:17:55.000+0000", "description": "binary.BinaryClassificationMetrics is the only public API under evaluation. We should move it one level up, and possibly clean the names.", "comments": ["https://github.com/apache/spark/pull/524"], "derived": {"summary": "binary. BinaryClassificationMetrics is the only public API under evaluation.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Re-arrange public methods in evaluation. - binary. BinaryClassificationMetrics is the only public API under evaluation."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/524"}]}}
{"project": "SPARK", "issue_id": "SPARK-1597", "title": "Add a version of reduceByKey that takes the Partitioner as a second argument", "status": "Resolved", "priority": "Blocker", "reporter": "Matei Alexandru Zaharia", "assignee": "Sandeep Singh", "labels": [], "created": "2014-04-23T22:10:07.000+0000", "updated": "2014-09-21T04:59:57.000+0000", "description": "Most of our shuffle methods can take a Partitioner or a number of partitions as a second argument, but for some reason reduceByKey takes the Partitioner as a *first* argument: http://spark.apache.org/docs/0.9.1/api/core/#org.apache.spark.rdd.PairRDDFunctions. We should deprecate that version and add one where the Partitioner is the second argument.", "comments": ["https://github.com/apache/spark/pull/550", "User 'techaddict' has created a pull request for this issue:\n[https://github.com/apache/spark/pull/550|https://github.com/apache/spark/pull/550]", "See relevant comment here:\nhttps://github.com/apache/spark/pull/550#issuecomment-41483260"], "derived": {"summary": "Most of our shuffle methods can take a Partitioner or a number of partitions as a second argument, but for some reason reduceByKey takes the Partitioner as a *first* argument: http://spark. apache.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Add a version of reduceByKey that takes the Partitioner as a second argument - Most of our shuffle methods can take a Partitioner or a number of partitions as a second argument, but for some reason reduceByKey takes the Partitioner as a *first* argument: http://spark. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "See relevant comment here:\nhttps://github.com/apache/spark/pull/550#issuecomment-41483260"}]}}
{"project": "SPARK", "issue_id": "SPARK-1598", "title": "Mark main methods experimental", "status": "Resolved", "priority": "Major", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-04-23T22:42:49.000+0000", "updated": "2014-04-25T21:12:43.000+0000", "description": "We should treat the parameters in the main methods as part of our APIs. They are not quite consistent at this time, so we should mark them experimental and look for a unified solution in the next sprint.", "comments": ["We will move main methods to examples instead."], "derived": {"summary": "We should treat the parameters in the main methods as part of our APIs. They are not quite consistent at this time, so we should mark them experimental and look for a unified solution in the next sprint.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Mark main methods experimental - We should treat the parameters in the main methods as part of our APIs. They are not quite consistent at this time, so we should mark them experimental and look for a unified solution in the next sprint."}, {"q": "What updates or decisions were made in the discussion?", "a": "We will move main methods to examples instead."}]}}
{"project": "SPARK", "issue_id": "SPARK-1599", "title": "Allow to use intercept in Ridge and Lasso", "status": "Resolved", "priority": "Major", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-04-23T23:01:53.000+0000", "updated": "2014-05-06T05:18:08.000+0000", "description": "AddIntercept in Ridge and Lasso was disabled in 0.9.1 for a quick fix. For 1.0, we should remove this restriction.\n\nPenalizing the intercept variable may not be the right thing to do. We can solve this problem by adding weighted regularization later.", "comments": ["https://github.com/apache/spark/pull/524"], "derived": {"summary": "AddIntercept in Ridge and Lasso was disabled in 0. 9.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Allow to use intercept in Ridge and Lasso - AddIntercept in Ridge and Lasso was disabled in 0. 9."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/524"}]}}
{"project": "SPARK", "issue_id": "SPARK-1600", "title": "flaky \"recovery with file input stream\" test in streaming.CheckpointSuite", "status": "Resolved", "priority": "Major", "reporter": "Nan Zhu", "assignee": "Josh Rosen", "labels": ["flaky-test"], "created": "2014-04-23T23:29:57.000+0000", "updated": "2015-02-16T23:42:36.000+0000", "description": "the case \"recovery with file input stream.recovery with file input stream  \" sometimes fails when the Jenkins is very busy with an unrelated change \n\nI have met it for 3 times, I also saw it in other places, \n\nthe latest example is in \n\nhttps://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/14397/\n\nwhere the modification is just in YARN related files....\n\nI once reported in dev mail list: http://apache-spark-developers-list.1001551.n3.nabble.com/a-weird-test-case-in-Streaming-td6116.html\n", "comments": ["Will try to address this post 1.0 release", "This is still flaky; here's the test result from a recent failure:\n\n{code}\nError Message\n\nList() was empty\nStacktrace\n\nsbt.ForkMain$ForkError: List() was empty\n\tat org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:500)\n\tat org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1555)\n\tat org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:466)\n\tat org.apache.spark.streaming.CheckpointSuite$$anonfun$12.apply$mcV$sp(CheckpointSuite.scala:336)\n\tat org.apache.spark.streaming.CheckpointSuite$$anonfun$12.apply(CheckpointSuite.scala:282)\n\tat org.apache.spark.streaming.CheckpointSuite$$anonfun$12.apply(CheckpointSuite.scala:282)\n\tat org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)\n\tat org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)\n\tat org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)\n\tat org.scalatest.Transformer.apply(Transformer.scala:22)\n\tat org.scalatest.Transformer.apply(Transformer.scala:20)\n\tat org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)\n\tat org.scalatest.Suite$class.withFixture(Suite.scala:1122)\n\tat org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)\n\tat org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)\n\tat org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)\n\tat org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)\n\tat org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)\n\tat org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)\n\tat org.apache.spark.streaming.CheckpointSuite.org$scalatest$BeforeAndAfter$$super$runTest(CheckpointSuite.scala:43)\n\tat org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)\n\tat org.apache.spark.streaming.CheckpointSuite.runTest(CheckpointSuite.scala:43)\n\tat org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)\n\tat org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)\n\tat org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)\n\tat org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)\n\tat scala.collection.immutable.List.foreach(List.scala:318)\n\tat org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)\n\tat org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)\n\tat org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)\n\tat org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)\n\tat org.scalatest.FunSuite.runTests(FunSuite.scala:1555)\n\tat org.scalatest.Suite$class.run(Suite.scala:1424)\n\tat org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)\n\tat org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)\n\tat org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)\n\tat org.scalatest.SuperEngine.runImpl(Engine.scala:545)\n\tat org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)\n\tat org.apache.spark.streaming.CheckpointSuite.org$scalatest$BeforeAndAfter$$super$run(CheckpointSuite.scala:43)\n\tat org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)\n\tat org.apache.spark.streaming.CheckpointSuite.run(CheckpointSuite.scala:43)\n\tat org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)\n\tat org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)\n\tat sbt.ForkMain$Run$2.call(ForkMain.java:294)\n\tat sbt.ForkMain$Run$2.call(ForkMain.java:284)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n{code}", "I'm planning to address this as part of https://github.com/apache/spark/pull/3687", "User 'JoshRosen' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/3801", "User 'JoshRosen' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/4633", "I've also merged this into branch-1.2."], "derived": {"summary": "the case \"recovery with file input stream. recovery with file input stream  \" sometimes fails when the Jenkins is very busy with an unrelated change \n\nI have met it for 3 times, I also saw it in other places, \n\nthe latest example is in \n\nhttps://amplab.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "flaky \"recovery with file input stream\" test in streaming.CheckpointSuite - the case \"recovery with file input stream. recovery with file input stream  \" sometimes fails when the Jenkins is very busy with an unrelated change \n\nI have met it for 3 times, I also saw it in other places, \n\nthe latest example is in \n\nhttps://amplab."}, {"q": "What updates or decisions were made in the discussion?", "a": "I've also merged this into branch-1.2."}]}}
{"project": "SPARK", "issue_id": "SPARK-1601", "title": "CacheManager#getOrCompute() does not return an InterruptibleIterator", "status": "Resolved", "priority": "Major", "reporter": "Aaron Davidson", "assignee": "Reynold Xin", "labels": [], "created": "2014-04-24T02:52:48.000+0000", "updated": "2014-05-15T18:27:00.000+0000", "description": "When getOrCompute goes down the \"compute\" path for an RDD that should be stored in memory, it returns an iterator over an array, which is not interruptible. This mainly means that any consumers of that iterator, which may consume slowly, will not be interrupted in a timely manner.", "comments": ["This was fixed in:\nhttps://github.com/apache/spark/pull/521"], "derived": {"summary": "When getOrCompute goes down the \"compute\" path for an RDD that should be stored in memory, it returns an iterator over an array, which is not interruptible. This mainly means that any consumers of that iterator, which may consume slowly, will not be interrupted in a timely manner.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "CacheManager#getOrCompute() does not return an InterruptibleIterator - When getOrCompute goes down the \"compute\" path for an RDD that should be stored in memory, it returns an iterator over an array, which is not interruptible. This mainly means that any consumers of that iterator, which may consume slowly, will not be interrupted in a timely manner."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was fixed in:\nhttps://github.com/apache/spark/pull/521"}]}}
{"project": "SPARK", "issue_id": "SPARK-1602", "title": "Cancelled jobs can lead to corrupted cached partitions", "status": "Resolved", "priority": "Critical", "reporter": "Aaron Davidson", "assignee": "Reynold Xin", "labels": [], "created": "2014-04-24T02:55:53.000+0000", "updated": "2014-04-29T00:35:53.000+0000", "description": "When jobs are cancelled, the InterruptibleIterator simply returns hasNext = false, which can confuse the CacheManager into thinking that the RDD was fully computed, causing it to store the incomplete result into the BlockManager.\n\nThis unfortunately will lead to incorrect results being returned on all future operations containing this RDD while it's still cached.", "comments": [], "derived": {"summary": "When jobs are cancelled, the InterruptibleIterator simply returns hasNext = false, which can confuse the CacheManager into thinking that the RDD was fully computed, causing it to store the incomplete result into the BlockManager. This unfortunately will lead to incorrect results being returned on all future operations containing this RDD while it's still cached.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Cancelled jobs can lead to corrupted cached partitions - When jobs are cancelled, the InterruptibleIterator simply returns hasNext = false, which can confuse the CacheManager into thinking that the RDD was fully computed, causing it to store the incomplete result into the BlockManager. This unfortunately will lead to incorrect results being returned on all future operations containing this RDD while it's still cached."}]}}
{"project": "SPARK", "issue_id": "SPARK-1603", "title": "Flaky test: o.a.s.streaming.StreamingContextSuite", "status": "Resolved", "priority": "Critical", "reporter": "Nan Zhu", "assignee": "Tathagata Das", "labels": ["flaky-test"], "created": "2014-04-24T04:26:19.000+0000", "updated": "2016-01-13T10:21:22.000+0000", "description": "When Jenkins was testing 5 PRs at the same time, the test results in my PR shows that  stop gracefully in StreamingContextSuite failed, \n\nthe stacktrace is as\n\n{quote}\n\n stop gracefully *** FAILED *** (8 seconds, 350 milliseconds)\n[info]   akka.actor.InvalidActorNameException: actor name [JobScheduler] is not unique!\n[info]   at akka.actor.dungeon.ChildrenContainer$TerminatingChildrenContainer.reserve(ChildrenContainer.scala:192)\n[info]   at akka.actor.dungeon.Children$class.reserveChild(Children.scala:77)\n[info]   at akka.actor.ActorCell.reserveChild(ActorCell.scala:338)\n[info]   at akka.actor.dungeon.Children$class.makeChild(Children.scala:186)\n[info]   at akka.actor.dungeon.Children$class.attachChild(Children.scala:42)\n[info]   at akka.actor.ActorCell.attachChild(ActorCell.scala:338)\n[info]   at akka.actor.ActorSystemImpl.actorOf(ActorSystem.scala:518)\n[info]   at org.apache.spark.streaming.scheduler.JobScheduler.start(JobScheduler.scala:57)\n[info]   at org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:434)\n[info]   at org.apache.spark.streaming.StreamingContextSuite$$anonfun$14$$anonfun$apply$mcV$sp$3.apply$mcVI$sp(StreamingContextSuite.scala:174)\n[info]   at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)\n[info]   at org.apache.spark.streaming.StreamingContextSuite$$anonfun$14.apply$mcV$sp(StreamingContextSuite.scala:163)\n[info]   at org.apache.spark.streaming.StreamingContextSuite$$anonfun$14.apply(StreamingContextSuite.scala:159)\n[info]   at org.apache.spark.streaming.StreamingContextSuite$$anonfun$14.apply(StreamingContextSuite.scala:159)\n[info]   at org.scalatest.FunSuite$$anon$1.apply(FunSuite.scala:1265)\n[info]   at org.scalatest.Suite$class.withFixture(Suite.scala:1974)\n[info]   at org.apache.spark.streaming.StreamingContextSuite.withFixture(StreamingContextSuite.scala:34)\n[info]   at org.scalatest.FunSuite$class.invokeWithFixture$1(FunSuite.scala:1262)\n[info]   at org.scalatest.FunSuite$$anonfun$runTest$1.apply(FunSuite.scala:1271)\n[info]   at org.scalatest.FunSuite$$anonfun$runTest$1.apply(FunSuite.scala:1271)\n[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:198)\n[info]   at org.scalatest.FunSuite$class.runTest(FunSuite.scala:1271)\n[info]   at org.apache.spark.streaming.StreamingContextSuite.org$scalatest$BeforeAndAfter$$super$runTest(StreamingContextSuite.scala:34)\n[info]   at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:171)\n[info]   at org.apache.spark.streaming.StreamingContextSuite.runTest(StreamingContextSuite.scala:34)\n[info]   at org.scalatest.FunSuite$$anonfun$runTests$1.apply(FunSuite.scala:1304)\n[info]   at org.scalatest.FunSuite$$anonfun$runTests$1.apply(FunSuite.scala:1304)\n[info]   at org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:260)\n[info]   at org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:249)\n[info]   at scala.collection.immutable.List.foreach(List.scala:318)\n[info]   at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:249)\n[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:326)\n[info]   at org.scalatest.FunSuite$class.runTests(FunSuite.scala:1304)\n[info]   at org.apache.spark.streaming.StreamingContextSuite.runTests(StreamingContextSuite.scala:34)\n[info]   at org.scalatest.Suite$class.run(Suite.scala:2303)\n[info]   at org.apache.spark.streaming.StreamingContextSuite.org$scalatest$FunSuite$$super$run(StreamingContextSuite.scala:34)\n[info]   at org.scalatest.FunSuite$$anonfun$run$1.apply(FunSuite.scala:1310)\n[info]   at org.scalatest.FunSuite$$anonfun$run$1.apply(FunSuite.scala:1310)\n[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:362)\n[info]   at org.scalatest.FunSuite$class.run(FunSuite.scala:1310)\n[info]   at org.apache.spark.streaming.StreamingContextSuite.org$scalatest$BeforeAndAfter$$super$run(StreamingContextSuite.scala:34)\n[info]   at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:208)\n[info]   at org.apache.spark.streaming.StreamingContextSuite.run(StreamingContextSuite.scala:34)\n[info]   at org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:214)\n[info]   at sbt.RunnerWrapper$1.runRunner2(FrameworkWrapper.java:223)\n[info]   at sbt.RunnerWrapper$1.execute(FrameworkWrapper.java:236)\n[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:294)\n[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:284)\n[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n[info]   at java.lang.Thread.run(Thread.java:744)\n\n{quote}\n\nI think we don't need to assign a fixed name to JobScheduler Actor, instead, we can just use auto-generated name in Akka", "comments": ["I ran into this myself as well. From the code I can see that actor should be shut down correctly when streaming context is shutdown. So there should not be lingering actors. I think this is happening because of the clearing of \"JobScheduler\" from the actorSystem's name space is done lazily after the actorSystem.stop(<actor>) returns.\n\nAutogenerated name, or JobScheduler-<current-system-time> may be the solution. ", "[~tdas], I checked the code, the fix itself should be pretty easy, just use autogenerated name is OK, \n\nthe reason is that akka.system.stop() is an asynchronous method, which means that you have no guarantee on when the actor is really stopped...", "I think I made a temporary fix for now for that test. Bundled it with this PR.\nhttps://github.com/apache/spark/pull/652/files#diff-e144dbee130ed84f9465853ddce65f8eR186 \n\nI am a little afraid that there is some corner case that leads to actors being leaked and adding auto-generated names would mask that problem. So I just added a Thread.sleep(100) in that graceful shutdown test so that it gives the system time to stop and cleanup the actor before a new StreamingContext is started (100 ms should be enough). If the problem persists (test is still flaky) then it is more likely that there is corner where the actor is not being stopped every time.\n\nWill close this JIRA after a few days of observation.", "Ah, just made a PR at the same time, https://github.com/apache/spark/pull/659\n\nI'm afraid a fixed time threshold cannot resolve the problem (especially that this case is hard to reproduce, always happen when Jenkins is super overloaded)....I once met the similar problem in another PR: https://github.com/apache/spark/pull/186, there, we have an asynchronous    constructor...\n\nyou are right, we should check if there are some cases we forgot to close the actor (but I think since you call ssc.stop() after each test case, the actor should be closed eventually)", "I think we havent seen the flakiness since then. So I am marking this as resolved. ", "Reopening caused it failed here:\n\nhttps://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-pre-YARN/hadoop.version=2.0.0-mr1-cdh4.1.2,label=centos/1656/\nhttps://amplab.cs.berkeley.edu/jenkins/job/Spark-1.3-SBT/AMPLAB_JENKINS_BUILD_PROFILE=hadoop2.3,label=centos/139/\nhttps://amplab.cs.berkeley.edu/jenkins/job/Spark-1.3-Maven-with-YARN/HADOOP_PROFILE=hadoop-2.4,label=centos/179/\n\n{code}\nError Message\n\n10431395 did not equal 6237090, and 10431395 did not equal 6237091 Received records = 10431395, processed records = 6237089\nStacktrace\n\n      org.scalatest.exceptions.TestFailedException: 10431395 did not equal 6237090, and 10431395 did not equal 6237091 Received records = 10431395, processed records = 6237089\n      at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:500)\n      at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1555)\n      at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:466)\n      at org.apache.spark.streaming.StreamingContextSuite$$anonfun$16$$anonfun$apply$mcV$sp$3.apply$mcVI$sp(StreamingContextSuite.scala:198)\n      at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)\n      at org.apache.spark.streaming.StreamingContextSuite$$anonfun$16.apply$mcV$sp(StreamingContextSuite.scala:181)\n      at org.apache.spark.streaming.StreamingContextSuite$$anonfun$16.apply(StreamingContextSuite.scala:177)\n      at org.apache.spark.streaming.StreamingContextSuite$$anonfun$16.apply(StreamingContextSuite.scala:177)\n      at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)\n      at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)\n      at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)\n      at org.scalatest.Transformer.apply(Transformer.scala:22)\n      at org.scalatest.Transformer.apply(Transformer.scala:20)\n      at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)\n      at org.scalatest.Suite$class.withFixture(Suite.scala:1122)\n      at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)\n{code}\n\nand\n\n{code}\nError Message\n\nThe code passed to failAfter did not complete within 10000 milliseconds.\nStacktrace\n\n      org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to failAfter did not complete within 10000 milliseconds.\n      at org.scalatest.concurrent.Timeouts$$anonfun$failAfter$1.apply(Timeouts.scala:249)\n      at org.scalatest.concurrent.Timeouts$$anonfun$failAfter$1.apply(Timeouts.scala:249)\n      at org.scalatest.concurrent.Timeouts$class.timeoutAfter(Timeouts.scala:345)\n      at org.scalatest.concurrent.Timeouts$class.failAfter(Timeouts.scala:245)\n{code}", "[~andrewor14] Do you see this error any more?"], "derived": {"summary": "When Jenkins was testing 5 PRs at the same time, the test results in my PR shows that  stop gracefully in StreamingContextSuite failed, \n\nthe stacktrace is as\n\n{quote}\n\n stop gracefully *** FAILED *** (8 seconds, 350 milliseconds)\n[info]   akka. actor.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Flaky test: o.a.s.streaming.StreamingContextSuite - When Jenkins was testing 5 PRs at the same time, the test results in my PR shows that  stop gracefully in StreamingContextSuite failed, \n\nthe stacktrace is as\n\n{quote}\n\n stop gracefully *** FAILED *** (8 seconds, 350 milliseconds)\n[info]   akka. actor."}, {"q": "What updates or decisions were made in the discussion?", "a": "[~andrewor14] Do you see this error any more?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1604", "title": "Couldn't run spark-submit with yarn cluster mode when built with assemble-deps", "status": "Resolved", "priority": "Major", "reporter": "Kan Zhang", "assignee": null, "labels": [], "created": "2014-04-24T05:14:16.000+0000", "updated": "2015-02-26T01:20:03.000+0000", "description": "{code}\nSPARK_JAR=./assembly/target/scala-2.10/spark-assembly-1.0.0-SNAPSHOT-hadoop2.3.0-deps.jar ./bin/spark-submit ./examples/target/scala-2.10/spark-examples_2.10-1.0.0-SNAPSHOT.jar --master yarn --deploy-mode cluster --class org.apache.spark.examples.sql.JavaSparkSQL \nException in thread \"main\" java.lang.ClassNotFoundException: org.apache.spark.deploy.yarn.Client\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:366)\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:358)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:270)\n\tat org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:234)\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:47)\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n{code}", "comments": ["I would not be surprised if this is the same issue as SPARK-1520 ?\nhttps://issues.apache.org/jira/browse/SPARK-1520", "I doubt it, since when I ran it in YARN client mode, it did work.", "How did you build?  I see you have the \"-deps.jar\"\n\nWhen I build the assembly  it works fine for me.  I'm using maven to build though too.\n\nSPARK_JAR=assembly/target/scala-2.10/spark-assembly-1.0.0-SNAPSHOT-hadoop2.3.0.jar\n\n./bin/spark-submit  examples/target/scala-2.10/spark-examples-1.0.0-SNAPSHOT-hadoop2.3.0.jar  --master yarn --deploy-mode cluster --class org.apache.spark.examples.SparkPi --arg yarn-cluster \n", "Ah, that could be the reason. I was using sbt assemble-deps and then package to build. Just verified, when building the normal sbt assembly jar, problem disappears. Could be an issue with the former build sequence. \n\nMoving this to BUILD.", "if its just in that particular sequence of build steps, perhaps also lower to not be a blocker.", "Sure, lowered it to Major.", "I'm not sure why you're including both assemble-deps and the examples jar. The examples jar includes all of spark and its dependencies.\n\nI've noted here we should probably mark spark as provided in the examples jar so it doesn't embed Spark. https://issues.apache.org/jira/browse/SPARK-1565", "My bad. Just tested, the issue remains without including the assemble-deps jar.\n\nAgree on examples jar not including Spark.", "This may be obsolete anyway, but sounds like it only occurred with an assembly built in a certain way with SBT, but published assemblies used to run examples would be built by Maven now."], "derived": {"summary": "{code}\nSPARK_JAR=. /assembly/target/scala-2.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Couldn't run spark-submit with yarn cluster mode when built with assemble-deps - {code}\nSPARK_JAR=. /assembly/target/scala-2."}, {"q": "What updates or decisions were made in the discussion?", "a": "This may be obsolete anyway, but sounds like it only occurred with an assembly built in a certain way with SBT, but published assemblies used to run examples would be built by Maven now."}]}}
{"project": "SPARK", "issue_id": "SPARK-1605", "title": "Improve mllib.linalg.Vector", "status": "Resolved", "priority": "Major", "reporter": "Sandeep Singh", "assignee": null, "labels": [], "created": "2014-04-24T05:37:58.000+0000", "updated": "2014-10-13T17:12:44.000+0000", "description": "We can make current Vector a wrapper around Breeze.linalg.Vector ?", "comments": ["I think this was on purpose, to try to hide breeze as an implementation detail, at least in public APIs?", "[~srowen] shouldn't method's like toBreeze be public ? ", "`toBreeze` exposes a breeze type. We might want to mark it DeveloperApi and make it public, but I'm not sure whether we should do that in v1.0. Given a `mllib.linalg.Vector`, you can call `toArray` to get the values or operate directly on DenseVector/SparseVector.", "Another WontFix then?"], "derived": {"summary": "We can make current Vector a wrapper around Breeze. linalg.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Improve mllib.linalg.Vector - We can make current Vector a wrapper around Breeze. linalg."}, {"q": "What updates or decisions were made in the discussion?", "a": "Another WontFix then?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1606", "title": "spark-submit needs `--arg` for every application parameter", "status": "Resolved", "priority": "Blocker", "reporter": "Xiangrui Meng", "assignee": "Patrick Wendell", "labels": [], "created": "2014-04-24T07:00:25.000+0000", "updated": "2014-05-03T17:42:33.000+0000", "description": "If the application has a few parameters, the spark-submit command looks like the following:\n\n{code}\nspark-submit --master yarn-cluster --class main.Class --arg --numPartitions --arg 8 --arg --kryo --arg true\n{code}\n\nIt is a little bit hard to read and modify. Maybe it is okay to treat all arguments after `main.Class` as application parameters.\n\n{code}\nspark-submit --master yarn-cluster --class main.Class --numPartitions 8 --kryo true\n{code}", "comments": ["I submitted a PR here that adds the following syntax.\n\n{code}\n./bin/spark-submit [options] user.jar [user options]\n{code}\n\nhttps://github.com/apache/spark/pull/563", "testing jira integration", "Testing automated closing of JIRA's", "Testing automated closing of JIRA's", "Testing automated closing of JIRA's", "[SparkSubmitArguments.scala#L287|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala#L287] \n{code}--arg ARG Argument to be passed to your application's main class. This option can be specified multiple times for multiple args.\n{code} should be deleted"], "derived": {"summary": "If the application has a few parameters, the spark-submit command looks like the following:\n\n{code}\nspark-submit --master yarn-cluster --class main. Class --arg --numPartitions --arg 8 --arg --kryo --arg true\n{code}\n\nIt is a little bit hard to read and modify.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "spark-submit needs `--arg` for every application parameter - If the application has a few parameters, the spark-submit command looks like the following:\n\n{code}\nspark-submit --master yarn-cluster --class main. Class --arg --numPartitions --arg 8 --arg --kryo --arg true\n{code}\n\nIt is a little bit hard to read and modify."}, {"q": "What updates or decisions were made in the discussion?", "a": "[SparkSubmitArguments.scala#L287|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala#L287] \n{code}--arg ARG Argument to be passed to your application's main class. This option can be specified multiple times for multiple args.\n{code} should be deleted"}]}}
{"project": "SPARK", "issue_id": "SPARK-1607", "title": "Remove use of octal literals, deprecated in Scala 2.10 / removed in 2.11", "status": "Resolved", "priority": "Minor", "reporter": "Sean R. Owen", "assignee": null, "labels": ["literal", "octal", "scala", "yarn"], "created": "2014-04-24T08:40:32.000+0000", "updated": "2014-04-25T07:39:43.000+0000", "description": "Octal literals like \"0700\" are deprecated in Scala 2.10, generating a warning. They have been removed entirely in 2.11. See https://issues.scala-lang.org/browse/SI-7618\n\nThis change simply replaces two uses of octals with hex literals, which seemed the next-best representation since they express a bit mask (file permission in particular)", "comments": ["Thank you [~srowen], just a small thing yet such an annoying warning."], "derived": {"summary": "Octal literals like \"0700\" are deprecated in Scala 2. 10, generating a warning.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Remove use of octal literals, deprecated in Scala 2.10 / removed in 2.11 - Octal literals like \"0700\" are deprecated in Scala 2. 10, generating a warning."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thank you [~srowen], just a small thing yet such an annoying warning."}]}}
{"project": "SPARK", "issue_id": "SPARK-1608", "title": "Cast.nullable should be true when cast from StringType to NumericType/TimestampType", "status": "Resolved", "priority": "Major", "reporter": "Takuya Ueshin", "assignee": null, "labels": [], "created": "2014-04-24T10:29:04.000+0000", "updated": "2014-04-29T22:36:11.000+0000", "description": "Cast.nullable should be true when cast from StringType to NumericType or TimestampType.\nBecause if StringType expression has an illegal number string or illegal timestamp string, the casted value becomes null.", "comments": ["Pull-requested: https://github.com/apache/spark/pull/532"], "derived": {"summary": "Cast. nullable should be true when cast from StringType to NumericType or TimestampType.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Cast.nullable should be true when cast from StringType to NumericType/TimestampType - Cast. nullable should be true when cast from StringType to NumericType or TimestampType."}, {"q": "What updates or decisions were made in the discussion?", "a": "Pull-requested: https://github.com/apache/spark/pull/532"}]}}
{"project": "SPARK", "issue_id": "SPARK-1609", "title": "Executor fails to start when Command.extraJavaOptions contains multiple Java options", "status": "Resolved", "priority": "Blocker", "reporter": "Guoqiang Li", "assignee": "Guoqiang Li", "labels": [], "created": "2014-04-24T11:04:47.000+0000", "updated": "2014-05-15T18:32:15.000+0000", "description": "{code}\nexport SPARK_JAVA_OPTS=\"-server -Dspark.ui.killEnabled=false -Dspark.akka.askTimeout=120 -Dspark.akka.timeout=120 -Dspark.locality.wait=10000 -Dspark.storage.blockManagerTimeoutIntervalMs=6000000 -Dspark.storage.memoryFraction=0.7 -Dspark.broadcast.factory=org.apache.spark.broadcast.TorrentBroadcastFactory\"\n{code}\nExecutor fails to start.\n{code}\nexport SPARK_JAVA_OPTS=\"-Dspark.ui.killEnabled=false -Dspark.akka.askTimeout=120 -Dspark.akka.timeout=120 -Dspark.locality.wait=10000 -Dspark.storage.blockManagerTimeoutIntervalMs=6000000 -Dspark.storage.memoryFraction=0.7 -Dspark.broadcast.factory=org.apache.spark.broadcast.TorrentBroadcastFactory\"\n{code}\nExecutor can work \n\n\n\n", "comments": ["Hi witgo, on several issues you have opened, I have had trouble understanding what is being reported and what the proposed solution is. Here I had to look a while to see that the difference is \"-server\"? Are you proposing not setting that option? \n\nIt's a JVM option. Does it cause the JVM to not start? You're running Java 8, but I think -server is still a valid (if redundant) flag in that version.\n\nYour log shows that SPARK_JAVA_OPTS is deprecated anyway. What about using the invocation it suggests?\n\nThe errors are not obviously related to -server. It looks like workers are simply failing to join the master. What about their logs?\n", "Hi Sean Owen \nI guess all(except spark.*)  will cause executor fails to start .\nLog has been re-upload", "Spark Executor Command: \"/opt/jdk1.8.0/bin/java\" \"-cp\" \":/opt/spark/classes/echo-1.0-SNAPSHOT.jar:/opt/spark/classes/toona-assembly-1.0.0-SNAPSHOT.jar:/opt/spark/spark-1.0.0-cdh3/conf:/opt/spark/spark-1.0.0-cdh3/lib/spark-assembly-1.0.0-SNAPSHOT-hadoop0.20.2-cdh3u5.jar\" \"-Xss2m -Dspark.ui.killEnabled=false\" \"-Xms5120M\" \"-Xmx5120M\" \"org.apache.spark.executor.CoarseGrainedExecutorBackend\" \"akka.tcp://spark@spark:47185/user/CoarseGrainedScheduler\" \"7\" \"spark\" \"4\" \"akka.tcp://sparkWorker@spark:35646/user/Worker\" \"app-20140425183255-0000\"\n========================================\n\nInvalid thread stack size: -Xss2m -Dspark.ui.killEnabled=false\nError: Could not create the Java Virtual Machine.\nError: A fatal exception has occurred. Program will exit.", "Yeah you got it, that is the problem:\n\n{code}\"-Xss2m -Dspark.ui.killEnabled=false\"{code}\n... as one argument\n\nI agree with your change in the PR.", "Issue resolved by pull request 547\n[https://github.com/apache/spark/pull/547]", "This was not actually closed - I was testing a new script for automatically closing issues."], "derived": {"summary": "{code}\nexport SPARK_JAVA_OPTS=\"-server -Dspark. ui.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Executor fails to start when Command.extraJavaOptions contains multiple Java options - {code}\nexport SPARK_JAVA_OPTS=\"-server -Dspark. ui."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was not actually closed - I was testing a new script for automatically closing issues."}]}}
{"project": "SPARK", "issue_id": "SPARK-1610", "title": "Cast from BooleanType to NumericType should use exact type value.", "status": "Resolved", "priority": "Major", "reporter": "Takuya Ueshin", "assignee": null, "labels": [], "created": "2014-04-24T11:56:24.000+0000", "updated": "2014-04-29T22:35:34.000+0000", "description": "Cast from BooleanType to NumericType are all using Int value.\nBut it causes ClassCastException when the casted value is used by the following evaluation like the code below:\n\n{quote}\nscala> import org.apache.spark.sql.catalyst._\nimport org.apache.spark.sql.catalyst._\n\nscala> import types._\nimport types._\n\nscala> import expressions._\nimport expressions._\n\nscala> Add(Cast(Literal(true), ShortType), Literal(1.toShort)).eval()\njava.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Short\n\tat scala.runtime.BoxesRunTime.unboxToShort(BoxesRunTime.java:102)\n\tat scala.math.Numeric$ShortIsIntegral$.plus(Numeric.scala:72)\n\tat org.apache.spark.sql.catalyst.expressions.Add$$anonfun$eval$2.apply(arithmetic.scala:58)\n\tat org.apache.spark.sql.catalyst.expressions.Add$$anonfun$eval$2.apply(arithmetic.scala:58)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.n2(Expression.scala:114)\n\tat org.apache.spark.sql.catalyst.expressions.Add.eval(arithmetic.scala:58)\n\tat .<init>(<console>:17)\n\tat .<clinit>(<console>)\n\tat .<init>(<console>:7)\n\tat .<clinit>(<console>)\n\tat $print(<console>)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:734)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:983)\n\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:573)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:604)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:568)\n\tat scala.tools.nsc.interpreter.ILoop.reallyInterpret$1(ILoop.scala:760)\n\tat scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:805)\n\tat scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:717)\n\tat scala.tools.nsc.interpreter.ILoop.processLine$1(ILoop.scala:581)\n\tat scala.tools.nsc.interpreter.ILoop.innerLoop$1(ILoop.scala:588)\n\tat scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:591)\n\tat scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply$mcZ$sp(ILoop.scala:882)\n\tat scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:837)\n\tat scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:837)\n\tat scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)\n\tat scala.tools.nsc.interpreter.ILoop.process(ILoop.scala:837)\n\tat scala.tools.nsc.MainGenericRunner.runTarget$1(MainGenericRunner.scala:83)\n\tat scala.tools.nsc.MainGenericRunner.process(MainGenericRunner.scala:96)\n\tat scala.tools.nsc.MainGenericRunner$.main(MainGenericRunner.scala:105)\n\tat scala.tools.nsc.MainGenericRunner.main(MainGenericRunner.scala)\n{quote}\n", "comments": ["Pull-requested: https://github.com/apache/spark/pull/533"], "derived": {"summary": "Cast from BooleanType to NumericType are all using Int value. But it causes ClassCastException when the casted value is used by the following evaluation like the code below:\n\n{quote}\nscala> import org.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Cast from BooleanType to NumericType should use exact type value. - Cast from BooleanType to NumericType are all using Int value. But it causes ClassCastException when the casted value is used by the following evaluation like the code below:\n\n{quote}\nscala> import org."}, {"q": "What updates or decisions were made in the discussion?", "a": "Pull-requested: https://github.com/apache/spark/pull/533"}]}}
{"project": "SPARK", "issue_id": "SPARK-1611", "title": "Incorrect initialization order in AppendOnlyMap", "status": "Resolved", "priority": "Minor", "reporter": "Shixiong Zhu", "assignee": "Shixiong Zhu", "labels": ["easyfix"], "created": "2014-04-24T12:51:37.000+0000", "updated": "2014-04-25T06:45:20.000+0000", "description": "The initialization order of \"growThreshold\" and \"LOAD_FACTOR\" is incorrect. \"growThreshold\" will be initialized to 0.", "comments": ["PR: https://github.com/apache/spark/pull/534"], "derived": {"summary": "The initialization order of \"growThreshold\" and \"LOAD_FACTOR\" is incorrect. \"growThreshold\" will be initialized to 0.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Incorrect initialization order in AppendOnlyMap - The initialization order of \"growThreshold\" and \"LOAD_FACTOR\" is incorrect. \"growThreshold\" will be initialized to 0."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR: https://github.com/apache/spark/pull/534"}]}}
{"project": "SPARK", "issue_id": "SPARK-1612", "title": "Potential resource leaks in Utils.copyStream and Utils.offsetBytes", "status": "Resolved", "priority": "Minor", "reporter": "Shixiong Zhu", "assignee": "Shixiong Zhu", "labels": ["easyfix"], "created": "2014-04-24T13:57:52.000+0000", "updated": "2014-08-01T20:25:29.000+0000", "description": "Should move the \"close\" statements into a \"finally\" block.", "comments": ["PR: https://github.com/apache/spark/pull/535", "A pull request has been posted for this issue:\nAuthor: zsxwing\nURL: [https://github.com/apache/spark/pull/535|https://github.com/apache/spark/pull/535]"], "derived": {"summary": "Should move the \"close\" statements into a \"finally\" block.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Potential resource leaks in Utils.copyStream and Utils.offsetBytes - Should move the \"close\" statements into a \"finally\" block."}, {"q": "What updates or decisions were made in the discussion?", "a": "A pull request has been posted for this issue:\nAuthor: zsxwing\nURL: [https://github.com/apache/spark/pull/535|https://github.com/apache/spark/pull/535]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1613", "title": "Difficulty starting up cluster on Amazon EC2", "status": "Resolved", "priority": "Major", "reporter": "Johnny King", "assignee": null, "labels": ["ec2", "pyspark", "spark-shell", "sparkcontext"], "created": "2014-04-24T14:08:21.000+0000", "updated": "2014-12-13T22:30:46.000+0000", "description": "When I use Pyspark and the Spark-shell, master disconnects from all slaves. \n\nExecutor updated: ...  is now FAILED (Command exited with code 1)\nMaster disconnected from cluster\n\nIf it's working for you, can you show me the exact steps taken? I'm not doing anything out of the ordinary, just following deploy EC2 instructions.", "comments": ["I'm resolving this as \"Incomplete\" since this JIRA doesn't have enough information to allow me to debug / diagnose the problem.  Please re-open if you have additional information / a more complete description of the problem.  Thanks!"], "derived": {"summary": "When I use Pyspark and the Spark-shell, master disconnects from all slaves. Executor updated:.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Difficulty starting up cluster on Amazon EC2 - When I use Pyspark and the Spark-shell, master disconnects from all slaves. Executor updated:."}, {"q": "What updates or decisions were made in the discussion?", "a": "I'm resolving this as \"Incomplete\" since this JIRA doesn't have enough information to allow me to debug / diagnose the problem.  Please re-open if you have additional information / a more complete description of the problem.  Thanks!"}]}}
{"project": "SPARK", "issue_id": "SPARK-1614", "title": "Move Mesos protobufs out of TaskState", "status": "Resolved", "priority": "Minor", "reporter": "Shivaram Venkataraman", "assignee": null, "labels": ["Starter"], "created": "2014-04-24T16:44:39.000+0000", "updated": "2016-01-12T13:44:37.000+0000", "description": "To isolate usage of Mesos protobufs it would be good to move them out of TaskState into either a new class (MesosUtils ?) or CoarseGrainedMesos{Executor, Backend}.\n\nThis would allow applications to build Spark to run without including protobuf from Mesos in their shaded jars.  This is one way to avoid protobuf conflicts between Mesos and Hadoop (https://issues.apache.org/jira/browse/MESOS-1203)\n", "comments": ["I am considering moving the protobufs to a new object - something like object org.apache.spark.MesosTaskState. Is that acceptable solution with regards to the requirements (to avoid the conflicts)? If not, can you please suggest which place would be the best for it?"], "derived": {"summary": "To isolate usage of Mesos protobufs it would be good to move them out of TaskState into either a new class (MesosUtils ?) or CoarseGrainedMesos{Executor, Backend}. This would allow applications to build Spark to run without including protobuf from Mesos in their shaded jars.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Move Mesos protobufs out of TaskState - To isolate usage of Mesos protobufs it would be good to move them out of TaskState into either a new class (MesosUtils ?) or CoarseGrainedMesos{Executor, Backend}. This would allow applications to build Spark to run without including protobuf from Mesos in their shaded jars."}, {"q": "What updates or decisions were made in the discussion?", "a": "I am considering moving the protobufs to a new object - something like object org.apache.spark.MesosTaskState. Is that acceptable solution with regards to the requirements (to avoid the conflicts)? If not, can you please suggest which place would be the best for it?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1615", "title": "Very subtle race condition in SparkListenerSuite", "status": "Resolved", "priority": "Minor", "reporter": "Andrew Or", "assignee": "Andrew Or", "labels": [], "created": "2014-04-24T17:33:03.000+0000", "updated": "2014-11-05T10:45:26.000+0000", "description": "Much of SparkListenerSuite relies on LiveListenerBus's waitUntilEmpty() method. As the name suggests, this waits until the event queue is empty. However, the following race condition could happen:\n\n(1) We dequeue the event\n(2) The queue is empty, we return true\n(3) The test asserts something assuming that all listeners have finished executing (and fails)\n(4) The listeners receive the event\n\nThis has been a possible race condition for a long time, but for some reason we've never run into it.", "comments": [], "derived": {"summary": "Much of SparkListenerSuite relies on LiveListenerBus's waitUntilEmpty() method. As the name suggests, this waits until the event queue is empty.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Very subtle race condition in SparkListenerSuite - Much of SparkListenerSuite relies on LiveListenerBus's waitUntilEmpty() method. As the name suggests, this waits until the event queue is empty."}]}}
{"project": "SPARK", "issue_id": "SPARK-1616", "title": "input file not found issue ", "status": "Resolved", "priority": "Major", "reporter": "prasad potipireddi", "assignee": null, "labels": [], "created": "2014-04-24T19:42:54.000+0000", "updated": "2014-05-10T03:39:38.000+0000", "description": null, "comments": ["Env:\n      I have setup spark cluster with 3 nodes. one master and 3 workers\n\nUsecase:\n     1. I have copied jar file into master node\n     2. I have provided the input file only in master node but not in any worker nodes\n     3. Run the jar using \n              java -cp \"~/spark/assembly/target/scala-2.10/spark-assembly_2.10-0.9.0-incubating-hadoop2.2.0.jar:`echo $SCALA_HOME/lib/*.jar | sed 's/ /:/g'`:~/examples/jars/SparkPoc-0.0.1-SNAPSHOT.jar\" -Dspark.master=local com.spark.mllib.LinearRegression spark://<master node>:7077 ~/examples/data/sampledata.csv\n\n  then i got File Not Found error\n\n\nAlternative Solution:\n        I have copied the same Sampledata.csv into all other worker nodes then its working as expected.\n\n\n\n", "Hi Prasad,\n\nThis doesn't really sound like a bug, but a mismatch between your expectations and Spark's.\n\nWhen you tell a Spark job to read data from a file, Spark expects the file to be available to all the workers. This can be achieved in several ways:\n\n* Using a distributed file system such as HDFS\n* Using a networked file system such as NFS\n* Using Spark's file distribution mechanism, which will copy the file to the workers for you (e.g. spark-submit's --files argument if you run 1.0)\n* Manually copying the file like you did\n\nBut Spark will not automatically copy data to worker nodes on your behalf."], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "input file not found issue"}, {"q": "What updates or decisions were made in the discussion?", "a": "Hi Prasad,\n\nThis doesn't really sound like a bug, but a mismatch between your expectations and Spark's.\n\nWhen you tell a Spark job to read data from a file, Spark expects the file to be available to all the workers. This can be achieved in several ways:\n\n* Using a distributed file system such as HDFS\n* Using a networked file system such as NFS\n* Using Spark's file distribution mechanism, which will copy the file to the workers for you (e.g. spark-submit's --files argument if you run 1.0)\n* Manually copying the file like you did\n\nBut Spark will not automatically copy data to worker nodes on your behalf."}]}}
{"project": "SPARK", "issue_id": "SPARK-1617", "title": "Exposing receiver state and errors in the streaming UI", "status": "Resolved", "priority": "Major", "reporter": "Tathagata Das", "assignee": "Tathagata Das", "labels": [], "created": "2014-04-24T21:55:05.000+0000", "updated": "2014-04-25T04:36:56.000+0000", "description": "The receiver state (active or inactive) and last error was not exposed in the UI prior to these changes.", "comments": ["https://github.com/apache/spark/pull/540/"], "derived": {"summary": "The receiver state (active or inactive) and last error was not exposed in the UI prior to these changes.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Exposing receiver state and errors in the streaming UI - The receiver state (active or inactive) and last error was not exposed in the UI prior to these changes."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/540/"}]}}
{"project": "SPARK", "issue_id": "SPARK-1618", "title": "Socket receiver not restarting properly when connection is refused", "status": "Resolved", "priority": "Major", "reporter": "Tathagata Das", "assignee": "Tathagata Das", "labels": [], "created": "2014-04-24T21:58:41.000+0000", "updated": "2014-04-25T04:37:05.000+0000", "description": "If the socket receiver cannot connect in the first attempt, it should try to restart after a delay. That was broken, as the thread that restarts (hence, stops) the receiver waited on Thread.join on itself!", "comments": ["https://github.com/apache/spark/pull/540/"], "derived": {"summary": "If the socket receiver cannot connect in the first attempt, it should try to restart after a delay. That was broken, as the thread that restarts (hence, stops) the receiver waited on Thread.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Socket receiver not restarting properly when connection is refused - If the socket receiver cannot connect in the first attempt, it should try to restart after a delay. That was broken, as the thread that restarts (hence, stops) the receiver waited on Thread."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/540/"}]}}
{"project": "SPARK", "issue_id": "SPARK-1619", "title": "Spark shell should use spark-submit", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "labels": [], "created": "2014-04-24T22:17:48.000+0000", "updated": "2014-04-25T07:00:27.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Spark shell should use spark-submit"}]}}
{"project": "SPARK", "issue_id": "SPARK-1620", "title": "Uncaught exception from Akka scheduler", "status": "Resolved", "priority": "Blocker", "reporter": "Mark Hamstra", "assignee": "Mark Hamstra", "labels": [], "created": "2014-04-24T22:49:32.000+0000", "updated": "2014-05-14T17:07:56.000+0000", "description": "I've been looking at this one in the context of a BlockManagerMaster that OOMs and doesn't respond to heartBeat(), but I suspect that there may be problems elsewhere where we use Akka's scheduler.\n\nThe basic nature of the problem is that we are expecting exceptions thrown from a scheduled function to be caught in the thread where _ActorSystem_.scheduler.schedule() or scheduleOnce() has been called.  In fact, the scheduled function runs on its own thread, so any exceptions that it throws are not caught in the thread that called schedule() -- e.g., unanswered BlockManager heartBeats (scheduled in BlockManager#initialize) that end up throwing exceptions in BlockManagerMaster#askDriverWithReply do not cause those exceptions to be handled by the Executor thread's UncaughtExceptionHandler. ", "comments": ["I'm going to close this one for now, since I think something different is happening than I previously thought.", "On further investigation, it looks like there really is a problem with exceptions thrown by scheduled functions not being caught by any uncaught exception handler.", "Another two instances of the problem that actually aren't a problem at the moment: In deploy.worker.Worker and deploy.client.AppClient, tryRegisterAllMasters() can throw exceptions (e.g., from Master.toAkkaUrl(masterUrl)), and those exception would go unhandled in the calls from within the Akka scheduler -- i.e. within an invocation of registerWithMaster, all but the first call to tryRegisterAllMasters.  Right now, any later call to tryRegisterAllMasters() that would throw an exception should already have thrown in the first call that occurs outside the scheduled thread, so we should never get to the problem case.  If in the future, however, that behavior would change so that tryRegisterAllMasters() could succeed on the first call but throw within the later, scheduled calls (or if code added within the scheduled retryTimer could throw an exception) then the exception thrown from the scheduler thread will not be caught. ", "And one last instance: In scheduler.TaskSchedulerImpl#start(), checkSpeculatableTasks() is scheduled to be called every SPECULATION_INTERVAL.  If checkSpeculatableTasks() throws an exception, that exception will not be caught and no more invocations of checkSpeculatableTasks() will occur. ", "Issue resolved by pull request 622\n[https://github.com/apache/spark/pull/622]"], "derived": {"summary": "I've been looking at this one in the context of a BlockManagerMaster that OOMs and doesn't respond to heartBeat(), but I suspect that there may be problems elsewhere where we use Akka's scheduler. The basic nature of the problem is that we are expecting exceptions thrown from a scheduled function to be caught in the thread where _ActorSystem_.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Uncaught exception from Akka scheduler - I've been looking at this one in the context of a BlockManagerMaster that OOMs and doesn't respond to heartBeat(), but I suspect that there may be problems elsewhere where we use Akka's scheduler. The basic nature of the problem is that we are expecting exceptions thrown from a scheduled function to be caught in the thread where _ActorSystem_."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 622\n[https://github.com/apache/spark/pull/622]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1621", "title": "Update Chill to 0.3.6", "status": "Resolved", "priority": "Minor", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "labels": [], "created": "2014-04-24T23:23:14.000+0000", "updated": "2014-04-25T18:13:17.000+0000", "description": "It registers more Scala classes, including things like Ranges that we had to register manually before. See https://github.com/twitter/chill/releases for Chill's change log.", "comments": [], "derived": {"summary": "It registers more Scala classes, including things like Ranges that we had to register manually before. See https://github.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Update Chill to 0.3.6 - It registers more Scala classes, including things like Ranges that we had to register manually before. See https://github."}]}}
{"project": "SPARK", "issue_id": "SPARK-1622", "title": "Expose input split(s) accessed by a task in UI or logs", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": null, "labels": [], "created": "2014-04-25T02:23:44.000+0000", "updated": "2016-01-13T12:10:04.000+0000", "description": "Right now it's hard to debug which input files or blocks therein have invalid data. The InputSplit for a HadoopRDD is not even exposed programmatically in Scala/Java (it's private[spark]).", "comments": ["I think it would be good to have a general mechanism for RDD implementations to add contextual information for each task that ends up `TaskInfo` and ultimately in the UI. This could include values pinned to a specific task but also counters which can be aggregated across all the tasks in a stage.", "I assume this is WontFix given the severity, and lack of activity in over a year"], "derived": {"summary": "Right now it's hard to debug which input files or blocks therein have invalid data. The InputSplit for a HadoopRDD is not even exposed programmatically in Scala/Java (it's private[spark]).", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Expose input split(s) accessed by a task in UI or logs - Right now it's hard to debug which input files or blocks therein have invalid data. The InputSplit for a HadoopRDD is not even exposed programmatically in Scala/Java (it's private[spark])."}, {"q": "What updates or decisions were made in the discussion?", "a": "I assume this is WontFix given the severity, and lack of activity in over a year"}]}}
{"project": "SPARK", "issue_id": "SPARK-1623", "title": "SPARK-1623. Broadcast cleaner should use getCanonicalPath when deleting files by name", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Niraj Suthar", "labels": [], "created": "2014-04-25T03:10:35.000+0000", "updated": "2014-11-25T11:58:41.000+0000", "description": null, "comments": ["I'm not sure if this solves the issue reported, but we did merge this patch:\nhttps://github.com/apache/spark/pull/749", "User 'nsuthar' has created a pull request for this issue:\n[https://github.com/apache/spark/pull/546|https://github.com/apache/spark/pull/546]", "Yes, the original PR was clearly superseded by https://github.com/apache/spark/pull/749 , which was merged. It resolves the same issue."], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "SPARK-1623. Broadcast cleaner should use getCanonicalPath when deleting files by name"}, {"q": "What updates or decisions were made in the discussion?", "a": "Yes, the original PR was clearly superseded by https://github.com/apache/spark/pull/749 , which was merged. It resolves the same issue."}]}}
{"project": "SPARK", "issue_id": "SPARK-1624", "title": "addShutdownHook in DiskBlockManager Doesn't work the way it is supposed to work", "status": "Closed", "priority": "Blocker", "reporter": "Sandeep Singh", "assignee": "Sandeep Singh", "labels": [], "created": "2014-04-25T04:54:47.000+0000", "updated": "2014-04-25T05:11:09.000+0000", "description": "method Stop() is called in new Thread which calls Thead.stop() instead of method stop() provided.", "comments": ["https://github.com/apache/spark/pull/527"], "derived": {"summary": "method Stop() is called in new Thread which calls Thead. stop() instead of method stop() provided.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "addShutdownHook in DiskBlockManager Doesn't work the way it is supposed to work - method Stop() is called in new Thread which calls Thead. stop() instead of method stop() provided."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/527"}]}}
{"project": "SPARK", "issue_id": "SPARK-1625", "title": "Ensure all legacy YARN options are supported with spark-submit", "status": "Closed", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "labels": [], "created": "2014-04-25T06:13:31.000+0000", "updated": "2014-04-29T17:19:11.000+0000", "description": null, "comments": ["These aren't the only things broken. One big issue is that authentication isn't being passed properly anymore.  Unless that was fixed under another jira?", "I'll create a separate jira for that."], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Ensure all legacy YARN options are supported with spark-submit"}, {"q": "What updates or decisions were made in the discussion?", "a": "I'll create a separate jira for that."}]}}
{"project": "SPARK", "issue_id": "SPARK-1626", "title": "Update Spark YARN docs to use spark-submit", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Sandy Ryza", "labels": [], "created": "2014-04-25T06:14:04.000+0000", "updated": "2014-04-30T06:26:31.000+0000", "description": null, "comments": ["this is dup of SPARK-1492"], "derived": {"summary": "", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Update Spark YARN docs to use spark-submit"}, {"q": "What updates or decisions were made in the discussion?", "a": "this is dup of SPARK-1492"}]}}
{"project": "SPARK", "issue_id": "SPARK-1627", "title": "Support external aggregation in Spark SQL", "status": "Resolved", "priority": "Major", "reporter": "Chen Chao", "assignee": null, "labels": [], "created": "2014-04-25T07:36:32.000+0000", "updated": "2014-12-11T20:19:51.000+0000", "description": "Spark SQL Aggregator does not support external sorting now which is extremely important when data is much larger than memory. ", "comments": ["A related pull request: https://github.com/apache/spark/pull/867", "The discussion in https://github.com/apache/spark/pull/867 suggests this was subsumed by SPARK-2873."], "derived": {"summary": "Spark SQL Aggregator does not support external sorting now which is extremely important when data is much larger than memory.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support external aggregation in Spark SQL - Spark SQL Aggregator does not support external sorting now which is extremely important when data is much larger than memory."}, {"q": "What updates or decisions were made in the discussion?", "a": "The discussion in https://github.com/apache/spark/pull/867 suggests this was subsumed by SPARK-2873."}]}}
{"project": "SPARK", "issue_id": "SPARK-1628", "title": "Missing hashCode methods in Partitioner subclasses", "status": "Resolved", "priority": "Minor", "reporter": "Shixiong Zhu", "assignee": "Shixiong Zhu", "labels": ["easyfix"], "created": "2014-04-25T07:46:55.000+0000", "updated": "2014-06-08T21:19:19.000+0000", "description": "`hashCode` is not override in HashPartitioner, RangePartitioner, PythonPartitioner and PageRankUtils.CustomPartitioner. Should override hashcode() if overriding equals().", "comments": ["PR merged: https://github.com/apache/spark/pull/549"], "derived": {"summary": "`hashCode` is not override in HashPartitioner, RangePartitioner, PythonPartitioner and PageRankUtils. CustomPartitioner.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Missing hashCode methods in Partitioner subclasses - `hashCode` is not override in HashPartitioner, RangePartitioner, PythonPartitioner and PageRankUtils. CustomPartitioner."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR merged: https://github.com/apache/spark/pull/549"}]}}
{"project": "SPARK", "issue_id": "SPARK-1629", "title": "Spark should inline use of commons-lang `SystemUtils.IS_OS_WINDOWS` ", "status": "Resolved", "priority": "Minor", "reporter": "Guoqiang Li", "assignee": "Guoqiang Li", "labels": [], "created": "2014-04-25T10:54:24.000+0000", "updated": "2014-04-30T16:54:23.000+0000", "description": "Right now we use this but don't depend on it explicitly (which is wrong). We should probably just inline this function and remove the need to add a dependency.", "comments": ["I don't see any usage of Commons Lang in the whole project?\nTachyon uses commons-lang3 but it also brings it in as a dependency.", "Hi Sean Owen,see [Utils.scala|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/Utils.scala#L33]", "Oh I see, that was added yesterday and I hadn't updated my fork. Yes that's right. It only works since commons-lang happens to be imported by another dependency. I don't think it's worth bringing it all in just for one method.\n\nHow about we just 'inline' this with a utility method like:\n\n{code:scala}\ndef isWindows(): Boolean = {\n  try {\n    val osName = System.getProperty(\"os.name\")\n    osName != null && osName.startsWith(\"Windows\")\n  } catch {\n    case e: SecurityException => (log a warning and return false)\n  }\n}\n{code}\n", "[The PR 569|https://github.com/apache/spark/pull/569]", "Thanks for this fix"], "derived": {"summary": "Right now we use this but don't depend on it explicitly (which is wrong). We should probably just inline this function and remove the need to add a dependency.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Spark should inline use of commons-lang `SystemUtils.IS_OS_WINDOWS`  - Right now we use this but don't depend on it explicitly (which is wrong). We should probably just inline this function and remove the need to add a dependency."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for this fix"}]}}
{"project": "SPARK", "issue_id": "SPARK-1630", "title": "PythonRDDs don't handle nulls gracefully", "status": "Resolved", "priority": "Major", "reporter": "Kalpit Shah", "assignee": "Davies Liu", "labels": [], "created": "2014-04-25T17:35:14.000+0000", "updated": "2015-01-08T21:53:30.000+0000", "description": "If PythonRDDs receive a null element in iterators, they currently NPE. It would be better do log a DEBUG message and skip the write of NULL elements.\n\nHere are the 2 stack traces :\n\n14/04/22 03:44:19 ERROR executor.Executor: Uncaught exception in thread Thread[stdin writer for python,5,main]\njava.lang.NullPointerException\n  at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:267)\n  at org.apache.spark.api.python.PythonRDD$$anon$2.run(PythonRDD.scala:88)\n\n-------------------------------------------------------------------------------------\n\nPy4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.writeToFile.\n: java.lang.NullPointerException\n  at org.apache.spark.api.python.PythonRDD$.writeUTF(PythonRDD.scala:273)\n  at org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$2.apply(PythonRDD.scala:247)\n  at org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$2.apply(PythonRDD.scala:246)\n  at scala.collection.Iterator$class.foreach(Iterator.scala:727)\n  at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n  at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:246)\n  at org.apache.spark.api.python.PythonRDD$.writeToFile(PythonRDD.scala:285)\n  at org.apache.spark.api.python.PythonRDD$.writeToFile(PythonRDD.scala:280)\n  at org.apache.spark.api.python.PythonRDD.writeToFile(PythonRDD.scala)\n  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:606)\n  at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n  at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n  at py4j.Gateway.invoke(Gateway.java:259)\n  at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n  at py4j.commands.CallCommand.execute(CallCommand.java:79)\n  at py4j.GatewayConnection.run(GatewayConnection.java:207)\n  at java.lang.Thread.run(Thread.java:744)  \n\n\n\n", "comments": ["https://github.com/apache/spark/pull/554", "User 'kalpit' has created a pull request for this issue:\n[https://github.com/apache/spark/pull/554|https://github.com/apache/spark/pull/554]", "User 'davies' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1551", "We aren't passing completely arbitrary iterators of Java objects to {{writeIteratorToStream}}; instead, we only handle iterators of strings and byte arrays.  Nulls in data read from Hadoop input formats should already be converted to None by the Java pickling code.  Do you have an example where PythonRDD receives a null element and it's not due to a bug?  I'm worried that this patch will mask the presence of other errors.", "If a RDD is generated in Scala/Java by user code, such as rdd.map(user_func), it's possible to generate an Null in it (depend on some corner cases), then it will cause NPE.\n\nGiven RDD[String], it's correct that some row will be null, so it's better handle it gracefully.\n\nThis issue can not be reproduced in pure Python code.", "In the current Spark codebase, the PythonRDD constructor is only called from Python code.  In order for a user-created Scala/Java RDD<String> to be passed to PySpark, the user would need to have dug deep into PySpark's private APIs to call out to Java/Scala code to create a transformed RDD and wrap it into a PythonRDD.  Given this, is it fair to say that any in-the-wild NPEs encountered here by using Spark's public APIs are due to bugs in Spark/PySpark, or is there a case that I'm overlooking (e.g. is TextInputFormat allowed to return nulls?)?", "Here's my case that led me to filing this bug and a patch : I have a custom RDD which is implemented in Java. It implements compute() and partitions() API with semantics specific to our application. For some of our cases, CustomRDD<String> could have NULL values. In those cases, we didn't have a way to access the same in Python.\n\nIMO, this patch helps serve two purposes :\n1. If a CustomRDD<String> is implemented using Java or Scala and a user wishes to access this RDD in Python,R or some other language, they will be able to do so without loss of information (NULLs preserved).\n2. It facilitates preservation of cardinality and order of elements within a partition.", "Hi Kalpit,\n\nThanks for sharing your use-case; it seems like a reasonable thing that we should support.\n\nAs part of [~mlnick]'s patch for [SPARK-1416], we now have {{SerDeUtil.rddToPython}} for converting pair RDDs of arbitrary Java objects into RDDs that can be read by PySpark.  One alternative to this fix proposed here would be to add a similar converter from non-pair-RDDs to PythonRDDs that used the Java-side pickle library to pickle the strings as nulls.  However, this could have a negative performance impact since we'd be passing pickled objects instead of UTF-8 strings.\n\nGiven that the current proposed fix only affects RDD<String> and seems unlikely to mask serious bugs, I'm inclined to merge it.", "Based on some discussion in https://github.com/apache/spark/pull/1551, we've decided to hold off on fixing this: this issue only affects users that are calling private APIs and the fix adds complexity and could mask bugs in other parts of the code.", "We hit this issue with Kafka Python API, it will be fixed in https://github.com/apache/spark/pull/3715"], "derived": {"summary": "If PythonRDDs receive a null element in iterators, they currently NPE. It would be better do log a DEBUG message and skip the write of NULL elements.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "PythonRDDs don't handle nulls gracefully - If PythonRDDs receive a null element in iterators, they currently NPE. It would be better do log a DEBUG message and skip the write of NULL elements."}, {"q": "What updates or decisions were made in the discussion?", "a": "We hit this issue with Kafka Python API, it will be fixed in https://github.com/apache/spark/pull/3715"}]}}
{"project": "SPARK", "issue_id": "SPARK-1631", "title": "App name set in SparkConf (not in JVM properties) not respected by Yarn backend", "status": "Resolved", "priority": "Blocker", "reporter": "Marcelo Masiero Vanzin", "assignee": "Marcelo Masiero Vanzin", "labels": [], "created": "2014-04-25T18:07:02.000+0000", "updated": "2014-05-09T03:46:28.000+0000", "description": "When you submit an application that sets its name using a SparkContext constructor or SparkConf.setAppName(), the Yarn app name is not set and the app shows up as \"Spark\" in the RM UI.\n\nThat's because YarnClientSchedulerBackend only looks at the system properties to look for the app name, instead of looking at the app's config.\n\ne.g., app initializes like this:\n\n{code}\n    val sc = new SparkContext(new SparkConf().setAppName(\"Blah\"));\n{code}\n\nStart app like this:\n\n{noformat}\n  ./bin/spark-submit --master yarn --deploy-mode client blah blah blah\n{noformat}\n\nAnd app name in RM UI does not reflect the code.", "comments": ["PR: https://github.com/apache/spark/pull/539", "Issue resolved by pull request 539\n[https://github.com/apache/spark/pull/539]"], "derived": {"summary": "When you submit an application that sets its name using a SparkContext constructor or SparkConf. setAppName(), the Yarn app name is not set and the app shows up as \"Spark\" in the RM UI.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "App name set in SparkConf (not in JVM properties) not respected by Yarn backend - When you submit an application that sets its name using a SparkContext constructor or SparkConf. setAppName(), the Yarn app name is not set and the app shows up as \"Spark\" in the RM UI."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 539\n[https://github.com/apache/spark/pull/539]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1632", "title": "Avoid boxing in ExternalAppendOnlyMap compares", "status": "Resolved", "priority": "Major", "reporter": "Sandy Ryza", "assignee": "Sandy Ryza", "labels": [], "created": "2014-04-25T20:05:44.000+0000", "updated": "2014-04-26T00:55:54.000+0000", "description": "Hitting an OOME in ExternalAppendOnlyMap.KCComparator while boxing an int.  I don't know if this is the root cause, but the boxing is also avoidable.\n\nCode:\n{code}\n    def compare(kc1: (K, C), kc2: (K, C)): Int = {\n      kc1._1.hashCode().compareTo(kc2._1.hashCode())\n    }\n{code}\n\nError:\n{code}\njava.lang.OutOfMemoryError: GC overhead limit exceeded\n     at java.lang.Integer.valueOf(Integer.java:642)\n     at scala.Predef$.int2Integer(Predef.scala:370)\n     at org.apache.spark.util.collection.ExternalAppendOnlyMap$KCComparator.compare(ExternalAppendOnlyMap.scala:432)\n     at org.apache.spark.util.collection.ExternalAppendOnlyMap$KCComparator.compare(ExternalAppendOnlyMap.scala:430)\n     at org.apache.spark.util.collection.AppendOnlyMap$$anon$3.compare(AppendOnlyMap.scala:271)\n     at java.util.TimSort.mergeLo(TimSort.java:687)\n     at java.util.TimSort.mergeAt(TimSort.java:483)\n     at java.util.TimSort.mergeCollapse(TimSort.java:410)\n     at java.util.TimSort.sort(TimSort.java:214)\n     at java.util.Arrays.sort(Arrays.java:727)\n     at org.apache.spark.util.collection.AppendOnlyMap.destructiveSortedIterator(AppendOnlyMap.scala:274)\n     at org.apache.spark.util.collection.ExternalAppendOnlyMap.spill(ExternalAppendOnlyMap.scala:188)\n     at org.apache.spark.util.collection.ExternalAppendOnlyMap.insert(ExternalAppendOnlyMap.scala:141)\n     at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:59)\n     at org.apache.spark.rdd.PairRDDFunctions$$anonfun$1.apply(PairRDDFunctions.scala:96)\n     at org.apache.spark.rdd.PairRDDFunctions$$anonfun$1.apply(PairRDDFunctions.scala:95)\n     at org.apache.spark.rdd.RDD$$anonfun$3.apply(RDD.scala:471)\n     at org.apache.spark.rdd.RDD$$anonfun$3.apply(RDD.scala:471)\n     at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:34)\n     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)\n     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)\n     at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:161)\n     at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)\n     at org.apache.spark.scheduler.Task.run(Task.scala:53)\n     at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)\n     at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)\n     at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)\n     at java.security.AccessController.doPrivileged(Native Method)\n     at javax.security.auth.Subject.doAs(Subject.java:415)\n     at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n     at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)\n     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)\n{code}", "comments": [], "derived": {"summary": "Hitting an OOME in ExternalAppendOnlyMap. KCComparator while boxing an int.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Avoid boxing in ExternalAppendOnlyMap compares - Hitting an OOME in ExternalAppendOnlyMap. KCComparator while boxing an int."}]}}
{"project": "SPARK", "issue_id": "SPARK-1633", "title": "Various examples for Scala and Java custom receiver, etc. ", "status": "Resolved", "priority": "Major", "reporter": "Tathagata Das", "assignee": "Tathagata Das", "labels": [], "created": "2014-04-25T20:12:13.000+0000", "updated": "2014-05-15T20:26:53.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Various examples for Scala and Java custom receiver, etc."}]}}
{"project": "SPARK", "issue_id": "SPARK-1634", "title": "Java API docs contain test cases", "status": "Closed", "priority": "Blocker", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-04-25T20:48:28.000+0000", "updated": "2014-04-26T01:58:19.000+0000", "description": "The generated Java API docs contain all test cases.", "comments": ["Re-tried with 'sbt/sbt clean`. The generated docs for test cases were gone."], "derived": {"summary": "The generated Java API docs contain all test cases.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Java API docs contain test cases - The generated Java API docs contain all test cases."}, {"q": "What updates or decisions were made in the discussion?", "a": "Re-tried with 'sbt/sbt clean`. The generated docs for test cases were gone."}]}}
{"project": "SPARK", "issue_id": "SPARK-1635", "title": "Java API docs do not show annotation.", "status": "Closed", "priority": "Minor", "reporter": "Xiangrui Meng", "assignee": null, "labels": [], "created": "2014-04-25T20:51:06.000+0000", "updated": "2015-07-01T21:28:47.000+0000", "description": "The generated Java API docs do not contain Developer/Experimental annotations. The :: Developer/Experimental :: tag is in the generated doc.", "comments": ["Hi, I'd like to work on this."], "derived": {"summary": "The generated Java API docs do not contain Developer/Experimental annotations. The :: Developer/Experimental :: tag is in the generated doc.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Java API docs do not show annotation. - The generated Java API docs do not contain Developer/Experimental annotations. The :: Developer/Experimental :: tag is in the generated doc."}, {"q": "What updates or decisions were made in the discussion?", "a": "Hi, I'd like to work on this."}]}}
{"project": "SPARK", "issue_id": "SPARK-1636", "title": "Move main methods to examples", "status": "Resolved", "priority": "Major", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-04-25T21:11:55.000+0000", "updated": "2014-04-29T08:03:23.000+0000", "description": "Move the main methods to examples and make them compatible with spark-submit.", "comments": ["PR: https://github.com/apache/spark/pull/584"], "derived": {"summary": "Move the main methods to examples and make them compatible with spark-submit.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Move main methods to examples - Move the main methods to examples and make them compatible with spark-submit."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR: https://github.com/apache/spark/pull/584"}]}}
{"project": "SPARK", "issue_id": "SPARK-1637", "title": "Clean up examples for 1.0", "status": "Resolved", "priority": "Critical", "reporter": "Matei Alexandru Zaharia", "assignee": "Sandeep Singh", "labels": [], "created": "2014-04-25T21:12:07.000+0000", "updated": "2014-05-07T04:56:06.000+0000", "description": "- Move all of them into subpackages of org.apache.spark.examples (right now some are in org.apache.spark.streaming.examples, for instance, and others are in org.apache.spark.examples.mllib)\n- Move Python examples into examples/src/main/python\n- Update docs to reflect these changes\n- Clarify that the hand-written K-means and logistic regression examples are for demo purposes, but in reality you might want to use MLlib (we will add examples for these using MLlib too)", "comments": ["https://github.com/apache/spark/pull/571", "There was a follow on for this also:\nhttps://github.com/apache/spark/pull/673"], "derived": {"summary": "- Move all of them into subpackages of org. apache.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Clean up examples for 1.0 - - Move all of them into subpackages of org. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "There was a follow on for this also:\nhttps://github.com/apache/spark/pull/673"}]}}
{"project": "SPARK", "issue_id": "SPARK-1638", "title": "Executors fail to come up if \"spark.executor.extraJavaOptions\" is set ", "status": "Resolved", "priority": "Major", "reporter": "Kalpit Shah", "assignee": null, "labels": [], "created": "2014-04-25T21:18:24.000+0000", "updated": "2014-05-15T18:32:15.000+0000", "description": "If you try to launch a PySpark shell with \"spark.executor.extraJavaOptions\" set to \"-XX:+UseCompressedOops -XX:+UseCompressedStrings -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps\", the executors never come up on any of the workers.\n\nI see the following error in log file :\n\nSpark Executor Command: \"/usr/lib/jvm/java/bin/java\" \"-cp\" \"/root/c3/lib/*::/root/ephemeral-hdfs/conf:/root/spark/conf:/root/spark/assembly/target/scala-2.10/spark-assembly-1.0.0-SNAPSHOT-hadoop1.0.4.jar:\" \"-XX:+UseCompressedOops -XX:+UseCompressedStrings -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps\" \"-Xms13312M\" \"-Xmx13312M\" \"org.apache.spark.executor.CoarseGrainedExecutorBackend\" \"akka.tcp://spark@HOSTNAME:45429/user/CoarseGrainedScheduler\" \"7\" \"HOSTNAME\" \"4\" \"akka.tcp://sparkWorker@HOSTNAME:39727/user/Worker\" \"app-20140423224526-0000\"\n========================================\n\nUnrecognized VM option 'UseCompressedOops -XX:+UseCompressedStrings -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps'\nError: Could not create the Java Virtual Machine.\nError: A fatal exception has occurred. Program will exit.\n\n\n\n\n\n\n\n ", "comments": ["Almost certainly a duplicate of https://issues.apache.org/jira/browse/SPARK-1609", "Yeah, its very likely. I am going to pull the latest master and retest the fix for SPARK-1609 today. Will close this ticket after validation."], "derived": {"summary": "If you try to launch a PySpark shell with \"spark. executor.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Executors fail to come up if \"spark.executor.extraJavaOptions\" is set  - If you try to launch a PySpark shell with \"spark. executor."}, {"q": "What updates or decisions were made in the discussion?", "a": "Yeah, its very likely. I am going to pull the latest master and retest the fix for SPARK-1609 today. Will close this ticket after validation."}]}}
{"project": "SPARK", "issue_id": "SPARK-1639", "title": "Some tidying of Spark on YARN code", "status": "Resolved", "priority": "Major", "reporter": "Sandy Ryza", "assignee": "Sandy Ryza", "labels": [], "created": "2014-04-25T21:38:03.000+0000", "updated": "2014-06-11T13:00:29.000+0000", "description": "I found a few places where we can consolidate duplicate methods, fix typos, add comments, and make what's going on more clear.", "comments": ["https://github.com/apache/spark/pull/561"], "derived": {"summary": "I found a few places where we can consolidate duplicate methods, fix typos, add comments, and make what's going on more clear.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Some tidying of Spark on YARN code - I found a few places where we can consolidate duplicate methods, fix typos, add comments, and make what's going on more clear."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/561"}]}}
{"project": "SPARK", "issue_id": "SPARK-1640", "title": "In yarn-client mode, pass preferred node locations to AM", "status": "Resolved", "priority": "Major", "reporter": "Sandy Ryza", "assignee": null, "labels": [], "created": "2014-04-25T21:54:54.000+0000", "updated": "2015-07-20T19:06:00.000+0000", "description": "In yarn-cluster mode, if the user passes preferred node location data to the SparkContext, the AM requests containers based on that data.\n\nIn yarn-client mode, it would be good to do this as well.  This required some way of passing this data from the client process to the AM.", "comments": ["[~sandyr] is this still relevant given more recent work on YARN data locality?"], "derived": {"summary": "In yarn-cluster mode, if the user passes preferred node location data to the SparkContext, the AM requests containers based on that data. In yarn-client mode, it would be good to do this as well.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "In yarn-client mode, pass preferred node locations to AM - In yarn-cluster mode, if the user passes preferred node location data to the SparkContext, the AM requests containers based on that data. In yarn-client mode, it would be good to do this as well."}, {"q": "What updates or decisions were made in the discussion?", "a": "[~sandyr] is this still relevant given more recent work on YARN data locality?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1641", "title": "Spark submit warning tells the user to use spark-submit", "status": "Closed", "priority": "Minor", "reporter": "Andrew Or", "assignee": null, "labels": [], "created": "2014-04-25T22:01:11.000+0000", "updated": "2014-11-05T10:43:35.000+0000", "description": "$ bin/spark-submit ...\n\nSpark assembly has been built with Hive, including Datanucleus jars on classpath\nWARNING: This client is deprecated and will be removed in a future version of Spark.\nUse ./bin/spark-submit with \"--master yarn\"\n\nThis is printed in org.apache.spark.deploy.yarn.Client.", "comments": ["Andrew, I think this is a duplicate of SPARK-1534. \n\nI haven't gotten around to work on it. Feel free to take it. ", "Ah yes I didn't notice the other one. Me neither, so feel free to take it (or not)", "Duplicates SPARK-1534"], "derived": {"summary": "$ bin/spark-submit. Spark assembly has been built with Hive, including Datanucleus jars on classpath\nWARNING: This client is deprecated and will be removed in a future version of Spark.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Spark submit warning tells the user to use spark-submit - $ bin/spark-submit. Spark assembly has been built with Hive, including Datanucleus jars on classpath\nWARNING: This client is deprecated and will be removed in a future version of Spark."}, {"q": "What updates or decisions were made in the discussion?", "a": "Duplicates SPARK-1534"}]}}
{"project": "SPARK", "issue_id": "SPARK-1642", "title": "Upgrade FlumeInputDStream's FlumeReceiver to support FLUME-2083", "status": "Resolved", "priority": "Minor", "reporter": "Theodore michael Malaska", "assignee": "Theodore michael Malaska", "labels": ["bulk-closed"], "created": "2014-04-26T13:33:07.000+0000", "updated": "2019-06-06T13:57:59.000+0000", "description": "This will add support for SSL encryption between Flume AvroSink and Spark Streaming.\n\nIt is based on FLUME-2083", "comments": ["This is an easy change once a1478 is committed", "For future convenience, can you just add a note on what FLUME-2083 is all about? So that FLUME-noobs like me can figure it out more easily :)", "Will do.  :)", "OK I'm back.  I will start working on this Jira this week.", "Ted, is this ready, now that 1478 is getting in (as soon as Jenkins stop misbehaving)?", "1478 is merged. Do you have this ready?", "Hey Tdas.  Yes now that SPARK-1478 is done I will do this today.  I will have it soon.", "OK I got it working and all the test passed.  I'll clean it up and do a pull request tomorrow.\n\nThanks again for your help", "OK I finished testing and cleaning up.  Its ready for a review.\n\nI even ordered all the imports by ABC order.\n\nHere is the link\nhttps://github.com/apache/spark/pull/1386\n\nGoing to start 2447 now.\n\nThanks again.", "Are there any changes needed here?", "Since the PR of this issue has been closed for a while, and there seems to be not further work in this, I am going to close this JIRA. [~ted.m] If you are interested in updating the PR, feel free to reopen the JIRA.", "42 bulk-closed issues were still \"In Progress\" when they should be resolved."], "derived": {"summary": "This will add support for SSL encryption between Flume AvroSink and Spark Streaming. It is based on FLUME-2083.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Upgrade FlumeInputDStream's FlumeReceiver to support FLUME-2083 - This will add support for SSL encryption between Flume AvroSink and Spark Streaming. It is based on FLUME-2083."}, {"q": "What updates or decisions were made in the discussion?", "a": "42 bulk-closed issues were still \"In Progress\" when they should be resolved."}]}}
{"project": "SPARK", "issue_id": "SPARK-1643", "title": "hql(\"show tables\") throw an exception", "status": "Closed", "priority": "Major", "reporter": "Guoqiang Li", "assignee": null, "labels": [], "created": "2014-04-26T16:08:05.000+0000", "updated": "2014-04-29T08:10:16.000+0000", "description": "cat conf/hive-site.xml\n{code}\n<configuration>\n  <property>\n    <name>javax.jdo.option.ConnectionURL</name>\n    <value>jdbc:postgresql://bj-java-hugedata1:7432/hive</value>\n  </property>\n  <property>\n    <name>javax.jdo.option.ConnectionDriverName</name>\n    <value>org.postgresql.Driver</value>\n  </property>\n  <property>\n    <name>javax.jdo.option.ConnectionUserName</name>\n    <value>hive</value>\n  </property>\n  <property>\n    <name>javax.jdo.option.ConnectionPassword</name>\n    <value>passwd</value>\n  </property>\n  <property>\n    <name>hive.metastore.local</name>\n    <value>false</value>\n  </property>\n  <property>\n    <name>hive.metastore.warehouse.dir</name>\n    <value>hdfs://host:8020/user/hive/warehouse</value>\n  </property>\n</configuration>\n{code}", "comments": ["This is incomplete, there is not mention of an exception. Please attach one and re-open it, Thanks!"], "derived": {"summary": "cat conf/hive-site. xml\n{code}\n<configuration>\n  <property>\n    <name>javax.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "hql(\"show tables\") throw an exception - cat conf/hive-site. xml\n{code}\n<configuration>\n  <property>\n    <name>javax."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is incomplete, there is not mention of an exception. Please attach one and re-open it, Thanks!"}]}}
{"project": "SPARK", "issue_id": "SPARK-1644", "title": "The org.datanucleus:*  should not be packaged into spark-assembly-*.jar", "status": "Resolved", "priority": "Major", "reporter": "Guoqiang Li", "assignee": "Guoqiang Li", "labels": [], "created": "2014-04-26T16:10:27.000+0000", "updated": "2014-05-10T17:15:27.000+0000", "description": "cat conf/hive-site.xml\n{code:xml}\n<configuration>\n  <property>\n    <name>javax.jdo.option.ConnectionURL</name>\n    <value>jdbc:postgresql://bj-java-hugedata1:7432/hive</value>\n  </property>\n  <property>\n    <name>javax.jdo.option.ConnectionDriverName</name>\n    <value>org.postgresql.Driver</value>\n  </property>\n  <property>\n    <name>javax.jdo.option.ConnectionUserName</name>\n    <value>hive</value>\n  </property>\n  <property>\n    <name>javax.jdo.option.ConnectionPassword</name>\n    <value>passwd</value>\n  </property>\n  <property>\n    <name>hive.metastore.local</name>\n    <value>false</value>\n  </property>\n  <property>\n    <name>hive.metastore.warehouse.dir</name>\n    <value>hdfs://host:8020/user/hive/warehouse</value>\n  </property>\n</configuration>\n{code}", "comments": ["The org.datanucleus:*  can't be packaged into spark-assembly-*.jar \n{code}\nCaused by: org.datanucleus.exceptions.NucleusUserException: Persistence process has been specified to use a ClassLoaderResolver of name \"datanucleus\" yet this has not been found by the DataNucleus plugin mechanism. Please check your CLASSPATH and plugin specification.\n{code}\n\nThe plugin.xml,MANIFEST.MF is damaged", "{code}\n# When Hive support is needed, Datanucleus jars must be included on the classpath.\n# Datanucleus jars do not work if only included in the  uber jar as plugin.xml metadata is lost.\n# Both sbt and maven will populate \"lib_managed/jars/\" with the datanucleus jars when Spark is\n# built with Hive, so first check if the datanucleus jars exist, and then ensure the current Spark\n# assembly is built for Hive, before actually populating the CLASSPATH with the jars.\n# Note that this check order is faster (by up to half a second) in the case where Hive is not used.\nnum_datanucleus_jars=$(ls \"$FWDIR\"/lib_managed/jars/ 2>/dev/null | grep \"datanucleus-.*\\\\.jar\" | wc -l)\nif [ $num_datanucleus_jars -gt 0 ]; then\n  AN_ASSEMBLY_JAR=${ASSEMBLY_JAR:-$DEPS_ASSEMBLY_JAR}\n  num_hive_files=$(jar tvf \"$AN_ASSEMBLY_JAR\" org/apache/hadoop/hive/ql/exec 2>/dev/null | wc -l)\n  if [ $num_hive_files -gt 0 ]; then\n    echo \"Spark assembly has been built with Hive, including Datanucleus jars on classpath\" 1>&2\n    DATANUCLEUSJARS=$(echo \"$FWDIR/lib_managed/jars\"/datanucleus-*.jar | tr \" \" :)\n    CLASSPATH=$CLASSPATH:$DATANUCLEUSJARS\n  fi\nfi\n{code} \nonly add /lib_managed/jars of files to the CLASSPATH. In the current directory is dist is unable to work\n \n", "Why {{assignee to me}} button can not find? Adjust the permissions? :)", "Issue resolved by pull request 688\n[https://github.com/apache/spark/pull/688]"], "derived": {"summary": "cat conf/hive-site. xml\n{code:xml}\n<configuration>\n  <property>\n    <name>javax.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "The org.datanucleus:*  should not be packaged into spark-assembly-*.jar - cat conf/hive-site. xml\n{code:xml}\n<configuration>\n  <property>\n    <name>javax."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 688\n[https://github.com/apache/spark/pull/688]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1645", "title": "Improve Spark Streaming compatibility with Flume", "status": "Resolved", "priority": "Major", "reporter": "Hari Shreedharan", "assignee": "Tathagata Das", "labels": [], "created": "2014-04-26T19:22:51.000+0000", "updated": "2014-08-01T20:47:26.000+0000", "description": "Currently the following issues affect Spark Streaming and Flume compatibilty:\n\n* If a spark worker goes down, it needs to be restarted on the same node, else Flume cannot send data to it. We can fix this by adding a Flume receiver that is polls Flume, and a Flume sink that supports this.\n\n* Receiver sends acks to Flume before the driver knows about the data. The new receiver should also handle this case.\n\n* Data loss when driver goes down - This is true for any streaming ingest, not just Flume. I will file a separate jira for this and we should work on it there. This is a longer term project and requires considerable development work.\n\n\nI intend to start working on these soon. Any input is appreciated. (It'd be great if someone can add me as a contributor on jira, so I can assign the jira to myself).", "comments": ["Hey [~hshreedharan] Thanks for jotting these down. Once Spark 1.0 is out of the picture I will start working up a design document for this. The more I think, all these issues (reliable storing of data, driver failure recovery using tachyon, etc. ) are quite intricately linked with each other and we need to do a proper design of the solution before jump into this. Well, for Spark Streaming, this is the highest priority issue, so we will start working on this soon after Spark 1.0\n\nAlso, this is a massive task involving many subtask. I may actually break up this JIRA into multiple sub-JIRAs, some of which we can start working on independently. ", "Yep, that is correct. I'd like to contribute to the design as much as possible - so perhaps we can work on the design document together. Once we start looking into this, we can definitely have to proceed on multiple fronts so we can get more of these features committed faster.", "Yes, we will keep you posted. \n\nThough one thing that is reasonably independent is to add the ability for Flume receivers to be launched on multiple workers, such that one can act ask standby, when the primary receiver fails. ", "Yes, so I have a rough design for that in mind. The idea is to add a sink which plugs into Flume, which gets polled by the Spark receiver. That way, even if the node on which the worker is running fails, the receiver on another node can poll the sink and pull data. From the Flume point of view, the sink does not \"conform\" to the definition of standard sinks (all Flume sinks are push only), but it can be written such that we don't lose data. Later if/when Flume adds support for pollable sinks this sink can be ported.", "Let me understand this. Is this sink going to run as a separate process outside the Spark executor? If it is running as a thread in the same executor process as the receiver, then that is no better than what it is now, as it will fail when the executor fails. So I am guessing it will be a process outside the executor. Doesnt introduce the headache of managing that process separately? And what happens when the whole worker node dies? ", "No, the sink would run inside the Flume agent that Spark is receiving data from. (Sink is a flume component that pushes data out - this is managed by Flume). Basically, this sink pulls data from the Flume agent's buffer when Spark receiver polls it. If the receiver dies and restarts, as long as the receiver knows which agent to poll the receiver will be able to get the data. This solves the case where Flume is pushing data to a receiver which may have died and restarted elsewhere - since Spark now polls Flume", "But this does not solve the scenario where the whole worker running the receiver dies. If worker dies, then receiver and sink all of it is gone, and Flume source has nowhere to send the data to, isnt it?\n\nAs far as I understand, the only way to deal with a worker failure is to configure a pool of workers as sinks. If the one of the sinks dont work because the worker failed, the second sink on a the second worker can still receive data. Am I missing something?", "No, Flume source and sink reside within the same JVM(http://flume.apache.org/FlumeUserGuide.html#architecture). So the receiver polls the Flume sink running on a different node (the node that runs the Flume agent pushing the data). If the node running the receiver goes down, then another worker starts up and reads from the same Flume agent. If the Flume agent goes down the receiver polls and fails to get data until the agent is back up. ", "Ah, I think get it now. So instead of the default push-based as it is now (where a sink is running with the receiver), you simply want to make pull-based. \n\nSo if the current situation is this \n\n!http://i.imgur.com/m8oiOwl.png?1!  \n\nyou propose this  \n\n!http://i.imgur.com/N6Ee1cb.png?1!\n\nRight?\nAssuming it is right, that does make it very convenient for Spark Streaming's receivers. However what does it mean for reliable receiving? When the receiver pulls the data from the source, it will acknowledge the source only when the Spark acknowledges that it has reliably saved the data?\n", "The first one is not exactly accurate, though it explains the idea. The second is what I suggest.\n\nAs a first step, we do what is currently done - the receiver stores the data locally and acknowledges, so the reliability does not improve. Later we can make the improvement for all receivers that the data becomes persisted all the way to the driver (add a new API like storeReliably or something). \n\nWe would have to do a two step Poll-ACK process. We can have the initial poll create a new request added to the ones that are pending for commit in the sink. Once the receiver has written it (for now in the current way, later reliably) - it sends out an ACK for the request id, that causes the request to be committed, so Flume can remove the events. If the receiver does not send the ACK, then the sink can have a scheduled thread come (this timeout can be specified in the flume config) and rollback and make the data available again (Flume already has the capability to make uncommitted txns to be made available if that agent fails).", "This makes sense from the integration point of view. Though I wonder from thePOV of Flume's deployment configuration does it make things more complex? Like for example, if someone has a the flume system already setup, in the current situation, the configuration change to add a new sink seems standard and easy to understand. However, in the proposed model, since Flume's data pushing node has to run a sink, how much complicated does this configuration step become?", "All flume agents usually have a sink deployed, since that is what pushes the data out to the next flume agent. So configuration would not be difficult on the Flume side. The spark side though would need to know which flume agent(s) to pull the data from. The nice part about doing this is that each spark receiver can potentially pull data from several flume agents - not just one.", "Thats good!\n\nI was thinking from the API POV as well. Would the Spark Streaming's flumeStream API need to change? Currently the API requires a hostname as the host to which flume would sent that data to (i.e where the receiver would run). The new model still requires a hostname, but those are nodes not ones on which the receivers will be scheduled. So the same API would work, but given the semantic difference (different model requiring different configurations), its probably best to have a different set of methods that are documented to use the new API. What do you think?\n", "Yes, it is better to add new methods rather than reusing the old ones and confusing existing users.\n\nIn fact, I think we should add a new receiver for the time being and only deprecate the old one initially. We can remove the old one in a later release.", "I agree. New receiver for the new API is the best way. ", "I made two sub-JIRAs to track this. "], "derived": {"summary": "Currently the following issues affect Spark Streaming and Flume compatibilty:\n\n* If a spark worker goes down, it needs to be restarted on the same node, else Flume cannot send data to it. We can fix this by adding a Flume receiver that is polls Flume, and a Flume sink that supports this.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Improve Spark Streaming compatibility with Flume - Currently the following issues affect Spark Streaming and Flume compatibilty:\n\n* If a spark worker goes down, it needs to be restarted on the same node, else Flume cannot send data to it. We can fix this by adding a Flume receiver that is polls Flume, and a Flume sink that supports this."}, {"q": "What updates or decisions were made in the discussion?", "a": "I made two sub-JIRAs to track this."}]}}
{"project": "SPARK", "issue_id": "SPARK-1646", "title": "ALS micro-optimisation", "status": "Resolved", "priority": "Trivial", "reporter": "Tor Myklebust", "assignee": "Tor Myklebust", "labels": [], "created": "2014-04-26T19:29:23.000+0000", "updated": "2014-05-14T22:07:55.000+0000", "description": "Scala \"for\" loop bodies turn into methods and the loops themselves into repeated invocations of the body method.  This may make Hotspot make poor optimisation decisions.  (Xiangrui mentioned that there was a speed improvement from doing similar transformations elsewhere.)\n\nThe loops on i and p in the ALS training code are prime candidates for this transformation, as is the \"foreach\" loop doing regularisation.", "comments": ["PR: https://github.com/apache/spark/pull/568"], "derived": {"summary": "Scala \"for\" loop bodies turn into methods and the loops themselves into repeated invocations of the body method. This may make Hotspot make poor optimisation decisions.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "ALS micro-optimisation - Scala \"for\" loop bodies turn into methods and the loops themselves into repeated invocations of the body method. This may make Hotspot make poor optimisation decisions."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR: https://github.com/apache/spark/pull/568"}]}}
{"project": "SPARK", "issue_id": "SPARK-1647", "title": "Prevent data loss when Streaming driver goes down", "status": "Closed", "priority": "Major", "reporter": "Hari Shreedharan", "assignee": "Hari Shreedharan", "labels": [], "created": "2014-04-27T01:16:31.000+0000", "updated": "2015-03-05T20:55:29.000+0000", "description": "Currently when the driver goes down, any uncheckpointed data is lost from within spark. If the system from which messages are pulled can  replay messages, the data may be available - but for some systems, like Flume this is not the case. \n\nAlso, all windowing information is lost for windowing functions. \n\nWe must persist raw data somehow, and be able to replay this data if required. We also must persist windowing information with the data itself.\n\nThis will likely require quite a bit of work to complete and probably will have to be split into several sub-jiras.", "comments": ["Is there a a particular design in mind for this or is this just being submitted as a feature request?", "I plan to work on this one, but I won't get to it until SPARK-1645.", "Ah okay - I wasn't asking about timing per-se, just wondering if there was an associated design being proposed, because this will require some thought from the design perspective before getting started. It sounds like that will wait until SPARK-1645.", "Yep, this will require quite a bit of design work first. I will get back to this one in some time.", "Marking this as a duplicate issue with https://issues.apache.org/jira/browse/SPARK-3129 . This is a older JIRA, but still marking this as duplicate to preserve the discussion in the new JIRA."], "derived": {"summary": "Currently when the driver goes down, any uncheckpointed data is lost from within spark. If the system from which messages are pulled can  replay messages, the data may be available - but for some systems, like Flume this is not the case.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Prevent data loss when Streaming driver goes down - Currently when the driver goes down, any uncheckpointed data is lost from within spark. If the system from which messages are pulled can  replay messages, the data may be available - but for some systems, like Flume this is not the case."}, {"q": "What updates or decisions were made in the discussion?", "a": "Marking this as a duplicate issue with https://issues.apache.org/jira/browse/SPARK-3129 . This is a older JIRA, but still marking this as duplicate to preserve the discussion in the new JIRA."}]}}
{"project": "SPARK", "issue_id": "SPARK-1648", "title": "Support closing JIRA's as part of merge script", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "labels": [], "created": "2014-04-27T05:52:23.000+0000", "updated": "2014-04-27T22:43:09.000+0000", "description": "We should have an automated hook in the merge script that can close the JIRA, set the fix versions, and leave a comment on the JIRA indicating the PR in which it was resolved.\n\nThis would both (a) make sure we always close JIRA's when issues are merged and (b) add a link to the pull request in every JIRA.", "comments": ["Issue resolved by pull request 570\n[https://github.com/apache/spark/pull/570]"], "derived": {"summary": "We should have an automated hook in the merge script that can close the JIRA, set the fix versions, and leave a comment on the JIRA indicating the PR in which it was resolved. This would both (a) make sure we always close JIRA's when issues are merged and (b) add a link to the pull request in every JIRA.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support closing JIRA's as part of merge script - We should have an automated hook in the merge script that can close the JIRA, set the fix versions, and leave a comment on the JIRA indicating the PR in which it was resolved. This would both (a) make sure we always close JIRA's when issues are merged and (b) add a link to the pull request in every JIRA."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 570\n[https://github.com/apache/spark/pull/570]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1649", "title": "Figure out Nullability semantics for Array elements and Map values", "status": "Resolved", "priority": "Critical", "reporter": "Andre Schumacher", "assignee": "Yin Huai", "labels": [], "created": "2014-04-27T08:40:05.000+0000", "updated": "2015-07-20T01:51:00.000+0000", "description": "For the underlying storage layer it would simplify things such as schema conversions, predicate filter determination and such to record in the data type itself whether a column can be nullable. So the DataType type could look like like this:\n\nabstract class DataType(nullable: Boolean = true)\n\nConcrete subclasses could then override the nullable val. Mostly this could be left as the default but when types can be contained in nested types one could optimize for, e.g., arrays with elements that are nullable and those that are not.", "comments": ["Why do you think it would be better to have the nullability bit in the data type?  Both attribute references and struct fields already have a nullable bit, so we can always describe whether or not a given attribute can be null or not.\n\nRight now we use primitive datatypes mostly as enums, so adding this bit to them would mean that everywhere we pattern match on datatype we would need to include a wildcard for nullability.  This would also require a pretty big change to all expressions since right now we determine nullability propagation independent of datatype.", "OK, I now understand that this would be a bigger change.\n\nIt's not just struct fields for nested types, array element types, map value value types, etc. IMHO it would be cleaner to have it inside the DataType. But since this seems to be mostly relevant only for nested types could one have a special DataType for them, something like \"NestedDataType(val nullable: Boolean) extends DataType?", "Thinking about it a bit longer.. could Nullable maybe be a mixin? But what should the default be, nullable or not nullable?", "Oh, I see.  I forgot that we would also need this inside of ArrayType.  Also, for MapType it seems like it only matters for the value, not the key as I'm not sure we would allow null keys.\n\nThis is something we need to consider. However, I think I'm going to change the title to something less prescriptive.  Could we just for now say that null values are not supported in arrays of parquet files?", "Sorry for the delay. OK, not allowing null values inside Parquet arrays would seem the cleanest solution for now until this issue has been thought through.", "My PR for SPARK-2179 (https://github.com/apache/spark/pull/1346) introduces the \"containsNull\" field to the ArrayType. For Parquet, we still do not support null values insides a Parquet array.\n\nFor the key and value of MapType, [~marmbrus] and I discussed about it. We think it is not semantically clear what a null means when it appears in the key or value field (considering a null is used to indicate a missing data value). So, we decided that key and value in a MapType should not contain any null value and we will not introduce containsNull to MapType", "Seems Hive supports null values in a Map, to be consistent with Hive, we will also support that. I will introduce a boolean \"valueContainsNull\" to MapType. For null map keys, Hive has inconsistent behaviors. Here are examples (using \"sbt/sbt hive/console\"). \n{code}\nrunSqlHive(\"select map(null, 1, null, 2, null, 3, 4, null, 5, null) from src limit 1\")\nres6: Seq[String] = Buffer({4:null,5:null})\nrunSqlHive(\"select map_keys(map(null, 1, null, 2, null, 3, 4, null, 5, null)) from src limit 1\")\nres7: Seq[String] = Buffer([null,4,5])\nrunSqlHive(\"select map_values(map(null, 1, null, 2, null, 3, 4, null, 5, null)) from src limit 1\")\nres8: Seq[String] = Buffer([3,null,null])\n{code}\nAlso, different implementations handle null keys in different ways (e.g. HashMap supports an entry with a null key. But, TreeMap will throw a NPE when a user want to insert an entry with a null key). So, I think we will not allow null keys in a map.", "Thrift also supports null values in a map and this makes any thrift generated parquet files that contain a map unreadable by spark sql due to the following code in parquet-thrift for generating the schema for maps:\n\n{code:title=parquet.thrift.ThriftSchemaConverter.java|borderStyle=solid}\n  @Override\n  public void visit(ThriftType.MapType mapType) {\n    final ThriftField mapKeyField = mapType.getKey();\n    final ThriftField mapValueField = mapType.getValue();\n\n    //save env for map\n    String mapName = currentName;\n    Type.Repetition mapRepetition = currentRepetition;\n\n    //=========handle key\n    currentFieldPath.push(mapKeyField);\n    currentName = \"key\";\n    currentRepetition = REQUIRED;\n    mapKeyField.getType().accept(this);\n    Type keyType = currentType;//currentType is the already converted type\n    currentFieldPath.pop();\n\n    //=========handle value\n    currentFieldPath.push(mapValueField);\n    currentName = \"value\";\n    currentRepetition = OPTIONAL;\n    mapValueField.getType().accept(this);\n    Type valueType = currentType;\n    currentFieldPath.pop();\n\n    if (keyType == null && valueType == null) {\n      currentType = null;\n      return;\n    }\n\n    if (keyType == null && valueType != null)\n      throw new ThriftProjectionException(\"key of map is not specified in projection: \" + currentFieldPath);\n\n    //restore Env\n    currentName = mapName;\n    currentRepetition = mapRepetition;\n    currentType = ConversionPatterns.mapType(currentRepetition, currentName,\n            keyType,\n            valueType);\n  }\n{code}\n\nWhich causes an error on the spark side when we reach this step in the toDataType function that asserts that both the key and value are of repetition level REQUIRED:\n\n{code:title=org.apache.spark.sql.parquet.ParquetTypes.scala|borderStyle=solid}\n        case ParquetOriginalType.MAP => {\n          assert(\n            !groupType.getFields.apply(0).isPrimitive,\n            \"Parquet Map type malformatted: expected nested group for map!\")\n          val keyValueGroup = groupType.getFields.apply(0).asGroupType()\n          assert(\n            keyValueGroup.getFieldCount == 2,\n            \"Parquet Map type malformatted: nested group should have 2 (key, value) fields!\")\n          val keyType = toDataType(keyValueGroup.getFields.apply(0))\n          println(\"here\")\n          assert(keyValueGroup.getFields.apply(0).getRepetition == Repetition.REQUIRED)\n          val valueType = toDataType(keyValueGroup.getFields.apply(1))\n          assert(keyValueGroup.getFields.apply(1).getRepetition == Repetition.REQUIRED)\n          new MapType(keyType, valueType)\n        }\n{code}\n\nCurrently I have modified parquet-thrift to use repetition REQUIRED just to make spark sql able to work on the parquet files since we don't actually use null values in our maps. However it would be preferred to use parquet-thrift and spark sql out of the box and have them work nicely together with our existing thrift data types without having to modify dependencies.", "[~rrusso2007] Can you open a JIRA for the issue of reading Parquet datasets?", "Just opened https://issues.apache.org/jira/browse/SPARK-2721", "[~yhuai] / [~liancheng]  does this ticket still apply? If not, please close it. Thanks.\n", "We have {{containsNull}} in {{ArrayType}} and {{valueContainsNull}} in {{MapType}}. I am resolving it as \"Done\"."], "derived": {"summary": "For the underlying storage layer it would simplify things such as schema conversions, predicate filter determination and such to record in the data type itself whether a column can be nullable. So the DataType type could look like like this:\n\nabstract class DataType(nullable: Boolean = true)\n\nConcrete subclasses could then override the nullable val.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "Figure out Nullability semantics for Array elements and Map values - For the underlying storage layer it would simplify things such as schema conversions, predicate filter determination and such to record in the data type itself whether a column can be nullable. So the DataType type could look like like this:\n\nabstract class DataType(nullable: Boolean = true)\n\nConcrete subclasses could then override the nullable val."}, {"q": "What updates or decisions were made in the discussion?", "a": "We have {{containsNull}} in {{ArrayType}} and {{valueContainsNull}} in {{MapType}}. I am resolving it as \"Done\"."}]}}
{"project": "SPARK", "issue_id": "SPARK-1650", "title": "Correctly identify maven project version in distribution script", "status": "Resolved", "priority": "Major", "reporter": "Rahul Singhal", "assignee": "Rahul Singhal", "labels": [], "created": "2014-04-27T20:05:56.000+0000", "updated": "2014-04-27T22:20:04.000+0000", "description": "make-distribution.sh uses \"mvn help:evaluate -Dexpression=project.version\" to extract the project version but this command also has a side-effect of downloading any dependencies from maven servers. So sometimes wrong version string is used.", "comments": ["PR: https://github.com/apache/spark/pull/572"], "derived": {"summary": "make-distribution. sh uses \"mvn help:evaluate -Dexpression=project.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Correctly identify maven project version in distribution script - make-distribution. sh uses \"mvn help:evaluate -Dexpression=project."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR: https://github.com/apache/spark/pull/572"}]}}
{"project": "SPARK", "issue_id": "SPARK-1651", "title": "Delete existing (tar) deployment directory in distribution script", "status": "Resolved", "priority": "Minor", "reporter": "Rahul Singhal", "assignee": "Rahul Singhal", "labels": [], "created": "2014-04-27T20:08:56.000+0000", "updated": "2014-04-27T22:51:33.000+0000", "description": null, "comments": ["PR: https://github.com/apache/spark/pull/573", "Issue resolved by pull request 573\n[https://github.com/apache/spark/pull/573]"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Delete existing (tar) deployment directory in distribution script"}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 573\n[https://github.com/apache/spark/pull/573]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1652", "title": "Fixes and improvements for spark-submit/configs", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "labels": [], "created": "2014-04-28T02:31:01.000+0000", "updated": "2014-12-27T07:40:19.000+0000", "description": "These are almost all a result of my config patch. Unfortunately the changes were difficult to unit-test and there several edge cases reported.", "comments": ["I added the spark.executor.extraLibraryPath here but I don't necessarily consider it a blocker since there is a workaround of using SPARK_YARN_USER_ENV and setting LD_LIBRARY_PATH", "The remaining issues here all have work-arounds in 1.0. So I'm bumping this to 1.1", "[~pwendell], is there currently a work-around for running Python driver programs on the cluster?\n\nTrying this in 1.0.0 currently yields a succinct: \n{code}\nError: Cannot currently run Python driver programs on cluster\n{code}\n\nI'm not sure if there is a separate issue to track this, or if this is the issue I should watch."], "derived": {"summary": "These are almost all a result of my config patch. Unfortunately the changes were difficult to unit-test and there several edge cases reported.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Fixes and improvements for spark-submit/configs - These are almost all a result of my config patch. Unfortunately the changes were difficult to unit-test and there several edge cases reported."}, {"q": "What updates or decisions were made in the discussion?", "a": "[~pwendell], is there currently a work-around for running Python driver programs on the cluster?\n\nTrying this in 1.0.0 currently yields a succinct: \n{code}\nError: Cannot currently run Python driver programs on cluster\n{code}\n\nI'm not sure if there is a separate issue to track this, or if this is the issue I should watch."}]}}
{"project": "SPARK", "issue_id": "SPARK-1653", "title": "Spark submit should not use SPARK_CLASSPATH internally (creates warnings)", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "labels": [], "created": "2014-04-28T02:32:40.000+0000", "updated": "2014-04-29T00:31:06.000+0000", "description": "Right now it's confusing because it warns and tells you to use spark-submit, but then spark-submit sets SPARK_CLASSPATH thus causing a warning.", "comments": [], "derived": {"summary": "Right now it's confusing because it warns and tells you to use spark-submit, but then spark-submit sets SPARK_CLASSPATH thus causing a warning.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Spark submit should not use SPARK_CLASSPATH internally (creates warnings) - Right now it's confusing because it warns and tells you to use spark-submit, but then spark-submit sets SPARK_CLASSPATH thus causing a warning."}]}}
{"project": "SPARK", "issue_id": "SPARK-1654", "title": "Spark shell doesn't correctly pass quoted arguments to spark-submit", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "labels": [], "created": "2014-04-28T02:34:18.000+0000", "updated": "2014-04-29T00:30:33.000+0000", "description": "For instance:\n\n{code}\n./bin/spark-shell --driver-java-options \"-Dfoo=a -Dbar=b\"\n{code}\n\nDoesn't work correctly.", "comments": ["Issue resolved by pull request 576\n[https://github.com/apache/spark/pull/576]"], "derived": {"summary": "For instance:\n\n{code}. /bin/spark-shell --driver-java-options \"-Dfoo=a -Dbar=b\"\n{code}\n\nDoesn't work correctly.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Spark shell doesn't correctly pass quoted arguments to spark-submit - For instance:\n\n{code}. /bin/spark-shell --driver-java-options \"-Dfoo=a -Dbar=b\"\n{code}\n\nDoesn't work correctly."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 576\n[https://github.com/apache/spark/pull/576]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1655", "title": "In naive Bayes, store conditional probabilities distributively.", "status": "Resolved", "priority": "Major", "reporter": "Xiangrui Meng", "assignee": "Aaron Staple", "labels": ["bulk-closed"], "created": "2014-04-28T02:54:29.000+0000", "updated": "2019-06-06T13:57:41.000+0000", "description": "In the current implementation, we collect all conditional probabilities to the driver node. When there are many labels and many features, this puts heavy load on the driver. For scalability, we should provide a way to store conditional probabilities distributively.", "comments": ["User 'staple' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/2491", "Mind if I change the target version to 1.6?", "Totally fine with me, but I'm not sure who the main decision maker on this is.", "42 bulk-closed issues were still \"In Progress\" when they should be resolved."], "derived": {"summary": "In the current implementation, we collect all conditional probabilities to the driver node. When there are many labels and many features, this puts heavy load on the driver.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "In naive Bayes, store conditional probabilities distributively. - In the current implementation, we collect all conditional probabilities to the driver node. When there are many labels and many features, this puts heavy load on the driver."}, {"q": "What updates or decisions were made in the discussion?", "a": "42 bulk-closed issues were still \"In Progress\" when they should be resolved."}]}}
{"project": "SPARK", "issue_id": "SPARK-1656", "title": "Potential resource leak in HttpBroadcast, SparkSubmitArguments, FileSystemPersistenceEngine and DiskStore", "status": "Resolved", "priority": "Major", "reporter": "Shixiong Zhu", "assignee": "Shixiong Zhu", "labels": ["easyfix"], "created": "2014-04-28T04:02:16.000+0000", "updated": "2014-10-08T02:38:53.000+0000", "description": "Again... I'm trying to review all `close` statements to find such issues.", "comments": ["PR: https://github.com/apache/spark/pull/577", "Already merged into master and branch-1.1"], "derived": {"summary": "Again. I'm trying to review all `close` statements to find such issues.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Potential resource leak in HttpBroadcast, SparkSubmitArguments, FileSystemPersistenceEngine and DiskStore - Again. I'm trying to review all `close` statements to find such issues."}, {"q": "What updates or decisions were made in the discussion?", "a": "Already merged into master and branch-1.1"}]}}
{"project": "SPARK", "issue_id": "SPARK-1657", "title": "Spark submit should fail gracefully if YARN support not enabled", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "labels": [], "created": "2014-04-28T05:23:31.000+0000", "updated": "2014-04-29T00:28:27.000+0000", "description": "Currently it throws a ClassNotFoundException when trying to reflectively load the class. We should check if the yarn Client class is loadable and throw a nicer exception if it's not found.", "comments": [], "derived": {"summary": "Currently it throws a ClassNotFoundException when trying to reflectively load the class. We should check if the yarn Client class is loadable and throw a nicer exception if it's not found.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Spark submit should fail gracefully if YARN support not enabled - Currently it throws a ClassNotFoundException when trying to reflectively load the class. We should check if the yarn Client class is loadable and throw a nicer exception if it's not found."}]}}
{"project": "SPARK", "issue_id": "SPARK-1658", "title": "Correctly identify if maven is installed and working", "status": "Resolved", "priority": "Trivial", "reporter": "Rahul Singhal", "assignee": null, "labels": [], "created": "2014-04-28T16:34:54.000+0000", "updated": "2014-05-04T18:09:05.000+0000", "description": "The current test in make-distribution.sh to identify if maven is installed is incorrect since the exit code is being checked for \"tail\" rather than \"mvn\"", "comments": ["PR: https://github.com/apache/spark/pull/580", "Issue resolved by pull request 580\n[https://github.com/apache/spark/pull/580]"], "derived": {"summary": "The current test in make-distribution. sh to identify if maven is installed is incorrect since the exit code is being checked for \"tail\" rather than \"mvn\".", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Correctly identify if maven is installed and working - The current test in make-distribution. sh to identify if maven is installed is incorrect since the exit code is being checked for \"tail\" rather than \"mvn\"."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 580\n[https://github.com/apache/spark/pull/580]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1659", "title": "improvements spark-submit usage", "status": "Resolved", "priority": "Major", "reporter": "Guoqiang Li", "assignee": "Guoqiang Li", "labels": [], "created": "2014-04-28T16:50:28.000+0000", "updated": "2014-05-02T04:40:55.000+0000", "description": "Delete spark-submit obsolete usage: \"--arg ARG\"", "comments": ["./bin/spark-submit /opt/spark/classes/toona-assembly-1.0.0-SNAPSHOT.jar  --verbose  --master spark://spark:7077 --deploy-mode client --class com.zhe800.toona.als.computation.DealCF 20140425  => \n{code}\nUsing properties file: /opt/spark/spark-1.0.0-cdh3/conf/spark-defaults.conf\nAdding default property: spark.eventLog.enabled=true\nAdding default property: spark.akka.askTimeout=120\nAdding default property: spark.default.parallelism=32\nAdding default property: spark.executor.extraJavaOptions=-Xss5m -server -XX:+UseConcMarkSweepGC -XX:+ExplicitGCInvokesConcurrent -XX:+CMSClassUnloadingEnabled -XX:+AggressiveOpts -XX:PermSize=150M -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=150M\nAdding default property: spark.ui.killEnabled=false\nAdding default property: spark.storage.memoryFraction=0.7\nAdding default property: spark.locality.wait=10000\nAdding default property: spark.executor.memory=13g\nAdding default property: spark.master=spark://spark:7077\nAdding default property: spark.storage.blockManagerTimeoutIntervalMs=6000000\nAdding default property: spark.akka.timeout=120\nAdding default property: spark.akka.frameSize=1600\nAdding default property: spark.broadcast.blockSize=4096\nAdding default property: spark.eventLog.dir=/opt/spark/logs/\nAdding default property: spark.driver.extraJavaOptions=-Xss5m -server -XX:+UseConcMarkSweepGC -XX:+ExplicitGCInvokesConcurrent -XX:+CMSClassUnloadingEnabled -XX:+AggressiveOpts -XX:PermSize=150M -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=150M\nUsing properties file: /opt/spark/spark-1.0.0-cdh3/conf/spark-defaults.conf\nAdding default property: spark.eventLog.enabled=true\nAdding default property: spark.akka.askTimeout=120\nAdding default property: spark.default.parallelism=32\nAdding default property: spark.executor.extraJavaOptions=-Xss5m -server -XX:+UseConcMarkSweepGC -XX:+ExplicitGCInvokesConcurrent -XX:+CMSClassUnloadingEnabled -XX:+AggressiveOpts -XX:PermSize=150M -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=150M\nAdding default property: spark.ui.killEnabled=false\nAdding default property: spark.storage.memoryFraction=0.7\nAdding default property: spark.locality.wait=10000\nAdding default property: spark.executor.memory=13g\nAdding default property: spark.master=spark://spark:7077\nAdding default property: spark.storage.blockManagerTimeoutIntervalMs=6000000\nAdding default property: spark.akka.timeout=120\nAdding default property: spark.akka.frameSize=1600\nAdding default property: spark.broadcast.blockSize=4096\nAdding default property: spark.eventLog.dir=/opt/spark/logs/\nAdding default property: spark.driver.extraJavaOptions=-Xss5m -server -XX:+UseConcMarkSweepGC -XX:+ExplicitGCInvokesConcurrent -XX:+CMSClassUnloadingEnabled -XX:+AggressiveOpts -XX:PermSize=150M -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=150M\n{code}\nprinted twice"], "derived": {"summary": "Delete spark-submit obsolete usage: \"--arg ARG\".", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "improvements spark-submit usage - Delete spark-submit obsolete usage: \"--arg ARG\"."}, {"q": "What updates or decisions were made in the discussion?", "a": "./bin/spark-submit /opt/spark/classes/toona-assembly-1.0.0-SNAPSHOT.jar  --verbose  --master spark://spark:7077 --deploy-mode client --class com.zhe800.toona.als.computation.DealCF 20140425  => \n{code}\nUsing properties file: /opt/spark/spark-1.0.0-cdh3/conf/spark-defaults.conf\nAdding default property: spark.eventLog.enabled=true\nAdding default property: spark.akka.askTimeout=120\nAdding default property: spark.default.parallelism=32\nAdding default property: spark.executor.extraJavaOptions=-Xss5m -server -XX:+UseConcMarkSweepGC -XX:+ExplicitGCInvokesConcurrent -XX:+CMSClassUnloadingEnabled -XX:+AggressiveOpts -XX:PermSize=150M -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=150M\nAdding default property: spark.ui.killEnabled=false\nAdding default property: spark.storage.memoryFraction=0.7\nAdding default property: spark.locality.wait=10000\nAdding default property: spark.executor.memory=13g\nAdding default property: spark.master=spark://spark:7077\nAdding default property: spark.storage.blockManagerTimeoutIntervalMs=6000000\nAdding default property: spark.akka.timeout=120\nAdding default property: spark.akka.frameSize=1600\nAdding default property: spark.broadcast.blockSize=4096\nAdding default property: spark.eventLog.dir=/opt/spark/logs/\nAdding default property: spark.driver.extraJavaOptions=-Xss5m -server -XX:+UseConcMarkSweepGC -XX:+ExplicitGCInvokesConcurrent -XX:+CMSClassUnloadingEnabled -XX:+AggressiveOpts -XX:PermSize=150M -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=150M\nUsing properties file: /opt/spark/spark-1.0.0-cdh3/conf/spark-defaults.conf\nAdding default property: spark.eventLog.enabled=true\nAdding default property: spark.akka.askTimeout=120\nAdding default property: spark.default.parallelism=32\nAdding default property: spark.executor.extraJavaOptions=-Xss5m -server -XX:+UseConcMarkSweepGC -XX:+ExplicitGCInvokesConcurrent -XX:+CMSClassUnloadingEnabled -XX:+AggressiveOpts -XX:PermSize=150M -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=150M\nAdding default property: spark.ui.killEnabled=false\nAdding default property: spark.storage.memoryFraction=0.7\nAdding default property: spark.locality.wait=10000\nAdding default property: spark.executor.memory=13g\nAdding default property: spark.master=spark://spark:7077\nAdding default property: spark.storage.blockManagerTimeoutIntervalMs=6000000\nAdding default property: spark.akka.timeout=120\nAdding default property: spark.akka.frameSize=1600\nAdding default property: spark.broadcast.blockSize=4096\nAdding default property: spark.eventLog.dir=/opt/spark/logs/\nAdding default property: spark.driver.extraJavaOptions=-Xss5m -server -XX:+UseConcMarkSweepGC -XX:+ExplicitGCInvokesConcurrent -XX:+CMSClassUnloadingEnabled -XX:+AggressiveOpts -XX:PermSize=150M -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=150M\n{code}\nprinted twice"}]}}
{"project": "SPARK", "issue_id": "SPARK-1660", "title": "Centralize the definition of property names and default values", "status": "Closed", "priority": "Major", "reporter": "Kousuke Saruta", "assignee": null, "labels": [], "created": "2014-04-28T23:37:57.000+0000", "updated": "2015-01-23T23:48:54.000+0000", "description": "There are lots of multiple definition of property names and default values in the code.\nLet's consolidate and clean up.", "comments": ["Can this be closed? or made specific? declaring property names as constants helps avoid typos, for sure, but I'm not sure there is a problem of this form in the code now. It's actually hard to know where to put this class with all the constant definitions; even Core is too heavy a dependency for some users of the property values. Certainly if there are cases of defaults being inconsistent that should just be fixed in its own right. Are there some?", "Closing for now."], "derived": {"summary": "There are lots of multiple definition of property names and default values in the code. Let's consolidate and clean up.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Centralize the definition of property names and default values - There are lots of multiple definition of property names and default values in the code. Let's consolidate and clean up."}, {"q": "What updates or decisions were made in the discussion?", "a": "Closing for now."}]}}
{"project": "SPARK", "issue_id": "SPARK-1661", "title": "the result of querying table created with RegexSerDe is all null", "status": "Resolved", "priority": "Major", "reporter": "likunjian", "assignee": null, "labels": ["HQL", "hadoop", "hive", "regex", "shark"], "created": "2014-04-29T06:40:05.000+0000", "updated": "2014-04-30T01:43:56.000+0000", "description": "\nthe result of querying table created with RegexSerDe is all null\n\n\n\n\n\n\n\nwhen i query the table created with org.apache.hadoop.hive.contrib.serde2.RegexSerDe by shark,the columns in the result is all null\nselect * from access_log where logdate='2014-04-28' limit 10;\nOK\nip      host    time    method  request protocol        status  size    referer cookieuid       requesttime     session httpxrequestedwith      agent   upstreamresponsetime    logdate\nNULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    2014-04-28\nNULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    2014-04-28\nNULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    2014-04-28\nNULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    2014-04-28\nNULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    2014-04-28\nNULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    2014-04-28\nNULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    2014-04-28\nNULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    2014-04-28\nNULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    2014-04-28\nNULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    2014-04-28\nTime taken: 4.362 seconds\n\nmy regex is\n ^([^ ]*) [^ ]* ([^ ]*) \\\\[([^\\]]*)\\\\] \\\"([^ ]*) ([^ ]*) ([^ ]*)\\\" (-|[0-9]*) (-|[0-9]*) \\\"(\\.\\+\\?|-)\\\" ([^ ]*) ([^ ]*) ([^ ]*) \\\"(\\.\\+\\?|-)\\\" \\\"(\\.\\+\\?|-)\\\" \\\"(\\.\\+\\?|-)\\\"$\n\nnginx log example:\n42.49.44.61 - www.xxxx.comm [20/Apr/2014:23:58:03 +0800] \"GET /xxxxx/296837 HTTP/1.1\" 200 3871 \"http://www.xxxxx.com/xxxxx/296837\" - 0.015 63hbb4om2cvtjs0f7d969n1uf4 \"com.xxxxx.browser\" \"Mozilla/5.0 (Linux; U; xxxxx 4.1.2; zh-cn; ZTE N919 Build/JZO54K) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30\" \"0.015\"\n111.121.176.149 - www.xxxx.comm [20/Apr/2014:23:58:03 +0800] \"GET /xxxxx/264904 HTTP/1.1\" 200 3827 \"http://m.baidu.com/s?from=2001a&bd_page_type=1&word=%E8%8E%B2%E8%97%95%E6%80%8E%E6%A0%B7%E5%8D%A4%E6%89%8D%E5%A5%BD%E5%90%83\" - 0.015 ft7tr4b06b23ub9lnugdf4gcq3 \"-\" \"Mozilla/5.0 (Linux; U; xxxxx 4.1.2; zh-CN; 8190Q Build/JZO54K) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 UCBrowser/9.5.2.394 U3/0.8.0 Mobile Safari/533.1\" \"0.015\"\n222.209.97.169 - www.xxxx.comm [20/Apr/2014:23:58:04 +0800] \"GET / HTTP/1.1\" 200 3188 \"http://m.idea123.cn/food.html\" - 0.014 - \"-\" \"Lenovo S890/S100 Linux/3.0.13 xxxxx/4.0.3 Release/12.12.2011 Browser/AppleWebKit534.30 Profile/MIDP-2.0 Configuration/CLDC-1.1 Mobile Safari/534.30\" \"0.014\"\n59.36.84.241 - www.xxxx.comm [20/Apr/2014:23:58:05 +0800] \"GET /app/xxxxx/topic/view.php?id=138555 HTTP/1.1\" 200 3151 \"-\" - 0.009 - \"-\" \"Mozilla/5.0 (Linux; U; xxxxx 2.3.7; zh-cn; TD500 Build/GWK74) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30\" \"0.009\"\n113.242.39.81 - www.xxxx.comm [20/Apr/2014:23:58:07 +0800] \"GET /xxxxx/419691 HTTP/1.1\" 200 4174 \"http://www.xxxx.comm/xxxxx/all/308?p=3\" - 0.013 1n579ukg1gho7i7mr3q8ic8j97 \"-\" \"Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_5_7; en-us) AppleWebKit/530.17 (KHTML, like Gecko) Version/4.0 Safari/530.17; 360browser(securitypay,securityinstalled); 360(xxxxx,uppayplugin); 360 Aphone Browser (5.3.1)\" \"0.013\"\n\nVery strange, I execute a query in Hive is normal. I really do not understand. . .  :-(\nOK\nip      host    time    method  request protocol        status  size    referer cookieuid       requesttime     session httpxrequestedwith      agent   upstreamresponsetime    logdate\n14.151.40.117   www.xxxx.com  10/Apr/2014:23:58:01 +0800      POST    /xxxx.jsp?appid=4&appkey=573bbd2fbd1a6bac082ff4727d952ba3&format=json&sessionid=1397145480&vc=24&vn=v3.5.1&loguid=&deviceid=0f607264fc6318a92b9e13c65db7cd3c%7C02C105A6-6DC8-43D5-879E-46AD603AC34E%7C2096145A-114C-4B6E-BE91-1AC740D9BD21&channel=appstore&method=Update.forceUpdate      HTTP/1.1        200     88      -  0.052    -       -       xxxxx xxxxx iPhone Client API 0.005   2014-04-11\n112.91.89.149   www.xxxx.com  10/Apr/2014:23:58:02 +0800      POST    /xxxx.jsp?appid=2&appkey=9ef269eec4f7a9d07c73952d06b5413f&format=json&sessionid=1397141523139&vc=53&vn=3.5.2&loguid=2888069&deviceid=xxxxx354244055617944&uuid=c7d71164-bae7-4d51-8032-ec9cfafb5e7e&channel=default&method=Info.getinfoV3&virtual=O1wq33EpQ%2Fa%2B8NxjEsy57ZezKBefR85F4L%2BXPZlcSETtw5Fl%2FdQFuLuNUg4Co9zHJyw2jPJipOR3%0Acrc59PUeFvM5hU82AdDQGjIkXa%2FLMWtxuJYz6fJBHLxkQRPWUVCdpeENwrnYlvgAY6DqM9G%2Fh5g1%0AbZamnAgUMERY1iZzPLk%3D%0A HTTP/1.1        200     2008    xxxxx xxxxx xxxxx POST      -       0.033       -       -       Mozilla/5.0 (xxxxx) xxxxx/20100101 xxxxx/1.0.0       0.033   2014-04-11\n113.133.68.221  www.xxxx.com  10/Apr/2014:23:58:02 +0800      POST    /xxxx.jsp?appid=2&appkey=9ef269eec4f7a9d07c73952d06b5413f&format=json&sessionid=1397145345830&vc=53&vn=3.5.2&loguid=4377880&deviceid=xxxxx355369055653422&uuid=546e07f7-6439-48d4-8d6d-e5dc0001e569&channel=91_v352&method=Ad.getAd_iMocha&virtual=       HTTP/1.1        200     88      xxxxx xxxxx xxxxx POST      -  0.127    -       -       Mozilla/5.0 (xxxxx) xxxxx/20100101 xxxxx/1.0.0       0.127   2014-04-11\n\nthis is my HQL:\nCREATE external TABLE access_log (\n  ip STRING,\n  host STRING,\n  time STRING,\n  method STRING,\n  request STRING,\n  protocol STRING,\n  status STRING,\n  size STRING,\n  referer STRING,\n  cookieuid STRING,\n  requestTime STRING,\n  session STRING,\n  httpXRequestedWith STRING,\n  agent STRING,\n  upstreamResponseTime STRING\n)\npartitioned by (logdate string)\nROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe'\nWITH SERDEPROPERTIES  (\n\"input.regex\" = \"^([^ ]*) - ([^ ]*) \\\\[([^\\]]*)\\\\] \\\"([^ ]*) ([^ ]*) ([^ ]*)\\\" (-|[0-9]*) (-|[0-9]*) \\\"(\\.\\+\\?|-)\\\" ([^ ]*) ([^ ]*) ([^ ]*) \\\"(\\.\\+\\?|-)\\\" \\\"(\\.\\+\\?|-)\\\" \\\"(\\.\\+\\?|-)\\\"$\",\n\"output.format.string\" = \"%1$s %2$s %3$s %4$s %5$s %6$s %7$s %8$s %9$s %10$s %11$s %12$s %13$s %14$s %15$s\"\n)\nSTORED AS TEXTFILE;", "comments": ["all content", "Thanks for your report.  This JIRA is for reporting bugs with Spark and its components.  Shark is a separate project and issues with older versions of Shark should probably be filed on the Shark issue tracker.\n\nHowever, I did add a test to make sure the RegexSerDe was working with Spark SQL (which is a nearly from scratch rewrite of Shark, that will be included in the 1.0 release of Spark as an Alpha component).  If you find you are still having problems with Spark SQL, please reopen this issue.\n\nNew spark tests: https://github.com/apache/spark/pull/595"], "derived": {"summary": "the result of querying table created with RegexSerDe is all null\n\n\n\n\n\n\n\nwhen i query the table created with org. apache.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "the result of querying table created with RegexSerDe is all null - the result of querying table created with RegexSerDe is all null\n\n\n\n\n\n\n\nwhen i query the table created with org. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for your report.  This JIRA is for reporting bugs with Spark and its components.  Shark is a separate project and issues with older versions of Shark should probably be filed on the Shark issue tracker.\n\nHowever, I did add a test to make sure the RegexSerDe was working with Spark SQL (which is a nearly from scratch rewrite of Shark, that will be included in the 1.0 release of Spark as an Alpha component).  If you find you are still having problems with Spark SQL, please reopen this issue.\n\nNew spark tests: https://github.com/apache/spark/pull/595"}]}}
{"project": "SPARK", "issue_id": "SPARK-1662", "title": "PySpark fails if python class is used as a data container", "status": "Closed", "priority": "Minor", "reporter": "Chandan Kumar", "assignee": null, "labels": [], "created": "2014-04-29T08:57:20.000+0000", "updated": "2014-07-18T09:53:18.000+0000", "description": "PySpark fails if RDD operations are performed on data encapsulated in Python objects (rare use case where plain python objects are used as data containers instead of regular dict or tuples).\n\nI have written a small piece of code to reproduce the bug:\nhttps://gist.github.com/nrchandan/11394440\n\n<script src=\"https://gist.github.com/nrchandan/11394440.js\"></script>\n\n", "comments": ["[~nrchandan] and [~pwendell] - i recommend you close this as not a bug. it's not pyspark's fault that the user-defined class is not able to be pickled. you can change the Point class in the example to make it pickleable and the example program will work. see https://docs.python.org/2/library/pickle.html#what-can-be-pickled-and-unpickled\n\noriginal gist for posterity -\n{code}\nimport pyspark\n\nclass Point(object):\n    '''this class being used as container'''\n    pass\n\ndef to_point_obj(point_as_dict):\n    '''convert a dict representation of a point to Point object'''\n    p = Point()\n    p.x = point_as_dict['x']\n    p.y = point_as_dict['y']\n    return p\n\ndef add_two_points(point_obj1, point_obj2):\n    print type(point_obj1), type(point_obj2)\n    point_obj1.x += point_obj2.x\n    point_obj1.y += point_obj2.y\n    return point_obj1\n\ndef zero_point():\n    p = Point()\n    p.x = p.y = 0\n    return p\n\nsc = pyspark.SparkContext('local', 'test_app')\n\na = sc.parallelize([{'x':1, 'y':1}, {'x':2, 'y':2}, {'x':3, 'y':3}])\nb = a.map(to_point_obj)   # convert to an RDD of Point objects\nc = b.fold(zero_point(), add_two_points)\n{code}", "Sounds good. Closing the issue.", "The issue is due to a limitation with Python's pickle mechanism. Probably not worth the effort to use something other than pickle. There are workarounds anyway."], "derived": {"summary": "PySpark fails if RDD operations are performed on data encapsulated in Python objects (rare use case where plain python objects are used as data containers instead of regular dict or tuples). I have written a small piece of code to reproduce the bug:\nhttps://gist.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "PySpark fails if python class is used as a data container - PySpark fails if RDD operations are performed on data encapsulated in Python objects (rare use case where plain python objects are used as data containers instead of regular dict or tuples). I have written a small piece of code to reproduce the bug:\nhttps://gist."}, {"q": "What updates or decisions were made in the discussion?", "a": "The issue is due to a limitation with Python's pickle mechanism. Probably not worth the effort to use something other than pickle. There are workarounds anyway."}]}}
{"project": "SPARK", "issue_id": "SPARK-1663", "title": "Spark Streaming docs code has several small errors", "status": "Resolved", "priority": "Minor", "reporter": "Sean R. Owen", "assignee": "Sean R. Owen", "labels": ["streaming"], "created": "2014-04-29T16:22:49.000+0000", "updated": "2015-01-15T09:08:39.000+0000", "description": "The changes are easiest to elaborate in the PR, which I will open shortly.\n\nThose changes raised a few little questions about the API too.", "comments": ["PR: https://github.com/apache/spark/pull/589", "Issue resolved by pull request 589\n[https://github.com/apache/spark/pull/589]"], "derived": {"summary": "The changes are easiest to elaborate in the PR, which I will open shortly. Those changes raised a few little questions about the API too.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Spark Streaming docs code has several small errors - The changes are easiest to elaborate in the PR, which I will open shortly. Those changes raised a few little questions about the API too."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 589\n[https://github.com/apache/spark/pull/589]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1664", "title": "spark-submit --name doesn't work in yarn-client mode", "status": "Resolved", "priority": "Blocker", "reporter": "Thomas Graves", "assignee": null, "labels": [], "created": "2014-04-29T17:05:05.000+0000", "updated": "2014-05-09T05:15:03.000+0000", "description": "When using spark-submit in yarn-client mode, the --name option doesn't properly set the application name in either the ResourceManager UI.", "comments": [], "derived": {"summary": "When using spark-submit in yarn-client mode, the --name option doesn't properly set the application name in either the ResourceManager UI.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "spark-submit --name doesn't work in yarn-client mode - When using spark-submit in yarn-client mode, the --name option doesn't properly set the application name in either the ResourceManager UI."}]}}
{"project": "SPARK", "issue_id": "SPARK-1665", "title": "add a config to replace SPARK_YARN_USER_ENV", "status": "Resolved", "priority": "Major", "reporter": "Thomas Graves", "assignee": null, "labels": [], "created": "2014-04-29T17:11:26.000+0000", "updated": "2014-05-05T21:45:31.000+0000", "description": "we should add a config to replace the env variable SPARK_YARN_USER_ENV.  If it makes sense we should make it generic to all Spark.  If it doesn't then we should atleast have a yarn config so we aren't using environment variables anymore.", "comments": ["dup of SPARK-1680"], "derived": {"summary": "we should add a config to replace the env variable SPARK_YARN_USER_ENV. If it makes sense we should make it generic to all Spark.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "add a config to replace SPARK_YARN_USER_ENV - we should add a config to replace the env variable SPARK_YARN_USER_ENV. If it makes sense we should make it generic to all Spark."}, {"q": "What updates or decisions were made in the discussion?", "a": "dup of SPARK-1680"}]}}
{"project": "SPARK", "issue_id": "SPARK-1666", "title": "document examples", "status": "Resolved", "priority": "Major", "reporter": "Diana Carroll", "assignee": null, "labels": [], "created": "2014-04-29T17:12:47.000+0000", "updated": "2016-01-04T14:41:58.000+0000", "description": "It would be great if there were some guidance about what the example code shipped with Spark (under $SPARKHOME/examples and $SPARKHOME/python/examples) does and how to run it.  Perhaps a comment block at the beginning explaining what the code accomplishes and what parameters it takes.  Also, if there are sample datasets on which the example is designed to run, please point to those.  \n\n(As an example, look at kmeans.py, which takes a file argument, but has no hint about what sort of data is in the file or what format the data should be in.", "comments": ["The examples have been broadly improved, including compiling the test code. I don't know if it specifically addressed this issue, but given how old this is I'm going to call this somewhere between \"done\" and \"wont fix\"."], "derived": {"summary": "It would be great if there were some guidance about what the example code shipped with Spark (under $SPARKHOME/examples and $SPARKHOME/python/examples) does and how to run it. Perhaps a comment block at the beginning explaining what the code accomplishes and what parameters it takes.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "document examples - It would be great if there were some guidance about what the example code shipped with Spark (under $SPARKHOME/examples and $SPARKHOME/python/examples) does and how to run it. Perhaps a comment block at the beginning explaining what the code accomplishes and what parameters it takes."}, {"q": "What updates or decisions were made in the discussion?", "a": "The examples have been broadly improved, including compiling the test code. I don't know if it specifically addressed this issue, but given how old this is I'm going to call this somewhere between \"done\" and \"wont fix\"."}]}}
{"project": "SPARK", "issue_id": "SPARK-1667", "title": "Jobs never finish successfully once bucket file missing occurred", "status": "Closed", "priority": "Major", "reporter": "Kousuke Saruta", "assignee": null, "labels": [], "created": "2014-04-29T18:18:41.000+0000", "updated": "2020-05-17T18:31:19.000+0000", "description": "If jobs execute shuffle, bucket files are created in a temporary directory (named like spark-local-*).\nWhen the bucket files are missing cased by disk failure or any reasons, jobs cannot execute shuffle which has same shuffle id for the bucket files.", "comments": ["PR: https://github.com/apache/spark/pull/1383", "Can anyone review the patch or have opinions?", "Thanks. Just took a look and left some comments.", "Is this still an issue?  Was it resolved in a different PR?", "Hi [~sarutak] it looks like you sent in a better fix for this problem in SPARK-2670.  Are we good to close this ticket now?", "[~Andrew Ash] Oh yeah, I close this ticket.", "This ticket is resolved by SPARK-2670."], "derived": {"summary": "If jobs execute shuffle, bucket files are created in a temporary directory (named like spark-local-*). When the bucket files are missing cased by disk failure or any reasons, jobs cannot execute shuffle which has same shuffle id for the bucket files.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Jobs never finish successfully once bucket file missing occurred - If jobs execute shuffle, bucket files are created in a temporary directory (named like spark-local-*). When the bucket files are missing cased by disk failure or any reasons, jobs cannot execute shuffle which has same shuffle id for the bucket files."}, {"q": "What updates or decisions were made in the discussion?", "a": "This ticket is resolved by SPARK-2670."}]}}
{"project": "SPARK", "issue_id": "SPARK-1668", "title": "Add implicit preference as an option to examples/MovieLensALS", "status": "Resolved", "priority": "Minor", "reporter": "Xiangrui Meng", "assignee": "Sandeep Singh", "labels": [], "created": "2014-04-29T19:44:34.000+0000", "updated": "2014-05-08T04:16:10.000+0000", "description": "Add --implicitPrefs as an command-line option to the example app MovieLensALS under examples/. For evaluation, we should map ratings to range [0, 1] and compare it with predictions. It would be better if we add unobserved ratings (assuming negatives) to evaluation.", "comments": ["https://github.com/apache/spark/pull/597"], "derived": {"summary": "Add --implicitPrefs as an command-line option to the example app MovieLensALS under examples/. For evaluation, we should map ratings to range [0, 1] and compare it with predictions.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add implicit preference as an option to examples/MovieLensALS - Add --implicitPrefs as an command-line option to the example app MovieLensALS under examples/. For evaluation, we should map ratings to range [0, 1] and compare it with predictions."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/597"}]}}
{"project": "SPARK", "issue_id": "SPARK-1669", "title": "SQLContext.cacheTable() should be idempotent", "status": "Resolved", "priority": "Major", "reporter": "Cheng Lian", "assignee": "Cheng Lian", "labels": ["cache", "column"], "created": "2014-04-29T23:12:24.000+0000", "updated": "2014-06-23T21:55:26.000+0000", "description": "Calling {{cacheTable()}} on some table {{t}} multiple times causes table {{t}} to  be cached multiple times. This semantics is different from {{RDD.cache()}}, which is idempotent.\n\nWe can check whether a table is already cached by checking:\n\n# whether the structure of the underlying logical plan of the table is matches the pattern {{Subquery(\\_, SparkLogicalPlan(inMem @ InMemoryColumnarTableScan(_, _)))}}\n# whether {{inMem.cachedColumnBuffers.getStorageLevel.useMemory}} is true", "comments": ["https://github.com/apache/spark/pull/1183"], "derived": {"summary": "Calling {{cacheTable()}} on some table {{t}} multiple times causes table {{t}} to  be cached multiple times. This semantics is different from {{RDD.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "SQLContext.cacheTable() should be idempotent - Calling {{cacheTable()}} on some table {{t}} multiple times causes table {{t}} to  be cached multiple times. This semantics is different from {{RDD."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/1183"}]}}
{"project": "SPARK", "issue_id": "SPARK-1670", "title": "PySpark Fails to Create SparkContext Due To Debugging Options in conf/java-opts", "status": "Resolved", "priority": "Major", "reporter": "Pat McDonough", "assignee": null, "labels": [], "created": "2014-04-29T23:20:14.000+0000", "updated": "2015-02-17T01:03:18.000+0000", "description": "When JVM debugging options are in conf/java-opts, it causes pyspark to fail when creating the SparkContext. The java-opts file looks like the following:\n{code}-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005\n{code}\nHere's the error:\n{code}---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n/Library/Python/2.7/site-packages/IPython/utils/py3compat.pyc in execfile(fname, *where)\n    202             else:\n    203                 filename = fname\n--> 204             __builtin__.execfile(filename, *where)\n\n/Users/pat/Projects/spark/python/pyspark/shell.py in <module>()\n     41     SparkContext.setSystemProperty(\"spark.executor.uri\", os.environ[\"SPARK_EXECUTOR_URI\"])\n     42 \n---> 43 sc = SparkContext(os.environ.get(\"MASTER\", \"local[*]\"), \"PySparkShell\", pyFiles=add_files)\n     44 \n     45 print(\"\"\"Welcome to\n\n/Users/pat/Projects/spark/python/pyspark/context.pyc in __init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway)\n     92             tempNamedTuple = namedtuple(\"Callsite\", \"function file linenum\")\n     93             self._callsite = tempNamedTuple(function=None, file=None, linenum=None)\n---> 94         SparkContext._ensure_initialized(self, gateway=gateway)\n     95 \n     96         self.environment = environment or {}\n\n/Users/pat/Projects/spark/python/pyspark/context.pyc in _ensure_initialized(cls, instance, gateway)\n    172         with SparkContext._lock:\n    173             if not SparkContext._gateway:\n--> 174                 SparkContext._gateway = gateway or launch_gateway()\n    175                 SparkContext._jvm = SparkContext._gateway.jvm\n    176                 SparkContext._writeToFile = SparkContext._jvm.PythonRDD.writeToFile\n\n/Users/pat/Projects/spark/python/pyspark/java_gateway.pyc in launch_gateway()\n     44         proc = Popen(command, stdout=PIPE, stdin=PIPE)\n     45     # Determine which ephemeral port the server started on:\n---> 46     port = int(proc.stdout.readline())\n     47     # Create a thread to echo output from the GatewayServer, which is required\n     48     # for Java log output to show up:\n\nValueError: invalid literal for int() with base 10: 'Listening for transport dt_socket at address: 5005\\n'\n{code}\n\nNote that when you use JVM debugging, the very first line of output (e.g. when running spark-shell) looks like this:\n{code}Listening for transport dt_socket at address: 5005{code}", "comments": ["FYI [~ahirreddy] [~matei], here's the pyspark issue I was talking to you guys about", "SPARK-2313 is the root cause of this. a workaround for this would be complex because the extra text on stdout is coming from the same jvm that should produce the py4j port.", "The root cause, SPARK-2313, was fixed for 1.3, so I'm going to mark this as Fixed."], "derived": {"summary": "When JVM debugging options are in conf/java-opts, it causes pyspark to fail when creating the SparkContext. The java-opts file looks like the following:\n{code}-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005\n{code}\nHere's the error:\n{code}---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n/Library/Python/2.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "PySpark Fails to Create SparkContext Due To Debugging Options in conf/java-opts - When JVM debugging options are in conf/java-opts, it causes pyspark to fail when creating the SparkContext. The java-opts file looks like the following:\n{code}-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005\n{code}\nHere's the error:\n{code}---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n/Library/Python/2."}, {"q": "What updates or decisions were made in the discussion?", "a": "The root cause, SPARK-2313, was fixed for 1.3, so I'm going to mark this as Fixed."}]}}
{"project": "SPARK", "issue_id": "SPARK-1671", "title": "Cached tables should follow write-through policy", "status": "Resolved", "priority": "Major", "reporter": "Cheng Lian", "assignee": "Michael Armbrust", "labels": ["cache", "column"], "created": "2014-04-29T23:25:02.000+0000", "updated": "2014-10-03T19:37:09.000+0000", "description": "Writing (insert / load) to a cached table causes cache inconsistency, and user have to unpersist and cache the whole table again.\n\nThe write-through policy may be implemented with {{RDD.union}}.", "comments": ["I'm gonna mark this as resolved now that we do at least invalidate the cache when writing through.  We can create a follow up JIRA for partial invalidation if we want."], "derived": {"summary": "Writing (insert / load) to a cached table causes cache inconsistency, and user have to unpersist and cache the whole table again. The write-through policy may be implemented with {{RDD.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Cached tables should follow write-through policy - Writing (insert / load) to a cached table causes cache inconsistency, and user have to unpersist and cache the whole table again. The write-through policy may be implemented with {{RDD."}, {"q": "What updates or decisions were made in the discussion?", "a": "I'm gonna mark this as resolved now that we do at least invalidate the cache when writing through.  We can create a follow up JIRA for partial invalidation if we want."}]}}
{"project": "SPARK", "issue_id": "SPARK-1672", "title": "Support separate partitioners (and numbers of partitions) for users and products", "status": "Closed", "priority": "Minor", "reporter": "Tor Myklebust", "assignee": "Tor Myklebust", "labels": [], "created": "2014-04-29T23:26:28.000+0000", "updated": "2014-07-20T00:15:25.000+0000", "description": "The user ought to be able to specify a partitioning of his data if he knows a good one.  It's convenient to have separate partitioners for users and products so that no strange mapping step needs to happen.\n\nIt may also be reasonable to partition the users and products into different numbers of partitions (for instance, to balance memory requirements) if the dataset is tall, thin, and very sparse.", "comments": ["PR: https://github.com/apache/spark/pull/1014"], "derived": {"summary": "The user ought to be able to specify a partitioning of his data if he knows a good one. It's convenient to have separate partitioners for users and products so that no strange mapping step needs to happen.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support separate partitioners (and numbers of partitions) for users and products - The user ought to be able to specify a partitioning of his data if he knows a good one. It's convenient to have separate partitioners for users and products so that no strange mapping step needs to happen."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR: https://github.com/apache/spark/pull/1014"}]}}
{"project": "SPARK", "issue_id": "SPARK-1673", "title": "GLMNET implementation in Spark", "status": "Closed", "priority": "Major", "reporter": "Sung Chung", "assignee": null, "labels": [], "created": "2014-04-29T23:48:16.000+0000", "updated": "2015-03-12T20:43:33.000+0000", "description": "This is a Spark implementation of GLMNET by Jerome Friedman, Trevor Hastie, Rob Tibshirani.\n\nhttp://www.jstatsoft.org/v33/i01/paper\n\nIt's a straightforward implementation of the Coordinate-Descent based L1/L2 regularized linear models, including Linear/Logistic/Multinomial regressions.", "comments": ["This is eventually going to be taken care of by the OWLQN addition. https://issues.apache.org/jira/browse/SPARK-1892", "Some colleagues and I have a Spark version of glmnet working and have started some discussion.  Joseph Bradley suggested that we copy the discussion here in order to keep track of it.  Here's the discussion thread in the usual last-first order.  Besides myself, the thread involves Joseph and Debashish Das who is working on the OWLQN implementation.  \n\nOn Wed, Feb 25, 2015 at 10:35 AM, <mike@mbowles.com> wrote:\n\nHi Debasish,\nAny method that generates point solutions to the minimization problem could simply be run a number of times to generate the coefficient paths as a function of the penalty parameter.  I think the only issues are how easy the method is to use and how much training and developer time is required to produce an answer. \n\nWith regard to training time, Friedman says in his paper that they found problems where glmnet would generate the entire coefficient path more rapidly than sophisticated single point methods would generate single point solutions - not all problems, but some problems.  Ryan Tibshirani (Robert's son) who's a professor and researcher at CMU in convex function optimization has echoed that assertion for the particular case of the elasticnet penalty function (that's from slides of his that are available online).  So there's an open question about the training speed that i believe we can answer in fairly short order.  I'm eager to explore that.  Does OWLQN do a pass through the data for each iteration?  The linear version of GLMNET does not.  On the other hand, OWLQN may be able to take coarser steps through parameter space.  \n\nWith regard to developer time, glmnet doesn't require the user to supply a starting point for the penalty parameter.  It calculates the starting point.  That makes it completely automatic.  you've probably been through the process of manually searching regularization parameter space with SVM.  Pick out a set of regularization parameter values like 10 raised to the (-2 through +5 in steps of 1).  See if there's a minimum in the range and if not shift to the right or left.  One of the reasons I pick up glmnet first for a new problem is that you just drop in the training set and out pop the coefficient curves.  Usually the defaults work.  One time out of 50 (or so) it doesn't converge.  It alerts you that it didn't converge and you change one parameter and rerun.  If you also drop in a test set then it even picks the optimum solution andproduces an estimate of out-of-sample error.  \n\nWe're going to make some speed/scaling runs on the synthetic data sets (in a range of sizes) that are used in Spark for testing linear regression.  We need some wider data sets.  Joseph mentioned some that we'll look at.  I've got a gene expression data set that's 30k wide by 15k tall.  That takes a few hours to train using R version of glmnet.  We're also talking to some biology friends to find other interesting data sets. \n\nI really am eager to see the comparisons.  And happy to help you tailor OWLQN to generate coefficient paths.  We might be able to produce a hybrid of Friedman's algorithm using his basic algorithm outline but substituting OWLQN for his round-robin coordinate descent.  But i'm a little cocerned that it's the round-robin coordinate descent that makes it possible to skip passing through the full data set for 4 out of 5 iterations.  We might be able to work a way around that. \n\nI'm just eager to have parallel versions of the tools available.  I'll keep you posted on our results.  We should aim for running one another's code.  I'll check with my colleagues and see when we'll have something we can hand out.  We've delayed putting together a release version in favor of generating some scaling results, as Joseph suggested.  Discussions like this may have some impact on what the release code looks like. \nMike\n\nFrom: Debasish Das [mailto:debasish.das83@gmail.com]\nSent: Wednesday, February 25, 2015 08:50 AM\nTo: 'Joseph Bradley'\nCc: mike@mbowles.com, 'dev'\nSubject: Re: Have Friedman's glmnet algo running in Spark\n\nAny reason why the regularization path cannot be implemented using current owlqn pr ?\n\nWe can change owlqn in breeze to fit your needs...\n\n*From:* Joseph Bradley [mailto:joseph@databricks.com]\n*Sent:* Sunday, February 22, 2015 06:48 PM\n*To:* mike@mbowles.com\n*Cc:* dev@spark.apache.org\n *Subject:* Re: Have Friedman's glmnet algo running in Spark\n Hi Mike, glmnet has definitely been very successful, and it would be great to see how we can improve optimization in MLlib! There is some related work  ongoing; here are the JIRAs: GLMNET implementation in Spark LinearRegression with L1/L2 (elastic net) using OWLQN in new ML package The GLMNET JIRA has actually been closed in favor of the latter JIRA.   However, if you're getting good results in your experiments, could you please post them on the GLMNET JIRA and link them from the other JIRA? If  it's faster and more scalable, that would be great to find out. As far as where the code should go and the APIs, that can be discussed on the JIRA. I hope this helps, and I'll keep an eye out for updates on the JIRAs! Joseph", "Some thoughts:\n\n{quote}\nFriedman says in his paper that they found problems where glmnet would generate the entire coefficient path more rapidly than sophisticated single point methods would generate single point solutions\n{quote}\n\nThis is true, but it's actually often even better to use an approximate path instead of an exact path (which glmnet uses).  There is a lot of literature discussing \"continuation,\" \"warm-starts,\" \"approximate regularization paths,\" and \"homotopy\" (which is sometimes overloaded to mean approximate homotopy).  I worry about glmnet doing a lot of iterations, whereas analogous but approximate methods could make larger jumps along the regularization path.\n\nContinuation (following an approximate regularization path) can actually be used as a wrapper around a lot of optimization algorithms to speed them up; I've used it successfully with coordinate descent, accelerated gradient, and others.  I haven't tried it with OWL-QN.  It might be interesting to explore a general continuation wrapper.  Some of the other benefits you mention apply to any algorithm wrapped with continuation (e.g., automatically choosing a starting point for the penalty parameter).", "Good discussion.  I can see how it might be faster to propagate an approximate path as a way to provide good starting conditions for an accurate iteration.  to some extent the accuracy of the glmnet path can be modulated by loosening the convergence criteria for the inner iteration (the iteration done to find the new minimum after the penalty parameter is decremented).  \n\nThe big time sink is making passes through the data.  with glmnet regression the inner iterations don't require making passes through the data so they are much less expensive than the steps in the penalty parameter, which may provoke a pass through the data to deal with a new element being added to the active list.  \n\nIt would be interesting to see what happens if the active set of coefficients was constrained to change less frequently than the penalty parameter.  I have a hunch that it might take more (inexpensive) inner iterations to converge when the coefficient were allowed to change, but it would save passes through the data.  \n\nIt would be relatively easy for us to implement this in our code.  We can try only letting the active set change every other or every third step in the penalty parameter and see how much change it makes in the coefficient curves.  \n\nThanks for the idea.  ", "That sounds good---I'll look forward to hearing how it does!", "Here's a table of scaling results for our implementation of glmnet regression.  These are run locally on a 4-core server.  The data set is the higgs boson data set (available on aws).  We measured training times for various numbers of rows of data from 1000 to 10 million.  The attribute space is 28 variables wide.  We ran on 1 through 4 cores on the server.  \n\nTraining times (sec)\n#rows   1-core  2-core  3-core  4-cores\n100K    4.88    3.79    3.41    3.48\n1M      20.5    10.6    9.51    8.45\n5M      71.2    37.1    26.7    25.5\n10M     155     70.5    59.7    49.7\nThe structure of the algorithm suggests that the training times should be linear in the number of rows and the test results bear that out.  Two cores shows a speedup of ~2 over one core and three cores shows ~2.6 over one core and four cores speeds up by ~3.11.  The four core result probably lags due to conflict with system function etc.  Running on AWS will make that clearer.  That's in process now.\n\nOur next steps are \n1.  run on some wider data sets\n2.  run on larger cluster\n3.  run OWLQN on the same problems in the same setting\n4.  experiment with speedups - Joseph Bradley's approximation idea and cutting the number of data passes down by predicting what variables are going to become active instead of waiting until they do.  "], "derived": {"summary": "This is a Spark implementation of GLMNET by Jerome Friedman, Trevor Hastie, Rob Tibshirani. http://www.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "GLMNET implementation in Spark - This is a Spark implementation of GLMNET by Jerome Friedman, Trevor Hastie, Rob Tibshirani. http://www."}, {"q": "What updates or decisions were made in the discussion?", "a": "Here's a table of scaling results for our implementation of glmnet regression.  These are run locally on a 4-core server.  The data set is the higgs boson data set (available on aws).  We measured training times for various numbers of rows of data from 1000 to 10 million.  The attribute space is 28 variables wide.  We ran on 1 through 4 cores on the server.  \n\nTraining times (sec)\n#rows   1-core  2-core  3-core  4-cores\n100K    4.88    3.79    3.41    3.48\n1M      20.5    10.6    9.51    8.45\n5M      71.2    37.1    26.7    25.5\n10M     155     70.5    59.7    49.7\nThe structure of the algorithm suggests that the training times should be linear in the number of rows and the test results bear that out.  Two cores shows a speedup of ~2 over one core and three cores shows ~2.6 over one core and four cores speeds up by ~3.11.  The four core result probably lags due to conflict with system function etc.  Running on AWS will make that clearer.  That's in process now.\n\nOur next steps are \n1.  run on some wider data sets\n2.  run on larger cluster\n3.  run OWLQN on the same problems in the same setting\n4.  experiment with speedups - Joseph Bradley's approximation idea and cutting the number of data passes down by predicting what variables are going to become active instead of waiting until they do."}]}}
{"project": "SPARK", "issue_id": "SPARK-1674", "title": "Interrupted system call error in pyspark's RDD.pipe", "status": "Resolved", "priority": "Major", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-04-30T00:13:00.000+0000", "updated": "2014-04-30T01:07:58.000+0000", "description": "RDD.pipe's doctest throws interrupted system call exception on Mac. It can be fixed by wrapping pipe.stdout.readline in an iterator.", "comments": ["PR: https://github.com/apache/spark/pull/594"], "derived": {"summary": "RDD. pipe's doctest throws interrupted system call exception on Mac.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Interrupted system call error in pyspark's RDD.pipe - RDD. pipe's doctest throws interrupted system call exception on Mac."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR: https://github.com/apache/spark/pull/594"}]}}
{"project": "SPARK", "issue_id": "SPARK-1675", "title": "Make clear whether computePrincipalComponents requires centered data", "status": "Resolved", "priority": "Trivial", "reporter": "Sandy Ryza", "assignee": "Sandy Ryza", "labels": [], "created": "2014-04-30T03:02:51.000+0000", "updated": "2014-07-03T18:55:43.000+0000", "description": null, "comments": ["Centering in PCA should be the standard practice.", "Is this still valid? Looking at the code, PCA is computed as the SVD of the covariance matrix. The means implicitly don't matter. they are not explicitly subtracted, and do not matter. Or is there still a doc change desired?", "I think it still wouldn't hurt to add a remark that input data doesn't need to be centered.  Should have marked this trivial when I filed it - not a big deal either way.", "Issue resolved by pull request 1171\n[https://github.com/apache/spark/pull/1171]"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Make clear whether computePrincipalComponents requires centered data"}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 1171\n[https://github.com/apache/spark/pull/1171]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1676", "title": "HDFS FileSystems continually pile up in the FS cache", "status": "Resolved", "priority": "Critical", "reporter": "Aaron Davidson", "assignee": "Thomas Graves", "labels": [], "created": "2014-04-30T04:08:31.000+0000", "updated": "2017-10-30T17:09:25.000+0000", "description": "Due to HDFS-3545, FileSystem.get() always produces (and caches) a new FileSystem when provided with a new UserGroupInformation (UGI), even if the UGI represents the same user as another UGI. This causes a buildup of FileSystem objects at an alarming rate, often one per task for something like sc.textFile(). The bug is especially hard-hitting for NativeS3FileSystem, which also maintains an open connection to S3, clogging up the system file handles.\n\nThe bug was introduced in https://github.com/apache/spark/pull/29, where doAs was made the default behavior.\n\nA fix is not forthcoming for the general case, as UGIs do not cache well, but this problem can lead to spark clusters entering into a failed state and requiring executors be restarted.", "comments": ["Discussion for this issue lies mostly in https://github.com/apache/spark/pull/607\n\nThe current master branch PR by Tom Graves: https://github.com/apache/spark/pull/621", "https://github.com/apache/spark/pull/621"], "derived": {"summary": "Due to HDFS-3545, FileSystem. get() always produces (and caches) a new FileSystem when provided with a new UserGroupInformation (UGI), even if the UGI represents the same user as another UGI.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "HDFS FileSystems continually pile up in the FS cache - Due to HDFS-3545, FileSystem. get() always produces (and caches) a new FileSystem when provided with a new UserGroupInformation (UGI), even if the UGI represents the same user as another UGI."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/621"}]}}
{"project": "SPARK", "issue_id": "SPARK-1677", "title": "Allow users to avoid Hadoop output checks if desired", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Nan Zhu", "labels": [], "created": "2014-04-30T05:42:41.000+0000", "updated": "2016-11-23T01:14:04.000+0000", "description": "For compatibility with older versions of Spark it would be nice to have an option `spark.hadoop.validateOutputSpecs` (default true) and a description \"If set to true, validates the output specification used in saveAsHadoopFile and other variants. This can be disabled to silence exceptions due to pre-existing output directories.\"\n\nThis would just wrap the checking done in this PR:\nhttps://issues.apache.org/jira/browse/SPARK-1100\nhttps://github.com/apache/spark/pull/11\n\nBy first checking the spark conf.", "comments": ["Issue resolved by pull request 947\n[https://github.com/apache/spark/pull/947]", "Hi Spark Community,\n\nI'm curious on the behavior of this \"spark.hadoop.validateOutputSpecs\" option. If I set it to 'false', will existing files in output directory get wiped out beforehand? For example, if spark job is to output file Y under directory A, which already contain file X, do we expect both file X and Y under folder A? Or just Y will be retained after the job completion.\n\nThanks!", "User 'CodingCat' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/947"], "derived": {"summary": "For compatibility with older versions of Spark it would be nice to have an option `spark. hadoop.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Allow users to avoid Hadoop output checks if desired - For compatibility with older versions of Spark it would be nice to have an option `spark. hadoop."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'CodingCat' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/947"}]}}
{"project": "SPARK", "issue_id": "SPARK-1678", "title": "Compression loses repeated values.", "status": "Resolved", "priority": "Blocker", "reporter": "Michael Armbrust", "assignee": "Cheng Lian", "labels": [], "created": "2014-04-30T06:17:40.000+0000", "updated": "2014-05-06T02:39:24.000+0000", "description": "Here's a test case:\n\n{code}\n  test(\"all the same strings\") {\n    sparkContext.parallelize(1 to 1000).map(_ => StringData(\"test\")).registerAsTable(\"test1000\")\n    assert(sql(\"SELECT * FROM test1000\").count() === 1000)\n    cacheTable(\"test1000\")\n    assert(sql(\"SELECT * FROM test1000\").count() === 1000)\n  }\n{code}\n\nFirst assert passes, second one fails.", "comments": ["Pull request: https://github.com/apache/spark/pull/608", "Issue resolved by pull request 608\n[https://github.com/apache/spark/pull/608]"], "derived": {"summary": "Here's a test case:\n\n{code}\n  test(\"all the same strings\") {\n    sparkContext. parallelize(1 to 1000).", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Compression loses repeated values. - Here's a test case:\n\n{code}\n  test(\"all the same strings\") {\n    sparkContext. parallelize(1 to 1000)."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 608\n[https://github.com/apache/spark/pull/608]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1679", "title": "In-Memory compression needs to be configurable.", "status": "Resolved", "priority": "Blocker", "reporter": "Michael Armbrust", "assignee": "Cheng Lian", "labels": [], "created": "2014-04-30T06:20:11.000+0000", "updated": "2014-05-06T19:09:05.000+0000", "description": "Since we are still finding bugs in the compression code I think we should make it configurable in SparkConf and turn it off by default for the 1.0 release.", "comments": ["Pull request: https://github.com/apache/spark/pull/608"], "derived": {"summary": "Since we are still finding bugs in the compression code I think we should make it configurable in SparkConf and turn it off by default for the 1. 0 release.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "In-Memory compression needs to be configurable. - Since we are still finding bugs in the compression code I think we should make it configurable in SparkConf and turn it off by default for the 1. 0 release."}, {"q": "What updates or decisions were made in the discussion?", "a": "Pull request: https://github.com/apache/spark/pull/608"}]}}
{"project": "SPARK", "issue_id": "SPARK-1680", "title": "Clean up use of setExecutorEnvs in SparkConf ", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Thomas Graves", "labels": [], "created": "2014-04-30T06:32:29.000+0000", "updated": "2016-01-21T17:45:06.000+0000", "description": "We should make this consistent between YARN and Standalone. Basically, YARN mode should just use the executorEnvs from the Spark conf and not need SPARK_YARN_USER_ENV.", "comments": ["I think this is dup of SPARK-1665.", "so what do we want to do here. Do we want to expose spark.executorEnv. to the user to allow them to set environment variables?  I assume since it isn't documented it wasn't meant for end users but perhaps we just missed documenting it.  I would like to add a config to replace the env variable SPARK_YARN_USER_ENV. \n\nEither way I will probably need another config to set env variables on the application master.  I could make that one yarn specific like spark.yarn.appMasterEnv.", "https://github.com/apache/spark/pull/1512", "User 'tgravescs' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1512", "User 'weineran' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/10869"], "derived": {"summary": "We should make this consistent between YARN and Standalone. Basically, YARN mode should just use the executorEnvs from the Spark conf and not need SPARK_YARN_USER_ENV.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Clean up use of setExecutorEnvs in SparkConf  - We should make this consistent between YARN and Standalone. Basically, YARN mode should just use the executorEnvs from the Spark conf and not need SPARK_YARN_USER_ENV."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'weineran' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/10869"}]}}
{"project": "SPARK", "issue_id": "SPARK-1681", "title": "Handle hive support correctly in ./make-distribution.sh", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "labels": [], "created": "2014-04-30T06:37:04.000+0000", "updated": "2014-07-02T09:45:23.000+0000", "description": "When Hive support is enabled we should copy the datanucleus jars to the packaged distribution. The simplest way would be to create a lib_managed folder in the final distribution so that the compute-classpath script searches in exactly the same way whether or not it's a release.\n\nA slightly nicer solution is to put the jars inside of `/lib` and have some fancier check for the jar location in the compute-classpath script.\n\nWe should also document how to run Spark SQL on YARN when hive support is enabled. In particular how to add the necessary jars to spark-submit.", "comments": ["[The related work|https://github.com/apache/spark/pull/598]", "Closed via:\nhttps://github.com/apache/spark/pull/610"], "derived": {"summary": "When Hive support is enabled we should copy the datanucleus jars to the packaged distribution. The simplest way would be to create a lib_managed folder in the final distribution so that the compute-classpath script searches in exactly the same way whether or not it's a release.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Handle hive support correctly in ./make-distribution.sh - When Hive support is enabled we should copy the datanucleus jars to the packaged distribution. The simplest way would be to create a lib_managed folder in the final distribution so that the compute-classpath script searches in exactly the same way whether or not it's a release."}, {"q": "What updates or decisions were made in the discussion?", "a": "Closed via:\nhttps://github.com/apache/spark/pull/610"}]}}
{"project": "SPARK", "issue_id": "SPARK-1682", "title": "Add gradient descent w/o sampling and RDA L1 updater", "status": "Closed", "priority": "Major", "reporter": "Dong Wang", "assignee": null, "labels": [], "created": "2014-04-30T07:23:17.000+0000", "updated": "2014-11-10T17:30:01.000+0000", "description": "The GradientDescent optimizer does sampling before a gradient step. When input data is already shuffled beforehand, it is possible to scan data and make gradient descent for each data instance. This could be potentially more efficient.\n\nAdd enhanced RDA L1 updater, which could produce even sparse solutions with comparable quality compared with L1. Reference: \nLin Xiao, \"Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization\", Journal of Machine Learning Research 11 (2010) 2543-2596.\n\nSmall fix: add options to BinaryClassification example to read and write model file\n", "comments": ["User 'dongwang218' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/643", "revisit later"], "derived": {"summary": "The GradientDescent optimizer does sampling before a gradient step. When input data is already shuffled beforehand, it is possible to scan data and make gradient descent for each data instance.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add gradient descent w/o sampling and RDA L1 updater - The GradientDescent optimizer does sampling before a gradient step. When input data is already shuffled beforehand, it is possible to scan data and make gradient descent for each data instance."}, {"q": "What updates or decisions were made in the discussion?", "a": "revisit later"}]}}
{"project": "SPARK", "issue_id": "SPARK-1683", "title": "Display filesystem read statistics with each task", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Kay Ousterhout", "labels": [], "created": "2014-04-30T16:58:43.000+0000", "updated": "2014-08-07T06:31:09.000+0000", "description": null, "comments": ["I've already done this and will submit a PR this weekend", "https://github.com/apache/spark/pull/962"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Display filesystem read statistics with each task"}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/962"}]}}
{"project": "SPARK", "issue_id": "SPARK-1684", "title": "Merge script should standardize SPARK-XXX prefix", "status": "Closed", "priority": "Minor", "reporter": "Patrick Wendell", "assignee": "Michelle Casbon", "labels": ["starter"], "created": "2014-04-30T17:07:56.000+0000", "updated": "2015-04-22T01:07:51.000+0000", "description": "If users write \"[SPARK-XXX] Issue\" or \"SPARK-XXX. Issue\" or \"SPARK XXX: Issue\" we should convert it to \"SPARK-XXX: Issue\"", "comments": ["(Can I be pedantic and suggest standardizing to SPARK-XXX ? this is how issues are reported in other projects, like HIVE-123 and MAPREDUCE-234. In fact see how JIRA doesn't link this: HIVE 123 )", "Just a typo in the description! SPARK-XXX is the correct format ala:\nhttps://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark", "User 'texasmichelle' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/5149", "Test data (spark_pulls_before_after.txt): titles from existing pull requests in their original form and after being modified with the new function. It's not perfect, but it cleans up most of the common problems."], "derived": {"summary": "If users write \"[SPARK-XXX] Issue\" or \"SPARK-XXX. Issue\" or \"SPARK XXX: Issue\" we should convert it to \"SPARK-XXX: Issue\".", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Merge script should standardize SPARK-XXX prefix - If users write \"[SPARK-XXX] Issue\" or \"SPARK-XXX. Issue\" or \"SPARK XXX: Issue\" we should convert it to \"SPARK-XXX: Issue\"."}, {"q": "What updates or decisions were made in the discussion?", "a": "Test data (spark_pulls_before_after.txt): titles from existing pull requests in their original form and after being modified with the new function. It's not perfect, but it cleans up most of the common problems."}]}}
{"project": "SPARK", "issue_id": "SPARK-1685", "title": "retryTimer not canceled on actor restart in Worker and AppClient", "status": "Resolved", "priority": "Major", "reporter": "Mark Hamstra", "assignee": "Mark Hamstra", "labels": [], "created": "2014-04-30T18:23:12.000+0000", "updated": "2014-05-06T19:56:57.000+0000", "description": "Both deploy.worker.Worker and deploy.client.AppClient try to registerWithMaster when those Actors start.  The attempt at registration is accomplished by starting a retryTimer via the Akka scheduler that will use the registered timeout interval and retry number to make repeated attempts to register with all known Masters before giving up and either marking as dead or calling System.exit.\n\nThe receive methods of these actors can, however, throw exceptions, which will lead to the actor restarting, registerWithMaster being called again on restart, and another retryTimer being scheduled without canceling the already running retryTimer.  Assuming that all of the rest of the restart logic is correct for these actors (which I don't believe is actually a given), having multiple retryTimers running presents at least a condition in which the restarted actor may not be able to make the full number of retry attempts before an earlier retryTimer takes the \"give up\" action.\n\nCanceling the retryTimer in the actor's postStop hook should suffice. ", "comments": [], "derived": {"summary": "Both deploy. worker.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "retryTimer not canceled on actor restart in Worker and AppClient - Both deploy. worker."}]}}
{"project": "SPARK", "issue_id": "SPARK-1686", "title": "Master switches thread when ElectedLeader", "status": "Resolved", "priority": "Major", "reporter": "Mark Hamstra", "assignee": "Nan Zhu", "labels": [], "created": "2014-04-30T21:02:55.000+0000", "updated": "2014-05-10T04:53:21.000+0000", "description": "In deploy.master.Master, the completeRecovery method is the last thing to be called when a standalone Master is recovering from failure.  It is responsible for resetting some state, relaunching drivers, and eventually resuming its scheduling duties.\n\nThere are currently four places in Master.scala where completeRecovery is called.  Three of them are from within the actor's receive method, and aren't problems.  The last starts from within receive when the ElectedLeader message is received, but the actual completeRecovery() call is made from the Akka scheduler.  That means that it will execute on a different scheduler thread, and Master itself will end up running (i.e., schedule() ) from that Akka scheduler thread.  Among other things, that means that uncaught exception handling will be different -- https://issues.apache.org/jira/browse/SPARK-1620 ", "comments": [], "derived": {"summary": "In deploy. master.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Master switches thread when ElectedLeader - In deploy. master."}]}}
{"project": "SPARK", "issue_id": "SPARK-1687", "title": "Support NamedTuples in RDDs", "status": "Resolved", "priority": "Major", "reporter": "Pat McDonough", "assignee": "Davies Liu", "labels": [], "created": "2014-04-30T21:48:59.000+0000", "updated": "2014-08-04T21:38:09.000+0000", "description": "Add Support for NamedTuples in RDDs. Some sample code is below, followed by the current error that comes from it.\n\nBased on a quick conversation with [~ahirreddy], [Dill|https://github.com/uqfoundation/dill] might be a good solution here.\n\n{code}\nIn [26]: from collections import namedtuple\n...\nIn [33]: Person = namedtuple('Person', 'id firstName lastName')\n\nIn [34]: jon = Person(1, \"Jon\", \"Doe\")\n\nIn [35]: jane = Person(2, \"Jane\", \"Doe\")\n\nIn [36]: theDoes = sc.parallelize((jon, jane))\n\nIn [37]: theDoes.collect()\nOut[37]: \n[Person(id=1, firstName='Jon', lastName='Doe'),\n Person(id=2, firstName='Jane', lastName='Doe')]\n\nIn [38]: theDoes.count()\nPySpark worker failed with exception:\nPySpark worker failed with exception:\nTraceback (most recent call last):\n  File \"/Users/pat/Projects/spark/python/pyspark/worker.py\", line 77, in main\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/pat/Projects/spark/python/pyspark/rdd.py\", line 1373, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/pat/Projects/spark/python/pyspark/rdd.py\", line 1373, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/pat/Projects/spark/python/pyspark/rdd.py\", line 283, in func\n    def func(s, iterator): return f(iterator)\n  File \"/Users/pat/Projects/spark/python/pyspark/rdd.py\", line 708, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/Users/pat/Projects/spark/python/pyspark/rdd.py\", line 708, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/Users/pat/Projects/spark/python/pyspark/serializers.py\", line 129, in load_stream\n    yield self._read_with_length(stream)\n  File \"/Users/pat/Projects/spark/python/pyspark/serializers.py\", line 146, in _read_with_length\n    return self.loads(obj)\nAttributeError: 'module' object has no attribute 'Person'\n\nTraceback (most recent call last):\n  File \"/Users/pat/Projects/spark/python/pyspark/worker.py\", line 77, in main\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/pat/Projects/spark/python/pyspark/rdd.py\", line 1373, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/pat/Projects/spark/python/pyspark/rdd.py\", line 1373, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/pat/Projects/spark/python/pyspark/rdd.py\", line 283, in func\n    def func(s, iterator): return f(iterator)\n  File \"/Users/pat/Projects/spark/python/pyspark/rdd.py\", line 708, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/Users/pat/Projects/spark/python/pyspark/rdd.py\", line 708, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/Users/pat/Projects/spark/python/pyspark/serializers.py\", line 129, in load_stream\n    yield self._read_with_length(stream)\n  File \"/Users/pat/Projects/spark/python/pyspark/serializers.py\", line 146, in _read_with_length\n    return self.loads(obj)\nAttributeError: 'module' object has no attribute 'Person'\n\n14/04/30 14:43:53 ERROR Executor: Exception in task ID 23\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/pat/Projects/spark/python/pyspark/worker.py\", line 77, in main\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/pat/Projects/spark/python/pyspark/rdd.py\", line 1373, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/pat/Projects/spark/python/pyspark/rdd.py\", line 1373, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/pat/Projects/spark/python/pyspark/rdd.py\", line 283, in func\n    def func(s, iterator): return f(iterator)\n  File \"/Users/pat/Projects/spark/python/pyspark/rdd.py\", line 708, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/Users/pat/Projects/spark/python/pyspark/rdd.py\", line 708, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/Users/pat/Projects/spark/python/pyspark/serializers.py\", line 129, in load_stream\n    yield self._read_with_length(stream)\n  File \"/Users/pat/Projects/spark/python/pyspark/serializers.py\", line 146, in _read_with_length\n    return self.loads(obj)\nAttributeError: 'module' object has no attribute 'Person'\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:190)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:214)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:151)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:229)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:51)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:210)\n\tat org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:46)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:175)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\n14/04/30 14:43:53 ERROR Executor: Exception in task ID 21\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/pat/Projects/spark/python/pyspark/worker.py\", line 77, in main\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/pat/Projects/spark/python/pyspark/rdd.py\", line 1373, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/pat/Projects/spark/python/pyspark/rdd.py\", line 1373, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/pat/Projects/spark/python/pyspark/rdd.py\", line 283, in func\n    def func(s, iterator): return f(iterator)\n  File \"/Users/pat/Projects/spark/python/pyspark/rdd.py\", line 708, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/Users/pat/Projects/spark/python/pyspark/rdd.py\", line 708, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/Users/pat/Projects/spark/python/pyspark/serializers.py\", line 129, in load_stream\n    yield self._read_with_length(stream)\n  File \"/Users/pat/Projects/spark/python/pyspark/serializers.py\", line 146, in _read_with_length\n    return self.loads(obj)\nAttributeError: 'module' object has no attribute 'Person'\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:190)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:214)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:151)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:229)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:51)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:210)\n\tat org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:46)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:175)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\n14/04/30 14:43:53 ERROR TaskSetManager: Task 5.0:3 failed 1 times; aborting job\n---------------------------------------------------------------------------\nPy4JJavaError                             Traceback (most recent call last)\n<ipython-input-38-8689b264fa46> in <module>()\n----> 1 theDoes.count()\n\n/Users/pat/Projects/spark/python/pyspark/rdd.pyc in count(self)\n    706         3\n    707         \"\"\"\n--> 708         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n    709 \n    710     def stats(self):\n\n/Users/pat/Projects/spark/python/pyspark/rdd.pyc in sum(self)\n    697         6.0\n    698         \"\"\"\n--> 699         return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)\n    700 \n    701     def count(self):\n\n/Users/pat/Projects/spark/python/pyspark/rdd.pyc in reduce(self, f)\n    617             if acc is not None:\n    618                 yield acc\n--> 619         vals = self.mapPartitions(func).collect()\n    620         return reduce(f, vals)\n    621 \n\n/Users/pat/Projects/spark/python/pyspark/rdd.pyc in collect(self)\n    581         \"\"\"\n    582         with _JavaStackTrace(self.context) as st:\n--> 583           bytesInJava = self._jrdd.collect().iterator()\n    584         return list(self._collect_iterator_through_file(bytesInJava))\n    585 \n\n/Users/pat/Projects/spark/python/lib/py4j-0.8.1-src.zip/py4j/java_gateway.py in __call__(self, *args)\n    535         answer = self.gateway_client.send_command(command)\n    536         return_value = get_return_value(answer, self.gateway_client,\n--> 537                 self.target_id, self.name)\n    538 \n    539         for temp_arg in temp_args:\n\n/Users/pat/Projects/spark/python/lib/py4j-0.8.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)\n    298                 raise Py4JJavaError(\n    299                     'An error occurred while calling {0}{1}{2}.\\n'.\n--> 300                     format(target_id, '.', name), value)\n    301             else:\n    302                 raise Py4JError(\n\nPy4JJavaError: An error occurred while calling o53.collect.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5.0:3 failed 1 times, most recent failure: Exception failure in TID 23 on host localhost: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/pat/Projects/spark/python/pyspark/worker.py\", line 77, in main\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/pat/Projects/spark/python/pyspark/rdd.py\", line 1373, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/pat/Projects/spark/python/pyspark/rdd.py\", line 1373, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/pat/Projects/spark/python/pyspark/rdd.py\", line 283, in func\n    def func(s, iterator): return f(iterator)\n  File \"/Users/pat/Projects/spark/python/pyspark/rdd.py\", line 708, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/Users/pat/Projects/spark/python/pyspark/rdd.py\", line 708, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/Users/pat/Projects/spark/python/pyspark/serializers.py\", line 129, in load_stream\n    yield self._read_with_length(stream)\n  File \"/Users/pat/Projects/spark/python/pyspark/serializers.py\", line 146, in _read_with_length\n    return self.loads(obj)\nAttributeError: 'module' object has no attribute 'Person'\n\n        org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:190)\n        org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:214)\n        org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:151)\n        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)\n        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)\n        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)\n        org.apache.spark.scheduler.Task.run(Task.scala:51)\n        org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:210)\n        org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:46)\n        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:175)\n        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        java.lang.Thread.run(Thread.java:744)\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1033)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1017)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1015)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1015)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:633)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:633)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:633)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1207)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:456)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:219)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n{code}", "comments": ["Dill is implemented in pure Python, so it will have similar performance with pickle, but much slower than cPickle, which is the default serializer we use as default. So we could not switch the the default serializer to Dill.\n\nWe could provide an customized namedtuple (which can be serialized by cPickle), also replace the one in collections with it.\n\nI will send an PR, if it make sense.", "Sure, pls go ahead and feel free to take over this JIRA.", "User 'davies' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1623", "User 'davies' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1771"], "derived": {"summary": "Add Support for NamedTuples in RDDs. Some sample code is below, followed by the current error that comes from it.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Support NamedTuples in RDDs - Add Support for NamedTuples in RDDs. Some sample code is below, followed by the current error that comes from it."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'davies' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1771"}]}}
{"project": "SPARK", "issue_id": "SPARK-1688", "title": "PySpark throws unhelpful exception when pyspark cannot be loaded", "status": "Resolved", "priority": "Major", "reporter": "Andrew Or", "assignee": "Andrew Or", "labels": [], "created": "2014-04-30T23:38:45.000+0000", "updated": "2014-11-05T10:45:24.000+0000", "description": "Currently, if pyspark cannot be loaded, this happens:\n\njava.io.EOFException\n        at java.io.DataInputStream.readInt(DataInputStream.java:392)\n        at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:183)\n        at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:55)\n        at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:42)\n        at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:97)\n        at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:57)\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)\n\nThis can be caused by a few things:\n  (1) PYTHONPATH is not set\n  (2) PYTHONPATH does not contain the python directory (or jar, in the case of YARN)\n  (3) The jar does not contain pyspark files (YARN)\n  (4) The jar does not contain py4j files (YARN)\n\nWe should have explicit error messages for each one of them. For (2 - 4), we should print out the PYTHONPATH so the user doesn't have to SSH into the executor machines themselves to figure this out.", "comments": ["https://github.com/apache/spark/pull/603"], "derived": {"summary": "Currently, if pyspark cannot be loaded, this happens:\n\njava. io.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "PySpark throws unhelpful exception when pyspark cannot be loaded - Currently, if pyspark cannot be loaded, this happens:\n\njava. io."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/603"}]}}
{"project": "SPARK", "issue_id": "SPARK-1689", "title": "AppClient does not respond correctly to RemoveApplication", "status": "Resolved", "priority": "Major", "reporter": "Aaron Davidson", "assignee": "Aaron Davidson", "labels": [], "created": "2014-04-30T23:50:00.000+0000", "updated": "2014-05-20T04:02:10.000+0000", "description": "When the Master removes an application (usually due to too many executor failures), it means no future executors will be assigned to that app. Currently, the AppClient just marks the application as \"disconnected\", which is intended as a transient state during a period of reconnection. Thus, RemoveApplication just causes the application to enter a state where it has no executors and it doesn't die.", "comments": ["Issue resolved by pull request 605\n[https://github.com/apache/spark/pull/605]", "The new behavior correctly informs the scheduler of the failed state, but does not exit though we've been removed.\n\nCreated https://github.com/apache/spark/pull/832 to fix this issue."], "derived": {"summary": "When the Master removes an application (usually due to too many executor failures), it means no future executors will be assigned to that app. Currently, the AppClient just marks the application as \"disconnected\", which is intended as a transient state during a period of reconnection.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "AppClient does not respond correctly to RemoveApplication - When the Master removes an application (usually due to too many executor failures), it means no future executors will be assigned to that app. Currently, the AppClient just marks the application as \"disconnected\", which is intended as a transient state during a period of reconnection."}, {"q": "What updates or decisions were made in the discussion?", "a": "The new behavior correctly informs the scheduler of the failed state, but does not exit though we've been removed.\n\nCreated https://github.com/apache/spark/pull/832 to fix this issue."}]}}
{"project": "SPARK", "issue_id": "SPARK-1690", "title": "RDD.saveAsTextFile throws scala.MatchError if RDD contains empty elements", "status": "Resolved", "priority": "Minor", "reporter": "Glenn K. Lockwood", "assignee": "Kan Zhang", "labels": [], "created": "2014-05-01T00:08:16.000+0000", "updated": "2014-05-10T21:02:48.000+0000", "description": "The following pyspark code fails with a scala.MatchError exception if sample.txt contains any empty lines:\n\nfile = sc.textFile('hdfs://gcn-3-45.ibnet0:54310/user/glock/sample.txt')\nfile.saveAsTextFile('hdfs://gcn-3-45.ibnet0:54310/user/glock/sample.out')\n\nThe resulting stack trace:\n\n14/04/30 17:02:46 WARN scheduler.TaskSetManager: Lost TID 0 (task 0.0:0)\n14/04/30 17:02:46 WARN scheduler.TaskSetManager: Loss was due to scala.MatchError\nscala.MatchError: 0 (of class java.lang.Integer)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:129)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:119)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:112)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat org.apache.spark.rdd.PairRDDFunctions.org$apache$spark$rdd$PairRDDFunctions$$writeToFile$1(PairRDDFunctions.scala:732)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$2.apply(PairRDDFunctions.scala:741)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$2.apply(PairRDDFunctions.scala:741)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:53)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)\n\tat org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)\n\tat org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n\tat org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:722)\n\nThis can be reproduced with a sample.txt containing\n\n\"\"\"\nfoo\n\nbar\n\"\"\"\n\nand disappears if sample.txt is\n\n\"\"\"\nfoo\nbar\n\"\"\"", "comments": ["PR: https://github.com/apache/spark/pull/644", "Fixed by:\nhttps://github.com/apache/spark/pull/644"], "derived": {"summary": "The following pyspark code fails with a scala. MatchError exception if sample.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "RDD.saveAsTextFile throws scala.MatchError if RDD contains empty elements - The following pyspark code fails with a scala. MatchError exception if sample."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed by:\nhttps://github.com/apache/spark/pull/644"}]}}
{"project": "SPARK", "issue_id": "SPARK-1691", "title": "Support quoted arguments inside of spark-submit", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "labels": [], "created": "2014-05-01T05:20:41.000+0000", "updated": "2017-07-07T05:15:02.000+0000", "description": "Currently due to the way we send arguments on to spark-class, it doesn't work with quoted strings. For instance:\n\n{code}\n./bin/spark-submit --name \"My app\" --spark-driver-extraJavaOptions \"-Dfoo=x -Dbar=y\"\n{code}", "comments": ["Issue resolved by pull request 609\n[https://github.com/apache/spark/pull/609]", "User 'pwendell' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/609"], "derived": {"summary": "Currently due to the way we send arguments on to spark-class, it doesn't work with quoted strings. For instance:\n\n{code}.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support quoted arguments inside of spark-submit - Currently due to the way we send arguments on to spark-class, it doesn't work with quoted strings. For instance:\n\n{code}."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'pwendell' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/609"}]}}
{"project": "SPARK", "issue_id": "SPARK-1692", "title": "Enable external sorting in Spark SQL aggregates", "status": "Resolved", "priority": "Minor", "reporter": "Sudip Roy", "assignee": "Reynold Xin", "labels": [], "created": "2014-05-01T05:48:23.000+0000", "updated": "2014-05-25T03:51:57.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Enable external sorting in Spark SQL aggregates"}]}}
{"project": "SPARK", "issue_id": "SPARK-1693", "title": "Dependent on multiple versions of servlet-api jars lead to throw an SecurityException when Spark built for hadoop 2.3.0 , 2.4.0 ", "status": "Resolved", "priority": "Blocker", "reporter": "Guoqiang Li", "assignee": "Guoqiang Li", "labels": [], "created": "2014-05-01T07:43:47.000+0000", "updated": "2017-03-02T21:42:28.000+0000", "description": "{code}mvn test -Pyarn -Dhadoop.version=2.4.0 -Dyarn.version=2.4.0 > log.txt{code}\n\nThe log: \n{code}\nUnpersistSuite:\n- unpersist RDD *** FAILED ***\n  java.lang.SecurityException: class \"javax.servlet.FilterRegistration\"'s signer information does not match signer information of other classes in the same package\n  at java.lang.ClassLoader.checkCerts(ClassLoader.java:952)\n  at java.lang.ClassLoader.preDefineClass(ClassLoader.java:666)\n  at java.lang.ClassLoader.defineClass(ClassLoader.java:794)\n  at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n  at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)\n  at java.net.URLClassLoader.access$100(URLClassLoader.java:71)\n  at java.net.URLClassLoader$1.run(URLClassLoader.java:361)\n  at java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n  at java.security.AccessController.doPrivileged(Native Method)\n  at java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n{code}", "comments": ["I suspect this occurs because two copies of the servlet API jars are included from two sources, and one of those sources includes jar signing information in its manifest. The resulting merged jar has collisions in the signing information and it no longer matches.\n\nIf that's right, the fastest way to avoid this is usually to drop signing information that is in the manifest since it is not helpful in the assembly jar. Of course it's ideal to avoid merging two copies of the same dependencies, since only one can be included, and that's why we see some [warn] in the build. In just about all cases it is harmless since they are actually copies of the same version of the same classes.\n\nI will look into what ends up in the manifest.", "I think this is traceable to a case of jar conflict. I am not sure whether the ultimate cause is signing, but it doesn't matter, since we should simply resolve the conflict. (But I think something like that may be at play, since one of the problem dependencies is from Eclipse's jetty, and there is an Eclipse cert in the manifest at META-INF/ECLIPSEF.RSA...) Anyway.\n\nThis is another fun jar hell puzzler, albeit one with a probable solution. The basic issues is that Jetty brings in the Servlet 3.0 API:\n\n{code}\n[INFO] |  +- org.eclipse.jetty:jetty-server:jar:8.1.14.v20131031:compile\n[INFO] |  |  +- org.eclipse.jetty.orbit:javax.servlet:jar:3.0.0.v201112011016:compile\n{code}\n\n... but in Hadoop 2.3.0+, so does Hadoop client:\n\n{code}\n[INFO] |  +- org.apache.hadoop:hadoop-client:jar:2.4.0:compile\n...\n[INFO] |  |  +- org.apache.hadoop:hadoop-mapreduce-client-core:jar:2.4.0:compile\n[INFO] |  |  |  \\- org.apache.hadoop:hadoop-yarn-common:jar:2.4.0:compile\n[INFO] |  |  |     +- javax.xml.bind:jaxb-api:jar:2.2.2:compile\n...\n[INFO] |  |  |     +- javax.servlet:servlet-api:jar:2.5:compile\n{code}\n\nEclipse is naughty for packaging the same API classes in a different artifact, rather than just using javax.servlet:servlet-api 3.0. There may be a reason for that, which is what worries me. In theory Servlet 3.0 is a superset of 2.5, so Hadoop's client should be happy with the 3.0 API on the classpath.\n\nSo solution #1 to try is to exclude javax.servlet:servlet-api from the hadoop-client dependency. It won't affect earlier versions at all since they don't have this dependency, and therefore need not even be version-specific.\n\nHadoop 2.3+ then in theory should happily find Eclipse's Servlet 3.0 API classes and work as normal. I give that about an 90% chance of being true.\n\nSolution #2 is more theoretically sound. We should really exclude Jetty's custom copy of Servlet 3.0, and depend on javax.servlet:servlet-api:jar:3.0 as a runtime dependency. This should transparently override Hadoop 2.3+'s version, and still work for Hadoop. Messing with Jetty increases the chance of another snag. Probability of success: 80%\n\n\nLi would you be able to try either of those ideas?", "Hi, Sean Owen \nThe simplest solution is to remove the dependency of servlet-api\n[The related work | https://github.com/witgo/spark/commit/0ed124dc0e453a0a59d3c387651be970859a9a0a]", "Ah didn't even see that! Yeah that's my \"#1\", which you already identified. It is simple. Does it resolve this error? (I assume so.)\n\nThe only reason this feels undesirable is that Hadoop's web app then only happens to work because Jetty is also bundled. That kind of accidental working is brittle. (Although, why does the Hadoop client lib need a web app? could be an accident of the dependency graph that totally doesn't matter.)\n\nI had thought that the real fix is to declare that the project's conflicting dependencies must agree, by declaring the dependency they must agree on and then excluding the offenders. That is, does #2 work for you too? It is not much more work and more robust. But I am not 100% sure it works.", "The dependency path is as follows\n{code}hadoop-client=> hadoop-mapreduce-client-core => hadoop-yarn-common=>servlet-api{code}\nYarn work in the hadoop cluster,and hadoop already contains servlet-api in {code}${HADOOP_COMMON_HOME}/share/hadoop/yarn/lib/servlet-api-2.5.jar{code}\nI guess \"#1\" should be no problem,but there is no test", "Correct. I thought you had tried option #1 since you had suggested it too. Can you not test it in the same way that you discovered the problem? If you try, I'd suggest the #2 option, because if that works, it is a more robust solution.", "Servlet 3.0, 2.5 together on the classpath,Is that ok?", "No, that's exactly the problem as far as I can tell. My suggestion is:\n\n- In core, exclude org.eclipse.jetty.orbit:javax.servlet from org.eclipse.jetty:jetty-server\n- Declare a runtime-scope dependency in core, on javax.servlet:servlet-api:jar:3.0\n- (And that will happen to override Hadoop's javax.servlet:servlet-api:jar:2.5 !)\n- Update the SBT build as closely as possible to match", "Hi Sean Owen\nYou have time to review [the code|https://github.com/witgo/spark/tree/SPARK-1693 ]?", "Yes looks very close to what I had in mind; I have two suggestions:\n- To be ultra-safe, use version 3.0.0 of the Servlet API not 3.0.1\n- Maybe drop a comment in the parent pom about why this dependency exists -- even just a reference to this JIRA\n\nDoes it work for you then? fingers crossed.", "I didn't find servlet-api  3.0.0 stable version\n[http://mvnrepository.com/artifact/javax.servlet]\n", "Huh! you're right. Yes I agree with what you have done then.", "Thanks Sean Owen .\nCode has been submitted to [PR 590|https://github.com/apache/spark/pull/590]", "I'm confused about something [~sowen]: If Hadoop 2.3/2.4 wants version 3 of the servlet-api dependency, why does the Hadoop build declare version 2.5 as a dependency in their POM's?\n\nhttp://repo1.maven.org/maven2/org/apache/hadoop/hadoop-project/2.3.0/hadoop-project-2.3.0.pom\nhttp://repo1.maven.org/maven2/org/apache/hadoop/hadoop-project/2.4.0/hadoop-project-2.4.0.pom\n\nAlso - is there not a risk of bumping this version to 3.X when users are building against older hadoop versions? I noticed the older hadoop versions depend 2.X versions of this... will it break them if 2.X and 3.X are present at runtime?\n\n[~sowen] If you could look over this PR it would be great. It's really scary merging a big change like this to the build at the 11th hour. So the more eyes the better.", "Now, the main problem is to ensure that hadoop 2.3, 2.4 can work in this version.But I don't have Hadoop 2.3, 2.4 clusters.", "[~pwendell] The Hadoop artifacts want version 2.5, as you see. The Servlet 3.0.x *APIs* should be a superset of 2.5. In theory clients implementing or using 2.5 will be fine with it, which means there's an 80% chance it's actually true. Hence I think a solution is to enforce use of 3.0.1, and exclude all other copies of the servlet API.\n\nNormally that's as simple as declaring a runtime dependency on the latest version of the artifact, which will override any transitive dependencies. The problem here is exactly because that's not true; Eclipse has its own artifact for the servlet 3.0 APIs.\n\nAnd [~witgo], as you noted, the artifact name actually changed from 2.5 to 3.0.1! So depending on it will *not* remove Hadoop's dependency. I think one more change is needed, to exclude Hadoop's dependency on the old javax.servlet:servlet-api artifact.\n\nBasically, \"mvn ... dependency:tree | grep javax.servlet\" should only show the new 3.0.1 dependency.\n\nThis should hopefully solve the client-side, unit test failures at least, which is necessary. [~witgo] I assume you have found it makes the tests pass? I do not think it will affect compatibility with a cluster, as this isn't changing or affecting how the client talks to a cluster. ", "Is this only seen in the tests?  When I run on a real hadoop2.4 cluster with spark built for hadoop 2.4 I don't see any exceptions. ", "[~tgraves]\nYou're right . Different versions of the Servlet APIs are forced to merge into spark-assembly.*hadoop.*.jar,each .class file will only have a presence", "[~tgraves] I see it in tests. It sounds like it does not affect simple, normal operation, which is good. It would still be good to fix up, for propiety's sake and because of the tests. [~gq] the problem is exactly that several copies of the same class are merged, right?", "[~srowen] You're right,preferably only one version exists. ", "Issue resolved by pull request 628\n[https://github.com/apache/spark/pull/628]", "I think the fix version of this issue is incorrect, I could reproduce the problem with spark-core 1.1.0-cdh5.2.1. With version 1.3.0-cdh5.4.2, the same code works fine in local execution.", "SUCCESS: Integrated in gora-trunk #1610 (See [https://builds.apache.org/job/gora-trunk/1610/])\nservlet dependency is excluded from spark, not ant. Without excluding it throws a java security exception. Motivation for it: https://issues.apache.org/jira/browse/SPARK-1693 and (furkankamaci: rev ad4f5601ebd52e182bf79ba4032dcb85d918b925)\n* gora-core/pom.xml\n* pom.xml\n", "Can you explain what the problem is and how to fix it? We are hitting the same problem on the hive on spark work.", "As I recall this happens because the official JavaEE servlet API jar includes MANIFEST.MF entries with a signature. If these classes are repackaged in an assembly jar with these same MANIFEST.MF entries, you get this failure. There's a lot of discussion above, but one quick fix is to manage to not include the signatures from anywhere. ", "We just upgraded to CDH 5.10, which has Spark 1.6.0, Hadoop 2.6.0, Hive 1.1.0, and Oozie 4.1.0.\n\nWe are having trouble running Spark jobs that use HiveContext from Oozie.  They run perfectly fine from the CLI with spark-submit, just not in Oozie.  We aren't certain that HiveContext is related, but we can reproduce regularly with a job that uses HiveContext.\n\nAnyway, I post this here, because the error we are getting is the same that started this issue:\n\n{code}class \"javax.servlet.FilterRegistration\"'s signer information does not match signer information of other classes in the same package{code}\n\nI've noticed that the Oozie sharelib includes javax.servlet-3.0.0.v201112011016.jar.  I also see that spark-assembly.jar includes a javax.servlet.FilterRegistration class, although its hard for me to tell which version.  The jetty pom.xml files in spark-assembly.jar seem to say {{javax.servlet.\\*;version=\"2.6.0\"}}, but I'm a little green on how all these dependencies get resolved.  I don't see any javax.servlet .jars in any of /usr/lib/hadoop* (where CDH installs hadoop jars).\n\nHelp!  :)  If this is not related to this issue, I'll open a new one.\n"], "derived": {"summary": "{code}mvn test -Pyarn -Dhadoop. version=2.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Dependent on multiple versions of servlet-api jars lead to throw an SecurityException when Spark built for hadoop 2.3.0 , 2.4.0  - {code}mvn test -Pyarn -Dhadoop. version=2."}, {"q": "What updates or decisions were made in the discussion?", "a": "We just upgraded to CDH 5.10, which has Spark 1.6.0, Hadoop 2.6.0, Hive 1.1.0, and Oozie 4.1.0.\n\nWe are having trouble running Spark jobs that use HiveContext from Oozie.  They run perfectly fine from the CLI with spark-submit, just not in Oozie.  We aren't certain that HiveContext is related, but we can reproduce regularly with a job that uses HiveContext.\n\nAnyway, I post this here, because the error we are getting is the same that started this issue:\n\n{code}class \"javax.servlet.FilterRegistration\"'s signer information does not match signer information of other classes in the same package{code}\n\nI've noticed that the Oozie sharelib includes javax.servlet-3.0.0.v201112011016.jar.  I also see that spark-assembly.jar includes a javax.servlet.FilterRegistration class, although its hard for me to tell which version.  The jetty pom.xml files in spark-assembly.jar seem to say {{javax.servlet.\\*;version=\"2.6.0\"}}, but I'm a little green on how all these dependencies get resolved.  I don't see any javax.servlet .jars in any of /usr/lib/hadoop* (where CDH installs hadoop jars).\n\nHelp!  :)  If this is not related to this issue, I'll open a new one."}]}}
{"project": "SPARK", "issue_id": "SPARK-1694", "title": "Simplify ColumnBuilder/Accessor class hierarchy", "status": "Resolved", "priority": "Minor", "reporter": "Cheng Lian", "assignee": "Cheng Lian", "labels": [], "created": "2014-05-01T15:46:40.000+0000", "updated": "2014-09-17T18:29:56.000+0000", "description": "Current {{ColumnBuilder/Accessor}} class hierarchy design was largely refactored from the in-memory columnar storage component of Shark. Code related to null values and compression were factored into {{NullableColumnBuilder/Accessor}} and {{CompressibleColumnBuilder/Accessor}} and then mixed in as stackable traits. The drawback is:\n\n# Interactions among these classes were unnecessarily complicated and error prone.\n# Flexibility provided by this design now seems useless\n\nTo simplify this, we can merge {{CompressibleColumnBuilder/Accessor}} and {{NullableColumnBuilder/Accessor}} into {{NativeColumnBuilder/Accessor}}, simply hard code null value processing and compression logic together.", "comments": [], "derived": {"summary": "Current {{ColumnBuilder/Accessor}} class hierarchy design was largely refactored from the in-memory columnar storage component of Shark. Code related to null values and compression were factored into {{NullableColumnBuilder/Accessor}} and {{CompressibleColumnBuilder/Accessor}} and then mixed in as stackable traits.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Simplify ColumnBuilder/Accessor class hierarchy - Current {{ColumnBuilder/Accessor}} class hierarchy design was largely refactored from the in-memory columnar storage component of Shark. Code related to null values and compression were factored into {{NullableColumnBuilder/Accessor}} and {{CompressibleColumnBuilder/Accessor}} and then mixed in as stackable traits."}]}}
{"project": "SPARK", "issue_id": "SPARK-1695", "title": "java8-tests compiler error: package com.google.common.collections does not exist", "status": "Resolved", "priority": "Major", "reporter": "Guoqiang Li", "assignee": "Guoqiang Li", "labels": [], "created": "2014-05-01T15:51:21.000+0000", "updated": "2014-05-02T19:41:00.000+0000", "description": null, "comments": ["Issue resolved by pull request 611\n[https://github.com/apache/spark/pull/611]"], "derived": {"summary": "", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "java8-tests compiler error: package com.google.common.collections does not exist"}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 611\n[https://github.com/apache/spark/pull/611]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1696", "title": "RowMatrix.dspr is not using parameter alpha for DenseVector", "status": "Resolved", "priority": "Minor", "reporter": "Anish Patel", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-05-01T19:08:01.000+0000", "updated": "2014-05-15T00:21:56.000+0000", "description": "In the master branch, method dspr of RowMatrix takes parameter alpha, but does not use it when given a DenseVector.\n\nThis probably slid by because when method computeGramianMatrix calls dspr, it provides an alpha value of 1.0.", "comments": ["Thanks! I sent a PR: https://github.com/apache/spark/pull/778"], "derived": {"summary": "In the master branch, method dspr of RowMatrix takes parameter alpha, but does not use it when given a DenseVector. This probably slid by because when method computeGramianMatrix calls dspr, it provides an alpha value of 1.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "RowMatrix.dspr is not using parameter alpha for DenseVector - In the master branch, method dspr of RowMatrix takes parameter alpha, but does not use it when given a DenseVector. This probably slid by because when method computeGramianMatrix calls dspr, it provides an alpha value of 1."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks! I sent a PR: https://github.com/apache/spark/pull/778"}]}}
{"project": "SPARK", "issue_id": "SPARK-1697", "title": "Driver error org.apache.spark.scheduler.TaskSetManager - Loss was due to java.io.FileNotFoundException", "status": "Resolved", "priority": "Major", "reporter": "Arup Malakar", "assignee": null, "labels": [], "created": "2014-05-01T20:23:27.000+0000", "updated": "2020-05-17T17:48:29.000+0000", "description": "We are running spark-streaming 0.9.0 on top of Yarn (Hadoop 2.2.0-cdh5.0.0-beta-2). It reads from kafka and processes the data. So far we haven't seen any issues, except today we saw an exception in the driver log and it is not consuming kafka messages any more. \n\nHere is the exception we saw:\n\n{code}\n\n2014-05-01 10:00:43,962 [Result resolver thread-3] WARN  org.apache.spark.scheduler.TaskSetManager - Loss was due to java.io.FileNotFoundException\njava.io.FileNotFoundException: http://10.50.40.85:53055/broadcast_2412\n\tat sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1624)\n\tat org.apache.spark.broadcast.HttpBroadcast$.read(HttpBroadcast.scala:156)\n\tat org.apache.spark.broadcast.HttpBroadcast.readObject(HttpBroadcast.scala:56)\n\tat sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)\n\tat scala.collection.immutable.$colon$colon.readObject(List.scala:362)\n\tat sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)\n\tat scala.collection.immutable.$colon$colon.readObject(List.scala:362)\n\tat sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)\n\tat scala.collection.immutable.$colon$colon.readObject(List.scala:362)\n\tat sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)\n\tat scala.collection.immutable.$colon$colon.readObject(List.scala:362)\n\tat sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)\n\tat scala.collection.immutable.$colon$colon.readObject(List.scala:362)\n\tat sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)\n\tat scala.collection.immutable.$colon$colon.readObject(List.scala:362)\n\tat sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)\n\tat scala.collection.immutable.$colon$colon.readObject(List.scala:362)\n\tat sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)\n\tat scala.collection.immutable.$colon$colon.readObject(List.scala:362)\n\tat sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)\n\tat scala.collection.immutable.$colon$colon.readObject(List.scala:362)\n\tat sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:40)\n\tat org.apache.spark.scheduler.ResultTask$.deserializeInfo(ResultTask.scala:63)\n\tat org.apache.spark.scheduler.ResultTask.readExternal(ResultTask.scala:139)\n\tat java.io.ObjectInputStream.readExternalData(ObjectInputStream.java:1837)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:40)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:62)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:195)\n\tat org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\n2014-05-01 10:00:43,963 [Result resolver thread-1] WARN  org.apache.spark.scheduler.TaskSetManager - Lost TID 468814 (task 4827.0:12)\n2014-05-01 10:00:43,973 [Result resolver thread-0] WARN  org.apache.spark.scheduler.TaskSetManager - Lost TID 468815 (task 4827.0:11)\n2014-05-01 10:00:43,985 [Result resolver thread-2] WARN  org.apache.spark.scheduler.TaskSetManager - Lost TID 468816 (task 4827.0:12)\n2014-05-01 10:00:43,996 [Result resolver thread-3] WARN  org.apache.spark.scheduler.TaskSetManager - Lost TID 468817 (task 4827.0:11)\n2014-05-01 10:00:44,006 [Result resolver thread-1] WARN  org.apache.spark.scheduler.TaskSetManager - Lost TID 468818 (task 4827.0:12)\n2014-05-01 10:00:44,027 [Result resolver thread-0] WARN  org.apache.spark.scheduler.TaskSetManager - Lost TID 468819 (task 4827.0:11)\n2014-05-01 10:00:44,029 [Result resolver thread-0] ERROR org.apache.spark.scheduler.TaskSetManager - Task 4827.0:11 failed 4 times; aborting job\n{code}\n\nI would be happy to provide more details if required.", "comments": ["I would suspect this is due to the cleaner thread kicking out 'old' broadcast variables - except some old broadcast variables (like conf from SparkContext) must never be cleared.\nNow that [~tdas]'s patch for gc'ing spark data is in; you could try with simply disabling all timeout based cleaner's in spark.", "Thanks for the info [~mridulm80]. Few questions:\n\n1. Which version is the GC code part of (we are using 0.9.0 but could move to 0.9.1 if required)?\n2. How do we disable the timeout based cleaners?\n", "[~mridulm80] We saw this issue again. Are you referring to SPARK-1592 GC patch? That would require us to upgrade to trunk right? Also could you advice me on how to disable the cleaners?\n", "This is either stale, or likely the same issue identified in SPARK-2243"], "derived": {"summary": "We are running spark-streaming 0. 9.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Driver error org.apache.spark.scheduler.TaskSetManager - Loss was due to java.io.FileNotFoundException - We are running spark-streaming 0. 9."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is either stale, or likely the same issue identified in SPARK-2243"}]}}
{"project": "SPARK", "issue_id": "SPARK-1698", "title": "Improve spark integration", "status": "Closed", "priority": "Major", "reporter": "Guoqiang Li", "assignee": "Guoqiang Li", "labels": [], "created": "2014-05-02T07:30:32.000+0000", "updated": "2014-06-22T03:02:09.000+0000", "description": "Use the shade plugin to create a big JAR with all the dependencies can cause a few problems\n1. Missing jar's meta information\n2. Some file is covered, eg: plugin.xml\n3. Different versions of a jar may co-exist\n4. Too big, java 6 does not support\n", "comments": ["[~srowen]\nAbout [SPARK-1681|https://issues.apache.org/jira/browse/SPARK-1681] there is only one solution: The datanucleus jars is added to the CLASSPATH.\nWell,there may be other better solution, but I didn't find it\n\nI disagree with [PR 610|https://github.com/apache/spark/pull/610],It's not perfect.\n\n[The PR 598|https://github.com/apache/spark/pull/598] reference [HADOOP-7939|https://issues.apache.org/jira/browse/HADOOP-7939],I think that is better.\n\nThere is [another solution|https://github.com/witgo/spark/tree/standalone] reference [Packaging a multimodule maven Spring app in a standalone jar|http://ath3nd.wordpress.com/2013/12/25/packaging-a-multimodule-maven-spring-app-in-a-standalone-jar/].\nBut this involves [SI-6660 REPL: load transitive dependencies of JARs on classpath|https://issues.scala-lang.org/browse/SI-6660]\n", "(Copying an earlier comment that went to the mailing list, but didn't make it here:)\n\n#1 and #2 are not relevant the issue of jar size. These can be problems in general, but don't think there have been issues attributable to file clashes. Shading has mechanisms to deal with this anyway.\n\n#3 is a problem in general too, but is not specific to shading. Where versions collide, build processes like Maven and shading must be used to resolve them. But this happens regardless of whether you shade a fat jar.\n\n#4 is a real problem specific to Java 6. It does seem like it will be important to identify and remove more unnecessary dependencies to work around it.\n\nBut shading per se is not the problem, and it is important to make a packaged jar for the app. What are you proposing? Dependencies to be removed?", "What is the suggested change in this particular JIRA? I saw the PR, which seems to replace the shade with assembly plugin. Given the reference to https://issues.scala-lang.org/browse/SI-6660 are you suggesting that your assembly change packages differently, by putting jars in jars? Yes, the issue you link to is exactly the kind of problem that can occur with this approach. It comes up a bit in Hadoop as well. Even though it is in theory a fine way to do things. But is that what you're getting at?", "[~srowen]\nIn [The PR 598|https://github.com/apache/spark/pull/598] #1,#2,#4 do not occur and #3 is very easy to find.\nThe directory structure of spark similar to hadoop 2.3.0.\nThere are three subcomponents: core,examples,hive,The directory structure of spark: \n{code}\n+- SPARK_HOME\n|  +- bin\n|     \\- *\n|  +- sbin\n|     \\- *\n|  +- RELEASE\n|  +- conf\n|     \\- *\n|  +- python\n|     \\- *\n|  \\- share\n|     \\- spark\n|        +- core\n|           +- lib\n|              \\- *.jar\n|           +- spark-core*.jar\n|           +- spark-repl*.jar\n|           +- spark-yarn*.jar\n|           +- spark-bagel*.jar\n|           +- spark-graphx*.jar\n|           +- spark-sql*.jar\n|           +- spark-catalyst*.jar\n|           +- spark-mllib*.jar\n|           \\- spark-streaming*.jar\n|        +- hive\n|           +- lib\n|              \\- *.jar\n|           \\- spark-hive*.jar\n|        +- examples\n|           +- lib\n|              \\- *.jar\n|           \\- spark-examples*.jar\n{code}", "Can this be closed? the child tasks are closed and I don't know whether there is a remaining change to act on."], "derived": {"summary": "Use the shade plugin to create a big JAR with all the dependencies can cause a few problems\n1. Missing jar's meta information\n2.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Improve spark integration - Use the shade plugin to create a big JAR with all the dependencies can cause a few problems\n1. Missing jar's meta information\n2."}, {"q": "What updates or decisions were made in the discussion?", "a": "Can this be closed? the child tasks are closed and I don't know whether there is a remaining change to act on."}]}}
{"project": "SPARK", "issue_id": "SPARK-1699", "title": "Python relative should be independence from the core, becomes subprojects", "status": "Closed", "priority": "Major", "reporter": "Guoqiang Li", "assignee": "Guoqiang Li", "labels": [], "created": "2014-05-02T18:22:24.000+0000", "updated": "2014-06-11T16:25:02.000+0000", "description": "Python is not required in some cases.Refer to the spark-hive", "comments": ["[The PR 624 |https://github.com/apache/spark/pull/624]"], "derived": {"summary": "Python is not required in some cases. Refer to the spark-hive.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Python relative should be independence from the core, becomes subprojects - Python is not required in some cases. Refer to the spark-hive."}, {"q": "What updates or decisions were made in the discussion?", "a": "[The PR 624 |https://github.com/apache/spark/pull/624]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1700", "title": "PythonRDD leaks socket descriptors during cancellation", "status": "Resolved", "priority": "Major", "reporter": "Aaron Davidson", "assignee": "Aaron Davidson", "labels": [], "created": "2014-05-02T22:11:42.000+0000", "updated": "2014-12-11T20:32:38.000+0000", "description": "Sockets from Spark to Python workers are not cleaned up over the duration of a job, causing the total number of opened file descriptors to grow to around the number of partitions in the job. Usually these go away if the job is successful, but in the case of cancellation (and possibly exceptions, though I haven't investigated), the socket file descriptors remain indefinitely.", "comments": ["The PR was https://github.com/apache/spark/pull/623 and says it was merged in 1.0."], "derived": {"summary": "Sockets from Spark to Python workers are not cleaned up over the duration of a job, causing the total number of opened file descriptors to grow to around the number of partitions in the job. Usually these go away if the job is successful, but in the case of cancellation (and possibly exceptions, though I haven't investigated), the socket file descriptors remain indefinitely.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "PythonRDD leaks socket descriptors during cancellation - Sockets from Spark to Python workers are not cleaned up over the duration of a job, causing the total number of opened file descriptors to grow to around the number of partitions in the job. Usually these go away if the job is successful, but in the case of cancellation (and possibly exceptions, though I haven't investigated), the socket file descriptors remain indefinitely."}, {"q": "What updates or decisions were made in the discussion?", "a": "The PR was https://github.com/apache/spark/pull/623 and says it was merged in 1.0."}]}}
{"project": "SPARK", "issue_id": "SPARK-1701", "title": "Inconsistent naming: \"slice\" or \"partition\"", "status": "Resolved", "priority": "Minor", "reporter": "Daniel Darabos", "assignee": "Thomas Graves", "labels": ["starter"], "created": "2014-05-02T23:16:21.000+0000", "updated": "2015-04-13T21:40:40.000+0000", "description": "Throughout the documentation and code \"slice\" and \"partition\" are used interchangeably. (Or so it seems to me.) It would avoid some confusion for new users to settle on one name. I think \"partition\" is winning, since that is the name of the class representing the concept.\n\nThis should not be much more complicated to do than a search & replace. I can take a stab at it, if you agree.", "comments": ["Some examples are mentioned in http://stackoverflow.com/questions/23436640/what-is-the-different-between-an-rdd-partition-and-a-slice-in-apache-spark", "In addition to \"slice\" and \"partition\", we also have \"task\".\n\nHere's an example from Spark 1.0.2:\n* PySpark {{reduceByKey}} [API documentation|http://spark.apache.org/docs/1.0.2/api/python/pyspark.rdd.RDD-class.html#reduceByKey] says {{numPartitions}}\n* [Spark Programming Guide|http://spark.apache.org/docs/1.0.2/programming-guide.html#transformations] calls the same thing {{numTasks}}", "[~pwendell] and [~rxin], I'm pinging you here to put this issue on your radar. \n\nWith some guidance on what terms are favored and how a solution should be approached (e.g. can parameter names can be changed?), I'm sure a contributor could pick up this task.", "\"Partition\" is the standard term. ", "I think in the current code \"slices\" are just a (probably redundant) synonym for \"partitions\". We could probably clean it up a bit, for instance in example code and the programming guide. I think in those cases it's best to just say \"partitions\".\n\nThe number of partitions and the number of tasks are distinct concepts in general, but in the cases you listed either applies, and I think that saying \"number of tasks\" might be slightly more understandable for users.", "OK, so it sounds like action is:\n* Replace all occurrences of \"slice\" with \"partition\"\n* Leave occurrences of \"task\" as-is\n\n[~darabos] - Feel free to run with this.", "I think that's a straw man. A closer review once there is a patch\nmight reveal some corner cases.\n\nOn Mon, Sep 1, 2014 at 10:59 PM, Nicholas Chammas (JIRA)\n", "Oh absolutely; sorry, didn't mean to mischaracterize your recommendations. \n\nI think this simplistic summary should help direct an initial PR which can then be reviewed in more detail.", "slice vs partition has also come up on stackoverflow and just recently the user list.\n\ni'm going to write up a patch for the programming-guide to at least clarify the situation.\n\ni intend my pr to partially address this jira.", "User 'mattf' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/2299", "User 'mattf' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/2302", "User 'mattf' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/2303", "User 'mattf' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/2304", "User 'mattf' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/2305", "Issue resolved by pull request 2304\n[https://github.com/apache/spark/pull/2304]", "[~tgraves] - Shouldn't this issue be assigned to [~farrellee]?"], "derived": {"summary": "Throughout the documentation and code \"slice\" and \"partition\" are used interchangeably. (Or so it seems to me.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Inconsistent naming: \"slice\" or \"partition\" - Throughout the documentation and code \"slice\" and \"partition\" are used interchangeably. (Or so it seems to me."}, {"q": "What updates or decisions were made in the discussion?", "a": "[~tgraves] - Shouldn't this issue be assigned to [~farrellee]?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1702", "title": "Mesos executor won't start because of a ClassNotFoundException", "status": "Resolved", "priority": "Major", "reporter": "Bouke van der Bijl", "assignee": null, "labels": ["executors", "mesos", "spark"], "created": "2014-05-02T23:21:51.000+0000", "updated": "2015-05-15T13:48:18.000+0000", "description": "Some discussion here: http://apache-spark-user-list.1001560.n3.nabble.com/java-lang-ClassNotFoundException-spark-on-mesos-td3510.html\n\nFix here (which is probably not the right fix): https://github.com/apache/spark/pull/620\n\nThis was broken in v0.9.0, was fixed in v0.9.1 and is now broken again.\n\nError in Mesos executor stderr:\n\nWARNING: Logging before InitGoogleLogging() is written to STDERR\nI0502 17:31:42.672224 14688 exec.cpp:131] Version: 0.18.0\nI0502 17:31:42.674959 14707 exec.cpp:205] Executor registered on slave 20140501-182306-16842879-5050-10155-0\n14/05/02 17:31:42 INFO MesosExecutorBackend: Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n14/05/02 17:31:42 INFO MesosExecutorBackend: Registered with Mesos as executor ID 20140501-182306-16842879-5050-10155-0\n14/05/02 17:31:43 INFO SecurityManager: Changing view acls to: vagrant\n14/05/02 17:31:43 INFO SecurityManager: SecurityManager, is authentication enabled: false are ui acls enabled: false users with view permissions: Set(vagrant)\n14/05/02 17:31:43 INFO Slf4jLogger: Slf4jLogger started\n14/05/02 17:31:43 INFO Remoting: Starting remoting\n14/05/02 17:31:43 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://spark@localhost:50843]\n14/05/02 17:31:43 INFO Remoting: Remoting now listens on addresses: [akka.tcp://spark@localhost:50843]\njava.lang.ClassNotFoundException: org/apache/spark/serializer/JavaSerializer\n    at java.lang.Class.forName0(Native Method)\n    at java.lang.Class.forName(Class.java:270)\n    at org.apache.spark.SparkEnv$.instantiateClass$1(SparkEnv.scala:165)\n    at org.apache.spark.SparkEnv$.create(SparkEnv.scala:176)\n    at org.apache.spark.executor.Executor.<init>(Executor.scala:106)\n    at org.apache.spark.executor.MesosExecutorBackend.registered(MesosExecutorBackend.scala:56)\nException in thread \"Thread-0\" I0502 17:31:43.710039 14707 exec.cpp:412] Deactivating the executor libprocess\n\n\nThe problem is that it can't find the class. ", "comments": ["The PR is merged and closed already, is this still an issue?", "I don't believe it is an issue, but there appears to be a number of stale tickets that are not being cleaned up. ", "I met this on spak 1.3.0 + mesos 0.21.1 with \"run-example SparkPi\"\n\nException in thread \"main\" java.lang.NoClassDefFoundError: org/apache/spark/executor/MesosExecutorBackend\nCaused by: java.lang.ClassNotFoundException: org.apache.spark.executor.MesosExecutorBackend\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:217)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:205)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:321)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:266)\nCould not find the main class: org.apache.spark.executor.MesosExecutorBackend"], "derived": {"summary": "Some discussion here: http://apache-spark-user-list. 1001560.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Mesos executor won't start because of a ClassNotFoundException - Some discussion here: http://apache-spark-user-list. 1001560."}, {"q": "What updates or decisions were made in the discussion?", "a": "I met this on spak 1.3.0 + mesos 0.21.1 with \"run-example SparkPi\"\n\nException in thread \"main\" java.lang.NoClassDefFoundError: org/apache/spark/executor/MesosExecutorBackend\nCaused by: java.lang.ClassNotFoundException: org.apache.spark.executor.MesosExecutorBackend\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:217)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:205)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:321)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:266)\nCould not find the main class: org.apache.spark.executor.MesosExecutorBackend"}]}}
{"project": "SPARK", "issue_id": "SPARK-1703", "title": "Warn users if Spark is run on JRE6 but compiled with JDK7", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "labels": [], "created": "2014-05-03T06:59:30.000+0000", "updated": "2015-03-03T14:40:18.000+0000", "description": "Right now since the JRE silently swallows the invalid jar, it will produce really confusing behavior if users hit this. We should check if we are in this situation (either in spark-class or compute-classpath) and fail with an explicit error.\n\nWe can do something like:\n{code}\n$JAVA_HOME/bin/jar -tf lib/spark-assembly-1.0.0-SNAPSHOT-hadoop1.0.4.jar org/apache/spark/SparkContext\n{code}\n\nWhich, when a user is running with JRE 6 and a JDK-7-compiled jar will produce:\n{code}\njava.util.zip.ZipException: invalid CEN header (bad signature)\n\tat java.util.zip.ZipFile.open(Native Method)\n\tat java.util.zip.ZipFile.<init>(ZipFile.java:132)\n\tat java.util.zip.ZipFile.<init>(ZipFile.java:93)\n\tat sun.tools.jar.Main.list(Main.java:997)\n\tat sun.tools.jar.Main.run(Main.java:242)\n\tat sun.tools.jar.Main.main(Main.java:1167)\n{code}\n", "comments": ["Issue resolved by pull request 627\n[https://github.com/apache/spark/pull/627]"], "derived": {"summary": "Right now since the JRE silently swallows the invalid jar, it will produce really confusing behavior if users hit this. We should check if we are in this situation (either in spark-class or compute-classpath) and fail with an explicit error.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Warn users if Spark is run on JRE6 but compiled with JDK7 - Right now since the JRE silently swallows the invalid jar, it will produce really confusing behavior if users hit this. We should check if we are in this situation (either in spark-class or compute-classpath) and fail with an explicit error."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 627\n[https://github.com/apache/spark/pull/627]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1704", "title": "Support EXPLAIN in Spark SQL", "status": "Resolved", "priority": "Major", "reporter": "Yangjp", "assignee": "Zongheng Yang", "labels": ["sql"], "created": "2014-05-03T14:51:16.000+0000", "updated": "2014-11-03T07:22:50.000+0000", "description": "14/05/03 22:08:40 INFO ParseDriver: Parsing command: explain select * from src\n14/05/03 22:08:40 INFO ParseDriver: Parse Completed\n14/05/03 22:08:40 WARN LoggingFilter: EXCEPTION :\njava.lang.AssertionError: assertion failed: No plan for ExplainCommand (Project [*])\n\n        at scala.Predef$.assert(Predef.scala:179)\n        at org.apache.spark.sql.catalyst.planning.QueryPlanner.apply(QueryPlanner.scala:59)\n        at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:263)\n        at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:263)\n        at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:264)\n        at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:264)\n        at org.apache.spark.sql.hive.HiveContext$QueryExecution.toRdd$lzycompute(HiveContext.scala:260)\n        at org.apache.spark.sql.hive.HiveContext$QueryExecution.toRdd(HiveContext.scala:248)\n        at org.apache.spark.sql.hive.api.java.JavaHiveContext.hql(JavaHiveContext.scala:39)\n        at org.apache.spark.examples.TimeServerHandler.messageReceived(TimeServerHandler.java:72)\n        at org.apache.mina.core.filterchain.DefaultIoFilterChain$TailFilter.messageReceived(DefaultIoFilterChain.java:690)\n        at org.apache.mina.core.filterchain.DefaultIoFilterChain.callNextMessageReceived(DefaultIoFilterChain.java:417)\n        at org.apache.mina.core.filterchain.DefaultIoFilterChain.access$1200(DefaultIoFilterChain.java:47)\n        at org.apache.mina.core.filterchain.DefaultIoFilterChain$EntryImpl$1.messageReceived(DefaultIoFilterChain.java:765)\n        at org.apache.mina.filter.codec.ProtocolCodecFilter$ProtocolDecoderOutputImpl.flush(ProtocolCodecFilter.java:407)\n        at org.apache.mina.filter.codec.ProtocolCodecFilter.messageReceived(ProtocolCodecFilter.java:236)\n        at org.apache.mina.core.filterchain.DefaultIoFilterChain.callNextMessageReceived(DefaultIoFilterChain.java:417)\n        at org.apache.mina.core.filterchain.DefaultIoFilterChain.access$1200(DefaultIoFilterChain.java:47)\n        at org.apache.mina.core.filterchain.DefaultIoFilterChain$EntryImpl$1.messageReceived(DefaultIoFilterChain.java:765)\n        at org.apache.mina.filter.logging.LoggingFilter.messageReceived(LoggingFilter.java:208)\n        at org.apache.mina.core.filterchain.DefaultIoFilterChain.callNextMessageReceived(DefaultIoFilterChain.java:417)\n        at org.apache.mina.core.filterchain.DefaultIoFilterChain.access$1200(DefaultIoFilterChain.java:47)\n        at org.apache.mina.core.filterchain.DefaultIoFilterChain$EntryImpl$1.messageReceived(DefaultIoFilterChain.java:765)\n        at org.apache.mina.core.filterchain.IoFilterAdapter.messageReceived(IoFilterAdapter.java:109)\n        at org.apache.mina.core.filterchain.DefaultIoFilterChain.callNextMessageReceived(DefaultIoFilterChain.java:417)\n        at org.apache.mina.core.filterchain.DefaultIoFilterChain.fireMessageReceived(DefaultIoFilterChain.java:410)\n        at org.apache.mina.core.polling.AbstractPollingIoProcessor.read(AbstractPollingIoProcessor.java:710)\n        at org.apache.mina.core.polling.AbstractPollingIoProcessor.process(AbstractPollingIoProcessor.java:664)\n        at org.apache.mina.core.polling.AbstractPollingIoProcessor.process(AbstractPollingIoProcessor.java:653)\n        at org.apache.mina.core.polling.AbstractPollingIoProcessor.access$600(AbstractPollingIoProcessor.java:67)\n        at org.apache.mina.core.polling.AbstractPollingIoProcessor$Processor.run(AbstractPollingIoProcessor.java:1124)\n        at org.apache.mina.util.NamePreservingRunnable.run(NamePreservingRunnable.java:64)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:701)", "comments": ["Thanks for reporting this.  Looks like we need to special case explain queries in the various SQL contexts.", "Explain should probably just print out the sql query plan in Spark SQL instead of delegating to Hive ...", "Just so I understand the desirable output for Explain: sql(\"EXPLAIN SELECT * FROM src\") should return a SchemaRDD, on which when .collect() is called returns:\n\n== Query Plan ==\nHiveTableScan [key#0,value#1], (MetastoreRelation default, src, None), None\n\nwhere each Row corresponds to one line of the description. Does this sound good?", "Github pull request: https://github.com/apache/spark/pull/1003", "User 'concretevitamin' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1003"], "derived": {"summary": "14/05/03 22:08:40 INFO ParseDriver: Parsing command: explain select * from src\n14/05/03 22:08:40 INFO ParseDriver: Parse Completed\n14/05/03 22:08:40 WARN LoggingFilter: EXCEPTION :\njava. lang.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Support EXPLAIN in Spark SQL - 14/05/03 22:08:40 INFO ParseDriver: Parsing command: explain select * from src\n14/05/03 22:08:40 INFO ParseDriver: Parse Completed\n14/05/03 22:08:40 WARN LoggingFilter: EXCEPTION :\njava. lang."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'concretevitamin' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1003"}]}}
{"project": "SPARK", "issue_id": "SPARK-1705", "title": "Allow multiple instances per node with SPARK-EC2", "status": "Resolved", "priority": "Major", "reporter": "Allan Douglas R. de Oliveira", "assignee": "Allan Douglas R. de Oliveira", "labels": [], "created": "2014-05-03T21:35:12.000+0000", "updated": "2014-05-03T23:54:13.000+0000", "description": "SPARK_WORKER_INSTANCES should be properly configurable when using the EC2 scripts.", "comments": ["Here is a pull-request:\nhttps://github.com/apache/spark/pull/612", "Issue resolved by pull request 612\n[https://github.com/apache/spark/pull/612]"], "derived": {"summary": "SPARK_WORKER_INSTANCES should be properly configurable when using the EC2 scripts.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Allow multiple instances per node with SPARK-EC2 - SPARK_WORKER_INSTANCES should be properly configurable when using the EC2 scripts."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 612\n[https://github.com/apache/spark/pull/612]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1706", "title": "Allow multiple executors per worker in Standalone mode", "status": "Closed", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Nan Zhu", "labels": [], "created": "2014-05-03T21:53:01.000+0000", "updated": "2015-04-14T20:32:31.000+0000", "description": "Right now if people want to launch multiple executors on each machine they need to start multiple standalone workers. This is not too difficult, but it means you have extra JVM's sitting around.\n\nWe should just allow users to set a number of cores they want per-executor in standalone mode and then allow packing multiple executors on each node. This would make standalone mode more consistent with YARN in the way you request resources.\n\nIt's not too big of a change as far as I can see. You'd need to:\n\n1. Introduce a configuration for how many cores you want per executor.\n2. Change the scheduling logic in Master.scala to take this into account.\n3. Change CoarseGrainedSchedulerBackend to not assume a 1<->1 correspondence between hosts and executors.\n\nAnd maybe modify a few other places.\n", "comments": ["Oh my, this was supposed to be logical addition once yarn changes were done.\nYarn changes were very heavily modelled on standalone mode (hence why yarn-standalone !) : and it was supposed to be a two way street : changes made for yarn support (multi-tennancy, etc) was supposed to have been added back to standalone mode when yarn support stabilized.\nDid not realize I never got around to it - my apologies !", "made the PR: https://github.com/apache/spark/pull/636", "new PR: https://github.com/apache/spark/pull/731", "Oh, the PR has been there for a while....I just rebased it, anyone wants to have a look ?"], "derived": {"summary": "Right now if people want to launch multiple executors on each machine they need to start multiple standalone workers. This is not too difficult, but it means you have extra JVM's sitting around.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Allow multiple executors per worker in Standalone mode - Right now if people want to launch multiple executors on each machine they need to start multiple standalone workers. This is not too difficult, but it means you have extra JVM's sitting around."}, {"q": "What updates or decisions were made in the discussion?", "a": "Oh, the PR has been there for a while....I just rebased it, anyone wants to have a look ?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1707", "title": "Remove 3 second sleep before starting app on YARN", "status": "Resolved", "priority": "Major", "reporter": "Sandy Ryza", "assignee": "Sandy Ryza", "labels": [], "created": "2014-05-03T23:51:05.000+0000", "updated": "2014-07-21T18:16:02.000+0000", "description": "After acquiring allocations from YARN and launching containers, Spark currently waits for 3 seconds for executors to connect to the driver.  On Spark standalone, nothing like this happens.\n\nI'm wondering whether we can just remove this sleep entirely.  Is there a reason I'm missing why YARN is different than standalone in this regard?  At the least we could do something smarter like wait until all executors have registered.", "comments": ["[~tgraves] [~mridulm@yahoo-inc.com] any thoughts on this?", "https://github.com/apache/spark/pull/634", "See discussions on SPARK-1453."], "derived": {"summary": "After acquiring allocations from YARN and launching containers, Spark currently waits for 3 seconds for executors to connect to the driver. On Spark standalone, nothing like this happens.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Remove 3 second sleep before starting app on YARN - After acquiring allocations from YARN and launching containers, Spark currently waits for 3 seconds for executors to connect to the driver. On Spark standalone, nothing like this happens."}, {"q": "What updates or decisions were made in the discussion?", "a": "See discussions on SPARK-1453."}]}}
{"project": "SPARK", "issue_id": "SPARK-1708", "title": "Add ClassTag parameter on accumulator and broadcast methods", "status": "Resolved", "priority": "Blocker", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "labels": [], "created": "2014-05-04T00:26:13.000+0000", "updated": "2014-05-10T19:11:22.000+0000", "description": "ClassTags will be needed by some serializers, such as a Scala Pickling based one, to come up with efficient serialization. We need to add them on Broadcast and probably also Accumulator and Accumulable. Since we're freezing the public API in 1.0 we have to do this before the release.", "comments": ["Issue resolved by pull request 700\n[https://github.com/apache/spark/pull/700]"], "derived": {"summary": "ClassTags will be needed by some serializers, such as a Scala Pickling based one, to come up with efficient serialization. We need to add them on Broadcast and probably also Accumulator and Accumulable.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add ClassTag parameter on accumulator and broadcast methods - ClassTags will be needed by some serializers, such as a Scala Pickling based one, to come up with efficient serialization. We need to add them on Broadcast and probably also Accumulator and Accumulable."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 700\n[https://github.com/apache/spark/pull/700]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1709", "title": "spark-submit should use \"main class\" attribute of JAR if no --class is given", "status": "Resolved", "priority": "Minor", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "labels": ["Starter"], "created": "2014-05-04T01:17:03.000+0000", "updated": "2014-05-06T22:13:23.000+0000", "description": "Right now it always asks you to specify --class", "comments": ["Sorry Sandeep, I actually have a patch done for this that I'll post soon (see this branch: https://github.com/mateiz/spark/compare/py-submit)", "Should've assigned it to myself earlier.", "Sure no problem. I haven't done much work on it.", "Fixed in https://github.com/apache/spark/pull/664"], "derived": {"summary": "Right now it always asks you to specify --class.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "spark-submit should use \"main class\" attribute of JAR if no --class is given - Right now it always asks you to specify --class."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed in https://github.com/apache/spark/pull/664"}]}}
{"project": "SPARK", "issue_id": "SPARK-1710", "title": "spark-submit should print better errors than \"InvocationTargetException\"", "status": "Resolved", "priority": "Minor", "reporter": "Matei Alexandru Zaharia", "assignee": "Sandeep Singh", "labels": ["Starter"], "created": "2014-05-04T01:18:30.000+0000", "updated": "2014-05-05T03:52:18.000+0000", "description": "It's not horrible, but it is a bit confusing that exceptions in your driver program get hidden inside InvocationTargetException:\n\n{code}\nmatei@mbp-3:~/workspace/apache-spark$ bin/spark-submit --class SparkTest ../spark-test/target/scala-2.10/simple-project_2.10-1.0.jar \nException in thread \"main\" java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:256)\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:54)\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\nCaused by: org.apache.spark.SparkException: An application name must be set in your configuration\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:163)\n\tat SparkTest$.main(Test.scala:7)\n\tat SparkTest.main(Test.scala)\n\t... 7 more\n{code}\n\nIt would be better to print just the stack trace of the nested exception", "comments": ["https://github.com/apache/spark/pull/630", "Issue resolved by pull request 630\n[https://github.com/apache/spark/pull/630]"], "derived": {"summary": "It's not horrible, but it is a bit confusing that exceptions in your driver program get hidden inside InvocationTargetException:\n\n{code}\nmatei@mbp-3:~/workspace/apache-spark$ bin/spark-submit --class SparkTest. /spark-test/target/scala-2.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "spark-submit should print better errors than \"InvocationTargetException\" - It's not horrible, but it is a bit confusing that exceptions in your driver program get hidden inside InvocationTargetException:\n\n{code}\nmatei@mbp-3:~/workspace/apache-spark$ bin/spark-submit --class SparkTest. /spark-test/target/scala-2."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 630\n[https://github.com/apache/spark/pull/630]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1711", "title": "We should not use two build tools at the same time, should remove one of them", "status": "Closed", "priority": "Major", "reporter": "Guoqiang Li", "assignee": null, "labels": [], "created": "2014-05-04T14:21:07.000+0000", "updated": "2014-05-09T05:36:52.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "We should not use two build tools at the same time, should remove one of them"}]}}
{"project": "SPARK", "issue_id": "SPARK-1712", "title": "ParallelCollectionRDD operations hanging forever without any error messages ", "status": "Resolved", "priority": "Major", "reporter": "Piotr Kolaczkowski", "assignee": "Guoqiang Li", "labels": [], "created": "2014-05-04T19:14:52.000+0000", "updated": "2014-06-18T14:41:21.000+0000", "description": " conf/spark-defaults.conf\n{code}\nspark.akka.frameSize         5\nspark.default.parallelism    1\n{code}\n{noformat}\nscala> val collection = (1 to 1000000).map(i => (\"foo\" + i, i)).toVector\ncollection: Vector[(String, Int)] = Vector((foo1,1), (foo2,2), (foo3,3), (foo4,4), (foo5,5), (foo6,6), (foo7,7), (foo8,8), (foo9,9), (foo10,10), (foo11,11), (foo12,12), (foo13,13), (foo14,14), (foo15,15), (foo16,16), (foo17,17), (foo18,18), (foo19,19), (foo20,20), (foo21,21), (foo22,22), (foo23,23), (foo24,24), (foo25,25), (foo26,26), (foo27,27), (foo28,28), (foo29,29), (foo30,30), (foo31,31), (foo32,32), (foo33,33), (foo34,34), (foo35,35), (foo36,36), (foo37,37), (foo38,38), (foo39,39), (foo40,40), (foo41,41), (foo42,42), (foo43,43), (foo44,44), (foo45,45), (foo46,46), (foo47,47), (foo48,48), (foo49,49), (foo50,50), (foo51,51), (foo52,52), (foo53,53), (foo54,54), (foo55,55), (foo56,56), (foo57,57), (foo58,58), (foo59,59), (foo60,60), (foo61,61), (foo62,62), (foo63,63), (foo64,64), (foo...\n\nscala> val rdd = sc.parallelize(collection)\nrdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[0] at parallelize at <console>:24\n\nscala> rdd.first\nres4: (String, Int) = (foo1,1)\n\nscala> rdd.map(_._2).sum\n// nothing happens\n\n{noformat}\n\nCPU and I/O idle. \nMemory usage reported by JVM, after manually triggered GC:\nrepl: 216 MB / 2 GB\nexecutor: 67 MB / 2 GB\nworker: 6 MB / 128 MB\nmaster: 6 MB / 128 MB\n\nNo errors found in worker's stderr/stdout. \n\nIt works fine with 700,000 elements and then it takes about 1 second to process the request and calculate the sum. With 700,000 items the spark executor memory doesn't even exceed 300 MB out of 2GB available. It fails with 800,000 items.\n\nMultiple parralelized collections of size 700,000 items at the same time in the same session work fine.", "comments": ["The problem goes away if I increase parallelismLevel. \n{noformat}\nval rdd = sc.parallelize(collection, 64)\n{noformat}\n\nSo I guess this is something related to partition size.", "Unable to reproduce the bug.Can you upload the log?", "This is log from shell:\n\n{noformat}\nscala> val rdd = sc.parallelize(collection)\n14/05/05 21:23:16 DEBUG SparkILoop$SparkILoopInterpreter: parse(\"       val rdd = sc.parallelize(collection)\n\") Some(List(val rdd = sc.parallelize(collection)))\n14/05/05 21:23:16 DEBUG SparkILoop$SparkILoopInterpreter:   11: ValDef\n  11: TypeTree\n  31: Apply\n  20: Select\n  17: Ident\n  32: Ident\n\n14/05/05 21:23:16 DEBUG SparkILoop$SparkILoopInterpreter: parse(\"\nclass $read extends Serializable {\n  class $iwC extends Serializable {\nval $VAL2 = $line3.$read.INSTANCE;\nimport $VAL2.$iw.$iw.`sc`;\nclass $iwC extends Serializable {\nimport org.apache.spark.SparkContext._\nclass $iwC extends Serializable {\nclass $iwC extends Serializable {\nimport com.datastax.bdp.spark.CassandraFunctions._\nclass $iwC extends Serializable {\nimport com.datastax.bdp.spark.context.CassandraContext\nclass $iwC extends Serializable {\nimport com.tuplejump.calliope.Implicits._\nclass $iwC extends Serializable {\nval $VAL3 = $line17.$read.INSTANCE;\nimport $VAL3.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.`collection`;\nclass $iwC extends Serializable {\n       val rdd = sc.parallelize(collection)\n\n      \n\n}\nval $iw = new $iwC;\n}\nval $iw = new $iwC;\n}\nval $iw = new $iwC;\n}\nval $iw = new $iwC;\n}\nval $iw = new $iwC;\n}\nval $iw = new $iwC;\n}\nval $iw = new $iwC;\n}\nval $iw = new $iwC;\n\n}\nobject $read {\n  val INSTANCE = new $read();\n}\n\n\") Some(List(class $read extends Serializable {\n  def <init>() = {\n    super.<init>();\n    ()\n  };\n  class $iwC extends Serializable {\n    def <init>() = {\n      super.<init>();\n      ()\n    };\n    val $VAL2 = $line3.$read.INSTANCE;\n    import $VAL2.$iw.$iw.sc;\n    class $iwC extends Serializable {\n      def <init>() = {\n        super.<init>();\n        ()\n      };\n      import org.apache.spark.SparkContext._;\n      class $iwC extends Serializable {\n        def <init>() = {\n          super.<init>();\n          ()\n        };\n        class $iwC extends Serializable {\n          def <init>() = {\n            super.<init>();\n            ()\n          };\n          import com.datastax.bdp.spark.CassandraFunctions._;\n          class $iwC extends Serializable {\n            def <init>() = {\n              super.<init>();\n              ()\n            };\n            import com.datastax.bdp.spark.context.CassandraContext;\n            class $iwC extends Serializable {\n              def <init>() = {\n                super.<init>();\n                ()\n              };\n              import com.tuplejump.calliope.Implicits._;\n              class $iwC extends Serializable {\n                def <init>() = {\n                  super.<init>();\n                  ()\n                };\n                val $VAL3 = $line17.$read.INSTANCE;\n                import $VAL3.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.collection;\n                class $iwC extends Serializable {\n                  def <init>() = {\n                    super.<init>();\n                    ()\n                  };\n                  val rdd = sc.parallelize(collection)\n                };\n                val $iw = new $iwC()\n              };\n              val $iw = new $iwC()\n            };\n            val $iw = new $iwC()\n          };\n          val $iw = new $iwC()\n        };\n        val $iw = new $iwC()\n      };\n      val $iw = new $iwC()\n    };\n    val $iw = new $iwC()\n  };\n  val $iw = new $iwC()\n}, object $read extends scala.AnyRef {\n  def <init>() = {\n    super.<init>();\n    ()\n  };\n  val INSTANCE = new $read()\n}))\n14/05/05 21:23:16 DEBUG SparkILoop$SparkILoopInterpreter: class $read extends Serializable {\n  def <init>() = {\n    super.<init>;\n    ()\n  };\n  class $iwC extends Serializable {\n    def <init>() = {\n      super.<init>;\n      ()\n    };\n    val $VAL2 = $line3.$read.INSTANCE;\n    import $VAL2.$iw.$iw.sc;\n    class $iwC extends Serializable {\n      def <init>() = {\n        super.<init>;\n        ()\n      };\n      import org.apache.spark.SparkContext._;\n      class $iwC extends Serializable {\n        def <init>() = {\n          super.<init>;\n          ()\n        };\n        class $iwC extends Serializable {\n          def <init>() = {\n            super.<init>;\n            ()\n          };\n          import com.datastax.bdp.spark.CassandraFunctions._;\n          class $iwC extends Serializable {\n            def <init>() = {\n              super.<init>;\n              ()\n            };\n            import com.datastax.bdp.spark.context.CassandraContext;\n            class $iwC extends Serializable {\n              def <init>() = {\n                super.<init>;\n                ()\n              };\n              import com.tuplejump.calliope.Implicits._;\n              class $iwC extends Serializable {\n                def <init>() = {\n                  super.<init>;\n                  ()\n                };\n                val $VAL3 = $line17.$read.INSTANCE;\n                import $VAL3.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.collection;\n                class $iwC extends Serializable {\n                  def <init>() = {\n                    super.<init>;\n                    ()\n                  };\n                  val rdd = sc parallelize collection\n                };\n                val $iw = new $iwC.<init>\n              };\n              val $iw = new $iwC.<init>\n            };\n            val $iw = new $iwC.<init>\n          };\n          val $iw = new $iwC.<init>\n        };\n        val $iw = new $iwC.<init>\n      };\n      val $iw = new $iwC.<init>\n    };\n    val $iw = new $iwC.<init>\n  };\n  val $iw = new $iwC.<init>\n}\n14/05/05 21:23:16 DEBUG SparkILoop$SparkILoopInterpreter: object $read extends scala.AnyRef {\n  def <init>() = {\n    super.<init>;\n    ()\n  };\n  val INSTANCE = new $read.<init>\n}\n14/05/05 21:23:16 DEBUG SparkILoop$SparkILoopInterpreter: Set symbol of rdd to val rdd(): org.apache.spark.rdd.RDD\n14/05/05 21:23:16 DEBUG SparkILoop$SparkILoopInterpreter: parse(\"\nobject $eval {\n  lazy val $result = $line18.$read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.`rdd`\n  val $print: String =  {\n    $read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw\n    (\"\"\n      \n + \"rdd: org.apache.spark.rdd.RDD[(String, Int)] = \" + scala.runtime.ScalaRunTime.replStringOf($line18.$read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.`rdd`, 1000)\n\n    )\n  }\n}\n      \n\") Some(List(object $eval extends scala.AnyRef {\n  def <init>() = {\n    super.<init>();\n    ()\n  };\n  lazy val $result = $line18.$read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.rdd;\n  val $print: String = {\n    $read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw;\n    \"\".$plus(\"rdd: org.apache.spark.rdd.RDD[(String, Int)] = \").$plus(scala.runtime.ScalaRunTime.replStringOf($line18.$read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.rdd, 1000))\n  }\n}))\n14/05/05 21:23:16 DEBUG SparkILoop$SparkILoopInterpreter: object $eval extends scala.AnyRef {\n  def <init>() = {\n    super.<init>;\n    ()\n  };\n  lazy val $result = $line18.$read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.rdd;\n  val $print: String = {\n    $read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw;\n    \"\".+(\"rdd: org.apache.spark.rdd.RDD[(String, Int)] = \").+(scala.runtime.ScalaRunTime.replStringOf($line18.$read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.rdd, 1000))\n  }\n}\n14/05/05 21:23:16 DEBUG SparkILoop$SparkILoopInterpreter: Invoking: public static java.lang.String $line18.$eval.$print()\nrdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[0] at parallelize at <console>:21\n\nscala> rdd.map(_._2).sum\n14/05/05 21:23:45 DEBUG SparkILoop$SparkILoopInterpreter: parse(\"       rdd.map(_._2).sum\n\") Some(List(rdd.map(((x$1) => x$1._2)).sum))\n14/05/05 21:23:45 DEBUG SparkILoop$SparkILoopInterpreter:   21: Select\n  14: Apply\n  11: Select\n  7: Ident\n  17: Function\n  15: ValDef\n  15: TypeTree\n  -1: EmptyTree\n  17: Select\n  15: Ident\n\n14/05/05 21:23:45 DEBUG SparkILoop$SparkILoopInterpreter: parse(\"       val res3 =\n              rdd.map(_._2).sum\n\") Some(List(val res3 = rdd.map(((x$1) => x$1._2)).sum))\n14/05/05 21:23:45 DEBUG SparkILoop$SparkILoopInterpreter:   11: ValDef\n  11: TypeTree\n  46: Select\n  39: Apply\n  36: Select\n  32: Ident\n  42: Function\n  40: ValDef\n  40: TypeTree\n  -1: EmptyTree\n  42: Select\n  40: Ident\n\n14/05/05 21:23:45 DEBUG SparkILoop$SparkILoopInterpreter: parse(\"\nclass $read extends Serializable {\n  class $iwC extends Serializable {\nval $VAL4 = $line3.$read.INSTANCE;\nimport $VAL4.$iw.$iw.`sc`;\nclass $iwC extends Serializable {\nimport org.apache.spark.SparkContext._\nclass $iwC extends Serializable {\nclass $iwC extends Serializable {\nimport com.datastax.bdp.spark.CassandraFunctions._\nclass $iwC extends Serializable {\nimport com.datastax.bdp.spark.context.CassandraContext\nclass $iwC extends Serializable {\nimport com.tuplejump.calliope.Implicits._\nclass $iwC extends Serializable {\nval $VAL5 = $line17.$read.INSTANCE;\nimport $VAL5.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.`collection`;\nval $VAL6 = $line18.$read.INSTANCE;\nimport $VAL6.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.`rdd`;\nclass $iwC extends Serializable {\n       val res3 =\n              rdd.map(_._2).sum\n\n      \n\n}\nval $iw = new $iwC;\n}\nval $iw = new $iwC;\n}\nval $iw = new $iwC;\n}\nval $iw = new $iwC;\n}\nval $iw = new $iwC;\n}\nval $iw = new $iwC;\n}\nval $iw = new $iwC;\n}\nval $iw = new $iwC;\n\n}\nobject $read {\n  val INSTANCE = new $read();\n}\n\n\") Some(List(class $read extends Serializable {\n  def <init>() = {\n    super.<init>();\n    ()\n  };\n  class $iwC extends Serializable {\n    def <init>() = {\n      super.<init>();\n      ()\n    };\n    val $VAL4 = $line3.$read.INSTANCE;\n    import $VAL4.$iw.$iw.sc;\n    class $iwC extends Serializable {\n      def <init>() = {\n        super.<init>();\n        ()\n      };\n      import org.apache.spark.SparkContext._;\n      class $iwC extends Serializable {\n        def <init>() = {\n          super.<init>();\n          ()\n        };\n        class $iwC extends Serializable {\n          def <init>() = {\n            super.<init>();\n            ()\n          };\n          import com.datastax.bdp.spark.CassandraFunctions._;\n          class $iwC extends Serializable {\n            def <init>() = {\n              super.<init>();\n              ()\n            };\n            import com.datastax.bdp.spark.context.CassandraContext;\n            class $iwC extends Serializable {\n              def <init>() = {\n                super.<init>();\n                ()\n              };\n              import com.tuplejump.calliope.Implicits._;\n              class $iwC extends Serializable {\n                def <init>() = {\n                  super.<init>();\n                  ()\n                };\n                val $VAL5 = $line17.$read.INSTANCE;\n                import $VAL5.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.collection;\n                val $VAL6 = $line18.$read.INSTANCE;\n                import $VAL6.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.rdd;\n                class $iwC extends Serializable {\n                  def <init>() = {\n                    super.<init>();\n                    ()\n                  };\n                  val res3 = rdd.map(((x$1) => x$1._2)).sum\n                };\n                val $iw = new $iwC()\n              };\n              val $iw = new $iwC()\n            };\n            val $iw = new $iwC()\n          };\n          val $iw = new $iwC()\n        };\n        val $iw = new $iwC()\n      };\n      val $iw = new $iwC()\n    };\n    val $iw = new $iwC()\n  };\n  val $iw = new $iwC()\n}, object $read extends scala.AnyRef {\n  def <init>() = {\n    super.<init>();\n    ()\n  };\n  val INSTANCE = new $read()\n}))\n14/05/05 21:23:45 DEBUG SparkILoop$SparkILoopInterpreter: class $read extends Serializable {\n  def <init>() = {\n    super.<init>;\n    ()\n  };\n  class $iwC extends Serializable {\n    def <init>() = {\n      super.<init>;\n      ()\n    };\n    val $VAL4 = $line3.$read.INSTANCE;\n    import $VAL4.$iw.$iw.sc;\n    class $iwC extends Serializable {\n      def <init>() = {\n        super.<init>;\n        ()\n      };\n      import org.apache.spark.SparkContext._;\n      class $iwC extends Serializable {\n        def <init>() = {\n          super.<init>;\n          ()\n        };\n        class $iwC extends Serializable {\n          def <init>() = {\n            super.<init>;\n            ()\n          };\n          import com.datastax.bdp.spark.CassandraFunctions._;\n          class $iwC extends Serializable {\n            def <init>() = {\n              super.<init>;\n              ()\n            };\n            import com.datastax.bdp.spark.context.CassandraContext;\n            class $iwC extends Serializable {\n              def <init>() = {\n                super.<init>;\n                ()\n              };\n              import com.tuplejump.calliope.Implicits._;\n              class $iwC extends Serializable {\n                def <init>() = {\n                  super.<init>;\n                  ()\n                };\n                val $VAL5 = $line17.$read.INSTANCE;\n                import $VAL5.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.collection;\n                val $VAL6 = $line18.$read.INSTANCE;\n                import $VAL6.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.rdd;\n                class $iwC extends Serializable {\n                  def <init>() = {\n                    super.<init>;\n                    ()\n                  };\n                  val res3 = rdd.map(((x$1) => x$1._2)).sum\n                };\n                val $iw = new $iwC.<init>\n              };\n              val $iw = new $iwC.<init>\n            };\n            val $iw = new $iwC.<init>\n          };\n          val $iw = new $iwC.<init>\n        };\n        val $iw = new $iwC.<init>\n      };\n      val $iw = new $iwC.<init>\n    };\n    val $iw = new $iwC.<init>\n  };\n  val $iw = new $iwC.<init>\n}\n14/05/05 21:23:45 DEBUG SparkILoop$SparkILoopInterpreter: object $read extends scala.AnyRef {\n  def <init>() = {\n    super.<init>;\n    ()\n  };\n  val INSTANCE = new $read.<init>\n}\n14/05/05 21:23:45 DEBUG SparkILoop$SparkILoopInterpreter: Set symbol of res3 to val res3(): Double\n14/05/05 21:23:45 DEBUG SparkILoop$SparkILoopInterpreter: parse(\"\nobject $eval {\n  lazy val $result = $line19.$read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.`res3`\n  val $print: String =  {\n    $read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw\n    (\"\"\n      \n + \"res3: Double = \" + scala.runtime.ScalaRunTime.replStringOf($line19.$read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.`res3`, 1000)\n\n    )\n  }\n}\n      \n\") Some(List(object $eval extends scala.AnyRef {\n  def <init>() = {\n    super.<init>();\n    ()\n  };\n  lazy val $result = $line19.$read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.res3;\n  val $print: String = {\n    $read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw;\n    \"\".$plus(\"res3: Double = \").$plus(scala.runtime.ScalaRunTime.replStringOf($line19.$read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.res3, 1000))\n  }\n}))\n14/05/05 21:23:45 DEBUG SparkILoop$SparkILoopInterpreter: object $eval extends scala.AnyRef {\n  def <init>() = {\n    super.<init>;\n    ()\n  };\n  lazy val $result = $line19.$read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.res3;\n  val $print: String = {\n    $read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw;\n    \"\".+(\"res3: Double = \").+(scala.runtime.ScalaRunTime.replStringOf($line19.$read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.res3, 1000))\n  }\n}\n14/05/05 21:23:45 DEBUG SparkILoop$SparkILoopInterpreter: Invoking: public static java.lang.String $line19.$eval.$print()\n14/05/05 21:23:46 INFO SharkContext: Starting job: sum at <console>:24\n14/05/05 21:23:46 INFO DAGScheduler: Got job 0 (sum at <console>:24) with 2 output partitions (allowLocal=false)\n14/05/05 21:23:46 INFO DAGScheduler: Final stage: Stage 0 (sum at <console>:24)\n14/05/05 21:23:46 INFO DAGScheduler: Parents of final stage: List()\n14/05/05 21:23:46 INFO DAGScheduler: Missing parents: List()\n14/05/05 21:23:46 DEBUG DAGScheduler: submitStage(Stage 0)\n14/05/05 21:23:46 DEBUG DAGScheduler: missing: List()\n14/05/05 21:23:46 INFO DAGScheduler: Submitting Stage 0 (MappedRDD[2] at numericRDDToDoubleRDDFunctions at <console>:24), which has no missing parents\n14/05/05 21:23:46 DEBUG DAGScheduler: submitMissingTasks(Stage 0)\n14/05/05 21:23:46 INFO DAGScheduler: Submitting 2 missing tasks from Stage 0 (MappedRDD[2] at numericRDDToDoubleRDDFunctions at <console>:24)\n14/05/05 21:23:46 DEBUG DAGScheduler: New pending tasks: Set(ResultTask(0, 0), ResultTask(0, 1))\n14/05/05 21:23:46 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks\n14/05/05 21:23:46 DEBUG TaskSetManager: Epoch for TaskSet 0.0: 0\n14/05/05 21:23:46 DEBUG TaskSetManager: Valid locality levels for TaskSet 0.0: ANY\n14/05/05 21:23:46 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 0\n14/05/05 21:23:46 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 0\n14/05/05 21:23:46 INFO TaskSetManager: Starting task 0.0:0 as TID 0 on executor 0: 127.0.0.1 (PROCESS_LOCAL)\n14/05/05 21:23:47 INFO TaskSetManager: Serialized task 0.0:0 as 13890654 bytes in 617 ms\n14/05/05 21:23:47 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 1\n14/05/05 21:23:47 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 1\n14/05/05 21:23:48 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 1\n14/05/05 21:23:48 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 1\n14/05/05 21:23:49 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 1\n14/05/05 21:23:49 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 1\n14/05/05 21:23:50 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 1\n14/05/05 21:23:50 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 1\n14/05/05 21:23:51 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 1\n14/05/05 21:23:51 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 1\n{noformat}\n\nHow to enable logging from Executor? It doesn't seem to log anything anywhere, but maybe something is misconfigured.", "There are some logs from SparkWorker and SparkMaster:\n{noformat}\nINFO 21:32:45,654 SparkMaster: 14/05/05 21:32:45 INFO Slf4jLogger: Slf4jLogger started\n INFO 21:32:45,698 SparkMaster: 14/05/05 21:32:45 INFO Remoting: Starting remoting\n INFO 21:32:45,849 SparkMaster: 14/05/05 21:32:45 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkMaster@127.0.0.1:7077]\n INFO 21:32:46,112 SparkMaster: 14/05/05 21:32:46 INFO Master: Starting Spark master at spark://127.0.0.1:7077\n INFO 21:32:46,136 SparkMaster: 14/05/05 21:32:46 INFO Server: jetty-7.6.8.v20121106\n INFO 21:32:46,142 SparkMaster: 14/05/05 21:32:46 INFO ContextHandler: started o.e.j.s.h.ContextHandler{/metrics/master/json,null}\n INFO 21:32:46,142 SparkMaster: 14/05/05 21:32:46 INFO ContextHandler: started o.e.j.s.h.ContextHandler{/metrics/applications/json,null}\n INFO 21:32:46,143 SparkMaster: 14/05/05 21:32:46 INFO ContextHandler: started o.e.j.s.h.ContextHandler{/static,null}\n INFO 21:32:46,143 SparkMaster: 14/05/05 21:32:46 INFO ContextHandler: started o.e.j.s.h.ContextHandler{/app/json,null}\n INFO 21:32:46,143 SparkMaster: 14/05/05 21:32:46 INFO ContextHandler: started o.e.j.s.h.ContextHandler{/app,null}\n INFO 21:32:46,143 SparkMaster: 14/05/05 21:32:46 INFO ContextHandler: started o.e.j.s.h.ContextHandler{/json,null}\n INFO 21:32:46,143 SparkMaster: 14/05/05 21:32:46 INFO ContextHandler: started o.e.j.s.h.ContextHandler{*,null}\n INFO 21:32:46,152 SparkMaster: 14/05/05 21:32:46 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:7080\n INFO 21:32:46,153 SparkMaster: 14/05/05 21:32:46 INFO MasterWebUI: Started Master web UI at http://m4600.local:7080\n INFO 21:32:46,169 SparkMaster: 14/05/05 21:32:46 INFO Master: I have been elected leader! New state: ALIVE\n INFO 21:32:46,474 Started SparkWork connected to 127.0.0.1:7077\n INFO 21:32:46,994 SparkWorker: 14/05/05 21:32:46 WARN Utils: Your hostname, m4600 resolves to a loopback address: 127.0.0.2; using 192.168.122.1 instead (on interface virbr0)\n INFO 21:32:46,994 SparkWorker: 14/05/05 21:32:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n INFO 21:32:47,473 SparkWorker: 14/05/05 21:32:47 INFO Slf4jLogger: Slf4jLogger started\n INFO 21:32:47,519 SparkWorker: 14/05/05 21:32:47 INFO Remoting: Starting remoting\n INFO 21:32:47,661 SparkWorker: 14/05/05 21:32:47 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkWorker@127.0.0.1:45858]\n INFO 21:32:47,827 SparkWorker: 14/05/05 21:32:47 INFO Worker: Starting Spark worker 127.0.0.1:45858 with 8 cores, 4.0 GB RAM\n INFO 21:32:47,828 SparkWorker: 14/05/05 21:32:47 INFO Worker: Spark home: /home/pkolaczk/Projekty/datastax/bdp/resources/spark\n INFO 21:32:47,945 SparkWorker: 14/05/05 21:32:47 INFO Server: jetty-7.6.8.v20121106\n INFO 21:32:47,951 SparkWorker: 14/05/05 21:32:47 INFO ContextHandler: started o.e.j.s.h.ContextHandler{/metrics/json,null}\n INFO 21:32:47,951 SparkWorker: 14/05/05 21:32:47 INFO ContextHandler: started o.e.j.s.h.ContextHandler{/static,null}\n INFO 21:32:47,952 SparkWorker: 14/05/05 21:32:47 INFO ContextHandler: started o.e.j.s.h.ContextHandler{/log,null}\n INFO 21:32:47,952 SparkWorker: 14/05/05 21:32:47 INFO ContextHandler: started o.e.j.s.h.ContextHandler{/logPage,null}\n INFO 21:32:47,952 SparkWorker: 14/05/05 21:32:47 INFO ContextHandler: started o.e.j.s.h.ContextHandler{/json,null}\n INFO 21:32:47,952 SparkWorker: 14/05/05 21:32:47 INFO ContextHandler: started o.e.j.s.h.ContextHandler{*,null}\n INFO 21:32:47,961 SparkWorker: 14/05/05 21:32:47 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:7081\n INFO 21:32:47,962 SparkWorker: 14/05/05 21:32:47 INFO WorkerWebUI: Started Worker web UI at http://m4600.local:7081\n INFO 21:32:47,963 SparkWorker: 14/05/05 21:32:47 INFO Worker: Connecting to master spark://127.0.0.1:7077...\n INFO 21:32:48,199 SparkMaster: 14/05/05 21:32:48 INFO Master: Registering worker 127.0.0.1:45858 with 8 cores, 4.0 GB RAM\n INFO 21:32:48,225 SparkWorker: 14/05/05 21:32:48 INFO Worker: Successfully registered with master spark://127.0.0.1:7077\n INFO 21:33:00,119 SparkMaster: 14/05/05 21:33:00 INFO Master: Registering app Spark shell\n INFO 21:33:00,124 SparkMaster: 14/05/05 21:33:00 INFO Master: Registered app Spark shell with ID app-20140505213300-0000\n INFO 21:33:00,145 SparkMaster: 14/05/05 21:33:00 INFO Master: Launching executor app-20140505213300-0000/0 on worker worker-20140505213247-127.0.0.1-45858\n INFO 21:33:00,185 SparkWorker: 14/05/05 21:33:00 INFO Worker: Asked to launch executor app-20140505213300-0000/0 for Spark shell\n INFO 21:33:00,812 SparkWorker: 14/05/05 21:33:00 INFO ExecutorRunner: Launch command: \"/opt/jdk/bin/java\" \"-cp\" \":/home/pkolaczk/Projekty/datastax/bdp/build/dse-4.5.0-SNAPSHOT.jar:/home/pkolaczk/Projekty/datastax/bdp/build/maven-ant-tasks-2.1.3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/cassandra-driver-core-2.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/commons-codec-1.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/commons-io-2.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/guava-15.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/HdrHistogram-1.0.9.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/java-uuid-generator-3.1.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/jbcrypt-0.3m.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/jline-1.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/jna-3.4.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/journalio-1.4.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/log4j-1.2.17.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/metrics-core-3.0.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/netty-3.9.0.Final.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/netty-all-4.0.13.Final.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/slf4j-api-1.7.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/slf4j-log4j12-1.7.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/conf:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/conf:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/conf:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/tools/lib/stress.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/antlr-2.7.7.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/antlr-3.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/antlr-runtime-3.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/cassandra-all-2.0.7.31.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/cassandra-clientutil-2.0.7.31.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/cassandra-thrift-2.0.7.31.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/commons-cli-1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/commons-codec-1.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/commons-lang-2.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/commons-lang3-3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/commons-logging-1.1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/compress-lzf-0.8.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/concurrentlinkedhashmap-lru-1.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/disruptor-3.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/elephant-bird-hadoop-compat-4.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/guava-15.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/hibernate-validator-4.3.0.Final.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/high-scale-lib-1.1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/httpclient-4.2.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/httpcore-4.2.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/jackson-core-asl-1.9.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/jackson-mapper-asl-1.9.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/jamm-0.2.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/jbcrypt-0.3m.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/joda-time-1.6.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/json-simple-1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/libthrift-0.9.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/log4j-1.2.16.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/lz4-1.2.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/metrics-core-2.2.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/netty-3.6.6.Final.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/reporter-config-2.1.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/slf4j-api-1.7.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/snakeyaml-1.11.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/snappy-java-1.0.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/snaptree-0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/stringtemplate-3.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/super-csv-2.1.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/thrift-server-0.3.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/validation-api-1.0.0.GA.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/../driver/lib/cassandra-driver-core-2.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/../driver/lib/cassandra-driver-dse-2.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/../driver/lib/metrics-core-3.0.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/../driver/lib/netty-3.9.0.Final.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/../driver/lib/slf4j-api-1.7.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/slf4j-api-1.7.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/slf4j-log4j12-1.7.2.jar:::/home/pkolaczk/Projekty/datastax/bdp/resources/spark/conf:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/activation-1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/akka-actor_2.10-2.2.3-shaded-protobuf.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/akka-remote_2.10-2.2.3-shaded-protobuf.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/akka-slf4j_2.10-2.2.3-shaded-protobuf.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/akka-zeromq_2.10-2.2.3-shaded-protobuf.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/algebird-core_2.10-0.1.11.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/asm-4.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/asm-commons-4.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/asm-tree-4.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/avro-1.7.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/avro-ipc-1.7.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/calliope_2.10-0.9.0-EA.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/chill_2.10-0.3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/chill-java-0.3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/colt-1.2.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-beanutils-1.7.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-beanutils-core-1.8.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-cli-1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-codec-1.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-collections-3.2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-compress-1.4.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-configuration-1.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-digester-1.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-el-1.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-httpclient-3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-io-2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-lang-2.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-logging-1.1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/compress-lzf-1.0.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/concurrent-1.3.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/config-1.0.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/core-3.1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/fastutil-6.4.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/flume-ng-sdk-1.2.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/gmetric4j-1.0.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/guava-14.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/hadoop-client-1.0.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/hbase-0.94.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/high-scale-lib-1.1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/httpclient-4.1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/httpcore-4.1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jackson-annotations-2.2.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jackson-core-2.2.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jackson-core-asl-1.8.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jackson-databind-2.2.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jackson-jaxrs-1.8.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jackson-mapper-asl-1.8.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jackson-xc-1.8.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jamon-runtime-2.3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jansi-1.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jasper-compiler-5.5.23.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jasper-runtime-5.5.23.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/JavaEWAH-0.6.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/java-xmlbuilder-0.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/javax.servlet-2.5.0.v201103041518.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jaxb-api-2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jaxb-impl-2.2.3-1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jblas-1.2.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jersey-core-1.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jersey-json-1.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jersey-server-1.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jets3t-0.9.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jettison-1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jetty-6.1.26.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jetty-continuation-7.6.8.v20121106.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jetty-http-7.6.8.v20121106.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jetty-io-7.6.8.v20121106.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jetty-server-7.6.8.v20121106.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jetty-util-6.1.26.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jetty-util-7.6.8.v20121106.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jline-2.10.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jnr-constants-0.8.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jruby-complete-1.6.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jsp-2.1-6.1.14.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jsp-api-2.1-6.1.14.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jsr305-1.3.9.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jul-to-slf4j-1.7.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/kafka_2.10-0.8.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/kryo-2.21.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/libthrift-0.7.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/lift-json_2.10-2.5.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/mesos-0.13.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/metrics-annotation-2.2.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/metrics-core-2.1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/metrics-core-3.0.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/metrics-ganglia-3.0.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/metrics-graphite-3.0.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/metrics-json-3.0.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/metrics-jvm-3.0.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/minlog-1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/mqtt-client-0.4.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/netty-3.5.9.Final.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/netty-all-4.0.13.Final.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/objenesis-1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/oncrpc-1.0.7.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/paranamer-2.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/protobuf-java-2.4.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/protobuf-java-2.4.1-shaded.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/reflectasm-1.07-shaded.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/scala-compiler-2.10.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/scala-library-2.10.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/scala-reflect-2.10.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/servlet-api-2.5-20081211.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/servlet-api-2.5-6.1.14.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-bagel_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-core_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-examples_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-mllib_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-repl_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-streaming_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-streaming-flume_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-streaming-kafka_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-streaming-mqtt_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-streaming-twitter_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-streaming-zeromq_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/stax-api-1.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/stream-2.4.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/twitter4j-core-3.0.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/twitter4j-stream-3.0.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/uncommons-maths-1.2.2a.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/velocity-1.7.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/xz-1.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/zeromq-scala-binding_2.10-0.0.7.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/zkclient-0.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/zookeeper-3.4.5.jar:/home/pkolaczk/.spark/cassandra-context/spark-cassandra-context.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/conf::/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/commons-codec-1.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/commons-httpclient-3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/commons-io-2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/commons-logging-1.1.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/guava-14.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/hadoop-client-1.0.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/httpclient-4.3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/httpcore-4.3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/javax.servlet-2.5.0.v201103041518.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/jets3t-0.7.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/shark_2.10-0.9.0.1-DSP-3062-SNAPSHOT.jar::/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/conf:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/elephant-bird-hadoop-compat-4.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/hadoop-core-1.0.4.10.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/hadoop-examples-1.0.4.10.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/hadoop-fairscheduler-1.0.4.10.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/hadoop-streaming-1.0.4.10.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/hadoop-test-1.0.4.10.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/hadoop-tools-1.0.4.10.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/ant-1.6.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/automaton-1.11-8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-beanutils-1.7.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-beanutils-core-1.8.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-cli-1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-codec-1.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-collections-3.2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-configuration-1.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-digester-1.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-el-1.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-httpclient-3.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-lang-2.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-logging-1.1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-math-2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-net-1.4.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/core-3.1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/ftplet-api-1.0.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/ftpserver-core-1.0.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/ftpserver-deprecated-1.0.0-M2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/hsqldb-1.8.0.10.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/httpclient-4.1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/httpcore-4.1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jackson-core-asl-1.8.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jackson-mapper-asl-1.8.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jasper-compiler-5.5.12.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jasper-runtime-5.5.12.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/java-xmlbuilder-0.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jets3t-0.9.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jetty-6.1.26.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jetty-util-6.1.26.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jsp-2.1-6.1.14.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jsp-api-2.1-6.1.14.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/kfs-0.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/mina-core-2.0.0-M5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/oro-2.0.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/servlet-api-2.5-20081211.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/servlet-api-2.5-6.1.14.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/snappy-java-1.0.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/xmlenc-0.52.jar::/home/pkolaczk/Projekty/datastax/bdp/build/dse-4.5.0-SNAPSHOT.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/antlr-runtime-3.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/conf::/home/pkolaczk/Projekty/datastax/bdp/build/dse-4.5.0-SNAPSHOT.jar:/home/pkolaczk/Projekty/datastax/bdp/build/dse-4.5.0-SNAPSHOT.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/antlr-runtime-3.2.jar::/home/pkolaczk/Projekty/datastax/bdp/resources/hive/conf:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/antlr-2.7.7.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/antlr-3.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/antlr-runtime-3.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/asm-4.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/avro-1.7.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/avro-ipc-1.7.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/avro-mapred-1.7.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/bonecp-0.7.1.RELEASE.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-beanutils-1.7.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-beanutils-core-1.8.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-cli-1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-codec-1.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-collections-3.2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-compress-1.4.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-configuration-1.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-digester-1.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-io-2.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-lang-2.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-lang3-3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-logging-1.1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-logging-api-1.0.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-pool-1.5.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/datanucleus-api-jdo-3.2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/datanucleus-core-3.2.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/datanucleus-rdbms-3.2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/derby-10.4.2.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/guava-15.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-cli-0.12.0.3-20140319.091653-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-common-0.12.0.3-20140319.091659-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-exec-0.12.0.3-20140319.091716-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-hwi-0.12.0.3-20140319.091745-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-jdbc-0.12.0.3-20140319.091754-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-metastore-0.12.0.3-20140319.091801-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-serde-0.12.0.3-20140319.091811-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-service-0.12.0.3-20140319.091817-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-shims-0.12.0.3-20140319.091825-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/httpclient-4.2.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/httpcore-4.2.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/jackson-core-asl-1.8.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/jackson-mapper-asl-1.8.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/JavaEWAH-0.3.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/java-xmlbuilder-0.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/javolution-5.5.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/jdo-api-3.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/jets3t-0.9.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/jetty-util-6.1.26.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/json-20090211.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/jta-1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/libfb303-0.9.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/libthrift-0.9.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/log4j-1.2.16.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/netty-3.5.9.Final.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/paranamer-2.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/protobuf-java-2.4.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/servlet-api-2.5-20081211.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/slf4j-api-1.6.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/snappy-0.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/snappy-java-1.0.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/ST4-4.0.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/stringtemplate-3.2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/velocity-1.7.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/xz-1.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/zookeeper-3.4.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/conf\" \"-XX:MaxPermSize=256M\" \"-XX:MaxPermSize=256M\" \"-Xms2048M\" \"-Xmx2048M\" \"org.apache.spark.executor.CoarseGrainedExecutorBackend\" \"akka.tcp://spark@m4600.local:45753/user/CoarseGrainedScheduler\" \"0\" \"127.0.0.1\" \"1\" \"akka.tcp://sparkWorker@127.0.0.1:45858/user/Worker\" \"app-20140505213300-0000\"\n{noformat}", "Add conf/log4j.properties file\n{code}\n# Set everything to be logged to the console\n#log4j.rootCategory=INFO,console\nlog4j.rootCategory=INFO,file\n#log4j.rootCategory=DEBUG,file\n#- size rotation with log cleanup.\nlog4j.appender.file=org.apache.log4j.RollingFileAppender\nlog4j.appender.file.MaxFileSize=100MB\nlog4j.appender.file.MaxBackupIndex=10\n\n#- File to log to and log format\nlog4j.appender.file.File=/opt/spark_local/logs/spark.log\nlog4j.appender.file.layout=org.apache.log4j.PatternLayout\nlog4j.appender.file.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\nlog4j.logger.DataNucleus=DEBUG\n# Ignore messages below warning level from Jetty, because it's a bit verbose\nlog4j.logger.org.eclipse.jetty=WARN\n#log4j.logger.org.eclipse.jetty=DEBUG\n{code}\n{code}log4j.appender.file.File=/opt/spark_local/logs/spark.log{code} change the directory to fit here", "What is the value of {{spark.akka.frameSize}}? you can find it  in {{http://host:4040/environment/}}", "Hmm, it is not listed in the environment at all. What value does it need to have?\nShould I expect parallelized collection partitions larger than akka.frame.size to cause problems?\n", "May be caused by the imprope {{spark.akka.frameSize}} value\nthe value can be changed little, try {{5}} .", "Setting {{spark.akka.frameSize}} to {{20}} helped. {{5}} or {{10}} wasn't enough.", "According to my not comprehensive test. 20 is too big.\nA real example when 25889467 bytes serialized result be sent directly to driver [Executor.scala#L248|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/executor/Executor.scala#L248] will cause a problem", "Hi [~pwendell]  \nWhat are your thoughts on this issue?", "I modified log4j config and this is what I got in the spark.log:\n{noformat}\n14/05/06 10:17:29 INFO HttpServer: Starting HTTP Server\n14/05/06 10:17:33 WARN Utils: Your hostname, m4600 resolves to a loopback address: 127.0.0.2; using 192.168.122.1 instead (on interface virbr0)\n14/05/06 10:17:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n14/05/06 10:17:34 INFO Slf4jLogger: Slf4jLogger started\n14/05/06 10:17:34 INFO Remoting: Starting remoting\n14/05/06 10:17:34 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://spark@m4600.local:33012]\n14/05/06 10:17:34 INFO Remoting: Remoting now listens on addresses: [akka.tcp://spark@m4600.local:33012]\n14/05/06 10:17:34 INFO SparkEnv: Registering BlockManagerMaster\n14/05/06 10:17:34 INFO DiskBlockManager: Created local directory at /tmp/spark-local-20140506101734-42f3\n14/05/06 10:17:34 INFO MemoryStore: MemoryStore started with capacity 1178.1 MB.\n14/05/06 10:17:34 INFO ConnectionManager: Bound socket to port 60842 with id = ConnectionManagerId(m4600.local,60842)\n14/05/06 10:17:34 INFO BlockManagerMaster: Trying to register BlockManager\n14/05/06 10:17:34 INFO BlockManagerMasterActor$BlockManagerInfo: Registering block manager m4600.local:60842 with 1178.1 MB RAM\n14/05/06 10:17:34 INFO BlockManagerMaster: Registered BlockManager\n14/05/06 10:17:34 INFO HttpServer: Starting HTTP Server\n14/05/06 10:17:34 INFO HttpBroadcast: Broadcast server started at http://192.168.122.1:51030\n14/05/06 10:17:35 INFO SparkEnv: Registering MapOutputTracker\n14/05/06 10:17:35 INFO HttpFileServer: HTTP File server directory is /tmp/spark-5013023c-f851-4398-a344-5493e62edd26\n14/05/06 10:17:35 INFO HttpServer: Starting HTTP Server\n14/05/06 10:17:35 INFO SparkUI: Started Spark Web UI at http://m4600.local:4040\n14/05/06 10:17:35 INFO SharkContext: Added JAR /home/pkolaczk/.spark/cassandra-context/spark-cassandra-context.jar at http://192.168.122.1:49386/jars/spark-cassandra-context.jar with timestamp 1399364255576\n14/05/06 10:17:35 INFO AppClient$ClientActor: Connecting to master spark://127.0.0.1:7077...\n14/05/06 10:17:35 INFO Master: Registering app Spark shell\n14/05/06 10:17:35 INFO Master: Registered app Spark shell with ID app-20140506101735-0001\n14/05/06 10:17:35 INFO Master: Launching executor app-20140506101735-0001/0 on worker worker-20140506101633-127.0.0.1-44566\n14/05/06 10:17:35 INFO Worker: Asked to launch executor app-20140506101735-0001/0 for Spark shell\n14/05/06 10:17:35 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20140506101735-0001\n14/05/06 10:17:35 INFO AppClient$ClientActor: Executor added: app-20140506101735-0001/0 on worker-20140506101633-127.0.0.1-44566 (127.0.0.1:44566) with 1 cores\n14/05/06 10:17:35 INFO SparkDeploySchedulerBackend: Granted executor ID app-20140506101735-0001/0 on hostPort 127.0.0.1:44566 with 1 cores, 2.0 GB RAM\n14/05/06 10:17:35 INFO AppClient$ClientActor: Executor updated: app-20140506101735-0001/0 is now RUNNING\n14/05/06 10:17:36 INFO ExecutorRunner: Launch command: \"/opt/jdk/bin/java\" \"-cp\" \":/home/pkolaczk/Projekty/datastax/bdp/build/dse-4.5.0-SNAPSHOT.jar:/home/pkolaczk/Projekty/datastax/bdp/build/maven-ant-tasks-2.1.3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/cassandra-driver-core-2.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/commons-codec-1.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/commons-io-2.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/guava-15.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/HdrHistogram-1.0.9.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/java-uuid-generator-3.1.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/jbcrypt-0.3m.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/jline-1.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/jna-3.4.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/journalio-1.4.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/log4j-1.2.17.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/metrics-core-3.0.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/netty-3.9.0.Final.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/netty-all-4.0.13.Final.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/slf4j-api-1.7.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/slf4j-log4j12-1.7.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/conf:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/conf:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/conf:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/tools/lib/stress.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/antlr-2.7.7.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/antlr-3.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/antlr-runtime-3.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/cassandra-all-2.0.7.31.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/cassandra-clientutil-2.0.7.31.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/cassandra-thrift-2.0.7.31.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/commons-cli-1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/commons-codec-1.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/commons-lang-2.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/commons-lang3-3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/commons-logging-1.1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/compress-lzf-0.8.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/concurrentlinkedhashmap-lru-1.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/disruptor-3.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/elephant-bird-hadoop-compat-4.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/guava-15.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/hibernate-validator-4.3.0.Final.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/high-scale-lib-1.1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/httpclient-4.2.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/httpcore-4.2.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/jackson-core-asl-1.9.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/jackson-mapper-asl-1.9.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/jamm-0.2.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/jbcrypt-0.3m.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/joda-time-1.6.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/json-simple-1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/libthrift-0.9.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/log4j-1.2.16.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/lz4-1.2.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/metrics-core-2.2.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/netty-3.6.6.Final.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/reporter-config-2.1.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/slf4j-api-1.7.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/snakeyaml-1.11.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/snappy-java-1.0.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/snaptree-0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/stringtemplate-3.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/super-csv-2.1.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/thrift-server-0.3.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/validation-api-1.0.0.GA.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/../driver/lib/cassandra-driver-core-2.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/../driver/lib/cassandra-driver-dse-2.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/../driver/lib/metrics-core-3.0.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/../driver/lib/netty-3.9.0.Final.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/../driver/lib/slf4j-api-1.7.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/slf4j-api-1.7.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/dse/lib/slf4j-log4j12-1.7.2.jar:::/home/pkolaczk/Projekty/datastax/bdp/resources/spark/conf:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/activation-1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/akka-actor_2.10-2.2.3-shaded-protobuf.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/akka-remote_2.10-2.2.3-shaded-protobuf.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/akka-slf4j_2.10-2.2.3-shaded-protobuf.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/akka-zeromq_2.10-2.2.3-shaded-protobuf.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/algebird-core_2.10-0.1.11.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/asm-4.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/asm-commons-4.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/asm-tree-4.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/avro-1.7.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/avro-ipc-1.7.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/calliope_2.10-0.9.0-EA.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/chill_2.10-0.3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/chill-java-0.3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/colt-1.2.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-beanutils-1.7.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-beanutils-core-1.8.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-cli-1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-codec-1.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-collections-3.2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-compress-1.4.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-configuration-1.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-digester-1.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-el-1.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-httpclient-3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-io-2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-lang-2.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/commons-logging-1.1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/compress-lzf-1.0.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/concurrent-1.3.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/config-1.0.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/core-3.1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/fastutil-6.4.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/flume-ng-sdk-1.2.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/gmetric4j-1.0.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/guava-14.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/hadoop-client-1.0.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/hbase-0.94.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/high-scale-lib-1.1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/httpclient-4.1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/httpcore-4.1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jackson-annotations-2.2.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jackson-core-2.2.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jackson-core-asl-1.8.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jackson-databind-2.2.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jackson-jaxrs-1.8.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jackson-mapper-asl-1.8.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jackson-xc-1.8.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jamon-runtime-2.3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jansi-1.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jasper-compiler-5.5.23.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jasper-runtime-5.5.23.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/JavaEWAH-0.6.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/java-xmlbuilder-0.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/javax.servlet-2.5.0.v201103041518.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jaxb-api-2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jaxb-impl-2.2.3-1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jblas-1.2.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jersey-core-1.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jersey-json-1.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jersey-server-1.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jets3t-0.9.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jettison-1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jetty-6.1.26.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jetty-continuation-7.6.8.v20121106.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jetty-http-7.6.8.v20121106.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jetty-io-7.6.8.v20121106.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jetty-server-7.6.8.v20121106.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jetty-util-6.1.26.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jetty-util-7.6.8.v20121106.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jline-2.10.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jnr-constants-0.8.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jruby-complete-1.6.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jsp-2.1-6.1.14.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jsp-api-2.1-6.1.14.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jsr305-1.3.9.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/jul-to-slf4j-1.7.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/kafka_2.10-0.8.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/kryo-2.21.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/libthrift-0.7.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/lift-json_2.10-2.5.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/mesos-0.13.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/metrics-annotation-2.2.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/metrics-core-2.1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/metrics-core-3.0.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/metrics-ganglia-3.0.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/metrics-graphite-3.0.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/metrics-json-3.0.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/metrics-jvm-3.0.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/minlog-1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/mqtt-client-0.4.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/netty-3.5.9.Final.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/netty-all-4.0.13.Final.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/objenesis-1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/oncrpc-1.0.7.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/paranamer-2.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/protobuf-java-2.4.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/protobuf-java-2.4.1-shaded.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/reflectasm-1.07-shaded.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/scala-compiler-2.10.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/scala-library-2.10.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/scala-reflect-2.10.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/servlet-api-2.5-20081211.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/servlet-api-2.5-6.1.14.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-bagel_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-core_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-examples_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-mllib_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-repl_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-streaming_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-streaming-flume_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-streaming-kafka_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-streaming-mqtt_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-streaming-twitter_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/spark-streaming-zeromq_2.10-0.9.0-incubating.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/stax-api-1.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/stream-2.4.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/twitter4j-core-3.0.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/twitter4j-stream-3.0.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/uncommons-maths-1.2.2a.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/velocity-1.7.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/xz-1.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/zeromq-scala-binding_2.10-0.0.7.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/zkclient-0.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/lib/zookeeper-3.4.5.jar:/home/pkolaczk/.spark/cassandra-context/spark-cassandra-context.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/spark/conf::/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/commons-codec-1.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/commons-httpclient-3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/commons-io-2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/commons-logging-1.1.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/guava-14.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/hadoop-client-1.0.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/httpclient-4.3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/httpcore-4.3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/javax.servlet-2.5.0.v201103041518.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/jets3t-0.7.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/shark/lib/shark_2.10-0.9.0.1-DSP-3062-SNAPSHOT.jar::/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/conf:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/elephant-bird-hadoop-compat-4.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/hadoop-core-1.0.4.10.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/hadoop-examples-1.0.4.10.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/hadoop-fairscheduler-1.0.4.10.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/hadoop-streaming-1.0.4.10.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/hadoop-test-1.0.4.10.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/hadoop-tools-1.0.4.10.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/ant-1.6.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/automaton-1.11-8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-beanutils-1.7.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-beanutils-core-1.8.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-cli-1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-codec-1.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-collections-3.2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-configuration-1.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-digester-1.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-el-1.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-httpclient-3.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-lang-2.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-logging-1.1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-math-2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/commons-net-1.4.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/core-3.1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/ftplet-api-1.0.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/ftpserver-core-1.0.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/ftpserver-deprecated-1.0.0-M2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/hsqldb-1.8.0.10.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/httpclient-4.1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/httpcore-4.1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jackson-core-asl-1.8.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jackson-mapper-asl-1.8.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jasper-compiler-5.5.12.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jasper-runtime-5.5.12.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/java-xmlbuilder-0.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jets3t-0.9.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jetty-6.1.26.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jetty-util-6.1.26.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jsp-2.1-6.1.14.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/jsp-api-2.1-6.1.14.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/kfs-0.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/mina-core-2.0.0-M5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/oro-2.0.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/servlet-api-2.5-20081211.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/servlet-api-2.5-6.1.14.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/snappy-java-1.0.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/lib/xmlenc-0.52.jar::/home/pkolaczk/Projekty/datastax/bdp/build/dse-4.5.0-SNAPSHOT.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/antlr-runtime-3.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/conf::/home/pkolaczk/Projekty/datastax/bdp/build/dse-4.5.0-SNAPSHOT.jar:/home/pkolaczk/Projekty/datastax/bdp/build/dse-4.5.0-SNAPSHOT.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/cassandra/lib/antlr-runtime-3.2.jar::/home/pkolaczk/Projekty/datastax/bdp/resources/hive/conf:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/antlr-2.7.7.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/antlr-3.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/antlr-runtime-3.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/asm-4.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/avro-1.7.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/avro-ipc-1.7.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/avro-mapred-1.7.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/bonecp-0.7.1.RELEASE.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-beanutils-1.7.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-beanutils-core-1.8.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-cli-1.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-codec-1.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-collections-3.2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-compress-1.4.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-configuration-1.6.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-digester-1.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-io-2.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-lang-2.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-lang3-3.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-logging-1.1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-logging-api-1.0.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/commons-pool-1.5.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/datanucleus-api-jdo-3.2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/datanucleus-core-3.2.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/datanucleus-rdbms-3.2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/derby-10.4.2.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/guava-15.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-cli-0.12.0.3-20140319.091653-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-common-0.12.0.3-20140319.091659-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-exec-0.12.0.3-20140319.091716-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-hwi-0.12.0.3-20140319.091745-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-jdbc-0.12.0.3-20140319.091754-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-metastore-0.12.0.3-20140319.091801-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-serde-0.12.0.3-20140319.091811-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-service-0.12.0.3-20140319.091817-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/hive-shims-0.12.0.3-20140319.091825-2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/httpclient-4.2.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/httpcore-4.2.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/jackson-core-asl-1.8.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/jackson-mapper-asl-1.8.8.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/JavaEWAH-0.3.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/java-xmlbuilder-0.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/javolution-5.5.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/jdo-api-3.0.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/jets3t-0.9.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/jetty-util-6.1.26.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/json-20090211.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/jta-1.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/libfb303-0.9.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/libthrift-0.9.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/log4j-1.2.16.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/netty-3.5.9.Final.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/paranamer-2.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/protobuf-java-2.4.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/servlet-api-2.5-20081211.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/slf4j-api-1.6.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/snappy-0.2.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/snappy-java-1.0.5.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/ST4-4.0.4.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/stringtemplate-3.2.1.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/velocity-1.7.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/xz-1.0.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/lib/zookeeper-3.4.3.jar:/home/pkolaczk/Projekty/datastax/bdp/resources/hive/conf\" \"-Djava.library.path=:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/native/Linux-amd64-64/lib\" \"-Djava.system.class.loader=com.datastax.bdp.loader.DseClientClassLoader\" \"-XX:MaxPermSize=256M\" \"-Djava.library.path=:/home/pkolaczk/Projekty/datastax/bdp/resources/hadoop/native/Linux-amd64-64/lib\" \"-Djava.system.class.loader=com.datastax.bdp.loader.DseClientClassLoader\" \"-XX:MaxPermSize=256M\" \"-Xms2048M\" \"-Xmx2048M\" \"org.apache.spark.executor.CoarseGrainedExecutorBackend\" \"akka.tcp://spark@m4600.local:33012/user/CoarseGrainedScheduler\" \"0\" \"127.0.0.1\" \"1\" \"akka.tcp://sparkWorker@127.0.0.1:44566/user/Worker\" \"app-20140506101735-0001\"\n14/05/06 10:17:37 INFO Slf4jLogger: Slf4jLogger started\n14/05/06 10:17:37 INFO Remoting: Starting remoting\n14/05/06 10:17:38 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkExecutor@127.0.0.1:39919]\n14/05/06 10:17:38 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkExecutor@127.0.0.1:39919]\n14/05/06 10:17:38 INFO CoarseGrainedExecutorBackend: Connecting to driver: akka.tcp://spark@m4600.local:33012/user/CoarseGrainedScheduler\n14/05/06 10:17:38 INFO WorkerWatcher: Connecting to worker akka.tcp://sparkWorker@127.0.0.1:44566/user/Worker\n14/05/06 10:17:38 INFO WorkerWatcher: Successfully connected to akka.tcp://sparkWorker@127.0.0.1:44566/user/Worker\n14/05/06 10:17:38 INFO SparkDeploySchedulerBackend: Registered executor: Actor[akka.tcp://sparkExecutor@127.0.0.1:39919/user/Executor#-1863004534] with ID 0\n14/05/06 10:17:38 INFO CoarseGrainedExecutorBackend: Successfully registered with driver\n14/05/06 10:17:38 INFO Executor: Using REPL class URI: http://192.168.122.1:58332\n14/05/06 10:17:38 INFO Slf4jLogger: Slf4jLogger started\n14/05/06 10:17:38 INFO Remoting: Starting remoting\n14/05/06 10:17:38 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://spark@127.0.0.1:49111]\n14/05/06 10:17:38 INFO Remoting: Remoting now listens on addresses: [akka.tcp://spark@127.0.0.1:49111]\n14/05/06 10:17:38 INFO SparkEnv: Connecting to BlockManagerMaster: akka.tcp://spark@m4600.local:33012/user/BlockManagerMaster\n14/05/06 10:17:38 INFO DiskBlockManager: Created local directory at /tmp/spark-local-20140506101738-4884\n14/05/06 10:17:38 INFO MemoryStore: MemoryStore started with capacity 1178.1 MB.\n14/05/06 10:17:38 INFO ConnectionManager: Bound socket to port 32898 with id = ConnectionManagerId(127.0.0.1,32898)\n14/05/06 10:17:38 INFO BlockManagerMaster: Trying to register BlockManager\n14/05/06 10:17:38 INFO BlockManagerMasterActor$BlockManagerInfo: Registering block manager 127.0.0.1:32898 with 1178.1 MB RAM\n14/05/06 10:17:38 INFO BlockManagerMaster: Registered BlockManager\n14/05/06 10:17:38 INFO SparkEnv: Connecting to MapOutputTracker: akka.tcp://spark@m4600.local:33012/user/MapOutputTracker\n14/05/06 10:17:38 INFO HttpFileServer: HTTP File server directory is /tmp/spark-ed849647-1ea9-4446-9798-326ddf33c8da\n14/05/06 10:17:38 INFO HttpServer: Starting HTTP Server\n14/05/06 10:17:38 WARN Utils: Your hostname, m4600 resolves to a loopback address: 127.0.0.2; using 192.168.122.1 instead (on interface virbr0)\n14/05/06 10:17:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n14/05/06 10:17:57 INFO SharkContext: Starting job: sum at <console>:24\n14/05/06 10:17:57 INFO DAGScheduler: Got job 0 (sum at <console>:24) with 2 output partitions (allowLocal=false)\n14/05/06 10:17:57 INFO DAGScheduler: Final stage: Stage 0 (sum at <console>:24)\n14/05/06 10:17:57 INFO DAGScheduler: Parents of final stage: List()\n14/05/06 10:17:57 INFO DAGScheduler: Missing parents: List()\n14/05/06 10:17:57 INFO DAGScheduler: Submitting Stage 0 (MappedRDD[2] at numericRDDToDoubleRDDFunctions at <console>:24), which has no missing parents\n14/05/06 10:17:58 INFO DAGScheduler: Submitting 2 missing tasks from Stage 0 (MappedRDD[2] at numericRDDToDoubleRDDFunctions at <console>:24)\n14/05/06 10:17:58 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks\n14/05/06 10:17:58 INFO TaskSetManager: Starting task 0.0:0 as TID 0 on executor 0: 127.0.0.1 (PROCESS_LOCAL)\n14/05/06 10:17:59 INFO TaskSetManager: Serialized task 0.0:0 as 13890653 bytes in 615 ms\n{noformat}\n", "This is just a part of it,  the whole file upload?", "The whole part from before the moment I started the job.\nIt ends with {{14/05/06 10:17:59 INFO TaskSetManager: Serialized task 0.0:0 as 13890653 bytes in 615 ms}}", "{{TaskDescription}} instance is too big causes the issue. [CoarseGrainedSchedulerBackend.scala#L144|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala#L144]\n[~pwendell] This is a very serious bug", "Do you need more info / help reproducing this?\nI'm worried mostly not by the fact that it failed but because of the manifestation of the bug (no errors anywhere). \nIs it possible we misconfigured something related to logging or setup?", "Thank you, temporarily need not, I roughly to locate.I'm debugging", "We should probably add a check that the task is smaller than the akka frame size and throw an error message. Unfortunately akka fails silently here. See a similar check in the MapOutputTracker:\n\nhttps://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/MapOutputTracker.scala#L48", "[The PR 694|https://github.com/apache/spark/pull/694],but I think this solution imperfect.\n[Executor.scala#L235|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/executor/Executor.scala#L235] is a good solution.\n", "Merged the frame size check into 0.9.2 as well as 1.0.1"], "derived": {"summary": "conf/spark-defaults. conf\n{code}\nspark.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "ParallelCollectionRDD operations hanging forever without any error messages  - conf/spark-defaults. conf\n{code}\nspark."}, {"q": "What updates or decisions were made in the discussion?", "a": "Merged the frame size check into 0.9.2 as well as 1.0.1"}]}}
{"project": "SPARK", "issue_id": "SPARK-1713", "title": "YarnAllocationHandler starts a thread for every executor it runs", "status": "Resolved", "priority": "Major", "reporter": "Sandy Ryza", "assignee": "Sandy Ryza", "labels": [], "created": "2014-05-04T20:07:10.000+0000", "updated": "2014-09-10T19:35:15.000+0000", "description": null, "comments": ["https://github.com/apache/spark/pull/663"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "YarnAllocationHandler starts a thread for every executor it runs"}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/663"}]}}
{"project": "SPARK", "issue_id": "SPARK-1714", "title": "Take advantage of AMRMClient APIs to simplify logic in YarnAllocationHandler", "status": "Resolved", "priority": "Major", "reporter": "Sandy Ryza", "assignee": "Sandy Ryza", "labels": [], "created": "2014-05-04T20:21:43.000+0000", "updated": "2017-01-26T14:41:43.000+0000", "description": null, "comments": ["https://github.com/apache/spark/pull/655", "User 'sryza' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/3765", "{code}\n        if (completedContainer.getExitStatus == -103) { // vmem limit exceeded\n{code}\nShould ContainerExitStatus#KILLED_EXCEEDED_VMEM be referenced above ?", "allocatedHostToContainersMap.synchronized is absent for the following operation in runAllocatedContainers():\n{code}\n      val containerSet = allocatedHostToContainersMap.getOrElseUpdate(executorHostname,\n        new HashSet[ContainerId])\n\n      containerSet += containerId\n      allocatedContainerToHostMap.put(containerId, executorHostname)\n{code}\nIs that intentional ?", "No it doesn't need to be synchronized, its only called from within a synchronized method allocateResources().   The other case where it does synchronize in processCompletedContainers should be removed, I missed that wasn't removed in the final change.   [~sandyr] would you mind filing jira to remove that one too? Sorry I missed that.", "Oops yeah, my bad.  Filed SPARK-5370 and posted a patch.", "User 'sryza' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/4192"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Take advantage of AMRMClient APIs to simplify logic in YarnAllocationHandler"}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'sryza' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/4192"}]}}
{"project": "SPARK", "issue_id": "SPARK-1715", "title": "Ensure actor is self-contained in DAGScheduler", "status": "Closed", "priority": "Major", "reporter": "Nan Zhu", "assignee": "Nan Zhu", "labels": [], "created": "2014-05-04T22:47:51.000+0000", "updated": "2020-05-17T17:48:05.000+0000", "description": "Though the current supervisor-child structure works fine for fault-tolerance, it violates the basic rule that the actor is better to be self-contained\n\nWe should forward the message from supervisor to the child actor, so that we can eliminate the hard-coded timeout threshold for starting the DAGScheduler and provide more convenient interface for future development like parallel DAGScheduler, or new changes to the DAGScheduler", "comments": ["This shouldn't involve any change to the public API, so we should be able to do it already in 1.0.1.", "Akka actor has been removed from DAGScheduler"], "derived": {"summary": "Though the current supervisor-child structure works fine for fault-tolerance, it violates the basic rule that the actor is better to be self-contained\n\nWe should forward the message from supervisor to the child actor, so that we can eliminate the hard-coded timeout threshold for starting the DAGScheduler and provide more convenient interface for future development like parallel DAGScheduler, or new changes to the DAGScheduler.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Ensure actor is self-contained in DAGScheduler - Though the current supervisor-child structure works fine for fault-tolerance, it violates the basic rule that the actor is better to be self-contained\n\nWe should forward the message from supervisor to the child actor, so that we can eliminate the hard-coded timeout threshold for starting the DAGScheduler and provide more convenient interface for future development like parallel DAGScheduler, or new changes to the DAGScheduler."}, {"q": "What updates or decisions were made in the discussion?", "a": "Akka actor has been removed from DAGScheduler"}]}}
{"project": "SPARK", "issue_id": "SPARK-1716", "title": "EC2 script should exit with non-zero code on UsageError", "status": "Resolved", "priority": "Major", "reporter": "Allan Douglas R. de Oliveira", "assignee": "Allan Douglas R. de Oliveira", "labels": [], "created": "2014-05-05T01:13:38.000+0000", "updated": "2014-05-05T03:37:25.000+0000", "description": "One reason is that some ssh errors are raised as UsageError, preventing an automated usage of the script from detecting the failure.", "comments": ["PR here:\nhttps://github.com/apache/spark/pull/638", "Issue resolved by pull request 638\n[https://github.com/apache/spark/pull/638]"], "derived": {"summary": "One reason is that some ssh errors are raised as UsageError, preventing an automated usage of the script from detecting the failure.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "EC2 script should exit with non-zero code on UsageError - One reason is that some ssh errors are raised as UsageError, preventing an automated usage of the script from detecting the failure."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 638\n[https://github.com/apache/spark/pull/638]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1717", "title": "spark-ec2.py sometimes doesn't wait long enough for EC2 to stand up", "status": "Resolved", "priority": "Minor", "reporter": "Madhu Siddalingaiah", "assignee": null, "labels": ["easyfix"], "created": "2014-05-05T03:11:03.000+0000", "updated": "2014-05-05T04:59:46.000+0000", "description": "spark-ec2.py tries several times to perform operations on EC2. Sometimes EC2 takes longer than expected (I've seen this with spot instances, not sure if it's related) and spark-ec2.py raises an exception even though the instances do eventually come up.", "comments": ["Increased number of tries from 2 to 5 in two places.", "Issue resolved by pull request 641\n[https://github.com/apache/spark/pull/641]"], "derived": {"summary": "spark-ec2. py tries several times to perform operations on EC2.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "spark-ec2.py sometimes doesn't wait long enough for EC2 to stand up - spark-ec2. py tries several times to perform operations on EC2."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 641\n[https://github.com/apache/spark/pull/641]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1718", "title": "pyspark doesn't work with assembly jar containing over 65536 files/dirs built on redhat ", "status": "Resolved", "priority": "Major", "reporter": "Thomas Graves", "assignee": null, "labels": [], "created": "2014-05-05T13:07:14.000+0000", "updated": "2014-06-24T19:17:18.000+0000", "description": "Recently pyspark was ported to yarn (pr 30), but when I went to try it I couldn't get it work.  I was building on a redhat 6 box.  I figured out that if the assembly jar file contained over 65536 files/directories then it wouldn't work.  If I unjarred the assembly and removed some stuff to get it under 65536 and jarred it back up, then it would work.     \n\nIt appears to only be an issue when building on a redhat box as I can build on my mac and it works just fine there.   ", "comments": ["This is the same issue as https://issues.apache.org/jira/browse/SPARK-1520 right?", "No, see discussion on https://github.com/apache/spark/pull/30.   This happens if you build on a redhat box and you build and run with jdk7.  \n", "Yeah, but it seems like it could well be that the JDK binaries being used during the build aren't quite what is expected, because some home or path variable points to JDK6. That was the substance of Patrick's last comment, and I wasn't sure whether Andrew was definitely confirming the build happened with Java 7, just that it was installed. (?) \n\nI suppose it could also be some old version of zip being used to re-zip jars and such, though it strikes me as less likely, but hey.", "I can build the jar on my mac, copy it over to the same redhat boxes I run on and it works fine.  If it was the runtime environment was using jdk6 then that wouldn't work.  I assume you are saying that if you build the jar with jdk6 and try to run on jdk7 it has the same issue?\n\nI also checked the MANIFEST to verify it was build with jdk7.  \n\n$ cat MANIFEST.MF \nBuild-Jdk: 1.7.0_25\n\nI also went in and changed the pom.xml to use java version 1.7 as source and target, but that doesn't look like its working as when I check the .class files for the major version it comes back as 50 (jdk6) so perhaps this is what is causing the issue.\n\nIt could still be possible there is something in my environment causing it but as of yet haven't figured out what so wanted to file a jira to track the issue. I took Andrew's comment as he also tried it and ran into the same issue but perhaps I misunderstood.\nDo you happen to have redhat box you could try it on?  ", "I could be mistaken here. The primary problem combination is building with 7 and running with 6. But I also thought I understood that building or running with an older JDK 6 could be a problem too. (That, I am not 100% sure of.)\n\nIf you are running with 6, then you definitely don't want to build with 7. (The source/target can't be set to 7 in this case; either you build with 6 and it balks, or, you successfully build with 7 but at runtime, 6 won't accept the bytecode.) It sounds like you are building with 7 then? but is that your Mac build? if your RedHat build is using JDK 7, then I think this is just the same problem as in SPARK-1520 and you should use JDK 6 to build on that machine.\n\n(keep in mind that unzipping / rezipping, and unjarring / rejarring, might affect the result, as it affects the format of the .jar file! Worth noting whether that alone is causing or solving the issue.)\n\nIf you are sure you're building with 6, then my next question would be whether it's actually building with an older JDK 6, and whether that can be upgraded perhaps, and whether that resolves it. \n\nRunning on JDK 7 should be fine either way. I wasn't clear whether Andrew was saying that didn't work either: https://github.com/apache/spark/pull/30#issuecomment-42057384   But I assume the question is how to get it running on 6.", "I am running with jdk7 and building with jdk7.", "Yeah I may not be adding anything here. I suppose I just advise to double-check what's being used to build, to run, and anything in between (like zip or jar). \n\nLike, does the python-related build zip or jar anything? (I don't know that part of the build.) That could reintroduce the problem if something outside of Java land is not using the zip64 format.", "So this actually appears to be an issue because of using jdk7 on redhat.  If I switch back to use jdk6 then the build works and pyspark works.  Note that in both cases I'm using jdk7 to run with, so it doesn't appear to be the same as SPARK-1520\n"], "derived": {"summary": "Recently pyspark was ported to yarn (pr 30), but when I went to try it I couldn't get it work. I was building on a redhat 6 box.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "pyspark doesn't work with assembly jar containing over 65536 files/dirs built on redhat  - Recently pyspark was ported to yarn (pr 30), but when I went to try it I couldn't get it work. I was building on a redhat 6 box."}, {"q": "What updates or decisions were made in the discussion?", "a": "So this actually appears to be an issue because of using jdk7 on redhat.  If I switch back to use jdk6 then the build works and pyspark works.  Note that in both cases I'm using jdk7 to run with, so it doesn't appear to be the same as SPARK-1520"}]}}
{"project": "SPARK", "issue_id": "SPARK-1719", "title": "spark.executor.extraLibraryPath isn't applied on yarn", "status": "Resolved", "priority": "Major", "reporter": "Thomas Graves", "assignee": "Guoqiang Li", "labels": [], "created": "2014-05-05T14:40:28.000+0000", "updated": "2020-02-26T05:29:28.000+0000", "description": "Looking through the code for spark on yarn I don't see that spark.executor.extraLibraryPath is being properly applied when it launches executors.  It is using the spark.driver.libraryPath in the ClientBase.\n\nNote I didn't actually test it so its possible I missed something.\n\nI also think better to use LD_LIBRARY_PATH rather then -Djava.library.path.  once  java.library.path is set, it doesn't search LD_LIBRARY_PATH.  In Hadoop we switched to use LD_LIBRARY_PATH instead of java.library.path.  See https://issues.apache.org/jira/browse/MAPREDUCE-4072.  I'll split this into separate jira.", "comments": ["Is this related? When I run spark-shell with \"--master yarn --driver-library-path /opt/cloudera/parcels/GPLEXTRAS/lib/hadoop/lib/native/\", then I can load a lzo file as:\n\nval textFile = sc.textFile(\"/some/lzo/file.lzo\")\ntextFile.first()\n<returns a string from the file>\n\ntextFile.count()\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 3.0:0 failed 4 times, most recent failure: Exception failure in TID 7 on host node1-9-ops.abc.net: java.lang.RuntimeException: native-lzo library not available\n        com.hadoop.compression.lzo.LzopCodec.getDecompressorType(LzopCodec.java:96)\n        org.apache.hadoop.io.compress.CodecPool.getDecompressor(CodecPool.java:176)\n        org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:110)\n        org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n        org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:193)\n        org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:184)\n        org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:93)\n        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)\n        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)\n        org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)\n        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)\n        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)\n        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)\n        org.apache.spark.scheduler.Task.run(Task.scala:51)\n        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)\n        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        java.lang.Thread.run(Thread.java:745)\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1033)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1017)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1015)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1015)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:633)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:633)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:633)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1207)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:456)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:219)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n", "Seems to be related. ", "I looked through the pull request linked here and the pull request that closed the linked one and I can not see any reference to the spark.executor.extraLibraryPath. I went through the trunk and the only place I can see it is in the mesos code.\nCan you explain how the change in https://github.com/apache/spark/pull/1031 fixes the issue reported without referencing the setting at all?", "[~wilfreds] \nThe relevant code in [ExecutorRunnableUtil.scala#L62|https://github.com/witgo/spark/blob/SPARK-1720/yarn/common/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnableUtil.scala#L62]\n{code}\n    sys.props.get(\"spark.executor.extraLibraryPath\").foreach { p =>\n      val libraryPath = Seq(p, Utils.libraryPath$).mkString(File.pathSeparator)\n      javaOpts += s\"-Djava.library.path=$libraryPath\"\n    }\n{code}\neg: In linux \nspark-defaults.conf =>\n{noformat}\nspark.executor.extraLibraryPath /usr/local/hadoop/lib\n{noformat}\n\nExecutorRunnableUtil generated command line =>\n\n{noformat}\n java -server  ... -Djava.library.path=/usr/local/hadoop/lib:$LD_LIBRARY_PATH ... org.apache.spark.executor.CoarseGrainedExecutorBackend\n{noformat}\n", "oops, not sure how I missed that last file change :-(", "User 'witgo' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/2711", "Issue resolved by pull request 2711\n[https://github.com/apache/spark/pull/2711]\n"], "derived": {"summary": "Looking through the code for spark on yarn I don't see that spark. executor.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "spark.executor.extraLibraryPath isn't applied on yarn - Looking through the code for spark on yarn I don't see that spark. executor."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 2711\n[https://github.com/apache/spark/pull/2711]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1720", "title": "use LD_LIBRARY_PATH instead of -Djava.library.path", "status": "Resolved", "priority": "Critical", "reporter": "Thomas Graves", "assignee": "Guoqiang Li", "labels": [], "created": "2014-05-05T14:43:36.000+0000", "updated": "2014-10-30T09:58:34.000+0000", "description": "I think it would be better to use LD_LIBRARY_PATH rather then -Djava.library.path.  Once  java.library.path is set, it doesn't search LD_LIBRARY_PATH.  In Hadoop we switched to use LD_LIBRARY_PATH instead of java.library.path.  See https://issues.apache.org/jira/browse/MAPREDUCE-4072.", "comments": ["Another user reported this issue, so let's try to get it into spark 1.2", "User 'witgo' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/2711", "Issue resolved by pull request 2711\n[https://github.com/apache/spark/pull/2711]\n"], "derived": {"summary": "I think it would be better to use LD_LIBRARY_PATH rather then -Djava. library.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "use LD_LIBRARY_PATH instead of -Djava.library.path - I think it would be better to use LD_LIBRARY_PATH rather then -Djava. library."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 2711\n[https://github.com/apache/spark/pull/2711]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1721", "title": "Classloaders not used correctly in Mesos", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Bouke van der Bijl", "labels": [], "created": "2014-05-05T17:30:20.000+0000", "updated": "2014-05-05T18:20:50.000+0000", "description": "Reported in https://github.com/apache/spark/pull/620. \n\nBasically, there are still some cases not caught in SPARK-1480 where we still need to pass the correct classloader when loading classes.\n\n{code}\nI0502 17:31:42.672224 14688 exec.cpp:131] Version: 0.18.0\nI0502 17:31:42.674959 14707 exec.cpp:205] Executor registered on slave 20140501-182306-16842879-5050-10155-0\n14/05/02 17:31:42 INFO MesosExecutorBackend: Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n14/05/02 17:31:42 INFO MesosExecutorBackend: Registered with Mesos as executor ID 20140501-182306-16842879-5050-10155-0\n14/05/02 17:31:43 INFO SecurityManager: Changing view acls to: vagrant\n14/05/02 17:31:43 INFO SecurityManager: SecurityManager, is authentication enabled: false are ui acls enabled: false users with view permissions: Set(vagrant)\n14/05/02 17:31:43 INFO Slf4jLogger: Slf4jLogger started\n14/05/02 17:31:43 INFO Remoting: Starting remoting\n14/05/02 17:31:43 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://spark@localhost:50843]\n14/05/02 17:31:43 INFO Remoting: Remoting now listens on addresses: [akka.tcp://spark@localhost:50843]\njava.lang.ClassNotFoundException: org/apache/spark/serializer/JavaSerializer\n    at java.lang.Class.forName0(Native Method)\n    at java.lang.Class.forName(Class.java:270)\n    at org.apache.spark.SparkEnv$.instantiateClass$1(SparkEnv.scala:165)\n    at org.apache.spark.SparkEnv$.create(SparkEnv.scala:176)\n    at org.apache.spark.executor.Executor.<init>(Executor.scala:106)\n    at org.apache.spark.executor.MesosExecutorBackend.registered(MesosExecutorBackend.scala:56)\nException in thread \"Thread-0\" I0502 17:31:43.710039 14707 exec.cpp:412] Deactivating the executor libprocess\n{code}", "comments": ["Fixed by:\nhttps://github.com/apache/spark/pull/620"], "derived": {"summary": "Reported in https://github. com/apache/spark/pull/620.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Classloaders not used correctly in Mesos - Reported in https://github. com/apache/spark/pull/620."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed by:\nhttps://github.com/apache/spark/pull/620"}]}}
{"project": "SPARK", "issue_id": "SPARK-1722", "title": "Unable to create KafkaStream", "status": "Closed", "priority": "Major", "reporter": "William Droste", "assignee": null, "labels": [], "created": "2014-05-05T17:59:58.000+0000", "updated": "2014-05-05T19:52:18.000+0000", "description": "Getting a NoSuchMethodError when creating a Kafka stream.\n{code}\nscala>     val stream = KafkaUtils.createStream[String, ParameterMessage, StringDecoder, ParameterMessageDecoder](\n     |       ssc,\n     |       kafkaParams,\n     |       kafkaTopics,\n     |       StorageLevel.MEMORY_ONLY_SER_2\n     |     )\njava.lang.NoSuchMethodError: org.apache.spark.streaming.StreamingContext.getNewNetworkStreamId()I\n        at org.apache.spark.streaming.dstream.NetworkInputDStream.<init>(NetworkInputDStream.scala:53)\n        at org.apache.spark.streaming.kafka.KafkaInputDStream.<init>(KafkaInputDStream.scala:47)\n        at org.apache.spark.streaming.kafka.KafkaUtils$.createStream(KafkaUtils.scala:74)\n{code}", "comments": ["Used the wrong versions for build and runtime."], "derived": {"summary": "Getting a NoSuchMethodError when creating a Kafka stream. {code}\nscala>     val stream = KafkaUtils.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Unable to create KafkaStream - Getting a NoSuchMethodError when creating a Kafka stream. {code}\nscala>     val stream = KafkaUtils."}, {"q": "What updates or decisions were made in the discussion?", "a": "Used the wrong versions for build and runtime."}]}}
{"project": "SPARK", "issue_id": "SPARK-1723", "title": "Add saveAsLibSVMFile", "status": "Resolved", "priority": "Major", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-05-05T18:17:15.000+0000", "updated": "2014-05-06T05:18:19.000+0000", "description": "Provide a method to save labeled data in LIBSVM format.", "comments": ["https://github.com/apache/spark/pull/524"], "derived": {"summary": "Provide a method to save labeled data in LIBSVM format.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add saveAsLibSVMFile - Provide a method to save labeled data in LIBSVM format."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/524"}]}}
{"project": "SPARK", "issue_id": "SPARK-1724", "title": "Add appendBias", "status": "Resolved", "priority": "Major", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-05-05T18:19:09.000+0000", "updated": "2014-05-06T05:18:28.000+0000", "description": "Add `appendBias` to MLUtils to avoid adding the bias term in computation.", "comments": ["https://github.com/apache/spark/pull/524"], "derived": {"summary": "Add `appendBias` to MLUtils to avoid adding the bias term in computation.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add appendBias - Add `appendBias` to MLUtils to avoid adding the bias term in computation."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/524"}]}}
{"project": "SPARK", "issue_id": "SPARK-1725", "title": "Can't use broadcast variables in pyspark on Mesos because pyspark isn't added to PYTHONPATH", "status": "Resolved", "priority": "Blocker", "reporter": "Bouke van der Bijl", "assignee": "Bouke van der Bijl", "labels": [], "created": "2014-05-05T19:13:36.000+0000", "updated": "2014-05-09T03:44:31.000+0000", "description": null, "comments": ["Closed via:\nhttps://github.com/apache/spark/pull/651"], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Can't use broadcast variables in pyspark on Mesos because pyspark isn't added to PYTHONPATH"}, {"q": "What updates or decisions were made in the discussion?", "a": "Closed via:\nhttps://github.com/apache/spark/pull/651"}]}}
{"project": "SPARK", "issue_id": "SPARK-1726", "title": "Tasks that fail to serialize remain in active stages forever.", "status": "Resolved", "priority": "Major", "reporter": "Michael Armbrust", "assignee": "Kay Ousterhout", "labels": [], "created": "2014-05-05T22:10:41.000+0000", "updated": "2014-07-25T22:17:12.000+0000", "description": "In the spark shell.\n{code}\nscala> class Adder(x: Int) { def apply(a: Int) = a + x }\ndefined class Adder\nscala> val add = new Adder(10)\nscala> sc.parallelize(1 to 10).map(add(_)).collect()\n{code}\n\nYou get:\n{code}\norg.apache.spark.SparkException: Job aborted due to stage failure: Task not serializable: java.io.NotSerializableException: $iwC$$iwC$$iwC$$iwC$Adder\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1033)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1017)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1015)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1015)\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitMissingTasks(DAGScheduler.scala:770)\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:713)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1176)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:456)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:219)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n{code}\n\nHowever, the web ui is messed up.  See attached screen shot.", "comments": ["[~kayousterhout] reports this is fixed in master.", "[~marmbrus] Sorry I spoke too soon -- this is still broken but much less broken than before.  By default, SparkContext.clean() now checks if tasks are serializable, which typically prevents this problem because the stage never gets submitted.  However, checking if tasks are serializable can be disabled (see https://github.com/apache/spark/pull/143), in which case you can still see this problem.", "User 'kayousterhout' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1566"], "derived": {"summary": "In the spark shell. {code}\nscala> class Adder(x: Int) { def apply(a: Int) = a + x }\ndefined class Adder\nscala> val add = new Adder(10)\nscala> sc.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Tasks that fail to serialize remain in active stages forever. - In the spark shell. {code}\nscala> class Adder(x: Int) { def apply(a: Int) = a + x }\ndefined class Adder\nscala> val add = new Adder(10)\nscala> sc."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'kayousterhout' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1566"}]}}
{"project": "SPARK", "issue_id": "SPARK-1727", "title": "Correct small compile errors, typos, and markdown issues in (primarly) MLlib docs", "status": "Resolved", "priority": "Minor", "reporter": "Sean R. Owen", "assignee": "Sean R. Owen", "labels": [], "created": "2014-05-05T22:19:43.000+0000", "updated": "2015-01-15T09:08:38.000+0000", "description": "While play-testing the Scala and Java code examples in the MLlib docs, I noticed a number of small compile errors, and some typos. This led to finding and fixing a few similar items in other docs. \n\nThen in the course of building the site docs to check the result, I found a few small suggestions for the build instructions. I also found a few more formatting and markdown issues uncovered when I accidentally used maruku instead of kramdown.", "comments": ["Issue resolved by pull request 653\n[https://github.com/apache/spark/pull/653]"], "derived": {"summary": "While play-testing the Scala and Java code examples in the MLlib docs, I noticed a number of small compile errors, and some typos. This led to finding and fixing a few similar items in other docs.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Correct small compile errors, typos, and markdown issues in (primarly) MLlib docs - While play-testing the Scala and Java code examples in the MLlib docs, I noticed a number of small compile errors, and some typos. This led to finding and fixing a few similar items in other docs."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 653\n[https://github.com/apache/spark/pull/653]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1728", "title": "JavaRDDLike.mapPartitionsWithIndex requires ClassTag", "status": "Resolved", "priority": "Blocker", "reporter": "Sandy Ryza", "assignee": "Sandy Ryza", "labels": [], "created": "2014-05-05T22:24:13.000+0000", "updated": "2014-05-06T01:26:58.000+0000", "description": "We should pass a fake classtag like other JavaRDDLike methods", "comments": ["https://github.com/apache/spark/pull/657", "Issue resolved by pull request 657\n[https://github.com/apache/spark/pull/657]"], "derived": {"summary": "We should pass a fake classtag like other JavaRDDLike methods.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "JavaRDDLike.mapPartitionsWithIndex requires ClassTag - We should pass a fake classtag like other JavaRDDLike methods."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 657\n[https://github.com/apache/spark/pull/657]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1729", "title": "Make Flume pull data from source, rather than the current push model", "status": "Resolved", "priority": "Major", "reporter": "Tathagata Das", "assignee": "Hari Shreedharan", "labels": [], "created": "2014-05-05T23:43:15.000+0000", "updated": "2014-07-29T18:30:11.000+0000", "description": "This makes sure that the if the Spark executor running the receiver goes down, the new receiver on a new node can still get data from Flume.\n\nThis is not possible in the current model, as Flume is configured to push data to a executor/worker and if that worker is down, Flume cant push data.", "comments": ["[~hshreedharan] I assigned this to you as per our conversation in the previous JIRA.", "PR: https://github.com/apache/spark/pull/807", "Thanks for merging!"], "derived": {"summary": "This makes sure that the if the Spark executor running the receiver goes down, the new receiver on a new node can still get data from Flume. This is not possible in the current model, as Flume is configured to push data to a executor/worker and if that worker is down, Flume cant push data.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Make Flume pull data from source, rather than the current push model - This makes sure that the if the Spark executor running the receiver goes down, the new receiver on a new node can still get data from Flume. This is not possible in the current model, as Flume is configured to push data to a executor/worker and if that worker is down, Flume cant push data."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for merging!"}]}}
{"project": "SPARK", "issue_id": "SPARK-1730", "title": "Make receiver store data reliably to avoid data-loss on executor failures", "status": "Closed", "priority": "Major", "reporter": "Tathagata Das", "assignee": "Hari Shreedharan", "labels": [], "created": "2014-05-05T23:45:18.000+0000", "updated": "2014-08-01T20:44:16.000+0000", "description": null, "comments": ["[~hshreedharan] This could also be something you can work on after 1729. The way this can be done is to add a set of new methods to the Receiver name storeReliably, which will return true or false based on whether the data was successfully saved or not. And for now the system will decide true or false after the receiver has reported the name of the block to the driver. This ignores driver failures and assumes reporting to the driver is sufficient for ensuring reliable receiving. Once we have a better idea of driver recovery, we can figure out how to extend this to make it more reliable.", "I am starting work on this one. I have some ideas. I will post them on this jira soon.", "[~hshreedharan] I am closing this JIRA based on our previous conversation. SPARK-1729 was done using store(buffer), which returns only after replicating the block into multiple executors. So the transaction between Spark and the new Flume's Sink in SPARK-1729 succeeds only after data has been replicated. So executor failure should not cause data loss."], "derived": {"summary": "", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Make receiver store data reliably to avoid data-loss on executor failures"}, {"q": "What updates or decisions were made in the discussion?", "a": "[~hshreedharan] I am closing this JIRA based on our previous conversation. SPARK-1729 was done using store(buffer), which returns only after replicating the block into multiple executors. So the transaction between Spark and the new Flume's Sink in SPARK-1729 succeeds only after data has been replicated. So executor failure should not cause data loss."}]}}
{"project": "SPARK", "issue_id": "SPARK-1731", "title": "PySpark broadcast values with custom classes won't depickle properly", "status": "Resolved", "priority": "Major", "reporter": "Bouke van der Bijl", "assignee": "Bouke van der Bijl", "labels": [], "created": "2014-05-06T00:10:00.000+0000", "updated": "2014-05-10T20:02:41.000+0000", "description": null, "comments": ["Issue resolved by pull request 656\n[https://github.com/apache/spark/pull/656]"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "PySpark broadcast values with custom classes won't depickle properly"}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 656\n[https://github.com/apache/spark/pull/656]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1732", "title": "Support for primitive nulls in SparkSQL", "status": "Resolved", "priority": "Major", "reporter": "Michael Armbrust", "assignee": "Michael Armbrust", "labels": [], "created": "2014-05-06T00:21:43.000+0000", "updated": "2014-05-06T06:00:10.000+0000", "description": "Right now you can't have null values for primitives, we should support Option and primitive classes (Integer).", "comments": [], "derived": {"summary": "Right now you can't have null values for primitives, we should support Option and primitive classes (Integer).", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support for primitive nulls in SparkSQL - Right now you can't have null values for primitives, we should support Option and primitive classes (Integer)."}]}}
{"project": "SPARK", "issue_id": "SPARK-1733", "title": "Pluggable storage support for BlockManager", "status": "Resolved", "priority": "Major", "reporter": "Raymond Liu", "assignee": null, "labels": [], "created": "2014-05-06T01:07:16.000+0000", "updated": "2016-01-04T14:48:12.000+0000", "description": "Since there are Pluggable storage support mentioned in 1.0 meetup. And we are planning to do something on this part. So I create this issue to document some of my initial ideas. Just try to figure out what is the plan on this part, and  to find out what should be covered and have a discussion on the approaching", "comments": [], "derived": {"summary": "Since there are Pluggable storage support mentioned in 1. 0 meetup.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Pluggable storage support for BlockManager - Since there are Pluggable storage support mentioned in 1. 0 meetup."}]}}
{"project": "SPARK", "issue_id": "SPARK-1734", "title": "spark-submit throws an exception: Exception in thread \"main\" java.lang.ClassNotFoundException: org.apache.spark.broadcast.TorrentBroadcastFactory", "status": "Resolved", "priority": "Blocker", "reporter": "Guoqiang Li", "assignee": null, "labels": [], "created": "2014-05-06T02:38:52.000+0000", "updated": "2014-05-06T21:18:06.000+0000", "description": "{{conf/spark-defaults.conf}}:\n{code}\nspark.broadcast.factory      org.apache.spark.broadcast.TorrentBroadcastFactory\n{code}\n{{./bin/spark-submit /opt/spark/classes/toona-assembly-1.0.2-SNAPSHOT.jar  --verbose  --master spark://spark:7077 --deploy-mode client --class com.zhe800.toona.knn.computation.DealKNN}} =>\n{code}\nException in thread \"main\" java.lang.ClassNotFoundException: org.apache.spark.broadcast.TorrentBroadcastFactory\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:372)\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:361)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:360)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:259)\n\tat org.apache.spark.broadcast.BroadcastManager.initialize(BroadcastManager.scala:43)\n\tat org.apache.spark.broadcast.BroadcastManager.<init>(BroadcastManager.scala:33)\n\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:218)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:196)\n\tat com.zhe800.toona.knn.computation.DealKNN$.main(DealKNN.scala:93)\n\tat com.zhe800.toona.knn.computation.DealKNN.main(DealKNN.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:258)\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:55)\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n{code}", "comments": ["This is pretty strange, I would think this should work since spark-submit itself is launched with Spark in the classpath.\n\nOne thing would be to see if this fixes it... I'm not proposing this as a solution, just a debug step:\n{code}  \nbroadcastFactory =\n  Class.forName(broadcastFactoryClass).newInstance.asInstanceOf[BroadcastFactory]\n{code}\n\n{code}\nbroadcastFactory = Class.forName(broadcastFactoryClass, false, Utils.getSparkClassLoader)\n   .newInstance.asInstanceOf[BroadcastFactory]\n{code}\n", "There are spaces behind the {{org.apache.spark.broadcast.TorrentBroadcastFactory}} \n[PR 665|https://github.com/apache/spark/pull/665]", "Issue resolved by pull request 665\n[https://github.com/apache/spark/pull/665]"], "derived": {"summary": "{{conf/spark-defaults. conf}}:\n{code}\nspark.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "spark-submit throws an exception: Exception in thread \"main\" java.lang.ClassNotFoundException: org.apache.spark.broadcast.TorrentBroadcastFactory - {{conf/spark-defaults. conf}}:\n{code}\nspark."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 665\n[https://github.com/apache/spark/pull/665]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1735", "title": "Make distribution script has missing profiles for special hadoop versions", "status": "Resolved", "priority": "Minor", "reporter": "Andrew Or", "assignee": "Andrew Or", "labels": [], "created": "2014-05-06T03:37:22.000+0000", "updated": "2014-11-05T10:45:24.000+0000", "description": "SPARK-1556 introduced new profiles for hadoop versions 2.2.x, 2.3.x and 2.4.x, such that without these a java version error will be thrown at run time.", "comments": ["Issue resolved by pull request 660\n[https://github.com/apache/spark/pull/660]"], "derived": {"summary": "SPARK-1556 introduced new profiles for hadoop versions 2. 2.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Make distribution script has missing profiles for special hadoop versions - SPARK-1556 introduced new profiles for hadoop versions 2. 2."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 660\n[https://github.com/apache/spark/pull/660]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1736", "title": "spark-submit on Windows", "status": "Resolved", "priority": "Blocker", "reporter": "Matei Alexandru Zaharia", "assignee": "Andrew Or", "labels": [], "created": "2014-05-06T09:02:52.000+0000", "updated": "2014-11-05T10:45:26.000+0000", "description": "- spark-submit needs a Windows version (shouldn't be too hard, it's just launching a Java process)\n- spark-shell.cmd needs to run through spark-submit like it does on Unix", "comments": ["Fixed in: \nhttps://github.com/apache/spark/pull/745"], "derived": {"summary": "- spark-submit needs a Windows version (shouldn't be too hard, it's just launching a Java process)\n- spark-shell. cmd needs to run through spark-submit like it does on Unix.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "spark-submit on Windows - - spark-submit needs a Windows version (shouldn't be too hard, it's just launching a Java process)\n- spark-shell. cmd needs to run through spark-submit like it does on Unix."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed in: \nhttps://github.com/apache/spark/pull/745"}]}}
{"project": "SPARK", "issue_id": "SPARK-1737", "title": "Warn rather than fail when Java 7+ is used to create distributions", "status": "Resolved", "priority": "Major", "reporter": "Daniel Fry", "assignee": "Patrick Wendell", "labels": [], "created": "2014-05-06T14:24:21.000+0000", "updated": "2014-05-06T22:42:10.000+0000", "description": "the merged PR for SPARK-1703 made it so that make-distribution.sh will never work with JDK 1.7+\n\nwhy not have make-distribution.sh perform a check similar to compute-classpath.sh?\nhttps://github.com/apache/spark/pull/627/files", "comments": ["Even though SPARK-1703 adds logging, once the user sees this logging it's too late, so the idea here was to try and make sure people creating distributions understood the bug.\n\nI'd be fine to add a flag called --allow-java-7 to make-distribution that avoids this check.", "thanks for the insight & flexibility; i'm not quite sure i understand though, is Spark not expected to work properly/well if the user builds it with JDK-7 and runs with JDK-7 ?", "Is there same issue between JDK-7 and JDK8? At the time when there's a tendency of introducing Java 8 features to Spark I don't see a good reason why limit Spark only to JDK-6 (http://blog.cloudera.com/blog/2014/04/making-apache-spark-easier-to-use-in-java-with-java-8/)\n\nWouldn't be better if `make-distribution.sh` would store JAVA_VERSION in the distribution and then check it against JAVA_VERSION on slave node?", "Yes let's actually make this more tame. We can just log a warning in make-distribution that explains if it's compiled with Java 7+ it won't work on Java 6. Indeed, some people will even want to compile with Java 8.", "https://github.com/apache/spark/pull/669/files", "The only problem is if Spark is built with Java 7 or 8 and run with Java 6. We just want to make it clear to someone building that they might hit this problem.", "Issue resolved by pull request 669\n[https://github.com/apache/spark/pull/669]"], "derived": {"summary": "the merged PR for SPARK-1703 made it so that make-distribution. sh will never work with JDK 1.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Warn rather than fail when Java 7+ is used to create distributions - the merged PR for SPARK-1703 made it so that make-distribution. sh will never work with JDK 1."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 669\n[https://github.com/apache/spark/pull/669]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1738", "title": "Is spark-debugger still available?", "status": "Resolved", "priority": "Minor", "reporter": "Tao Wang", "assignee": null, "labels": [], "created": "2014-05-06T16:20:48.000+0000", "updated": "2014-10-13T17:11:04.000+0000", "description": "I see the \"arthur branch\"(https://github.com/apache/spark/tree/arthur) described in docs/spark-debugger.md does not exist.\nSo the spark-debugger is still available? If not, should the document be deleted?", "comments": ["That document was since deleted at some point anyway, and I assume the answer is that it does not exit."], "derived": {"summary": "I see the \"arthur branch\"(https://github. com/apache/spark/tree/arthur) described in docs/spark-debugger.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Is spark-debugger still available? - I see the \"arthur branch\"(https://github. com/apache/spark/tree/arthur) described in docs/spark-debugger."}, {"q": "What updates or decisions were made in the discussion?", "a": "That document was since deleted at some point anyway, and I assume the answer is that it does not exit."}]}}
{"project": "SPARK", "issue_id": "SPARK-1739", "title": "Close PR's after period of inactivity", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Josh Rosen", "labels": ["bulk-closed"], "created": "2014-05-06T17:00:16.000+0000", "updated": "2019-06-06T14:01:51.000+0000", "description": "Sometimes PR's get abandoned if people aren't responsive to feedback or it just falls to a lower priority. We should automatically close stale PR's in order to keep the queue from growing infinitely.\n\nI think we just want to do this with a friendly message that says \"This seems inactive, please re-open this if you are interested in contributing the patch.\". We should also explicitly ping any reviewers (via @mentioning) them and ask them to provide feedback one way or the other, for instance, if the feature is being rejected.\n\nThis will help us avoid letting features slip through the cracks by forcing some action when there is no activity after 30 days. Also, it's ASF policy that we should really be tracking our feature backlog and prioritization in JIRA and only be using Github for active reviews.\n\nI don't think we should close it if there was _no_ feedback from any reviewer - in that case we should leave it open (we should be providing at least some feedback on all incoming patches).", "comments": ["We've introduced a different mechanism for manual closing, so I don't think this is necessary.", "I'd actually like to re-open this since I think there is still a need for it. Will update the description.", "Sounds great. Even better if old PRs with no review can get an automated comment pinging the maintainer. Not sure if that is easy or not. ", "My proposal would be to have SparkQA post a comment in the PR that mentions the component maintainers.  This could happen once a PR sits inactive or unreviewed for more than X days.  I can do this myself, but I'm kind of overloaded with other work so this is going to be a low priority.  I'd welcome pull requests for this, though: https://github.com/databricks/spark-pr-dashboard"], "derived": {"summary": "Sometimes PR's get abandoned if people aren't responsive to feedback or it just falls to a lower priority. We should automatically close stale PR's in order to keep the queue from growing infinitely.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Close PR's after period of inactivity - Sometimes PR's get abandoned if people aren't responsive to feedback or it just falls to a lower priority. We should automatically close stale PR's in order to keep the queue from growing infinitely."}, {"q": "What updates or decisions were made in the discussion?", "a": "My proposal would be to have SparkQA post a comment in the PR that mentions the component maintainers.  This could happen once a PR sits inactive or unreviewed for more than X days.  I can do this myself, but I'm kind of overloaded with other work so this is going to be a low priority.  I'd welcome pull requests for this, though: https://github.com/databricks/spark-pr-dashboard"}]}}
{"project": "SPARK", "issue_id": "SPARK-1740", "title": "Pyspark cancellation kills unrelated pyspark workers", "status": "Resolved", "priority": "Critical", "reporter": "Aaron Davidson", "assignee": "Davies Liu", "labels": [], "created": "2014-05-06T19:28:36.000+0000", "updated": "2014-08-03T22:53:37.000+0000", "description": "PySpark cancellation calls SparkEnv#destroyPythonWorker. Since there is one python worker per process, this would seem like a sensible thing to do. Unfortunately, this method actually destroys a python daemon, and all associated workers, which generally means that we can cause failures in unrelated Pyspark jobs.\n\nThe severity of this bug is limited by the fact that the Pyspark daemon is easily recreated, so the tasks will succeed after being restarted.", "comments": ["On yarn when I run pyspark it kills the executors after a single action.  Perhaps this is caused by this same issue. \n\nThis could be a much bigger deal on yarn since when it kills the executors, it has to go back to the resource manager to get more containers.  This is an aweful lot of thrashing of containers and could cause major headaches.", "The \"Python daemon -> multiple workers\" architecture was motivated by the high cost of forking a JVM with a large heap, so this issue could also lead to performance problems if we attempt to re-launch the daemon once a huge amount of data has been cached in the Spark worker JVM.", "User 'davies' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1643"], "derived": {"summary": "PySpark cancellation calls SparkEnv#destroyPythonWorker. Since there is one python worker per process, this would seem like a sensible thing to do.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Pyspark cancellation kills unrelated pyspark workers - PySpark cancellation calls SparkEnv#destroyPythonWorker. Since there is one python worker per process, this would seem like a sensible thing to do."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'davies' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1643"}]}}
{"project": "SPARK", "issue_id": "SPARK-1741", "title": "Add predict(JavaRDD) to predictive models", "status": "Resolved", "priority": "Major", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-05-06T21:30:04.000+0000", "updated": "2014-05-15T19:01:19.000+0000", "description": "`model.predict` returns a RDD of Scala primitive type (Int/Double), which is recognized as Object in Java. Adding predict(JavaRDD) could make life easier for Java users.", "comments": ["https://github.com/apache/spark/pull/670", "Issue resolved by pull request 670\n[https://github.com/apache/spark/pull/670]", "This might end up in 1.0 or not depending on whether we release the current RC."], "derived": {"summary": "`model. predict` returns a RDD of Scala primitive type (Int/Double), which is recognized as Object in Java.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add predict(JavaRDD) to predictive models - `model. predict` returns a RDD of Scala primitive type (Int/Double), which is recognized as Object in Java."}, {"q": "What updates or decisions were made in the discussion?", "a": "This might end up in 1.0 or not depending on whether we release the current RC."}]}}
{"project": "SPARK", "issue_id": "SPARK-1742", "title": "Profiler for Spark", "status": "Resolved", "priority": "Major", "reporter": "Kousuke Saruta", "assignee": null, "labels": [], "created": "2014-05-06T23:55:53.000+0000", "updated": "2015-02-07T16:43:38.000+0000", "description": "Sometimes, I hope there were the profiler for Spark Job.\nI think it's useful to find critical path of DAG or  find the bottle-necked Stage / transformations.\n", "comments": ["Is it realistic to expect that this would be part of Spark? You can always attach a standard Java profiler to an executor, and there are rich tools for that. Anything else developed is probably a third party project right?", "It's since been documented how to use standard profiling tools with Spark:\n\nhttps://cwiki.apache.org/confluence/display/SPARK/Profiling+Spark+Applications+Using+YourKit"], "derived": {"summary": "Sometimes, I hope there were the profiler for Spark Job. I think it's useful to find critical path of DAG or  find the bottle-necked Stage / transformations.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Profiler for Spark - Sometimes, I hope there were the profiler for Spark Job. I think it's useful to find critical path of DAG or  find the bottle-necked Stage / transformations."}, {"q": "What updates or decisions were made in the discussion?", "a": "It's since been documented how to use standard profiling tools with Spark:\n\nhttps://cwiki.apache.org/confluence/display/SPARK/Profiling+Spark+Applications+Using+YourKit"}]}}
{"project": "SPARK", "issue_id": "SPARK-1743", "title": "Add mllib.util.MLUtils.{loadLibSVMFile, saveAsLibSVMFile} to pyspark", "status": "Resolved", "priority": "Major", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-05-06T23:56:40.000+0000", "updated": "2014-05-07T23:01:44.000+0000", "description": "Make loading/saving labeled data easier for pyspark users.", "comments": ["PR: https://github.com/apache/spark/pull/672", "Fixed in https://github.com/apache/spark/pull/672"], "derived": {"summary": "Make loading/saving labeled data easier for pyspark users.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add mllib.util.MLUtils.{loadLibSVMFile, saveAsLibSVMFile} to pyspark - Make loading/saving labeled data easier for pyspark users."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed in https://github.com/apache/spark/pull/672"}]}}
{"project": "SPARK", "issue_id": "SPARK-1744", "title": "Document how to pass in preferredNodeLocationData", "status": "Resolved", "priority": "Major", "reporter": "Sandy Ryza", "assignee": null, "labels": [], "created": "2014-05-07T01:39:41.000+0000", "updated": "2015-07-23T22:15:15.000+0000", "description": null, "comments": ["Ditto this one -- still relevant?"], "derived": {"summary": "", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Document how to pass in preferredNodeLocationData"}, {"q": "What updates or decisions were made in the discussion?", "a": "Ditto this one -- still relevant?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1745", "title": "TaskContext.interrupted should probably not be a constructor argument", "status": "Resolved", "priority": "Minor", "reporter": "Andrew Or", "assignee": "Andrew Or", "labels": [], "created": "2014-05-07T03:35:28.000+0000", "updated": "2014-11-05T10:45:24.000+0000", "description": "It makes little sense to start a TaskContext that is interrupted. Indeed, I searched for all use cases of it and didn't find a single instance in which interrupted is true on construction.", "comments": [], "derived": {"summary": "It makes little sense to start a TaskContext that is interrupted. Indeed, I searched for all use cases of it and didn't find a single instance in which interrupted is true on construction.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "TaskContext.interrupted should probably not be a constructor argument - It makes little sense to start a TaskContext that is interrupted. Indeed, I searched for all use cases of it and didn't find a single instance in which interrupted is true on construction."}]}}
{"project": "SPARK", "issue_id": "SPARK-1746", "title": "Support setting SPARK_JAVA_OPTS on executors for backwards compatibility", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "labels": [], "created": "2014-05-07T04:50:01.000+0000", "updated": "2014-05-07T07:11:43.000+0000", "description": "This is subsumed by the new config system, but we should support it (with a warning) for compatibility.", "comments": ["Closed via https://github.com/apache/spark/pull/676"], "derived": {"summary": "This is subsumed by the new config system, but we should support it (with a warning) for compatibility.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Support setting SPARK_JAVA_OPTS on executors for backwards compatibility - This is subsumed by the new config system, but we should support it (with a warning) for compatibility."}, {"q": "What updates or decisions were made in the discussion?", "a": "Closed via https://github.com/apache/spark/pull/676"}]}}
{"project": "SPARK", "issue_id": "SPARK-1747", "title": "check for Spark on Yarn ApplicationMaster split brain", "status": "Resolved", "priority": "Major", "reporter": "Thomas Graves", "assignee": null, "labels": ["bulk-closed"], "created": "2014-05-07T14:04:46.000+0000", "updated": "2019-05-21T05:36:42.000+0000", "description": "On yarn there is a possibility that applications can end up with an issue referred to as \"split brain\".  This problem is that you have one Application Master running, something happens like a network split that the AM can no longer talk to the ResourceManager. After some time the ResourceManager will start a new application attempt assuming the old one failed and you end up with 2 application masters.  Note the network split could prevent it from talking to the RM but it could still be running along contacting regular executors. \n\nIf the previous AM does not need any more resources from the RM it could try to commit. This could cause lots of problems where the second AM finishes and tries to commit too. This could potentially result in data corruption.\n\nI believe this same issue can happen on Spark since its using the hadoop output formats.  One instance that has this issue is the FileOutputCommitter.  It first writes to a temporary directory (task commit) and then  moves the file to the final directory (job commit).  The first AM could finish the job commit, tell the user its done, the user starts another down stream job, but then the second AM comes in to do the job commit and files the down stream job are processing could disappear until the second AM finishes the job commit. \n\nThis was fixed in MR by https://issues.apache.org/jira/browse/MAPREDUCE-4832", "comments": ["[~tgraves] is there a chance this is going to result in a change?", "This should stay open. "], "derived": {"summary": "On yarn there is a possibility that applications can end up with an issue referred to as \"split brain\". This problem is that you have one Application Master running, something happens like a network split that the AM can no longer talk to the ResourceManager.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "check for Spark on Yarn ApplicationMaster split brain - On yarn there is a possibility that applications can end up with an issue referred to as \"split brain\". This problem is that you have one Application Master running, something happens like a network split that the AM can no longer talk to the ResourceManager."}, {"q": "What updates or decisions were made in the discussion?", "a": "This should stay open."}]}}
{"project": "SPARK", "issue_id": "SPARK-1748", "title": "I installed the spark_standalone,but I did not know how to use sbt to compile the programme of spark?", "status": "Closed", "priority": "Major", "reporter": "lxflyl", "assignee": null, "labels": [], "created": "2014-05-07T14:43:38.000+0000", "updated": "2014-09-21T14:16:06.000+0000", "description": "I installed the mode of spark standalone ,but I did not understand how to use sbt to compile the program of spark", "comments": ["thanks for the question. you'll get a better response asking on the mailing lists, see http://spark.apache.org/community.html, so i'm going to close this out."], "derived": {"summary": "I installed the mode of spark standalone ,but I did not understand how to use sbt to compile the program of spark.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "I installed the spark_standalone,but I did not know how to use sbt to compile the programme of spark? - I installed the mode of spark standalone ,but I did not understand how to use sbt to compile the program of spark."}, {"q": "What updates or decisions were made in the discussion?", "a": "thanks for the question. you'll get a better response asking on the mailing lists, see http://spark.apache.org/community.html, so i'm going to close this out."}]}}
{"project": "SPARK", "issue_id": "SPARK-1749", "title": "DAGScheduler supervisor strategy broken with Mesos", "status": "Resolved", "priority": "Blocker", "reporter": "Bouke van der Bijl", "assignee": "Mark Hamstra", "labels": ["mesos", "scheduler", "scheduling"], "created": "2014-05-07T19:19:17.000+0000", "updated": "2014-06-26T03:58:15.000+0000", "description": "Any bad Python code will trigger this bug, for example `sc.parallelize(range(100)).map(lambda n: undefined_variable * 2).collect()` will cause a `undefined_variable isn't defined`, which will cause spark to try to kill the task, resulting in the following stacktrace:\n\njava.lang.UnsupportedOperationException\n\tat org.apache.spark.scheduler.SchedulerBackend$class.killTask(SchedulerBackend.scala:32)\n\tat org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend.killTask(MesosSchedulerBackend.scala:41)\n\tat org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:184)\n\tat org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:182)\n\tat org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:182)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$3.apply(TaskSchedulerImpl.scala:182)\n\tat org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$3.apply(TaskSchedulerImpl.scala:175)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:175)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1058)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1045)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1045)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1045)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:998)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:499)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:499)\n\tat org.apache.spark.scheduler.DAGSchedulerActorSupervisor$$anonfun$2.applyOrElse(DAGScheduler.scala:1151)\n\tat org.apache.spark.scheduler.DAGSchedulerActorSupervisor$$anonfun$2.applyOrElse(DAGScheduler.scala:1147)\n\tat akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:295)\n\tat akka.actor.dungeon.FaultHandling$class.handleFailure(FaultHandling.scala:253)\n\tat akka.actor.ActorCell.handleFailure(ActorCell.scala:338)\n\tat akka.actor.ActorCell.invokeAll$1(ActorCell.scala:423)\n\tat akka.actor.ActorCell.systemInvoke(ActorCell.scala:447)\n\tat akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:262)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:218)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n\nThis is because killTask isn't implemented for the MesosSchedulerBackend. I assume this isn't pyspark-specific, as there will be other instances where you might want to kill the task ", "comments": ["The failure is bad, but not all that bad, since it occurs when the Supervisor actor in the DAGScheduler is already trying to cancel running jobs and shut down the system after having caught an exception thrown during the processing of DAGScheduler event.\n\nI'm not going to try to fix anything in PySpark in the PR addressing this issue -- in part because it isn't clear to me from the stack trace just what PySpark is doing to cause the failure in the DAGScheduler; but once the DAGScheduler does catch the exception thrown by the eventProcessActor, it should handle it better when Mesos is doing the task scheduling, and I can fix that.\n\nAnother issue specific to Python may need to be filed as a follow up.  ", "This isn't really PySpark specific, this works fine on other backends which will mark the task as failed and just keep the SparkContext running.\n\nIt shouldn't be shutting down the whole SparkContext just because a single job failed", "Issue resolved by pull request 1219\n[https://github.com/apache/spark/pull/1219]"], "derived": {"summary": "Any bad Python code will trigger this bug, for example `sc. parallelize(range(100)).", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "DAGScheduler supervisor strategy broken with Mesos - Any bad Python code will trigger this bug, for example `sc. parallelize(range(100))."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 1219\n[https://github.com/apache/spark/pull/1219]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1750", "title": "EdgePartition is not serialized properly", "status": "Resolved", "priority": "Major", "reporter": "Ankur Dave", "assignee": "Joseph E. Gonzalez", "labels": [], "created": "2014-05-07T20:54:26.000+0000", "updated": "2014-05-26T20:20:38.000+0000", "description": "The GraphX design attempts to avoid moving edges across the network, instead shipping the vertices to the edge partitions. However, Spark sometimes needs to move the edges, such as for straggler mitigation.\n\nAll EdgePartition fields are currently declared transient, so the edges will not be serialized properly. Even if they are not marked transient, Kryo is unable to serialize the EdgePartition, failing with the following error:\n\n{code}\njava.lang.IllegalArgumentException: Can not set final org.apache.spark.graphx.util.collection.PrimitiveKeyOpenHashMap field org.apache.spark.graphx.impl.EdgePartition.index to scala.collection.immutable.$colon$colon\n{code}\n\nA workaround is to discourage Spark from moving the edges by setting {{spark.locality.wait}} to a high value such as 100000.", "comments": ["I believe this issue is resolved with PR #724.\n\n", "https://github.com/apache/spark/pull/742"], "derived": {"summary": "The GraphX design attempts to avoid moving edges across the network, instead shipping the vertices to the edge partitions. However, Spark sometimes needs to move the edges, such as for straggler mitigation.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "EdgePartition is not serialized properly - The GraphX design attempts to avoid moving edges across the network, instead shipping the vertices to the edge partitions. However, Spark sometimes needs to move the edges, such as for straggler mitigation."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/742"}]}}
{"project": "SPARK", "issue_id": "SPARK-1751", "title": "spark ec2 scripts should check for SSh to be up", "status": "Resolved", "priority": "Trivial", "reporter": "Holden Karau", "assignee": "Nicholas Chammas", "labels": [], "created": "2014-05-07T21:15:26.000+0000", "updated": "2014-12-13T22:21:11.000+0000", "description": "When launching a large cluster you currently need large delay for after launch so that ssh has come up on the hosts. Instead we could poll for ssh being up on the hosts.", "comments": ["This was addressed by SPARK-3398, so I'm marking this issue as 'Fixed'."], "derived": {"summary": "When launching a large cluster you currently need large delay for after launch so that ssh has come up on the hosts. Instead we could poll for ssh being up on the hosts.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "spark ec2 scripts should check for SSh to be up - When launching a large cluster you currently need large delay for after launch so that ssh has come up on the hosts. Instead we could poll for ssh being up on the hosts."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was addressed by SPARK-3398, so I'm marking this issue as 'Fixed'."}]}}
{"project": "SPARK", "issue_id": "SPARK-1752", "title": "Standardize input/output format for vectors and labeled points", "status": "Resolved", "priority": "Critical", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-05-07T21:56:54.000+0000", "updated": "2014-06-04T19:57:29.000+0000", "description": "We should standardize the text format used to represent vectors and labeled points. The proposed formats are the following:\n\n1. dense vector: [v0,v1,..]\n2. sparse vector: (size,[i0,i1],[v0,v1])\n3. labeled point: (label,vector)\n\nwhere \"(..)\" indicates a tuple and \"[...]\" indicate an array. Those are compatible with Python's syntax and can be easily parsed using `eval`.", "comments": [], "derived": {"summary": "We should standardize the text format used to represent vectors and labeled points. The proposed formats are the following:\n\n1.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Standardize input/output format for vectors and labeled points - We should standardize the text format used to represent vectors and labeled points. The proposed formats are the following:\n\n1."}]}}
{"project": "SPARK", "issue_id": "SPARK-1753", "title": "PySpark on YARN does not work on assembly jar built on Red Hat based OS", "status": "Resolved", "priority": "Major", "reporter": "Andrew Or", "assignee": "Andrew Or", "labels": [], "created": "2014-05-08T00:36:47.000+0000", "updated": "2015-03-03T14:40:18.000+0000", "description": "If the jar is built on a Red Hat based OS, the additional python files included in the jar cannot be accessed. This means PySpark doesn't work on YARN because in this mode it relies on the python files within this jar.\n\nI have confirmed that my Java, Scala, and maven versions are all exactly the same on my CentOS environment and on my local OSX environment, and the former does not work. Thomas Graves also struggled with the same problem.\n\nUntil a fix is found, we should at the very least document this peculiarity.", "comments": ["Issue resolved by pull request 701\n[https://github.com/apache/spark/pull/701]"], "derived": {"summary": "If the jar is built on a Red Hat based OS, the additional python files included in the jar cannot be accessed. This means PySpark doesn't work on YARN because in this mode it relies on the python files within this jar.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "PySpark on YARN does not work on assembly jar built on Red Hat based OS - If the jar is built on a Red Hat based OS, the additional python files included in the jar cannot be accessed. This means PySpark doesn't work on YARN because in this mode it relies on the python files within this jar."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 701\n[https://github.com/apache/spark/pull/701]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1754", "title": "Add missing arithmetic DSL operations.", "status": "Resolved", "priority": "Major", "reporter": "Takuya Ueshin", "assignee": "Takuya Ueshin", "labels": [], "created": "2014-05-08T03:24:05.000+0000", "updated": "2014-05-08T22:33:30.000+0000", "description": "Add missing arithmetic DSL operations: {{unary_-}}, {{%}}.", "comments": ["Pull-requested: https://github.com/apache/spark/pull/689", "Resolved in:\nhttps://github.com/apache/spark/pull/689"], "derived": {"summary": "Add missing arithmetic DSL operations: {{unary_-}}, {{%}}.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add missing arithmetic DSL operations. - Add missing arithmetic DSL operations: {{unary_-}}, {{%}}."}, {"q": "What updates or decisions were made in the discussion?", "a": "Resolved in:\nhttps://github.com/apache/spark/pull/689"}]}}
{"project": "SPARK", "issue_id": "SPARK-1755", "title": "Spark-submit --name does not resolve to application name on YARN", "status": "Resolved", "priority": "Blocker", "reporter": "Andrew Or", "assignee": "Andrew Or", "labels": [], "created": "2014-05-08T04:15:28.000+0000", "updated": "2014-11-05T10:45:21.000+0000", "description": "In YARN client mode, --name is ignored because the deploy mode is client, and the name is for some reason a [cluster config|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala#L170)].\n\nIn YARN cluster mode, --name is passed to the org.apache.spark.deploy.yarn.Client as a command line argument. The Client class, however, uses this name only as the [app name for the RM|https://github.com/apache/spark/blob/master/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/Client.scala#L80], but not for Spark. In other words, when SparkConf attempts to load default configs, application name is not set.\n\nIn both cases, passing --name to SparkSubmit does not actually cause Spark to adopt it as its application name, despite what the usage promises.", "comments": ["SPARK-1631 is concerned with fixing the case in which the appName is set manually through the SparkConf, while SPARK-1755 is concerned with SparkSubmit.", "I believe this is a dup of SPARK-1664\nspark-submit --name doesn't work in yarn-client mode", "You are correct. However, it also doesn't work for yarn-cluster.", "https://github.com/apache/spark/pull/699", "Issue resolved by pull request 699\n[https://github.com/apache/spark/pull/699]"], "derived": {"summary": "In YARN client mode, --name is ignored because the deploy mode is client, and the name is for some reason a [cluster config|https://github. com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Spark-submit --name does not resolve to application name on YARN - In YARN client mode, --name is ignored because the deploy mode is client, and the name is for some reason a [cluster config|https://github. com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/SparkSubmit."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 699\n[https://github.com/apache/spark/pull/699]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1756", "title": "Add missing description to spark-env.sh.template", "status": "Resolved", "priority": "Major", "reporter": "Guoqiang Li", "assignee": "Guoqiang Li", "labels": [], "created": "2014-05-08T06:01:02.000+0000", "updated": "2014-05-11T06:11:32.000+0000", "description": null, "comments": ["[PR 646| https://github.com/apache/spark/pull/646]", "This option is deprecated now and has been replaced by setting --driver-memory. If there is a case where this isn't sufficient, please re-open this and describe.", "Hi [~pwendell]\n\n{{./bin/spark-shell --driver-memory 2g}} => \nSpark Command:\n{code}\n/System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home/bin/java -cp ::/Users/witgo/work/code/java/spark/dist/conf:/Users/witgo/work/code/java/spark/dist/lib/spark-assembly-1.0.0-SNAPSHOT-hadoop0.23.9.jar -Djava.library.path= -Xms512m -Xmx512m org.apache.spark.deploy.SparkSubmit spark-internal --driver-memory 2g --class org.apache.spark.repl.Main\n{code}\n{{-Xms512m -Xmx512m}} is not correct"], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add missing description to spark-env.sh.template"}, {"q": "What updates or decisions were made in the discussion?", "a": "Hi [~pwendell]\n\n{{./bin/spark-shell --driver-memory 2g}} => \nSpark Command:\n{code}\n/System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home/bin/java -cp ::/Users/witgo/work/code/java/spark/dist/conf:/Users/witgo/work/code/java/spark/dist/lib/spark-assembly-1.0.0-SNAPSHOT-hadoop0.23.9.jar -Djava.library.path= -Xms512m -Xmx512m org.apache.spark.deploy.SparkSubmit spark-internal --driver-memory 2g --class org.apache.spark.repl.Main\n{code}\n{{-Xms512m -Xmx512m}} is not correct"}]}}
{"project": "SPARK", "issue_id": "SPARK-1757", "title": "Support saving null primitives with .saveAsParquetFile()", "status": "Closed", "priority": "Major", "reporter": "Andrew Ash", "assignee": null, "labels": [], "created": "2014-05-08T06:24:35.000+0000", "updated": "2014-05-14T09:04:10.000+0000", "description": "See stack trace below:\n\n{noformat}\n14/05/07 21:45:51 INFO analysis.Analyzer: Max iterations (2) reached for batch MultiInstanceRelations\n14/05/07 21:45:51 INFO analysis.Analyzer: Max iterations (2) reached for batch CaseInsensitiveAttributeReferences\n14/05/07 21:45:51 INFO optimizer.Optimizer$: Max iterations (2) reached for batch ConstantFolding\n14/05/07 21:45:51 INFO optimizer.Optimizer$: Max iterations (2) reached for batch Filter Pushdown\njava.lang.RuntimeException: Unsupported datatype StructType(List())\n        at scala.sys.package$.error(package.scala:27)\n        at org.apache.spark.sql.parquet.ParquetTypesConverter$.fromDataType(ParquetRelation.scala:201)\n        at org.apache.spark.sql.parquet.ParquetTypesConverter$$anonfun$1.apply(ParquetRelation.scala:235)\n        at org.apache.spark.sql.parquet.ParquetTypesConverter$$anonfun$1.apply(ParquetRelation.scala:235)\n        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n        at scala.collection.immutable.List.foreach(List.scala:318)\n        at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n        at scala.collection.AbstractTraversable.map(Traversable.scala:105)\n        at org.apache.spark.sql.parquet.ParquetTypesConverter$.convertFromAttributes(ParquetRelation.scala:234)\n        at org.apache.spark.sql.parquet.ParquetTypesConverter$.writeMetaData(ParquetRelation.scala:267)\n        at org.apache.spark.sql.parquet.ParquetRelation$.createEmpty(ParquetRelation.scala:143)\n        at org.apache.spark.sql.parquet.ParquetRelation$.create(ParquetRelation.scala:122)\n        at org.apache.spark.sql.execution.SparkStrategies$ParquetOperations$.apply(SparkStrategies.scala:139)\n        at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58)\n        at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58)\n        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n        at org.apache.spark.sql.catalyst.planning.QueryPlanner.apply(QueryPlanner.scala:59)\n        at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:264)\n        at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:264)\n        at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:265)\n        at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:265)\n        at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:268)\n        at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:268)\n        at org.apache.spark.sql.SchemaRDDLike$class.saveAsParquetFile(SchemaRDDLike.scala:66)\n        at org.apache.spark.sql.SchemaRDD.saveAsParquetFile(SchemaRDD.scala:96)\n{noformat}", "comments": ["https://github.com/apache/spark/pull/690"], "derived": {"summary": "See stack trace below:\n\n{noformat}\n14/05/07 21:45:51 INFO analysis. Analyzer: Max iterations (2) reached for batch MultiInstanceRelations\n14/05/07 21:45:51 INFO analysis.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Support saving null primitives with .saveAsParquetFile() - See stack trace below:\n\n{noformat}\n14/05/07 21:45:51 INFO analysis. Analyzer: Max iterations (2) reached for batch MultiInstanceRelations\n14/05/07 21:45:51 INFO analysis."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/690"}]}}
{"project": "SPARK", "issue_id": "SPARK-1758", "title": "failing test org.apache.spark.JavaAPISuite.wholeTextFiles", "status": "Resolved", "priority": "Major", "reporter": "Nishkam Ravi", "assignee": null, "labels": [], "created": "2014-05-08T06:49:49.000+0000", "updated": "2014-10-26T18:24:53.000+0000", "description": "Test org.apache.spark.JavaAPISuite.wholeTextFiles fails (during sbt/sbt test) with the following error message:\n\nTest org.apache.spark.JavaAPISuite.wholeTextFiles failed: java.lang.AssertionError: expected:<spark is also easy to use.\nbut was:<null>\n\n\n", "comments": ["The prefix \"file:\" is missing in the string inserted as key in HashMap.\nPatch attached. ", "Related with SPARK-1575", "Pull requests: \nhttps://github.com/apache/spark/pull/691   \nhttps://github.com/apache/spark/pull/692\n\n", "Keeping only pull request 692 open.", "Resolving this as \"Cannot Reproduce\" for now, since I haven't observed this problem and both PRs for this were closed."], "derived": {"summary": "Test org. apache.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "failing test org.apache.spark.JavaAPISuite.wholeTextFiles - Test org. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "Resolving this as \"Cannot Reproduce\" for now, since I haven't observed this problem and both PRs for this were closed."}]}}
{"project": "SPARK", "issue_id": "SPARK-1759", "title": "sbt/sbt package fail cause by directory", "status": "Resolved", "priority": "Major", "reporter": "Jian Pan", "assignee": null, "labels": [], "created": "2014-05-08T08:55:14.000+0000", "updated": "2014-07-20T00:31:29.000+0000", "description": "1.create a project named simpleApp\n$cd simpleApp\n\n$ find .\n.\n./simple.sbt\n./src\n./src/main\n./src/main/scala\n./src/main/scala/simpleApp.scala\n\n$ ~/Software/spark-0.9.1/sbt/sbt \nawk: fatal: cannot open file `./project/build.properties' for reading (No such file or directory)\nAttempting to fetch sbt\n/home/jpan/Software/spark-0.9.1/sbt/sbt: line 35: /sbt/sbt-launch-.jar: No such file or directory\n/home/jpan/Software/spark-0.9.1/sbt/sbt: line 35: /sbt/sbt-launch-.jar: No such file or directory\nOur attempt to download sbt locally to /sbt/sbt-launch-.jar failed. Please install sbt manually from http://www.scala-sbt.org/\n\nit failed because sbt  use relative path\n", "comments": ["As discussed at https://github.com/apache/spark/pull/693 I think this can be closed."], "derived": {"summary": "1. create a project named simpleApp\n$cd simpleApp\n\n$ find.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "sbt/sbt package fail cause by directory - 1. create a project named simpleApp\n$cd simpleApp\n\n$ find."}, {"q": "What updates or decisions were made in the discussion?", "a": "As discussed at https://github.com/apache/spark/pull/693 I think this can be closed."}]}}
{"project": "SPARK", "issue_id": "SPARK-1760", "title": " mvn  -Dsuites=*  test throw an ClassNotFoundException", "status": "Resolved", "priority": "Major", "reporter": "Guoqiang Li", "assignee": "Guoqiang Li", "labels": [], "created": "2014-05-08T14:29:00.000+0000", "updated": "2014-05-09T09:53:30.000+0000", "description": "{{mvn -Dhadoop.version=0.23.9 -Phadoop-0.23 -Dsuites=org.apache.spark.repl.ReplSuite test}} => \n{code}\n*** RUN ABORTED ***\n  java.lang.ClassNotFoundException: org.apache.spark.repl.ReplSuite\n  at java.net.URLClassLoader$1.run(URLClassLoader.java:366)\n  at java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n  at java.security.AccessController.doPrivileged(Native Method)\n  at java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n  at java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n  at java.lang.ClassLoader.loadClass(ClassLoader.java:358)\n  at org.scalatest.tools.Runner$$anonfun$21.apply(Runner.scala:1470)\n  at org.scalatest.tools.Runner$$anonfun$21.apply(Runner.scala:1469)\n  at scala.collection.TraversableLike$$anonfun$filter$1.apply(TraversableLike.scala:264)\n  at scala.collection.immutable.List.foreach(List.scala:318)\n  ...\n{code}", "comments": ["Yeah I think you would have to run this from the `repl/` module for it it work. At least it does for me, and that makes sense. I think the docs just need to note that, at: https://spark.apache.org/docs/0.9.1/building-with-maven.html  (and that suite name can be updated to include \"org.apache\")", "Hi, [~srowen]\nIs there a perfect solution?\nThe [building-with-maven.md|https://github.com/apache/spark/blob/master/docs/building-with-maven.md] has been updated", "{{mvn -Dhadoop.version=0.23.9 -Phadoop-0.23 -DwildcardSuites=org.apache.spark.rdd.RDDSuite test}} is OK", "Issue resolved by pull request 712\n[https://github.com/apache/spark/pull/712]", "If `wildcardSuites` lets you invoke specific suites across the whole project, then that sounds like an ideal solution. If it works then I'd propose that as a small doc change?", "Yes, [PR 712 |https://github.com/apache/spark/pull/712] have been merged"], "derived": {"summary": "{{mvn -Dhadoop. version=0.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "mvn  -Dsuites=*  test throw an ClassNotFoundException - {{mvn -Dhadoop. version=0."}, {"q": "What updates or decisions were made in the discussion?", "a": "Yes, [PR 712 |https://github.com/apache/spark/pull/712] have been merged"}]}}
{"project": "SPARK", "issue_id": "SPARK-1761", "title": "Add broadcast information on SparkUI storage tab", "status": "Closed", "priority": "Major", "reporter": "Andrew Or", "assignee": null, "labels": [], "created": "2014-05-08T17:06:21.000+0000", "updated": "2014-11-05T10:43:30.000+0000", "description": "It would be nice to know where the broadcast blocks are persisted. More details coming.", "comments": ["This would be very useful actually. ", "Closing in favor of SPARK-3957, which is more descriptive."], "derived": {"summary": "It would be nice to know where the broadcast blocks are persisted. More details coming.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add broadcast information on SparkUI storage tab - It would be nice to know where the broadcast blocks are persisted. More details coming."}, {"q": "What updates or decisions were made in the discussion?", "a": "Closing in favor of SPARK-3957, which is more descriptive."}]}}
{"project": "SPARK", "issue_id": "SPARK-1762", "title": "Add functionality to pin RDDs in cache", "status": "Resolved", "priority": "Major", "reporter": "Andrew Or", "assignee": null, "labels": [], "created": "2014-05-08T17:09:03.000+0000", "updated": "2016-10-08T09:32:01.000+0000", "description": "Right now, all RDDs are created equal, and there is no mechanism to identify a certain RDD to be more important than the rest. This is a problem if the RDD fraction is small, because just caching a few RDDs can evict more important ones.\n\nA side effect of this feature is that we can now more safely allocate a smaller spark.storage.memoryFraction if we know how large our important RDDs are, without having to worry about them being evicted. This allows us to use more memory for shuffles, for instance, and avoid disk spills.", "comments": ["What is the current eviction policy?\nInstead of pinning, what if we just make the eviction policy smarter? (from a quick look, it seems like the current policy is FIFO)\n\nWe want developers to think about how much memory the system has less, not more.", "Is this something we are still interested in? I could see it becoming more important with `Datasets`/`DataFrames` where a partial cache miss is much more expensive (potentially) than with `RDD`s."], "derived": {"summary": "Right now, all RDDs are created equal, and there is no mechanism to identify a certain RDD to be more important than the rest. This is a problem if the RDD fraction is small, because just caching a few RDDs can evict more important ones.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add functionality to pin RDDs in cache - Right now, all RDDs are created equal, and there is no mechanism to identify a certain RDD to be more important than the rest. This is a problem if the RDD fraction is small, because just caching a few RDDs can evict more important ones."}, {"q": "What updates or decisions were made in the discussion?", "a": "Is this something we are still interested in? I could see it becoming more important with `Datasets`/`DataFrames` where a partial cache miss is much more expensive (potentially) than with `RDD`s."}]}}
{"project": "SPARK", "issue_id": "SPARK-1763", "title": "SparkSubmit arguments do not propagate to python files on YARN", "status": "Closed", "priority": "Blocker", "reporter": "Andrew Or", "assignee": null, "labels": [], "created": "2014-05-08T17:18:13.000+0000", "updated": "2014-11-05T10:43:33.000+0000", "description": "The python SparkConf load defaults does not pick up system properties set by SparkSubmit.", "comments": ["Is this only for Python or for spark-submit in general?", "This is only for python. There is a seemingly related issue with the app name (SPARK-1755), but that is actually a more specific issue.", "Wait, unable to reproduce with the latest mater... I can get my simple python app to inherit configs now. Not sure what the issue was, but it works now."], "derived": {"summary": "The python SparkConf load defaults does not pick up system properties set by SparkSubmit.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "SparkSubmit arguments do not propagate to python files on YARN - The python SparkConf load defaults does not pick up system properties set by SparkSubmit."}, {"q": "What updates or decisions were made in the discussion?", "a": "Wait, unable to reproduce with the latest mater... I can get my simple python app to inherit configs now. Not sure what the issue was, but it works now."}]}}
{"project": "SPARK", "issue_id": "SPARK-1764", "title": "EOF reached before Python server acknowledged", "status": "Resolved", "priority": "Blocker", "reporter": "Bouke van der Bijl", "assignee": "Davies Liu", "labels": ["mesos", "pyspark"], "created": "2014-05-08T17:52:34.000+0000", "updated": "2014-09-15T17:23:47.000+0000", "description": "I'm getting \"EOF reached before Python server acknowledged\" while using PySpark on Mesos. The error manifests itself in multiple ways. One is:\n\n{noformat}\n14/05/08 18:10:40 ERROR DAGSchedulerActorSupervisor: eventProcesserActor failed due to the error EOF reached before Python server acknowledged; shutting down SparkContext\n{noformat}\n\nAnd the other has a full stacktrace:\n\n{noformat}\n14/05/08 18:03:06 ERROR OneForOneStrategy: EOF reached before Python server acknowledged\norg.apache.spark.SparkException: EOF reached before Python server acknowledged\n\tat org.apache.spark.api.python.PythonAccumulatorParam.addInPlace(PythonRDD.scala:416)\n\tat org.apache.spark.api.python.PythonAccumulatorParam.addInPlace(PythonRDD.scala:387)\n\tat org.apache.spark.Accumulable.$plus$plus$eq(Accumulators.scala:71)\n\tat org.apache.spark.Accumulators$$anonfun$add$2.apply(Accumulators.scala:279)\n\tat org.apache.spark.Accumulators$$anonfun$add$2.apply(Accumulators.scala:277)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:98)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)\n\tat org.apache.spark.Accumulators$.add(Accumulators.scala:277)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:818)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1204)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:456)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:219)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n{noformat}\n\nThis error causes the SparkContext to shutdown. I have not been able to reliably reproduce this bug, it seems to happen randomly, but if you run enough tasks on a SparkContext it'll hapen eventually", "comments": ["I can semi-reliably recreate this by just running this code:\n\n{quote}\nwhile True:\n  sc.parallelize(range(100)).map(lambda n: n * 2).collect()\n{quote}\n\nRunning this on Mesos will eventually crash with \n\nPy4JJavaError: An error occurred while calling o1142.collect.\n: org.apache.spark.SparkException: Job 101 cancelled as part of cancellation of all jobs\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1033)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:998)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:499)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:499)\n\tat org.apache.spark.scheduler.DAGSchedulerActorSupervisor$$anonfun$2.applyOrElse(DAGScheduler.scala:1151)\n\tat org.apache.spark.scheduler.DAGSchedulerActorSupervisor$$anonfun$2.applyOrElse(DAGScheduler.scala:1147)\n\tat akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:295)\n\tat akka.actor.dungeon.FaultHandling$class.handleFailure(FaultHandling.scala:253)\n\tat akka.actor.ActorCell.handleFailure(ActorCell.scala:338)\n\tat akka.actor.ActorCell.invokeAll$1(ActorCell.scala:423)\n\tat akka.actor.ActorCell.systemInvoke(ActorCell.scala:447)\n\tat akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:262)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:218)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n\n\nI0508 18:29:03.623627  7868 sched.cpp:730] Stopping framework '20140508-173240-16842879-5050-24645-0032'\n14/05/08 18:29:04 ERROR OneForOneStrategy: EOF reached before Python server acknowledged\norg.apache.spark.SparkException: EOF reached before Python server acknowledged\n\tat org.apache.spark.api.python.PythonAccumulatorParam.addInPlace(PythonRDD.scala:416)\n\tat org.apache.spark.api.python.PythonAccumulatorParam.addInPlace(PythonRDD.scala:387)\n\tat org.apache.spark.Accumulable.$plus$plus$eq(Accumulators.scala:71)\n\tat org.apache.spark.Accumulators$$anonfun$add$2.apply(Accumulators.scala:279)\n\tat org.apache.spark.Accumulators$$anonfun$add$2.apply(Accumulators.scala:277)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:98)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)\n\tat org.apache.spark.Accumulators$.add(Accumulators.scala:277)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:818)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1204)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:456)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:219)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)", "I did some more digging into this and I have no idea what's the exact issue. The write to the Python server succeeds (which I checked from the Python side) but the Scala side doesn't seem to be able to read the acknowledgement. \n\nI have also confirmed that it isn't an issue with the Python broadcast server dying, as commented out the exception makes it work fine (!) ", "We just ran `sc.parallelize(range(100)).map(lambda n: n * 2).collect()` on a Mesos 0.18.1 cluster with the latest spark and it worked. Could you confirm the Spark & Mesos version you are using (if using master please include the sha/commit hash).", "Interesting, including SPARK-1806 in our build made it stop failing... I guess this can be considered fixed then", "I have seen this issue in a cut of the main branch take on friday 27th June (last commit https://github.com/srowen/spark/commit/18f29b96c7e0948f5f504e522e5aa8a8d1ab163e )\n\nIt does run successfully for a time (~800 requests) but then fails. This has happened on two different clusters, mesos 0.18.0 and 0.18.2, different sites.\n\nOne interesting side effect is as this is running, the number of file handles in use on the slaves goes up to many thousands. This could be the resource that is running out. One of the slaves (different machines) always dies and the job is eventually aborted.\n\nThis happens with a real app that is actually doing stuff with 18GB RDDs, but also with the simple, but brutal while true command below. A similar loop in scala seems to work fine, indefinitely.\n\nI would be interested to hear if anyone can run this command for a significant amount of time because that would point to our environment in some way.\n\nUsing Python version 2.7.6 (default, Jan 17 2014 10:13:17)\nSparkContext available as sc.\n\nIn [1]: while True:                       \n    sc.parallelize(range(100)).map(lambda n: n * 2).collect()\n   ...:     \n\nit ran for a while then...\n\n14/06/27 13:59:08 INFO TaskSetManager: Finished TID 778 in 63 ms on guavcpt-ch2-a28p.sys.comcast.net (progress: 1/8)\n14/06/27 13:59:08 INFO DAGScheduler: Completed ResultTask(97, 2)\n14/06/27 13:59:08 INFO TaskSetManager: Finished TID 777 in 70 ms on guavcpt-ch2-a32p.sys.comcast.net (progress: 2/8)\n14/06/27 13:59:08 INFO DAGScheduler: Completed ResultTask(97, 1)\n14/06/27 13:59:08 INFO TaskSetManager: Finished TID 782 in 104 ms on guavcpt-ch2-a28p.sys.comcast.net (progress: 3/8)\n14/06/27 13:59:08 INFO DAGScheduler: Completed ResultTask(97, 6)\n14/06/27 13:59:08 INFO TaskSetManager: Finished TID 781 in 105 ms on guavcpt-ch2-a32p.sys.comcast.net (progress: 4/8)\n14/06/27 13:59:08 INFO DAGScheduler: Completed ResultTask(97, 5)\n14/06/27 13:59:08 ERROR DAGSchedulerActorSupervisor: eventProcesserActor failed due to the error EOF reached before Python server acknowledged; shutting down SparkContext\n14/06/27 13:59:08 INFO TaskSchedulerImpl: Cancelling stage 97\n14/06/27 13:59:08 INFO DAGScheduler: Could not cancel tasks for stage 97\njava.lang.UnsupportedOperationException\nat org.apache.spark.scheduler.SchedulerBackend$class.killTask(SchedulerBackend.scala:32)\nat org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend.killTask(MesosSchedulerBackend.scala:41)\nat org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:185)\nat org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:183)\nat org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:183)\nat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\nat org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$3.apply(TaskSchedulerImpl.scala:183)\nat org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$3.apply(TaskSchedulerImpl.scala:176)\nat scala.Option.foreach(Option.scala:236)\nat org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:176)\nat org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1066)\nat org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1052)\nat org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1052)\nat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\nat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1052)\nat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1005)\nat org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:502)\nat org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:502)\nat org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:502)\nat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\nat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:502)\nat org.apache.spark.scheduler.DAGSchedulerActorSupervisor$$anonfun$2.applyOrElse(DAGScheduler.scala:1167)\nat org.apache.spark.scheduler.DAGSchedulerActorSupervisor$$anonfun$2.applyOrElse(DAGScheduler.scala:1162)\nat akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:295)\nat akka.actor.dungeon.FaultHandling$class.handleFailure(FaultHandling.scala:253)\nat akka.actor.ActorCell.handleFailure(ActorCell.scala:338)\nat akka.actor.ActorCell.invokeAll$1(ActorCell.scala:423)\nat akka.actor.ActorCell.systemInvoke(ActorCell.scala:447)\nat akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:262)\nat akka.dispatch.Mailbox.run(Mailbox.scala:218)\nat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)\nat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\nat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\nat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\nat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n14/06/27 13:59:08 INFO TaskSetManager: Finished TID 776 in 151 ms on guavcpt-ch2-a31p.sys.comcast.net (progress: 5/8)\n14/06/27 13:59:08 INFO TaskSetManager: Finished TID 780 in 147 ms on guavcpt-ch2-a31p.sys.comcast.net (progress: 6/8)\n14/06/27 13:59:08 INFO TaskSetManager: Finished TID 779 in 188 ms on guavcpt-ch2-a27p.sys.comcast.net (progress: 7/8)\n14/06/27 13:59:08 INFO TaskSetManager: Finished TID 783 in 193 ms on guavcpt-ch2-a27p.sys.comcast.net (progress: 8/8)\n14/06/27 13:59:08 INFO TaskSchedulerImpl: Removed TaskSet 97.0, whose tasks have all completed, from pool \n14/06/27 13:59:09 WARN QueuedThreadPool: 5 threads could not be stopped\n14/06/27 13:59:09 INFO SparkUI: Stopped Spark web UI at http://guavcpt-ch2-a26p.sys.comcast.net:4041\n14/06/27 13:59:09 INFO DAGScheduler: Stopping DAGScheduler\nI0627 13:59:09.129179 14267 sched.cpp:730] Stopping framework '20140605-200137-607132588-5050-19211-0073'\n14/06/27 13:59:09 INFO MesosSchedulerBackend: driver.run() returned with code DRIVER_STOPPED\n14/06/27 13:59:10 INFO MapOutputTrackerMasterActor: MapOutputTrackerActor stopped!\n14/06/27 13:59:10 INFO ConnectionManager: Selector thread was interrupted!\n14/06/27 13:59:10 INFO ConnectionManager: ConnectionManager stopped\n14/06/27 13:59:10 INFO MemoryStore: MemoryStore cleared\n14/06/27 13:59:10 INFO BlockManager: BlockManager stopped\n14/06/27 13:59:10 INFO BlockManagerMasterActor: Stopping BlockManagerMaster\n14/06/27 13:59:10 INFO BlockManagerMaster: BlockManagerMaster stopped\n14/06/27 13:59:10 INFO SparkContext: Successfully stopped SparkContext\n14/06/27 13:59:10 ERROR OneForOneStrategy: EOF reached before Python server acknowledged\norg.apache.spark.SparkException: EOF reached before Python server acknowledged\nat org.apache.spark.api.python.PythonAccumulatorParam.addInPlace(PythonRDD.scala:613)\nat org.apache.spark.api.python.PythonAccumulatorParam.addInPlace(PythonRDD.scala:584)\nat org.apache.spark.Accumulable.$plus$plus$eq(Accumulators.scala:72)\nat org.apache.spark.Accumulators$$anonfun$add$2.apply(Accumulators.scala:280)\nat org.apache.spark.Accumulators$$anonfun$add$2.apply(Accumulators.scala:278)\nat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)\nat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)\nat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)\nat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)\nat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)\nat scala.collection.mutable.HashMap.foreach(HashMap.scala:98)\nat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)\nat org.apache.spark.Accumulators$.add(Accumulators.scala:278)\nat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:825)\nat org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1223)\nat akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)\nat akka.actor.ActorCell.invoke(ActorCell.scala:456)\nat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)\nat akka.dispatch.Mailbox.run(Mailbox.scala:219)\nat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)\nat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\nat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\nat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\nat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n14/06/27 13:59:10 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.\n14/06/27 13:59:10 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.\n14/06/27 13:59:10 INFO Remoting: Remoting shut down\n\n", "I'm not sure how this is related to Mesos, is this reproable using YARN or standalone?", "Hi;\nNever used yarn. Doesn't happen on standalone.\n\n", "This issue should be fixed in SPARK-2282 [1], I had ran the jobs above against mesos-0.19.1 after more than a hour without problems.\n\n[~therealnb] Could you also verify this?\n\n[1] https://github.com/apache/spark/commit/ef4ff00f87a4e8d38866f163f01741c2673e41da", "Hi;\nSadly I moved jobs and I don't have a working Spark environment at the moment (I will be doing some Spark work soon :-). I'll pass this on to the guys that are still there and get them to confirm. \nCheers", "FYI the workaround described in SPARK-2282 got me past the same \"EOF reached\" issue.\n\ni.e.\n\necho \"1\" > /proc/sys/net/ipv4/tcp_tw_reuse\necho \"1\" > /proc/sys/net/ipv4/tcp_tw_recycle\n\nthen restart your spark shell (or program).\n\nApparently a fix is slated for Spark 1.1\n\nI am running Spark 1.0.2 under Mesos 0.18.2\n", "This is fixed by #2282"], "derived": {"summary": "I'm getting \"EOF reached before Python server acknowledged\" while using PySpark on Mesos. The error manifests itself in multiple ways.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "EOF reached before Python server acknowledged - I'm getting \"EOF reached before Python server acknowledged\" while using PySpark on Mesos. The error manifests itself in multiple ways."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is fixed by #2282"}]}}
{"project": "SPARK", "issue_id": "SPARK-1765", "title": "Modify a typo in monitoring.md", "status": "Resolved", "priority": "Minor", "reporter": "Kousuke Saruta", "assignee": null, "labels": [], "created": "2014-05-08T18:18:46.000+0000", "updated": "2014-05-15T17:59:06.000+0000", "description": "There is a word 'JXM' In monitoring.md.\nI guess, it's a typo for 'JMX'.", "comments": ["I've pull-requested for PR-698.", "https://github.com/apache/spark/pull/698\n\nThis can now be closed"], "derived": {"summary": "There is a word 'JXM' In monitoring. md.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Modify a typo in monitoring.md - There is a word 'JXM' In monitoring. md."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/698\n\nThis can now be closed"}]}}
{"project": "SPARK", "issue_id": "SPARK-1766", "title": "Move reduceByKey definitions next to each other in PairRDDFunctions", "status": "Resolved", "priority": "Trivial", "reporter": "Sandy Ryza", "assignee": "Chris Cope", "labels": [], "created": "2014-05-08T20:25:04.000+0000", "updated": "2014-08-12T15:46:19.000+0000", "description": "Sorry, I know this is pedantic, but I've been browsing the source multiple times and gotten fooled into thinking reduceByKey always requires a partitioner.", "comments": ["User 'copester' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1859", "Resolved by: https://github.com/apache/spark/pull/1859", "User 'copester' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1906"], "derived": {"summary": "Sorry, I know this is pedantic, but I've been browsing the source multiple times and gotten fooled into thinking reduceByKey always requires a partitioner.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Move reduceByKey definitions next to each other in PairRDDFunctions - Sorry, I know this is pedantic, but I've been browsing the source multiple times and gotten fooled into thinking reduceByKey always requires a partitioner."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'copester' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1906"}]}}
{"project": "SPARK", "issue_id": "SPARK-1767", "title": "Prefer HDFS-cached replicas when scheduling data-local tasks", "status": "Resolved", "priority": "Major", "reporter": "Sandy Ryza", "assignee": "Colin McCabe", "labels": [], "created": "2014-05-08T20:44:06.000+0000", "updated": "2014-10-02T07:32:36.000+0000", "description": null, "comments": ["Currently, RDDs only support a single level of location preference through RDD#preferredLocations(split), which returns a sequence of strings.  To prefer cached-replicas, this needs to be extended in some way.  We could deprecate preferredLocations and add a preferredLocations(split, storageType), where storageType is MEMORY, DISK, and eventually FLASH?  Maybe more hackily, we could give the location strings a prefix like \"inmem:\" that specifies the storage type.", "-One simple workaround to this is to just make sure that partitions that are in memory are ordered first in the list of partitions, as Spark will try to place executors based on the order in this list.- This is, of course, not a complete solution, as we would not utilize the locality-wait logic within Spark and would immediately fallback to a non-cached node if the cached node was busy, rather than waiting for some period of time for the cached node to become available.\n\nEdit: I was wrong about how Spark schedules partitions -- ordering is not sufficient.", "\nDid not realize that mail replies to JIRA mails did not get mirrored to JIRA ! Replicating my mail here :\n\n-- cut and paste --\nHi Sandy,\n\n  I assume you are referring to caching added to datanodes via new caching api via NN ? (To preemptively mmap blocks).\n\nI have not looked in detail, but does NN tell us about this in block locations?\nIf yes, we can simply make those process local instead of node local for executors on that node.\n\nThis would simply be a change to hadoop based rdd partitioning (what makes it tricky is to expose currently 'alive' executors to partition)\n\nThanks\nMridul", "That is the caching I am referring to.  HDFS does expose the locations of in-memory blocks in the same way it exposes the locations of on-disk blocks.\n\nUnfortunately, I just realized that Spark gets this information through org.apache.hadoop.mapred.InputSplit#getLocations, so I think we will need to expose this information in MapReduce before we can expose it in Spark.\n", "what is status of MapReduce patch for exposing HDFS cache location?", "It will be in the Hadoop 2.5 release", "I posted a pull request here: https://github.com/apache/spark/pull/1486\n\nThis illustrates how to do it with Hadoop 2.5.  I'm still not sure about whether we should change the type of getPreferredLocations (see the pull request)", "I think the solution that [~adav] proposed will not work, because IIRC the order of preferredLocations does not translate to a priority in the current implementation. There are some comments on the PR from [~kayousterhout] in this regard.", "[~pwendell], [~kayousterhout]: yeah, you're right.  The order in {{preferredLocations}} doesn't matter here.  I think we need to pass back some more information through this API to the scheduler.  I believe we can do this by using a short string prefix.  If it contains an underscore, it won't be possible to confuse it with a valid hostname.  This will avoid creating compatibility problems, I believe.", "Fixed by:https://github.com/apache/spark/pull/1486"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Prefer HDFS-cached replicas when scheduling data-local tasks"}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed by:https://github.com/apache/spark/pull/1486"}]}}
{"project": "SPARK", "issue_id": "SPARK-1768", "title": "History Server enhancements", "status": "Resolved", "priority": "Major", "reporter": "Marcelo Masiero Vanzin", "assignee": "Marcelo Masiero Vanzin", "labels": [], "created": "2014-05-08T22:09:33.000+0000", "updated": "2014-06-23T20:54:08.000+0000", "description": "The history server currently has some limitations; the one that currently concerns me the most is that it limits the number of applications it will show, to avoid having to hold all applications in memory.\n\nIt would be better if the code were smarter and able to show any application available in the history storage.\n\nAlso, thinking forward a little bit (I'm thinking SPARK-1537), it would be nice to separate the serving logic from the logic to access app log data.", "comments": ["BTW I'm working on this (patch isn't ready yet), but I can't seem to be able to reassign the bug to me.", "https://github.com/apache/spark/pull/718", "Issue resolved by pull request 718\n[https://github.com/apache/spark/pull/718]"], "derived": {"summary": "The history server currently has some limitations; the one that currently concerns me the most is that it limits the number of applications it will show, to avoid having to hold all applications in memory. It would be better if the code were smarter and able to show any application available in the history storage.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "History Server enhancements - The history server currently has some limitations; the one that currently concerns me the most is that it limits the number of applications it will show, to avoid having to hold all applications in memory. It would be better if the code were smarter and able to show any application available in the history storage."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 718\n[https://github.com/apache/spark/pull/718]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1769", "title": "Executor loss can cause race condition in Pool", "status": "Resolved", "priority": "Major", "reporter": "Aaron Davidson", "assignee": "Andrew Or", "labels": [], "created": "2014-05-08T23:34:42.000+0000", "updated": "2014-11-05T10:45:26.000+0000", "description": "Loss of executors (in this case due to OOMs) exposes a race condition in Pool.scala, evident from this stack trace:\n\n{code}\n14/05/08 22:41:48 ERROR OneForOneStrategy:\njava.lang.NullPointerException\n        at org.apache.spark.scheduler.Pool$$anonfun$executorLost$1.apply(Pool.scala:87)\n        at org.apache.spark.scheduler.Pool$$anonfun$executorLost$1.apply(Pool.scala:87)\n        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n        at org.apache.spark.scheduler.Pool.executorLost(Pool.scala:87)\n        at org.apache.spark.scheduler.Pool$$anonfun$executorLost$1.apply(Pool.scala:87)\n        at org.apache.spark.scheduler.Pool$$anonfun$executorLost$1.apply(Pool.scala:87)\n        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n        at org.apache.spark.scheduler.Pool.executorLost(Pool.scala:87)\n        at org.apache.spark.scheduler.TaskSchedulerImpl.removeExecutor(TaskSchedulerImpl.scala:412)\n        at org.apache.spark.scheduler.TaskSchedulerImpl.executorLost(TaskSchedulerImpl.scala:385)\n        at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverActor.removeExecutor(CoarseGrainedSchedulerBackend.scala:160)\n        at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverActor$$anonfun$receive$1$$anonfun$applyOrElse$5.apply(CoarseGrainedSchedulerBackend.scala:123)\n        at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverActor$$anonfun$receive$1$$anonfun$applyOrElse$5.apply(CoarseGrainedSchedulerBackend.scala:123)\n        at scala.Option.foreach(Option.scala:236)\n        at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverActor$$anonfun$receive$1.applyOrElse(CoarseGrainedSchedulerBackend.scala:123)\n        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)\n        at akka.actor.ActorCell.invoke(ActorCell.scala:456)\n        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)\n        at akka.dispatch.Mailbox.run(Mailbox.scala:219)\n        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)\n        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n{code}\n\nNote that the line of code that throws this exception is here:\n{code}\nschedulableQueue.foreach(_.executorLost(executorId, host))\n{code}\n\nBy the stack trace, it's not schedulableQueue that is null, but an element therein. As far as I could tell, we never add a null element to this queue. Rather, I could see that removeSchedulable() and executorLost() were called at about the same time (via log messages), and suspect that since this ArrayBuffer is in no way synchronized, that we iterate through the list while it's in an incomplete state.", "comments": ["https://github.com/apache/spark/pull/762"], "derived": {"summary": "Loss of executors (in this case due to OOMs) exposes a race condition in Pool. scala, evident from this stack trace:\n\n{code}\n14/05/08 22:41:48 ERROR OneForOneStrategy:\njava.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Executor loss can cause race condition in Pool - Loss of executors (in this case due to OOMs) exposes a race condition in Pool. scala, evident from this stack trace:\n\n{code}\n14/05/08 22:41:48 ERROR OneForOneStrategy:\njava."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/762"}]}}
{"project": "SPARK", "issue_id": "SPARK-1770", "title": "repartition and coalesce(shuffle=true) put objects with the same key in the same bucket", "status": "Resolved", "priority": "Blocker", "reporter": "Matei Alexandru Zaharia", "assignee": "Patrick Wendell", "labels": ["Starter"], "created": "2014-05-08T23:59:21.000+0000", "updated": "2014-05-27T01:02:02.000+0000", "description": "This is bad when you have many identical objects. We should assign each one a random key.", "comments": ["I think this is fixed in PR https://github.com/apache/spark/pull/704 by [~pwendell]", "Ah, that PR seems unrelated.", "Sorry, you're right, it was somehow committed when that PR was merged (https://github.com/apache/spark/commit/06b15baab25951d124bbe6b64906f4139e037deb) though the change doesn't actually show up in the PR itself.", "It was a mistake on my part - I accidentally pulled it into a different merge."], "derived": {"summary": "This is bad when you have many identical objects. We should assign each one a random key.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "repartition and coalesce(shuffle=true) put objects with the same key in the same bucket - This is bad when you have many identical objects. We should assign each one a random key."}, {"q": "What updates or decisions were made in the discussion?", "a": "It was a mistake on my part - I accidentally pulled it into a different merge."}]}}
{"project": "SPARK", "issue_id": "SPARK-1771", "title": "CoarseGrainedSchedulerBackend is not resilient to Akka restarts", "status": "Resolved", "priority": "Major", "reporter": "Aaron Davidson", "assignee": null, "labels": [], "created": "2014-05-09T00:18:18.000+0000", "updated": "2014-12-11T20:27:21.000+0000", "description": "The exception reported in SPARK-1769 was propagated through the CoarseGrainedSchedulerBackend, and caused an Actor restart of the DriverActor. Unfortunately, this actor does not seem to have been written with Akka restartability in mind. For instance, the new DriverActor has lost all state about the prior Executors without cleanly disconnecting them. This means that the driver actually has executors attached to it, but doesn't think it does, which leads to mayhem of various sorts.", "comments": ["[#Aaron Davidson], I think there are basically two ways to fix this bug, which depends on whether we want to allow the restarting of the driver\n\n1. assume we allow the restarting, we may need something similar to the persistentEngine in the deploy package\n\n2. if not, we can introduce a supervisor actor to stop the DriverActor and kill the executors....just similar with what we just did in the DAGScheduler....", "User 'CodingCat' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/3129", "The PR says this was abandoned in favor of SPARK-4004"], "derived": {"summary": "The exception reported in SPARK-1769 was propagated through the CoarseGrainedSchedulerBackend, and caused an Actor restart of the DriverActor. Unfortunately, this actor does not seem to have been written with Akka restartability in mind.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "CoarseGrainedSchedulerBackend is not resilient to Akka restarts - The exception reported in SPARK-1769 was propagated through the CoarseGrainedSchedulerBackend, and caused an Actor restart of the DriverActor. Unfortunately, this actor does not seem to have been written with Akka restartability in mind."}, {"q": "What updates or decisions were made in the discussion?", "a": "The PR says this was abandoned in favor of SPARK-4004"}]}}
{"project": "SPARK", "issue_id": "SPARK-1772", "title": "Spark executors do not successfully die on OOM", "status": "Resolved", "priority": "Major", "reporter": "Aaron Davidson", "assignee": null, "labels": [], "created": "2014-05-09T00:33:52.000+0000", "updated": "2014-05-13T03:11:26.000+0000", "description": "Executor catches Throwable, and does not always die when JVM fatal exceptions occur. This is a problem because any subsequent use of these Executors are very likely to fail.", "comments": ["Issue resolved by pull request 715\n[https://github.com/apache/spark/pull/715]", "BTW, for specific case of OOM, there is another way to handle it - via -XX flags : which is what we do in yarn case.", "Could you point me to the flag you're referring to? The only one I knew about was the one that prints a heap dump on error."], "derived": {"summary": "Executor catches Throwable, and does not always die when JVM fatal exceptions occur. This is a problem because any subsequent use of these Executors are very likely to fail.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Spark executors do not successfully die on OOM - Executor catches Throwable, and does not always die when JVM fatal exceptions occur. This is a problem because any subsequent use of these Executors are very likely to fail."}, {"q": "What updates or decisions were made in the discussion?", "a": "Could you point me to the flag you're referring to? The only one I knew about was the one that prints a heap dump on error."}]}}
{"project": "SPARK", "issue_id": "SPARK-1773", "title": "Standalone cluster docs should be updated to reflect Spark Submit", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Andrew Or", "labels": [], "created": "2014-05-09T00:36:39.000+0000", "updated": "2014-11-05T10:45:23.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Standalone cluster docs should be updated to reflect Spark Submit"}]}}
{"project": "SPARK", "issue_id": "SPARK-1774", "title": "SparkSubmit --jars not working for yarn-client", "status": "Resolved", "priority": "Blocker", "reporter": "Andrew Or", "assignee": "Andrew Or", "labels": [], "created": "2014-05-09T00:49:36.000+0000", "updated": "2014-11-05T10:45:25.000+0000", "description": "If the mode is yarn-client, SparkSubmit ignores --jars altogether. The other thing is that the application jar is not automatically added the --jars, which leads to spurious ClassNotFoundExceptions. (This latter point is already implemented, but only for Standalone mode)", "comments": ["Issue resolved by pull request 710\n[https://github.com/apache/spark/pull/710]"], "derived": {"summary": "If the mode is yarn-client, SparkSubmit ignores --jars altogether. The other thing is that the application jar is not automatically added the --jars, which leads to spurious ClassNotFoundExceptions.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "SparkSubmit --jars not working for yarn-client - If the mode is yarn-client, SparkSubmit ignores --jars altogether. The other thing is that the application jar is not automatically added the --jars, which leads to spurious ClassNotFoundExceptions."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 710\n[https://github.com/apache/spark/pull/710]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1775", "title": "Unneeded lock in ShuffleMapTask.deserializeInfo", "status": "Resolved", "priority": "Critical", "reporter": "Matei Alexandru Zaharia", "assignee": "Sandeep Singh", "labels": ["Starter"], "created": "2014-05-09T02:06:17.000+0000", "updated": "2014-05-10T06:11:22.000+0000", "description": "This was used in the past to have a cache of deserialized ShuffleMapTasks, but that's been removed, so there's no need for a lock. It slows down Spark when task descriptions are large, e.g. due to large lineage graphs or local variables.", "comments": ["Issue resolved by pull request 707\n[https://github.com/apache/spark/pull/707]"], "derived": {"summary": "This was used in the past to have a cache of deserialized ShuffleMapTasks, but that's been removed, so there's no need for a lock. It slows down Spark when task descriptions are large, e.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "Unneeded lock in ShuffleMapTask.deserializeInfo - This was used in the past to have a cache of deserialized ShuffleMapTasks, but that's been removed, so there's no need for a lock. It slows down Spark when task descriptions are large, e."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 707\n[https://github.com/apache/spark/pull/707]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1776", "title": "Have Spark's SBT build read dependencies from Maven", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Prashant Sharma", "labels": [], "created": "2014-05-09T03:53:37.000+0000", "updated": "2015-12-10T15:05:22.000+0000", "description": "We've wanted to consolidate Spark's build for a while see [here|http://mail-archives.apache.org/mod_mbox/spark-dev/201307.mbox/%3C39343FA4-3CF4-4349-99E7-2B20E1AEDB77@gmail.com%3E] and [here|http://apache-spark-developers-list.1001551.n3.nabble.com/DISCUSS-Necessity-of-Maven-and-SBT-Build-in-Spark-td2315.html].\n\nI'd like to propose using the sbt-pom-reader plug-in to allow us to keep our sbt build (for ease of development) while also holding onto our Maven build which almost all downstream packagers use.\n\nI've prototyped this a bit locally and I think it's do-able, but will require making some contributions to the sbt-pom-reader plugin. Josh Suereth who maintains both sbt and the plug-in has agreed to help merge any patches we need for this.", "comments": ["Even so, there are many maintenance costs.We should not use two build tools at the same time, only use maven is better\n", "Issue resolved by pull request 772\n[https://github.com/apache/spark/pull/772]", "User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/772"], "derived": {"summary": "We've wanted to consolidate Spark's build for a while see [here|http://mail-archives. apache.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Have Spark's SBT build read dependencies from Maven - We've wanted to consolidate Spark's build for a while see [here|http://mail-archives. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/772"}]}}
{"project": "SPARK", "issue_id": "SPARK-1777", "title": "Pass \"cached\" blocks directly to disk if memory is not large enough", "status": "Resolved", "priority": "Critical", "reporter": "Patrick Wendell", "assignee": "Andrew Or", "labels": [], "created": "2014-05-09T06:37:34.000+0000", "updated": "2014-11-05T10:45:25.000+0000", "description": "Currently in Spark we entirely unroll a partition and then check whether it will cause us to exceed the storage limit. This has an obvious problem - if the partition itself is enough to push us over the storage limit (and eventually over the JVM heap), it will cause an OOM.\n\nThis can happen in cases where a single partition is very large or when someone is running examples locally with a small heap.\n\nhttps://github.com/apache/spark/blob/f6ff2a61d00d12481bfb211ae13d6992daacdcc2/core/src/main/scala/org/apache/spark/CacheManager.scala#L148\n\nWe should think a bit about the most elegant way to fix this - it shares some similarities with the external aggregation code.\n\nA simple idea is to periodically check the size of the buffer as we are unrolling and see if we are over the memory limit. If we are we could prepend the existing buffer to the iterator and write that entire thing out to disk.", "comments": ["An easy way reproduce this: run spark shell local mode with default settings (i.e. 512MB executor memory)\n{code}\nsc.parallelize(1 to 20 * 1000 * 1000, 1).persist().count()\n{code}\nIt's trying to unroll the entire partition to check if it fits in the cache, but by then it's too late.\n\n", "Fix: https://github.com/apache/spark/pull/1165", "User 'liyezhang556520' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1892"], "derived": {"summary": "Currently in Spark we entirely unroll a partition and then check whether it will cause us to exceed the storage limit. This has an obvious problem - if the partition itself is enough to push us over the storage limit (and eventually over the JVM heap), it will cause an OOM.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Pass \"cached\" blocks directly to disk if memory is not large enough - Currently in Spark we entirely unroll a partition and then check whether it will cause us to exceed the storage limit. This has an obvious problem - if the partition itself is enough to push us over the storage limit (and eventually over the JVM heap), it will cause an OOM."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'liyezhang556520' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1892"}]}}
{"project": "SPARK", "issue_id": "SPARK-1778", "title": "Add 'limit' transformation to SchemaRDD.", "status": "Resolved", "priority": "Major", "reporter": "Takuya Ueshin", "assignee": "Takuya Ueshin", "labels": [], "created": "2014-05-09T08:02:10.000+0000", "updated": "2014-05-10T19:04:06.000+0000", "description": "Add {{limit}} transformation to {{SchemaRDD}}.", "comments": ["Pull-requested: https://github.com/apache/spark/pull/711", "Issue resolved by pull request 711\n[https://github.com/apache/spark/pull/711]"], "derived": {"summary": "Add {{limit}} transformation to {{SchemaRDD}}.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add 'limit' transformation to SchemaRDD. - Add {{limit}} transformation to {{SchemaRDD}}."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 711\n[https://github.com/apache/spark/pull/711]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1779", "title": "Warning when spark.storage.memoryFraction is not between 0 and 1", "status": "Resolved", "priority": "Major", "reporter": "Fei Wang", "assignee": null, "labels": [], "created": "2014-05-09T11:58:58.000+0000", "updated": "2014-08-05T07:52:53.000+0000", "description": "There should be a warning when memoryFraction is lower than 0 or greater than 1", "comments": ["I think here should throw an exception.", "I'll volunteer to take this, can somebody assign it to me?", "i have fix it by throw an exception\nhttps://github.com/apache/spark/pull/714", "I'd like to propose an additional (somewhat orthogonal) solution to PR-714, which can be viewed here:\n\nhttps://github.com/erikerlandson/spark/compare/SPARK-1779-memoryFraction\n\nThe basic elements of this proposal are:\n*) getInt, getLong, and getDouble support new optional arguments:  minValue and maxValue.   If either or both of these are supplied, the corresponding boundaries are checked.  A parameter setting that violates either bound results in an exception by default.\n*)  getInt, getLong, getDouble and getBoolean support an optional argument:  defaultBadValue.  This argument defaults to false.  If set to true, a bounds checking failure (or other value failures) will result in a log warning message, and the return of the given defaultValue.\n*) new arguments are applied to some invocations of getDouble, e.g. for memoryFraction.\n*) unit testing for getInt, getLong, getDouble, getBoolean\n\nA couple advantages of this approach are:\n*) it provides a standardized interface for applying bound checking on SparkConf settings\n*) SparkConf owns the checking logic, which helps keep code DRY\n\nNote, there is also a proposal for more generalized checking predicates here:  SPARK-1781\n\nAnother possibly useful feature I've seen on other projects is to have validation logic be data driven.  That is, required properties for each configuration parameter can reside in a file and be deserialized at startup time, or alternatively have them reside in some appropriate standardized data structure that lives in the code.\n", "Fixed via:\nhttps://github.com/apache/spark/pull/714"], "derived": {"summary": "There should be a warning when memoryFraction is lower than 0 or greater than 1.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Warning when spark.storage.memoryFraction is not between 0 and 1 - There should be a warning when memoryFraction is lower than 0 or greater than 1."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed via:\nhttps://github.com/apache/spark/pull/714"}]}}
{"project": "SPARK", "issue_id": "SPARK-1780", "title": "Non-existent SPARK_DAEMON_OPTS is referred to in a few places", "status": "Resolved", "priority": "Major", "reporter": "Andrew Or", "assignee": "Andrew Or", "labels": [], "created": "2014-05-09T19:28:16.000+0000", "updated": "2014-11-05T10:45:21.000+0000", "description": "SparkConf.scala and spark-env.sh refer to a non-existent SPARK_DAEMON_OPTS. What they really mean SPARK_DAEMON_JAVA_OPTS.", "comments": ["https://github.com/apache/spark/pull/751"], "derived": {"summary": "SparkConf. scala and spark-env.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Non-existent SPARK_DAEMON_OPTS is referred to in a few places - SparkConf. scala and spark-env."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/751"}]}}
{"project": "SPARK", "issue_id": "SPARK-1781", "title": "Generalized validity checking for configuration parameters", "status": "Resolved", "priority": "Minor", "reporter": "William Benton", "assignee": null, "labels": [], "created": "2014-05-09T20:35:54.000+0000", "updated": "2015-07-20T12:50:35.000+0000", "description": "Issues like SPARK-1779 could be handled easily by a general mechanism for specifying whether or not a configuration parameter value is valid or not (and then excepting or warning and switching to a default value if it is not).  I think it's possible to do this in a fairly lightweight fashion.", "comments": ["Could someone assign this issue to me?", "Ideally, pre-fab predicates could be defined on a per-parameter basis, that could live somewhere, either serialized in a file, or in the code, etc.    For example, from SPARK-1779, memoryFraction should always be checked for being on [0.0, 1.0].  A caller of getDouble() should not have to specify that if they're asking for memoryFraction (and perhaps should not be allowed to override that requirement either)"], "derived": {"summary": "Issues like SPARK-1779 could be handled easily by a general mechanism for specifying whether or not a configuration parameter value is valid or not (and then excepting or warning and switching to a default value if it is not). I think it's possible to do this in a fairly lightweight fashion.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Generalized validity checking for configuration parameters - Issues like SPARK-1779 could be handled easily by a general mechanism for specifying whether or not a configuration parameter value is valid or not (and then excepting or warning and switching to a default value if it is not). I think it's possible to do this in a fairly lightweight fashion."}, {"q": "What updates or decisions were made in the discussion?", "a": "Ideally, pre-fab predicates could be defined on a per-parameter basis, that could live somewhere, either serialized in a file, or in the code, etc.    For example, from SPARK-1779, memoryFraction should always be checked for being on [0.0, 1.0].  A caller of getDouble() should not have to specify that if they're asking for memoryFraction (and perhaps should not be allowed to override that requirement either)"}]}}
{"project": "SPARK", "issue_id": "SPARK-1782", "title": "svd for sparse matrix using ARPACK", "status": "Resolved", "priority": "Major", "reporter": "Li Pu", "assignee": null, "labels": [], "created": "2014-05-09T21:33:33.000+0000", "updated": "2014-12-20T11:02:56.000+0000", "description": "Currently the svd implementation in mllib calls the dense matrix svd in breeze, which has a limitation of fitting n^2 Gram matrix entries in memory (n is the number of rows or number of columns of the matrix, whichever is smaller). In many use cases, the original matrix is sparse but the Gram matrix might not, and we often need only the largest k singular values/vectors. To make svd really scalable, the memory usage must be propositional to the non-zero entries in the matrix. \n\nOne solution is to call the de facto standard eigen-decomposition package ARPACK. For an input matrix M, we compute a few eigenvalues and eigenvectors of M^t*M (or M*M^t if its size is smaller) using ARPACK, then use the eigenvalues/vectors to reconstruct singular values/vectors. ARPACK has a reverse communication interface. The user provides a function to multiply a square matrix to be decomposed with a dense vector provided by ARPACK, and return the resulting dense vector to ARPACK. Inside ARPACK it uses an Implicitly Restarted Lanczos Method for symmetric matrix. Outside what we need to provide are two matrix-vector multiplications, first M*x then M^t*x. These multiplications can be done in Spark in a distributed manner.\n\nThe working memory used by ARPACK is O(n*k). When k (the number of desired singular values) is small, it can be easily fit into the memory of the master machine. The overall model is master machine runs ARPACK, and distribute matrix-vector multiplication onto working executors in each iteration. \n\nI made a PR to breeze with an ARPACK-backed svds interface (https://github.com/scalanlp/breeze/pull/240). The interface takes anything that can be multiplied by a DenseVector. On Spark/milib side, just need to implement the sparsematrix-vector multiplication. \n\nIt might take some time to optimize and fully test this implementation, so set the workload estimate to 4 weeks. \n", "comments": ["This sounds good to me. Let's assume that A is m-by-n where n is small (<1e6). Note that (A^T A) x = (A^T A)^T x can be computed in a single pass, which is sum_i (a_i^T x) a_i . So we don't need to implement A^T y, which simplifies the task.", "Btw, this approach only gives us \\Sigma and V. If we compute U via A V \\Sigma^{-1} (current implementation in MLlib), very likely we lose orthogonality. For sparse SVD, the best package is PROPACK, which implements Lanczos bidiagonalization with partial reorthogonalization (http://sun.stanford.edu/~rmunk/PROPACK/). But let us use ARPACK now since we can call it from Breeze.", "[~mengxr] thank you for the comments! You are right, (A^T A) x can be done in a single pass. What we need is actually a eigs solver. Seems that mllib depends on Breeze 0.7 (though I cannot find this release version in scalanlp/breeze). I think the code would be cleaner if we call eigs in Breeze? or do you prefer to have more control in mllib by calling ARPACK directly?\n\nAlso thank you for the pointer to PROPACK. I looked into the svd routine. It calls lapack dbdsqr for actual svd computation. I will try to find a better way to incorporate Fortran routines in a distributed way. For now we can use ARPACK.", "If you need the the latest Breeze to use eigs, I would prefer calling ARPACK directly.", "PR: https://github.com/apache/spark/pull/964", "Issue resolved by pull request 964\n[https://github.com/apache/spark/pull/964]", "I am interested to try this new svd implementation. Is there an estimate when will spark 1.1.0 be officially released ?", "The plan is to release v1.1 by the end of the month. The feature is available in both master and branch-1.1. You can also checkout the current snapshot and have a try."], "derived": {"summary": "Currently the svd implementation in mllib calls the dense matrix svd in breeze, which has a limitation of fitting n^2 Gram matrix entries in memory (n is the number of rows or number of columns of the matrix, whichever is smaller). In many use cases, the original matrix is sparse but the Gram matrix might not, and we often need only the largest k singular values/vectors.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "svd for sparse matrix using ARPACK - Currently the svd implementation in mllib calls the dense matrix svd in breeze, which has a limitation of fitting n^2 Gram matrix entries in memory (n is the number of rows or number of columns of the matrix, whichever is smaller). In many use cases, the original matrix is sparse but the Gram matrix might not, and we often need only the largest k singular values/vectors."}, {"q": "What updates or decisions were made in the discussion?", "a": "The plan is to release v1.1 by the end of the month. The feature is available in both master and branch-1.1. You can also checkout the current snapshot and have a try."}]}}
{"project": "SPARK", "issue_id": "SPARK-1783", "title": "Title contains html code in MLlib guide", "status": "Resolved", "priority": "Minor", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "labels": [], "created": "2014-05-09T21:40:03.000+0000", "updated": "2014-05-19T00:20:46.000+0000", "description": "We use \n\n---\nlayout: global\ntitle: <a href=\"mllib-guide.html\">MLlib</a> - Clustering\n---\n\nto create a link in the title to the main page of MLlib's guide. However, the generated title contains raw html code, which shows up in the tab or title bar of the browser.", "comments": ["Added `displayTitle` variable to the global layout. If this is defined, use it instead of `title` for page title in `<h1>`.", "Fixed in PR: https://github.com/apache/spark/pull/816"], "derived": {"summary": "We use \n\n---\nlayout: global\ntitle: <a href=\"mllib-guide. html\">MLlib</a> - Clustering\n---\n\nto create a link in the title to the main page of MLlib's guide.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Title contains html code in MLlib guide - We use \n\n---\nlayout: global\ntitle: <a href=\"mllib-guide. html\">MLlib</a> - Clustering\n---\n\nto create a link in the title to the main page of MLlib's guide."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed in PR: https://github.com/apache/spark/pull/816"}]}}
{"project": "SPARK", "issue_id": "SPARK-1784", "title": "Add a partitioner which partitions an RDD with each partition having specified # of keys", "status": "Closed", "priority": "Minor", "reporter": "Syed A. Hashmi", "assignee": null, "labels": [], "created": "2014-05-09T23:27:07.000+0000", "updated": "2014-05-29T06:17:07.000+0000", "description": "At times on mailing lists, I have seen people complaining about having no control over # of keys per partition. RangePartitioner partitions keys in to roughly equal sized partitions, but in cases where user wants full control over specifying exact size, it is not possible today.", "comments": ["[~matei]: Can you please assign this JIRA to me? I have the code in my repo and will submit the PR shortly to get feedback.", "I think this might be subsumed by the fix to SPARK-1770.\n\nhttps://github.com/apache/spark/pull/727/files", "As discussed on https://github.com/apache/spark/pull/727/files, round-robin assignment does not fit the contract of Partitioner -- Partitioner is supposed to consistently map each key to one partition ID. You can use RDD.repartition() or coalesce() to get even partitioning for arbitrary datasets."], "derived": {"summary": "At times on mailing lists, I have seen people complaining about having no control over # of keys per partition. RangePartitioner partitions keys in to roughly equal sized partitions, but in cases where user wants full control over specifying exact size, it is not possible today.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add a partitioner which partitions an RDD with each partition having specified # of keys - At times on mailing lists, I have seen people complaining about having no control over # of keys per partition. RangePartitioner partitions keys in to roughly equal sized partitions, but in cases where user wants full control over specifying exact size, it is not possible today."}, {"q": "What updates or decisions were made in the discussion?", "a": "As discussed on https://github.com/apache/spark/pull/727/files, round-robin assignment does not fit the contract of Partitioner -- Partitioner is supposed to consistently map each key to one partition ID. You can use RDD.repartition() or coalesce() to get even partitioning for arbitrary datasets."}]}}
{"project": "SPARK", "issue_id": "SPARK-1785", "title": "Streaming requires receivers to be serializable", "status": "Closed", "priority": "Major", "reporter": "Hari Shreedharan", "assignee": null, "labels": [], "created": "2014-05-10T01:07:36.000+0000", "updated": "2014-07-10T20:22:56.000+0000", "description": "When the ReceiverTracker starts the receivers it creates a temporary RDD to  send the receivers over to the workers. Then they are started on the workers  using a the startReceivers method.\n\nLooks like this means that the receivers have to really be serializable. In case of the Flume receiver, the Avro IPC components are not serializable causing an error that looks like this:\n{code}\nException in thread \"Thread-46\" org.apache.spark.SparkException: Job aborted due to stage failure: Task not serializable: java.io.NotSerializableException: org.apache.avro.ipc.specific.SpecificResponder\n\t- field (class \"org.apache.spark.streaming.flume.FlumeReceiver\", name: \"responder\", type: \"class org.apache.avro.ipc.specific.SpecificResponder\")\n\t- object (class \"org.apache.spark.streaming.flume.FlumeReceiver\", org.apache.spark.streaming.flume.FlumeReceiver@5e6bbb36)\n\t- element of array (index: 0)\n\t- array (class \"[Lorg.apache.spark.streaming.receiver.Receiver;\", size: 1)\n\t- field (class \"scala.collection.mutable.WrappedArray$ofRef\", name: \"array\", type: \"class [Ljava.lang.Object;\")\n\t- object (class \"scala.collection.mutable.WrappedArray$ofRef\", WrappedArray(org.apache.spark.streaming.flume.FlumeReceiver@5e6bbb36))\n\t- field (class \"org.apache.spark.rdd.ParallelCollectionPartition\", name: \"values\", type: \"interface scala.collection.Seq\")\n\t- custom writeObject data (class \"org.apache.spark.rdd.ParallelCollectionPartition\")\n\t- object (class \"org.apache.spark.rdd.ParallelCollectionPartition\", org.apache.spark.rdd.ParallelCollectionPartition@691)\n\t- writeExternal data\n\t- root object (class \"org.apache.spark.scheduler.ResultTask\", ResultTask(0, 0))\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1033)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1017)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1015)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1015)\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitMissingTasks(DAGScheduler.scala:770)\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:713)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1176)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:456)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:219)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n{code}\n\nA way out of this is to simply send the class name (or .class) to the workers in the tempRDD and have the workers instantiate and start the receiver.\n\nMy analysis maybe wrong. but if it makes sense, I will submit a PR to fix this.", "comments": ["The problem with sending only the class file across is that any configuration of the receivers is lost. We might need to find another way around it - the first being making the receivers explicitly extend Serializable. \n\nIn the Flume receiver case, we can fix this in some way by instantiating the requestor and the server only later. I am actually working on a different receiver anyway, so I will fix the current one too - but we need to look at other receivers as well.", "This really is a documentation jira. In Flume's case, having internal implementation being initialized lazily seems to save the day (I removed the lazy which resulted in the error).", "Yeah, this will be good to document. This can be added the guide for creating custom receivers. I am adding this to this overarching documentation JIRA.\nhttps://issues.apache.org/jira/browse/SPARK-2419"], "derived": {"summary": "When the ReceiverTracker starts the receivers it creates a temporary RDD to  send the receivers over to the workers. Then they are started on the workers  using a the startReceivers method.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Streaming requires receivers to be serializable - When the ReceiverTracker starts the receivers it creates a temporary RDD to  send the receivers over to the workers. Then they are started on the workers  using a the startReceivers method."}, {"q": "What updates or decisions were made in the discussion?", "a": "Yeah, this will be good to document. This can be added the guide for creating custom receivers. I am adding this to this overarching documentation JIRA.\nhttps://issues.apache.org/jira/browse/SPARK-2419"}]}}
{"project": "SPARK", "issue_id": "SPARK-1786", "title": "Kryo Serialization Error in GraphX", "status": "Resolved", "priority": "Major", "reporter": "Joseph E. Gonzalez", "assignee": "Joseph E. Gonzalez", "labels": [], "created": "2014-05-10T01:29:21.000+0000", "updated": "2014-05-12T02:22:01.000+0000", "description": "The following code block will generate a serialization error when run in the spark-shell with Kryo enabled:\n\n{code}\nimport org.apache.spark.storage._\nimport org.apache.spark.graphx._\nimport org.apache.spark.graphx.util._\n\nval g = GraphGenerators.gridGraph(sc, 100, 100)\nval e = g.edges\ne.persist(StorageLevel.MEMORY_ONLY_SER)\ne.collect().foreach(println(_)) // <- Runs successfully the first time.\n\n// The following line will fail:\ne.collect().foreach(println(_))\n{code}\n\nThe following error is generated:\n\n{code}\nscala> e.collect().foreach(println(_))\n14/05/09 18:31:13 INFO SparkContext: Starting job: collect at EdgeRDD.scala:59\n14/05/09 18:31:13 INFO DAGScheduler: Got job 1 (collect at EdgeRDD.scala:59) with 8 output partitions (allowLocal=false)\n14/05/09 18:31:13 INFO DAGScheduler: Final stage: Stage 1(collect at EdgeRDD.scala:59)\n14/05/09 18:31:13 INFO DAGScheduler: Parents of final stage: List()\n14/05/09 18:31:13 INFO DAGScheduler: Missing parents: List()\n14/05/09 18:31:13 INFO DAGScheduler: Submitting Stage 1 (MappedRDD[15] at map at EdgeRDD.scala:59), which has no missing parents\n14/05/09 18:31:13 INFO DAGScheduler: Submitting 8 missing tasks from Stage 1 (MappedRDD[15] at map at EdgeRDD.scala:59)\n14/05/09 18:31:13 INFO TaskSchedulerImpl: Adding task set 1.0 with 8 tasks\n14/05/09 18:31:13 INFO TaskSetManager: Starting task 1.0:0 as TID 8 on executor localhost: localhost (PROCESS_LOCAL)\n14/05/09 18:31:13 INFO TaskSetManager: Serialized task 1.0:0 as 1779 bytes in 3 ms\n14/05/09 18:31:13 INFO TaskSetManager: Starting task 1.0:1 as TID 9 on executor localhost: localhost (PROCESS_LOCAL)\n14/05/09 18:31:13 INFO TaskSetManager: Serialized task 1.0:1 as 1779 bytes in 4 ms\n14/05/09 18:31:13 INFO TaskSetManager: Starting task 1.0:2 as TID 10 on executor localhost: localhost (PROCESS_LOCAL)\n14/05/09 18:31:13 INFO TaskSetManager: Serialized task 1.0:2 as 1779 bytes in 4 ms\n14/05/09 18:31:13 INFO TaskSetManager: Starting task 1.0:3 as TID 11 on executor localhost: localhost (PROCESS_LOCAL)\n14/05/09 18:31:13 INFO TaskSetManager: Serialized task 1.0:3 as 1779 bytes in 4 ms\n14/05/09 18:31:13 INFO TaskSetManager: Starting task 1.0:4 as TID 12 on executor localhost: localhost (PROCESS_LOCAL)\n14/05/09 18:31:13 INFO TaskSetManager: Serialized task 1.0:4 as 1779 bytes in 3 ms\n14/05/09 18:31:13 INFO TaskSetManager: Starting task 1.0:5 as TID 13 on executor localhost: localhost (PROCESS_LOCAL)\n14/05/09 18:31:13 INFO TaskSetManager: Serialized task 1.0:5 as 1782 bytes in 4 ms\n14/05/09 18:31:13 INFO TaskSetManager: Starting task 1.0:6 as TID 14 on executor localhost: localhost (PROCESS_LOCAL)\n14/05/09 18:31:13 INFO TaskSetManager: Serialized task 1.0:6 as 1783 bytes in 4 ms\n14/05/09 18:31:13 INFO TaskSetManager: Starting task 1.0:7 as TID 15 on executor localhost: localhost (PROCESS_LOCAL)\n14/05/09 18:31:13 INFO TaskSetManager: Serialized task 1.0:7 as 1783 bytes in 4 ms\n14/05/09 18:31:13 INFO Executor: Running task ID 9\n14/05/09 18:31:13 INFO Executor: Running task ID 8\n14/05/09 18:31:13 INFO Executor: Running task ID 11\n14/05/09 18:31:13 INFO Executor: Running task ID 14\n14/05/09 18:31:13 INFO Executor: Running task ID 10\n14/05/09 18:31:13 INFO Executor: Running task ID 13\n14/05/09 18:31:13 INFO Executor: Running task ID 15\n14/05/09 18:31:13 INFO Executor: Running task ID 12\n14/05/09 18:31:13 INFO BlockManager: Found block rdd_12_6 locally\n14/05/09 18:31:13 INFO BlockManager: Found block rdd_12_4 locally\n14/05/09 18:31:13 INFO BlockManager: Found block rdd_12_2 locally\n14/05/09 18:31:13 INFO BlockManager: Found block rdd_12_7 locally\n14/05/09 18:31:13 INFO BlockManager: Found block rdd_12_1 locally\n14/05/09 18:31:13 INFO BlockManager: Found block rdd_12_3 locally\n14/05/09 18:31:13 INFO BlockManager: Found block rdd_12_0 locally\n14/05/09 18:31:13 INFO BlockManager: Found block rdd_12_5 locally\n14/05/09 18:31:13 ERROR Executor: Exception in task ID 13\njava.lang.NullPointerException\n\tat org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:269)\n\tat org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:262)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1157)\n\tat org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)\n\tat org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:51)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:208)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\n14/05/09 18:31:13 ERROR Executor: Exception in task ID 10\njava.lang.NullPointerException\n\tat org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:269)\n\tat org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:262)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1157)\n\tat org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)\n\tat org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:51)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:208)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\n14/05/09 18:31:13 ERROR Executor: Exception in task ID 11\njava.lang.NullPointerException\n\tat org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:269)\n\tat org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:262)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1157)\n\tat org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)\n\tat org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:51)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:208)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\n14/05/09 18:31:13 ERROR Executor: Exception in task ID 12\njava.lang.NullPointerException\n\tat org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:269)\n\tat org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:262)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1157)\n\tat org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)\n\tat org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:51)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:208)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\n14/05/09 18:31:13 ERROR Executor: Exception in task ID 15\njava.lang.NullPointerException\n\tat org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:269)\n\tat org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:262)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1157)\n\tat org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)\n\tat org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:51)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:208)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\n14/05/09 18:31:13 ERROR Executor: Exception in task ID 8\njava.lang.NullPointerException\n\tat org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:269)\n\tat org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:262)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1157)\n\tat org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)\n\tat org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:51)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:208)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\n14/05/09 18:31:13 ERROR Executor: Exception in task ID 9\njava.lang.NullPointerException\n\tat org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:269)\n\tat org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:262)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1157)\n\tat org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)\n\tat org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:51)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:208)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\n14/05/09 18:31:13 ERROR Executor: Exception in task ID 14\njava.lang.NullPointerException\n\tat org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:269)\n\tat org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:262)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1157)\n\tat org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)\n\tat org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:51)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:208)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\n14/05/09 18:31:13 WARN TaskSetManager: Lost TID 11 (task 1.0:3)\n14/05/09 18:31:13 WARN TaskSetManager: Loss was due to java.lang.NullPointerException\njava.lang.NullPointerException\n\tat org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:269)\n\tat org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:262)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1157)\n\tat org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)\n\tat org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:51)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:208)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\n14/05/09 18:31:13 ERROR TaskSetManager: Task 1.0:3 failed 1 times; aborting job\n14/05/09 18:31:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n14/05/09 18:31:13 INFO TaskSetManager: Loss was due to java.lang.NullPointerException [duplicate 1]\n14/05/09 18:31:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n14/05/09 18:31:13 INFO TaskSetManager: Loss was due to java.lang.NullPointerException [duplicate 2]\n14/05/09 18:31:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n14/05/09 18:31:13 INFO TaskSetManager: Loss was due to java.lang.NullPointerException [duplicate 3]\n14/05/09 18:31:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n14/05/09 18:31:13 INFO TaskSetManager: Loss was due to java.lang.NullPointerException [duplicate 4]\n14/05/09 18:31:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n14/05/09 18:31:13 INFO DAGScheduler: Failed to run collect at EdgeRDD.scala:59\n14/05/09 18:31:13 INFO TaskSchedulerImpl: Cancelling stage 1\n14/05/09 18:31:13 INFO TaskSetManager: Loss was due to java.lang.NullPointerException [duplicate 5]\n14/05/09 18:31:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n14/05/09 18:31:13 INFO TaskSetManager: Loss was due to java.lang.NullPointerException [duplicate 6]\n14/05/09 18:31:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n14/05/09 18:31:13 INFO TaskSetManager: Loss was due to java.lang.NullPointerException [duplicate 7]\n14/05/09 18:31:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \norg.apache.spark.SparkException: Job aborted due to stage failure: Task 1.0:3 failed 1 times, most recent failure: Exception failure in TID 11 on host localhost: java.lang.NullPointerException\n        org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:269)\n        org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:262)\n        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n        scala.collection.Iterator$class.foreach(Iterator.scala:727)\n        scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n        scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n        scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n        scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n        scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n        scala.collection.AbstractIterator.to(Iterator.scala:1157)\n        scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n        scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)\n        scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n        scala.collection.AbstractIterator.toArray(Iterator.scala:1157)\n        org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)\n        org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)\n        org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)\n        org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)\n        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)\n        org.apache.spark.scheduler.Task.run(Task.scala:51)\n        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:208)\n        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        java.lang.Thread.run(Thread.java:744)\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1033)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1017)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1015)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1015)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:633)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:633)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:633)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1207)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:456)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:219)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107\n{code}\n\nWe believe the error is associated with serialization of the EdgePartition.", "comments": ["https://github.com/apache/spark/pull/724"], "derived": {"summary": "The following code block will generate a serialization error when run in the spark-shell with Kryo enabled:\n\n{code}\nimport org. apache.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Kryo Serialization Error in GraphX - The following code block will generate a serialization error when run in the spark-shell with Kryo enabled:\n\n{code}\nimport org. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/724"}]}}
{"project": "SPARK", "issue_id": "SPARK-1787", "title": "Build failure on JDK8 :: SBT fails to load build configuration file", "status": "Resolved", "priority": "Minor", "reporter": "Richard Gomes", "assignee": null, "labels": [], "created": "2014-05-10T14:33:48.000+0000", "updated": "2014-10-13T17:09:48.000+0000", "description": "SBT fails to build under JDK8.\nPlease find steps to reproduce the error below:\n\n\n(j8s10)rgomes@terra:~/workspace/spark-0.9.1$ uname -a\nLinux terra 3.13-1-amd64 #1 SMP Debian 3.13.10-1 (2014-04-15) x86_64 GNU/Linux\n\n\n(j8s10)rgomes@terra:~/workspace/spark-0.9.1$ java -version\njava version \"1.8.0_05\"\nJava(TM) SE Runtime Environment (build 1.8.0_05-b13)\nJava HotSpot(TM) 64-Bit Server VM (build 25.5-b02, mixed mode)\n\n\n(j8s10)rgomes@terra:~/workspace/spark-0.9.1$ scala -version\nScala code runner version 2.10.3 -- Copyright 2002-2013, LAMP/EPFL\n\n\n(j8s10)rgomes@terra:~/workspace/spark-0.9.1$ sbt/sbt clean\nLaunching sbt from sbt/sbt-launch-0.12.4.jar\nJava HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=350m; support was removed in 8.0\n[info] Loading project definition from /home/rgomes/workspace/spark-0.9.1/project/project\n[info] Compiling 1 Scala source to /home/rgomes/workspace/spark-0.9.1/project/project/target/scala-2.9.2/sbt-0.12/classes...\n[error] error while loading CharSequence, class file '/opt/developer/jdk1.8.0_05/jre/lib/rt.jar(java/lang/CharSequence.class)' is broken\n[error] (bad constant pool tag 15 at byte 1501)\n[error] error while loading Comparator, class file '/opt/developer/jdk1.8.0_05/jre/lib/rt.jar(java/util/Comparator.class)' is broken\n[error] (bad constant pool tag 15 at byte 5003)\n[error] two errors found\n[error] (compile:compile) Compilation failed\nProject loading failed: (r)etry, (q)uit, (l)ast, or (i)gnore? q\n", "comments": ["If I switch to JDK7, keeping everything else unchanged, SBT is able to load the build file.\n\n\n(j7s10)rgomes@terra:~/workspace/spark-0.9.1$ java -version\njava version \"1.7.0_51\"\nJava(TM) SE Runtime Environment (build 1.7.0_51-b13)\nJava HotSpot(TM) 64-Bit Server VM (build 24.51-b03, mixed mode)\n\n\n(j7s10)rgomes@terra:~/workspace/spark-0.9.1$ scala -version\nScala code runner version 2.10.3 -- Copyright 2002-2013, LAMP/EPFL\n\n\n(j7s10)rgomes@terra:~/workspace/spark-0.9.1$ sbt/sbt clean\nLaunching sbt from sbt/sbt-launch-0.12.4.jar\n[info] Loading project definition from /home/rgomes/workspace/spark-0.9.1/project/project\n[info] Compiling 1 Scala source to /home/rgomes/workspace/spark-0.9.1/project/project/target/scala-2.9.2/sbt-0.12/classes...\n[info] Loading project definition from /home/rgomes/workspace/spark-0.9.1/project\n[info] Set current project to root (in build file:/home/rgomes/workspace/spark-0.9.1/)\n[success] Total time: 0 s, completed 10-May-2014 15:40:26\n", "Duplicate of https://issues.apache.org/jira/browse/SPARK-1444 it appears", "FWIW SBT + Java 8 has worked fine for me on master for a long while, so assume this does not affect 1.1 or perhaps 1.0."], "derived": {"summary": "SBT fails to build under JDK8. Please find steps to reproduce the error below:\n\n\n(j8s10)rgomes@terra:~/workspace/spark-0.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Build failure on JDK8 :: SBT fails to load build configuration file - SBT fails to build under JDK8. Please find steps to reproduce the error below:\n\n\n(j8s10)rgomes@terra:~/workspace/spark-0."}, {"q": "What updates or decisions were made in the discussion?", "a": "FWIW SBT + Java 8 has worked fine for me on master for a long while, so assume this does not affect 1.1 or perhaps 1.0."}]}}
{"project": "SPARK", "issue_id": "SPARK-1788", "title": "Upgrade Parquet to 1.4.3", "status": "Resolved", "priority": "Major", "reporter": "Michael Armbrust", "assignee": "Michael Armbrust", "labels": [], "created": "2014-05-10T18:51:01.000+0000", "updated": "2014-05-10T18:51:15.000+0000", "description": "https://github.com/apache/spark/pull/684", "comments": [], "derived": {"summary": "https://github. com/apache/spark/pull/684.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Upgrade Parquet to 1.4.3 - https://github. com/apache/spark/pull/684."}]}}
{"project": "SPARK", "issue_id": "SPARK-1789", "title": "Multiple versions of Netty dependencies cause FlumeStreamSuite failure", "status": "Resolved", "priority": "Major", "reporter": "Sean R. Owen", "assignee": "Sean R. Owen", "labels": ["flume", "netty", "test"], "created": "2014-05-10T20:00:30.000+0000", "updated": "2015-01-15T09:08:42.000+0000", "description": "TL;DR is there is a bit of JAR hell trouble with Netty, that can be mostly resolved and will resolve a test failure.\n\n\nI hit the error described at http://apache-spark-user-list.1001560.n3.nabble.com/SparkContext-startup-time-out-td1753.html while running FlumeStreamingSuite, and have for a short while (is it just me?)\n\nvelvia notes:\n\"I have found a workaround.  If you add akka 2.2.4 to your dependencies, then everything works, probably because akka 2.2.4 brings in newer version of Jetty.\" \n\nThere are at least 3 versions of Netty in play in the build:\n\n- the new Flume 1.4.0 dependency brings in io.netty:netty:3.4.0.Final, and that is the immediate problem\n- the custom version of akka 2.2.3 depends on io.netty:netty:3.6.6.\n- but, Spark Core directly uses io.netty:netty-all:4.0.17.Final\n\nThe POMs try to exclude other versions of netty, but are excluding org.jboss.netty:netty, when in fact older versions of io.netty:netty (not netty-all) are also an issue.\n\nThe org.jboss.netty:netty excludes are largely unnecessary. I replaced many of them with io.netty:netty exclusions until everything agreed on io.netty:netty-all:4.0.17.Final.\n\nBut this didn't work, since Akka 2.2.3 doesn't work with Netty 4.x. Down-grading to 3.6.6.Final across the board made some Spark code not compile.\n\nIf the build *keeps* io.netty:netty:3.6.6.Final as well, everything seems to work. Part of the reason seems to be that Netty 3.x used the old `org.jboss.netty` packages. This is less than ideal, but is no worse than the current situation. \n\n\nSo this PR resolves the issue and improves the JAR hell, even if it leaves the existing theoretical Netty 3-vs-4 conflict:\n\n- Remove org.jboss.netty excludes where possible, for clarity; they're not needed except with Hadoop artifacts\n- Add io.netty:netty excludes where needed -- except, let akka keep its io.netty:netty\n- Change a bit of test code that actually depended on Netty 3.x, to use 4.x equivalent\n- Update SBT build accordingly\n\nA better change would be to update Akka far enough such that it agrees on Netty 4.x, but I don't know if that's feasible.\n", "comments": ["Issue resolved by pull request 723\n[https://github.com/apache/spark/pull/723]", "Sean, we're currently building against Akka 2.3.0 in Fedora (it's a trivial source patch against 0.9.1; I haven't investigated the delta against 1.0 yet).  Are there reasons why Akka 2.3.0 is a bad idea for Spark in general?  If not, I'm happy to file a JIRA for updating the dependency and contribute my patch upstream.", "I don't have any info either way on that. Later is always better no? probably OK to consider post-1.0? \n\nThe issue here was to do with Netty, and the comment about Akka that I quoted was really meant to suggest that it was Netty (as it happens being imported by Akka) that was relevant.", "Yes, this is absolutely a post-1.0 thing.  I'm just saying that by updating the version of Akka to 2.3 we'd eliminate one of Spark's dependencies that can't work with Netty 4.  The issue of only transitively depending on at most one version of Netty 3 and at most one version of Netty 4 (and choosing ones that can work different coordinates) is orthogonal, but still an issue."], "derived": {"summary": "TL;DR is there is a bit of JAR hell trouble with Netty, that can be mostly resolved and will resolve a test failure. I hit the error described at http://apache-spark-user-list.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Multiple versions of Netty dependencies cause FlumeStreamSuite failure - TL;DR is there is a bit of JAR hell trouble with Netty, that can be mostly resolved and will resolve a test failure. I hit the error described at http://apache-spark-user-list."}, {"q": "What updates or decisions were made in the discussion?", "a": "Yes, this is absolutely a post-1.0 thing.  I'm just saying that by updating the version of Akka to 2.3 we'd eliminate one of Spark's dependencies that can't work with Netty 4.  The issue of only transitively depending on at most one version of Netty 3 and at most one version of Netty 4 (and choosing ones that can work different coordinates) is orthogonal, but still an issue."}]}}
{"project": "SPARK", "issue_id": "SPARK-1790", "title": "Update EC2 scripts to support r3 instance types", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Sujeet Varakhedi", "labels": ["Starter"], "created": "2014-05-10T20:10:37.000+0000", "updated": "2014-06-04T23:11:30.000+0000", "description": "These were recently added by Amazon as a cheaper high-memory option", "comments": ["I will work on this", "Thanks Sujeet! Just post here when you have a pull request to fix it.", "Sorry I just got to this yesterday. Currently testing the changes. One note is; Certain instance types (R3 is one of them) are not available on some regions. I don't see that check anywhere in the script for other instance types so I am not going check that for R3 either. Let me know if that is something we should have here. One drawback of adding such check is that as AWS changes their  availability matrix the script will need to be maintained too.", "It's fine to skip the check right now; I think Amazon will soon make them available anywhere anyway.", "Hey [~sujeetv] someone asked me to look at this today and I think I have a patch. I didn't realize you were working on this, sorry about that. I'd be curious to see your progress and see if you ended up doing something similar.\n\nhttps://github.com/mesos/spark-ec2/pull/53/files\nhttps://github.com/apache/spark/pull/957\n", "https://github.com/apache/spark/pull/960", "Hey Patrick. Yeah my patch is pretty much the same as users. Just 2 changes. Also I tested with all the instance types and one I noticed is that with default ssh timeouts the scripts sometimes just bails out because AWS is not able to bring up instances quick enough. This might not specific to r3 instances though.", "I closed mine in favor of yours - thanks!", "I went ahead and merged this. There are several relevant commits:\n\n[master] https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=11ded3f66f178e4d8d2b23491dd5e0ea23bcf719\n\n[branch-1.0] https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=914d98ecd5a734710d269a2ab70b804b13c46ea3\n\n[branch-0.9] https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=6634a348adfcda4cd50c50bc61927b6ca41f4f5f\n\nspark-ec2 changes:\nhttps://github.com/mesos/spark-ec2/pull/53\nhttps://github.com/mesos/spark-ec2/pull/54\n"], "derived": {"summary": "These were recently added by Amazon as a cheaper high-memory option.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Update EC2 scripts to support r3 instance types - These were recently added by Amazon as a cheaper high-memory option."}, {"q": "What updates or decisions were made in the discussion?", "a": "I went ahead and merged this. There are several relevant commits:\n\n[master] https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=11ded3f66f178e4d8d2b23491dd5e0ea23bcf719\n\n[branch-1.0] https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=914d98ecd5a734710d269a2ab70b804b13c46ea3\n\n[branch-0.9] https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=6634a348adfcda4cd50c50bc61927b6ca41f4f5f\n\nspark-ec2 changes:\nhttps://github.com/mesos/spark-ec2/pull/53\nhttps://github.com/mesos/spark-ec2/pull/54"}]}}
{"project": "SPARK", "issue_id": "SPARK-1791", "title": "SVM implementation does not use threshold parameter", "status": "Resolved", "priority": "Major", "reporter": "Andrew Tulloch", "assignee": null, "labels": [], "created": "2014-05-10T23:26:11.000+0000", "updated": "2014-05-14T21:37:54.000+0000", "description": "The key error is in SVM.scala, in `predictPoint`\n\n```\n    threshold match {\n      case Some(t) => if (margin < 0.0) 0.0 else 1.0\n      case None => margin\n    }\n ```", "comments": ["PR: https://github.com/apache/spark/pull/725"], "derived": {"summary": "The key error is in SVM. scala, in `predictPoint`\n\n```\n    threshold match {\n      case Some(t) => if (margin < 0.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "SVM implementation does not use threshold parameter - The key error is in SVM. scala, in `predictPoint`\n\n```\n    threshold match {\n      case Some(t) => if (margin < 0."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR: https://github.com/apache/spark/pull/725"}]}}
{"project": "SPARK", "issue_id": "SPARK-1792", "title": "Missing Spark-Shell Configure Options", "status": "Closed", "priority": "Major", "reporter": "Joseph E. Gonzalez", "assignee": null, "labels": [], "created": "2014-05-10T23:29:32.000+0000", "updated": "2020-01-17T19:19:54.000+0000", "description": "The `conf/spark-env.sh.template` does not have configure options for the spark shell.   For example to enable Kryo for GraphX when using the spark shell in stand alone mode it appears you must add:\n\n{code}\nSPARK_SUBMIT_OPTS=\"-Dspark.serializer=org.apache.spark.serializer.KryoSerializer \"\nSPARK_SUBMIT_OPTS+=\"-Dspark.kryo.registrator=org.apache.spark.graphx.GraphKryoRegistrator  \"\n{code}\n\nHowever SPARK_SUBMIT_OPTS is not documented anywhere.  Perhaps the spark-shell should have its own options (e.g., SPARK_SHELL_OPTS).\n\n", "comments": ["[~matei] [~pwendell]", "It would be nice if the spark-submit script could automatically set arbitrary spark options. The way we have it now is you can set them in the following ways:\n\n1. You explicitly set an option when you create a SparkConf()\n2. You have default values for spark conf options in spark-defaults.conf.", "Spark shell calls spark submit, which loads configuration options from conf/spark-defaults.conf. This is documented here, but not clearly: https://github.com/apache/spark/blob/master/docs/cluster-overview.md\n\nWe should add this to the configuration page itself (https://github.com/apache/spark/blob/master/docs/configuration.md). I already have a PR on the docs (https://github.com/apache/spark/pull/701) so maybe I can add that.", "That makes a lot of sense!  Perhaps the template should list more options (e.g., how to enable kryo and disable/enable kryo reference tracking)?\n\nAlso it might be helpful to print a message on launch of the spark-shell stating that additional configuration options can be set in conf/spark-defaults.conf.\n\n", "Was there any movement on this? I am seeing an unresolved status and a date of 2014? Does it really exist and it not deprecated? Thanks\n\nBACKGROUND FWIW: As a newb to Spark for a month or two I didn't realize there was a SPARK_SUBMIT_OPTS that can be set in spark-env.sh. \n\nI found this in this blog page: https://abrv8.wordpress.com/2014/10/06/debugging-java-spark-applications/\n\nThat blog pages was linked from this SO question: http://stackoverflow.com/questions/29090745/debugging-spark-applications", "It feels like we've already got a pretty good mechanism for handling this with `spark-defaults.conf` and we've made a lot of progress with documenting many of the configuration enviroment variables in http://spark.apache.org/docs/latest/submitting-applications.html and http://spark.apache.org/docs/latest/configuration.html , so personally I think we should probably mark this issue as resolved."], "derived": {"summary": "The `conf/spark-env. sh.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Missing Spark-Shell Configure Options - The `conf/spark-env. sh."}, {"q": "What updates or decisions were made in the discussion?", "a": "It feels like we've already got a pretty good mechanism for handling this with `spark-defaults.conf` and we've made a lot of progress with documenting many of the configuration enviroment variables in http://spark.apache.org/docs/latest/submitting-applications.html and http://spark.apache.org/docs/latest/configuration.html , so personally I think we should probably mark this issue as resolved."}]}}
{"project": "SPARK", "issue_id": "SPARK-1793", "title": "Heavily duplicated test setup code in SVMSuite", "status": "Resolved", "priority": "Minor", "reporter": "Andrew Tulloch", "assignee": null, "labels": [], "created": "2014-05-10T23:48:15.000+0000", "updated": "2014-07-20T00:31:53.000+0000", "description": "Refactor the code to remove the repeated initialization of test/validation RDDs in every test.", "comments": [], "derived": {"summary": "Refactor the code to remove the repeated initialization of test/validation RDDs in every test.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Heavily duplicated test setup code in SVMSuite - Refactor the code to remove the repeated initialization of test/validation RDDs in every test."}]}}
{"project": "SPARK", "issue_id": "SPARK-1794", "title": "Generic ADMM implementation for SVM, lasso, and L1-regularized logistic regression", "status": "Closed", "priority": "Minor", "reporter": "Andrew Tulloch", "assignee": null, "labels": [], "created": "2014-05-11T00:21:36.000+0000", "updated": "2015-02-21T00:04:31.000+0000", "description": null, "comments": ["Is this a duplicate of https://issues.apache.org/jira/browse/SPARK-1543 ?"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Generic ADMM implementation for SVM, lasso, and L1-regularized logistic regression"}, {"q": "What updates or decisions were made in the discussion?", "a": "Is this a duplicate of https://issues.apache.org/jira/browse/SPARK-1543 ?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1795", "title": "Add recursive directory file search to fileInputStream", "status": "Resolved", "priority": "Major", "reporter": "Rick OToole", "assignee": null, "labels": [], "created": "2014-05-11T06:01:45.000+0000", "updated": "2015-01-23T14:12:51.000+0000", "description": "When writing logs, they are often partitioned into a hierarchical directory structure. This change will allow spark streaming to monitor all sub-directories of a parent directory to find new files as they are added. \n\nSee https://github.com/apache/spark/pull/537", "comments": ["User 'patrickotoole' has created a pull request for this issue:\n[https://github.com/apache/spark/pull/537|https://github.com/apache/spark/pull/537]", "I suggest merging this into SPARK-3586 which is a slight superset of functionality, has a PR that merges. Both were still waiting on review."], "derived": {"summary": "When writing logs, they are often partitioned into a hierarchical directory structure. This change will allow spark streaming to monitor all sub-directories of a parent directory to find new files as they are added.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add recursive directory file search to fileInputStream - When writing logs, they are often partitioned into a hierarchical directory structure. This change will allow spark streaming to monitor all sub-directories of a parent directory to find new files as they are added."}, {"q": "What updates or decisions were made in the discussion?", "a": "I suggest merging this into SPARK-3586 which is a slight superset of functionality, has a PR that merges. Both were still waiting on review."}]}}
{"project": "SPARK", "issue_id": "SPARK-1796", "title": "spark-submit does not set driver memory correctly", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "labels": [], "created": "2014-05-11T06:10:07.000+0000", "updated": "2014-05-12T01:18:32.000+0000", "description": "This is not working correctly at present. We should assume that SPARK_DRIVER_MEM should be set unless --deploy-mode is explicitly set to \"cluster\".\n\n{code}\npatrick@patrick-t430s:~/Documents/spark$ SPARK_PRINT_LAUNCH_COMMAND=1 ./bin/spark-shell --driver-mem\nory 2g\nSpark Command: /usr/lib/jvm/jdk1.7.0_25/bin/java -cp ::/home/patrick/Documents/spark/conf:/home/patrick/Documents/spark/assembly/target/scala-2.10/spark-assembly-1.0.0-hadoop1.0.4.jar -Xms512m -Xmx512m org.apache.spark.deploy.SparkSubmit spark-internal --driver-memory 2g --class org.apache.spark.repl.Main\n========================================\n\n{code}", "comments": ["https://github.com/apache/spark/pull/730"], "derived": {"summary": "This is not working correctly at present. We should assume that SPARK_DRIVER_MEM should be set unless --deploy-mode is explicitly set to \"cluster\".", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "spark-submit does not set driver memory correctly - This is not working correctly at present. We should assume that SPARK_DRIVER_MEM should be set unless --deploy-mode is explicitly set to \"cluster\"."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/730"}]}}
{"project": "SPARK", "issue_id": "SPARK-1797", "title": "streaming on hdfs can detected all new file, but the sum of all the rdd.count() not equals which had detected", "status": "Closed", "priority": "Major", "reporter": "QingFeng Zhang", "assignee": null, "labels": [], "created": "2014-05-11T11:28:50.000+0000", "updated": "2014-05-12T12:27:50.000+0000", "description": "when I put 200 png files to Hdfs , I found sparkStreaming counld detect 200 files , but the sum of rdd.count() is less than 200, always  between 130 and 170, I don't know why...Is this a Bug?\nPS: When I put 200 files in hdfs before streaming run , It get the correct count and right result.\n\n  ", "comments": [], "derived": {"summary": "when I put 200 png files to Hdfs , I found sparkStreaming counld detect 200 files , but the sum of rdd. count() is less than 200, always  between 130 and 170, I don't know why.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "streaming on hdfs can detected all new file, but the sum of all the rdd.count() not equals which had detected - when I put 200 png files to Hdfs , I found sparkStreaming counld detect 200 files , but the sum of rdd. count() is less than 200, always  between 130 and 170, I don't know why."}]}}
{"project": "SPARK", "issue_id": "SPARK-1798", "title": "Tests should clean up temp files", "status": "Resolved", "priority": "Minor", "reporter": "Sean R. Owen", "assignee": "Sean R. Owen", "labels": [], "created": "2014-05-11T15:23:23.000+0000", "updated": "2015-01-15T09:08:44.000+0000", "description": "Three issues related to temp files that tests generate -- these should be touched up for hygiene but are not urgent.\n\nModules have a log4j.properties which directs the unit-test.log output file to a directory like [module]/target/unit-test.log. But this ends up creating [module]/[module]/target/unit-test.log instead of former.\n\nThe work/ directory is not deleted by \"mvn clean\", in the parent and in modules. Neither is the checkpoint/ directory created under the various external modules.\n\nMany tests create a temp directory, which is not usually deleted. This can be largely resolved by calling deleteOnExit() at creation and trying to call Utils.deleteRecursively consistently to clean up, sometimes in an \"@After\" method.\n\n(If anyone seconds the motion, I can create a more significant change that introduces a new test trait along the lines of LocalSparkContext, which provides management of temp directories for subclasses to take advantage of.)", "comments": ["Issue resolved by pull request 732\n[https://github.com/apache/spark/pull/732]"], "derived": {"summary": "Three issues related to temp files that tests generate -- these should be touched up for hygiene but are not urgent. Modules have a log4j.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Tests should clean up temp files - Three issues related to temp files that tests generate -- these should be touched up for hygiene but are not urgent. Modules have a log4j."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 732\n[https://github.com/apache/spark/pull/732]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1799", "title": "Add init script to the debian packaging", "status": "Resolved", "priority": "Major", "reporter": "Nicolas Laleve", "assignee": null, "labels": [], "created": "2014-05-11T16:39:40.000+0000", "updated": "2015-02-11T08:42:58.000+0000", "description": "See https://github.com/apache/spark/pull/733", "comments": ["cc [~markhamstra], [~srowen], [~pwendell]", "This adds a load more stuff to the build for the Debian package. I don't feel qualified to review it, and sounds like there's a question about how much it's worth maintaining this. I myself would hesitate to decide to merge it.", "Per SPARK-5727, I believe the outstanding Debian issues should be closed."], "derived": {"summary": "See https://github. com/apache/spark/pull/733.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add init script to the debian packaging - See https://github. com/apache/spark/pull/733."}, {"q": "What updates or decisions were made in the discussion?", "a": "Per SPARK-5727, I believe the outstanding Debian issues should be closed."}]}}
{"project": "SPARK", "issue_id": "SPARK-1800", "title": "Add broadcast hash join operator", "status": "Resolved", "priority": "Major", "reporter": "Michael Armbrust", "assignee": "Zongheng Yang", "labels": [], "created": "2014-05-11T18:47:27.000+0000", "updated": "2014-06-26T21:01:14.000+0000", "description": null, "comments": ["https://github.com/apache/spark/pull/734", "Maybe add an improvement in future that tasks in the same node can share those hashtables. \n\nAlso, if we have a star join, maybe we want to limit the total size of those hashtables? So, they will not occupy too much space.", "https://github.com/apache/spark/pull/1163"], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add broadcast hash join operator"}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/1163"}]}}
{"project": "SPARK", "issue_id": "SPARK-1801", "title": "Open up some private APIs related to creating new RDDs for developers", "status": "Resolved", "priority": "Minor", "reporter": "koert kuipers", "assignee": null, "labels": [], "created": "2014-05-12T00:44:32.000+0000", "updated": "2014-05-14T07:56:26.000+0000", "description": "in writing my own RDD i ran into a few issues with respect to stuff being private in spark.\n\nin compute i would like to return an iterator that respects task killing (as HadoopRDD does), but the mechanics for that are inside the private InterruptibleIterator. also the exception i am supposed to throw (TaskKilledException) is private to spark.\n\nSee also:\nhttp://apache-spark-user-list.1001560.n3.nabble.com/Re-writing-my-own-RDD-td5558.html", "comments": ["https://github.com/apache/spark/pull/764"], "derived": {"summary": "in writing my own RDD i ran into a few issues with respect to stuff being private in spark. in compute i would like to return an iterator that respects task killing (as HadoopRDD does), but the mechanics for that are inside the private InterruptibleIterator.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Open up some private APIs related to creating new RDDs for developers - in writing my own RDD i ran into a few issues with respect to stuff being private in spark. in compute i would like to return an iterator that respects task killing (as HadoopRDD does), but the mechanics for that are inside the private InterruptibleIterator."}, {"q": "What updates or decisions were made in the discussion?", "a": "https://github.com/apache/spark/pull/764"}]}}
{"project": "SPARK", "issue_id": "SPARK-1802", "title": "Audit dependency graph when Spark is built with -Phive", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Sean R. Owen", "labels": [], "created": "2014-05-12T06:27:16.000+0000", "updated": "2015-01-15T09:08:39.000+0000", "description": "I'd like to have binary release for 1.0 include Hive support. Since this isn't enabled by default in the build I don't think it's as well tested, so we should dig around a bit and decide if we need to e.g. add any excludes.\n\n{code}\n$ mvn install -Phive -DskipTests && mvn dependency:build-classpath -pl assembly | grep -v INFO | tr \":\" \"\\n\" |  awk ' { FS=\"/\"; print ( $(NF) ); }' | sort > without_hive.txt\n\n$ mvn install -Phive -DskipTests && mvn dependency:build-classpath -Phive -pl assembly | grep -v INFO | tr \":\" \"\\n\" |  awk ' { FS=\"/\"; print ( $(NF) ); }' | sort > with_hive.txt\n\n$ diff without_hive.txt with_hive.txt\n< antlr-2.7.7.jar\n< antlr-3.4.jar\n< antlr-runtime-3.4.jar\n10,14d6\n< avro-1.7.4.jar\n< avro-ipc-1.7.4.jar\n< avro-ipc-1.7.4-tests.jar\n< avro-mapred-1.7.4.jar\n< bonecp-0.7.1.RELEASE.jar\n22d13\n< commons-cli-1.2.jar\n25d15\n< commons-compress-1.4.1.jar\n33,34d22\n< commons-logging-1.1.1.jar\n< commons-logging-api-1.0.4.jar\n38d25\n< commons-pool-1.5.4.jar\n46,49d32\n< datanucleus-api-jdo-3.2.1.jar\n< datanucleus-core-3.2.2.jar\n< datanucleus-rdbms-3.2.1.jar\n< derby-10.4.2.0.jar\n53,57d35\n< hive-common-0.12.0.jar\n< hive-exec-0.12.0.jar\n< hive-metastore-0.12.0.jar\n< hive-serde-0.12.0.jar\n< hive-shims-0.12.0.jar\n60,61d37\n< httpclient-4.1.3.jar\n< httpcore-4.1.3.jar\n68d43\n< JavaEWAH-0.3.2.jar\n73d47\n< javolution-5.5.1.jar\n76d49\n< jdo-api-3.0.1.jar\n78d50\n< jetty-6.1.26.jar\n87d58\n< jetty-util-6.1.26.jar\n93d63\n< json-20090211.jar\n98d67\n< jta-1.1.jar\n103,104d71\n< libfb303-0.9.0.jar\n< libthrift-0.9.0.jar\n112d78\n< mockito-all-1.8.5.jar\n136d101\n< servlet-api-2.5-20081211.jar\n139d103\n< snappy-0.2.jar\n144d107\n< spark-hive_2.10-1.0.0.jar\n151d113\n< ST4-4.0.4.jar\n153d114\n< stringtemplate-3.2.1.jar\n156d116\n< velocity-1.7.jar\n158d117\n< xz-1.0.jar\n{code}\n\nSome initial investigation suggests we may need to take some precaution surrounding (a) jetty and (b) servlet-api.", "comments": ["[~pwendell] You can see my start on it here:\n\nhttps://github.com/srowen/spark/commits/SPARK-1802\nhttps://github.com/srowen/spark/commit/a856604cfc67cb58146ada01fda6dbbb2515fa00\n\nThis resolves the new issues you note in your diff.\n\n\nNext issue is that hive-exec, quite awfully, includes a copy of all of its transitive dependencies in its artifact. See https://issues.apache.org/jira/browse/HIVE-5733 and note the warnings you'll get during assembly:\n\n{code}\n[WARNING] hive-exec-0.12.0.jar, libthrift-0.9.0.jar define 153 overlappping classes: \n[WARNING]   - org.apache.thrift.transport.TSaslTransport$SaslResponse\n...\n{code}\n\nhive-exec is in fact used in this module. Aside from actual surgery on the artifact with the shade plugin, you can't control the dependencies as a result. This may be simply \"the best that can be done\" right now. If it has worked, it has worked.\n\n\nAm I right that the datanucleus JARs *are* meant to be in the assembly, only for the Hive build?\nhttps://github.com/apache/spark/pull/688\nhttps://github.com/apache/spark/pull/610\n\nThat's good if so since that's what your diff shows.\n\n\nFinally, while we're here, I note that there are still a few JAR conflicts that turn up when you build the assembly *without* Hive. (I'm going to ignore conflicts in examples; these can be cleaned up but aren't really a big deal given its nature.)  We could touch those up too.\n\nThis is in the normal build (and I know how to zap most of this problem):\n{code}\n[WARNING] commons-beanutils-core-1.8.0.jar, commons-beanutils-1.7.0.jar define 82 overlappping classes: \n{code}\n\nThese turn up in the Hadoop 2.x + YARN build:\n{code}\n[WARNING] servlet-api-2.5.jar, javax.servlet-3.0.0.v201112011016.jar define 42 overlappping classes: \n...\n[WARNING] jcl-over-slf4j-1.7.5.jar, commons-logging-1.1.3.jar define 6 overlappping classes: \n...\n[WARNING] activation-1.1.jar, javax.activation-1.1.0.v201105071233.jar define 17 overlappping classes: \n...\n[WARNING] servlet-api-2.5.jar, javax.servlet-3.0.0.v201112011016.jar define 42 overlappping classes: \n{code}\n\nThese should be easy to track down. Shall I?", "Issue resolved by pull request 744\n[https://github.com/apache/spark/pull/744]", "(Edited to fix comment about protobuf versions)\n\nI looked further into just what might go wrong by including hive-exec into the assembly, since it includes its dependencies directly (i.e. Maven can't manage around it.)\n\nAttached is a full dump of the conflicts.\n\nThe ones that are potential issues appear to be the following, and one looks like it could be a deal-breaker -- protobuf -- since it's neither forwards nor backwards compatible. That is, I recommend testing this assembly with an *newer* Hadoop that needs 2.5 and see if it croaks.\n\nThe rest might be worked around but need some additional mojo to make sure the right version wins in the packaging.\n\nCertainly having hive-exec in the build is making me queasy!\n\n\n[WARNING] hive-exec-0.12.0.jar, libthrift-0.9.0.jar define 153 overlappping classes: \n\nHBase includes libthrift-0.8.0, but it's in examples, and so figure this is ignorable.\n\n\n[WARNING] hive-exec-0.12.0.jar, commons-lang-2.4.jar define 2 overlappping classes: \n\nProbably ignorable, but we have to make sure commons-lang-3.3.2 'wins' in the build.\n\n\n[WARNING] hive-exec-0.12.0.jar, jackson-core-asl-1.9.11.jar define 117 overlappping classes: \n[WARNING] hive-exec-0.12.0.jar, jackson-mapper-asl-1.8.8.jar define 432 overlappping classes: \n\nBelieve this are ignorable. (Not sure why the jackson versions are mismatched? another todo)\n\n\n[WARNING] hive-exec-0.12.0.jar, guava-14.0.1.jar define 1087 overlappping classes: \n\nShould be OK. Hive uses 11.0.2 like Hadoop; the build is already taking that particular risk. We need 14.0.1 to win.\n\n\n[WARNING] hive-exec-0.12.0.jar, protobuf-java-2.4.1.jar define 204 overlappping classes: \n\nOof. Hive has protobuf *2.4.1*. This has got to be a problem for newer Hadoop builds?\n\n(Edited to fix comment about protobuf versions)", "Let's keep this open given the ongoing discussion.", "[Related work|https://github.com/apache/spark/pull/754]", "This protobuf thing is very troubling. The options here are pretty limited since they publish this assembly jar. I see a few:\n\n1. Publish a Hive 0.12 that uses our shaded protobuf 2.4.1 (we already published a shaded version of protobuf 2.4.1). I actually have this working in a local build of Hive 0.12, but I haven't tried to push it to sonatype yet:\nhttps://github.com/pwendell/hive/commits/branch-0.12-shaded-protobuf\n\n2. Upgrade our use of hive to 0.13 (which bumps to protobuf 2.5.0) and only support Spark SQL with Hadoop 2+ - that is, versions of Hadoop that have also bumped to protobuf 2.5.0. I'm not sure how big of an effort that would be in terms of the code changes between 0.12 and 0.13. Spark didn't recompile trivially. I can talk to Michael Armbrust tomorrow morning about this.\n\nOne thing I don't totally understand is how Hive itself deals with this conflict. For instance, if someone wants to run Hive 0.12 with Hadoop 2. Presumably both the Hive protobuf 2.4.1 and the HDFS client protobuf 2.5.0 will be in the JVM at the same time... I'm not sure how they are isolated from each-other. HDP 2.1 for instance, seems to have both (http://hortonworks.com/hdp/whats-new/)", "First, I apologize, I misspoke above. Actually hive-exec 0.12.0 depends on protobuf 2.4.1. I got this wrong since I was evaluating dependencies under the Hadoop 2.3 profile.\n\nThat leaves the same but opposite problem, meaning it probably won't work with later versions of Hadoop at some level.\n\nThe 'right' place to solve this is the hive-exec build, and I know there's a Hive JIRA open for this. Maybe people who are interested in making this work can push that through? But that's more of a medium-term answer.\n\nI suppose it's possible to *also* vary the Hive version with Hadoop version? Assuming the Spark code can be made to work both ways, and I don't know that.This may or may not work as I don't know if Hive-Hadoop are compatible in exactly the same versions that they agree on protobuf.\n\nYou could go ahead with testing the Hive-enabled build anyway, with crossed fingers.\nYou could look at deploying two artifacts, with and without Hive, and simply give in to combinatorial explosion for the short term.\nYou could say Hive builds are do-it-yourself for the short-term, and bring it back in the medium term if/when the project drops support for older Hadoop -- which may be becoming more attractive all the time -- and combinatorial mess is much less\n\n"], "derived": {"summary": "I'd like to have binary release for 1. 0 include Hive support.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Audit dependency graph when Spark is built with -Phive - I'd like to have binary release for 1. 0 include Hive support."}, {"q": "What updates or decisions were made in the discussion?", "a": "First, I apologize, I misspoke above. Actually hive-exec 0.12.0 depends on protobuf 2.4.1. I got this wrong since I was evaluating dependencies under the Hadoop 2.3 profile.\n\nThat leaves the same but opposite problem, meaning it probably won't work with later versions of Hadoop at some level.\n\nThe 'right' place to solve this is the hive-exec build, and I know there's a Hive JIRA open for this. Maybe people who are interested in making this work can push that through? But that's more of a medium-term answer.\n\nI suppose it's possible to *also* vary the Hive version with Hadoop version? Assuming the Spark code can be made to work both ways, and I don't know that.This may or may not work as I don't know if Hive-Hadoop are compatible in exactly the same versions that they agree on protobuf.\n\nYou could go ahead with testing the Hive-enabled build anyway, with crossed fingers.\nYou could look at deploying two artifacts, with and without Hive, and simply give in to combinatorial explosion for the short term.\nYou could say Hive builds are do-it-yourself for the short-term, and bring it back in the medium term if/when the project drops support for older Hadoop -- which may be becoming more attractive all the time -- and combinatorial mess is much less"}]}}
{"project": "SPARK", "issue_id": "SPARK-1803", "title": "Rename test resources to be compatible with Windows FS", "status": "Resolved", "priority": "Trivial", "reporter": "Stevo Slavi", "assignee": null, "labels": [], "created": "2014-05-12T08:23:55.000+0000", "updated": "2014-06-22T07:02:51.000+0000", "description": "{{git clone}} of master branch and then {{git status}} on Windows reports untracked files:\n\n{noformat}\n# Untracked files:\n#   (use \"git add <file>...\" to include in what will be committed)\n#\n#       sql/hive/src/test/resources/golden/Column pruning\n#       sql/hive/src/test/resources/golden/Partition pruning\n#       sql/hive/src/test/resources/golden/Partiton pruning\n{noformat}\n\nActual issue is that several files under {{sql/hive/src/test/resources/golden}} directory have colon in name which is invalid character in file name on Windows.\n\nPlease have these files renamed to a Windows compatible file name.", "comments": ["Created pull request with fix for this issue (see [here|https://github.com/apache/spark/pull/739]).", "PR was committed so this is another that seems to be closeable.", "Resolved via:\nhttps://github.com/apache/spark/pull/739"], "derived": {"summary": "{{git clone}} of master branch and then {{git status}} on Windows reports untracked files:\n\n{noformat}\n# Untracked files:\n#   (use \"git add <file>. \" to include in what will be committed)\n#\n#       sql/hive/src/test/resources/golden/Column pruning\n#       sql/hive/src/test/resources/golden/Partition pruning\n#       sql/hive/src/test/resources/golden/Partiton pruning\n{noformat}\n\nActual issue is that several files under {{sql/hive/src/test/resources/golden}} directory have colon in name which is invalid character in file name on Windows.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Rename test resources to be compatible with Windows FS - {{git clone}} of master branch and then {{git status}} on Windows reports untracked files:\n\n{noformat}\n# Untracked files:\n#   (use \"git add <file>. \" to include in what will be committed)\n#\n#       sql/hive/src/test/resources/golden/Column pruning\n#       sql/hive/src/test/resources/golden/Partition pruning\n#       sql/hive/src/test/resources/golden/Partiton pruning\n{noformat}\n\nActual issue is that several files under {{sql/hive/src/test/resources/golden}} directory have colon in name which is invalid character in file name on Windows."}, {"q": "What updates or decisions were made in the discussion?", "a": "Resolved via:\nhttps://github.com/apache/spark/pull/739"}]}}
{"project": "SPARK", "issue_id": "SPARK-1804", "title": "Mark 0.9.1 as released in JIRA", "status": "Resolved", "priority": "Trivial", "reporter": "Stevo Slavi", "assignee": null, "labels": [], "created": "2014-05-12T08:26:04.000+0000", "updated": "2014-06-22T07:01:36.000+0000", "description": "0.9.1 has been released but is labeled as unreleased in SPARK JIRA project. Please have it marked as released. Also please document that step in release process.", "comments": ["Looks like this can be closed as resolved.\nhttps://issues.apache.org/jira/browse/SPARK/?selectedTab=com.atlassian.jira.jira-projects-plugin:versions-panel"], "derived": {"summary": "0. 9.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Mark 0.9.1 as released in JIRA - 0. 9."}, {"q": "What updates or decisions were made in the discussion?", "a": "Looks like this can be closed as resolved.\nhttps://issues.apache.org/jira/browse/SPARK/?selectedTab=com.atlassian.jira.jira-projects-plugin:versions-panel"}]}}
{"project": "SPARK", "issue_id": "SPARK-1805", "title": "Error launching cluster when master and slave machines are of different virtualization types", "status": "Resolved", "priority": "Minor", "reporter": "Han JU", "assignee": "Nicholas Chammas", "labels": [], "created": "2014-05-12T11:38:10.000+0000", "updated": "2015-04-15T16:03:22.000+0000", "description": "In the current EC2 script, the AMI image object is loaded only once. This is OK when the master and slave machines are of the same virtualization type (pv or hvm). But this won't work if, say, the master is pv and the slaves are hvm since the AMI is not compatible across these two kinds of virtualization.", "comments": ["User 'nchammas' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/4455", "I've created a PR to catch this error early.\n\nWe can create a separate PR later to actually support launching clusters where the master and slaves have different virtualization types.", "Issue resolved by pull request 4455\n[https://github.com/apache/spark/pull/4455]"], "derived": {"summary": "In the current EC2 script, the AMI image object is loaded only once. This is OK when the master and slave machines are of the same virtualization type (pv or hvm).", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Error launching cluster when master and slave machines are of different virtualization types - In the current EC2 script, the AMI image object is loaded only once. This is OK when the master and slave machines are of the same virtualization type (pv or hvm)."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 4455\n[https://github.com/apache/spark/pull/4455]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1806", "title": "Upgrade to Mesos 0.18.1 with Shaded Protobuf", "status": "Resolved", "priority": "Major", "reporter": "Bernardo Gomez Palacio", "assignee": null, "labels": ["mesos"], "created": "2014-05-12T17:15:59.000+0000", "updated": "2015-12-09T03:01:34.000+0000", "description": "Upgrade Spark to depend on Mesos 0.18.1 with shaded protobuf. This version of Mesos does not externalize its dependency on the protobuf version (now shaded through the namespace org.apache.mesos.protobuf) and therefore facilitates integration with systems that do depend on specific versions of protobufs such as Hadoop 1.0.x, 2.x, etc.", "comments": ["Should close SPARK-1433", "PR submitted  https://github.com/apache/spark/pull/741", "Issue resolved by pull request 741\n[https://github.com/apache/spark/pull/741]", "Thanks [~pwendell] for addressing this so quickly!"], "derived": {"summary": "Upgrade Spark to depend on Mesos 0. 18.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Upgrade to Mesos 0.18.1 with Shaded Protobuf - Upgrade Spark to depend on Mesos 0. 18."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks [~pwendell] for addressing this so quickly!"}]}}
{"project": "SPARK", "issue_id": "SPARK-1807", "title": "Modify SPARK_EXECUTOR_URI to allow for script execution in Mesos.", "status": "Closed", "priority": "Major", "reporter": "Timothy St. Clair", "assignee": null, "labels": [], "created": "2014-05-12T18:55:40.000+0000", "updated": "2014-09-16T19:45:42.000+0000", "description": "Modify Mesos Scheduler integration to allow SPARK_EXECUTOR_URI to be an executable script.  This allows admins to launch spark in any fashion they desire, vs. just tarball fetching + implied context.   \n\n", "comments": ["I'll be working on some related things this week; could someone assign this to me?", "Hi William I think the best thing is to just paste a link to your GitHub pull request once you've got some code you want people to look at.\n\nThis project seems to use the Assignee value in Jira tasks more as \"who fixed it\" than \"who's working on it / who's responsible for it\"", "Question: Are you saying the {{SPARK_EXECUTOR_URI}} should be a URI that points to a script?  Or that we can replace the URI with a simple path to an executable script.  \n\nProposed Alternative: Take  this extension to its logical extreme, and actually deprecate the SPARK_EXECUTOR_URI parameter, and instead have a parameter {{SPARK_EXECUTOR_CLASS}}, which is invoked with any number of args, one of which could be SPARK_EXECUTOR_URI.  \n\nSomething like this: \n{noformat}\nval c = Class.forName(conf.get(\"SPARK_EXECUTOR_CLASS\"))\nval m = c.getMethod(\"startSpark\",classOf[SparkExecutor])\nm.invoke(null,conf.get(\"SPARK_EXECUTOR_URI\"))\n{noformat}\n\nThe advantage of using a class, rather than a shell script, for this - is that we are gauranteed off the bat that the class is providing a contract - to start spark - rather than just a one off script which could do any number of things.  \n\nAlso it means that various classes for this can be maintained and tested over time as first class \"spark environment\" providers.\n\n", "I'm saying that the URI should *only* have an implied context of a tarball if it is a tarball, otherwise if it's a script then it should be able to detect and execute. \n\nI think simple extension detection should work and be backwards compatible. ", "i disagree. SPARK_EXECUTOR_URI has well defined semantics, they shouldn't be extended or altered.\n\ni recommend closing this as won't fix.", "Please close in favor of SPARK-2691"], "derived": {"summary": "Modify Mesos Scheduler integration to allow SPARK_EXECUTOR_URI to be an executable script. This allows admins to launch spark in any fashion they desire, vs.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Modify SPARK_EXECUTOR_URI to allow for script execution in Mesos. - Modify Mesos Scheduler integration to allow SPARK_EXECUTOR_URI to be an executable script. This allows admins to launch spark in any fashion they desire, vs."}, {"q": "What updates or decisions were made in the discussion?", "a": "Please close in favor of SPARK-2691"}]}}
{"project": "SPARK", "issue_id": "SPARK-1808", "title": "bin/pyspark does not load default configuration properties", "status": "Resolved", "priority": "Major", "reporter": "Andrew Or", "assignee": "Andrew Or", "labels": [], "created": "2014-05-12T19:14:01.000+0000", "updated": "2014-11-05T10:45:27.000+0000", "description": "... because it doesn't go through spark-submit. Either we make it go through spark-submit (hard), or we extract the load default configurations logic and set them for the JVM that launches the py4j GatewayServer (easier).\n\nRight now, the only way to set config values for bin/pyspark is to do it through SPARK_JAVA_OPTS in spark-env.sh, which is supposedly deprecated.", "comments": ["https://github.com/apache/spark/pull/799", "Issue resolved by pull request 799\n[https://github.com/apache/spark/pull/799]"], "derived": {"summary": "because it doesn't go through spark-submit. Either we make it go through spark-submit (hard), or we extract the load default configurations logic and set them for the JVM that launches the py4j GatewayServer (easier).", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "bin/pyspark does not load default configuration properties - because it doesn't go through spark-submit. Either we make it go through spark-submit (hard), or we extract the load default configurations logic and set them for the JVM that launches the py4j GatewayServer (easier)."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 799\n[https://github.com/apache/spark/pull/799]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1809", "title": "Mesos backend doesn't respect HADOOP_CONF_DIR", "status": "Closed", "priority": "Major", "reporter": "Andrew Ash", "assignee": null, "labels": [], "created": "2014-05-12T20:56:44.000+0000", "updated": "2017-04-12T23:00:13.000+0000", "description": "In order to use HDFS paths without the server component, standalone mode reads spark-env.sh and scans the HADOOP_CONF_DIR to open core-site.xml and get the fs.default.name parameter.\n\nThis lets you use HDFS paths like:\n- hdfs:///tmp/myfile.txt\ninstead of\n- hdfs://myserver.mydomain.com:8020/tmp/myfile.txt\n\nHowever as of recent 1.0.0 pre-release (hash 756c96) I had to specify HDFS paths with the full server even though I have HADOOP_CONF_DIR still set in spark-env.sh.  The HDFS, Spark, and Mesos nodes are all co-located and non-domain HDFS paths work fine when using the standalone mode.", "comments": ["I'm not using Mesos anymore, so closing"], "derived": {"summary": "In order to use HDFS paths without the server component, standalone mode reads spark-env. sh and scans the HADOOP_CONF_DIR to open core-site.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Mesos backend doesn't respect HADOOP_CONF_DIR - In order to use HDFS paths without the server component, standalone mode reads spark-env. sh and scans the HADOOP_CONF_DIR to open core-site."}, {"q": "What updates or decisions were made in the discussion?", "a": "I'm not using Mesos anymore, so closing"}]}}
{"project": "SPARK", "issue_id": "SPARK-1810", "title": "The spark tar ball does not unzip into a separate folder when un-tarred.", "status": "Resolved", "priority": "Minor", "reporter": "Manikandan Narayanaswamy", "assignee": null, "labels": ["maven"], "created": "2014-05-12T20:59:44.000+0000", "updated": "2014-05-15T18:01:56.000+0000", "description": "All other Hadoop components when extracted are contained within a new folder that is created. But, this is not the case for Spark. The Spark.tar decompresses all files into the Current Working Directory.", "comments": ["I can't reproduce this... I'm pretty sure we have a top level folder when you untar it. Please re-open though if I'm misunderstanding.\n\n{code}\n$ wget http://people.apache.org/~pwendell/spark-1.0.0-rc7/spark-1.0.0.tgz\n$ tar xvzf spark-1.0.0.tgz \n$ ls\nspark-1.0.0  spark-1.0.0.tgz\n{code}"], "derived": {"summary": "All other Hadoop components when extracted are contained within a new folder that is created. But, this is not the case for Spark.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "The spark tar ball does not unzip into a separate folder when un-tarred. - All other Hadoop components when extracted are contained within a new folder that is created. But, this is not the case for Spark."}, {"q": "What updates or decisions were made in the discussion?", "a": "I can't reproduce this... I'm pretty sure we have a top level folder when you untar it. Please re-open though if I'm misunderstanding.\n\n{code}\n$ wget http://people.apache.org/~pwendell/spark-1.0.0-rc7/spark-1.0.0.tgz\n$ tar xvzf spark-1.0.0.tgz \n$ ls\nspark-1.0.0  spark-1.0.0.tgz\n{code}"}]}}
{"project": "SPARK", "issue_id": "SPARK-1811", "title": "Support resizable output buffer for kryo serializer", "status": "Resolved", "priority": "Minor", "reporter": "koert kuipers", "assignee": "Koert Kuipers", "labels": [], "created": "2014-05-12T21:45:41.000+0000", "updated": "2014-08-05T03:02:08.000+0000", "description": "Currently the size of kryo serializer output buffer can be set with spark.kryoserializer.buffer.mb\n\nThe issue with this setting is that it has to be one-size-fits-all, so it ends up being the maximum size needed, even if only a single task out of many needs it to be that big. A resizable buffer will allow most tasks to use a modest sized buffer while the incidental task that needs a really big buffer can get it at a cost (allocating a new buffer and copying the contents over repeatedly as the buffer grows... with each new allocation the size doubles).\n\nThe class used for the buffer is kryo Output, which supports resizing if  maxCapacity is set bigger than capacity. I suggest we provide a setting spark.kryoserializer.buffer.max.mb which defaults to spark.kryoserializer.buffer.mb, and which sets Output's maxCapacity.\n\nPull request for this jira:\nhttps://github.com/apache/spark/pull/735\n\n\n", "comments": ["Closed this as a duplicate of https://issues.apache.org/jira/browse/SPARK-2543."], "derived": {"summary": "Currently the size of kryo serializer output buffer can be set with spark. kryoserializer.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support resizable output buffer for kryo serializer - Currently the size of kryo serializer output buffer can be set with spark. kryoserializer."}, {"q": "What updates or decisions were made in the discussion?", "a": "Closed this as a duplicate of https://issues.apache.org/jira/browse/SPARK-2543."}]}}
{"project": "SPARK", "issue_id": "SPARK-1812", "title": "Support cross-building with Scala 2.11", "status": "Resolved", "priority": "Major", "reporter": "Matei Alexandru Zaharia", "assignee": "Prashant Sharma", "labels": [], "created": "2014-05-12T23:53:49.000+0000", "updated": "2015-01-17T14:26:54.000+0000", "description": "Since Scala 2.10/2.11 are source compatible, we should be able to cross build for both versions. From what I understand there are basically three things we need to figure out:\n\n1. Have a two versions of our dependency graph, one that uses 2.11 dependencies and the other that uses 2.10 dependencies.\n2. Figure out how to publish different poms for 2.10 and 2.11.\n\nI think (1) can be accomplished by having a scala 2.11 profile. (2) isn't really well supported by Maven since published pom's aren't generated dynamically. But we can probably script around it to make it work. I've done some initial sanity checks with a simple build here:\n\nhttps://github.com/pwendell/scala-maven-crossbuild", "comments": ["We will need to release kafka, akka-zeromq and twitter chill for scala 2.11", "FYI - I am working on Scala 2.11 support, ongoing work can be seen at https://github.com/avati/spark/commits/scala-2.11", "FWIW scalatest can be pushed to 2.2.0 without any pain, and 2.2 does add some nice new functionality.", "So the akka-2.3.x incompatibility turns out to be because of:\n\njava.lang.VerifyError: class akka.remote.WireFormats$AkkaControlMessage overrides final method getUnknownFields.()Lcom/google/protobuf/UnknownFieldSet;\n        at java.lang.ClassLoader.defineClass1(Native Method)\n        at java.lang.ClassLoader.defineClass(ClassLoader.java:800)\n        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n        at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)\n        at java.net.URLClassLoader.access$100(URLClassLoader.java:71)\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:361)\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)\n        at akka.remote.transport.AkkaPduProtobufCodec$.constructControlMessagePdu(AkkaPduCodec.scala:231)\n        at akka.remote.transport.AkkaPduProtobufCodec$.<init>(AkkaPduCodec.scala:153)\n        at akka.remote.transport.AkkaPduProtobufCodec$.<clinit>(AkkaPduCodec.scala)\n        at akka.remote.EndpointManager$$anonfun$9.apply(Remoting.scala:714)\n        at akka.remote.EndpointManager$$anonfun$9.apply(Remoting.scala:684)\n        at scala.collection.TraversableLike$WithFilter$$anonfun$map$2.apply(TraversableLike.scala:728)\n        at scala.collection.Iterator$class.foreach(Iterator.scala:743)\n        at scala.collection.AbstractIterator.foreach(Iterator.scala:1174)\n        at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n        at scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n        at scala.collection.TraversableLike$WithFilter.map(TraversableLike.scala:727)\n        at akka.remote.EndpointManager.akka$remote$EndpointManager$$listens(Remoting.scala:684)\n        at akka.remote.EndpointManager$$anonfun$receive$2.applyOrElse(Remoting.scala:492)\n        at akka.actor.Actor$class.aroundReceive(Actor.scala:465)\n        at akka.remote.EndpointManager.aroundReceive(Remoting.scala:395)\n        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)\n        at akka.actor.ActorCell.invoke(ActorCell.scala:487)\n        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)\n        at akka.dispatch.Mailbox.run(Mailbox.scala:220)\n        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)\n        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n\nI'm not yet familiar what the previous akka-2.2.3-shaded-protobuf was doing under the covers, but I'm guessing that using a vanilla (i.e non -shaded-protobuf) akka-2.3 is making someone not happy. Odd thing, vanilla akka-2.2.3 works fine, just akka-2.3.x seems to have a problem with non -shaded-protobuf.\n\nAny pointers will be helpful while I dig further.", "Figured out. Having two different $protobuf.version in pom.xml (2.4.1 and 2.5.0) and being used for different components seems to be the root cause for akka-2.3 related exceptions. Using 2.5.0 everywhere consistently is making akka-2.3 start working.", "User 'avati' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1649", "User 'avati' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1685", "User 'avati' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1701", "User 'avati' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1702", "User 'avati' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1703", "User 'avati' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1704", "User 'avati' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1708", "User 'avati' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1709", "User 'avati' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/1711", "[~avati] can you create sub-tasks for the individual dependency upgrades? Right now you have a bunch of different PR's that are all sharing the same JIRA title... our development workflow assumes there is a 1<->1 relationship between JIRA's and patches.", "[~pwendell], sure I will create sub-tasks. Was not aware of this workflow requirement, sorry.", "[~pwendell] I have created sub-tasks for the dependencies and build. All source code changes (warning and error fixes due to 2.11 language changes) will continue against this Jira.", "User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/2615", "+1 on this issue =)", "User 'adampingel' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/2947", "User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/3111", "The initial version of this patch has been merged. There are some follow-up's that I am going to make separate JIRA's for.", "Have you created a follow-up JIRA for actually releasing spark against 2.11 to Maven Central?", "[~schmmd]: It looks like [~pwendell] created these issues for putting 2.11 on maven:\nhttps://issues.apache.org/jira/browse/SPARK-4466 (closed)\nhttps://issues.apache.org/jira/browse/SPARK-4357 (open)", "Hem. Both issues are now closed. Pinging [~pwendell]."], "derived": {"summary": "Since Scala 2. 10/2.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Support cross-building with Scala 2.11 - Since Scala 2. 10/2."}, {"q": "What updates or decisions were made in the discussion?", "a": "Hem. Both issues are now closed. Pinging [~pwendell]."}]}}
{"project": "SPARK", "issue_id": "SPARK-1813", "title": "Add a utility to SparkConf that makes using Kryo really easy", "status": "Resolved", "priority": "Major", "reporter": "Sandy Ryza", "assignee": "Sandy Ryza", "labels": [], "created": "2014-05-13T00:19:27.000+0000", "updated": "2014-11-01T15:07:16.000+0000", "description": "It would be nice to have a method in SparkConf that makes it really easy to turn on Kryo serialization and register a set of classes.\n\nUsing Kryo currently requires all this:\n{code}\nimport com.esotericsoftware.kryo.Kryo\nimport org.apache.spark.serializer.KryoRegistrator\n\nclass MyRegistrator extends KryoRegistrator {\n  override def registerClasses(kryo: Kryo) {\n    kryo.register(classOf[MyClass1])\n    kryo.register(classOf[MyClass2])\n  }\n}\n\nval conf = new SparkConf().setMaster(...).setAppName(...)\nconf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\nconf.set(\"spark.kryo.registrator\", \"mypackage.MyRegistrator\")\nval sc = new SparkContext(conf)\n{code}\n\nIt would be nice if it just required this:\n{code}\nSparkConf.setKryo(Array(classOf[MyClass1], classOf[MyClass2]))\n{code}", "comments": ["Writing a KryoRegistrator is the only requirement - rest are done as part of initialization anyway.\nRegistering classes with kryo is non trivial except for degenerate cases : for example, we have classes we have to use java read/write Object serialization, which support kyro serialization, which support java's external serialization, generated classes, etc.\nAnd we would need a registrator ... ofcourse, it could be argued this is corner case, though I dont think so.", "https://github.com/apache/spark/pull/789 is what I mean. [~mridulm@yahoo-inc.com], are there common uses of Kryo you're saying this misses?", "Fixed in https://github.com/apache/spark/pull/789"], "derived": {"summary": "It would be nice to have a method in SparkConf that makes it really easy to turn on Kryo serialization and register a set of classes. Using Kryo currently requires all this:\n{code}\nimport com.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add a utility to SparkConf that makes using Kryo really easy - It would be nice to have a method in SparkConf that makes it really easy to turn on Kryo serialization and register a set of classes. Using Kryo currently requires all this:\n{code}\nimport com."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed in https://github.com/apache/spark/pull/789"}]}}
{"project": "SPARK", "issue_id": "SPARK-1814", "title": "Splash page should include correct syntax for launching examples", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Andrew Or", "labels": [], "created": "2014-05-13T01:10:21.000+0000", "updated": "2014-11-05T10:45:21.000+0000", "description": null, "comments": [], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Splash page should include correct syntax for launching examples"}]}}
{"project": "SPARK", "issue_id": "SPARK-1815", "title": "SparkContext shouldn't be marked DeveloperApi", "status": "Resolved", "priority": "Major", "reporter": "Sandy Ryza", "assignee": null, "labels": [], "created": "2014-05-13T01:21:26.000+0000", "updated": "2014-05-13T03:08:53.000+0000", "description": null, "comments": ["Issue resolved by pull request 753\n[https://github.com/apache/spark/pull/753]"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "SparkContext shouldn't be marked DeveloperApi"}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 753\n[https://github.com/apache/spark/pull/753]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1816", "title": "LiveListenerBus dies if a listener throws an exception", "status": "Resolved", "priority": "Critical", "reporter": "Aaron Davidson", "assignee": "Andrew Or", "labels": [], "created": "2014-05-13T01:36:44.000+0000", "updated": "2014-12-22T22:11:29.000+0000", "description": "The exception isn't even printed.", "comments": ["https://github.com/apache/spark/pull/759", "Issue resolved by pull request 759\n[https://github.com/apache/spark/pull/759]"], "derived": {"summary": "The exception isn't even printed.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "LiveListenerBus dies if a listener throws an exception - The exception isn't even printed."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 759\n[https://github.com/apache/spark/pull/759]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1817", "title": "RDD zip erroneous when partitions do not divide RDD count", "status": "Resolved", "priority": "Major", "reporter": "Michael Malak", "assignee": "Kan Zhang", "labels": [], "created": "2014-05-13T05:26:21.000+0000", "updated": "2014-06-04T16:16:24.000+0000", "description": "Example:\n\nscala> sc.parallelize(1L to 2L,4).zip(sc.parallelize(11 to 12,4)).collect\nres1: Array[(Long, Int)] = Array((2,11))\n\nBut more generally, it's whenever the number of partitions does not evenly divide the total number of elements in the RDD.\n\nSee https://groups.google.com/forum/#!msg/spark-users/demrmjHFnoc/Ek3ijiXHr2MJ\n", "comments": ["I opened SPARK-1837 as a specific fix for the error reported in the description.", "There are 2 issues related to this bug. One is that we partition numeric ranges (e.g., Long and Double ranges) differently from other types of sequences (i.e, at different indexes). This causes elements to be dropped when zipping with numeric ranges since we zip by partition and partitions for numeric ranges may have different sizes from other sequences (even if the total length and the number of partitions are the same). This is fixed in SPARK-1837. One caveat is currently partitioning Double ranges still doesn't work properly due to a Scala bug that breaks {{take}} and {{drop}} on Double ranges (https://issues.scala-lang.org/browse/SI-8518).\n\nThe other issue is instead of dropping elements silently, we should throw an error during zipping when we found out that partition sizes are not the same between 2 sequences. This is fixed by https://github.com/apache/spark/pull/944"], "derived": {"summary": "Example:\n\nscala> sc. parallelize(1L to 2L,4).", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "RDD zip erroneous when partitions do not divide RDD count - Example:\n\nscala> sc. parallelize(1L to 2L,4)."}, {"q": "What updates or decisions were made in the discussion?", "a": "There are 2 issues related to this bug. One is that we partition numeric ranges (e.g., Long and Double ranges) differently from other types of sequences (i.e, at different indexes). This causes elements to be dropped when zipping with numeric ranges since we zip by partition and partitions for numeric ranges may have different sizes from other sequences (even if the total length and the number of partitions are the same). This is fixed in SPARK-1837. One caveat is currently partitioning Double ranges still doesn't work properly due to a Scala bug that breaks {{take}} and {{drop}} on Double ranges (https://issues.scala-lang.org/browse/SI-8518).\n\nThe other issue is instead of dropping elements silently, we should throw an error during zipping when we found out that partition sizes are not the same between 2 sequences. This is fixed by https://github.com/apache/spark/pull/944"}]}}
{"project": "SPARK", "issue_id": "SPARK-1818", "title": "Freshen Mesos docs", "status": "Resolved", "priority": "Major", "reporter": "Andrew Ash", "assignee": null, "labels": [], "created": "2014-05-13T06:26:25.000+0000", "updated": "2014-05-14T16:46:46.000+0000", "description": "They haven't been updated since 0.6.0 and encourage compiling both Mesos and Spark from scratch.  Include mention of the precompiled binary versions of both projects available and otherwise generally freshen the documentation for Mesos newcomers.", "comments": ["https://github.com/apache/spark/pull/756", "Issue resolved by pull request 756\n[https://github.com/apache/spark/pull/756]"], "derived": {"summary": "They haven't been updated since 0. 6.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Freshen Mesos docs - They haven't been updated since 0. 6."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 756\n[https://github.com/apache/spark/pull/756]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1819", "title": "Fix GetField.nullable.", "status": "Resolved", "priority": "Major", "reporter": "Takuya Ueshin", "assignee": "Takuya Ueshin", "labels": [], "created": "2014-05-13T08:51:07.000+0000", "updated": "2014-05-15T18:50:59.000+0000", "description": "{{GetField.nullable}} should be {{true}} not only when {{field.nullable}} is {{true}} but also when {{child.nullable}} is {{true}}.", "comments": ["Pull-requested: https://github.com/apache/spark/pull/757"], "derived": {"summary": "{{GetField. nullable}} should be {{true}} not only when {{field.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Fix GetField.nullable. - {{GetField. nullable}} should be {{true}} not only when {{field."}, {"q": "What updates or decisions were made in the discussion?", "a": "Pull-requested: https://github.com/apache/spark/pull/757"}]}}
{"project": "SPARK", "issue_id": "SPARK-1820", "title": "Make GenerateMimaIgnore @DeveloperApi annotation aware.", "status": "Resolved", "priority": "Major", "reporter": "Prashant Sharma", "assignee": "Prashant Sharma", "labels": [], "created": "2014-05-13T09:32:17.000+0000", "updated": "2015-12-10T15:05:55.000+0000", "description": "Ignore all the classes with DeveloperApi annotation, while doing Mima checks. ", "comments": ["Issue resolved by pull request 904\n[https://github.com/apache/spark/pull/904]", "User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/904"], "derived": {"summary": "Ignore all the classes with DeveloperApi annotation, while doing Mima checks.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Make GenerateMimaIgnore @DeveloperApi annotation aware. - Ignore all the classes with DeveloperApi annotation, while doing Mima checks."}, {"q": "What updates or decisions were made in the discussion?", "a": "User 'ScrapCodes' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/904"}]}}
{"project": "SPARK", "issue_id": "SPARK-1821", "title": "Document History Server", "status": "Closed", "priority": "Major", "reporter": "Nan Zhu", "assignee": null, "labels": [], "created": "2014-05-13T12:59:07.000+0000", "updated": "2014-05-13T18:37:13.000+0000", "description": "In 1.0, there is a new component, history server, which is not mentioned in http://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/\n\nI think we'd better add the missing document", "comments": ["sorry, missed some documents", "Yes, it should be documented under \"monitoring.html\" in the latest branch"], "derived": {"summary": "In 1. 0, there is a new component, history server, which is not mentioned in http://people.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "Document History Server - In 1. 0, there is a new component, history server, which is not mentioned in http://people."}, {"q": "What updates or decisions were made in the discussion?", "a": "Yes, it should be documented under \"monitoring.html\" in the latest branch"}]}}
{"project": "SPARK", "issue_id": "SPARK-1822", "title": "SchemaRDD.count() should use the optimizer.", "status": "Resolved", "priority": "Major", "reporter": "Michael Armbrust", "assignee": "Kan Zhang", "labels": [], "created": "2014-05-13T21:37:10.000+0000", "updated": "2014-05-25T07:07:21.000+0000", "description": null, "comments": ["PR: https://github.com/apache/spark/pull/841"], "derived": {"summary": "", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "SchemaRDD.count() should use the optimizer."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR: https://github.com/apache/spark/pull/841"}]}}
{"project": "SPARK", "issue_id": "SPARK-1823", "title": "ExternalAppendOnlyMap can still OOM if one key is very large", "status": "Resolved", "priority": "Major", "reporter": "Andrew Or", "assignee": null, "labels": ["bulk-closed"], "created": "2014-05-14T01:01:43.000+0000", "updated": "2019-05-21T05:37:09.000+0000", "description": "If the values for one key do not collectively fit into memory, then the map will still OOM when you merge the spilled contents back in.\n\nThis is a problem especially for PySpark, since we hash the keys (Python objects) before a shuffle, and there are only so many integers out there in the world, so there could potentially be many collisions.", "comments": ["// This was not fixed in Spark 1.1 and should be bumped to Spark 1.2\n\n[~pwendell] about this issue on dev@ Aug 25th:\n{quote}\nWe might create a new JIRA for it, but it doesn't exist yet. We'll create JIRA's for the major 1.2 issues at the beginning of September.\n{quote}", "Thanks Andrew, I have updated the versions.", "SPARK-3074 is a related issue for large group-bys PySpark.", "Hey [~srowen] this issue belongs more to the Core component rather than in PySpark. I believe SPARK-3074 is the mirror issue in python. I'm going to change it back.", "NP I might misclassify a few of these with the bulk changes, so please change anything you think is off.", "[~andrewor14], is anyone working on this actively? SPARK-3074 seems to have a reasonable fix for pyspark. Would this be simply a matter of writing a similar patch for Scala code?", "What is the status of this issue? Is there any other ticket for this?", "Hey, what is the status on this issues? is there another ticket for it?"], "derived": {"summary": "If the values for one key do not collectively fit into memory, then the map will still OOM when you merge the spilled contents back in. This is a problem especially for PySpark, since we hash the keys (Python objects) before a shuffle, and there are only so many integers out there in the world, so there could potentially be many collisions.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "ExternalAppendOnlyMap can still OOM if one key is very large - If the values for one key do not collectively fit into memory, then the map will still OOM when you merge the spilled contents back in. This is a problem especially for PySpark, since we hash the keys (Python objects) before a shuffle, and there are only so many integers out there in the world, so there could potentially be many collisions."}, {"q": "What updates or decisions were made in the discussion?", "a": "Hey, what is the status on this issues? is there another ticket for it?"}]}}
{"project": "SPARK", "issue_id": "SPARK-1824", "title": "Python examples still take in <master>", "status": "Resolved", "priority": "Major", "reporter": "Andrew Or", "assignee": "Andrew Or", "labels": [], "created": "2014-05-14T01:40:44.000+0000", "updated": "2014-11-05T10:45:25.000+0000", "description": "A recent commit https://github.com/apache/spark/commit/44dd57fb66bb676d753ad8d9757f9f4c03364113 changed existing Spark examples in Scala and Java such that they no longer take in <master> as an argument. We forgot to do the same for Python.", "comments": ["Issue resolved by pull request 802\n[https://github.com/apache/spark/pull/802]"], "derived": {"summary": "A recent commit https://github. com/apache/spark/commit/44dd57fb66bb676d753ad8d9757f9f4c03364113 changed existing Spark examples in Scala and Java such that they no longer take in <master> as an argument.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Python examples still take in <master> - A recent commit https://github. com/apache/spark/commit/44dd57fb66bb676d753ad8d9757f9f4c03364113 changed existing Spark examples in Scala and Java such that they no longer take in <master> as an argument."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 802\n[https://github.com/apache/spark/pull/802]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1825", "title": "Windows Spark fails to work with Linux YARN", "status": "Closed", "priority": "Major", "reporter": "Taeyun Kim", "assignee": "Masayoshi Tsuzuki", "labels": [], "created": "2014-05-14T02:38:06.000+0000", "updated": "2015-02-02T02:30:06.000+0000", "description": "Windows Spark fails to work with Linux YARN.\nThis is a cross-platform problem.\n\nThis error occurs when 'yarn-client' mode is used.\n(yarn-cluster/yarn-standalone mode was not tested.)\n\nOn YARN side, Hadoop 2.4.0 resolved the issue as follows:\nhttps://issues.apache.org/jira/browse/YARN-1824\n\nBut Spark YARN module does not incorporate the new YARN API yet, so problem persists for Spark.\n\nFirst, the following source files should be changed:\n- /yarn/common/src/main/scala/org/apache/spark/deploy/yarn/ClientBase.scala\n- /yarn/common/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnableUtil.scala\n\nChange is as follows:\n- Replace .$() to .$$()\n- Replace File.pathSeparator for Environment.CLASSPATH.name to ApplicationConstants.CLASS_PATH_SEPARATOR (import org.apache.hadoop.yarn.api.ApplicationConstants is required for this)\n\nUnless the above are applied, launch_container.sh will contain invalid shell script statements(since they will contain Windows-specific separators), and job will fail.\nAlso, the following symptom should also be fixed (I could not find the relevant source code):\n- SPARK_HOME environment variable is copied straight to launch_container.sh. It should be changed to the path format for the server OS, or, the better, a separate environment variable or a configuration variable should be created.\n- '%HADOOP_MAPRED_HOME%' string still exists in launch_container.sh, after the above change is applied. maybe I missed a few lines.\n\nI'm not sure whether this is all, since I'm new to both Spark and YARN.", "comments": ["Is it really necessary to change the file \"ExecutorRunnableUtil.scala\"?\n\nI'd just changed the file \"ClientBase.scala\" and it (apparently) works for Spark 1.1.\n\nIn order to make it work, you'll have to add the following configuration\n\n\t- Program arguments: --master yarn-cluster\n\t- VM arguments: -Dspark.app-submission.cross-platform=true\n\t\n", "I've had the following problems to make Windows+Pyspark+YARN work properly:\n\n1. net.ScriptBasedMapping: Exception running /etc/hadoop/conf.cloudera.yarn/topology.py\n\nFIX? Comment the \"net.topology.script.file.name\" property configuration in the file core-site.xml.\n\n2. Error from python worker:\n  /usr/bin/python: No module named pyspark\nPYTHONPATH was:\n  /yarn/nm/usercache/bigdata/filecache/63/spark-assembly-1.1.0-hadoop2.3.0-cdh5.0.1.jar\n\nFIX? Add the environment variable SPARK_YARN_USER_ENV to my client (Eclipse) launch configuration. Assign this value to the env var:\n\nPYTHONPATH=/opt/cloudera/parcels/CDH/lib/spark/python:/opt/cloudera/parcels/CDH/lib/spark/python/lib/py4j-0.8.2.1-src.zip\t\n\n\nis there any way to do it simpler? am I doing something wrong?", "User 'tsudukim' has created a pull request for this issue:\nhttps://github.com/apache/spark/pull/3943", "It is necessary to use $$() to solve this problem but as discussed on PR #899 if we use $$() build for hadoop<2.4 will fail.\nSo PR #3943 uses reflection to avoid build failure for every version of hadoop.\nWindows clilents works fine with Linux YARN cluetr only when we use hadoop 2.4+. But it doesn't work under hadoop<2.4 even after this patch.\n"], "derived": {"summary": "Windows Spark fails to work with Linux YARN. This is a cross-platform problem.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Windows Spark fails to work with Linux YARN - Windows Spark fails to work with Linux YARN. This is a cross-platform problem."}, {"q": "What updates or decisions were made in the discussion?", "a": "It is necessary to use $$() to solve this problem but as discussed on PR #899 if we use $$() build for hadoop<2.4 will fail.\nSo PR #3943 uses reflection to avoid build failure for every version of hadoop.\nWindows clilents works fine with Linux YARN cluetr only when we use hadoop 2.4+. But it doesn't work under hadoop<2.4 even after this patch."}]}}
{"project": "SPARK", "issue_id": "SPARK-1826", "title": "Some bad head notations in sparksql ", "status": "Resolved", "priority": "Major", "reporter": "Fei Wang", "assignee": null, "labels": [], "created": "2014-05-14T03:03:32.000+0000", "updated": "2014-05-15T18:14:14.000+0000", "description": "There are some obvious bad notations such as sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala.  ", "comments": ["Fixed in: https://github.com/apache/spark/pull/765"], "derived": {"summary": "There are some obvious bad notations such as sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package. scala.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Some bad head notations in sparksql  - There are some obvious bad notations such as sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package. scala."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed in: https://github.com/apache/spark/pull/765"}]}}
{"project": "SPARK", "issue_id": "SPARK-1827", "title": "LICENSE and NOTICE files need a refresh to contain transitive dependency info", "status": "Resolved", "priority": "Blocker", "reporter": "Sean R. Owen", "assignee": "Sean R. Owen", "labels": [], "created": "2014-05-14T07:07:16.000+0000", "updated": "2015-01-15T09:08:39.000+0000", "description": "(Pardon marking it a blocker, but think it needs doing before 1.0 per chat with [~pwendell])\n\nThe LICENSE and NOTICE files need to cover all transitive dependencies, since these are all distributed in the assembly jar. (c.f. http://www.apache.org/dev/licensing-howto.html )\n\nI don't believe the current files cover everything. It's possible to mostly-automatically generate these. I will generate this and propose a patch to both today.", "comments": ["LICENSE and NOTICE policy is explained here:\n\nhttp://www.apache.org/dev/licensing-howto.html\nhttp://www.apache.org/legal/3party.html\n\nThis leads to the following changes.\n\nFirst, this change enables two extensions to maven-shade-plugin in assembly/ that will try to include and merge all NOTICE and LICENSE files. This can't hurt.\n\nThis generates a consolidated NOTICE file that I manually added to NOTICE.\n\nNext, a list of all dependencies and their licenses was generated:\n\nmvn ... license:aggregate-add-third-party\n\nto create: target/generated-sources/license/THIRD-PARTY.txt\n\nEach dependency is listed with one or more licenses. Determine the most-compatible license for each if there is more than one.\n\nFor \"unknown\" license dependencies, I manually evaluateD their license. Many are actually Apache projects or components of projects covered already. The only non-trivial one was Colt, which has its own (compatible) license.\n\nI ignored Apache-licensed and public domain dependencies as these require no further action (beyond NOTICE above).\n\nBSD and MIT licenses (permissive Category A licenses) are evidently supposed to be mentioned in LICENSE, so I added a section without output from the THIRD-PARTY.txt file appropriately.\n\nEverything else, Category B licenses, are evidently mentioned in NOTICE (?) Same there.\n\nLICENSE contained some license statements for source code that is redistributed. I left this as I think that is the right place to put it.\n", "Fixed by:\nhttps://github.com/apache/spark/pull/770"], "derived": {"summary": "(Pardon marking it a blocker, but think it needs doing before 1. 0 per chat with [~pwendell])\n\nThe LICENSE and NOTICE files need to cover all transitive dependencies, since these are all distributed in the assembly jar.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "LICENSE and NOTICE files need a refresh to contain transitive dependency info - (Pardon marking it a blocker, but think it needs doing before 1. 0 per chat with [~pwendell])\n\nThe LICENSE and NOTICE files need to cover all transitive dependencies, since these are all distributed in the assembly jar."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed by:\nhttps://github.com/apache/spark/pull/770"}]}}
{"project": "SPARK", "issue_id": "SPARK-1828", "title": "Created forked version of hive-exec that doesn't bundle other dependencies", "status": "Resolved", "priority": "Blocker", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "labels": [], "created": "2014-05-14T07:52:57.000+0000", "updated": "2014-08-15T17:20:23.000+0000", "description": "The hive-exec jar includes a bunch of Hive's dependencies in addition to hive itself (protobuf, guava, etc). See HIVE-5733. This breaks any attempt in Spark to manage those dependencies.\n\nThe only solution to this problem is to publish our own version of hive-exec 0.12.0 that behaves correctly. While we are doing this, we might as well re-write the protobuf dependency to use the shaded version of protobuf 2.4.1 that we already have for Akka.", "comments": ["Issue resolved by pull request 767\n[https://github.com/apache/spark/pull/767]", "Because of this change any incompatibilities with Hive in Hadoop distros are hidden untile you run job on an actual cluster. Unless you are willing to keep your fork up to date with every major Hadoop distro of course. \n\nRight now we see incompatibility with CDH5.0.2 Hive, but I'd rather have it failing to compile rather that seeing problems at runtime", "Maxim - I think what you are pointing out is unrated to this exact issue. Spark hard-codes a specific version of Hive in our build. This is true whether or not we are pointing to a slightly modified version of Hive 0.12 or the actual Hive 0.12.\n\nThe issue is that Hive does not have stable API's so we can't provide a version of Spark that is cross-compatible with different versions of Hive. We are trying to simplify our dependency on Hive to fix this.\n\nAre you proposing a specific change here?", "I don't have a pull request at hand if you are askin that ;) But IMHO proper solution is to tinker with maven shade plugin, to drop classes pulled by hive dependency in favor of those specified in Spark POM. \n\nIf it is done that way, then it would be possible to specify hive version using \"-D\" param in the same way we can specify hadoop version and be sure (to some extent of course :) ) that if it builds,it works."], "derived": {"summary": "The hive-exec jar includes a bunch of Hive's dependencies in addition to hive itself (protobuf, guava, etc). See HIVE-5733.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Created forked version of hive-exec that doesn't bundle other dependencies - The hive-exec jar includes a bunch of Hive's dependencies in addition to hive itself (protobuf, guava, etc). See HIVE-5733."}, {"q": "What updates or decisions were made in the discussion?", "a": "I don't have a pull request at hand if you are askin that ;) But IMHO proper solution is to tinker with maven shade plugin, to drop classes pulled by hive dependency in favor of those specified in Spark POM. \n\nIf it is done that way, then it would be possible to specify hive version using \"-D\" param in the same way we can specify hadoop version and be sure (to some extent of course :) ) that if it builds,it works."}]}}
{"project": "SPARK", "issue_id": "SPARK-1829", "title": "Sub-second durations shouldn't round to \"0 s\"", "status": "Resolved", "priority": "Minor", "reporter": "Andrew Ash", "assignee": null, "labels": [], "created": "2014-05-14T08:13:50.000+0000", "updated": "2014-05-14T19:04:43.000+0000", "description": "Compare the first image to the second here:\n\nhttp://imgur.com/RaLEsSZ,7VTlgfo#0", "comments": ["Didn't make it into v1.0.0-rc6 but is on branch-1.0 so a future rc would likely have it, or 1.0.1 and 1.1"], "derived": {"summary": "Compare the first image to the second here:\n\nhttp://imgur. com/RaLEsSZ,7VTlgfo#0.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Sub-second durations shouldn't round to \"0 s\" - Compare the first image to the second here:\n\nhttp://imgur. com/RaLEsSZ,7VTlgfo#0."}, {"q": "What updates or decisions were made in the discussion?", "a": "Didn't make it into v1.0.0-rc6 but is on branch-1.0 so a future rc would likely have it, or 1.0.1 and 1.1"}]}}
{"project": "SPARK", "issue_id": "SPARK-1830", "title": "Deploy failover, Make Persistence engine and LeaderAgent Pluggable.", "status": "Resolved", "priority": "Major", "reporter": "Prashant Sharma", "assignee": "Prashant Sharma", "labels": [], "created": "2014-05-14T12:06:46.000+0000", "updated": "2015-04-30T06:03:20.000+0000", "description": "With current code base it is difficult to plugin an external user specified \"Persistence Engine\" or \"Election Agent\". It would be good to expose this as a pluggable API.", "comments": ["PR at https://github.com/apache/spark/pull/771", "I'm trying to implement a custom persistence engine and a leader agent in the Java environment. \n\nvis-a-vis scala, when I implement the PersistenceEngine trait in java, I would have to implement methods such as readPersistedData, removeDriver, etc together with read, persist and unpersist methods. \nbut the issue here is, methods such as readPersistedData etc are 'final def's, hence can not be overridden in the java environment. \n\nI am new to scala, but is there any workaround to implement the above traits in java? \n\nlook forward to hear from you. "], "derived": {"summary": "With current code base it is difficult to plugin an external user specified \"Persistence Engine\" or \"Election Agent\". It would be good to expose this as a pluggable API.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Deploy failover, Make Persistence engine and LeaderAgent Pluggable. - With current code base it is difficult to plugin an external user specified \"Persistence Engine\" or \"Election Agent\". It would be good to expose this as a pluggable API."}, {"q": "What updates or decisions were made in the discussion?", "a": "I'm trying to implement a custom persistence engine and a leader agent in the Java environment. \n\nvis-a-vis scala, when I implement the PersistenceEngine trait in java, I would have to implement methods such as readPersistedData, removeDriver, etc together with read, persist and unpersist methods. \nbut the issue here is, methods such as readPersistedData etc are 'final def's, hence can not be overridden in the java environment. \n\nI am new to scala, but is there any workaround to implement the above traits in java? \n\nlook forward to hear from you."}]}}
{"project": "SPARK", "issue_id": "SPARK-1831", "title": "add the security guide to the \"More\" drop down menu", "status": "Resolved", "priority": "Major", "reporter": "Thomas Graves", "assignee": null, "labels": [], "created": "2014-05-14T13:36:49.000+0000", "updated": "2014-07-21T14:01:25.000+0000", "description": "The security guide is linked from the \"Overview\" page but it looks like it isn't in the drop down \"More\" menu at the top of the page.  We should add it there so its easy to find.", "comments": ["looks like this is already got fixed."], "derived": {"summary": "The security guide is linked from the \"Overview\" page but it looks like it isn't in the drop down \"More\" menu at the top of the page. We should add it there so its easy to find.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "add the security guide to the \"More\" drop down menu - The security guide is linked from the \"Overview\" page but it looks like it isn't in the drop down \"More\" menu at the top of the page. We should add it there so its easy to find."}, {"q": "What updates or decisions were made in the discussion?", "a": "looks like this is already got fixed."}]}}
{"project": "SPARK", "issue_id": "SPARK-1832", "title": "Executor UI improvement suggestions", "status": "Resolved", "priority": "Major", "reporter": "Thomas Graves", "assignee": "Alex Bozarth", "labels": [], "created": "2014-05-14T14:32:39.000+0000", "updated": "2016-01-25T21:12:38.000+0000", "description": "I received some suggestions from a user for the /executors UI page to make it more helpful. This gets more important when you have a really large number of executors.\n\n     Fill some of the cells with color in order to make it easier to absorb the info, e.g.\n\nRED if Failed Tasks greater than 0 (maybe the more failed, the more intense the red)\nGREEN if Active Tasks greater than 0 (maybe more intense the larger the number)\n\nPossibly color code COMPLETE TASKS using various shades of blue (e.g., based on the log(# completed)\n- if dark blue then write the value in white (same for the RED and GREEN above\n\nMaybe mark the MASTER task somehow\n \n    Report the TOTALS in each column (do this at the TOP so no need to scroll to the bottom, or print both at top and bottom).\n", "comments": ["Merging another idea from SPARK-2132: \nColor GC time red when over a percentage of task time", "I've split off a subtask for the Color improvements that I will be submitting a PR for shortly.\n\nAlso what do you mean by \"the MASTER task\" in the description?\n\nI will also continue to work on the column totals task", "Updated my totals row solution to be independent of the colors sub-task, will submit PR after re-testing", "Opened a sub-task for the totals UI half. Unless there is a clarification on \"the MASTER task\" line in the description this should be resolved when the sub-tasks are resolved", "Unless there's more info on what \"MASTER task\" means, this is complete since it's sub-tasks are complete [~tgraves]", "I think they meant the driver.  I'll mark this as done."], "derived": {"summary": "I received some suggestions from a user for the /executors UI page to make it more helpful. This gets more important when you have a really large number of executors.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Executor UI improvement suggestions - I received some suggestions from a user for the /executors UI page to make it more helpful. This gets more important when you have a really large number of executors."}, {"q": "What updates or decisions were made in the discussion?", "a": "I think they meant the driver.  I'll mark this as done."}]}}
{"project": "SPARK", "issue_id": "SPARK-1833", "title": "Have an empty SparkContext constructor instead of relying on new SparkContext(new SparkConf())", "status": "Resolved", "priority": "Major", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "labels": [], "created": "2014-05-14T19:04:28.000+0000", "updated": "2014-05-14T19:53:53.000+0000", "description": null, "comments": ["Issue resolved by pull request 774\n[https://github.com/apache/spark/pull/774]"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Have an empty SparkContext constructor instead of relying on new SparkContext(new SparkConf())"}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 774\n[https://github.com/apache/spark/pull/774]"}]}}
{"project": "SPARK", "issue_id": "SPARK-1834", "title": "NoSuchMethodError when invoking JavaPairRDD.reduce() in Java", "status": "Resolved", "priority": "Major", "reporter": "John Snodgrass", "assignee": null, "labels": [], "created": "2014-05-14T19:41:09.000+0000", "updated": "2015-03-17T16:27:04.000+0000", "description": "I get a java.lang.NoSuchMethod error when invoking JavaPairRDD.reduce(). Here is the partial stack trace:\n\nException in thread \"main\" java.lang.reflect.InvocationTargetException\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:601)\n        at org.apache.spark.deploy.worker.DriverWrapper$.main(DriverWrapper.scala:39)\n        at org.apache.spark.deploy.worker.DriverWrapper.main(DriverWrapper.scala)\nCaused by: java.lang.NoSuchMethodError: org.apache.spark.api.java.JavaPairRDD.reduce(Lorg/apache/spark/api/java/function/Function2;)Lscala/Tuple2;\n        at JavaPairRDDReduceTest.main(JavaPairRDDReduceTest.java:49)    ...\n\nI'm using Spark 0.9.1. I checked to ensure that I'm compiling with the same version of Spark as I am running on the cluster. The reduce() method works fine with JavaRDD, just not with JavaPairRDD. Here is a code snippet that exhibits the problem: \n\n      ArrayList<Integer> array = new ArrayList<>();\n      for (int i = 0; i < 10; ++i) {\n        array.add(i);\n      }\n      JavaRDD<Integer> rdd = javaSparkContext.parallelize(array);\n\n      JavaPairRDD<String, Integer> testRDD = rdd.map(new PairFunction<Integer, String, Integer>() {\n        @Override\n        public Tuple2<String, Integer> call(Integer t) throws Exception {\n          return new Tuple2<>(\"\" + t, t);\n        }\n      }).cache();\n      \n      testRDD.reduce(new Function2<Tuple2<String, Integer>, Tuple2<String, Integer>, Tuple2<String, Integer>>() {\n        @Override\n        public Tuple2<String, Integer> call(Tuple2<String, Integer> arg0, Tuple2<String, Integer> arg1) throws Exception { \n          return new Tuple2<>(arg0._1 + arg1._1, arg0._2 * 10 + arg0._2);\n        }\n      });\n\n", "comments": ["There is no reduce function in JavaPairRDD", "[~franklynDsouza] There is a reduce method available in JavaPairRDD, but it is defined in the superclass JavaRDDLike.\n\n[~jws] This method does exist. When you encounter NoSuchMethodError it's a sign that the compile and runtime versions don't match, but here this method should have been present for a long time in Spark. I don't know if this matters, but I see you use map to generate a JavaPairRDD when I thought you had to use mapToPair to get it using a PairFunction.", "[~srowen] In version 0.9.1, the map method was overloaded:\n\nhttps://spark.apache.org/docs/0.9.1/api/core/index.html#org.apache.spark.api.java.JavaRDD\n\nIt was broken out into several different methods, including mapToPair, in a later version of Spark. I think it was 1.0.0. I'm running 0.9.1.", "Ah, you're right: https://github.com/apache/spark/commit/181ec5030792a10f3ce77e997d0e2eda9bcd6139\nIt was unlikely to be the problem anyway. Very strange.", "Same issue here with Spark 1.1.0: reduce() is not implemented on JavaPairRDD.\n\n{code}\nTuple2<String, Long> r = sc.textFile(filename)\n    .mapToPair(s -> new Tuple2<String, Long>(s[3], 1L))\n    .reduceByKey((x, y) -> x + y)\n    .reduce((t1, t2) -> t1._2 > t2._2 ? t1 : t2);\n{code}\n\nProduces:\n\n{code}\nException in thread \"main\" java.lang.NoSuchMethodError: org.apache.spark.api.java.JavaPairRDD.reduce(Lorg/apache/spark/api/java/function/Function2;)Lscala/Tuple2;\n\tat fr.ippon.dojo.spark.AnalyseParisTrees.main(AnalyseParisTrees.java:33)\n{code}\n\nHowever, reduce() is implemented on JavaRDD. I have had to add an intermediate map() operation:\n\n{code}\nTuple2<String, Long> r = sc.textFile(filename)\n    .mapToPair(s -> new Tuple2<String, Long>(s[3], 1L))\n    .reduceByKey((x, y) -> x + y)\n    .map(t -> t)\n    .reduce((t1, t2) -> t1._2 > t2._2 ? t1 : t2);\n{code}", "Weird, I can reproduce this. It compiles fine but fails at runtime. Another example, that doesn't even use lambdas:\n\n{code}\n  @Test\n  public void pairReduce() {\n    JavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(1, 1, 2, 3, 5, 8, 13));\n    JavaPairRDD<Integer,Integer> pairRDD = rdd.mapToPair(\n        new PairFunction<Integer, Integer, Integer>() {\n          @Override\n          public Tuple2<Integer, Integer> call(Integer i) {\n            return new Tuple2<Integer, Integer>(i, i + 1);\n          }\n        });\n\n    // See SPARK-1834\n    Tuple2<Integer, Integer> reduced = pairRDD.reduce(\n        new Function2<Tuple2<Integer,Integer>, Tuple2<Integer,Integer>, Tuple2<Integer,Integer>>() {\n          @Override\n          public Tuple2<Integer, Integer> call(Tuple2<Integer, Integer> t1,\n                                               Tuple2<Integer, Integer> t2) {\n            return new Tuple2<Integer, Integer>(t1._1() + t2._1(), t1._2() + t2._2());\n          }\n        });\n\n    Assert.assertEquals(33, reduced._1().intValue());\n    Assert.assertEquals(40, reduced._1().intValue());\n  }\n{code}\n\nbut...\n\n{code}\njava.lang.NoSuchMethodError: org.apache.spark.api.java.JavaPairRDD.reduce(Lorg/apache/spark/api/java/function/Function2;)Lscala/Tuple2;\n{code}\n\nI decompiled the class and it really looks like the method is there with the expected signature:\n\n{code}\n  public scala.Tuple2<K, V> reduce(org.apache.spark.api.java.function.Function2<scala.Tuple2<K, V>, scala.Tuple2<K, V>, scala.Tuple2<K, V>>);\n{code}\n\nColor me pretty confused.", "On another look, I'm almost sure this is the same issue as in SPARK-3266, which [~joshrosen] has been looking at.", "Note that SPARK-3266 has now been fixed for 1.3.1 / 1.4.0."], "derived": {"summary": "I get a java. lang.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "NoSuchMethodError when invoking JavaPairRDD.reduce() in Java - I get a java. lang."}, {"q": "What updates or decisions were made in the discussion?", "a": "Note that SPARK-3266 has now been fixed for 1.3.1 / 1.4.0."}]}}
{"project": "SPARK", "issue_id": "SPARK-1835", "title": "sbt gen-idea includes both mesos and mesos with shaded-protobuf into dependencies", "status": "Resolved", "priority": "Minor", "reporter": "Xiangrui Meng", "assignee": null, "labels": [], "created": "2014-05-14T20:54:44.000+0000", "updated": "2016-01-05T21:17:25.000+0000", "description": "gen-idea includes both mesos-0.18.1 and mesos-0.18.1-shaded-protobuf into dependencies. This generates compile error because mesos-0.18.1 comes first and there is no protobuf jar in the dependencies.\n\nA workaround is to delete mesos-0.18.1.jar manually from idea intellij. Another solution is to publish the shaded jar as a separate version instead of using classifier.", "comments": ["\nI followed the directions in the bug - to delete the mesos-18.1.jar   But the following errors now happen. Note this error has also been reported in the spark dev mailing list - so the following is just corroborating what others have already noted.\n\nError:scalac: \n     while compiling: C:\\apps\\incubator-spark\\sql\\core\\src\\main\\scala\\org\\apache\\spark\\sql\\test\\TestSQLContext.scala\n        during phase: jvm\n     library version: version 2.10.4\n    compiler version: version 2.10.4\n  reconstructed args: -classpath <long classpath . -bootclasspath C:\\apps\\jdk1.7.0_51\\jre\\lib\\resources.jar;C:\\apps\\jdk1.7.0_51\\jre\\lib\\rt.jar;C:\\apps\\jdk1.7.0_51\\jre\\lib\\sunrsasign.jar;C:\\apps\\jdk1.7.0_51\\jre\\lib\\jsse.jar;C:\\apps\\jdk1.7.0_51\\jre\\lib\\jce.jar;C:\\apps\\jdk1.7.0_51\\jre\\lib\\charsets.jar;C:\\apps\\jdk1.7.0_51\\jre\\lib\\jfr.jar;C:\\apps\\jdk1.7.0_51\\jre\\classes;C:\\Users\\s80035683\\.m2\\repository\\org\\scala-lang\\scala-library\\2.10.4\\scala-library-2.10.4.jar -deprecation -feature -unchecked -language:postfixOps\n  last tree to typer: Literal(Constant(org.apache.spark.sql.catalyst.types.PrimitiveType))\n              symbol: null\n   symbol definition: null\n                 tpe: Class(classOf[org.apache.spark.sql.catalyst.types.PrimitiveType])\n       symbol owners: \n      context owners: object TestSQLContext -> package test\n== Enclosing template or block ==\nTemplate( // val <local TestSQLContext>: <notype> in object TestSQLContext, tree.tpe=org.apache.spark.sql.test.TestSQLContext.type\n  \"org.apache.spark.sql.SQLContext\" // parents\n  ValDef(\n    private\n    \"_\"\n    <tpt>\n    <empty>\n  )\n  // 2 statements\n  DefDef( // private def readResolve(): Object in object TestSQLContext\n    <method> private <synthetic>\n    \"readResolve\"\n    []\n    List(Nil)\n    <tpt> // tree.tpe=Object\n    test.this.\"TestSQLContext\" // object TestSQLContext in package test, tree.tpe=org.apache.spark.sql.test.TestSQLContext.type\n  )\n  DefDef( // def <init>(): org.apache.spark.sql.test.TestSQLContext.type in object TestSQLContext\n    <method>\n    \"<init>\"\n    []\n    List(Nil)\n    <tpt> // tree.tpe=org.apache.spark.sql.test.TestSQLContext.type\n    Block( // tree.tpe=Unit\n      Apply( // def <init>(sparkContext: org.apache.spark.SparkContext): org.apache.spark.sql.SQLContext in class SQLContext, tree.tpe=org.apache.spark.sql.SQLContext\n        TestSQLContext.super.\"<init>\" // def <init>(sparkContext: org.apache.spark.SparkContext): org.apache.spark.sql.SQLContext in class SQLContext, tree.tpe=(sparkContext: org.apache.spark.SparkContext)org.apache.spark.sql.SQLContext\n        Apply( // def <init>(master: String,appName: String,conf: org.apache.spark.SparkConf): org.apache.spark.SparkContext in class SparkContext, tree.tpe=org.apache.spark.SparkContext\n          new org.apache.spark.SparkContext.\"<init>\" // def <init>(master: String,appName: String,conf: org.apache.spark.SparkConf): org.apache.spark.SparkContext in class SparkContext, tree.tpe=(master: String, appName: String, conf: org.apache.spark.SparkConf)org.apache.spark.SparkContext\n          // 3 arguments\n          \"local\"\n          \"TestSQLContext\"\n          Apply( // def <init>(): org.apache.spark.SparkConf in class SparkConf, tree.tpe=org.apache.spark.SparkConf\n            new org.apache.spark.SparkConf.\"<init>\" // def <init>(): org.apache.spark.SparkConf in class SparkConf, tree.tpe=()org.apache.spark.SparkConf\n            Nil\n          )\n        )\n      )\n      ()\n    )\n  )\n)\n== Expanded type of tree ==\nConstantType(\n  value = Constant(org.apache.spark.sql.catalyst.types.PrimitiveType)\n)\nuncaught exception during compilation: java.lang.AssertionError", "Try `sbt/sbt clean` or remove `.idea*` and re-run `sbt/sbt gen-idea`. It may be due to sbt cache.", "Hey thanks  - that did the trick!  So then it appears your fix of manually deleting the mesos-18.1.jar after doing a sbt gen-idea is working.", "FWIW I can confirm this still occurs:\n\n{code}\n$ grep -r mesos .idea\n.idea/libraries/SBT__org_apache_mesos_mesos_0_21_0.xml:  <library name=\"SBT: org.apache.mesos:mesos:0.21.0\">\n.idea/libraries/SBT__org_apache_mesos_mesos_0_21_0.xml:      <root url=\"jar://$PROJECT_DIR$/lib_managed/jars/mesos-0.21.0.jar!/\"/>\n.idea/libraries/SBT__org_apache_mesos_mesos_0_21_0.xml:      <root url=\"jar://$PROJECT_DIR$/lib_managed/jars/mesos-0.21.0-shaded-protobuf.jar!/\"/>\n.idea/libraries/SBT__org_apache_mesos_mesos_0_21_0.xml:      <root url=\"jar://$PROJECT_DIR$/lib_managed/docs/mesos-0.21.0-javadoc.jar!/\"/>\n.idea/libraries/SBT__org_apache_mesos_mesos_0_21_0.xml:      <root url=\"jar://$PROJECT_DIR$/lib_managed/srcs/mesos-0.21.0-sources.jar!/\"/>\n{code}\n\nBut is it fixable and/or worth a fix, if I assume it's pretty rare to use SBT to generate an IDEA project when it can read the Maven build quite well, directly?", "Resolving as \"Won't Fix.\" Use the IntelliJ Maven / SBT import instead of gen-idea."], "derived": {"summary": "gen-idea includes both mesos-0. 18.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "sbt gen-idea includes both mesos and mesos with shaded-protobuf into dependencies - gen-idea includes both mesos-0. 18."}, {"q": "What updates or decisions were made in the discussion?", "a": "Resolving as \"Won't Fix.\" Use the IntelliJ Maven / SBT import instead of gen-idea."}]}}
{"project": "SPARK", "issue_id": "SPARK-1836", "title": "REPL $outer type mismatch causes lookup() and equals() problems", "status": "Resolved", "priority": "Major", "reporter": "Michael Malak", "assignee": null, "labels": [], "created": "2014-05-14T21:23:08.000+0000", "updated": "2014-05-28T19:40:38.000+0000", "description": "Anand Avati partially traced the cause to REPL wrapping classes in $outer classes. There are at least two major symptoms:\n\n1. equals()\n=========\n\nIn REPL equals() (required in custom classes used as a key for groupByKey) seems to have to be written using instanceOf[] instead of the canonical match{}\n\nSpark Shell (equals uses match{}):\n\n{noformat}\nclass C(val s:String) extends Serializable {\n  override def equals(o: Any) = o match {\n    case that: C => that.s == s\n    case _ => false\n  }\n}\n\nval x = new C(\"a\")\nval bos = new java.io.ByteArrayOutputStream()\nval out = new java.io.ObjectOutputStream(bos)\nout.writeObject(x);\nval b = bos.toByteArray();\nout.close\nbos.close\nval y = new java.io.ObjectInputStream(new ava.io.ByteArrayInputStream(b)).readObject().asInstanceOf[C]\nx.equals(y)\n\nres: Boolean = false\n{noformat}\n\nSpark Shell (equals uses isInstanceOf[]):\n\n{noformat}\nclass C(val s:String) extends Serializable {\n  override def equals(o: Any) = if (o.isInstanceOf[C]) (o.asInstanceOf[C].s = s) else false\n}\n\nval x = new C(\"a\")\nval bos = new java.io.ByteArrayOutputStream()\nval out = new java.io.ObjectOutputStream(bos)\nout.writeObject(x);\nval b = bos.toByteArray();\nout.close\nbos.close\nval y = new java.io.ObjectInputStream(new ava.io.ByteArrayInputStream(b)).readObject().asInstanceOf[C]\nx.equals(y)\n\nres: Boolean = true\n{noformat}\n\nScala Shell (equals uses match{}):\n\n{noformat}\nclass C(val s:String) extends Serializable {\n  override def equals(o: Any) = o match {\n    case that: C => that.s == s\n    case _ => false\n  }\n}\n\nval x = new C(\"a\")\nval bos = new java.io.ByteArrayOutputStream()\nval out = new java.io.ObjectOutputStream(bos)\nout.writeObject(x);\nval b = bos.toByteArray();\nout.close\nbos.close\nval y = new java.io.ObjectInputStream(new java.io.ByteArrayInputStream(b)).readObject().asInstanceOf[C]\nx.equals(y)\n\nres: Boolean = true\n{noformat}\n\n2. lookup()\n=========\n\n{noformat}\nclass C(val s:String) extends Serializable {\n  override def equals(o: Any) = if (o.isInstanceOf[C]) o.asInstanceOf[C].s == s else false\n  override def hashCode = s.hashCode\n  override def toString = s\n}\nval r = sc.parallelize(Array((new C(\"a\"),11),(new C(\"a\"),12)))\nr.lookup(new C(\"a\"))\n<console>:17: error: type mismatch;\n found   : C\n required: C\n              r.lookup(new C(\"a\"))\n                       ^\n{noformat}\n\nSee\nhttp://mail-archives.apache.org/mod_mbox/spark-dev/201405.mbox/%3C1400019424.80629.YahooMailNeo%40web160801.mail.bf1.yahoo.com%3E", "comments": ["This sounds like it could be related to [SPARK-1199]", "Michael Ambrust: Indeed. Do you think I should add my additional case of equals() (and its workaround) as a comment to SPARK-1199 and mark this one as a duplicate?", "Yeah I think its likely they are related.  We can re-open this one later if fixing the other one doesn't solve your issue."], "derived": {"summary": "Anand Avati partially traced the cause to REPL wrapping classes in $outer classes. There are at least two major symptoms:\n\n1.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "REPL $outer type mismatch causes lookup() and equals() problems - Anand Avati partially traced the cause to REPL wrapping classes in $outer classes. There are at least two major symptoms:\n\n1."}, {"q": "What updates or decisions were made in the discussion?", "a": "Yeah I think its likely they are related.  We can re-open this one later if fixing the other one doesn't solve your issue."}]}}
{"project": "SPARK", "issue_id": "SPARK-1837", "title": "NumericRange should be partitioned in the same way as other sequences", "status": "Resolved", "priority": "Major", "reporter": "Kan Zhang", "assignee": "Kan Zhang", "labels": [], "created": "2014-05-14T21:23:56.000+0000", "updated": "2014-06-14T21:32:16.000+0000", "description": "Otherwise, RDD.zip() would behave unexpectedly. For example, as given in SPARK-1817:\n\nscala> sc.parallelize(1L to 2L,4).zip(sc.parallelize(11 to 12,4)).collect\nres1: Array[(Long, Int)] = Array((2,11))", "comments": ["PR: https://github.com/apache/spark/pull/776"], "derived": {"summary": "Otherwise, RDD. zip() would behave unexpectedly.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "NumericRange should be partitioned in the same way as other sequences - Otherwise, RDD. zip() would behave unexpectedly."}, {"q": "What updates or decisions were made in the discussion?", "a": "PR: https://github.com/apache/spark/pull/776"}]}}
{"project": "SPARK", "issue_id": "SPARK-1838", "title": "On a YARN cluster, Spark doesn't run on local mode", "status": "Closed", "priority": "Major", "reporter": "Andrew Or", "assignee": null, "labels": [], "created": "2014-05-14T22:35:19.000+0000", "updated": "2014-11-05T10:43:29.000+0000", "description": "Right now we throw an exception if YARN_LOCAL_DIRS is not set. However, we may want to just run Spark in local mode, which doesn't even use this environment variable.", "comments": ["Looks like I accidentally set SPARK_YARN_MODE to true manually, which directly conflicts with master being local mode. This isn't documented so users shouldn't be setting this variable anyway. Pas de problme."], "derived": {"summary": "Right now we throw an exception if YARN_LOCAL_DIRS is not set. However, we may want to just run Spark in local mode, which doesn't even use this environment variable.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "On a YARN cluster, Spark doesn't run on local mode - Right now we throw an exception if YARN_LOCAL_DIRS is not set. However, we may want to just run Spark in local mode, which doesn't even use this environment variable."}, {"q": "What updates or decisions were made in the discussion?", "a": "Looks like I accidentally set SPARK_YARN_MODE to true manually, which directly conflicts with master being local mode. This isn't documented so users shouldn't be setting this variable anyway. Pas de problme."}]}}
{"project": "SPARK", "issue_id": "SPARK-1839", "title": "PySpark take() does not launch a Spark job when it has to", "status": "Resolved", "priority": "Major", "reporter": "Hossein Falaki", "assignee": "Aaron Davidson", "labels": [], "created": "2014-05-14T23:45:45.000+0000", "updated": "2014-05-31T20:06:43.000+0000", "description": "If you call take() or first() on a large FilteredRDD, the driver attempts to scan all partitions to find the first valid item. If the RDD is large this would fail or hang.", "comments": [], "derived": {"summary": "If you call take() or first() on a large FilteredRDD, the driver attempts to scan all partitions to find the first valid item. If the RDD is large this would fail or hang.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "PySpark take() does not launch a Spark job when it has to - If you call take() or first() on a large FilteredRDD, the driver attempts to scan all partitions to find the first valid item. If the RDD is large this would fail or hang."}]}}
{"project": "SPARK", "issue_id": "SPARK-1840", "title": "SparkListenerBus prints out scary error message when terminating normally", "status": "Resolved", "priority": "Major", "reporter": "Andrew Or", "assignee": "Tathagata Das", "labels": [], "created": "2014-05-15T01:36:28.000+0000", "updated": "2014-11-05T10:43:36.000+0000", "description": "This is because the Scala's NonLocalReturnControl (which extends ControlThrowable) is being logged. However, this is expected when the SparkContext terminates.\n\n(OP is TD)", "comments": ["Issue resolved by pull request 783\n[https://github.com/apache/spark/pull/783]"], "derived": {"summary": "This is because the Scala's NonLocalReturnControl (which extends ControlThrowable) is being logged. However, this is expected when the SparkContext terminates.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "SparkListenerBus prints out scary error message when terminating normally - This is because the Scala's NonLocalReturnControl (which extends ControlThrowable) is being logged. However, this is expected when the SparkContext terminates."}, {"q": "What updates or decisions were made in the discussion?", "a": "Issue resolved by pull request 783\n[https://github.com/apache/spark/pull/783]"}]}}
