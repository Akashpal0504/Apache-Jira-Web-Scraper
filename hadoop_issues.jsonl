{"project": "HADOOP", "issue_id": "HADOOP-8", "title": "NDFS DataNode advertises localhost as it's address", "status": "Closed", "priority": "Major", "reporter": "Peter Sandström", "assignee": null, "labels": [], "created": "2005-07-24T23:46:18.000+0000", "updated": "2015-05-18T04:15:07.000+0000", "description": null, "comments": ["fixes the problem by connecting to the NameNode and using the address that the local socket is bound to instead of calling getLocalHost(), beware that this makes machineName in the DataNode constructor unused.", "The problem with this fix is that, if the namenode is down, the socket creation fails and we fail to launch a datanode.  So it would be great to have a better method than InetAddress.getLocalHost(), that tries harder to return the appropriate IP to advertise to the namenode, but that doesn't require the namenode to be up.\n\nPerhaps this could use NetworkInterfaces.getInetAddresses() and find the first that is not a loopback interface?", "This is a known JVM bug:\n\nhttp://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4665037\n\nSo let's let Sun fix it, rather than fix it ourselves.  One can workaround it by altering one's linux configuration."], "derived": {"summary": "", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "NDFS DataNode advertises localhost as it's address"}, {"q": "What updates or decisions were made in the discussion?", "a": "This is a known JVM bug:\n\nhttp://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4665037\n\nSo let's let Sun fix it, rather than fix it ourselves.  One can workaround it by altering one's linux configuration."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-19", "title": "Datanode corruption", "status": "Closed", "priority": "Critical", "reporter": "Rod Taylor", "assignee": "Doug Cutting", "labels": [], "created": "2005-10-07T11:46:42.000+0000", "updated": "2009-07-08T16:41:48.000+0000", "description": "Our admins accidentally started a second nutch datanode pointing to the same directories as one already running (same machine) which in turn caused the entire contents of the datanode to go disappear.\n\nThis happened because the blocking was based on the username (since fixed in our start scripts) and it was started as two different users.\n\nThe ndfs.name.dir and ndfs.data.dir directories were both completely devoid of content, where they had about 150GB not all that much earlier.\n\n\nI think the solution is improved interlocking within the data directory itself (file locked with flock or something similar).", "comments": ["I added a format command to namenode.  Now one must run 'bin/hadoop namenode -format' whenever a new dfs.data.dir is configured.  A misconfigured namenode will thus no longer create a new empty filesystem and then tell all datanodes to discard their blocks."], "derived": {"summary": "Our admins accidentally started a second nutch datanode pointing to the same directories as one already running (same machine) which in turn caused the entire contents of the datanode to go disappear. This happened because the blocking was based on the username (since fixed in our start scripts) and it was started as two different users.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Datanode corruption - Our admins accidentally started a second nutch datanode pointing to the same directories as one already running (same machine) which in turn caused the entire contents of the datanode to go disappear. This happened because the blocking was based on the username (since fixed in our start scripts) and it was started as two different users."}, {"q": "What updates or decisions were made in the discussion?", "a": "I added a format command to namenode.  Now one must run 'bin/hadoop namenode -format' whenever a new dfs.data.dir is configured.  A misconfigured namenode will thus no longer create a new empty filesystem and then tell all datanodes to discard their blocks."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-10", "title": "ndfs.replication is not documented within the nutch-default.xml configuration file.", "status": "Closed", "priority": "Trivial", "reporter": "Rod Taylor", "assignee": null, "labels": [], "created": "2005-10-14T06:02:45.000+0000", "updated": "2009-07-08T16:41:47.000+0000", "description": "ndfs.replication is not documented within the nutch-default.xml configuration file.", "comments": ["This has been fixed.  The paramter is now dfs.replication in hadoop-default.xml."], "derived": {"summary": "ndfs. replication is not documented within the nutch-default.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "ndfs.replication is not documented within the nutch-default.xml configuration file. - ndfs. replication is not documented within the nutch-default."}, {"q": "What updates or decisions were made in the discussion?", "a": "This has been fixed.  The paramter is now dfs.replication in hadoop-default.xml."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-18", "title": "Crash with multiple temp directories", "status": "Closed", "priority": "Critical", "reporter": "Rod Taylor", "assignee": null, "labels": [], "created": "2006-01-11T02:45:05.000+0000", "updated": "2009-07-08T16:51:40.000+0000", "description": "A brief read of the code indicated it may be possible to use multiple local directories using something like the below:\n\n  <property>\n    <name>mapred.local.dir</name>\n    <value>/local,/local1,/local2</value>\n    <description>The local directory where MapReduce stores intermediate\n    data files.\n    </description>\n  </property>\n\nThis failed with the below exception during either the generate or update phase (not entirely sure which).\n\njava.lang.ArrayIndexOutOfBoundsException\n        at java.util.zip.CRC32.update(CRC32.java:51)\n        at org.apache.nutch.fs.NFSDataInputStream$Checker.read(NFSDataInputStream.java:92)\n        at org.apache.nutch.fs.NFSDataInputStream$PositionCache.read(NFSDataInputStream.java:156)\n        at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)\n        at java.io.BufferedInputStream.read1(BufferedInputStream.java:256)\n        at java.io.BufferedInputStream.read(BufferedInputStream.java:313)\n        at java.io.DataInputStream.readFully(DataInputStream.java:176)\n        at org.apache.nutch.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:55)\n        at org.apache.nutch.io.DataOutputBuffer.write(DataOutputBuffer.java:89)\n        at org.apache.nutch.io.SequenceFile$Reader.next(SequenceFile.java:378)\n        at org.apache.nutch.io.SequenceFile$Reader.next(SequenceFile.java:301)\n        at org.apache.nutch.io.SequenceFile$Reader.next(SequenceFile.java:323)\n        at org.apache.nutch.mapred.SequenceFileRecordReader.next(SequenceFileRecordReader.java:60)\n        at org.apache.nutch.segment.SegmentReader$InputFormat$1.next(SegmentReader.java:80)\n        at org.apache.nutch.mapred.MapTask$2.next(MapTask.java:106)\n        at org.apache.nutch.mapred.MapRunner.run(MapRunner.java:48)\n        at org.apache.nutch.mapred.MapTask.run(MapTask.java:116)\n        at org.apache.nutch.mapred.TaskTracker$Child.main(TaskTracker.java:604)", "comments": ["Wish there was an \"edit\" option in JIRA.  Obviously it was within the SegmentReader -- though I don't believe it does anything special to try to read the data file off disk.\n\nI do not and have not seen this exception with a single local directory configuration.", "I have sucessfully used mapred.local.dir with multiple values on many occasions.\n\nCan you please try to distill this to an easy to reproduce test-case?  Thanks.", "I enabled multiple local directories on 50% of my machines. This error, which does not appear otherwise (we processed a few hundred million pages without error), happens across all machines BUT always on part-##'s which were created on the split directory boxes.\n\nSecondly, the error only occurs during the segread command, org.apache.nutch.segment.SegmentReader, while retrieving the data out of common storage (NAS in our case).\n\n060203 222553 task_m_4x4zan 0.7223333% /opt/sitesell/sbider_data/nutch/segments/segmentset-2006-02-02254/20060202181143-3/content/part-00001/data:0+21025408\n060203 222553 task_m_4x4zan  Problem reading checksum file: java.io.EOFException. Ignoring.\n060203 222553 task_m_4x4zan  Error running child\n060203 222553 task_m_4x4zan java.lang.ArrayIndexOutOfBoundsException\n060203 222553 task_m_4x4zan     at java.util.zip.CRC32.update(CRC32.java:43)\n060203 222553 task_m_4x4zan     at org.apache.nutch.fs.NFSDataInputStream$Checker.read(NFSDataInputStream.java:92)\n060203 222553 task_m_4x4zan     at org.apache.nutch.fs.NFSDataInputStream$PositionCache.read(NFSDataInputStream.java:156)\n060203 222553 task_m_4x4zan     at java.io.BufferedInputStream.read1(BufferedInputStream.java:254)\n060203 222553 task_m_4x4zan     at java.io.BufferedInputStream.read(BufferedInputStream.java:313)\n060203 222553 task_m_4x4zan     at java.io.DataInputStream.readFully(DataInputStream.java:176)\n060203 222553 task_m_4x4zan     at org.apache.nutch.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:55)\n060203 222553 task_m_4x4zan     at org.apache.nutch.io.DataOutputBuffer.write(DataOutputBuffer.java:89)\n060203 222553 task_m_4x4zan     at org.apache.nutch.io.SequenceFile$Reader.next(SequenceFile.java:378)\n060203 222553 task_m_4x4zan     at org.apache.nutch.io.SequenceFile$Reader.next(SequenceFile.java:301)\n060203 222553 task_m_4x4zan     at org.apache.nutch.io.SequenceFile$Reader.next(SequenceFile.java:323)\n060203 222553 task_m_4x4zan     at org.apache.nutch.mapred.SequenceFileRecordReader.next(SequenceFileRecordReader.java:60)\n060203 222553 task_m_4x4zan     at org.apache.nutch.segment.SegmentReader$InputFormat$1.next(SegmentReader.java:80)\n060203 222553 task_m_4x4zan     at org.apache.nutch.mapred.MapTask$2.next(MapTask.java:106)\n060203 222553 task_m_4x4zan     at org.apache.nutch.mapred.MapRunner.run(MapRunner.java:48)\n060203 222553 task_m_4x4zan     at org.apache.nutch.mapred.MapTask.run(MapTask.java:116)\n060203 222553 task_m_4x4zan     at org.apache.nutch.mapred.TaskTracker$Child.main(TaskTracker.java:603)\n\n060203 222557 task_m_4x4zan done; removing files.", "Finally figured it out. One temp directory was filling up (different sizes) and the fetch was aborting BUT I didn't see this in the logs for the longest time.\n\nThe patch in NUTCH-143 helped track down the issue because it caused the error to be noticed be the scripts driving the code in a location close to where the error took place rather than several steps (possibly many hours) later..\n\nPlease close this bug.", "Resoving at Rod's request."], "derived": {"summary": "A brief read of the code indicated it may be possible to use multiple local directories using something like the below:\n\n  <property>\n    <name>mapred. local.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Crash with multiple temp directories - A brief read of the code indicated it may be possible to use multiple local directories using something like the below:\n\n  <property>\n    <name>mapred. local."}, {"q": "What updates or decisions were made in the discussion?", "a": "Resoving at Rod's request."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-9", "title": "mapred.local.dir  temp dir. space allocation limited by smallest area", "status": "Closed", "priority": "Minor", "reporter": "Paul Baclace", "assignee": "Ariel Shemaiah Rabkin", "labels": [], "created": "2006-01-17T10:09:39.000+0000", "updated": "2010-11-06T05:27:57.000+0000", "description": "When mapred.local.dir is used to specify multiple  temp dir. areas, space allocation limited by smallest area because the temp dir. selection algorithm is \"round robin starting from a randomish point\".   When round robin is used with approximately constant sized chunks, the smallest area runs out of space first, and this is a fatal error. \n\nWorkaround: only list local fs dirs in mapred.local.dir with similarly-sized available areas.\n\nI wrote a patch to JobConf (currenly being tested) which uses df to check available space (once a minute or less often) and then uses an efficient roulette selection to do allocation weighted by magnitude of available space. \n\n", "comments": ["This is a less pressing issue these days, since you can pass a size to the local dir allocator and it'll do the right thing.\nDo people think this is still worth fixing?   It should be straightforward to do something roulette-y in LocalDirAllocator.getLocalPathForWrite for unknown-size writes.", "Patch fixing this issue.", "downgrading priority", "+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12386584/hadoop9.patch\n  against trunk revision 678593.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2920/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2920/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2920/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2920/console\n\nThis message is automatically generated.", "I just committed this. Thanks, Ari!", "Integrated in Hadoop-trunk #581 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/581/])", "You're welcome, Owen!   (I wrote the roulette selection patch.)\n", "Paul -- I'm confused. Are you sure this is the issue you meant to comment on?  The patch on this Jira wasn't yours. ", "Ari,\n\nThis issue was originally NUTCH-181 before Hadoop was split off.  I wrote a patch Dec. 29 2005 and used it at archive.org Jan-Feb 2006.  Looking at my old notes, I created this issue on Jan. 11 2006, and prepared the patch on Feb. 28 2006, but it was either lost in a Jira transition or the attachment somehow failed.  \n\nWhen I looked at your patch yesterday, it was similar enough to what I remembered (5 years ago) that I thought it must be a revision of the patch I did.  Today I found my source and 2005-2006 work notes and it is clear that you implemented the change without seeing mine.\n\nThanks for doing it in same \"roulette-y\" spirit of my lost patch! \n"], "derived": {"summary": "When mapred. local.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "mapred.local.dir  temp dir. space allocation limited by smallest area - When mapred. local."}, {"q": "What updates or decisions were made in the discussion?", "a": "Ari,\n\nThis issue was originally NUTCH-181 before Hadoop was split off.  I wrote a patch Dec. 29 2005 and used it at archive.org Jan-Feb 2006.  Looking at my old notes, I created this issue on Jan. 11 2006, and prepared the patch on Feb. 28 2006, but it was either lost in a Jira transition or the attachment somehow failed.  \n\nWhen I looked at your patch yesterday, it was similar enough to what I remembered (5 years ago) that I thought it must be a revision of the patch I did.  Today I found my source and 2005-2006 work notes and it is clear that you implemented the change without seeing mine.\n\nThanks for doing it in same \"roulette-y\" spirit of my lost patch!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-7", "title": "MapReduce has a series of problems concerning task-allocation to worker nodes", "status": "Closed", "priority": "Major", "reporter": "Mike Cafarella", "assignee": null, "labels": [], "created": "2006-01-21T05:40:53.000+0000", "updated": "2015-05-18T04:11:26.000+0000", "description": "The MapReduce JobTracker is not great at allocating tasks to TaskTracker worker nodes.\n\nHere are the problems:\n1) There is no speculative execution of tasks\n2) Reduce tasks must wait until all map tasks are completed before doing any work\n3) TaskTrackers don't distinguish between Map and Reduce jobs.  Also, the number of\ntasks at a single node is limited to some constant.  That means you can get weird deadlock\nproblems upon machine failure.  The reduces take up all the available execution slots, but they\ndon't do productive work, because they're waiting for a map task to complete.  Of course, that\nmap task won't even be started until the reduce tasks finish, so you can see the problem...\n4) The JobTracker is so complicated that it's hard to fix any of these.\n\n\nThe right solution is a rewrite of the JobTracker to be a lot more flexible in task handling.\nIt has to be a lot simpler.  One way to make it simpler is to add an abstraction I'll call\n\"TaskInProgress\".  Jobs are broken into chunks called TasksInProgress.  All the TaskInProgress\nobjects must be complete, somehow, before the Job is complete.\n\nA single TaskInProgress can be executed by one or more Tasks.  TaskTrackers are assigned Tasks.\nIf a Task fails, we report it back to the JobTracker, where the TaskInProgress lives.  The TIP can then\ndecide whether to launch additional  Tasks or not.\n\nSpeculative execution is handled within the TIP.  It simply launches multiple Tasks in parallel.  The\nTaskTrackers have no idea that these Tasks are actually doing the same chunk of work.  The TIP\nis complete when any one of its Tasks are complete.\n\n", "comments": ["\n  Say!  Here's a patch that implements the JobTracker rewrite.  Please take a look and let me know what you think....", "As Mr Burns would say \"eggcelent\"  I'll give this a try.  BTW, is it possible to implement functionality that would start jobs that are lagging on nodes that have completed tasks like google does?  for example if your 90% done and the last 10 jobs are hung because of bad hardware, slow response or failure and have the ability to redo the long running jobs in parallel on alternate nodes and complete the first one that finishes?  this way if you have a huge crawl and certain nodes slow or fail those jobs can be alternated on completed nodes to try and wrap up and terminate any dead jobs when done?\n\nhope that makes sense..", "Byron, that's exactly what Mike means by \"speculative execution\".", "Guys,\n\nGreg Barish and the folks who worked on the Theseus planning system for information agents at USC did a lot of work on this subject. Theseus is implemented in java, and as I recall, includes the capability to perform speculative execution...\n\nhttp://www.isi.edu/~barish/", "so thats what they call it :) thanks", "\n  One more thing:\n\n  You'll see in this patch that ReduceTasks contain a 2D array of map tasks.  They used to\ncontain a 1D array, one map task for each map-split of the data.\n\n  This 2D business is less than perfect, so let me explain...\n\n  Each Reduce task needs to know when its map task predecessors have completed.\nIn the old days, this was easy.  There was one map id for each split of the data, so for\nk splits, there were k map ids to know about.\n\n  But in a world with speculative execution and early-start of reduce tasks, there could\nbe multiple possible map tasks that work on the same split of data.  So for each of the\nk splits, there could be up to M tasks working on it.  Thus, the reduce task knows about\nk * M map ids.  When any one of the M for each split has completed, the reduce task knows\nit can move on.\n\n   But this is a little silly.  The JobTracker has a \"TaskInProgress\" abstraction that represents\nthe idea of a \"split's worth of work\".  A single TIP contains M map task ids.  Instead of the\nreduce task looking all over for map task ids, it should just deal with TIP ids.  That way,\nwe're back to a 1D array, and the reduce task code is easier to understand.\n\n  Anyway, it will still work as is.  I'll improve the code in a future patch.  For the moment,\nI'll let this patch stand.  I just wanted to let people know...\n", "I tested this patch and jobs seem to run into deadlocks when one node crashes while others are loading map output data from that node. Here some lines tasktracker on node B that tries to copy map data from node A which has crashed:\n\n060124 181752 task_r_27r56x 0.2212838% reduce > copy > task_m_1ujusx@nodeA:50040\n060124 181753 task_r_7jjqag 0.17820947% reduce > copy >\n060124 181753 task_r_27r56x 0.2212838% reduce > copy > task_m_1ujusx@nodeA:50040\n060124 181754 task_r_7jjqag 0.17820947% reduce > copy >\n060124 181754 task_r_27r56x 0.2212838% reduce > copy > task_m_1ujusx@nodeA:50040\n060124 181755 task_r_7jjqag 0.17820947% reduce > copy >\n060124 181755 task_r_27r56x 0.2212838% reduce > copy > task_m_1ujusx@nodeA:50040\n060124 181756 task_r_7jjqag 0.17820947% reduce > copy >\n[...]\n060124 223510 task_r_27r56x 0.2212838% reduce > copy > task_m_1ujusx@nodeA:50040\n060124 223511 task_r_7jjqag 0.17820947% reduce > copy >\n060124 223511 task_r_27r56x 0.2212838% reduce > copy > task_m_1ujusx@nodeA:50040\n060124 223512 task_r_7jjqag 0.17820947% reduce > copy >\n060124 223512 task_r_27r56x 0.2212838% reduce > copy > task_m_1ujusx@nodeA:50040\n060124 223513 task_r_7jjqag 0.17820947% reduce > copy >\n060124 223513 task_r_27r56x 0.2212838% reduce > copy > task_m_1ujusx@nodeA:50040\n\nNode A was removed from the jobtracker's node list but it seems like not all tasks depending on that node have been killed.", "Mike committed this last week.", "This is likely me being dumb, but I don't think this issue is fixed.\n\nWhen I run any of the provided example programs wordcount/grep (also pi with specualtive excecution enabled) reduce tasks does not start before all map tasks have completed.\n\nMy cluster contains three nodes and I am running Hadoop 0.4.0."], "derived": {"summary": "The MapReduce JobTracker is not great at allocating tasks to TaskTracker worker nodes. Here are the problems:\n1) There is no speculative execution of tasks\n2) Reduce tasks must wait until all map tasks are completed before doing any work\n3) TaskTrackers don't distinguish between Map and Reduce jobs.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "MapReduce has a series of problems concerning task-allocation to worker nodes - The MapReduce JobTracker is not great at allocating tasks to TaskTracker worker nodes. Here are the problems:\n1) There is no speculative execution of tasks\n2) Reduce tasks must wait until all map tasks are completed before doing any work\n3) TaskTrackers don't distinguish between Map and Reduce jobs."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is likely me being dumb, but I don't think this issue is fixed.\n\nWhen I run any of the provided example programs wordcount/grep (also pi with specualtive excecution enabled) reduce tasks does not start before all map tasks have completed.\n\nMy cluster contains three nodes and I am running Hadoop 0.4.0."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-12", "title": "InputFormat used in job must be in JobTracker classpath (not loaded from job JAR)", "status": "Closed", "priority": "Minor", "reporter": "Bryan Pendleton", "assignee": null, "labels": [], "created": "2006-01-31T07:00:17.000+0000", "updated": "2009-07-08T16:51:40.000+0000", "description": "During development, I've been creating/tweaking custom InputFormat implementations. However, when you try to run a job against a running cluster, you get:\n  Exception in thread \"main\" java.io.IOException: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: my.custom.InputFormat\n          at org.apache.nutch.ipc.Client.call(Client.java:294)\n          at org.apache.nutch.ipc.RPC$Invoker.invoke(RPC.java:127)\n          at $Proxy0.submitJob(Unknown Source)\n          at org.apache.nutch.mapred.JobClient.submitJob(JobClient.java:259)\n          at org.apache.nutch.mapred.JobClient.runJob(JobClient.java:288)\n          at com.parc.uir.wikipedia.WikipediaJob.main(WikipediaJob.java:85)\n\nThis error goes away if I restart the TaskTrackers/JobTracker with a classpath which includes the needed code. Other classes (Mapper, Reducer) appear to be available out of the jar file specified in the JobConf, but not the InputFormat. Obviously, it's less than idea to have to restart the JobTracker whenever there's a change to a job-specific class.", "comments": ["We've thus far avoided loading job-specific code in the JobTracker and TaskTracker, in order to keep these more reliable.  File splitting is performed by the job tracker.  So if you're overriding InputFormat.getSplits(), then fixing this is harder.  But if you're simply overriding getRecordReader(), then this should be easier to fix.  In that case one could fix this by moving getSplits() to a new interface that's used only by the TaskTracker.  If this is important to you, please submit a patch to this effect.", "Wouldn't it be appropriate to make input splitting into a task, so that getSplits could be run by the TaskTrackerChild? That way the current interfaces would remain and the user could override it from the job.jar. \n\nAn example where we would find it useful is where the map input is coming from external servers over sockets. getSplits could return splits of the form FileSplit(\"host:port\", 0 ,1000) and the RecordReader needs to know how to translate that name into a data stream.", "I think the reason to keep getSplits() in the jobtracker, is because the result of getSplits() determines the actual number of map tasks that's run, and the job tracker does more setup and tracking *after* getSplits(). How would you separate that out?", "I would schedule the getSplits task and when it completed, I would schedule the map jobs. It would be pretty parallel to the way the completion of the map tasks causes the reduces to be scheduled. I think the right place to hook it would be in JobTracker.JobInProgress.completedTask(String). One difference that I'm aware of, is that until getSplits returns, you don't have any idea how many maps will be needed, so you can't create the map tasks when the job is created.", "I fixed this by having the JobTracker load the InputFormat from the job's jar."], "derived": {"summary": "During development, I've been creating/tweaking custom InputFormat implementations. However, when you try to run a job against a running cluster, you get:\n  Exception in thread \"main\" java.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "InputFormat used in job must be in JobTracker classpath (not loaded from job JAR) - During development, I've been creating/tweaking custom InputFormat implementations. However, when you try to run a job against a running cluster, you get:\n  Exception in thread \"main\" java."}, {"q": "What updates or decisions were made in the discussion?", "a": "I fixed this by having the JobTracker load the InputFormat from the job's jar."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-1", "title": "initial import of code from Nutch", "status": "Closed", "priority": "Major", "reporter": "Doug Cutting", "assignee": "Doug Cutting", "labels": [], "created": "2006-02-01T02:54:35.000+0000", "updated": "2015-09-17T06:00:01.000+0000", "description": "The initial code for Hadoop will be copied from Nutch.", "comments": ["Link to the related Nutch issue.", "The relevant code from Nutch has now been moved to Hadoop's svn.", "Integrated in Hive-trunk-h0.21 #1845 (See [https://builds.apache.org/job/Hive-trunk-h0.21/1845/])\n    HIVE-3760 : TestNegativeMinimrCliDriver_mapreduce_stack_trace.q fails on hadoop-1 (Gunther Hagleitner via Ashutosh Chauhan) (Revision 1418837)\n\n     Result = FAILURE", "Integrated in Hive-trunk-h0.21 #1848 (See [https://builds.apache.org/job/Hive-trunk-h0.21/1848/])\n    HIVE-3782 : testCliDriver_sample_islocalmode_hook fails on hadoop-1 (Gunther Hagleitner via Ashutosh Chauhan) (Revision 1419205)\n\n     Result = FAILURE", "Integrated in Sqoop2-hadoop200 #12 (See [https://builds.apache.org/job/Sqoop2-hadoop200/12/])\n    SQOOP-789: Fix Hadoop-1 build (Revision 7ca9b4468846c382747df3e9c7fea56dfdc5bd70)\n\n     Result = SUCCESS", "Integrated in Sqoop2-hadoop100 #13 (See [https://builds.apache.org/job/Sqoop2-hadoop100/13/])\n    SQOOP-789: Fix Hadoop-1 build (Revision 7ca9b4468846c382747df3e9c7fea56dfdc5bd70)\n\n     Result = UNSTABLE", "Integrated in Hive-trunk-h0.21 #1882 (See [https://builds.apache.org/job/Hive-trunk-h0.21/1882/])\n    HIVE-3802 : testCliDriver_input39 fails on hadoop-1 (Gunther Hagleitner via Ashutosh Chauhan) (Revision 1426265)\nHIVE-3801 : testCliDriver_loadpart_err fails on hadoop-1 (Gunther Hagleitner via Ashutosh Chauhan) (Revision 1426263)\nHIVE-3800 : testCliDriver_combine2 fails on hadoop-1 (Gunther Hagleitner via Ashutosh Chauhan) (Revision 1426261)\n\n     Result = SUCCESS", "Integrated in Hive-trunk-hadoop2 #54 (See [https://builds.apache.org/job/Hive-trunk-hadoop2/54/])\n    HIVE-3802 : testCliDriver_input39 fails on hadoop-1 (Gunther Hagleitner via Ashutosh Chauhan) (Revision 1426265)\nHIVE-3801 : testCliDriver_loadpart_err fails on hadoop-1 (Gunther Hagleitner via Ashutosh Chauhan) (Revision 1426263)\nHIVE-3800 : testCliDriver_combine2 fails on hadoop-1 (Gunther Hagleitner via Ashutosh Chauhan) (Revision 1426261)\nHIVE-3782 : testCliDriver_sample_islocalmode_hook fails on hadoop-1 (Gunther Hagleitner via Ashutosh Chauhan) (Revision 1419205)\nHIVE-3760 : TestNegativeMinimrCliDriver_mapreduce_stack_trace.q fails on hadoop-1 (Gunther Hagleitner via Ashutosh Chauhan) (Revision 1418837)\nHIVE-2698 [jira] Enable Hadoop-1.0.0 in Hive\n(Enis Söztutar via Carl Steinbach)\n\nSummary:\nthird version of the patch\n\nHadoop-1.0.0 is recently released, which is AFAIK, API compatible to the 0.20S\nrelease.\n\nTest Plan: EMPTY\n\nReviewers: JIRA, cwsteinbach\n\nReviewed By: cwsteinbach\n\nCC: cwsteinbach, enis\n\nDifferential Revision: https://reviews.facebook.net/D1389 (Revision 1236023)\n\n     Result = ABORTED", "Integrated in hive-trunk-hadoop1 #96 (See [https://builds.apache.org/job/hive-trunk-hadoop1/96/])\n    HIVE-3788 : testCliDriver_repair fails on hadoop-1 (Gunther Hagleitner via Ashutosh Chauhan) (Revision 1448699)\n\n     Result = ABORTED", "Integrated in Hive-trunk-h0.21 #1981 (See [https://builds.apache.org/job/Hive-trunk-h0.21/1981/])\n    HIVE-3788 : testCliDriver_repair fails on hadoop-1 (Gunther Hagleitner via Ashutosh Chauhan) (Revision 1448699)\n\n     Result = FAILURE", "Integrated in Hive-trunk-hadoop2 #133 (See [https://builds.apache.org/job/Hive-trunk-hadoop2/133/])\n    HIVE-3788 : testCliDriver_repair fails on hadoop-1 (Gunther Hagleitner via Ashutosh Chauhan) (Revision 1448699)\n\n     Result = FAILURE", "Integrated in Hive-trunk-h0.21 #2012 (See [https://builds.apache.org/job/Hive-trunk-h0.21/2012/])\n    HIVE-3862 : testHBaseNegativeCliDriver_cascade_dbdrop fails on hadoop-1 (Gunther Hagleitner via Ashutosh Chauhan) (Revision 1455405)\n\n     Result = FAILURE", "Integrated in Hive-trunk-hadoop2 #138 (See [https://builds.apache.org/job/Hive-trunk-hadoop2/138/])\n    HIVE-3862 : testHBaseNegativeCliDriver_cascade_dbdrop fails on hadoop-1 (Gunther Hagleitner via Ashutosh Chauhan) (Revision 1455405)\n\n     Result = FAILURE", "Integrated in Hadoop-trunk-Commit #3719 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3719/])\n    MAPREDUCE-5159. Change ValueAggregatorJob to add APIs which can support binary compatibility with hadoop-1 examples. Contributed by Zhijie Shen. (Revision 1480394)\n\n     Result = SUCCESS", "Integrated in Hadoop-trunk-Commit #3721 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3721/])\n    MAPREDUCE-5157. Bring back old sampler related code so that we can support binary compatibility with hadoop-1 sorter example. Contributed by Zhijie Shen. (Revision 1480474)\n\n     Result = SUCCESS", "Integrated in Hadoop-Yarn-trunk #204 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/204/])\n    MAPREDUCE-5157. Bring back old sampler related code so that we can support binary compatibility with hadoop-1 sorter example. Contributed by Zhijie Shen. (Revision 1480474)\nMAPREDUCE-5159. Change ValueAggregatorJob to add APIs which can support binary compatibility with hadoop-1 examples. Contributed by Zhijie Shen. (Revision 1480394)\n\n     Result = SUCCESS", "Integrated in Hadoop-Hdfs-trunk #1393 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1393/])\n    MAPREDUCE-5157. Bring back old sampler related code so that we can support binary compatibility with hadoop-1 sorter example. Contributed by Zhijie Shen. (Revision 1480474)\nMAPREDUCE-5159. Change ValueAggregatorJob to add APIs which can support binary compatibility with hadoop-1 examples. Contributed by Zhijie Shen. (Revision 1480394)\n\n     Result = FAILURE", "Integrated in Hadoop-Mapreduce-trunk #1420 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1420/])\n    MAPREDUCE-5157. Bring back old sampler related code so that we can support binary compatibility with hadoop-1 sorter example. Contributed by Zhijie Shen. (Revision 1480474)\nMAPREDUCE-5159. Change ValueAggregatorJob to add APIs which can support binary compatibility with hadoop-1 examples. Contributed by Zhijie Shen. (Revision 1480394)\n\n     Result = FAILURE", "Integrated in Hadoop-trunk-Commit #3938 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3938/])\n    MAPREDUCE-5184. Document compatibility for MapReduce applications in hadoop-2 vis-a-vis hadoop-1. Contributed by Zhijie Shen. (Revision 1493570)\n\n     Result = SUCCESS", "Integrated in Hadoop-Yarn-trunk #243 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/243/])\n    MAPREDUCE-5184. Document compatibility for MapReduce applications in hadoop-2 vis-a-vis hadoop-1. Contributed by Zhijie Shen. (Revision 1493570)\n\n     Result = SUCCESS", "Integrated in Hadoop-Hdfs-trunk #1433 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1433/])\n    MAPREDUCE-5184. Document compatibility for MapReduce applications in hadoop-2 vis-a-vis hadoop-1. Contributed by Zhijie Shen. (Revision 1493570)\n\n     Result = FAILURE", "Integrated in Hadoop-Mapreduce-trunk #1460 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1460/])\n    MAPREDUCE-5184. Document compatibility for MapReduce applications in hadoop-2 vis-a-vis hadoop-1. Contributed by Zhijie Shen. (Revision 1493570)\n\n     Result = SUCCESS", "SUCCESS: Integrated in Hadoop-trunk-Commit #4484 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/4484/])\nMAPREDUCE-5531. Fix compat with hadoop-1 in mapreduce.(TaskID, TaskAttemptID) by re-introducing missing constructors. Contributed by Robert Kanter. (acmurthy: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1527033)\n* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt\n* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/TaskAttemptID.java\n* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/TaskID.java\n", "SUCCESS: Integrated in Hadoop-trunk-Commit #4486 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/4486/])\nMAPREDUCE-5529. Fix compat with hadoop-1 in mapred.TotalOrderPartitioner by re-introducing (get,set)PartitionFile which takes in JobConf. Contributed by Robert Kanter. (acmurthy: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1527048)\n* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt\n* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/TotalOrderPartitioner.java\n", "FAILURE: Integrated in Hadoop-Yarn-trunk #346 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/346/])\nMAPREDUCE-5529. Fix compat with hadoop-1 in mapred.TotalOrderPartitioner by re-introducing (get,set)PartitionFile which takes in JobConf. Contributed by Robert Kanter. (acmurthy: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1527048)\n* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt\n* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/TotalOrderPartitioner.java\nMAPREDUCE-5531. Fix compat with hadoop-1 in mapreduce.(TaskID, TaskAttemptID) by re-introducing missing constructors. Contributed by Robert Kanter. (acmurthy: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1527033)\n* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt\n* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/TaskAttemptID.java\n* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/TaskID.java\n", "SUCCESS: Integrated in Hadoop-Hdfs-trunk #1536 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1536/])\nMAPREDUCE-5529. Fix compat with hadoop-1 in mapred.TotalOrderPartitioner by re-introducing (get,set)PartitionFile which takes in JobConf. Contributed by Robert Kanter. (acmurthy: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1527048)\n* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt\n* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/TotalOrderPartitioner.java\nMAPREDUCE-5531. Fix compat with hadoop-1 in mapreduce.(TaskID, TaskAttemptID) by re-introducing missing constructors. Contributed by Robert Kanter. (acmurthy: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1527033)\n* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt\n* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/TaskAttemptID.java\n* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/TaskID.java\n", "FAILURE: Integrated in Hadoop-Mapreduce-trunk #1562 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1562/])\nMAPREDUCE-5529. Fix compat with hadoop-1 in mapred.TotalOrderPartitioner by re-introducing (get,set)PartitionFile which takes in JobConf. Contributed by Robert Kanter. (acmurthy: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1527048)\n* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt\n* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/TotalOrderPartitioner.java\nMAPREDUCE-5531. Fix compat with hadoop-1 in mapreduce.(TaskID, TaskAttemptID) by re-introducing missing constructors. Contributed by Robert Kanter. (acmurthy: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1527033)\n* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt\n* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/TaskAttemptID.java\n* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/TaskID.java\n", "SUCCESS: Integrated in Hadoop-trunk-Commit #4503 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/4503/])\nMAPREDUCE-5551. Fix compat with hadoop-1 in SequenceFileAsBinaryOutputFormat.WritableValueBytes by re-introducing missing constructors. Contributed by Zhijie Shen. (acmurthy: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1527848)\n* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt\n* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java\n", "FAILURE: Integrated in Hadoop-Yarn-trunk #349 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/349/])\nMAPREDUCE-5551. Fix compat with hadoop-1 in SequenceFileAsBinaryOutputFormat.WritableValueBytes by re-introducing missing constructors. Contributed by Zhijie Shen. (acmurthy: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1527848)\n* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt\n* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java\n", "FAILURE: Integrated in Hadoop-Hdfs-trunk #1539 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1539/])\nMAPREDUCE-5551. Fix compat with hadoop-1 in SequenceFileAsBinaryOutputFormat.WritableValueBytes by re-introducing missing constructors. Contributed by Zhijie Shen. (acmurthy: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1527848)\n* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt\n* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java\n", "FAILURE: Integrated in Hadoop-Mapreduce-trunk #1565 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1565/])\nMAPREDUCE-5551. Fix compat with hadoop-1 in SequenceFileAsBinaryOutputFormat.WritableValueBytes by re-introducing missing constructors. Contributed by Zhijie Shen. (acmurthy: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1527848)\n* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt\n* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java\n", "SUCCESS: Integrated in Hadoop-trunk-Commit #4517 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/4517/])\nMAPREDUCE-5530. Fix compat with hadoop-1 in mapred.lib.CombinFileInputFormat by re-introducing isSplittable(FileSystem, Path) api and ensuring semantic compatibility. Contributed by Robert Kanter. (acmurthy: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1528533)\n* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt\n* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/CombineFileInputFormat.java\n", "FAILURE: Integrated in Hadoop-Yarn-trunk #351 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/351/])\nMAPREDUCE-5530. Fix compat with hadoop-1 in mapred.lib.CombinFileInputFormat by re-introducing isSplittable(FileSystem, Path) api and ensuring semantic compatibility. Contributed by Robert Kanter. (acmurthy: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1528533)\n* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt\n* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/CombineFileInputFormat.java\n", "SUCCESS: Integrated in Hadoop-Hdfs-trunk #1541 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1541/])\nMAPREDUCE-5530. Fix compat with hadoop-1 in mapred.lib.CombinFileInputFormat by re-introducing isSplittable(FileSystem, Path) api and ensuring semantic compatibility. Contributed by Robert Kanter. (acmurthy: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1528533)\n* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt\n* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/CombineFileInputFormat.java\n", "FAILURE: Integrated in Hadoop-Mapreduce-trunk #1567 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1567/])\nMAPREDUCE-5530. Fix compat with hadoop-1 in mapred.lib.CombinFileInputFormat by re-introducing isSplittable(FileSystem, Path) api and ensuring semantic compatibility. Contributed by Robert Kanter. (acmurthy: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1528533)\n* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt\n* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/CombineFileInputFormat.java\n", "SUCCESS: Integrated in HBase-0.98-on-Hadoop-1.1 #181 (See [https://builds.apache.org/job/HBase-0.98-on-Hadoop-1.1/181/])\nHBASE-10644 TestSecureExportSnapshot#testExportFileSystemState fails on hadoop-1 (tedyu: rev 1573115)\n* /hbase/branches/0.98/hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/FsDelegationToken.java\n", "SUCCESS: Integrated in HBase-0.98 #193 (See [https://builds.apache.org/job/HBase-0.98/193/])\nHBASE-10644 TestSecureExportSnapshot#testExportFileSystemState fails on hadoop-1 (tedyu: rev 1573115)\n* /hbase/branches/0.98/hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/FsDelegationToken.java\n", "FAILURE: Integrated in hbase-0.96 #322 (See [https://builds.apache.org/job/hbase-0.96/322/])\nHBASE-10644 TestSecureExportSnapshot#testExportFileSystemState fails on hadoop-1 (tedyu: rev 1573117)\n* /hbase/branches/0.96/hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/FsDelegationToken.java\n", "FAILURE: Integrated in HBase-TRUNK-on-Hadoop-1.1 #103 (See [https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-1.1/103/])\nHBASE-10644 TestSecureExportSnapshot#testExportFileSystemState fails on hadoop-1 (tedyu: rev 1573116)\n* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/FsDelegationToken.java\n", "FAILURE: Integrated in HBase-TRUNK #4968 (See [https://builds.apache.org/job/HBase-TRUNK/4968/])\nHBASE-10644 TestSecureExportSnapshot#testExportFileSystemState fails on hadoop-1 (tedyu: rev 1573116)\n* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/FsDelegationToken.java\n", "FAILURE: Integrated in hbase-0.96-hadoop2 #222 (See [https://builds.apache.org/job/hbase-0.96-hadoop2/222/])\nHBASE-10644 TestSecureExportSnapshot#testExportFileSystemState fails on hadoop-1 (tedyu: rev 1573117)\n* /hbase/branches/0.96/hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/FsDelegationToken.java\n", "FAILURE: Integrated in HBase-0.98-on-Hadoop-1.1 #213 (See [https://builds.apache.org/job/HBase-0.98-on-Hadoop-1.1/213/])\nHBASE-10184 Addendum fixes compilation against hadoop-1 (tedyu: rev 1577393)\n* /hbase/branches/0.98/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestRowCounter.java\n", "FAILURE: Integrated in HBase-TRUNK #5008 (See [https://builds.apache.org/job/HBase-TRUNK/5008/])\nHBASE-10184 Addendum fixes compilation against hadoop-1 (tedyu: rev 1577394)\n* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestRowCounter.java\n", "FAILURE: Integrated in HBase-0.98 #228 (See [https://builds.apache.org/job/HBase-0.98/228/])\nHBASE-10184 Addendum fixes compilation against hadoop-1 (tedyu: rev 1577393)\n* /hbase/branches/0.98/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestRowCounter.java\n", "FAILURE: Integrated in HBase-TRUNK-on-Hadoop-1.1 #117 (See [https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-1.1/117/])\nHBASE-10184 Addendum fixes compilation against hadoop-1 (tedyu: rev 1577394)\n* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestRowCounter.java\n", "SUCCESS: Integrated in HBase-0.98-on-Hadoop-1.1 #270 (See [https://builds.apache.org/job/HBase-0.98-on-Hadoop-1.1/270/])\nHBASE-10948 Revert due to incompatibility with hadoop-1 (tedyu: rev 1588786)\n* /hbase/branches/0.98/hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java\n* /hbase/branches/0.98/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java\n", "SUCCESS: Integrated in HBase-0.98 #286 (See [https://builds.apache.org/job/HBase-0.98/286/])\nHBASE-10948 Revert due to incompatibility with hadoop-1 (tedyu: rev 1588786)\n* /hbase/branches/0.98/hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java\n* /hbase/branches/0.98/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java\n", "FAILURE: Integrated in HBase-0.98-on-Hadoop-1.1 #294 (See [https://builds.apache.org/job/HBase-0.98-on-Hadoop-1.1/294/])\nHBASE-11104 Addendum to fix compilation on hadoop-1 (tedyu: rev ef0ee0ff697192b6202ff62766370fbc9f3dfe38)\n* hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java\n* hbase-it/src/test/java/org/apache/hadoop/hbase/mapreduce/IntegrationTestImportTsv.java\n", "ABORTED: Integrated in HBase-0.98 #313 (See [https://builds.apache.org/job/HBase-0.98/313/])\nHBASE-11104 Addendum to fix compilation on hadoop-1 (tedyu: rev ef0ee0ff697192b6202ff62766370fbc9f3dfe38)\n* hbase-it/src/test/java/org/apache/hadoop/hbase/mapreduce/IntegrationTestImportTsv.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java\n", "SUCCESS: Integrated in HBase-0.98 #432 (See [https://builds.apache.org/job/HBase-0.98/432/])\nHBASE-11516 Track time spent in executing coprocessors in each region - addendum for hadoop-1 (tedyu: rev e02698c52dcfc7f9a2a7c41b9294a138c31ebc5c)\n* hbase-hadoop1-compat/src/test/java/org/apache/hadoop/hbase/regionserver/TestMetricsRegionSourceImpl.java\n", "SUCCESS: Integrated in HBase-0.98-on-Hadoop-1.1 #408 (See [https://builds.apache.org/job/HBase-0.98-on-Hadoop-1.1/408/])\nHBASE-11516 Track time spent in executing coprocessors in each region - addendum for hadoop-1 (tedyu: rev e02698c52dcfc7f9a2a7c41b9294a138c31ebc5c)\n* hbase-hadoop1-compat/src/test/java/org/apache/hadoop/hbase/regionserver/TestMetricsRegionSourceImpl.java\n", "SUCCESS: Integrated in HBase-0.98 #462 (See [https://builds.apache.org/job/HBase-0.98/462/])\nHBASE-11742 Addendum makes compilation on hadoop-1 pass (tedyu: rev 00a9cd3c342b59774e1068c705e454f94d861d14)\n* hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java\n", "SUCCESS: Integrated in HBase-0.98-on-Hadoop-1.1 #435 (See [https://builds.apache.org/job/HBase-0.98-on-Hadoop-1.1/435/])\nHBASE-11742 Addendum makes compilation on hadoop-1 pass (tedyu: rev 00a9cd3c342b59774e1068c705e454f94d861d14)\n* hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java\n", "FAILURE: Integrated in HBase-0.98-on-Hadoop-1.1 #504 (See [https://builds.apache.org/job/HBase-0.98-on-Hadoop-1.1/504/])\nHBASE-12033 SecurityUtil#doAsLoginUser is absent in hadoop-1, making AccessController#postCreateTableHandler() ineffective (tedyu: rev 0c58314f39bdc8c6f619c054bfd47b5c92990b0f)\n* hbase-common/src/main/java/org/apache/hadoop/hbase/security/User.java\n", "FAILURE: Integrated in HBase-0.98 #529 (See [https://builds.apache.org/job/HBase-0.98/529/])\nHBASE-12033 SecurityUtil#doAsLoginUser is absent in hadoop-1, making AccessController#postCreateTableHandler() ineffective (tedyu: rev 0c58314f39bdc8c6f619c054bfd47b5c92990b0f)\n* hbase-common/src/main/java/org/apache/hadoop/hbase/security/User.java\n", "FAILURE: Integrated in HBase-0.98 #648 (See [https://builds.apache.org/job/HBase-0.98/648/])\nHBASE-12406 Bulk load fails in 0.98 against hadoop-1 due to unmatched family name (tedyu: rev 632c286e08d5944d33e49c4614396949891eddc1)\n* hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/LoadIncrementalHFiles.java\n", "FAILURE: Integrated in HBase-0.98-on-Hadoop-1.1 #616 (See [https://builds.apache.org/job/HBase-0.98-on-Hadoop-1.1/616/])\nHBASE-12406 Bulk load fails in 0.98 against hadoop-1 due to unmatched family name (tedyu: rev 632c286e08d5944d33e49c4614396949891eddc1)\n* hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/LoadIncrementalHFiles.java\n", "FAILURE: Integrated in HBase-0.98-on-Hadoop-1.1 #656 (See [https://builds.apache.org/job/HBase-0.98-on-Hadoop-1.1/656/])\nHBASE-12539 Addendum fixes compilation against hadoop-1 (tedyu: rev 8dcb03c24530b66c6e93b1016cd25d5e5f10f0ae)\n* hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileLinkCleaner.java\n", "FAILURE: Integrated in HBase-0.98 #689 (See [https://builds.apache.org/job/HBase-0.98/689/])\nHBASE-12539 Addendum fixes compilation against hadoop-1 (tedyu: rev 8dcb03c24530b66c6e93b1016cd25d5e5f10f0ae)\n* hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileLinkCleaner.java\n", "SUCCESS: Integrated in HBase-0.98 #745 (See [https://builds.apache.org/job/HBase-0.98/745/])\nHBASE-12493 User class should provide a way to re-use existing token - addendum for hadoop-1 (tedyu: rev d6d22113c93d252b6b206d74e98da73273ba5a3a)\n* hbase-client/src/main/java/org/apache/hadoop/hbase/security/token/TokenUtil.java\n", "SUCCESS: Integrated in HBase-0.98-on-Hadoop-1.1 #711 (See [https://builds.apache.org/job/HBase-0.98-on-Hadoop-1.1/711/])\nHBASE-12493 User class should provide a way to re-use existing token - addendum for hadoop-1 (tedyu: rev d6d22113c93d252b6b206d74e98da73273ba5a3a)\n* hbase-client/src/main/java/org/apache/hadoop/hbase/security/token/TokenUtil.java\n", "SUCCESS: Integrated in gora-trunk #1611 (See [https://builds.apache.org/job/gora-trunk/1611/])\nGORA-417 Deploy Hadoop-1 compatible binaries for Apache Gora (lewis.j.mcgibbney: rev 983bed4f395e876c75d8419eddac9806f45a24da)\n* pom.xml\nGORA-417 Deploy Hadoop-1 compatible binaries for Apache Gora this closes #33 (lewis.j.mcgibbney: rev 97d5845100ee1460da36e0d94c504a125a081834)\n* CHANGES.txt\n", "FAILURE: Integrated in HBase-0.98-on-Hadoop-1.1 #1078 (See [https://builds.apache.org/job/HBase-0.98-on-Hadoop-1.1/1078/])\nHBASE-13250 Revert due to compilation error against hadoop-1 profile (tedyu: rev 38995fbd51ac4735b673dd1527cb2631b69b7474)\n* hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java\n", "FAILURE: Integrated in HBase-0.98 #1125 (See [https://builds.apache.org/job/HBase-0.98/1125/])\nHBASE-13250 Revert due to compilation error against hadoop-1 profile (tedyu: rev 38995fbd51ac4735b673dd1527cb2631b69b7474)\n* hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java\n"], "derived": {"summary": "The initial code for Hadoop will be copied from Nutch.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "initial import of code from Nutch - The initial code for Hadoop will be copied from Nutch."}, {"q": "What updates or decisions were made in the discussion?", "a": "FAILURE: Integrated in HBase-0.98 #1125 (See [https://builds.apache.org/job/HBase-0.98/1125/])\nHBASE-13250 Revert due to compilation error against hadoop-1 profile (tedyu: rev 38995fbd51ac4735b673dd1527cb2631b69b7474)\n* hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-16", "title": "RPC call times out while indexing map task is computing splits", "status": "Closed", "priority": "Major", "reporter": "Chris Schneider", "assignee": "Mike Cafarella", "labels": [], "created": "2006-02-01T08:25:38.000+0000", "updated": "2009-07-08T16:51:40.000+0000", "description": "We've been using Nutch 0.8 (MapReduce) to perform some internet crawling. Things seemed to be going well until...\n\n060129 222409 Lost tracker 'tracker_56288'\n060129 222409 Task 'task_m_10gs5f' has been lost.\n060129 222409 Task 'task_m_10qhzr' has been lost.\n   ........\n   ........\n060129 222409 Task 'task_r_zggbwu' has been lost.\n060129 222409 Task 'task_r_zh8dao' has been lost.\n060129 222455 Server handler 8 on 8010 caught: java.net.SocketException: Socket closed\njava.net.SocketException: Socket closed\n        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:99)\n        at java.net.SocketOutputStream.write(SocketOutputStream.java:136)\n        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)\n        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)\n        at java.io.DataOutputStream.flush(DataOutputStream.java:106)\n        at org.apache.nutch.ipc.Server$Handler.run(Server.java:216)\n060129 222455 Adding task 'task_m_cia5po' to set for tracker 'tracker_56288'\n060129 223711 Adding task 'task_m_ffv59i' to set for tracker 'tracker_25647'\n\nI'm hoping that someone could explain why task_m_cia5po got added to tracker_56288 after this tracker was lost.\n\nThe Crawl .main process died with the following output:\n\n060129 221129 Indexer: adding segment: /user/crawler/crawl-20060129091444/segments/20060129200246\nException in thread \"main\" java.io.IOException: timed out waiting for response\n    at org.apache.nutch.ipc.Client.call(Client.java:296)\n    at org.apache.nutch.ipc.RPC$Invoker.invoke(RPC.java:127)\n    at $Proxy1.submitJob(Unknown Source)\n    at org.apache.nutch.mapred.JobClient.submitJob(JobClient.java:259)\n    at org.apache.nutch.mapred.JobClient.runJob(JobClient.java:288)\n    at org.apache.nutch.indexer.Indexer.index(Indexer.java:263)\n    at org.apache.nutch.crawl.Crawl.main(Crawl.java:127)\n\nHowever, it definitely seems as if the JobTracker is still waiting for the job to finish (no failed jobs).\n\nDoug Cutting's response:\nThe bug here is that the RPC call times out while the map task is computing splits.  The fix is that the job tracker should not compute splits until after it has returned from the submitJob RPC.  Please submit a bug in Jira to help remind us to fix this.\n", "comments": ["I'm seeing a similar problem with the latest code. I have a task that takes a lot of input files (currently ~1400, each several gigabytes each). The amount of time spent hitting hasTaskWithHit() for each task is exhorbitant, easily causing a timeout. This code does all of the work up front - if you had, say, 1400 tasks, and 2 taskrunners, you could easily dispatch the first 2xsimultaneous running tasks first, then go back and spend time calculating the rest of the matchups. Certainly, a lot of the steps in the job setup after listFiles() could be potentially slow for certain problem sizes.", "Everyone should probably be made aware of the strange behavior we see during indexing, at least for a relatively large number of large segments (topN=500K, depth=20) with a relatively large crawldb (50M URLs). Note that this was all performed with ipc.client.timeout set to 30 minutes.\n\nAfter launching the indexing job, the web UI shows all of the TaskTrackers, but the numbers in the \"Secs since heartbeat\" column just keep increasing. This goes on for about 10 minutes until the JobTracker finally loses all of them (and the tasks they were working on), as is shown in its log:\n\n060210 224115 parsing file:/home/crawler/nutch/conf/nutch-site.xml\n060210 225151 Lost tracker 'tracker_37064'\n060210 225151 Task 'task_m_4ftk58' has been lost.\n060210 225151 Task 'task_m_6ww2ri' has been lost.\n\n...(snip)...\n\n060210 225151 Task 'task_r_y6d190' has been lost.\n060210 225151 Lost tracker 'tracker_92921'\n060210 225151 Task 'task_m_9p24at' has been lost.\n\n...(etc)...\n\nAt this point, the web UI is still up, the job shows 0% complete, and the TaskTrackers table is empty. It goes on for an hour or so like this, during which any rational person would probably want to kill the job and start over.\n\nDon't do this! Keep the faith!!!\n\nAbout an hour later, the JobTracker magically reestablishes its connection to the TaskTrackers (which now have new names), as is shown in its log:\n\n060210 225151 Task 'task_r_yj3y3o' has been lost.\n060210 235403 Adding task 'task_m_k9u9a8' to set for tracker 'tracker_85874'\n060210 235404 Adding task 'task_m_pijt4q' to set for tracker 'tracker_61888'\n\n...(etc)...\n\nThe web UI also shows that the TaskTrackers are back (with their new names).\n\nThere's nothing in the TaskTracker logs during the initial 10 minutes, then a bunch of exiting and closing messages, until finally the TaskTrackers start \"Reinitializing local state\":\n\n060210 225403 Stopping server on 50050\n060210 230102 Server handler 4 on 50050: exiting\n\n...(snip)...\n\n060210 230105 Server handler 7 on 50050: exiting\n060210 232024 Server listener on port 50050: exiting\n060210 232403 Stopping server on 50040\n060210 234902 Server listener on port 50040: exiting\n060210 234925 Server connection on port 50040 from 192.168.1.5: exiting\n\n...(snip)...\n\n060210 235009 Server connection on port 50040 from 192.168.1.10: exiting\n060210 235013 Client connection to 192.168.1.4:50040: closing\n060210 235014 Client connection to 192.168.1.7:50040: closing\n060210 235015 Server connection on port 50040 from 192.168.1.7: exiting\n060210 235016 Server handler 0 on 50040: exiting\n\n...(snip)...\n\n060210 235024 Server handler 2 on 50040: exiting\n060210 235403 Reinitializing local state\n060210 235403 Server listener on port 50050: starting\n060210 235403 Server handler 0 on 50050: starting\n\n...(etc)...\n\nDuring the time that the TaskTrackers are lost, neither the master nor the slave machines seem to be using much of the CPU or RAM, and the DataNode logs are quiet. I suppose that it's probably I/O bound on the master machine, but even that seems mysterious to me. It would seem particularly inappropriate for the JobTracker to punt the TaskTrackers because the master was too busy to listen for their heartbeats.\n\nAt any rate, once the TaskTrackers go through the \"Reinitializing local state\" thing, the indexing job seems to proceed normally, and it eventually completes with no errors.\n", "Looking further into my concern about hasTaskWithCacheHit overhead (called from obtainNewMapTask in JobInProgress).... it looks like obtainNewMapTask actually calls hasTaskWithCacheHit once for each map job, but always chooses the first choice - why even call the relatively expensive hasTaskWithCacheHit if you've already chosen a value for cacheTarget? Likewise, once you've filled in both cacheTarget and hasTarget, why not break out of that for loop entirely?\n\nAm I following the code flow incorrectly, or is obtainNewMapTask essentially making maps^2 RPC calls in response to a single incoming RPC call? Even if the jobtracker and namenode are running on the same host, that could easily explain the sensitivity to timeouts in the current code.\n", "Bryan: I agree, this looks like a serious bug.  The TaskTracker should minimize the calls it makes to the NameNode.  Ideally it should only make a single call per job on each input split.", "\n  Here's a patch for the problem.  I changed two things.\n\n  1) The comment is right, there's no need to iterate through all choices in JobTracker\nafter we find a TaskInProgress to take the task.\n\n  2) Within a TaskInProgress (TIP), which tracks an individual split within a Job, we cache\nthe results of the getHints() call to the Distributed File System.\n", "\n  Sorry, my blurb above was a little unclear.\n\n  I should have said:\n\n  1)  Bryan's comment is right, we don't need to iterate through the whole\nlist in JobTracker's obtainNewMapTask call.  We now just do it until we\nfind a good cacheTarget or stdTarget value.\n\n  2) A TIP object tracks each individual split in the Job.  We cache\nthe data at each TIP.  This will be handy in case the TIP has to\nbe re-executed due to machine failure.  \n\n  I don't mind caching the hints aggressively, because it's \njust task-placement we're after.  If the hint is wrong (which only happens \nin case of machine failure), we might send the task to a suboptimal \nmachine.  No big deal.\n\n", "\n  Patch comitted.", "I'm not sure this patch does quite what's desired.\n\nIt looks like before it would try as hard as possible to find a task with a cache hit, then fail to running either just the first executable task. Now, it will search for the first task that's either a cache hit *or* executable..... In principle, wouldn't you only want to break out of the loop for finding a cache hit? This, of course, still causes timeouts - is there any way to actually precompute the list of cache hits, so that it's just a matter of picking a task from a priority queue?", "Of course, given the existing code, maybe it's worth trying a little harder - iterate through the list as before, but keep track of the time that's been taken, and give up if it gets to 1/2 of the RPC timeout, or something of the like.", "But, you're right, Bryan, I think this is still not optimal.  It should certainly check to see if more map tasks have local input data for the calling node before it gives up.  Ideally it should not be iterating through all map tasks either, but rather create a mapping of node -> mapTask* for tasks that have local data for a node.  We could keep a queue of tasks whose entries in this table have not yet been computed.  When we fail to find a task with local data for a node, then we can pop a few (10?) entries off the queue and enter them in the map, and if there are still no matches, just give the node a task with remote input data.  This way we'd avoid ever doing too much work in a single call, and ever iterating over all tasks.", "I have another idea.\n\nFirst, switch the delegation of responsiblity for job assignment. Right now, it happens in the JobTracker instance, in response to an obtainNewMapTask call. This scales very poorly. In particular, it causes the RPC-timeouts if you do any sort of serious work in obtainNewMapTask. There's another bug, just reported as HADOOP-43, which occurs if you spend too long in a RPC call, and also described in the second comment, above.\n\nSo, instead of having the JobTracker do this reactively, either:\n1) Precompute - probably most scalably done by starting a mini-job, which just computes the list of who has precached data from a given FileSplit.\n2) Compute on demand - as a TaskTracker job. This could work by some protocol of offering the TaskTracker a set of possible jobs it could do work on, letting it pick the ones it thinks are best, and return the remainders for assignment. This, of course, would only work well for instances where tasks >> tasktracker instances.\n\nI looked at implementing something like 1, but decided I think 2 is a much better option. 1 would put a lot more instantaneous demand on the namenode. Plus, once you've finished precomputing the best nodes, if nodes come or go you don't really have a solution. 2 seems to distribute both the work and some of the demand, and it makes it possible for the cluster to grow or shrink dramatically without failing to take advantage of the local storage available at each node. Unfortunately, without any pre-work, it's possible that, doing option 2, you'd pick bad subsets of work to distribute to each node, and get no local I/O improvement at all.\n\nI'd really like to see something done, perferably soon. With dozens of nodes, and hundreds of gbs of data in my current problem set, it's very nearly impossible to get the current code to make progress, without killing tasktrackers (some with lots of work units already completed). I can do some of the coding, if there's agreement for what direction to push.", "I'm re-opening this & assigning it to Mike.\n\nI think we can fix this by changing the TaskTracker to incrementally calculate things, as I alluded above.  The TaskTracker should avoid ever doing anything that might take very long.  RPC timeouts to the TaskTracker are very bad and must be avoided.\n\nWe can maintain a mapping of taskTracker->split, initially empty, and a queue of splits, initially filled with all of the splits.  (If basic split-generation is too expensive, since it calls file-length on each input file,, then we can eventually change the split API into a generator, which incrementally enumerates splits and use that in place of this queue.)  When a request for a task arrives from a tasktracker we can first examine the table.  If any splits are present for the calling tracker, then we return one and remove it from the table.  Otherwise we pop a constant number of splits (10?) off of the queue, enumerate the tasktrackers that host each split by calling the namenode, and add entries to the table.  Then we consult the table again.  In most cases (many more splits than tasktrackers) we should identify suitable splits.  If we fail then we assign a randomly selected, non-local split to the tasktracker.  Make sense?", "Eric Baldeschwieler wrote:\n> [...] why not just dedicate a thread  \n> to planning and then load a complete plan?  That can produce more  \n> optimal placement and a simpler to understand initialization sequence.\n\nA separate thread in the JobTracker?  That could be a good approach.  We'd have a queue of submitted but as-yet unplanned jobs.  The thread can then pop a job off the queue, compute its splits, then start populating a tasktracker->split table.  When tasktrackers poll for work they can consult this table, potentially while the thread is still populating it.\n\nI'm hesitant to move this out of the JobTracker into the TaskTracker, since that introduces complexity.  But a single thread in the JobTracker should be simple to add and should mostly solve this.  +1", "My only concern is that this doesn't scale as well for really huge jobs, but, if implemented as you suggest (pre-filling, hand out non-matching jobs when you don't have matched jobs) seems like a reasonable trade off. \n\nI'd still think it'd be nice to better handle the case of newly-discovered tasktrackers, but that's probably much more minor than other scaling/responsiveness issues. Maybe a future enhancement for the JobTracker thread that does this work.", "Having a separate thread in JobTracker can even pre-compute a data locality map that will enable optimal task assignment to task trackers.\nHere is the basic idea. As the thread computes the splits for a job, it can determine which nodes have local data for each split. The thread can populate \na map mapping a node to a  list of the splits whose data are on the node. When a tasktracker polls for a task, the JobTracker can look up the map by \nthe node name of the task tracker to see whether there are any un-assigned splits in the corresponding list. If yes,  assign one of them to the task tracker. \nOtherwise, randomlly choose a split for the task tracker.   ", "\n  I created a separate thread on the JobTracker that handles file-splitting and\ngathering info about split-caches.  The job is placed into the PREP state\nuntil it is processed by this JobTracker thread.  After processing, it moves\ninto the RUNNING state.  \n\n  This should allow the hard split-work to continue but still allow the JobTracker\nto process heartbeats.  For now, the thread lives at the JobTracker but perhaps\nsomeday we'll move it to a TaskTracker thread.\n\n  I also fixed the alg for choosing a task-allocation given a tasktracker.\n(Whether cached, non-cached, or speculative.)\n\n  Let me know if this fixes some of the problems you've been seeing. \n\n", "I applied Mike's patch, with a few small changes, fixing an NPE and ArrayOutOfBounds in dfs.\n\nThere's still a bug where, with mapred.tasktracker.tasks.maximum=2, only a single map and a single reduce are running on each node.  I believe the intent is that there should be up to two of each related to a single job, so that the reduce tasks can be copying data while the maps are still running.", "This is great, and finally fixes my issues with a large job that would never start.\n\nHowever, the way things are in this patch (and the current code), the job doesn't get started until the background thread finishes computing all of the cache hints. This takes far too long - it took 15 minutes on a recent run. During that time, of course, no other work was getting done. How about moving the cachedHints-filling-loop to the end of initTasks(), and go ahead and set the job to RUNNING and \"tasksInited=true\" in the meantime?\n\nDoing this locally lets work commence immediately, while the cache hints continue to get filled in for future task allocations."], "derived": {"summary": "We've been using Nutch 0. 8 (MapReduce) to perform some internet crawling.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "RPC call times out while indexing map task is computing splits - We've been using Nutch 0. 8 (MapReduce) to perform some internet crawling."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is great, and finally fixes my issues with a large job that would never start.\n\nHowever, the way things are in this patch (and the current code), the job doesn't get started until the background thread finishes computing all of the cache hints. This takes far too long - it took 15 minutes on a recent run. During that time, of course, no other work was getting done. How about moving the cachedHints-filling-loop to the end of initTasks(), and go ahead and set the job to RUNNING and \"tasksInited=true\" in the meantime?\n\nDoing this locally lets work commence immediately, while the cache hints continue to get filled in for future task allocations."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-17", "title": "tool to mount ndfs on linux", "status": "Closed", "priority": "Major", "reporter": "John Xing", "assignee": null, "labels": [], "created": "2006-02-03T15:59:31.000+0000", "updated": "2006-08-03T17:46:25.000+0000", "description": "tool to mount ndfs on linux. It depends on fuse and fuse-j.", "comments": ["It works with current nutch-0.8-dev. Will be ported to hadoop after ndfs is moved."], "derived": {"summary": "tool to mount ndfs on linux. It depends on fuse and fuse-j.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "tool to mount ndfs on linux - tool to mount ndfs on linux. It depends on fuse and fuse-j."}, {"q": "What updates or decisions were made in the discussion?", "a": "It works with current nutch-0.8-dev. Will be ported to hadoop after ndfs is moved."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-2", "title": "Reused Keys and Values fail with a Combiner", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-02-04T05:54:30.000+0000", "updated": "2015-08-11T07:18:50.000+0000", "description": "If the map function reuses the key or value by destructively modifying it after the output.collect(key,value) call and your application uses a combiner, the data is corrupted by having lots of instances with the last key or value.", "comments": ["The reason that new objects are allocated during map is so that new objects will be available for the combiner.  So fixing HADOOP-110 is easy, but HADOOP-2 must be fixed first, since it is the underlying problem.", "This patch clones the keys and values before they are cached in the CombiningCollector.\nIt adds a new method named WritableUtils.clone(Writable, JobConf) that copies the the given Writable.", "I just committed this.  Thanks Owen!", "Integrated in HBase-TRUNK #3371 (See [https://builds.apache.org/job/HBase-TRUNK/3371/])\n    HBASE-6869 Update our hadoop-2 to 2.0.1-alpha (Revision 1389071)\n\n     Result = FAILURE", "Integrated in HBase-TRUNK-on-Hadoop-2.0.0 #189 (See [https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/189/])\n    HBASE-6869 Update our hadoop-2 to 2.0.1-alpha (Revision 1389071)\n\n     Result = FAILURE", "Integrated in flume-trunk #320 (See [https://builds.apache.org/job/flume-trunk/320/])\n    FLUME-1653: Update Hadoop-23 profile to point to hadoop-2 alpha artifacts (Revision 831a86fc5501a8624b184ea65e53749df31692b8)\n\n     Result = SUCCESS", "Integrated in Accumulo-1.5-Hadoop-2.0 #75 (See [https://builds.apache.org/job/Accumulo-1.5-Hadoop-2.0/75/])\n    ACCUMULO-804: start-dfs.sh is in a different place in hadoop-2 (Revision 1467401)\n\n     Result = UNSTABLE", "Integrated in Accumulo-1.5 #76 (See [https://builds.apache.org/job/Accumulo-1.5/76/])\n    ACCUMULO-804: start-dfs.sh is in a different place in hadoop-2 (Revision 1467401)\n\n     Result = SUCCESS", "Integrated in Hadoop-trunk-Commit #3938 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3938/])\n    MAPREDUCE-5184. Document compatibility for MapReduce applications in hadoop-2 vis-a-vis hadoop-1. Contributed by Zhijie Shen. (Revision 1493570)\n\n     Result = SUCCESS", "Integrated in Hadoop-Yarn-trunk #243 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/243/])\n    MAPREDUCE-5184. Document compatibility for MapReduce applications in hadoop-2 vis-a-vis hadoop-1. Contributed by Zhijie Shen. (Revision 1493570)\n\n     Result = SUCCESS", "Integrated in Hadoop-Hdfs-trunk #1433 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1433/])\n    MAPREDUCE-5184. Document compatibility for MapReduce applications in hadoop-2 vis-a-vis hadoop-1. Contributed by Zhijie Shen. (Revision 1493570)\n\n     Result = FAILURE", "Integrated in Hadoop-Mapreduce-trunk #1460 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1460/])\n    MAPREDUCE-5184. Document compatibility for MapReduce applications in hadoop-2 vis-a-vis hadoop-1. Contributed by Zhijie Shen. (Revision 1493570)\n\n     Result = SUCCESS", "Integrated in Hive-trunk-h0.21 #2175 (See [https://builds.apache.org/job/Hive-trunk-h0.21/2175/])\n    HIVE-4436 : hive.exec.parallel=true doesn't work on hadoop-2\n (Gopal V via Navis) (Revision 1498773)\n\n     Result = FAILURE", "Integrated in Hive-trunk-hadoop2 #269 (See [https://builds.apache.org/job/Hive-trunk-hadoop2/269/])\n    HIVE-4436 : hive.exec.parallel=true doesn't work on hadoop-2\n (Gopal V via Navis) (Revision 1498773)\n\n     Result = FAILURE", "FAILURE: Integrated in HBase-TRUNK #4578 (See [https://builds.apache.org/job/HBase-TRUNK/4578/])\nHBASE-9687 Revert due to some TestExportSnapshot failure on hadoop-2 (tedyu: rev 1527854)\n* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java\n", "FAILURE: Integrated in hbase-0.96 #111 (See [https://builds.apache.org/job/hbase-0.96/111/])\nHBASE-9687 Revert due to some TestExportSnapshot failure on hadoop-2 (tedyu: rev 1527853)\n* /hbase/branches/0.96/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java\n", "FAILURE: Integrated in hbase-0.96-hadoop2 #67 (See [https://builds.apache.org/job/hbase-0.96-hadoop2/67/])\nHBASE-9687 Revert due to some TestExportSnapshot failure on hadoop-2 (tedyu: rev 1527853)\n* /hbase/branches/0.96/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java\n", "FAILURE: Integrated in HBase-TRUNK-on-Hadoop-2.0.0 #769 (See [https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/769/])\nHBASE-9687 Revert due to some TestExportSnapshot failure on hadoop-2 (tedyu: rev 1527854)\n* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java\n", "SUCCESS: Integrated in HBase-TRUNK #5083 (See [https://builds.apache.org/job/HBase-TRUNK/5083/])\nHBASE-10956 Upgrade hadoop-2 dependency to 2.4.0 (tedyu: rev 1587264)\n* /hbase/trunk/pom.xml\n", "FAILURE: Integrated in Ambari-branch-1.7.0 #43 (See [https://builds.apache.org/job/Ambari-branch-1.7.0/43/])\nAMBARI-7229. Stack version must start with 2. to be considered Hadoop-2.x compatible. (jaimin) (jaimin: http://git-wip-us.apache.org/repos/asf?p=ambari.git&a=commit&h=25b5adc0e9233bffbbda877d807936c156de358d)\n* ambari-web/app/app.js\n* ambari-web/test/app_test.js\n* ambari-web/app/mappers/stack_service_mapper.js\n", "FAILURE: Integrated in Ambari-trunk-Commit #533 (See [https://builds.apache.org/job/Ambari-trunk-Commit/533/])\nAMBARI-7229. Stack version must start with 2. to be considered Hadoop-2.x compatible. (jaimin) (jaimin: http://git-wip-us.apache.org/repos/asf?p=ambari.git&a=commit&h=ab532a54bd2ebf851edeb82a3866a56f72a54557)\n* ambari-web/test/app_test.js\n* ambari-web/app/mappers/stack_service_mapper.js\n* ambari-web/app/app.js\n", "SUCCESS: Integrated in HBase-1.2-IT #82 (See [https://builds.apache.org/job/HBase-1.2-IT/82/])\nHBASE-5878 Use getVisibleLength public api from HdfsDataInputStream from Hadoop-2. (apurtell: rev 7f33e6330a37b0401c2f9143ddbea67361217453)\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogReader.java\n", "FAILURE: Integrated in HBase-1.3 #99 (See [https://builds.apache.org/job/HBase-1.3/99/])\nHBASE-5878 Use getVisibleLength public api from HdfsDataInputStream from Hadoop-2. (apurtell: rev 0862abd6599a6936fb8079f4c70afc660175ba11)\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogReader.java\n", "FAILURE: Integrated in HBase-0.98 #1072 (See [https://builds.apache.org/job/HBase-0.98/1072/])\nHBASE-5878 Use getVisibleLength public api from HdfsDataInputStream from Hadoop-2. (apurtell: rev b69569f512068d795199310ce662ab381bb6b6b7)\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogReader.java\nRevert \"HBASE-5878 Use getVisibleLength public api from HdfsDataInputStream from Hadoop-2.\" (apurtell: rev fabfb423f9cf48ddd52e9583ca66600004f42349)\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogReader.java\n", "FAILURE: Integrated in HBase-TRUNK #6712 (See [https://builds.apache.org/job/HBase-TRUNK/6712/])\nHBASE-5878 Use getVisibleLength public api from HdfsDataInputStream from Hadoop-2. (apurtell: rev 6e8cdec242b6c40c09601982bad0a79a569e66c4)\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogReader.java\n", "FAILURE: Integrated in HBase-1.2 #99 (See [https://builds.apache.org/job/HBase-1.2/99/])\nHBASE-5878 Use getVisibleLength public api from HdfsDataInputStream from Hadoop-2. (apurtell: rev 7f33e6330a37b0401c2f9143ddbea67361217453)\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogReader.java\n", "FAILURE: Integrated in HBase-0.98-on-Hadoop-1.1 #1026 (See [https://builds.apache.org/job/HBase-0.98-on-Hadoop-1.1/1026/])\nHBASE-5878 Use getVisibleLength public api from HdfsDataInputStream from Hadoop-2. (apurtell: rev b69569f512068d795199310ce662ab381bb6b6b7)\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogReader.java\nRevert \"HBASE-5878 Use getVisibleLength public api from HdfsDataInputStream from Hadoop-2.\" (apurtell: rev fabfb423f9cf48ddd52e9583ca66600004f42349)\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogReader.java\n", "FAILURE: Integrated in HBase-1.3-IT #82 (See [https://builds.apache.org/job/HBase-1.3-IT/82/])\nHBASE-5878 Use getVisibleLength public api from HdfsDataInputStream from Hadoop-2. (apurtell: rev 0862abd6599a6936fb8079f4c70afc660175ba11)\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogReader.java\n"], "derived": {"summary": "If the map function reuses the key or value by destructively modifying it after the output. collect(key,value) call and your application uses a combiner, the data is corrupted by having lots of instances with the last key or value.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Reused Keys and Values fail with a Combiner - If the map function reuses the key or value by destructively modifying it after the output. collect(key,value) call and your application uses a combiner, the data is corrupted by having lots of instances with the last key or value."}, {"q": "What updates or decisions were made in the discussion?", "a": "FAILURE: Integrated in HBase-1.3-IT #82 (See [https://builds.apache.org/job/HBase-1.3-IT/82/])\nHBASE-5878 Use getVisibleLength public api from HdfsDataInputStream from Hadoop-2. (apurtell: rev 0862abd6599a6936fb8079f4c70afc660175ba11)\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogReader.java"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-3", "title": "Output directories are not cleaned up before the reduces run", "status": "Closed", "priority": "Minor", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-02-04T05:58:10.000+0000", "updated": "2017-12-05T04:19:27.000+0000", "description": "The output directory for the reduces is not cleaned up and therefore if you can see left overs from previous runs, if they had more reduces. For example, if you run the application once with reduces=10 and then rerun with reduces=8, your output directory will have frag00000 to frag00009 with the first 8 fragments from the second run and the last 2 fragments from the first run.", "comments": ["This patch makes the driver process delete the output directory before submitting the job.", "An even safer way to fix this would be to have JobClient throw an exception if the output directory already exists.  That way folks won't inadvertantly overwrite things.", "Ok, this patch ensures that the output directory is set and does not exist.\nIf the application wants to clobber old data, they need to delete the files themselves.\nI added the check for the output directory being set, because otherwise the job doesn't fail until\nthe reduces try to run. With the added check, they fail before they are submitted. \n\nI wasn't sure we wanted to support the no reduces case, but it was pretty easy to handle here by\nnot requiring an output directory.", "I just committed this.  I converted under_scored variable names to camelCase.", "FAILURE: Integrated in Jenkins build HBase-2.0 #17 (See [https://builds.apache.org/job/HBase-2.0/17/])\nHBASE-18033 Fix license check for hadoop-3.x (busbey: rev eb5c5a9bc8aa3e39b7da87d0a4130da480ba3a95)\n* (edit) hbase-resource-bundle/src/main/resources/META-INF/LICENSE.vm\n* (edit) hbase-resource-bundle/src/main/resources/supplemental-models.xml\n", "FAILURE: Integrated in Jenkins build HBase-Trunk_matrix #3166 (See [https://builds.apache.org/job/HBase-Trunk_matrix/3166/])\nHBASE-18033 Fix license check for hadoop-3.x (busbey: rev a6216db16f7fa3342f9ab16a52b46270aea5b4ae)\n* (edit) hbase-resource-bundle/src/main/resources/META-INF/LICENSE.vm\n* (edit) hbase-resource-bundle/src/main/resources/supplemental-models.xml\n", "FAILURE: Integrated in Jenkins build HBase-1.3-JDK7 #290 (See [https://builds.apache.org/job/HBase-1.3-JDK7/290/])\nHBASE-17441 Fix invalid quoting around hadoop-3 build in yetus (busbey: rev d01b15f83fd465b1b75380122efe1e87b7909bed)\n* (edit) dev-support/hbase-personality.sh\n", "FAILURE: Integrated in Jenkins build HBase-1.2-JDK7 #228 (See [https://builds.apache.org/job/HBase-1.2-JDK7/228/])\nHBASE-17441 Fix invalid quoting around hadoop-3 build in yetus (busbey: rev 54ecaa94e4beb0b6d3a08de4b138fe193a804017)\n* (edit) dev-support/hbase-personality.sh\n", "SUCCESS: Integrated in Jenkins build HBase-1.3-IT #218 (See [https://builds.apache.org/job/HBase-1.3-IT/218/])\nHBASE-17441 Fix invalid quoting around hadoop-3 build in yetus (busbey: rev d01b15f83fd465b1b75380122efe1e87b7909bed)\n* (edit) dev-support/hbase-personality.sh\n", "SUCCESS: Integrated in Jenkins build HBase-1.2-IT #961 (See [https://builds.apache.org/job/HBase-1.2-IT/961/])\nHBASE-17441 Fix invalid quoting around hadoop-3 build in yetus (busbey: rev 54ecaa94e4beb0b6d3a08de4b138fe193a804017)\n* (edit) dev-support/hbase-personality.sh\n", "FAILURE: Integrated in Jenkins build HBase-2.0 #609 (See [https://builds.apache.org/job/HBase-2.0/609/])\nHBASE-17441 Fix invalid quoting around hadoop-3 build in yetus (elserj: rev 8c6ed571bdbbd3eb5fe38aa0833124ca75c19cfa)\n* (edit) dev-support/hbase-personality.sh\n", "FAILURE: Integrated in Jenkins build HBase-Trunk_matrix #3810 (See [https://builds.apache.org/job/HBase-Trunk_matrix/3810/])\nHBASE-17441 Fix invalid quoting around hadoop-3 build in yetus (elserj: rev 869b90c612ebf2f931dcd43be59b817e4b55ab32)\n* (edit) dev-support/hbase-personality.sh\n", "SUCCESS: Integrated in Jenkins build HBase-1.1-JDK7 #1915 (See [https://builds.apache.org/job/HBase-1.1-JDK7/1915/])\nHBASE-17441 Fix invalid quoting around hadoop-3 build in yetus (busbey: rev ea67acab70f87eb3a15119c2f4a393e2a12d8527)\n* (edit) dev-support/hbase-personality.sh\n", "SUCCESS: Integrated in Jenkins build HBase-1.1-JDK8 #1999 (See [https://builds.apache.org/job/HBase-1.1-JDK8/1999/])\nHBASE-17441 Fix invalid quoting around hadoop-3 build in yetus (busbey: rev ea67acab70f87eb3a15119c2f4a393e2a12d8527)\n* (edit) dev-support/hbase-personality.sh\n", "FAILURE: Integrated in Jenkins build HBase-1.5 #82 (See [https://builds.apache.org/job/HBase-1.5/82/])\nHBASE-17441 Fix invalid quoting around hadoop-3 build in yetus (busbey: rev 61173522dbbbbdd38f6a0aa8363e4421e910b839)\n* (edit) dev-support/hbase-personality.sh\n", "SUCCESS: Integrated in Jenkins build HBase-1.4 #938 (See [https://builds.apache.org/job/HBase-1.4/938/])\nHBASE-17441 Fix invalid quoting around hadoop-3 build in yetus (busbey: rev c3697bd2739db6de3ba337efadb7e543113a6477)\n* (edit) dev-support/hbase-personality.sh\n", "SUCCESS: Integrated in Jenkins build HBase-1.2-JDK8 #224 (See [https://builds.apache.org/job/HBase-1.2-JDK8/224/])\nHBASE-17441 Fix invalid quoting around hadoop-3 build in yetus (busbey: rev 54ecaa94e4beb0b6d3a08de4b138fe193a804017)\n* (edit) dev-support/hbase-personality.sh\n", "SUCCESS: Integrated in Jenkins build HBase-1.3-JDK8 #301 (See [https://builds.apache.org/job/HBase-1.3-JDK8/301/])\nHBASE-17441 Fix invalid quoting around hadoop-3 build in yetus (busbey: rev d01b15f83fd465b1b75380122efe1e87b7909bed)\n* (edit) dev-support/hbase-personality.sh\n", "FAILURE: Integrated in Jenkins build HBase-1.4 #1049 (See [https://builds.apache.org/job/HBase-1.4/1049/])\nHBASE-18942 hbase-hadoop2-compat module ignores hadoop-3 profile (apurtell: rev 9c8c0e08a66ced7dffb232facc51079e1de8a722)\n* (edit) hbase-hadoop2-compat/pom.xml\n", "FAILURE: Integrated in Jenkins build HBase-Trunk_matrix #4170 (See [https://builds.apache.org/job/HBase-Trunk_matrix/4170/])\nHBASE-18942 hbase-hadoop2-compat module ignores hadoop-3 profile (apurtell: rev 6d69fd9c7871ffc3878eefa307da0e49bb7205f5)\n* (edit) hbase-hadoop2-compat/pom.xml\n"], "derived": {"summary": "The output directory for the reduces is not cleaned up and therefore if you can see left overs from previous runs, if they had more reduces. For example, if you run the application once with reduces=10 and then rerun with reduces=8, your output directory will have frag00000 to frag00009 with the first 8 fragments from the second run and the last 2 fragments from the first run.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Output directories are not cleaned up before the reduces run - The output directory for the reduces is not cleaned up and therefore if you can see left overs from previous runs, if they had more reduces. For example, if you run the application once with reduces=10 and then rerun with reduces=8, your output directory will have frag00000 to frag00009 with the first 8 fragments from the second run and the last 2 fragments from the first run."}, {"q": "What updates or decisions were made in the discussion?", "a": "FAILURE: Integrated in Jenkins build HBase-Trunk_matrix #4170 (See [https://builds.apache.org/job/HBase-Trunk_matrix/4170/])\nHBASE-18942 hbase-hadoop2-compat module ignores hadoop-3 profile (apurtell: rev 6d69fd9c7871ffc3878eefa307da0e49bb7205f5)\n* (edit) hbase-hadoop2-compat/pom.xml"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-20", "title": "Mapper, Reducer need an occasion to cleanup after the last record is processed.", "status": "Closed", "priority": "Major", "reporter": "Michel Tourn", "assignee": null, "labels": [], "created": "2006-02-04T09:06:20.000+0000", "updated": "2009-07-08T16:51:40.000+0000", "description": "Mapper, Reducer need an occasion to do some cleanup after the last record is processed.\nProposal (patch attached)\nin interface Mapper:\n add method void finished();\nin interface Reducer:\n add method void finished();\n\nfinished() methods are called from MapTask, CombiningCollector, ReduceTask.\n------------\nKnown limitation: Fetcher (a multithreaded MapRunnable) does not call finished().\nThis is not currently a problem bec. fetcher Map/Reduce modules do not do anything in finished().\nThe right way to add finished() support to Fetcher would be to wait for all threads to finish, \nthen do:\n     if (collector instanceof CombiningCollector) ((CombiningCollector)collector).finished();\n------------\npatch begins: (svn trunk)\n\nIndex: src/test/org/apache/nutch/mapred/MapredLoadTest.java\n===================================================================\n--- src/test/org/apache/nutch/mapred/MapredLoadTest.java\t(revision 374781)\n+++ src/test/org/apache/nutch/mapred/MapredLoadTest.java\t(working copy)\n@@ -69,6 +69,8 @@\n                 out.collect(new IntWritable(Math.abs(r.nextInt())), new IntWritable(randomVal));\n             }\n         }\n+        public void finished() {\n+        }\n     }\n     static class RandomGenReducer implements Reducer {\n         public void configure(JobConf job) {\n@@ -81,6 +83,8 @@\n                 out.collect(new UTF8(\"\" + val), new UTF8(\"\"));\n             }\n         }\n+        public void finished() {\n+        }\n     }\n     static class RandomCheckMapper implements Mapper {\n         public void configure(JobConf job) {\n@@ -92,6 +96,8 @@\n \n             out.collect(new IntWritable(Integer.parseInt(str.toString().trim())), new IntWritable(1));\n         }\n+        public void finished() {\n+        }\n     }\n     static class RandomCheckReducer implements Reducer {\n         public void configure(JobConf job) {\n@@ -106,6 +112,8 @@\n             }\n             out.collect(new IntWritable(keyint), new IntWritable(count));\n         }\n+        public void finished() {\n+        }\n     }\n \n     int range;\nIndex: src/test/org/apache/nutch/fs/TestNutchFileSystem.java\n===================================================================\n--- src/test/org/apache/nutch/fs/TestNutchFileSystem.java\t(revision 374783)\n+++ src/test/org/apache/nutch/fs/TestNutchFileSystem.java\t(working copy)\n@@ -155,6 +155,8 @@\n \n       reporter.setStatus(\"wrote \" + name);\n     }\n+    \n+    public void finished() {}\n   }\n \n   public static void writeTest(NutchFileSystem fs, boolean fastCheck)\n@@ -247,6 +249,9 @@\n \n       reporter.setStatus(\"read \" + name);\n     }\n+    \n+    public void finished() {}\n+    \n   }\n \n   public static void readTest(NutchFileSystem fs, boolean fastCheck)\n@@ -339,6 +344,9 @@\n         in.close();\n       }\n     }\n+    \n+    public void finished() {}\n+    \n   }\n \n   public static void seekTest(NutchFileSystem fs, boolean fastCheck)\nIndex: src/java/org/apache/nutch/indexer/DeleteDuplicates.java\n===================================================================\n--- src/java/org/apache/nutch/indexer/DeleteDuplicates.java\t(revision 374776)\n+++ src/java/org/apache/nutch/indexer/DeleteDuplicates.java\t(working copy)\n@@ -225,6 +225,7 @@\n         }\n       }\n     }\n+    public void finished() {}\n   }\n     \n   private NutchFileSystem fs;\n@@ -265,6 +266,8 @@\n       reader.close();\n     }\n   }\n+  \n+  public void finished() {}\n \n   /** Write nothing. */\n   public RecordWriter getRecordWriter(final NutchFileSystem fs,\nIndex: src/java/org/apache/nutch/indexer/Indexer.java\n===================================================================\n--- src/java/org/apache/nutch/indexer/Indexer.java\t(revision 374778)\n+++ src/java/org/apache/nutch/indexer/Indexer.java\t(working copy)\n@@ -227,6 +227,8 @@\n \n     output.collect(key, new ObjectWritable(doc));\n   }\n+  \n+  public void finished() {}\n \n   public void index(File indexDir, File crawlDb, File linkDb, File[] segments)\n     throws IOException {\nIndex: src/java/org/apache/nutch/segment/SegmentReader.java\n===================================================================\n--- src/java/org/apache/nutch/segment/SegmentReader.java\t(revision 374778)\n+++ src/java/org/apache/nutch/segment/SegmentReader.java\t(working copy)\n@@ -143,7 +143,9 @@\n     }\n     output.collect(key, new ObjectWritable(dump.toString()));\n   }\n-\n+  \n+  public void finished() {}\n+  \n   public void reader(File segment) throws IOException {\n     LOG.info(\"Reader: segment: \" + segment);\n \nIndex: src/java/org/apache/nutch/mapred/Mapper.java\n===================================================================\n--- src/java/org/apache/nutch/mapred/Mapper.java\t(revision 374737)\n+++ src/java/org/apache/nutch/mapred/Mapper.java\t(working copy)\n@@ -39,4 +39,9 @@\n   void map(WritableComparable key, Writable value,\n            OutputCollector output, Reporter reporter)\n     throws IOException;\n+\n+  /** Called after the last {@link #map} call on this Mapper object.\n+      Typical implementations do nothing.\n+  */\n+  void finished();\n }\nIndex: src/java/org/apache/nutch/mapred/lib/RegexMapper.java\n===================================================================\n--- src/java/org/apache/nutch/mapred/lib/RegexMapper.java\t(revision 374737)\n+++ src/java/org/apache/nutch/mapred/lib/RegexMapper.java\t(working copy)\n@@ -53,4 +53,5 @@\n       output.collect(new UTF8(matcher.group(group)), new LongWritable(1));\n     }\n   }\n+  public void finished() {}\n }\nIndex: src/java/org/apache/nutch/mapred/lib/InverseMapper.java\n===================================================================\n--- src/java/org/apache/nutch/mapred/lib/InverseMapper.java\t(revision 374737)\n+++ src/java/org/apache/nutch/mapred/lib/InverseMapper.java\t(working copy)\n@@ -38,4 +38,6 @@\n     throws IOException {\n     output.collect((WritableComparable)value, key);\n   }\n+\n+  public void finished() {}\n }\nIndex: src/java/org/apache/nutch/mapred/lib/IdentityReducer.java\n===================================================================\n--- src/java/org/apache/nutch/mapred/lib/IdentityReducer.java\t(revision 374737)\n+++ src/java/org/apache/nutch/mapred/lib/IdentityReducer.java\t(working copy)\n@@ -42,4 +42,5 @@\n     }\n   }\n \n+  public void finished() {}\n }\nIndex: src/java/org/apache/nutch/mapred/lib/IdentityMapper.java\n===================================================================\n--- src/java/org/apache/nutch/mapred/lib/IdentityMapper.java\t(revision 374737)\n+++ src/java/org/apache/nutch/mapred/lib/IdentityMapper.java\t(working copy)\n@@ -39,4 +39,5 @@\n     output.collect(key, val);\n   }\n \n+  public void finished() {}\n }\nIndex: src/java/org/apache/nutch/mapred/lib/LongSumReducer.java\n===================================================================\n--- src/java/org/apache/nutch/mapred/lib/LongSumReducer.java\t(revision 374737)\n+++ src/java/org/apache/nutch/mapred/lib/LongSumReducer.java\t(working copy)\n@@ -47,4 +47,6 @@\n     // output sum\n     output.collect(key, new LongWritable(sum));\n   }\n+\n+  public void finished() {}\n }\nIndex: src/java/org/apache/nutch/mapred/lib/TokenCountMapper.java\n===================================================================\n--- src/java/org/apache/nutch/mapred/lib/TokenCountMapper.java\t(revision 374737)\n+++ src/java/org/apache/nutch/mapred/lib/TokenCountMapper.java\t(working copy)\n@@ -50,4 +50,6 @@\n       output.collect(new UTF8(st.nextToken()), new LongWritable(1));\n     }\n   }\n+\n+  public void finished() {}\n }\nIndex: src/java/org/apache/nutch/mapred/ReduceTask.java\n===================================================================\n--- src/java/org/apache/nutch/mapred/ReduceTask.java\t(revision 374781)\n+++ src/java/org/apache/nutch/mapred/ReduceTask.java\t(working copy)\n@@ -275,6 +275,7 @@\n       }\n \n     } finally {\n+      reducer.finished();\n       in.close();\n       lfs.delete(new File(sortedFile));           // remove sorted\n       out.close(reporter);\nIndex: src/java/org/apache/nutch/mapred/MapTask.java\n===================================================================\n--- src/java/org/apache/nutch/mapred/MapTask.java\t(revision 374737)\n+++ src/java/org/apache/nutch/mapred/MapTask.java\t(working copy)\n@@ -50,7 +50,7 @@\n   public void write(DataOutput out) throws IOException {\n     super.write(out);\n     split.write(out);\n-    \n+\n   }\n   public void readFields(DataInput in) throws IOException {\n     super.readFields(in);\n@@ -126,6 +126,10 @@\n         }\n \n       } finally {\n+        if (combining) {\n+          ((CombiningCollector)collector).finished();\n+        }\n+\n         in.close();                               // close input\n       }\n     } finally {\n@@ -147,5 +151,5 @@\n   public NutchConf getConf() {\n     return this.nutchConf;\n   }\n-  \n+\n }\nIndex: src/java/org/apache/nutch/mapred/MapRunner.java\n===================================================================\n--- src/java/org/apache/nutch/mapred/MapRunner.java\t(revision 374737)\n+++ src/java/org/apache/nutch/mapred/MapRunner.java\t(working copy)\n@@ -38,18 +38,22 @@\n   public void run(RecordReader input, OutputCollector output,\n                   Reporter reporter)\n     throws IOException {\n-    while (true) {\n-      // allocate new key & value instances\n-      WritableComparable key =\n-        (WritableComparable)job.newInstance(inputKeyClass);\n-      Writable value = (Writable)job.newInstance(inputValueClass);\n+    try {\n+      while (true) {\n+        // allocate new key & value instances\n+        WritableComparable key =\n+          (WritableComparable)job.newInstance(inputKeyClass);\n+        Writable value = (Writable)job.newInstance(inputValueClass);\n \n-      // read next key & value\n-      if (!input.next(key, value))\n-        return;\n+        // read next key & value\n+        if (!input.next(key, value))\n+          return;\n \n-      // map pair to output\n-      mapper.map(key, value, output, reporter);\n+        // map pair to output\n+        mapper.map(key, value, output, reporter);\n+      }\n+    } finally {\n+        mapper.finished();\n     }\n   }\n \nIndex: src/java/org/apache/nutch/mapred/CombiningCollector.java\n===================================================================\n--- src/java/org/apache/nutch/mapred/CombiningCollector.java\t(revision 374780)\n+++ src/java/org/apache/nutch/mapred/CombiningCollector.java\t(working copy)\n@@ -78,4 +78,9 @@\n     count = 0;\n   }\n \n+  public synchronized void finished()\n+  {\n+    combiner.finished();\n+  }\n+\n }\nIndex: src/java/org/apache/nutch/mapred/Reducer.java\n===================================================================\n--- src/java/org/apache/nutch/mapred/Reducer.java\t(revision 374737)\n+++ src/java/org/apache/nutch/mapred/Reducer.java\t(working copy)\n@@ -38,4 +38,10 @@\n   void reduce(WritableComparable key, Iterator values,\n               OutputCollector output, Reporter reporter)\n     throws IOException;\n+\n+  /** Called after the last {@link #reduce} call on this Reducer object.\n+      Typical implementations do nothing.\n+  */\n+  void finished();\n+\n }\nIndex: src/java/org/apache/nutch/crawl/CrawlDbReader.java\n===================================================================\n--- src/java/org/apache/nutch/crawl/CrawlDbReader.java\t(revision 374737)\n+++ src/java/org/apache/nutch/crawl/CrawlDbReader.java\t(working copy)\n@@ -50,9 +50,9 @@\n \n /**\n  * Read utility for the CrawlDB.\n- * \n+ *\n  * @author Andrzej Bialecki\n- * \n+ *\n  */\n public class CrawlDbReader {\n \n@@ -68,6 +68,7 @@\n       output.collect(new UTF8(\"retry\"), new LongWritable(cd.getRetriesSinceFetch()));\n       output.collect(new UTF8(\"score\"), new LongWritable((long) (cd.getScore() * 1000.0)));\n     }\n+    public void finished() {}\n   }\n \n   public static class CrawlDbStatReducer implements Reducer {\n@@ -121,6 +122,7 @@\n         output.collect(new UTF8(\"avg score\"), new LongWritable(total / cnt));\n       }\n     }\n+    public void finished() {}\n   }\n \n   public static class CrawlDbDumpReducer implements Reducer {\n@@ -133,8 +135,11 @@\n \n     public void configure(JobConf job) {\n     }\n+\n+    public void finished() {\n+    }\n   }\n-  \n+\n   public void processStatJob(String crawlDb, NutchConf config) throws IOException {\n     LOG.info(\"CrawlDb statistics start: \" + crawlDb);\n     File tmpFolder = new File(crawlDb, \"stat_tmp\" + System.currentTimeMillis());\n@@ -219,7 +224,7 @@\n       System.out.println(\"not found\");\n     }\n   }\n-  \n+\n   public void processDumpJob(String crawlDb, String output, NutchConf config) throws IOException {\n \n     LOG.info(\"CrawlDb dump: starting\");\n@@ -270,4 +275,5 @@\n     }\n     return;\n   }\n+\n }\nIndex: src/java/org/apache/nutch/crawl/LinkDb.java\n===================================================================\n--- src/java/org/apache/nutch/crawl/LinkDb.java\t(revision 374779)\n+++ src/java/org/apache/nutch/crawl/LinkDb.java\t(working copy)\n@@ -118,7 +118,8 @@\n     output.collect(key, result);\n   }\n \n-\n+  public void finished() {}\n+\t\n   public void invert(File linkDb, File segmentsDir) throws IOException {\n     LOG.info(\"LinkDb: starting\");\n     LOG.info(\"LinkDb: linkdb: \" + linkDb);\nIndex: src/java/org/apache/nutch/crawl/Injector.java\n===================================================================\n--- src/java/org/apache/nutch/crawl/Injector.java\t(revision 374779)\n+++ src/java/org/apache/nutch/crawl/Injector.java\t(working copy)\n@@ -65,6 +65,8 @@\n                                              interval));\n       }\n     }\n+    \n+    public void finished() {}\n   }\n \n   /** Combine multiple new entries for a url. */\n@@ -76,6 +78,7 @@\n       throws IOException {\n       output.collect(key, (Writable)values.next()); // just collect first value\n     }\n+    public void finished() {}\n   }\n \n   /** Construct an Injector. */\nIndex: src/java/org/apache/nutch/crawl/Generator.java\n===================================================================\n--- src/java/org/apache/nutch/crawl/Generator.java\t(revision 374779)\n+++ src/java/org/apache/nutch/crawl/Generator.java\t(working copy)\n@@ -63,6 +63,8 @@\n       output.collect(crawlDatum, key);          // invert for sort by score\n     }\n \n+    public void finished() {}\n+    \n     /** Partition by host (value). */\n     public int getPartition(WritableComparable key, Writable value,\n                             int numReduceTasks) {\nIndex: src/java/org/apache/nutch/crawl/CrawlDbReducer.java\n===================================================================\n--- src/java/org/apache/nutch/crawl/CrawlDbReducer.java\t(revision 374781)\n+++ src/java/org/apache/nutch/crawl/CrawlDbReducer.java\t(working copy)\n@@ -115,4 +115,5 @@\n     }\n   }\n \n+  public void finished() {}\n }\nIndex: src/java/org/apache/nutch/parse/ParseSegment.java\n===================================================================\n--- src/java/org/apache/nutch/parse/ParseSegment.java\t(revision 374776)\n+++ src/java/org/apache/nutch/parse/ParseSegment.java\t(working copy)\n@@ -78,6 +78,8 @@\n     throws IOException {\n     output.collect(key, (Writable)values.next()); // collect first value\n   }\n+  \n+  public void finished() {}\n \n   public void parse(File segment) throws IOException {\n     LOG.info(\"Parse: starting\");\n\n\n\n\n\n", "comments": ["Michel,\n\nCan you please re-generate this patch using the new hadoop packages?\n\nAlso, I'd prefer the method be named 'close()' instead of 'finished()'.\n\nFinally, perhaps we should move this into a Closeable interface that is extended by both Mapper and Reducer?\n\nThanks!\n\nDoug", "Here is the updated patch:\nright package names and close() instead of finished()\n", "I just committed this.  I changed it so that Mapper and Reducer extend a Closeable interface rather than both providing a close() method directly."], "derived": {"summary": "Mapper, Reducer need an occasion to do some cleanup after the last record is processed. Proposal (patch attached)\nin interface Mapper:\n add method void finished();\nin interface Reducer:\n add method void finished();\n\nfinished() methods are called from MapTask, CombiningCollector, ReduceTask.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Mapper, Reducer need an occasion to cleanup after the last record is processed. - Mapper, Reducer need an occasion to do some cleanup after the last record is processed. Proposal (patch attached)\nin interface Mapper:\n add method void finished();\nin interface Reducer:\n add method void finished();\n\nfinished() methods are called from MapTask, CombiningCollector, ReduceTask."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  I changed it so that Mapper and Reducer extend a Closeable interface rather than both providing a close() method directly."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-4", "title": "tool to mount dfs on linux", "status": "Closed", "priority": "Major", "reporter": "John Xing", "assignee": "Pete Wyckoff", "labels": [], "created": "2006-02-05T03:55:29.000+0000", "updated": "2012-07-19T11:12:58.000+0000", "description": "This is a FUSE module for Hadoop's HDFS.\n\nIt allows one to mount HDFS as a Unix filesystem and optionally export\nthat mount point to other machines.\n\nrmdir, mv, mkdir, rm are all supported. just not cp, touch, ..., but actual writes require: https://issues.apache.org/jira/browse/HADOOP-3485\n\nFor the most up-to-date documentation, see: http://wiki.apache.org/hadoop/MountableHDFS\n\nBUILDING:\n\n\nRequirements:\n\n   1. a Linux kernel > 2.6.9 or a kernel module from FUSE - i.e., you\n   compile it yourself and then modprobe it. Better off with the\n   former option if possible.  (Note for now if you use the kernel\n   with fuse included, it doesn't allow you to export this through NFS\n   so be warned. See the FUSE email list for more about this.)\n\n   2. FUSE should be installed in /usr/local or FUSE_HOME ant\n   environment variable\n\nTo build:\n\n   1. in HADOOP_HOME: ant compile-contrib -Dcompile.c++=1 -Dfusedfs=1 -Dlibhdfs=1\n\n\nNOTE: for amd64 architecture, libhdfs will not compile unless you edit\nthe Makefile in src/c++/libhdfs/Makefile and set OS_ARCH=amd64\n(probably the same for others too).\n\n--------------------------------------------------------------------------------\n\nCONFIGURING:\n\nLook at all the paths in fuse_dfs_wrapper.sh and either correct them\nor set them in your environment before running. (note for automount\nand mount as root, you probably cannnot control the environment, so\nbest to set them in the wrapper)\n\nINSTALLING:\n\n1. mkdir /mnt/dfs (or wherever you want to mount it)\n\n2. fuse_dfs_wrapper.sh dfs://hadoop_server1.foo.com:9000 /mnt/dfs -d\n; and from another terminal, try ls /mnt/dfs\n\nIf 2 works, try again dropping the debug mode, i.e., -d\n\n(note - common problems are that you don't have libhdfs.so or\nlibjvm.so or libfuse.so on your LD_LIBRARY_PATH, and your CLASSPATH\ndoes not contain hadoop and other required jars.)\n\n--------------------------------------------------------------------------------\n\n\nDEPLOYING:\n\nin a root shell do the following:\n\n1. add the following to /etc/fstab -\n  fuse_dfs#dfs://hadoop_server.foo.com:9000 /mnt/dfs fuse\n  allow_other,rw 0 0\n\n2. mount /mnt/dfs Expect problems with not finding fuse_dfs. You will\n   need to probably add this to /sbin and then problems finding the\n   above 3 libraries. Add these using ldconfig.\n\n--------------------------------------------------------------------------------\n\nEXPORTING:\n\nAdd the following to /etc/exports:\n\n  /mnt/hdfs *.foo.com(no_root_squash,rw,fsid=1,sync)\n\nNOTE - you cannot export this with a FUSE module built into the kernel\n- e.g., kernel 2.6.17. For info on this, refer to the FUSE wiki.\n--------------------------------------------------------------------------------\n\nADVANCED:\n\nyou may want to ensure certain directories cannot be deleted from the\nshell until the FS has permissions. You can set this in the build.xml\nfile in src/contrib/fuse-dfs/build.xml\n\n", "comments": ["It works with new hadoop project now (tarball attached).\nThe intended commit location is http://svn.apache.org/repos/asf/lucene/hadoop/trunk/contrib/fuse\nPlease vote on this issue.", "I changed the code for fuse-hadoop. This is a working version for mounting DFS to linux file system. This version works fine with FUSE-J.2.2.3 and HADOOP.0.5.0\n\nEnjoy", "I changed the code for fuse-hadoop. This is a working version for mounting DFS to linux file system. This version works fine with FUSE-J.2.4 and HADOOP.0.5.0\n\nEnjoy", "I changed the code for fuse-hadoop. This is a working version for mounting DFS to linux file system. This version works fine with HADOOP.0.5.0\n\nEnjoy", "I'd like to be able to commit this to the contrib tree, but it could use a bit more polish.  Unfortunately we cannot commit the fuse-j jar file, since it is released under the LGPL.  What would be great is a build.xml that downloaded and built fuse-j.\n\nThe README could also include more info on installing fuse.\n\nOn Ubuntu I was able to do this with:\n\nsudo apt-get install fuse-utils libfuse2 libfuse-devel\n\nIf you've never built things on your Ubuntu box then you might also need:\n\nsudo apt-get install make gcc libc6-dev\n", "There are already tools to mount WebDAV on Windows, MacOS and Linux.  So HADOOP-496 will provide a more universal solution for this issue.  I'd like to resolve this as a duplicate of that issue.  Does anyone object?\n", "Has anyone made any comparison (performance or other) of these two different approaches. It lloks like the webdav version (were dfs client is in servlet) introduces one more network hop compared to fuse version (where the dfs client is in machine using the dfs).\n\nOne way or the other I could easily satisfy my mounting needs with any one of these solutions - so hopefully one will be integrated (so it gets maintenance attention).\n", "I'm cancelling this patch to remove it from the queue of patches that we intend to immenently apply to the current trunk.  This is still useful stuff, and, depending on what happens with webdav, we may still decide to integrate it, so the issue should remain open for now.", "Hello,\nWe posted this on HADOOP-496 and were pointed to this jira entry as a better place to post this patch.  Pasting our original submission message below...\n\n--------------------------------------\nHi,\n\nWe revived the old fuse-hadoop project (a FUSE-J based plugin that lets you mount Hadoop-FS). We have tried this on a small cluster (10 nodes) and basic functionality works (mount, ls, cat,cp, mkdir, rm, mv, ...).\n\nThe main changes include some bug fixes to FUSE-J and changing the previous fuse-hadoop implementation to enforce write-once. We found the FUSE framework to be straightforward and simple.\n\nWe have seen several mentions of using FUSE with Hadoop, so if there is a better place to post these files, please let me know.\n\nAttachments to follow...\n\n-thanks\n--------------------------------------\n\nAttachments include the following:\n  * fuse-j-hadoop package\n  * fuse-j patch.\n", "It is unfortunate that this requires patches to the fuse-j sources.  The fuse-j project does not appear to be active, so I don't see much point in trying to submit these as a patch there.\n\nWe cannot host GPL'd code or even patches to GPL'd code at Apache.  We can however have optional commands in our build scripts that download GPL'd code.  So this could be structured as a contrib module whose build.xml, when a non-default build option is specified, downloads, patches and compiles fuse-j.  But since the patch to fuse-j itself cannot be hosted at Apache, it might be simpler to just create a jar file for the patched version, host that somewhere, and bypass the patch and compile steps.  It would be great to get this into a form that we can commit, so that it stays synchronized with the rest of Hadoop.", "I think I commented on another JIRA that I have implemented a dfs fuse module for the straight C fuse module.\n\nthus far I have only implemented read-only and everything works fine. It's been up for about a month with low use, but no problems.\n\npete\n", "> I think I commented on another JIRA that I have implemented a dfs fuse module for the straight C fuse module.\n\nYes, you noted that in HADOOP-496.  Can you post a patch that implements this?", "Yes but need a couple of days to get our build guy to make the Makefile.am less facebook-centric.\n\n", "hi Doug,\n\nThanks for pointing out this issue.  I will remove the FUSE-J patch and try one of the other routes you suggested (to have a patched FUSE-J available), and will come back with a resolution on this very soon.\n\n-anurag", "Hi Doug,\n\nI went through the license for Fuse-J and it is distributed under LGPL, do you think that would allow the Fuse-J patches to be hosted on Apache?\n\n(In the latter case we would still modify the submission above to be a contrib module that downloads Fuse-J, applies our patch, and builds it, except we won't have to find a place to host the patch).\n\n-thanks\n-anurag\n", "> I went through the license for Fuse-J and it is distributed under LGPL,\n\nUnfortunately, the ASF cannot host things published under LGPL either.  Sorry!", "hi Doug.  ok :- ), we will follow one of the alternate options you suggested of hosting either the patch or the jar file ourselves, and fixing the fuse-j-hadoop package build to work with this.  Will re-submit our changes soon.\n-thanks,\n-anurag", "Here's the source. I will attach the Makefiles and full tar tomorrow or Wed morning.\n\n", "hi,\nWe re-submitted the fuse-j-hadoopfs package with the following changes (as suggested above):\n- we are hosting a patched FUSE-J on a separate server.\n- the fuse-j-hadoopfs build downloads this patched version at compile time.\n\nWe restructured the fuse-j-hadoopfs build to be a contrib, and have tested it with the Hadoop source-tree build.\n\nThe fuse-j-hadoopfs build is a no-op when a standard \"compile\" target is specified.  To actually build fuse-j-hadoopfs, the user has to specify the following command line: \"ant compile -Dbuild-fuse-j-hadoopfs=1\".\n\nWe still have the following todo's remaining:\n- Pick up some environment variables dynamically, so the user doesn't have to set them in our build.properties file (these do not affect the no-op build).\n- Change the 'hadoopfs_fuse_mount.sh' script to use the 'hadoop/bin' scripts, so it can automatically pick up hadoop-specific conf, jar and class files.\n\nThe above tarball (fuse-j-hadoopfs-03.tar.gz) consists of a directory that can be placed inside \"hadoop/src/contrib\", please let us know if we should submit this as a patch-file instead, or if we need to make more changes...\n\n-thanks\n\n", "Here's the same code (w/o the closelog after the return in main :)) and the Makefile.am - sorry I haven't been able to get to this and making a nice autoconf. But, it is easy to build:\n\n0. install hadoop 14.x\n1. build and install latest fuse\n2. compile fuse_dfs.c with 2 includes - one for fuse and one for hadoop's hdfs.h and their libraries as well.\n3. run it ./fuse_dfs -debug -server <hadoopnn> -port <hadoopnnport> /mnt/hdfs -d -o allow_other\n\nObviously, hadoop needs to be in your class path and your ld library path needs the fuse and hdfs .sos\n\nWhen ready for production remove the -debug (which is an option to fuse-dfs for deubgging) and the -d which is the fuse debugging option.\n\nOn my return from vacation, I will make better docs and autoconf.\n\nNote again, this is read only, but it is really easy to implement writes.\n\n-- pete\n", "I implemented mkdir, mv, rm and rmdir. But, since the programmatic API doesn't use the Trash feature, this is a pretty big problem.\n\n2 solutions:\n\n1. have fuse dfs rename things into /Trash\n2. make the programmatic API use the Trash. Which is more general.\n\nI'm just wondering why the programmatic API never used trash in the first place??\n\n-- pete\n ", "> I'm just wondering why the programmatic API never used trash in the first place??\n\nBecause most other programmatic APIs don't.  Unix 'rm' does not use the trash, nor does the unlink system call, nor does DOS command line, etc.  Trash is usually only a feature of a user-interface, like a GUI and command shells designed for interactive use.\n", "The hadoop API is not POSIX-compliant. Wouldn't it be better to protect people from what is catastrophic data loss?\n\n", "newer version which supports mkdir, rmdir, mv, and rm. NOTE - this is still a work in progress. Any help appreciated :)\n", "Since  a) the latest patches from Pete are not depending on fuse-j and b) fuse is now part of linux kernel (2.6.14 and later) It would be really nice to get this into hadoop proper. \n\nBefore that it would be nice to try this out so i am begging for some more information on how to compile this :)\nI downloaded fuse-dfs.tgz and there were two files fuse_dfs.c and Makefile.am , with what commands do I generate the makefile, tried many starting with auto* but none were succesfull.\n\n\n\n", "I did some testing with the c version of this tool (and managed to write a working makefile :). For some reason it only worked for me when running it with fuse debug on, anyone else seen this? Other than that it worked great.", "I have seen the debug issue myself, I think it is related to fuse more \nthan the fuse_dfs.c, as i also see that with some of my ssh mounts to \nsome systems.\n\n\n", "Sami - can you attach the Makefile? \n", "should have mentioned the problem with mine is  that it is generated with some custom auto make macros that we use here. \n", "Here's the makefile I used. One should perhaps adapt to the model that the other c* modules are using.", "Here's a version with configure to make things easier.\n\nJust run bootstrap.sh after setting the right hdfs, fuse and jdk paths.\n\nI still haven't added a version # (next on my list) or better docs (next next on my list).\n\n--- pete\n", "Includes version # from the configure.ac into fuse_dfs.c so fuse_dfs --version prints it out. And also include better README documentation.\n\nThe package is pretty much complete now other than implementing writes! If someone has 0.15+ Hadoop installed and wants to work on it, that would be great. 15 because you can see file creates before they are closed which is needed for the implementation. I only have 0.14.2 right now.\n\nThis newest version should be pretty self explanatory.\n-- pete\n", "Pete,\n\nI have been experimenting with fuse_dfs.c and have a few questions:\n\n(1) I am using a previous version of fuse_dfs.c, mainly because I dont have bootstrap.sh. However, with respect to the new fuse_dfs.c option parsing - is this compatible with calling via mount.fuse, and autofs?\n\nThis how I currently mount using an autofs map containing:\n{code}\nhdfs            -fstype=fuse,rw,nodev,nonempty,noatime,allow_other  :/path/to/fuse_dfs_moutn/fuse_dfs.sh\\#dfs\\://namenode\\:9000\n{code}\nfuse_dfs.sh is just a shell script setting the CLASSPATH and LD_LIBRARY_PATH, and essentially, just execs the fuse_dfs. If I changed to the more recent version, I would probably have to put the dfs://namenode:9000 configuration into the script I think.\n\n(2) Have you done any sort of performance testing? I'm experimenting with HDFS for use in a mixed envionment (hadoop and non-hadoop jobs), and the throughput I see is miserable. For example, I use a test network of 8 P3-1GHz nodes, and a similar client on 100meg network.\n\nBelow, I compare cat-ing a 512MB file from (a) an NFS mount on the same network as the cluster nodes (b) using the hadoop frontend and (c) using the FUSE HDFS filesystem.\n\n{noformat}\n# (a)\n$ time cat /mnt/tmp/data.df > /dev/null\n\nreal 0m47.280s\nuser 0m0.059s\nsys 0m2.476s\n\n# (b)\n$ time bin/hadoop fs -cat hdfs:///user/craigm/data.df > /dev/null\n\nreal 0m48.839s\nuser 0m16.256s\nsys 0m7.001s\n\n# (c)\n$ time cat /misc/hdfs/user/craigm/data.df >/dev/null\n\nreal    1m41.686s\nuser    0m0.135s\nsys     0m2.302s\n{noformat}\n\nNote that the NFS and Hadoop fs -cat obtain about 10.5MB/sec, while the hdfs fuse mount (in /misc/hdfs) achieves only 5MB/sec. Is this an expected overhead for FUSE? \n\nI did try tuning rd_buf_size to match the size of reads that the kernel was requesting - ie 128KB instead of 32KB, however this made matters worse:\n\n{noformat}\n# with 128KB buffer size\n$ time cat /misc/hdfs/user/craigm/data.df >/dev/null\n\nreal    2m11.080s\nuser    0m0.113s\nsys     0m2.180s\n{noformat}\n\nPerhaps an option would be to keep the HDFS file open between reads and timeout the connection when not used, or something; read more than we need and then keep it in the memory? Both would overly complicate the neat code though!\n\n(3) If I use an autofs for hdfs, then mounts will timeout quickly (30 seconds), and then reconnect again on demand. Perhaps fuse_dfs.c can implement the destroy fuse operation to free up the connection to the namenode, etc?\n\nCheers\n\nCraig", "Hi Craig,\n\nI may be using an older fuse? I have #dfs#dfs://hadoopNN01.facebook.com:9000 /mnt/hdfs    fuse    allow_other,rw    0 0 \n\nFor performance, it's really fast when there's 0 load on the namenode, but start running a couple of jobs and it gets killed.  Obviously because one operation may require multiple fuse calls and thus multiple dfs calls versus the single one for bin/hadoop dfs.   I'm noticing that all the critical sections in the namenode do things like logging to the debug log *IN* the critical section. And since everything locks fsRoot, just one job can really hose a real-time system. I don't see anything to do here other than trying to fix these after verifying with jconsole this is the problem.\n\nAnd I will add the autofs destroy function this week.\n\nSo, what do you think I should do for #1, revert to the older configuration code? (which was way nicer anyway). Do you have your bash script so I can use it too?\n\nthanks, pete\n\n", "For #1, I think thats an fstab line, not an autofs line, perhaps? I'm not sure the fuse version will make any difference, mount will only pass through the source path, the dest path and the -o options. Would it be possible to keep all options for fuse_dfs.c in -o options? On the other hand, if a shell script is always needed to set the CLASSPATH and LD_LIBRARY_PATH options, then it is not as important how the options are set from fstab or autofs. (LD_LIBRARY_PATH could be fixed bya  /etc/ld.conf.d entry)\n\nMy test had no load on the namenode, and usage of the namenode CPU looks low. In contrast, the fuse_dcs CPU usage was high.\n\nI would like to profile or jconsole the fuse_dfs binary, but the getJNIEnv() method in src/c++/libhdfs/hdfsJniHelper.c could be a bit more helpful for passing agument to the JVM initialisation. Essentially, it only allows fills in -Djava.class.path from the CLASSPATH to be set, not any other arbitrary system properties or -X options etc, bah.  Reported separately as HADOOP-2857.\n\nWill attach bash script shortly.\n\nC", "Shell script to set LD_LIBRARY_PATH and CLASSPATH when called from mount.fuse (which can be called from mount, and hence from autofs etc).\n\n", "I made some changes - mainly caching the file handle in fi->fh and it's performing much better now. I'm attaching the new one.\n\n-- pete\n", "newest fuse dfs which should perofrm better on reads. I'm seeing this on a cluster in use:\n\n > time cat part-00000 > /dev/null \n\nreal    1m25.078s\nuser    0m0.056s\nsys     0m1.514s\n > du -kh part-00000 \n1.1G    part-00000\n\n", "Hi Craig,\n\nI'm setting fi->fh in open as you suggest. You're right I didn't look at pread and tell here - good point.\n\n-- pete\n", "I added the destroy method. Also looked and pread should be correct. \n\nI'm hoping to test with 0.16.x soon and see if writes work!\n\n-- pete\n", "version 0.2.0 includes better read and in theory writes but they won't work wi/o hadoop 0.16 and I can't test. Obviously, the writes have to be append only.  And I'm still not sure what the semantics are as far as block size.\n\n", "should have been this one that includes init method :)\n", "\nHi Pete,\n\n\nDefinently using the latest tar this time ;-)\nMy first time using the new build system - looks good! \n\nSome comments:\n\n1. Firstly, I shouldnt have deleted my last comment - though it was clearly in error as I was reading the wrong version of fuse_dfs.c. In your comments, can you say which file you've just uploaded?\n\nFor posterity, previous comment was:\n\n{quote}\n\nI will try the newer version tomorrow when @work. I note that fi->fh isnt used or set in dfs_read in your latest version. Could we set it in dfs_open for O_READONLY, and then use it if available? \n\nI'm not clear on the semantics of hdfsPread  - does it assume that offset is after previous offset?\nIf so then we need to check that the current read on a file is strictly after the previous read for a previously open FH to be of use - hdfsTell could be of use here.\n{quote}\n\n2. With respect to the read speed, this is indeed a bit faster in our test setting (nearer 6MB/sec), but not yet similar to the Hadoop fs shell (about 10.5MB/sec). Fuse version 2.7.2\n{noformat}\n# time bin/hadoop fs -cat /user/craigm/data.df > /dev/null \n\nreal    0m50.347s\nuser    0m16.023s\nsys     0m6.644s\n\n# time cat /misc/hdfs/user/craigm/data.df > /dev/null \n\nreal    1m31.263s\nuser    0m0.131s\nsys     0m2.384s\n\n{noformat}\nI'm trying to measure the CPU taken by fuse_dfs for the same read, so we know how much CPU time it burns.\n\nCan I ask how your test time test compares to using the Hadoop fs shell on the same machine? When reading, the CPU on the client is used 45%ish, similar to the Hadoop fs shell CPU use.\n\nI feel it would be good to aim for similar performance as the Hadoop fs shell, as this seems reasonable compared to NFS in my test setting, and should scale better as the number of concurrent reads increases, given available wire bandwidth.\n\n3. With respect to the build system, it could be clearer what --with-dfspath= is meant to point to. src/Makefile.am seems to assume that include files are at ${dfspath}/include/linux and the hdfs.so at ${dfspath}/include/shared. This isnt how the Hadoop installation is laid out. Perhaps it would be better if we could give an option to the hadoop installation and it's taken from there?\n\n4. src/Makefile.am assumes an amd64 architecture. Same problem I noted in my shell script about guessing the locations of the JRE shared lib files.\n\n5 (minor). the last tar.gz had a link to aclocal.m4 in the external folder that was absolute - ie to your installation. Should be deleted when building tar file.\n\n6 (minor). update print_usage if you're happy with the specification of filesystem options. I made no changes to my shell script or my autofs mount for this version to work :-)\n\n\nCheers\n\nCraig", "Hi Pete,\n\nCould you please explain in more details why write can not be implemented before 0.16. And how we expect write to work with 0.16?\n\nThanks a lot!\n\nRui\n", "Pete,\n\nI have tried everything to profile fuse_dfs. Valgrind (callgrind) doesnt play with Sun Java, and I failed to get the GNU profiler to give any output. I wrote a patch for HADOOP-2857, but using this I cant get any stack traces from the Java profiler - it's as if no Java code is run!\n\nCraig\n\n", "Hi Rui,\n\nIt's because of the order FUSE calls the implementation. The file is created with an open and then closed and then opened again. \n\nSo in <16, that second open in write mode will fail :(  I have tried hacking things so that the code doesn't do the close on the file in between and caches the filehandle and so fakes the second open and everything works fine.\n\nSo, I think as long as appends are coming in, it should work with 16. I haven't looked at the implementation, but hopefully it buffers things till it gets to a full block and isn't creating small blocks all over the place :)  If it does, it's easy enough to buffer in fuse (although 128MB is a big buffer).\n\npete\n\nps although we're on 15.3 at FB, I can't upgrade our test cluster to 16 yet as we want to install HBASE and need to try it there. I'm gonna try applying the hbase patch that  the powerset guys nicely gave me soon and then if all goes well, I can upgrade our test cluster to 16 and test things.\n\n", "Hi Craig,\n\nI see now that the buffer size for the OS reads is only 128K and since there's no ioctl for fuse to bump it up, it's a problem. I think what we can do is create a fuse block device mount and then using blockdev -setblocksize 128M, we should see some real speedups.\n\nUnfortunately, fuse on my dev machine isn't configured properly for this.  First there's no /dev/fuseblk  which it assumes and even with that there I get a useless error message.\n\nI just want to verify with one of the kernel guys here how big the buffer can get before going to crazy. I'll ask one of them today and update this tomorrow.\n\n-- pete\n", "Craig,\n\nI should mention I tried to get fuse to do more readahead than 128K, but setting that param didn't seem to do anything. I can probably play with this tomorrow.  To be honest, I don't know exactly what it means when you configure fuse module as a block device since you also need to specify the mount point. Is fuse under the covers just doing the block device so we can do better ioctl? I mean there's no way to implement a real block device since we'd have to make it look like a real filesystem. But, fuse requires both the block device and the mount point.\n\n-- pete\n", "Hi Pete,\n\nThe block stuff in fuse is appallingly documented. I have hunted the Web for info on this all afternoon, to understand it further. To be honest, the only thing I have found useful is reading the source of ntfs-3g.c at http://ntfs-3g.cvs.sourceforge.net/ntfs-3g/ntfs-3g/src/ntfs-3g.c?revision=1.106&view=markup\n\nI test I did do a few days ago was to comparing reading an NFS mounted file directly vs, the same file read via NFS via a FUSE fs - http://mattwork.potsdam.edu/projects/wiki/index.php/Rofs#rofs_code_.28C.29 (ROFS, the Read-Only Filesystem). Speed results were fairly comparable between NFS & NFS+ROFS, so it suggests that FUSE doesnt add too much overhead to IO. Hence then we can only suspect that the problem is in either (a) JNI interface, or (b) the size of the reads we're performing. A simple C tool can be generated to exclude (a).\n\nI dont have any objections to pretty large buffer sizes for fuse_dfs.c - HDFS is designed for large files, and streaming read access.\n\nBtw, you mentioned you are re-exporting the mounted FS as NFS - have you had any issues vs the issues described in fuses' README.NFS? \n\nRegards\n\nCraig\n", "\nI just talked to one of our kernel guys and he isn't 100% sure as he hasn't done that much IO stuff on Linux but thinks the 128K readahead may just be the maximum.  \n\nwe could always do like 1MB readaheads ourselves although that would complicate things - although not that much since we could keep the cached data with the open file handle so there's no dirty cache problems or garbage collection issues since we just dump it when we do the close. So, maybe that's the easiest way to go... I can probably look at that tomorrow or Thursday.\n\npete\n", "Hi Pete,\n\nThanks a lot for the explanation!\n\nBut as I heard from the hdfs team, 0.16 still does not support file appending. Suppose appending is not supported, can we still try writing file in Fuse as the work around you described?\n\nThanks,\n\nRui\n\n\n\n", "Hi Pete,\n\nHave you had a chance to look at FUSE readaheads? I have attached a version of fuse_dfs.c I have patched, which reads 10MB chunks from DFS, and cache these in the a struct held in the filehandle. \n\nI'm seeing some improvement (down to 1m 20 compared to \"bin/hadoop dfs -cat file > /dev/null\" which takes about 50 seconds). Increasing the buffer size shows some improvement [I only did some quick tests]  - I tried up to 30MB, but I dont think there's much improvement over 5-10MB\n\nDo you think we're reaching the limit such that the overheads of JNI are making it impossible to go any faster? Ie Where do we go from here?\n\nAnother comment I have is that the configure/makefile asks for a dfs_home. It might be easier to ask for Hadoop home, then build the appropriate paths from there (${hadoop_home}/libhdfs and ${hadoop_home}/src/c++/libhdfs). Hadoop has no include/linux folders etc. Finally, we need a way to detect whether to use i386 or amd64 to find jvm.so\n\nCraig", "HEre's my most recent one. I will try merging Craig's read ahead code in and then I guess see about getting it into contrib.\n", "I should have mentioned I fixed the autoconf problems and made the \"protectedpaths\" configurable. I guess we'll have to have a discussion about whether people like this because I think Doug clearly doesn't :)\n", "This is by no means a completely final product but more of a version 0.1. but, it has decent autoconf, comments and readme files and has been working in production for quite a while.\n", "First checkin.\n", "Changed the affected versions and fixes to unknown from 0.5", "-1 overall.  Here are the results of testing the latest attachment \nhttp://issues.apache.org/jira/secure/attachment/12379898/patch.txt\nagainst trunk revision 645773.\n\n    @author +1.  The patch does not contain any @author tags.\n\n    tests included -1.  The patch doesn't appear to include any new or modified tests.\n                        Please justify why no tests are needed for this patch.\n\n    javadoc +1.  The javadoc tool did not generate any warning messages.\n\n    javac +1.  The applied patch does not generate any new javac compiler warnings.\n\n    release audit -1.  The applied patch generated 211 release audit warnings (more than the trunk's current 202 warnings).\n\n    findbugs +1.  The patch does not introduce any new Findbugs warnings.\n\n    core tests +1.  The patch passed core unit tests.\n\n    contrib tests +1.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2201/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2201/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2201/artifact/trunk/build/test/checkstyle-errors.html\nRelease audit warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2201/artifact/trunk/current/releaseAuditDiffWarnings.txt\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2201/console\n\nThis message is automatically generated.", "Help - I don't know what a release audit warning is ?? It just lists the filenames in the release audit link.\n\nAlso, unit testing for this is pretty hard, but can be done to some extent in the future by running each function like fuse calls them, but these would be C unit tests anyway which I don't know if we have support for.\n\nDo people want to comment on the feature of moving deleted files to /Trash and also of not allowing rmdir on some \"special directories\" e.g., '/' '/user' /warehouse' ... ??", "The release audit flags new files that don't contain the Apache license (or old files that have had it removed).  In this case most of those flagged are fine to not have the Apache license, since they're automatically generated stuff, but it probably wouldn't hurt to add it to the shell scripts.\n\nSome automated tests would be good, e.g., an end-to-end test that starts HDFS, mounts it with fuse, and then lists and reads files through the mount.  But such tests should not be run by default, since the default build does not compile C++ code, nor should it depend on fuse being installed.  But it would be good to eventually configure Hudson to run these, to verify that fuse continues to show signs of life as Hadoop evolves.\n\nSo, in summary, Hudson will not generate a clean report card for this issue, since it will contain some files that don't have the Apache license, and Hudson will not, at this point, automatically run any new JUnit tests for it.  But that doesn't mean that some licenses and tests shouldn't still be added before we commit the patch.", "Hi Doug,\n\nI will add the header to all the files - think I just had it in the C file.\n\nSure, I will add a Python script or something to drive creating a few files in DFS sand then trying to ls and cat them from a mount.\n\npete\n", "> I will add a Python script or something to drive creating a few files in DFS sand then trying to ls and cat them from a mount.\n\nIt will be easier to integrate Java unit tests.  Also, currently I don't think we require Python, so I wouldn't want to add a system dependency just to test one component.  Perhaps, if you don't like Java, you could write tests in C or as bash scripts, then somehow hook them into the test-contrib target?", "FWIW, Hudson nightly and patch builds do run with -Dcompile.c++=yes so that tests for Pipes and libhdfs get built and run.  What doesn't get built is the eclipse plugin and the native compression library (libhadoop).", "Latest update - includes all the headers for license in every file and a test/TestFuseDFS.java . Not sure how to link this into the other build.xmls to have it buil\\t and run but assume we don't want that right now anwyay.\n", "I have created HADOOP-3264 noting the fact that permissions/owner/group-owner get/set isnt supported in libhdfs, and would be useful for fuse-hdfs. Not a blocker for this JIRA, but related all the same.\n\n", "I updated the patch and am hoping this operation re-starts things JIRA wise - ie runs tests and email Doug.\n", "newest patch.\n", "incremeneted patch #", "Pete,\n\nI havent had time to test your latest patch, but things seems to be improving. I note your comments about exporting the fuse mount. There is a README.NFS in the fuse distribution, which concerns exporting FUSE mounts.  I have copied it in verbatim below from version 2.7.3  - seems not quite mature yet.\n\n{noformat}\nFUSE module in official kernels (>= 2.6.14) don't support NFS\nexporting.  In this case if you need NFS exporting capability, use the\n'--enable-kernel-module' configure option to compile the module from\nthis package.  And make sure, that the FUSE is not compiled into the\nkernel (CONFIG_FUSE_FS must be 'm' or 'n').\n\nYou need to add an fsid=NNN option to /etc/exports to make exporting a\nFUSE directory work.\n\nYou may get ESTALE (Stale NFS file handle) errors with this.  This is\nbecause the current FUSE kernel API and the userspace library cannot\nhandle a situation where the kernel forgets about an inode which is\nstill referenced by the remote NFS client.  This problem will be\naddressed in a later version.\n\nIn the future it planned that NFS exporting will be done solely in\nuserspace.\n{noformat}\n\nRegards\n\nC", "+1 overall.  Here are the results of testing the latest attachment \nhttp://issues.apache.org/jira/secure/attachment/12380454/patch2.txt\nagainst trunk revision 645773.\n\n    @author +1.  The patch does not contain any @author tags.\n\n    tests included +1.  The patch appears to include 13 new or modified tests.\n\n    javadoc +1.  The javadoc tool did not generate any warning messages.\n\n    javac +1.  The applied patch does not generate any new javac compiler warnings.\n\n    release audit +1.  The applied patch does not generate any new release audit warnings.\n\n    findbugs +1.  The patch does not introduce any new Findbugs warnings.\n\n    core tests +1.  The patch passed core unit tests.\n\n    contrib tests +1.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2277/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2277/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2277/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2277/console\n\nThis message is automatically generated.", "Can someone please validate that this works for them?", "I finally got this to compile, after modifying plain-bootstrap.sh and src/Makefile.am.  The latter has some hardwired paths to make things work for Pete.  Most of the stuff in the former is stuff that's already known to Hadoop's build (JDK location, libhdfs location, etc.)\n\nIt should be possible to get this to build from the top-level build.xml, provided:\n - one specifies a -Dcompile.fuse=true or somesuch option\n - one has the appropriate unix environment (g++ installed, libfuse-dev installed, etc.)\nThe environmental requirements should be described as best as possible in the README.  The build should be dependent on libhdfs.\n\nPete, are you familiar with Ant?\n", "Addressed doug's concerns.\n\nI got rid of the home/pwyckoff sutff in Makefie.am, switched all the autoconf variables to match build.xml env vars and added a compile-fusedfs target to src/contrib/build-contrib.xml and I updated the documents.\n\nSo, now to build, you:\n\nant compile-contrib -Dfusedfs=1\n\n", "patch2.txt old news\n", "patch3.txt\n", "-1 overall.  Here are the results of testing the latest attachment \nhttp://issues.apache.org/jira/secure/attachment/12380975/patch3.txt\nagainst trunk revision 645773.\n\n    @author +1.  The patch does not contain any @author tags.\n\n    tests included +1.  The patch appears to include 13 new or modified tests.\n\n    patch -1.  The patch command could not apply the patch.\n\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2335/console\n\nThis message is automatically generated.", "Hi, \nprobably I'm missing something, but I don't understand how patch mechanism works. (Actually, I'm not sure this is the appropriate way, in term of netiquette, to find help)\nDo I download and untar every files present above?\nCould someone suggest me how I can install this sw?\n\nthanks in advance\n\nMaurizio", "I have some minor issues. I was working on compiling on Friday afternoon, but Doug beat me to it with identical comments ;-)\n\n\n * I see that the ant script calls the make file, with the correct env vars overridden. In that case, do we need the bootstrap, automake, configure etc? Why not a simpler build system like that used by libhdfs etc? The makefile contains references to the system that configure was run on.\n * build-contrib.xml in patch3.txt has the wrong path.\n * I think it would be better if the built fuse_dfs module was placed in $HADOOP_HOME/contrib/fuse-dfs, in a similar manner to libhdfs, etc\n * If we're keeping configure et al:\n ** README.BUILD refers to bootstrap.sh, while the script is plain_bootstrap.sh\n ** the configure script doesnt identify the JARCH for non-64bit platforms - see config.log for my platform\n{code}\nconfigure:1377: checking target system type\nconfigure:1391: result: i686-pc-linux-gnu\n{code}\nbut JARCH is unset, should be i386. I presume this works ok from the ant script?\n * fuse_dfs_wrapper.sh has some issues - perhaps you could base this more closely on the fuse_dfs.sh I attached previously to this JIRA?\n  ** various variables have to be written in, eg JAVA_HOME, OS_ARCH has already been identified in configure/ant?, why cant they be set in the script?\n  ** fuse_dfs should be called with \"$@\" instead of $1 $2. However, if i use \"$@\" then and call the wrapper script via mount, then mount add -o ro to the end, and fuse_dfs cant handle this?\n  ** the classpath is hard-coded - why can't you identify all the classpath automatically from the HADOOP_HOME path?\n \n\nWill test new build in due course.\n\nMaurizio - see http://en.wikipedia.org/wiki/Patch_(Unix) and apply the patch to a recent release of Hadoop. It requires FUSE.", "\nFixed a few changes and addressed the points Craig and Doug brought up.\n\nChanges:\n\n1. I changed the top-level build to have a compile-contrib-fuse target that exports the right properties and then has a subant task of the build.xml in the fuse-dfs directory.\n   NOTE: I couldn't use the normal compile-contrib path as that path doesn''t inherit properties to the subant call.  I think it's much easier for the million properties set in the top level build.xml to be available in the subant call rather than trying to re-create them. And since this is a build, I needed arch related ones.\n\n2. fixed fuse_dfs_wrapper.sh to set env vars only if not set and to pass all the args ala $@ to the executable\n\n3. added build.xml in src/contrib/fuse-dfs\n  It (A) calls bootstraph.sh to get a correct Makefile and then (B) calls the Makefile to build fuse_dfs\n  NOTE: I left all the autoconf stuff because doing something like libhdfs where there's only a Makefile would cause bad builds for environments unlike mine - I end up having to edit Makefile for libhdfs to set OS_ARCH=amd64.\n\n4. I updated README and I removed README.build", "Hi Maurizio,\n\nTo apply this patch, go to your checkout of hadoop - the top level and do \"patch -p0 < patch4.txt\"\n\nThis should apply it, and then read the README is src/contrib/fuse-dfs on instructions on how to compile. Let me know if you have any problems.\n\n-- pete\n", "patch4.txt", "-1 overall.  Here are the results of testing the latest attachment \nhttp://issues.apache.org/jira/secure/attachment/12381125/patch4.txt\nagainst trunk revision 645773.\n\n    @author +1.  The patch does not contain any @author tags.\n\n    tests included +1.  The patch appears to include 13 new or modified tests.\n\n    javadoc +1.  The javadoc tool did not generate any warning messages.\n\n    javac +1.  The applied patch does not generate any new javac compiler warnings.\n\n    release audit +1.  The applied patch does not generate any new release audit warnings.\n\n    findbugs -1.  The patch appears to cause Findbugs to fail.\n\n    core tests -1.  The patch failed core unit tests.\n\n    contrib tests -1.  The patch failed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2339/testReport/\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2339/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2339/console\n\nThis message is automatically generated.", "This still doesn't work out of the box for me.  I'll attach a new version that does.", "Here's a version that builds for me.  I changed it to fit within the normal contrib compilation framework.\n\nIf you want to compile it alone, manually, then you must first run 'ant -Dcompile.c++=1 compile-libhdfs' at root, then run 'ant -Dfusedfs=1' when connected to src/contrib/fuse-dfs, or you can simply run 'ant compile-contrib -Dcompile.c++=1 -Dcompile.fusedfs=1' at top-level to compile it along with all other contrib modules.\n\nI have not yet tested that it runs, however.\n\nBuilding this generates a lot of files that we'll need to add to the svn ignore list, and that are not removed by 'make clean'.  Are all of these needed?\n", "The right version of the patch...", "After applying the patch you must 'chmod +x src/contrib/fuse-dfs/bootstrap.sh' before building the first time.\n", "-1 overall.  Here are the results of testing the latest attachment \nhttp://issues.apache.org/jira/secure/attachment/12381140/HADOOP-4.patch\nagainst trunk revision 645773.\n\n    @author +1.  The patch does not contain any @author tags.\n\n    tests included +1.  The patch appears to include 13 new or modified tests.\n\n    javadoc +1.  The javadoc tool did not generate any warning messages.\n\n    javac +1.  The applied patch does not generate any new javac compiler warnings.\n\n    release audit +1.  The applied patch does not generate any new release audit warnings.\n\n    findbugs -1.  The patch appears to cause Findbugs to fail.\n\n    core tests -1.  The patch failed core unit tests.\n\n    contrib tests +1.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2344/testReport/\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2344/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2344/console\n\nThis message is automatically generated.", "thanks doug. I will provide a make clean that cleans up everything.\n\n", "Doug,\n\nYour patch didn't seem to work for me unless I modified the entries using ${basedir} in src/contrib/fuse-dfs/build.xml to  append ../../../ to it. It seems in my build, basedir is now the fuse-dfs dir whereas before, I think it was pointing to the top level ??\n\nI'm also uploading patch5.txt which  includes a clean: target that does all the clean up in the Makefile in src/contrib/fuse-dfs.\n\nBut, note I had to include the below difference to src/contrib/fuse-dfs/build.xml (other than that it's identical to your patch).\n\nthanks, pete\n\n\n\n--- fuse-dfs/build.xml  2008-04-29 17:41:55.000000000 -0700\n+++ fuse-dfs.new/build.xml  2008-04-29 17:37:47.000000000 -0700\n@@ -22,10 +22,9 @@\n   <!-- fuse-dfs targets.                                                  -->\n   <!-- ================================================================== -->\n   <target name=\"compile\" if=\"fusedfs\">\n-    <property name=\"fuse-dfs.dir\" value=\"${basedir}/src/contrib/fuse-dfs/\"/>\n+    <property name=\"fuse-dfs.dir\" value=\"${basedir}\"/>\n     <exec dir=\"${fuse-dfs.dir}\" executable=\"${fuse-dfs.dir}/bootstrap.sh\">\n     </exec>\n-\n     <exec dir=\"${fuse-dfs.dir}\" executable=\"make\">\n       <env key=\"OS_NAME\" value=\"${os.name}\"/>\n       <env key=\"OS_ARCH\" value=\"${os.arch}\"/>\n@@ -29,11 +28,10 @@\n     <exec dir=\"${fuse-dfs.dir}\" executable=\"make\">\n       <env key=\"OS_NAME\" value=\"${os.name}\"/>\n       <env key=\"OS_ARCH\" value=\"${os.arch}\"/>\n-      <env key=\"HADOOP_HOME\" value=\"${basedir}\"/>\n+      <env key=\"HADOOP_HOME\" value=\"${basedir}/../../..\"/>\n       <env key=\"PROTECTED_PATHS\" value=\"/,/Trash,/user\"/>\n       <env key=\"PACKAGE_VERSION\" value=\"0.1.0\"/>\n       <env key=\"FUSE_HOME\" value=\"/usr/local\"/>\n-      <env key=\"LIBHDFS_BUILD_DIR\" value=\"${basedir}/src/c++/libhdfs\"/>\n     </exec>\n   </target>\n </project>\n", "Note, the comment I made about the {basedir} in src/contrib/fuse-dfs/build.xml seems to be the thing that also made the last hudson build fail.", "sorry - should be patch4.txt\n", "Your diff above isn't to my patch.  My patch, e.g., sets HADOOP_HOME to ${hadoop.home}, not to ${basedir}.  In my patch, src/contrib/fuse-dfs/build.xml does not refer to ${basedir} at all, since ${basedir} is the CWD and can thus be elided in relative paths.", "fixes the cleanup problem", "Your changes to src/c++/libhdfs/Makefile break things for me.  Also, you undid one of my changes to build.xml, making compile-libhdfs conditional on compile.c++.  This is required to make compile-libhdfs an optional target, which we must do now that compile-contrib depends on it.  Finally, there are still a lot of generated symlinks and files left in src/contrib/fuse-dfs after a 'make clean'.", "all im' trying to do is add:\n\nclean:\n        rm -rf autom4te.cache config.guess config.log config.status config.sub configure depcomp src/.deps install-sh Makefile.in src/Makefile.in src/Makefile missing Makefile src/fuse_dfs.o src/fuse_dfs\n\nto src/contrib/Makefile.am   (this will clean up everything)\n\nand change src/contrib/build.xml to do executable bin/sh with arg value=bootstrap.sh to avoid the chmod +x problem.\n\nI thought that is all i changed.\n\n", "patch6.txt", "this is it", "last patch is just doug's HADOOP-4 and the 2 above changes and removing make clean from boostrap.sh and configure.ac", "> avoid the chmod +x problem\n\nI wouldn't worry too much about that.  The patch doesn't remember that it's executable, but subversion will.  But, sure, fixing that is fine too.\n\n> I thought that is all i changed.\n\nDid you 'svn revert -R .' and make sure that 'svn stat' reported nothing before applying my patch and making new mods?", "Yes, I think what may have happened is I uploaded an older patch.\n\nThis time I did a revert, applied HADOOP-4 and just edited those files and created the patch.", "This got assigned to by mistake. I haven't followed this jira closely till now.", "-1 overall.  Here are the results of testing the latest attachment \nhttp://issues.apache.org/jira/secure/attachment/12381278/patch6.txt\nagainst trunk revision 645773.\n\n    @author +1.  The patch does not contain any @author tags.\n\n    tests included +1.  The patch appears to include 13 new or modified tests.\n\n    javadoc +1.  The javadoc tool did not generate any warning messages.\n\n    javac +1.  The applied patch does not generate any new javac compiler warnings.\n\n    release audit +1.  The applied patch does not generate any new release audit warnings.\n\n    findbugs -1.  The patch appears to cause Findbugs to fail.\n\n    core tests -1.  The patch failed core unit tests.\n\n    contrib tests +1.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2360/testReport/\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2360/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2360/console\n\nThis message is automatically generated.", "\nHi Doug,\n\nThe following is the compile error that happens when running with Hudson - I don't understand why it's having this problem. Can you look at it? Thanks for your help with this.\n\npete\n\n-------------------\n\n\nBUILD FAILED\n/zonestorage/hudson/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build.xml:780: The following error occurred while executing this line:\n/zonestorage/hudson/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/contrib/build.xml:39: The following error occurred while executing this line:\n/zonestorage/hudson/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/contrib/build-contrib.xml:157: /zonestorage/hudson/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/contrib/fuse-dfs/classes not found.\n\n", "> I don't understand why it's having this problem.\n\nThe problem is that the \"jar\" and \"package\" targets are failing in fuse-dfs.", "Somehow you lost my 'if=\"compile.c++\" addition to the compile-libhdfs target in build.xml again.  I re-added that, updated the README, and added \"jar\" and \"package\" targets to make Hudson happier.\n\nThis now builds for me.  I also tested it.  I was able to mount an HDFS filesystem list directories, and read files.", "-1 overall.  Here are the results of testing the latest attachment \nhttp://issues.apache.org/jira/secure/attachment/12381349/HADOOP-4.patch\nagainst trunk revision 645773.\n\n    @author +1.  The patch does not contain any @author tags.\n\n    tests included +1.  The patch appears to include 13 new or modified tests.\n\n    javadoc +1.  The javadoc tool did not generate any warning messages.\n\n    javac +1.  The applied patch does not generate any new javac compiler warnings.\n\n    release audit +1.  The applied patch does not generate any new release audit warnings.\n\n    findbugs +1.  The patch does not introduce any new Findbugs warnings.\n\n    core tests -1.  The patch failed core unit tests.\n\n    contrib tests +1.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2372/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2372/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2372/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2372/console\n\nThis message is automatically generated.", "Comments on the latest patch:\n\n\n *  +1 It compiles for me from ant\n\n * -1 but only after I build libhdfs or alter fuse-dfs/src/Makefile.am to refer to $(HADOOP_HOME)/libhdfs and not $(HADOOP_HOME)/build/libhdfs\n\n * -1fuse_dfs_wrapper.sh : LD_LIBRARY_PATH should be set after JAVA_HOME; LD_LIBRARY_PATH should contain $HADOOP_HOME/libhdfs\n\n * -1 I think that there should be package target in fuse-dfs/build.xml that copies fuse-dfs stuff into $HADOOP_HOME/contrib/fuse-dfs - this to be the final place that fuse-dfs can always be found at in a hadoop installation\n\n * Note: when I run my fuse-dfs mount as root, I have to turn dfs.permissions to off. root doesnt have any permissions in our dfs setup, so hence everything gets permission denied. In future, we should alter libhdfs such that it has a \"super-user API\" - i.e. if I access file X as user Y, am I permitted to access it? See also HADOOP-3264, which deals with supporting permissions in libhdfs\n\n", "a) Rather than use LD_LIBRARY_PATH, would it be better to set a runtime link path that used $ORIGIN?\n\nb) what happens if root is part of the super group?", "@Allen\n\na) not sure what you mean here. Ideally, I'd like to use fuse_dfs_wrapper.sh in my fstab/automount lines, so it should have all env vars already set, if they can be derived at build or run time.\n\nb) This works fine. Permissions within fuse-dfs are a whole other kettle of fish, so I think I'll keep quiet until fuse-dfs is committed, then start another JIRA. It's just worth noting that if you want to share a fuse-dfs mount between multiple users, then the DFS permissions model will be broken.", "> -1 but only after I build libhdfs \n\nThe confusion is that libhdfs is now conditioned on compile.c++, but fuse-dfs does not, so it's possible to invoke ant at the top level in such a way that it will try to compile fuse-dfs without having compiled libhdfs.  To fix this we should probably make fuse-dfs conditional on compile.c++ too.  In any case, you need to specify compile.c++=1 for top-level builds of fuse-dfs to work.\n\n> -1 I think that there should be package target in fuse-dfs/build.xml that copies fuse-dfs stuff into $HADOOP_HOME/contrib/fuse-dfs\n\nGood idea.   The fuse-dfs package target should copy things to ${dist.dir}/contrib/${name}.\n\nWho will update the patch?\n\n", "i added the package target and also made fuse dfs build dependent on both compile.c++ and fusedfs properties.\n\nI also remove aclocal.m4 as that's a generated file.\n", "HADOOP-4.patch\n", "-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12381459/HADOOP-4.patch\n  against trunk revision 653638.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 13 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    -1 findbugs.  The patch appears to cause Findbugs to fail.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2401/testReport/\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2401/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2401/console\n\nThis message is automatically generated.", "my bad - added if for the package target.\n", "Here's a new version that:\n  - includes the README in releases\n  - doesn't overload \"compile.c++\" for libhdfs, but adds a new \"libhdfs\" property, since \"compile.c++\" is used by Hudson on Solaris, and libhdfs doesn't (yet) compile on Solaris.\n  - includes libhdfs & fuse-dfs in releases if libhdfs=1 and fusedfs=1\n", "+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12381541/HADOOP-4.patch\n  against trunk revision 653906.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 13 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2415/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2415/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2415/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2415/console\n\nThis message is automatically generated.", "Any reason for me not to commit this now?", "Sorry, will review over the weekend. Thesis writing this week...", "Minor notes:\n * I had to recompile set ant property -Dlibhdfs=1 to get this to compile - perhaps a warning message would have been desirable if the properties are not set?\n * Concerning the note on compiling libhdfs for 64bit arch:\n{noformat}\nNOTE: for amd64 architecture, libhdfs will not compile unless you edit\nthe Makefile in src/c++/libhdfs/Makefile and set OS_ARCH=amd64\n(probably the same for others too).\n{noformat}\nIn the editing of src/c++/libhdfs/Makefile it is sufficient to just change -m32 to -m64 in the CPPFLAGS and LDFLAGS lines, until HADOOP-3344 is ready.\n\nOtherwise, good to be committed.\n\nC", "I made the change to the README that craig suggested.\n", "> I made the change to the README that craig suggested. \n\nYes, but it doesn't look like you started with my patch, but rather with an older version...", "yes, it looks like i was thinking the jira attachment #-ing would be chronological. I think how to set the architecture to 64 bit is pretty trivial difference and would be done either way, so i recommend we just commit the latest good patch and then move on from there.\n\n--  pete\n", "I just committed this.  Thanks, Pete!", "Excellent. Thanks for your good work Pete & Doug. Doug or someone, can you create a contrib/fuse-dfs component to JIRA for any future issues.\n\nCheers :-)", "done!", "Since we have the component, we might as well use it!", "Some facebook related stuff found in the patch and got into trunk.  For example, search \"facebook\" in src/contrib/fuse-dfs/configure.ac", "> Some facebook related stuff found in the patch and got into trunk.\n\nDo you want to file another issue for this?\n", "Sure, created HADOOP-3476", "Integrated in Hadoop-trunk #509 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/509/])", "Fuse-dfs Make Error!!!\n\nHelp me please..\n\nMake Compile Error Message is....\n==================================================================================================\n[root@chosuan2 fuse-dfs]# make\nMaking all in .\nmake[1]: Entering directory `/root/fuse/fuse-dfs'\nmake[1]: Leaving directory `/root/fuse/fuse-dfs'\nMaking all in src\nmake[1]: Entering directory `/root/fuse/fuse-dfs/src'\ngcc  -Wall -O3 -L/hdfs/shared -lhdfs -L/usr/local/lib -lfuse -L/usr/jdk1.6.0_06/jre/lib/amd64/server -ljvm   -o fuse_dfs fuse_dfs.o  \n/usr/bin/ld: skipping incompatible /usr/jdk1.6.0_06/jre/lib/amd64/server/libhdfs.so when searching for -lhdfs\n/usr/bin/ld: cannot find -lhdfs\ncollect2: ld returned 1 exit status\nmake[1]: *** [fuse_dfs] error 1\nmake[1]: Leaving directory `/root/fuse/fuse-dfs/src'\nmake: *** [all-recursive] error 1\n==================================================================================================", "chousan:\n\nlibhdfs isnt build 64bit by default. You need to rebuild it 64bit, ensuring to remove -m32 from the Makefile.\n\nSee HADOOP-3344 for more details", "Moved the usage info form the release note to the description so that the release notes file can be generated automatically with modest sized notes for each item.\n\nCongratulations for getting this in!", "By looking at \n\nhdfs            -fstype=fuse,rw,nodev,nonempty,noatime,allow_other  :/path/to/fuse_dfs_moutn/fuse_dfs.sh\\#dfs\\://namenode\\:9000\n\nDoes this still work with a recent version of autofs? Can someone please confirm"], "derived": {"summary": "This is a FUSE module for Hadoop's HDFS. It allows one to mount HDFS as a Unix filesystem and optionally export\nthat mount point to other machines.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "tool to mount dfs on linux - This is a FUSE module for Hadoop's HDFS. It allows one to mount HDFS as a Unix filesystem and optionally export\nthat mount point to other machines."}, {"q": "What updates or decisions were made in the discussion?", "a": "By looking at \n\nhdfs            -fstype=fuse,rw,nodev,nonempty,noatime,allow_other  :/path/to/fuse_dfs_moutn/fuse_dfs.sh\\#dfs\\://namenode\\:9000\n\nDoes this still work with a recent version of autofs? Can someone please confirm"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-5", "title": "need commons-logging-api jar file", "status": "Closed", "priority": "Minor", "reporter": "Owen O'Malley", "assignee": null, "labels": [], "created": "2006-02-07T03:04:15.000+0000", "updated": "2006-08-03T17:46:26.000+0000", "description": "The hadoop lib directory needs a copy of the commons-logging-api jar file from nutch's lib directory.", "comments": ["You're right, this is needed to start the jobtracker, which is not tested by current unit tests.  We really need a unit test that uses the jobtracker and tasktracker.  Thanks for catching this, Owen!"], "derived": {"summary": "The hadoop lib directory needs a copy of the commons-logging-api jar file from nutch's lib directory.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "need commons-logging-api jar file - The hadoop lib directory needs a copy of the commons-logging-api jar file from nutch's lib directory."}, {"q": "What updates or decisions were made in the discussion?", "a": "You're right, this is needed to start the jobtracker, which is not tested by current unit tests.  We really need a unit test that uses the jobtracker and tasktracker.  Thanks for catching this, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-6", "title": "missing build directory in classpath", "status": "Closed", "priority": "Minor", "reporter": "Owen O'Malley", "assignee": null, "labels": [], "created": "2006-02-07T03:19:03.000+0000", "updated": "2006-08-03T17:46:26.000+0000", "description": "When running a developer build, the hadoop script needs the build directory on the classpath so that the job tracker can find the webapps directory.", "comments": ["I just committed this.  Thanks, Owen!"], "derived": {"summary": "When running a developer build, the hadoop script needs the build directory on the classpath so that the job tracker can find the webapps directory.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "missing build directory in classpath - When running a developer build, the hadoop script needs the build directory on the classpath so that the job tracker can find the webapps directory."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-21", "title": "the webapps need to be updated for the move from nutch", "status": "Closed", "priority": "Minor", "reporter": "Owen O'Malley", "assignee": null, "labels": [], "created": "2006-02-07T04:12:05.000+0000", "updated": "2009-07-08T16:51:40.000+0000", "description": "The webapp files need to be updated so that the reference the new packages. I'll include a patch to fix it.", "comments": ["I forgot to change the title line too. This patch fixes both problems.", "I applied the patch.  Thanks, again, Owen."], "derived": {"summary": "The webapp files need to be updated so that the reference the new packages. I'll include a patch to fix it.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "the webapps need to be updated for the move from nutch - The webapp files need to be updated so that the reference the new packages. I'll include a patch to fix it."}, {"q": "What updates or decisions were made in the discussion?", "a": "I applied the patch.  Thanks, again, Owen."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-22", "title": "remove unused imports", "status": "Closed", "priority": "Trivial", "reporter": "Sami Siren", "assignee": null, "labels": [], "created": "2006-02-07T04:33:17.000+0000", "updated": "2006-08-03T17:46:26.000+0000", "description": "Following patch will remove unused imports from java source files", "comments": ["I just applied this.  Thanks, Sami.  Sorry I messed this up the first time..."], "derived": {"summary": "Following patch will remove unused imports from java source files.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "remove unused imports - Following patch will remove unused imports from java source files."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just applied this.  Thanks, Sami.  Sorry I messed this up the first time..."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-23", "title": "single node cluster gets one reducer", "status": "Closed", "priority": "Minor", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-02-07T04:45:26.000+0000", "updated": "2013-06-22T00:35:03.000+0000", "description": "Running on a single node cluster (it runs a job tracker and a single task tracker), even though my application asks for 7 reduces, it only gets one. I haven't tracked down what is happening yet.", "comments": ["Look closely at the order the config files are loaded.  This code has been in flux, and may have bugs.", "Ok, I found the problem and it was a config file ordering problem.\n\nThe problem is that the job's config file is loaded as a default resource and the site config file is a final resource and I had set the number of reduces to one in the site file, because on a single node cluster that seemed like a reasonable default.\n\nObviously, removing the default in the site file is necessary.\n\nI realize now that is why you created the separate mapred-default.xml. It doesn't feel right to have a config file that is specific for map/reduce, but I think there should be some kind of default config file that the user can override in the job but that is editted by the site.\n\nThings that probably belong there:\n  fs.default.name\n  mapred.job.tracker\n  mapred.map.tasks\n  mapred.reduce.tasks", "Integrated in flume-trunk #320 (See [https://builds.apache.org/job/flume-trunk/320/])\n    FLUME-1653: Update Hadoop-23 profile to point to hadoop-2 alpha artifacts (Revision 831a86fc5501a8624b184ea65e53749df31692b8)\n\n     Result = SUCCESS", "Integrated in HBase-0.94-security #176 (See [https://builds.apache.org/job/HBase-0.94-security/176/])\n    HBASE-8743 upgrade hadoop-23 version to 0.23.7 (Revision 1495626)\n\n     Result = SUCCESS", "Integrated in HBase-0.94 #1021 (See [https://builds.apache.org/job/HBase-0.94/1021/])\n    HBASE-8743 upgrade hadoop-23 version to 0.23.7 (Revision 1495626)\n\n     Result = SUCCESS"], "derived": {"summary": "Running on a single node cluster (it runs a job tracker and a single task tracker), even though my application asks for 7 reduces, it only gets one. I haven't tracked down what is happening yet.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "single node cluster gets one reducer - Running on a single node cluster (it runs a job tracker and a single task tracker), even though my application asks for 7 reduces, it only gets one. I haven't tracked down what is happening yet."}, {"q": "What updates or decisions were made in the discussion?", "a": "Integrated in HBase-0.94 #1021 (See [https://builds.apache.org/job/HBase-0.94/1021/])\n    HBASE-8743 upgrade hadoop-23 version to 0.23.7 (Revision 1495626)\n\n     Result = SUCCESS"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-24", "title": "make Configuration an interface", "status": "Closed", "priority": "Major", "reporter": "Doug Cutting", "assignee": "Doug Cutting", "labels": [], "created": "2006-02-07T05:09:24.000+0000", "updated": "2009-04-23T19:24:57.000+0000", "description": "The Configuration class should become an interface, e.g.:\n\npublic interface Configuration {\n  String get(String nam);\n  String set(String name, String value);\n\n  int getInt(String name);\n  void setInt(String name, int value);\n  float getFloat(String name);\n  void setFloat(String name, float value);\n  //... other utility methods based on get(String) and set(String,String) ...\n}\n\nAn abstract class named ConfigurationBase should be implemented as follows:\n\npublic abstract class ConfigurationBase implements Configuration {\n  abstract public String get(String nam);\n  abstract public String set(String name, String value);\n\n  public  int getInt(String name) { ... implementation in terms of get(String) ... }\n  public void setInt(String name, int value) {... implementation in terms of set(String, String) ...}\n  public float getFloat(String name)  { ... implementation in terms of get(String) ... }\n  public void setFloat(String name, float value)  {... implementation in terms of set(String, String) ...}\n  //... other utility methods based on get(String) and set(String,String) ...\n}\n\nA concrete, default implementation will be provided as follows:\n\npublic class ConfigurationImpl implements Writable extends ConfigurationBase {\n  private Properties properties;\n\n  // implement abstract methods from ConfigurationBase\n  public String get(String name) { ... implemented in terms of props ...}\n  public String set(String name, String value) { .. implemented in terms of props ... }\n\n  // Writable methods\n  public write(DataOutputStream out);\n  public readFields(DataInputStream in);\n\n  // permit chaining of configurations\n  public Configuration getDefaults();\n  public void setDefaults(Configuration defaults);\n}\n\nOnly code which creates configurations should need to be updated, so this shouldn't be a huge change.", "comments": ["Hi,\n\nI have implemented the configuration interface.\n\nRemarks would be appreciated.\n\nGal.", "Fix previous patch. Forgot to add the actual files :)", "\nThe code contains getObject and setObject methods.  Is this a good idea - considering that it means that the ConfigurationImpl cannot be written out and read back without losing those values?  I think it would be better to remove these and maintain the ability to serialize and deserialize.  (Is there a reason not to have Configuration extend Writable?)\n\nWhat is a use case for ConfigurationBase.getProperties() which returns an object of unknown type?  It might be better to have something like String[] getPropertyNames(String regex).  (And this should be in Configuration also.)  This allows for things like treating the property name space as a hierarchy, so that e.g. you can find all the properties beginning with some string.\n\n- David\n\n\n", "In Nutch in many places we create objects with a \"pseudo-singleton\" semantics (i.e. one per task), that are costly to create. These methods were added to provide a convenient caching mechanism to carry around these objects within the same task. They don't have to be serialized/deserialized.", "Since this is my first interaction with Hadoop code \"on a first name basis\" :) I didn't want to rock the boat too much.\n\nMy first goal was to implement the interfaces and the implementation with as little as possible major changes all arround the code. As it is now all test are still working and nothing broken.\n\nSo if the patch would \"pass\" the reviews it would be easier to make additions when all blocks are in place.\n\ngetPropertyNames is good idea to add as an interface to configuration.\n\nThanks.", "Added some missing interfaces", "Last minute changes :(\n\nHad to move getObject setObject to ConfigurationImpl", "I finally had a chance to look closely at this.  One problem I now see is that it's incompatible: code that previously called 'new Configuration()' has to be updated to instead call 'new ConfigurationImpl()'.  Nutch is a good test-case for back-compatibility, since it uses so much of Hadoop's APIs.  Ideally we can figure out a way to do this so that, e.g., the new Hadoop jar can be dropped into existing Nutch and work correctly.\n\nI think this means that the interface cannot be named 'Configuration'.  Instead we should probably call it ConfigurationInterface, and change ConfigurationImpl to be Configuration.  Does that sound like a good solution?\n\nThanks for the patch, and thanks for your patience!", "I think that all declarations and parameters should say Configuration that is the interface.\nAnd the constructors should be the only places where ConfigurationImpl\nappears explicitly. E.g. all occurrences\n      private static ConfigurationImpl conf = new ConfigurationImpl();\nshould be replaced by\n      private static Configuration conf = new ConfigurationImpl();\nI haven't seen a lot of nutch code, but if it uses only the API, then\nthat should work. If it does construct config instances then there should be\na static newInstance() somewhere I guess.\n", "forgot to set abstract for getConfResource... in ConfigurationBase", "Hi,\n\nIn regards to the code compatibility - For Hadoop all new code is already included (tests also) is in this patch.\n\nFor Nutch I already have the patch ready and it is not that big a change since most code uses Configuration (so the actual change is only in construction) so ConfigurationImpl has a copy constructor that accepts a Configuration. I actualy tested it also. I can submit it here as well if we decide to use this patch.\n\nI'm not sure it would be a good idea to change the current design/implementation just for the sake of compatibility. I think the design(Doug's) and the implementation are clean and shall be more understandable to implementers/extenders.\n\n", "API back-compatibility is important.  We'd like folks to be able to upgrade to new releases without having to change their code.  The convention we've had is that, code which compiles against release 0.X without deprecation warnings will still compile against release 0.X+1, but may require changes to compile against release 0.X+2.  Is that reasonable?\n\nIf so, then 'new Configuration()' must still create a usable configuration and not a compilation error in release 0.3.  It may be deprecated, but it should still work.  Yes, this is a pain, since Configuration would be a great name for the interface, but there's code out there (more than just Nutch) that we should try not to break.  Nutch is a good, public test case for this sort of back-compatibility, that's the reason I mention it.\n\nAnother approach might be to use a new package.  We could put the new classes in org.apache.hadoop.config, and then have org.apache.hadoop.conf.Configuration be a deprecated subclass of org.apache.hadoop.config.ConfigurationImpl.  Then, after 0.3 is released, we could remove the entire org.apache.hadoop.conf package.", "I totaly agree with you Doug on API back compatibilty.\n\n+1 for org.apache.hadoop.config\n\nSo if Configuration subclasses ConfigurationImpl you would not have to introduce changes in Nutch for now? and you can postpone it to a suitable time?\n\n", "Right, we should not add dependencies to Nutch on as-yet-unreleased features in Hadoop.  But we want Nutch users to be able to use the current Hadoop trunk.  So once we've made the next Hadoop release, and upgraded Nutch's trunk to use that release, then we can introduce changes to Nutch.", "Great, I shall rewrite and submit a new patch for review.", "Hi, at last I found some time :(\n\nAttached the patch that changes Configuration into an Interface.\n\nIt passes all tests.\n\nEnjoy.", "a patch that changes Configuration class to interface", "please ignore previous attachment. this is the correct file.", "When I apply this to trunk things don't compile.  I think you forgot to include ConfigurationImpl.java.\n\nI also wonder if, since we're editing all the places where a configuration is constructed, we might change things to use a factory.  So, instead of 'new ConfigurationImpl()' we'd have something like 'ConfigurationFactory.get()'.  That way, we could in the future change the default implementation without changing all of these places again.  E.g., if you're nesting Hadoop in JMX then you might specify a JMX configuration factory as the default.  Does that make sense?\n", "I agree with you Doug. I also took the liberty to make the ConfigurationFactory an interface.\n\nsigned sealed and delivered (without missing classes I hope :-)  )\n\nGal.", "Done.... for some reason attaching thw file did not produce an email.", "Any comment???", "Looking at this, I'm not sure moving to a formal Interface for Configuration is the right tactic, not if your goal is to allow for different back-ends. Here's why\n\n-there's a lot of code in Configuration which makes config work easier on top of the underling read/write a few types of name-value mappings, replicating this is expensive and creates an ongoing maintenance nightmare. \n\n-JobConf subclasses Configuration, and there are places where code checks for instanceof JobConf and/or a new JobConf gets created. If you really wanted to hand configuration off to something other than XML file datasources, you need to handle JobConf too, which is trickier.\n\nWhich means, right now, you don't want to extend Configuration, you want to extend JobConf, then work out how to sneak in a different factory. \n\nWhat may be easier would be an inner ConfSource interface, which supported the raw get/set operations, possibly typed, possibly simple strings. Then you'd be able to create Configurations bound to different ConfSource back ends. \n\nWhere there are problems here is in error  handling. Currently its Configuration.loadResource() that faults on bad XML, by throwing a simple RuntimeException. If you allow different conf sources, you need to allow for trouble happening later on, when the LDAP or JDBC runtime decides to throw some exception. Either the ConfSource methods are allowed to throw any Exception, or there is a need for a standard ConfigurationRuntimeException to throw/wrap trouble.\n\n\n\n", "Steve, I agree with your analysis.  This is a very old issue!  Perhaps we should just close it as \"won't fix\" and, if someone needs to integrate Hadoop with a different configuration system, pursue the ConfSource approach?", "I concur; a WONTFIX is a better outcome, with\n\n1. a new improvement 'support multiple configuration back ends' added.\n\n2. a separate issue 'add a specific ConfigurationException that extends RuntimeException to throw for Configuration problems'...that can be added before any back-end improvements, and could also be used to address HADOOP-2081 and any number format exceptions thrown from any HADOOP-2461 -related changes. This is simpler and would seem to be higher priority. Its easier to test, too."], "derived": {"summary": "The Configuration class should become an interface, e. g.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "make Configuration an interface - The Configuration class should become an interface, e. g."}, {"q": "What updates or decisions were made in the discussion?", "a": "I concur; a WONTFIX is a better outcome, with\n\n1. a new improvement 'support multiple configuration back ends' added.\n\n2. a separate issue 'add a specific ConfigurationException that extends RuntimeException to throw for Configuration problems'...that can be added before any back-end improvements, and could also be used to address HADOOP-2081 and any number format exceptions thrown from any HADOOP-2461 -related changes. This is simpler and would seem to be higher priority. Its easier to test, too."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-25", "title": "a new map/reduce example and moving the examples from src/java to src/examples", "status": "Closed", "priority": "Minor", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-02-07T08:15:24.000+0000", "updated": "2012-07-24T19:08:08.000+0000", "description": "The new example is the word count example from Google's paper. I moved the examples into a separate jar file to demonstrate how to run stand-alone application code.", "comments": ["In the patch, it looks like I just deleted Grep instead of moving it to src/examples/.../Grep.java. Please make sure it doesn't disappear.", "A few comments:\n\nI prefer 'ant compile' to do the minimum required for development, so it shouldn't build examples.  Similarly, I prefer 'ant jar' to build only the primary jar.  Examples, test code, javadoc, etc. should be out of the compile-debug loop.\n\nThis should probably go in org.apache.hadoop.examples rather than org.apache.hadoop.mapred.demo, no?\n\nThe mapper is missing a javadoc comment.\n\nThe usage string in the javadoc should probably instead propose something like: bin/hadoop org.apache.hadoop.examples.WordCount, and we should make sure that the examples jar goes on the classpath.\n\nI also wonder whether we should encourage folks to explicitly set the number of map and reduce tasks for each job.  I prefer to think of this as normally a site configuration and/or automatic process.\n\nFinally, since I'm already nitpicking, doesn't the 'try' block in your main() do the same thing as just permitting main() to throw IOException?  The JVM prints a stack trace, no?\n", "Ok, how about if I add a target for \"examples\" that builds the examples tar ball. Although compiling the examples is really fast and if they are compiled by default, there is one less thing for the new user tutorial to explain.\n\nThe new package name makes sense.\n\nI guess we can use the hadoop script to set up the classpath, but the users are going to need to figure out the classpath when they write their own applications. I guess we could go one step further and modify the script to take a jar and run the \"main\" class of the jar. So it would look like:\n% bin/hadoop run my-app.jar [args...]\nFor the example jar, we could use a driver that tested the next string for the class to run, so it would look like:\n% bin/hadoop run build/hadoop-examples.jar (wordcount|grep|...) [args...]\nThoughts? \n\nI think that having the application set the number of maps and reduces is better than having it defined by the cluster. Certainly the number of reduces should be set by the user and/or application rather than the cluster since it controls the output fragmentation. But even the optimum number of map instances depends a lot on how resource hungry the map function is.\n\nAnd finally, you are right that letting main throw out the exception is equivalent, but I bet the Java language spec doesn't require that the JVM print the exception that was thrown out of main. I'll change it. (Too much C++ coding where exceptions don't have stacks associated with them by the runtime system.)", "I don't feel that strongly in particular about \"ant compile\" compiling the examples.  It's more the principle of keeping that command, the default, minimal.  When the next person comes along and adds something optional that compiles to build.xml I don't want them to also add it to \"ant compile\".\n\nOn the other hand, \"ant test\" should be maximal, compiling and testing as much as possible.  My rule is that \"ant clean test\" should be run before every commit.\n\nI like the idea of making bin/hadoop easily extensible with something like 'bin/hadoop run build/hadoop-examples.jar'.  We could by convention create executable jars whose default main() listed the commands that the jar supports.  We could even change hadoop.jar to be like this, moving the command selection logic out of the shell script and into a Java class.  +1\n\nI really think it would be nice if for common MapReduce operations (e.g., sorting, inverting, etc.) on a well configured cluster one does not have to specify number of map tasks or reduce tasks.  That way one can run something on one cluster with 20 single-processor machines, and then turn and run it on another with 200 dual-processor machines with the system doing a reasonable job.  One should also be able to fine-tune things for a particular cluster if one likes, but that should be optional.  There are cases where the precise number of outputs is critical, but there are (in my experience) many more where the precise number of outputs does not matter.\n\nOne way to sidestep this might be to add a standard  '-D' option to bin/hadoop that permits one to specify any configuration option.  That way one could, e.g., always easily set the number of map or reduce tasks for each job, but also not be forced to.\n\nAnd you're right: a JVM probably isn't required to print that stack, but I'm strongly in favor of things that make code smaller (easier to read, easier to maintain), especially example code.", "Ok, here is a new patch.\n\nI added the generic \"jar\" option to bin/hadoop. \nTo get it to work right, I couldn't use the -jar option to java because it ignores the -cp option, so I implemented a utility function that looks in a jar file to find the Main-Class attribute. (org.apache.hadoop.util.PrintJarMainClass)\n\nI moved the examples to org.apache.hadoop.examples.\n\nI created a example driver that you can give either 'wordcount' or 'grep' to run the appropriate class and made it the Main-Class of the example jar file.\n\nI made the map and reduce arguments optional to WordCount. (I thought about using jakarta-common-cli, but it just cluttered up the example.)\n\nI changed the Map class to MapClass so that it didn't conflict with java.utils.Map and put in a javadoc.\n\nI pulled the examples out of the compile and jar targets and added it to the test target.", "I realized that I should probably put the example jar file into the distribution. \n\nI also remove an incorrect javadoc attribute @date. (Why doesn't javadoc have a @date?)", "I made a few changes and committed this.  In particular, I:\n  - moved  the package.html to the examples package;\n  - put  the examples javadoc in a separate group, so it's clear it's not part of the core API\n  - put the version in the example jar file name\n  - added Apache licenses to newly contributed files\n", "When the example tarball was renamed, the package target was broken.", "Okay, I think I fixed that.  Thanks!", "There is one more problem with the example jar with the version, the string of the jar name was being set in the JobConf.\n\nClearly that doesn't work if the version is in the jar filename.\n\nHere is a patch to WordCount that finds the jar file that contains a given class.\n\nIf we want to go this way, it should probably be in hadoop.utils somewhere.\n\nThoughts?", "Ah, I missed that.\n\nI like the idea of adding findContainingJar() as a public utility method somewhere.  This could a jobConf method, since it is very relevant to jobs.  Can you imagine other Hadoop uses of this, that would motivate putting it in a class in the util package, or should we just put it on JobConf?\n\nThanks,\n\nDoug", "Until we come up with such a use, I'd go ahead and make it a static method of the JobConf.\n\nI accidently left a println in the findContainingJar method that needs to be removed.", "Okay, I did this.  I also added a JobConf constructor that specifies this."], "derived": {"summary": "The new example is the word count example from Google's paper. I moved the examples into a separate jar file to demonstrate how to run stand-alone application code.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "a new map/reduce example and moving the examples from src/java to src/examples - The new example is the word count example from Google's paper. I moved the examples into a separate jar file to demonstrate how to run stand-alone application code."}, {"q": "What updates or decisions were made in the discussion?", "a": "Okay, I did this.  I also added a JobConf constructor that specifies this."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-26", "title": "DFS node choice doesn't take available space into account effectively", "status": "Closed", "priority": "Major", "reporter": "Mike Cafarella", "assignee": "Mike Cafarella", "labels": [], "created": "2006-02-08T03:05:14.000+0000", "updated": "2009-07-08T16:41:48.000+0000", "description": "\n  We used to have node-allocation be sensitive to available disk space.  It turned out that\nthis was a little accident-prone and hard to debug on then-available machines, so I removed it.\n\n  DFS is mature enough now that this needs to come back and work properly.  A node with less\nthan X bytes should never accept a new block for allocation.\n", "comments": ["\n  OK, the above comment is a little misleading.\n\n  We used to have a difference space-management system in place, which would set\ncertain nodes off-limits if they didn't have enough space.  Now we choose nodes probabilistically,\nwith more weight given to nodes with more space.\n\n  That's probably a little too cute than is necessary.  We should just have hard limits on how\nmuch a given node will take before it complains and never receives further node allocations.", "\n  Patch for this problem.\n", "Tested and fixed, in revision 383247."], "derived": {"summary": "We used to have node-allocation be sensitive to available disk space. It turned out that\nthis was a little accident-prone and hard to debug on then-available machines, so I removed it.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "DFS node choice doesn't take available space into account effectively - We used to have node-allocation be sensitive to available disk space. It turned out that\nthis was a little accident-prone and hard to debug on then-available machines, so I removed it."}, {"q": "What updates or decisions were made in the discussion?", "a": "Tested and fixed, in revision 383247."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-27", "title": "MapRed tries to allocate tasks to nodes that have no available disk space", "status": "Closed", "priority": "Major", "reporter": "Mike Cafarella", "assignee": null, "labels": [], "created": "2006-02-08T03:07:03.000+0000", "updated": "2009-07-08T16:51:40.000+0000", "description": "\n  What it says above.  MapRed TaskTrackers should not offer task service if the local disk\nspace is too constrained.", "comments": ["Just to add specifics: since a task processor might've already completed previous tasks - when it runs out of space, it shouldn't be allocated new jobs, but it should stay available for serving map output to reduce jobs that need the completed output. Likewise, once such tasks have been cleared, and more space is available, the task runner should return to available status automatically.", "I just ran into this problem, but the machine did actually have space available.\nHowever, it was on another disk set in dfs.data.dir. So the first disk is full and the second one\nalmost empty. Would it not make sense to just continue putting the map output to the second disk?", "To begin with, disregard my last comment, I have no idea what I was thinking :)\n\nCreated this patch to stop a job from losing the map tasks already completed when a task tracker runs out of space.\nInstead, the task tracker will stop accepting new tasks if the space runs below a certain threshold.\nHowever, if for some reason space clears up it will start accepting tasks again.\n\nIf for example a reduce operation previously have been assigned to the task tracker there's a chance it will run out of space anyway. So if the tracker goes below a second threshold it will completely stop accepting new tasks until the job is done and also kill the reduce operation running, or if none is found a map task. It will try to take the one with the least progress.\n\nThe solution might not be ideal, but it's at least better then having the job fail all the time because the task trackers drop off one by one.\n\nSuggestions are of course welcome.\nI've tested this on our tiny cluster and it seems to work fine, just saved me a couple of hours of redundant computation on a big job\n\n/Johan", "Does anyone have any comments on the patch?\nI guess this issue is beneath the radar since it doesn't have a \"affects version\" added to it.\nIt is very important to me though, since we run a cluster with limited disk space on a few of the nodes.", "I still think this issue is important. The point of MapReduce is to squeeze as much performance out of what you've got as possible. I regularly have machines with hundreds of gigs of space fail because one of their 4 drives is full. And, yes, the cascading failure problem is the real kicker. It sounds like your patch attacks both of those things - as soon as I have a chance, I'll test it against my current cluster and see how it performs.", "I think this is a good change to make, but the patch still has a few issues.\n\nI don't think it is sufficient to check if any device has enough room, but rather, one should check that all devices have enough room.  Local file names are hashed to determine which device to store a file on.  Unless that algorithm changes (Configuration.getLocalPath()), we should make sure that all local devices have a minimum amount of space.\n\nAlso, the indentation and formatting of the patch is non-standard, using tabs instead of spaces, and one chunk of the patch fails to apply cleanly.\n\nThanks!", "Now checks all the local dirs as requested\nAlso, changed from tab -> space", "I just committed this.  Thanks, Johan.", "\nOne impact of this fix is that an invalid directory name in the temp files list will now prevent a node from running, whereas before that was tolerated.\n\nWe have a couple of nodes in our cluster with 3 drives instead of 4 (they each have a dead drive), and previously we were able to run just fine with one config file on all nodes. \n\n\n\n"], "derived": {"summary": "What it says above. MapRed TaskTrackers should not offer task service if the local disk\nspace is too constrained.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "MapRed tries to allocate tasks to nodes that have no available disk space - What it says above. MapRed TaskTrackers should not offer task service if the local disk\nspace is too constrained."}, {"q": "What updates or decisions were made in the discussion?", "a": "One impact of this fix is that an invalid directory name in the temp files list will now prevent a node from running, whereas before that was tolerated.\n\nWe have a couple of nodes in our cluster with 3 drives instead of 4 (they each have a dead drive), and previously we were able to run just fine with one config file on all nodes."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-28", "title": "webapps broken", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": null, "labels": [], "created": "2006-02-09T02:41:46.000+0000", "updated": "2009-07-08T16:51:40.000+0000", "description": "Changing the classes to private broke the webapps. \n\nThe required public classes are:\norg.apache.hadoop.mapred.JobInProgress\norg.apache.hadoop.mapred.JobProfile\norg.apache.hadoop.mapred.JobStatus\norg.apache.hadoop.mapred.TaskTrackerStatus\n\nTo fix, we need one of:\n  1. The classes need to be made public again\n  2. The functionality needs to be made available through the classes that are public\n  3. The webapps need to move into the mapred package.", "comments": ["Ugh.  I like (3).  We should really try to limit public classes to the user API we intend to support long-term.  The only way I can see to achieve this is to compile the jsp's to servlets offline using Ant's jspc task, where you can specify a package.", "Here's something that will compile the jsp pages as servlets.  More changes are required to configure jetty to use them, but this is a proof of concept.", "The JSP pages are now pre-compiled to servlets that can access package-private classes."], "derived": {"summary": "Changing the classes to private broke the webapps. The required public classes are:\norg.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "webapps broken - Changing the classes to private broke the webapps. The required public classes are:\norg."}, {"q": "What updates or decisions were made in the discussion?", "a": "The JSP pages are now pre-compiled to servlets that can access package-private classes."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-29", "title": "JobConf newInstance() method imposes a default constructor", "status": "Closed", "priority": "Minor", "reporter": "Jerome Charron", "assignee": "Owen O'Malley", "labels": [], "created": "2006-02-09T22:28:40.000+0000", "updated": "2009-07-08T16:51:40.000+0000", "description": "The Nutch parse command fails, because the ParseSegment class has no default constructor.\nHowever, ParseSegment extends Configured, so it can be directly instanciated with a Configuration parameter.\n", "comments": ["The attached patch directly uses the constructor with Configuration parameter if the class to instanciate extends Configurable.\n(Tested with Nutch).", "Wouldn't it be simpler to just add a default constructor to Configured?"], "derived": {"summary": "The Nutch parse command fails, because the ParseSegment class has no default constructor. However, ParseSegment extends Configured, so it can be directly instanciated with a Configuration parameter.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "JobConf newInstance() method imposes a default constructor - The Nutch parse command fails, because the ParseSegment class has no default constructor. However, ParseSegment extends Configured, so it can be directly instanciated with a Configuration parameter."}, {"q": "What updates or decisions were made in the discussion?", "a": "Wouldn't it be simpler to just add a default constructor to Configured?"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-30", "title": "DFS shell: support for ls -r and cat", "status": "Closed", "priority": "Major", "reporter": "Michel Tourn", "assignee": null, "labels": [], "created": "2006-02-10T08:14:53.000+0000", "updated": "2006-08-03T17:46:27.000+0000", "description": "patch attached", "comments": ["Example combined usage:\nbin/nutch ndfs -lsr /user/michel/ | cut -f 1 | xargs -i bin/nutch ndfs -cat {}\n\nlists all files/dirs, writes them all to stdout\n\n", "I just applied this.  Thanks, Michel!"], "derived": {"summary": "patch attached.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "DFS shell: support for ls -r and cat - patch attached."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just applied this.  Thanks, Michel!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-31", "title": "Stipulate main class in a job jar when using 'hadoop jar JARNAME'", "status": "Closed", "priority": "Minor", "reporter": "Michael Stack", "assignee": null, "labels": [], "created": "2006-02-10T09:16:53.000+0000", "updated": "2006-12-15T23:02:08.000+0000", "description": "One use case I forsee is building one jar but using this one jar running multiple jobs: E.g. A single nutch job jar would now be used to do indexing job, later same jar is used to do dedup, etc. Currently, the recently added hadoop 'jar' option just takes the jar name then looks in the jar MANIFEST.MF for the Main-Class, failing if not present. This is grand but for the scenario above, it means I have to create a jar per job I want to run -- each with a different MANIFEST.MF#Main-Class entry. \n\nCan we pass the Main-Class on the hadoop command-line as an (optional) argument to 'hadoop jar JAR_NAME'? (I can make a patch if wanted). ", "comments": ["I would argue that rather than creating more framework, you just use the arguments and dispatch on the first argument. If you need something generic you could do something like I did with the ExampleDriver.", "That will work.  Thanks.\n\n(I'd close this issue but I don't have sufficent permissions).", "Closing at  reporter's request.", "Perhaps ExampleDriver could itself be renamed/moved to more general 'Launcher' that's documented as a framework convenience class for Hadoop users who have a single-Jar, many main()s usage?"], "derived": {"summary": "One use case I forsee is building one jar but using this one jar running multiple jobs: E. g.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Stipulate main class in a job jar when using 'hadoop jar JARNAME' - One use case I forsee is building one jar but using this one jar running multiple jobs: E. g."}, {"q": "What updates or decisions were made in the discussion?", "a": "Perhaps ExampleDriver could itself be renamed/moved to more general 'Launcher' that's documented as a framework convenience class for Hadoop users who have a single-Jar, many main()s usage?"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-32", "title": "Creating job with InputDir set to non-existant directory locks up jobtracker", "status": "Closed", "priority": "Major", "reporter": "Bryan Pendleton", "assignee": "Owen O'Malley", "labels": [], "created": "2006-02-10T16:07:02.000+0000", "updated": "2009-07-08T16:51:39.000+0000", "description": "This, in the very least, affects anything using the default listFiles() from InputFormatBase. If no files are enumerated, an exception is thrown... but the JobTracker keeps attempting to run listFiles() for this job. Trying to stop the job with hadoop job -kill job_name just results in timeouts, and further started jobs also don't progress. This happens every single time with, say, \"wordcount\", and a non-existent input path.", "comments": ["This was fixed by 0.4.0 by the fix for HADOOP-278."], "derived": {"summary": "This, in the very least, affects anything using the default listFiles() from InputFormatBase. If no files are enumerated, an exception is thrown.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Creating job with InputDir set to non-existant directory locks up jobtracker - This, in the very least, affects anything using the default listFiles() from InputFormatBase. If no files are enumerated, an exception is thrown."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was fixed by 0.4.0 by the fix for HADOOP-278."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-33", "title": "DF enhancement: performance and win XP support", "status": "Closed", "priority": "Minor", "reporter": "Konstantin Shvachko", "assignee": "Konstantin Shvachko", "labels": [], "created": "2006-02-11T07:43:36.000+0000", "updated": "2009-07-08T16:41:48.000+0000", "description": "1. DF is called twice for each heartbeat, which happens each 3 seconds.\nThere is a simple fix for that in the attached patch.\n\n2. cygwin is required to run df program in windows environment.\nThere is a class org.apache.commons.io.FileSystemUtils, which can return disk free space\nfor different OSs, but it does not have means to get disk capacity.\nIn general in windows there is no efficient and uniform way to calculate disk capacity\nusing a shell command.\nThe choices are 'chkdsk' and 'defrag -a', but both of them are too slow to be called\nevery 3 seconds.\nWinXP and 2003 server have a new tool called fsutil, which provides all necessary info.\nI implemented a call to fsutil in case df fails, and the OS is right.\nOther win versions should still run cygwin.\nI tested this fetaure for linux, winXP and cygwin.\nSee attached patch.", "comments": ["DF.patch uses relative file names\nPlease disregard the DFpatch.txt attachment", "Fixing things to not call DF twice per heartbeat would be great.  But why do we need the DiskUsage class?  Can't we just keep the DF instance and reuse it?  I don't see the advantage of wrapping the DF inside another class.  It just adds code, and less code is better.  Also, the logic of getRemaining() is duplicated after your patch.\n\nPerhaps what's needed is a private getDF() method in FSDataset, that checks to see if a cached DF instance has been refreshed in the last N milleseconds.  If it has not then it is refreshed.  Then it is returned.  Something like:\n\nprivate synchronized DF getDF() {\n  long now = System.getMillisTime();\n  if ((now - lastDfTime) > DF_INTERVAL) {\n    df = new DF();\n    lastDfTime = now;\n  }\n  return DF();\n}\n\nThen getRemaining() and getCapacity() can be defined in terms of getDF().  Does this make sense?\n\nFinally, Hadoop currently requires Cygwin in a number of places, most notably in the startup scripts.  The current strategy is not to maintain native Windows versions of these, but rather to rely on Cygwin.  This patch increases the code size without removing the dependency on Cygwin.  If you like, we could start another bug to entirely remove the dependency on Cygwin, porting all scripts, DF, etc.  But that is a low-priority item for me, since Cygwin offers a fine solution with no code duplication.\n\nIn summary, I'd love to see a patch that fixes the DF problem with a minimum of code.  Thanks!", "1. Having the lastDfTime, and updating DF every time DF_INTERVAL passes is definitely a good solution.\nI would go even further and place the DF renew/refresh logic directly into the DF class so that functions\ncalling DF get-methods were free to assume the data is up to date.\nI don't know whether we need that, but DF_INTERVAL can be made a configurable parameter.\nThis will bring in more code, but will make the use of the DF class easier in the end.\nDo we want it?\n\n2. My patch does not remove the dependency on Cygwin. What it does is\nit removes dependency on Cygwin in one particular case without compromising performance for the mainstream OS.\nThe whole file system can run (and actually runs) on windows after that without overheads of cygwin.\nAdditional code is justifiable and inevitable in this case until Sun will implement this functionality for us using native libraries :-).\n\n3. What do you mean by minimizing the code?\nIs it \"the minimum of changes to the existing code that solve the problem\", or is it\nthe minimal amount of total code committed to the repository?\nOr is it minimizing the code required in the future to use the feature?\nThis is actually an interesting topic for discussion......\n", "Yes, DF_INTERVAL should be configurable.\n\nCaching inside DF sounds fine.  We'd then want to add a DF field to FSDataset, so that we always reuse the same instance.\n\nBy minimizing the code I primarily mean minimal total code committed to the repository.  Minimizing the size of patches is also good, since it makes it easier to understand.\n\nI do not see how removing the dependency on cygwin in this one case helps the project: it makes it bigger but adds no functionality and removes no dependencies.  Dependencies are also not bad: we don't want to re-invent things.  Cygwin has already solved this problem (and some others) for us permitting us to focus on Hadoop's more critical issues.", "Konstantin,\n\nThank you for submitting that patch, even though it wasn't accepted I just copied your tryOtherOS method into my code to get rid of that annoying Unix dependency.  I was suprised to see a dependency like that on an operating system command.  Frankly, as a developer and a user, a little extra code is a lot less annoying than several megs of Cygwin and a few hours getting it working (especially since it never worked right).", "So, Tim, I take it you're using Hadoop on XP without Cygwin?  What are you using for startup scripts?", "Doug,\n\nI'm getting there, so far the NameNode and the DataNode are working.  I've just got hacky batch files so far, I don't have the nice shell script system you guys developed.  Here's my batch file for the NameNode:\n\njava -Xmx256m -cp c:\\java\\lib\\hadoop-nightly\\conf;\"c:\\program files\\java\\jdk1.5.0_06\\lib\\tools.jar\";c:\\java\\lib\\hadoop-nightly\\hadoop-nightly.jar;c:\\java\\lib\\hadoop-nightly\\hadoop-nightly-examples.jar;c:\\java\\lib\\hadoop-nightly\\hadoop-nightly-examples.jar;c:\\java\\lib\\hadoop-nightly\\hadoop-nightly.jar;c:\\java\\lib\\hadoop-nightly\\lib\\commons-logging-api-1.0.4.jar;c:\\java\\lib\\hadoop-nightly\\lib\\jetty-5.1.4.jar;c:\\java\\lib\\hadoop-nightly\\lib\\junit-3.8.1.jar;c:\\java\\lib\\hadoop-nightly\\lib\\lucene-core-1.9-rc1-dev.jar;c:\\java\\lib\\hadoop-nightly\\lib\\servlet-api.jar;c:\\java\\lib\\hadoop-nightly\\lib\\jetty-ext\\ant.jar;c:\\java\\lib\\hadoop-nightly\\lib\\jetty-ext\\commons-el.jar;c:\\java\\lib\\hadoop-nightly\\lib\\jetty-ext\\jasper-compiler.jar;c:\\java\\lib\\hadoop-nightly\\lib\\jetty-ext\\jasper-runtime.jar;c:\\java\\lib\\hadoop-nightly\\lib\\jetty-ext\\jsp-api.jar org.apache.hadoop.dfs.NameNode\n\nNow I am just tryng to figure out why JobTracker won't work.  I couldn't get JobTracker working in Cygwin either, so I thought I'd edit the code to give me stack traces and see if I could narrow it down, which led me to try to get it working right in CodeGuide without having to keep going back to Cygwin...", "Oops, I didn't realize lines were not auto-wrapping.  Sorry about that.  And I meant to say I knew my batch file doesn't have relative paths, I was just copying and pasting to get it working.", "This patch (DF2.patch) covers DF caching and reuse of the same instance of DF in FSDataset.\nI removed main() from the DF class, and created class TestDF in the test directory.\nAdditionally, for those who want Windows XP/2003 df functionality without cygwin\nI attach DF.java, which covers that and is ready for adding other OSs, if desired.\nJust replace committed DF.java with the one attached.", "Done some code reduction.\nDF3.patch is the latest version now.", "This still needs a few improvements.\n\n- why are all the fields protected?  shouldn't they be private?\n- the test case isn't a junit test.\n- the toString() method now needs to call to doDF().\n- the javadoc now says it uses a windows command, but it doesn't\n\n", "- The fields are protected rather than private in order to keep the class extensible, if anybody\n  would want to support other OSs, like I did in the attached version of DF.java\n- I would remove this test entirely, don't see any reason to test DF outside the file system.\n- Don't think toString() should call doDF(). We probably want toString to reflect the current\n  state of the class, rather the current state of the disk drive.\n- The Javadoc should say exactly the same it was originally saying, my mistake.", "DF4.path\n\n- made fields private\n- made test junit\n- left toString() unchanged\n- removed WinXp comment\n\nIs there anything else holding us?", "Updating the DF patch. \nThe constructor is now calling df to assign actual values to the members.\nmain() is back instead of the testDF.\nDF.java is also updated to reflect these changes.", "The patch looks great.  I just committed it.  Thanks, Konstantin!", "I'm updating the universal version of DF.java to reflect changes introduced by HADOOP-344.\nDF is hadoop's only dependency on cygwin not counting the scripts.\nIn order to run it on windows XP (without cygwin) you need to replace DF.java with the\nfile attached.\n", "The result of 'fsutil' is localised.\n\nOn my french windows XP, the result is:\n\nNombre total d'octets libres             : 23775694848\nNombre total d'octets                    : 143913521152\nNombre total d'octets libres disponibles  : 23775694848\n\nI modified DF.java accordingly for my needs, but if the order of lines is always the same, I guess that parsing the lines sequentially should do the trick."], "derived": {"summary": "1. DF is called twice for each heartbeat, which happens each 3 seconds.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "DF enhancement: performance and win XP support - 1. DF is called twice for each heartbeat, which happens each 3 seconds."}, {"q": "What updates or decisions were made in the discussion?", "a": "The result of 'fsutil' is localised.\n\nOn my french windows XP, the result is:\n\nNombre total d'octets libres             : 23775694848\nNombre total d'octets                    : 143913521152\nNombre total d'octets libres disponibles  : 23775694848\n\nI modified DF.java accordingly for my needs, but if the order of lines is always the same, I guess that parsing the lines sequentially should do the trick."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-34", "title": "Build Paths Relative to PWD in build.xml", "status": "Closed", "priority": "Trivial", "reporter": "Jeremy Bensley", "assignee": null, "labels": [], "created": "2006-02-14T02:00:26.000+0000", "updated": "2006-08-03T17:46:27.000+0000", "description": "In the build.xml file, many paths are defined in terms of the present working directory (PWD) instead of relative to the location of the build.xml. Thus, whenever trying to compile from a directory other than the hadoop root, errors such as this appear:\n\nBUILD FAILED\n/home/jeremy/cvs/hadoop/build.xml:109: org.apache.jasper.JasperException: The -uriroot option must specify a pre-existing directory\n\nI have scripts / vim parameters that connect to other machines for compiling using ssh, and am not necessarily always in the root whenever I compile.\n\nI am attaching a patch which sets all paths relative to ${basedir}, and removes the override of ${basedir} to the PWD. Please let me know if there are reasons why the build environment must be relative to the working directory.", "comments": ["Proposed patch for issue", "I just comitted this.  Thanks!"], "derived": {"summary": "In the build. xml file, many paths are defined in terms of the present working directory (PWD) instead of relative to the location of the build.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Build Paths Relative to PWD in build.xml - In the build. xml file, many paths are defined in terms of the present working directory (PWD) instead of relative to the location of the build."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just comitted this.  Thanks!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-35", "title": "Files missing chunks can cause mapred runs to get stuck", "status": "Closed", "priority": "Major", "reporter": "Bryan Pendleton", "assignee": null, "labels": [], "created": "2006-02-14T03:30:18.000+0000", "updated": "2009-07-08T16:41:46.000+0000", "description": "I've now several times run into a problem where a large run gets stalled as a result of a missing data block. The latest was a stall in the Summer - ie, the data might've all been there, but it was impossible to proceed because the CRC file was missing a block. It would be nice to:\n\n1) Have a \"health check\" running on a map reduce. If any data isn't available, emmit periodic warnings, and maybe have a timeout for if the data never comes back. Such warnings *should* specify which file(s) are affected by the missing blocks.\n2) Have a utility, possible part of the existing dfs utility, which can check for dfs files with unlocatable blocks. Possibly, even show a 'health' of a file - ie, what percentage of its blocks are currently at the desired replication level. Currently, there's no way that I know of to find out if a file in DFS is going to be unreadable.", "comments": ["Alright, well, for diagnostic purposes, I added a \"health\" stat to each file in -ls. The getFileHealth() function I define could probably be stuck into a utility class somewhere... especially if it were to be called periodically during a mapreduce run. The patch also refactors how the \"Configuration\" instance is used in the DFSShell.\n\nAlso, I had to add a try/catch around the call to getHints(), because for several of my existing files I get the following stack track when trying to call getHints():\n\nException in thread \"main\" java.io.IOException: java.lang.NullPointerException\n        at org.apache.hadoop.ipc.Client.call(Client.java:301)\n        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:141)\n        at org.apache.hadoop.dfs.$Proxy0.getHints(Unknown Source)\n        at org.apache.hadoop.dfs.DFSClient.getHints(DFSClient.java:69)\n        at org.apache.hadoop.dfs.DistributedFileSystem.getFileCacheHints(DistributedFileSystem.java:63)\n        at org.apache.hadoop.dfs.DFSShell.getFileHealth(DFSShell.java:43)\n        at org.apache.hadoop.dfs.DFSShell.ls(DFSShell.java:117)\n        at org.apache.hadoop.dfs.DFSShell.main(DFSShell.java:283)", "Duplicate of HADOOP-83"], "derived": {"summary": "I've now several times run into a problem where a large run gets stalled as a result of a missing data block. The latest was a stall in the Summer - ie, the data might've all been there, but it was impossible to proceed because the CRC file was missing a block.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Files missing chunks can cause mapred runs to get stuck - I've now several times run into a problem where a large run gets stalled as a result of a missing data block. The latest was a stall in the Summer - ie, the data might've all been there, but it was impossible to proceed because the CRC file was missing a block."}, {"q": "What updates or decisions were made in the discussion?", "a": "Duplicate of HADOOP-83"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-36", "title": "Adding some uniformity/convenience to environment management", "status": "Closed", "priority": "Major", "reporter": "Bryan Pendleton", "assignee": null, "labels": [], "created": "2006-02-14T05:03:38.000+0000", "updated": "2006-08-03T17:46:27.000+0000", "description": "Currently, \"slaves\" are loaded from ~/.slaves. What would be better would be to default from something like conf/hadoop-slaves\n\nPerhaps split slaves, having a different set for \"datanodes\" vs. \"tasktracker\" nodes. ie, conf/hadoop-slaves-tasktracker, conf/hadoop-slaves-datanodes, or some similar split. There's the possibility it's worth building in the assumption that tasktracker is a superset, and thus implicitly includes datanodes, but this might be a bad assumption.\n\nAlso, make sure all scripts source something like conf/hadoop-env.sh. Thus, the user can edit hadoop-env.sh to specify JAVA_HOME, or an alternate HADOOP_SLAVES location. It would also be desirable to have a seed CLASSPATH here. Possibly name it HADOOP_CLASSPATH, to make it explicit and not make hadoop scripts possibly interact with an otherwise-set system CLASSPATH variable.\n\nThese changes would probably be useful to the nutch project, too.", "comments": ["I just committed fixes for most of this.  The slaves and environment variables are now read from the conf directory.  If there are further elements you'd like to see, please submit them as separate bugs."], "derived": {"summary": "Currently, \"slaves\" are loaded from ~/. slaves.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Adding some uniformity/convenience to environment management - Currently, \"slaves\" are loaded from ~/. slaves."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed fixes for most of this.  The slaves and environment variables are now read from the conf directory.  If there are further elements you'd like to see, please submit them as separate bugs."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-37", "title": "A way to determine the size and overall activity of the cluster", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": null, "labels": [], "created": "2006-02-14T17:11:58.000+0000", "updated": "2009-07-08T16:51:38.000+0000", "description": "There is currently no way for an application to determine the size or activity of the cluster.\n\n", "comments": ["This patch provides a new method in the JobClient that provides a summary of the size and activity in the cluster.", "I have committed this, but with the following modifications to ClusterStatus.java:\n\nFields are moved to the top of the class, as per:\n\nhttp://java.sun.com/docs/codeconv/html/CodeConventions.doc2.html#1852\n\nConstructors are no longer public: the public constructors had no javadoc.  But there's not much point in adding any, since user code should never need to call these.  The javadoc should only contain things that folks need to call.  I added a WritableFactory so that the io code can construct these, and made the no-arg constructor private, since it should only be called by the factory.  The other constructor I made package-private.\n\nFinally, I fixed a typo in the javadoc.\n"], "derived": {"summary": "There is currently no way for an application to determine the size or activity of the cluster.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "A way to determine the size and overall activity of the cluster - There is currently no way for an application to determine the size or activity of the cluster."}, {"q": "What updates or decisions were made in the discussion?", "a": "I have committed this, but with the following modifications to ClusterStatus.java:\n\nFields are moved to the top of the class, as per:\n\nhttp://java.sun.com/docs/codeconv/html/CodeConventions.doc2.html#1852\n\nConstructors are no longer public: the public constructors had no javadoc.  But there's not much point in adding any, since user code should never need to call these.  The javadoc should only contain things that folks need to call.  I added a WritableFactory so that the io code can construct these, and made the no-arg constructor private, since it should only be called by the factory.  The other constructor I made package-private.\n\nFinally, I fixed a typo in the javadoc."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-38", "title": "default splitter should incorporate fs block size", "status": "Closed", "priority": "Major", "reporter": "Doug Cutting", "assignee": null, "labels": [], "created": "2006-02-15T04:25:04.000+0000", "updated": "2009-07-08T16:51:38.000+0000", "description": "By default, the file splitting code should operate as follows.\n\n  inputs are <file>*, numMapTasks, minSplitSize, fsBlockSize\n  output is <file,start,length>*\n\n  totalSize = sum of all file sizes;\n\n  desiredSplitSize = totalSize / numMapTasks;\n  if (desiredSplitSize > fsBlockSize)             /* new */\n    desiredSplitSize = fsBlockSize;\n  if (desiredSplitSize < minSplitSize)\n    desiredSplitSize = minSplitSize;\n\n  chop input files into desiredSplitSize chunks & return them\n\nIn other words, the numMapTasks is a desired minimum.  We'll try to chop input into at least numMapTasks chunks, each ideally a single fs block.\n\nIf there's not enough input data to create numMapTasks tasks, each with an entire block, then we'll permit tasks whose input is smaller than a filesystem block, down to a minimum split size.\n\nThis handles cases where:\n  - each input record takes a lot of time to process.  In this case we want to make sure we use all of the cluster.  Thus it is important to permit splits smaller than the fs block size.\n  - input i/o dominates.  In this case we want to permit the placement of tasks on hosts where their data is local.  This is only possible if splits are fs block size or smaller.\n\nAre there other common cases that this algorithm does not handle well?\n\nThe part marked 'new' above is not currently implemented, but I'd like to add it.\n\nDoes this sound reasonble?\n", "comments": ["The idea sounds sound, but is blocksize the best unit? There's a certain overhead for each additional task added to a job - for jobs with really large input, this could cause really large task lists. Is there going to be any code for pre-replicating blocks? Maybe sequences, so there'd be a natural \"first choice\" node for many chunkings of larger than one block? Obviously, as datanodes come and go this might not always work ideally, but it could help in the 80% case.", "The surest way to get larger chunks is to increase the block size.\n\nThe default DFS blocksize is currently 32MB, which gives 31k tasks for terabyte inputs, which is reasonable.  I think we should design things to be able to handle perhaps a million tasks, which, with the current block size, would get us to 32 terabyte inputs.\n\nPerhaps the default should be 1GB/block.  With a million tasks, would get us to maximum of a petabyte per job.  On a 10k node cluster, a petabyte takes hours to read (100GB/node @ 10MB/second = 10k seconds).\n\nWe'll also need to revise the web UI to better handle a million tasks...\n", "1GB blocks have a lot of issues.  Until your networks get faster and RAMs get bigger, this is probably too big.  For many of our current tasks 1GB is too much input for reasonable restartabilty too.  I think 32M to 128M are a lot closer to the current sweet spot.\n", "I'm not sure what RAM size or network speeds have to do with it: we stream blocks into a task, we don't read them all at once.\n\nRestartability could be an issue.  If you have a petabyte of input, and you want restartability at 100M chunks, then that means you need to be able to support up to 10M tasks per job.  This is possible, but means the job tracker has to be even more careful not to store too much in RAM per task, nor iterate over all tasks, etc.\n\nBut I'm not convinced that 1GB would cause problems for restartability.  A petabyte input on 10k nodes (my current horizon), 1GB blocks gives 1M tasks, or 100 per node.  So each task will average around 1% of the execution time, so even those that are restarted near the end of job completion won't add much to the overall  time.", "some thoughts:\n\n1) Eventually you are going to want raid / erasure coding style things.  The simplest way to do this without breaking reads is to batch several blocks, keeping them linear and then generate parity once all blocks are full.  This gets more expensive as block size increases.  At current sizes, this can all be buffered in RAM in some cases.  1GB blocks rule that out.\n\n2) Currently you can trivially keep a block in RAM for a MAP task.  Depending on scaling factor, you can probably keep the output in ram for sorting, reduction, etc.  too.  This is nice.  As block size increases you loose this property.\n\n3) When you loose a node, the finer grained the lost data, the fewer hotspots you have in the system.  Today in a large cluster you can easily have choke points with ~33mbit aggregate all to all.  We've seen problems with larger data sizes slowing recovery times to a real problem.  1GB blocks take 10x as long to transmit, and this turns into minutes, which will require more sophisticated management.\n\n---\n\nNone of these are show stoppers, but one of the main reasons we are interested in hadoop is in getting off of our current very large storage chunk system, so I'd hate to see the default move quickly to something as large as 1GB.\n\nI can see the advantages of pushing the block size up to manage task tracker RAM size, but I doubt that alone will prove a compelling reason for us to change our default block size.  On the other hand, I also don't think we'll be pumping 1 peta byte through a single m/r in the near term, so we can assume the zero code solution, change block size, until we have more data to support some other approach.\n\nOf course at 1M tasks, you will want to be careful about linear scans anyway...\n\nI've no concern with the proposal in this bug.  Probably can take this discussion elsewhere", "I just committed this."], "derived": {"summary": "By default, the file splitting code should operate as follows. inputs are <file>*, numMapTasks, minSplitSize, fsBlockSize\n  output is <file,start,length>*\n\n  totalSize = sum of all file sizes;\n\n  desiredSplitSize = totalSize / numMapTasks;\n  if (desiredSplitSize > fsBlockSize)             /* new */\n    desiredSplitSize = fsBlockSize;\n  if (desiredSplitSize < minSplitSize)\n    desiredSplitSize = minSplitSize;\n\n  chop input files into desiredSplitSize chunks & return them\n\nIn other words, the numMapTasks is a desired minimum.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "default splitter should incorporate fs block size - By default, the file splitting code should operate as follows. inputs are <file>*, numMapTasks, minSplitSize, fsBlockSize\n  output is <file,start,length>*\n\n  totalSize = sum of all file sizes;\n\n  desiredSplitSize = totalSize / numMapTasks;\n  if (desiredSplitSize > fsBlockSize)             /* new */\n    desiredSplitSize = fsBlockSize;\n  if (desiredSplitSize < minSplitSize)\n    desiredSplitSize = minSplitSize;\n\n  chop input files into desiredSplitSize chunks & return them\n\nIn other words, the numMapTasks is a desired minimum."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-39", "title": "Create a job-configurable best effort for job execution", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Arun Murthy", "labels": [], "created": "2006-02-16T10:01:08.000+0000", "updated": "2009-07-08T16:51:40.000+0000", "description": "I propose having a job option that when a tip fails 4 times, stops trying to run that tip, but does not kill the job.", "comments": ["The point is to try to get map tasks with side effects to sometimes succeed, even with speculative execution?  That sounds like it could be a bad idea.  Wouldn't it be better to have map tasks with side effects fail more frequently with speculative execution, so that you find such problems sooner, with smaller datasets on a smaller cluster, before you try a big run?  Or am I misunderstanding you?", "My goal with this would be to do the equivalent of \"make -k\" or a \"best effort\" job. It the option was set, the job would continue after a given TIP had failed 4 times, but that TIP would be abandoned.", "Fixed as a part of HADOOP-1144"], "derived": {"summary": "I propose having a job option that when a tip fails 4 times, stops trying to run that tip, but does not kill the job.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Create a job-configurable best effort for job execution - I propose having a job option that when a tip fails 4 times, stops trying to run that tip, but does not kill the job."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed as a part of HADOOP-1144"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-40", "title": "bufferSize argument is ignored in FileSystem.create(File, boolean, int)", "status": "Closed", "priority": "Minor", "reporter": "Konstantin Shvachko", "assignee": null, "labels": [], "created": "2006-02-18T07:43:11.000+0000", "updated": "2006-08-03T17:46:28.000+0000", "description": "org.apache.hadoop.fs.FileSystem.create(File f, boolean overwrite, int bufferSize)\n\nignores the input parameter bufferSize.\nIt passes further down the internal configuration, which includes the buffer size, but not the parameter value.\nThis works fine within the file system, since everything that calls create extracts buffer size from the same config. \nMapReduce although is probably affected by that, see \n\norg.apache.hadoop.io.SequenceFile.Sorter.MergeQueue.MergeQueue(int size, String outName, boolean done)\n\nThe attached patch would fix it.", "comments": ["I don't think we should modify the configuration in this case, since that will affect code which uses this configuration that runs later.  SequenceFile uses very large buffers when sorting and merging, in order to minimize disk seeks, and we don't want everything to start using such large buffers.\n\nSo why not just pass the missing parameter down?  I've attached a patch that does this.  Does this look good to you?", "I just committed my patch for this.", "Yes. This is the right way of doing it.\nIt particularly makes sense, since the main file and the checksum file\ndo not necessarily need to share the same buffer size.\nWith the file buffer large the checksum buffer doesn't need to be large at all."], "derived": {"summary": "org. apache.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "bufferSize argument is ignored in FileSystem.create(File, boolean, int) - org. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "Yes. This is the right way of doing it.\nIt particularly makes sense, since the main file and the checksum file\ndo not necessarily need to share the same buffer size.\nWith the file buffer large the checksum buffer doesn't need to be large at all."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-41", "title": "JAVA_OPTS for the TaskRunner Child", "status": "Closed", "priority": "Minor", "reporter": "Michael Stack", "assignee": null, "labels": [], "created": "2006-02-18T09:47:33.000+0000", "updated": "2006-08-03T17:46:28.000+0000", "description": "Currently, its possible to set the java heap size the TaskRunner child runs in, but thats all thats configurable about the child process.  Hereabouts, we've found it useful being able to specify other options for the child JVM, especially when debugging and monitoring long-lived processes.  \n\nExamples of why its useful being able to set options are the child include:\n\n+ Being able to set '-server' option or '-c64'.\n+ Passing logging.properties to configure child logging.\n+ Enable and capture to file verbose GC logging or start the SUN JVM JMX agent for the child process.  Allows connecting with jconsole to watch long-lived children, their heap and thread usage, and when seemingly hung, take thread dumps.", "comments": ["Attached is a suggested patch for TaskRunner that reads a new configuration option, mapred.child.java.opts.  This new configuration subsumes mapred.child.heap.size. Paul Baclace coauthored this patch.", "I like this, except that it is not back-compatible.  We should continue to support mapred.child.heap.size, so that folks who upgrade are not broken.", "Second attempt.  Addresses Doug comment.  In this patch, if mapred.child.heap.size is present in configuration, rather than just ignore it, its heap size setting wins out over any setting present in mapred.child.java.opts and a warning is emitted that this setting is deprecated.", "I just committed this.  Thanks, Michael!"], "derived": {"summary": "Currently, its possible to set the java heap size the TaskRunner child runs in, but thats all thats configurable about the child process. Hereabouts, we've found it useful being able to specify other options for the child JVM, especially when debugging and monitoring long-lived processes.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "JAVA_OPTS for the TaskRunner Child - Currently, its possible to set the java heap size the TaskRunner child runs in, but thats all thats configurable about the child process. Hereabouts, we've found it useful being able to specify other options for the child JVM, especially when debugging and monitoring long-lived processes."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Michael!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-42", "title": "PositionCache decrements its position for reads at the end of file", "status": "Closed", "priority": "Major", "reporter": "Konstantin Shvachko", "assignee": null, "labels": [], "created": "2006-02-18T09:58:44.000+0000", "updated": "2006-08-03T17:46:28.000+0000", "description": "See\n\nint org.apache.hadoop.fs.FSDataInputStream.PositionCache.read(byte[] b, int off, int len) \n\nif in.read() returns -1 (e.g. at the end of file) the position in the cache will be decremented, while it should be retained.\n\nThe attached patch would fix it.", "comments": ["I just committed this.  Thanks, Konstantin!"], "derived": {"summary": "See\n\nint org. apache.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "PositionCache decrements its position for reads at the end of file - See\n\nint org. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Konstantin!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-43", "title": "JobTracker dumps TaskTrackers if it takes too long to service an RPC call", "status": "Closed", "priority": "Major", "reporter": "Bryan Pendleton", "assignee": "Owen O'Malley", "labels": [], "created": "2006-02-21T14:51:49.000+0000", "updated": "2009-07-08T16:51:41.000+0000", "description": "If the JobTracker takes too long to service any RPC request, it is unable to receive emitHeartBeat calls from TaskTrackers. The monitoring thread thus dumps the TaskTracker, losing any incomplete work and forcing a slow reconnect process to begin before it starts assigning work again.", "comments": ["Duplicate of HADOOP-181."], "derived": {"summary": "If the JobTracker takes too long to service any RPC request, it is unable to receive emitHeartBeat calls from TaskTrackers. The monitoring thread thus dumps the TaskTracker, losing any incomplete work and forcing a slow reconnect process to begin before it starts assigning work again.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "JobTracker dumps TaskTrackers if it takes too long to service an RPC call - If the JobTracker takes too long to service any RPC request, it is unable to receive emitHeartBeat calls from TaskTrackers. The monitoring thread thus dumps the TaskTracker, losing any incomplete work and forcing a slow reconnect process to begin before it starts assigning work again."}, {"q": "What updates or decisions were made in the discussion?", "a": "Duplicate of HADOOP-181."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-44", "title": "RPC exceptions should include remote stack trace", "status": "Closed", "priority": "Major", "reporter": "Doug Cutting", "assignee": "Doug Cutting", "labels": [], "created": "2006-02-22T07:06:46.000+0000", "updated": "2006-08-03T17:46:28.000+0000", "description": "Remote exceptions currently only report the exception string.  Instead they should report the entire remote stack trace, as a string, to facilitate debugging.", "comments": ["I've attached one possible fix.", "I just committed a fix for this.  The remote stack trace is now used as the exception message string in the re-thrown local exception.  The class of the local exception is now also RemoteException, so that it can be distinguished from other IOExceptions."], "derived": {"summary": "Remote exceptions currently only report the exception string. Instead they should report the entire remote stack trace, as a string, to facilitate debugging.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "RPC exceptions should include remote stack trace - Remote exceptions currently only report the exception string. Instead they should report the entire remote stack trace, as a string, to facilitate debugging."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed a fix for this.  The remote stack trace is now used as the exception message string in the re-thrown local exception.  The class of the local exception is now also RemoteException, so that it can be distinguished from other IOExceptions."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-45", "title": "JobTracker should log task errors", "status": "Closed", "priority": "Major", "reporter": "Doug Cutting", "assignee": "Doug Cutting", "labels": [], "created": "2006-02-22T07:08:46.000+0000", "updated": "2009-07-08T16:51:39.000+0000", "description": "Task errors are currently propagated to the JobTracker and viewable in the web interface but they are not logged.  These should be logged as well.", "comments": ["And this log should be in the DFS output directory, so there is a record.", "On Feb 22, 2006, at 10:51 AM, Michel Tourn wrote:\n...\nSounds like a good default.\nIf at all possible: please make this kind of side-effects configurable:\n  JobConf.setLogPath(\"/special/logs/\");\n...\n\n----\n\nGood point, I'd suggest a default of \"<outputDir>/_logs/\".  Make sense?\n\ne14", "That's a separate feature that we'll add later, described in NUTCH-53.", "I meant HADOOP-53.", "I just fixed this.  When a tasktracker reports a task error, the jobtracker now logs it.  These messages were previously available in the web ui, but were not logged."], "derived": {"summary": "Task errors are currently propagated to the JobTracker and viewable in the web interface but they are not logged. These should be logged as well.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "JobTracker should log task errors - Task errors are currently propagated to the JobTracker and viewable in the web interface but they are not logged. These should be logged as well."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just fixed this.  When a tasktracker reports a task error, the jobtracker now logs it.  These messages were previously available in the web ui, but were not logged."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-46", "title": "user-specified job names", "status": "Closed", "priority": "Major", "reporter": "Doug Cutting", "assignee": "Owen O'Malley", "labels": [], "created": "2006-02-22T07:10:56.000+0000", "updated": "2009-07-08T16:51:39.000+0000", "description": "It should be possible to supply a name when a job is submitted.  This can then be used in the web ui to describe the job.  Perhaps this should just be a job property (mapred.job.name).", "comments": ["Here is a patch that puts a job name in the JobConf, JobProfile, and webapp.\nThe examples use the new method to set job names for themselves.", "This looks great.  I just committed it.  Thanks, again, Owen!"], "derived": {"summary": "It should be possible to supply a name when a job is submitted. This can then be used in the web ui to describe the job.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "user-specified job names - It should be possible to supply a name when a job is submitted. This can then be used in the web ui to describe the job."}, {"q": "What updates or decisions were made in the discussion?", "a": "This looks great.  I just committed it.  Thanks, again, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-47", "title": "include records/second and bytes/second in  task reports", "status": "Closed", "priority": "Major", "reporter": "Doug Cutting", "assignee": null, "labels": [], "created": "2006-02-22T07:17:42.000+0000", "updated": "2009-07-08T16:51:39.000+0000", "description": "TaskReport should include fields for total records processed, total bytes processed and total seconds of task execution.  These should also be reported in the web ui, as bytes/second and records/second.  The job display could sum these to report total bytes/second and records/second for map and reduce.  Better yet would be a graph displaying the total rates over the course of the job...", "comments": ["+1 that\n\nyou might consider implementing this as a generic counter-object class that can be used by any job to create custom counters like dingbats per second or peak fidderwinks.", "Is this bug still relevant now that we have a fullfledged metrics API, and abovementioned metrics will be output to the configurable metrics collector (after applying the patch for hadoop-237) ?\n", "Yes, Milind, I agree this is effectively duplcated by HADOOP-237."], "derived": {"summary": "TaskReport should include fields for total records processed, total bytes processed and total seconds of task execution. These should also be reported in the web ui, as bytes/second and records/second.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "include records/second and bytes/second in  task reports - TaskReport should include fields for total records processed, total bytes processed and total seconds of task execution. These should also be reported in the web ui, as bytes/second and records/second."}, {"q": "What updates or decisions were made in the discussion?", "a": "Yes, Milind, I agree this is effectively duplcated by HADOOP-237."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-48", "title": "add user data to task reports", "status": "Closed", "priority": "Major", "reporter": "Doug Cutting", "assignee": null, "labels": [], "created": "2006-02-22T07:22:23.000+0000", "updated": "2009-07-08T16:51:39.000+0000", "description": "The Reporter interface should permit tasks to set user-data for a task, either as a String or, better-yet, as a Writable.  This should be returned with each TaskReport.  This would facilitate programmatic instrumentation of tasks.", "comments": ["+1 that", "I think that HADOOP-492 largely satisfied this need."], "derived": {"summary": "The Reporter interface should permit tasks to set user-data for a task, either as a String or, better-yet, as a Writable. This should be returned with each TaskReport.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "add user data to task reports - The Reporter interface should permit tasks to set user-data for a task, either as a String or, better-yet, as a Writable. This should be returned with each TaskReport."}, {"q": "What updates or decisions were made in the discussion?", "a": "I think that HADOOP-492 largely satisfied this need."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-49", "title": "JobClient cannot use a non-default server (unlike DFSShell)", "status": "Closed", "priority": "Major", "reporter": "Michel Tourn", "assignee": "Michel Tourn", "labels": [], "created": "2006-02-22T07:25:42.000+0000", "updated": "2009-07-08T16:51:39.000+0000", "description": "JobClient cannot use a non-default Job Tracker server:\nIt will use the Job Tracker specified in conf/hadoop-default.xml or conf/hadoop-site.xml\n\nFor users with multiple Hadoop systems, it is useful to be able to specify the Job Tracker.\n\nOther hadoop command-line tools like DFSShell already have:\n>bin/hadoop dfs\nUsage: java DFSShell [-local | -dfs <namenode:port>]  ...\n\nSimilarly I propose to add a -jt parameter:\n>bin/hadoop job\nJobClient -submit <job> | -status <id> | -kill <id> [-jt <jobtracker:port>|<config>]\n\nWhere: -jt <jobtracker:port> is similar to -dfs <namenode:port>\nAnd:  -jt <config> will load as a final resource: hadoop-<config>.xml\n\nThe latter syntax is discoverable by users because on failure the tool will say:\n\n>bin/hadoop job -kill m7n6pi -jt unknown\nException in thread \"main\" java.lang.RuntimeException: hadoop-unknown.xml not found on CLASSPATH\n\nOr in case of success:\n\n>bin/hadoop job -kill job_m7n6pi -jt myconfig\n060221 221911 parsing file:/trunk/conf/hadoop-default.xml\n060221 221911 parsing file:/trunk/conf/hadoop-myconfig.xml\n060221 221911 parsing file:/trunk/conf/hadoop-site.xml\n060221 221911 Client connection to 66.196.91.10:7020: starting\n\nAnd with a machine:port spec:\n>bin/hadoop job -kill job_m7n6pi -jt machine:8020\n060221 222109 parsing file:/trunk/conf/hadoop-default.xml\n060221 222109 parsing file:/trunk/conf/hadoop-site.xml\n060221 222109 Client connection to 66.196.91.10:8020: starting\n\n\nPatch attached.\n\n\n\n", "comments": ["I just committed this.\n\nLonger-term, perhaps we should have some standard command line options that all tools accept.  This could be implemented by having bin/hadoop always invoke a single class, e.g., org.apache.hadoop.ToolRunner.  This might looks something like\n\npublic class ToolRunner {\n  public static void main(String[] args) throws Exception {\n    Configuration conf = new Configuration();\n    ... initialize configuration from command line options..\n    ... get tool class from command line options ...\n    Tool tool = toolClass.newInstance();\n    tool.configure(conf);\n    tool.run(args);\n  }\n}\n\npublic interface Tool extends Configurable {\n  run(String[] args);\n}\n\nIf folks like this, we file this it as a separate issue.."], "derived": {"summary": "JobClient cannot use a non-default Job Tracker server:\nIt will use the Job Tracker specified in conf/hadoop-default. xml or conf/hadoop-site.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "JobClient cannot use a non-default server (unlike DFSShell) - JobClient cannot use a non-default Job Tracker server:\nIt will use the Job Tracker specified in conf/hadoop-default. xml or conf/hadoop-site."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.\n\nLonger-term, perhaps we should have some standard command line options that all tools accept.  This could be implemented by having bin/hadoop always invoke a single class, e.g., org.apache.hadoop.ToolRunner.  This might looks something like\n\npublic class ToolRunner {\n  public static void main(String[] args) throws Exception {\n    Configuration conf = new Configuration();\n    ... initialize configuration from command line options..\n    ... get tool class from command line options ...\n    Tool tool = toolClass.newInstance();\n    tool.configure(conf);\n    tool.run(args);\n  }\n}\n\npublic interface Tool extends Configurable {\n  run(String[] args);\n}\n\nIf folks like this, we file this it as a separate issue.."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-50", "title": "dfs datanode should store blocks in multiple directories", "status": "Closed", "priority": "Major", "reporter": "Doug Cutting", "assignee": "Milind Barve", "labels": [], "created": "2006-02-22T07:25:57.000+0000", "updated": "2009-07-08T16:41:47.000+0000", "description": "The datanode currently stores all file blocks in a single directory.  With 32MB blocks and terabyte filesystems, this will create too many files in a single directory for many filesystems.  Thus blocks should be stored in multiple directories, perhaps even a shallow hierarchy.", "comments": ["I think this is a valid concern. Most filesystems work poorly with thousands of files in a single directory. My recent tests on ext3 show that listing the data directory with 50,000 blocks takes several seconds.\n\nFSDataset:80 contains a commented out section, which seems to address this issue. Anyone knows why it's not used?", "Hi Andrzej,\n\nI wrote this code and got it 90% working some time ago, but then had to abandon\nit for a more important bug.  It is not ready to go in its current state, but shouldn't\nbe too hard.  I can bring this code back to life..\n\n--Mike", "That would be very useful. I've seen similar solutions in many places (e.g. squid, or Mozilla cache dir).\n\nCurrently, each time a block report is sent we need to list this huge dir. That's still ok, it's infrequent enough. However, each time we need to access a block, a correct file needs to be open. Inside the native code JVM uses an open(2) call, which causes the OS to perform a name-to-inode lookup. Even though OS is caching partial results of this lookup (in Linux this is known as dcache/dentries), still depending on the size of this LRU cache and the FS implementation details, doing real lookups for e.g. new blocks or newly requested blocks may take a long time.\n\nHaving said that, I'm not sure what would be the real performance benefit of this change, perhaps you could come up with a simpler test first...?", "\nThis fixes the multiple-directory storage problem.  It\nlazily creates a single level of 512 subdirectories, into which\nthe blocks are allocated according to the lower 9 bits of the\nblock id.  If mankind ever needs more blocks than this, it is easy\nto add an additional subdir layer and select on the lowest-but-9\nbits of the blockid.\n\nThis change is backwards-compatible with the previous block\nlayout.  Old blocks in the single-layer dir will always be kept in\nthat format; we don't migrate them.  New blocks will always be added \nto the new hierarchy.\n\nIf both versions of the storage system are present, we always test\nthe new one first.  If that fails, we test the old one.  (The new test\nshould be faster, so we do it first.)\n\nPlease let me know if this patch works for you.", "+1\nI didn't look at the patch, but I carefully read the commented out code. I vote yes. (is it already in? - close this issue).", "This was done as part of HADOOP-64"], "derived": {"summary": "The datanode currently stores all file blocks in a single directory. With 32MB blocks and terabyte filesystems, this will create too many files in a single directory for many filesystems.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "dfs datanode should store blocks in multiple directories - The datanode currently stores all file blocks in a single directory. With 32MB blocks and terabyte filesystems, this will create too many files in a single directory for many filesystems."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was done as part of HADOOP-64"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-51", "title": "per-file replication counts", "status": "Closed", "priority": "Major", "reporter": "Doug Cutting", "assignee": "Konstantin Shvachko", "labels": [], "created": "2006-02-22T07:29:49.000+0000", "updated": "2009-07-08T16:41:47.000+0000", "description": "It should be possible to specify different replication counts for different files.  Perhaps an option when creating a new file should be the desired replication count.  MapReduce should take advantage of this feature so that job.xml and job.jar files, which are frequently accessed by lots of machines, are more highly replicated than large data files.", "comments": ["I propose to add 2 functions.\n1) An additional create() method that would take an extra parameter for that file replication.\nThis should be a new method in order to retain existing programs in working condition.\n2) A new setReplication() method to be able to change replication for existing files.\nIt will also require an additional field in the INode class to store replication for each file\non the namenode.\n\n\n", "This sounds like a good plan to me.  +1", "+1 that\n\nit might be easier to use on a per-directory basis, examples:\n\n- /tmp directory, replication count 2 (or 1!), a good place for the output of intermediate reduce steps\n- /cached directory, infinite replicaton count, a good place for lookup files used in mappers or reducers\n", "Here is rather big patch. The changes are.\n\n- Create methods include new parameter \"short replication\"\n- If replication is not specified the default replication is used.\n- The namenode stores and maintains replication for each file separately.\n- File replication can be obtained from the namenode as a part of DFSFileInfo.\n- 2 new namenode config parameters\n   dfs.replication.max\n   dfs.replication.min\nwhich are checked when a new file is created.\n- Namenode image and edit log file format are modified. Both contain version\nnumber at the beginning now. The versions are negative. I started from version -1.\nWhen the namenode starts you should expect that your current dfs image will be\nloaded and converted into new format. All old files will have the same default\nreplication equal to the value of dfs.replication of your config.\n\n", "Great!\n\nA few comments from reading the patch (haven't test with it yet):\n1) The <description> for dfs.replication.min is wrong\n2) This is a wider concern, but on coding style - the idiom of conf.getType(\"config.value\",defaultValue) is good for user-defined values, but shouldn't the default be skipped for things that are defined in hadoop-default.xml, in general? It takes away the value of hadoop-default, and it also means changing that value might or might not always have the desired system-wide results.\n3) Wouldn't it be better to log at a severe level replications that are set below minReplication, or greater than maxReplication, and just set the replication to the nearest bound? Since replication is set per-file by the application, but min and max are probably set by the administrator of the hadoop cluster. Throwing an IOException causes failure where degraded performance would be preferable.\n4) I may be dense, but I didn't see any way to specify that replication be \"full\", ie, a copy per datanode. I got the feeling this was something that was desired of this functionality (ie, for job.jar files, job configs, and lookup data used widely in a job) Using a short means, if we ever scale to > 32k nodes, there'd be no way to manually specify this. Just using Short.MAX_VALUE means getting a lot of errors about not being able to replicate as fully as desired.\n\nOtherwise, this looks like a wonderful patch!", "> the idiom of conf.getType(\"config.value\",defaultValue) is good for user-defined values, but shouldn't the default be skipped for things that are defined in hadoop-default.xml, in general?\n\nThe value from hadoop-default.xml is used in preference to the defaultValue paramter.  The paramter is only used as a last resort when no value is found in hadoop-default.xml or any other config file.", "I just committed this.  I fixed the comment on dfs.replication.min.  I also added a message to CHANGES.txt.\n\nThanks, Konstantin!\n\nBryan: I think the issues you raise bear further discussion that I do not wish to stifle.  For example, we may someday want  to be able to specify more than 2^16 replications, and we may wish to handle replication requests outside of the configured limits differently.  But, for now, I think the patch fixes this bug and that those issues can be addressed though subsequent bugs as we gain experience.  So please file new bugs for any related issues that are important to you."], "derived": {"summary": "It should be possible to specify different replication counts for different files. Perhaps an option when creating a new file should be the desired replication count.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "per-file replication counts - It should be possible to specify different replication counts for different files. Perhaps an option when creating a new file should be the desired replication count."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  I fixed the comment on dfs.replication.min.  I also added a message to CHANGES.txt.\n\nThanks, Konstantin!\n\nBryan: I think the issues you raise bear further discussion that I do not wish to stifle.  For example, we may someday want  to be able to specify more than 2^16 replications, and we may wish to handle replication requests outside of the configured limits differently.  But, for now, I think the patch fixes this bug and that those issues can be addressed though subsequent bugs as we gain experience.  So please file new bugs for any related issues that are important to you."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-52", "title": "mapred input and output dirs must be absolute", "status": "Closed", "priority": "Major", "reporter": "Doug Cutting", "assignee": "Owen O'Malley", "labels": [], "created": "2006-02-22T07:38:02.000+0000", "updated": "2009-07-08T16:51:41.000+0000", "description": "DFS converts relative pathnames to be under /user/$USER.  But MapReduce jobs may be submitted by a different user than is running the jobtracker and tasktracker.  Thus relative paths must be resolved before a job is submitted, so that only absolute paths are seen on the job tracker and tasktracker.  I think the simplest way to fix this is to make JobConf.setInputDir(), setOutputDir(), etc. resolve relative pathnames. ", "comments": ["Here is a patch that fixes the problem. \n\nIt does:\n   1. It adds {set,get}WorkingDirectory to FileSystem.\n   2. It implements them in both LocalFileSystem and DFS.\n   3. The LocalFileSystem implementation both sets the System property user.dir and does an explicit\n        conversion to absolute filenames at the API.\n   4. Added new junit test cases to test the WorkingDirectory functionality.\n   5. Added a utility class in the test directory to create a single-process DFS cluster for junit tests.\n   6. Added the user name into the JobConf.\n   7. Added the user name into the JobProfile.\n   8. Added the user name into the webapp, so you can see who ran the job.\n   9. Added the working directory in the default file system to the JobConf.\n   10. Set the job's working directory before starting the user's Map or Reduce code. (The input splitter is given an absolute pathname\n          for the input directory, but the working directory is not set, since it is done in the context of the JobTracker.)\n   11. Changed the format of the percentage complete in the webapp to be ##0.00 so that you don't get 16 digits of meaningless precision\n          about your job status.", "Overall this looks great!  A few minor comments:\n\n. your new local variables and non-static fields use underscores as word separators, rather than camelCase, as is both the Java convention and the convention used in the files you're changing.  Can you please switch these to use camelCase?\n\n. In LocalJobRunner there are potentially multiple threads changing the working directory of a single fs.  Perhaps we should instead have a different fs instance for each job thread?  Should we make FileSystem support clone()?\n\nThanks!\n", "I'll go ahead and update the variable names to camelCast.\n\nThe LocalJobRunner only has a problem when the user is submitting multiple jobs at the same time, right?\n\nI knew I was opening a can of worms with local directories with respect to threads. There are lots of potential solutions including making the current directory thread local. (Although the LocalFileSystem would have discontinuities, because it also sets the system property, which is by definition global.) I think for now that we are better off following the unix semantics of treating the working directory as global.\n\nFor now, I'm proposing synchronizing on the file system, like:\n\n        synchronize (fs) {\n          setWorkingDirectory(job, fs);\n          MapTask map = new MapTask(file, (String)mapIds.get(i), splits[i]);\n        }\n\naround each of the places where the LocalJobRunner sets the working directory.\n\nOn a side note, if we are worried about local runners with multiple jobs, we should put synchronization in around the updates of the jobs list and other fields of the LocalJobRunner.", "The 'synchronize (fs) ...' is worthless unless everyone does it, which they don't, so I wouldn't bother.\n\nI think you're right that my concern about LocalJobRunner is misplaced.  It is not a public class, and a new instance is created for each job, so there's currently no way to have multiple jobs sharing a LocalJobRunner.\n\nLonger term, if this becomes an issue, I think making FileSystem cloneable is preferable to synchronization.\n", "Ok, here is an updated patch that uses camelCase everywhere for variable names.\n\nI also put in synchronization between the setWorkingDirectory and running the \napplication code in the LocalJobRunner.", "The synchronized (fs) will help the case where the application submits two jobs to the LocalJobRunner with different working directories. Because the LocalJobRunner obeys the synchronization with respect to the file system object, it should work. (I don't have any parallel map/reduce job applications to test it with and testing for missing synchronization is almost impossible anyways.)", "The synchronization fixes the problem, but causes everything to run sequentially.  I think with the same amount of code we could add a clone() method to FileSystem and provide a better fix.  Currently one cannot pass a FileSystem instance to JobClient, but if one could, then we would be changing the CWD of that filesystem underneath other processes that might be using it w/o synchronization.\n\nIf we want to be totally safe, we could make setWorkingDirectory return a clone, with no side-effects.  Cloning an FS should be cheap enough.", "By the way, i didn't see your comment from 10:17 when I submitted my patch at 10:18.\n\nI'll go ahead and unsynchronize LocalJobRunner and roll a new patch.\n\nI guess it does make sense to make FileSystem's clonable, even if LocalFileSystem sets the global system property. We can do that later.", "Ok, this patch includes cwd2.patch with the synchronization in LocalJobRunner rolled back.", "I have committed this.  Thanks for adding unit tests! I had to add a Thread.sleep(1000) to the dfs test code before the tests would pass, to give the datanode a chance to introduce itself to the namenode before the client code started using the fs.  Perhaps this is just masking a bug.  Perhaps DFSClient should sleep and retry when NameNode.create() throws an exception?", "Sorry\nbut It seems this code will thown IOException in windows cygwin env......\n\nit can't get file path correctly.", "Can you please include the text from the exception and the stack trace? Depending on where the exception was thrown, it might be in the server log files.", "I just update to the lately, start all the namenode, datanode, jobtracker and tasktracker\n\nrun nutch command (crawl / readdb etc.), and jobtracker thrown exception,\n\n\n060325 001911 Property 'file.separator' is \\\n060325 001911 Property 'java.vendor.url.bug' is http://java.sun.com/cgi-bin/bugr\neport.cgi\n060325 001911 Property 'sun.io.unicode.encoding' is UnicodeLittle\n060325 001911 Property 'sun.cpu.endian' is little\n060325 001911 Property 'sun.desktop' is windows\n060325 001911 Property 'sun.cpu.isalist' is\n060325 001911 Version Jetty/5.1.4\n060325 001911 Checking Resource aliases\n060325 001912 Started org.mortbay.jetty.servlet.WebApplicationHandler@1d7fbfb\n060325 001912 Started WebApplicationContext[/,/]\n060325 001912 Started SocketListener on 0.0.0.0:50030\n060325 001912 Started org.mortbay.jetty.Server@c88440\n060325 001921 Server connection on port 50020 from 192.168.1.10: starting\n060325 001927 Server connection on port 50020 from 192.168.1.11: starting\n060325 001933 Server connection on port 50020 from 192.168.1.12: starting\n060325 002240 parsing file:/D:/jobcall_trunk/hadoop/conf/hadoop-default.xml\n060325 002240 parsing file:/D:/jobcall_trunk/hadoop/conf/mapred-default.xml\n060325 002240 parsing file:/D:/jobcall_trunk/hadoop/conf/hadoop-site.xml\n060325 002240 parsing file:/D:/jobcall_trunk/hadoop/conf/hadoop-default.xml\n060325 002240 parsing file:/D:/jobcall_trunk/hadoop/conf/mapred-default.xml\n060325 002240 parsing \\tmp\\hadoop\\mapred\\local\\job_3dgnm3.xml\\jobTracker\n060325 002240 parsing file:/D:/jobcall_trunk/hadoop/conf/hadoop-site.xml\n060325 002258 parsing file:/D:/jobcall_trunk/hadoop/conf/hadoop-default.xml\n060325 002258 parsing file:/D:/jobcall_trunk/hadoop/conf/mapred-default.xml\n060325 002258 parsing \\tmp\\hadoop\\mapred\\local\\job_3dgnm3.xml\\jobTracker\n060325 002258 parsing file:/D:/jobcall_trunk/hadoop/conf/hadoop-site.xml\n060325 002258 job init failed\njava.io.IOException: No input directories specified in: Configuration: defaults:\n hadoop-default.xml , mapred-default.xml , \\tmp\\hadoop\\mapred\\local\\job_3dgnm3.x\nml\\jobTrackerfinal: hadoop-site.xml\n        at org.apache.hadoop.mapred.InputFormatBase.listFiles(InputFormatBase.ja\nva:90)\n        at org.apache.hadoop.mapred.InputFormatBase.getSplits(InputFormatBase.ja\nva:100)\n        at org.apache.hadoop.mapred.JobInProgress.initTasks(JobInProgress.java:1\n30)\n        at org.apache.hadoop.mapred.JobTracker$JobInitThread.run(JobTracker.java\n:204)\n        at java.lang.Thread.run(Thread.java:595)\n060325 002300 Server connection on port 50020 from 192.168.1.12: exiting\n"], "derived": {"summary": "DFS converts relative pathnames to be under /user/$USER. But MapReduce jobs may be submitted by a different user than is running the jobtracker and tasktracker.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "mapred input and output dirs must be absolute - DFS converts relative pathnames to be under /user/$USER. But MapReduce jobs may be submitted by a different user than is running the jobtracker and tasktracker."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just update to the lately, start all the namenode, datanode, jobtracker and tasktracker\n\nrun nutch command (crawl / readdb etc.), and jobtracker thrown exception,\n\n\n060325 001911 Property 'file.separator' is \\\n060325 001911 Property 'java.vendor.url.bug' is http://java.sun.com/cgi-bin/bugr\neport.cgi\n060325 001911 Property 'sun.io.unicode.encoding' is UnicodeLittle\n060325 001911 Property 'sun.cpu.endian' is little\n060325 001911 Property 'sun.desktop' is windows\n060325 001911 Property 'sun.cpu.isalist' is\n060325 001911 Version Jetty/5.1.4\n060325 001911 Checking Resource aliases\n060325 001912 Started org.mortbay.jetty.servlet.WebApplicationHandler@1d7fbfb\n060325 001912 Started WebApplicationContext[/,/]\n060325 001912 Started SocketListener on 0.0.0.0:50030\n060325 001912 Started org.mortbay.jetty.Server@c88440\n060325 001921 Server connection on port 50020 from 192.168.1.10: starting\n060325 001927 Server connection on port 50020 from 192.168.1.11: starting\n060325 001933 Server connection on port 50020 from 192.168.1.12: starting\n060325 002240 parsing file:/D:/jobcall_trunk/hadoop/conf/hadoop-default.xml\n060325 002240 parsing file:/D:/jobcall_trunk/hadoop/conf/mapred-default.xml\n060325 002240 parsing file:/D:/jobcall_trunk/hadoop/conf/hadoop-site.xml\n060325 002240 parsing file:/D:/jobcall_trunk/hadoop/conf/hadoop-default.xml\n060325 002240 parsing file:/D:/jobcall_trunk/hadoop/conf/mapred-default.xml\n060325 002240 parsing \\tmp\\hadoop\\mapred\\local\\job_3dgnm3.xml\\jobTracker\n060325 002240 parsing file:/D:/jobcall_trunk/hadoop/conf/hadoop-site.xml\n060325 002258 parsing file:/D:/jobcall_trunk/hadoop/conf/hadoop-default.xml\n060325 002258 parsing file:/D:/jobcall_trunk/hadoop/conf/mapred-default.xml\n060325 002258 parsing \\tmp\\hadoop\\mapred\\local\\job_3dgnm3.xml\\jobTracker\n060325 002258 parsing file:/D:/jobcall_trunk/hadoop/conf/hadoop-site.xml\n060325 002258 job init failed\njava.io.IOException: No input directories specified in: Configuration: defaults:\n hadoop-default.xml , mapred-default.xml , \\tmp\\hadoop\\mapred\\local\\job_3dgnm3.x\nml\\jobTrackerfinal: hadoop-site.xml\n        at org.apache.hadoop.mapred.InputFormatBase.listFiles(InputFormatBase.ja\nva:90)\n        at org.apache.hadoop.mapred.InputFormatBase.getSplits(InputFormatBase.ja\nva:100)\n        at org.apache.hadoop.mapred.JobInProgress.initTasks(JobInProgress.java:1\n30)\n        at org.apache.hadoop.mapred.JobTracker$JobInitThread.run(JobTracker.java\n:204)\n        at java.lang.Thread.run(Thread.java:595)\n060325 002300 Server connection on port 50020 from 192.168.1.12: exiting"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-53", "title": "MapReduce log files should be storable in dfs.", "status": "Closed", "priority": "Major", "reporter": "Doug Cutting", "assignee": "Enis Soztutar", "labels": [], "created": "2006-02-23T05:31:41.000+0000", "updated": "2009-07-08T16:51:38.000+0000", "description": "It should be possible to cause a job's log output to be stored in dfs.  The jobtracker's log output and (optionally) all tasktracker log output related to a job should be storable in a job-specified dfs directory.", "comments": ["We should determine how to capture and organize user logs in general before doing work to save them in HDFS", "Attaching a patch to store the tasks' logs to the FileSystem used. This is useful for example to store the logs permanently or to access the logs in a centralized way. \n\nThe patch adds a new log4j Appender, called FsLogAppender, which appends logs to the files in the FileSystem. The appender is called FSLA. The old appender(TLA) still continues. The user can select which appender to use, and which log level to use via JobConf(). ex : job.setTaskLogRootLogger(\"INFO,TLA,FSLA\"); \nThe user can also specify the location to save the logs : job.setTaskLogDir(Path);\n\nNow, at DEBUG level the logs from org.apache.hadoop will continue to polute the logs of the user's program, but we will defer this to a separate issue. \n\nThe appender can also be used to store the logs of the framework(for debugging etc,), but again it is a seperate issue. ", "-1 overall.  Here are the results of testing the latest attachment \nhttp://issues.apache.org/jira/secure/attachment/12367954/mapredDFSLog_v1.patch\nagainst trunk revision r586264.\n\n    @author +1.  The patch does not contain any @author tags.\n\n    javadoc -1.  The javadoc tool appears to have generated  messages.\n\n    javac +1.  The applied patch does not generate any new compiler warnings.\n\n    findbugs +1.  The patch does not introduce any new Findbugs warnings.\n\n    core tests +1.  The patch passed core unit tests.\n\n    contrib tests -1.  The patch failed contrib unit tests.\n\nTest results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/970/testReport/\nFindbugs warnings: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/970/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/970/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/970/console\n\nThis message is automatically generated.", "The tasks using FsLogAppender enter a zombie state and wont finalize. I will supply another patch. ", "Patch updated to trunk, FsLogAppender is refactored to o.a.h.util. \n\nbq. The tasks using FsLogAppender enter a zombie state and wont finalize.\nI have failed to reproduce this in several tests running different jobs, i guess the zombie processes were the leftovers caused by the earlier versions of the patch. \n\nCan someone review the patch please? ", "-1 overall.  Here are the results of testing the latest attachment \nhttp://issues.apache.org/jira/secure/attachment/12368384/mapredDFSLog_v2.patch\nagainst trunk revision r588341.\n\n    @author +1.  The patch does not contain any @author tags.\n\n    patch -1.  The patch command could not apply the patch.\n\nConsole output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/988/console\n\nThis message is automatically generated.", "I think it would be far better to have the FsLogAppender just ignore any messages that came when the appender wasn't \"ready\". In particular, we should avoid having special methods that need to be invoked for initialization and closing. The FsLogAppender should work with non-default file systems, by doing:\n\n{code}\nFileSystem fs = path.getFileSystem(conf);\nDataOutputStream out = fs.create(path);\n{code}", "The appender ignores the massages until it is properly initialized, but the thing is that the logger itself generates logging statements during initialization(for example ipc debug logs.). FsLogAppender will work on the filesystem that is active in the configuration given to its init() method. \n\nbq. In particular, we should avoid having special methods that need to be invoked for initialization and closing. \nCurrent design does need extra initialization and finalization because we are using log4j's configurable way of using appenders. It is good that we can configure logging either to use fs or local files, but then we need to let log4j construct the appender for us, so we should somehow pass the conf object to the appender, right? We can definitely use something like \n{code}\nJobConf#enableDFSLogging();\nJobConf#setLogLevel();\nJobConf#setLogDir();\n{code}\nthen construct FslogAppender, and add it to the rootLogger however what if we extend the logging system so that it can be used to store the logs of {job|task}trackers and {name|data}nodes. Then we should have custom code to set the appender rather that using conf/log4j.properties. \n\nLong story short, I think current architecture is slightly ugly, but I'm OK with it. \n\n\n\n", "Patch updated to meet svn trunk. ", "Any chance we could get a patch against 0.15 for this?", "bq. Any chance we could get a patch against 0.15 for this?\nYou can manually apply the patch to 0.15, it should not be hard. \n", "Closing this issue, since we will have a more general log aggregation framework : HADOOP-2206. "], "derived": {"summary": "It should be possible to cause a job's log output to be stored in dfs. The jobtracker's log output and (optionally) all tasktracker log output related to a job should be storable in a job-specified dfs directory.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "MapReduce log files should be storable in dfs. - It should be possible to cause a job's log output to be stored in dfs. The jobtracker's log output and (optionally) all tasktracker log output related to a job should be storable in a job-specified dfs directory."}, {"q": "What updates or decisions were made in the discussion?", "a": "Closing this issue, since we will have a more general log aggregation framework : HADOOP-2206."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-54", "title": "SequenceFile should compress blocks, not individual entries", "status": "Closed", "priority": "Major", "reporter": "Doug Cutting", "assignee": "Arun Murthy", "labels": [], "created": "2006-02-23T05:39:30.000+0000", "updated": "2013-05-02T02:28:58.000+0000", "description": "SequenceFile will optionally compress individual values.  But both compression and performance would be much better if sequences of keys and values are compressed together.  Sync marks should only be placed between blocks.  This will require some changes to MapFile too, so that all file positions stored there are the positions of blocks, not entries within blocks.  Probably this can be accomplished by adding a getBlockStartPosition() method to SequenceFile.Writer.", "comments": ["I've been thinking about this.  I think we should use a very simple scheme like the current sequence file.  The writer would take a configurable buffer, maybe 10 meg by default, fill that with key/values and then compress them.  The result would be stored with start markers like the current sequence file, so the partitioning logic could remain unchanged.\n\nWithin a block, I think we should compress the keys together in a block and the values in a following block.  Both blocks' lengths should be stored, so that one can quickly scan the keys and then scan only as far as needed in the value blocks.  This would allow very efficient sub-sampling when you have large data blocks.  Which could be a life saver in some of our typical apps.  You'll also get better compression, since you'll be compressing like items together.\n\nThoughts?", "Also, we should be able to specify a custom compressor for keys or values.", "I like this proposal.  Long-term, custom compressors make good sense, but for the first version let's just use a fixed algorithm and add that as a separate pass, ok?  That could be a separate issue in Jira.", "Here are some thoughts on how to go about this... inputs are much appreciated!\n \n <thoughts>\n\n    The key idea is to compress blocks and not individual 'values' as is done (optionally) today in a SequenceFile.\n\n    The plan is to have a configurable buffer (say default of 1MB? or 10MB?), fill it up with key/value pairs and then compress them. When the buffer is (almost) full we compress the keys together into a block and values into the following block and then write them out to file along with necessary headers and markers. The point of compressing keys and values separately (as Eric points out) is:\n    a) (hopefully) better compression since 'like' items are compressed together.\n    b) if need be, iterating over 'keys' itself is faster since we don't need to uncompress 'values'.\n\n    We could also write out 'sync' markers everytime the whole key-value compressed buffer is written out to dfs or we can double up the sync and end-of-block marker, thus these sync-markers also double up as end-of-compressed-block-markers. Of course the 'sync' marker is similar to the one used today in SequenceFiles. (thoughts?)\n\n\n    E.g.\n    a) configured buffer size - 4096b\n    b) key/values\n        keyA - 32b, valueA - 1024b\n        keyB - 64b, valueB - 2048b\n    c) compressedSize(keyA+keyB) - 75b\n    d) compressedSize(valueA+valueB) - 2500b\n\n\n    On disk:\n\n              no. of k/v pairs       (c)                        (d)\n                          |                 |                           |\n     ------------------------------------------------------------------------------------------------------------------------------------\n    | sync-marker | 2 | 32 | 64 | 75 | 1024 | 2048 | 2500 | compressedKeys (blob) | compressedValues (blob) | sync-marker |\n     ------------------------------------------------------------------------------------------------------------------------------------\n                                   |                     |     \n                              ______           ______\n                            (key sizes)     (value sizes)\n\n\n   Non-graphical version:\n    -------------------------\n    sync,2,32,64,75,1024,2048,2500,compressedKeys,compressedValuesBlob,sync.\n\n    Clarification: All lengths above are uncompressed and as is on disk. (write/read via writeInt/readInt apis).\n\n   Points to ponder:\n    -----------------\n    a) Since we need to store keys and values in separate (compressed) blocks on disk does it make sense to:\n        i) use 2 buffers (one for each) to store them before compressing i.e. before we hit the configured (1/10/* MB) limit (both buffers combined) - fairly simple to implement!\n        ii) interleave them in the buffer and then make 2 passes to compress and write to disk.\n    b) Strategy for buffer-size vis-a-vis dfs-block-size? Should we pad out after compressing and before writing to disk? Better to ignore this for now and let dfs handle it better in future?\n    c) Thoughts by Eric/Doug w.r.t custom compressors for keys/values.\n\n </thoughts>\n\n\nthanks,\nArun\n\n", "Another feature that might be useful: include an (optional) un-compressed copy of the *last* key in a given compression spill. Why? Because, for sorted SequenceFiles (specifically, for the data file of a MapFile), when seeking through to find a given key/value, knowing the last key in a given chunk allows skipping of decompressing the entire key array.\n\nIt should, of course, be optional, both because keys potentially be large, and because SequenceFiles aren't all sorted. But, in the MapFile case, it could reduce the cost of finding a hit during lookups.\n\nMight also be useful to try to do some DFS block-aligning, to avoid block requests and CRC calculations for data that's not really going to be used. That sounds like it might be tricky to get, though, because default GFS blocks are so large, and we're probably talking about much smaller compression chunks. Does DFS have variable-length block writing yet?", "Couple of implementation thoughts/refinements (thanks  by Eric) I wanted to bring to everyone's attention:\n\n  a) We could use BytesWritables for keys/values (which maintain their own sizes), thus the header would then only be:\n      sync,no.of k/v pairs, compressed-keys-size, compressed-values-size, compressedKeysBlob, compressedValuesBlob, sync\n\n  b) Use the 'zero-compressed-int-length' that Milind is exporting from recordio for the 'integer' values in the header to save some bytes.\n\nThoughts? Other refinements?\n\nthanks,\nArun", "Ok, I talked with Eric about this.\n\nThe thought about BytesWritable was just about trying to get rid of the redundant lengths in the SequenceFile format. (Both the Writable object and the SequenceFile encode the object's length.) This redundancy is caused because Writable.readFields is not given the length and yet the SequenceFile wants to be able to know the length so that it can copy/skip keys and values during copy and sorting. Unfortunately, to remove the redundancy would require extending the Writable interface, which would break a lot of application code. (We could make an optional extension that provides a mechanism for getting the length out of the serialized form or copying the raw bytes of the proper length and allow Writable types to implement the sub-interface if they want to have smaller files.)\n\nFor now, I'd suggest mixing in the length in a zero compressed format. \n\nSo inside the compressed key block would look like:\n<key1-length><key1-bytes><key2-length><key2-bytes><key3-length><key3-bytes>\n\nAnd the inside of the compressed value block would look like:\n<value1-length><value1-bytes><value2-length><value2-bytes><value3-length><value3-bytes>\n\nThe blocks would look like:\n<sync><num-records><compressed-key-bytes><compressed-value-bytes><compressed-keys-data><compressed-values-data>\n\nWith the blocks padding upto io.sequencefile.pad.percent to align to the DFS block boundary.\n\nDoes that sound reasonable?", "SequenceFile already has an API for reading and writing raw keys and values:\n\nhttp://lucene.apache.org/hadoop/docs/api/org/apache/hadoop/io/SequenceFile.Reader.html#next(org.apache.hadoop.io.DataOutputBuffer)\nhttp://lucene.apache.org/hadoop/docs/api/org/apache/hadoop/io/SequenceFile.Writer.html#append(byte[],%20int,%20int,%20int)\n\nSo it's easy to write an InputFormat or OutputFormat for entries that do not encode their own length.\n\nWritable implementations must encode their length in order to be easily nestable: if a key contains a struct with string, int and long values, then the lenghts of the nested strings must be explicitly written.\n\nIf we're really worried about redundant lengths, we could add a RawWritable sub-interface that adds something like rawLength(), rawWrite(DataOutput), and rawRead(DataInput, int length).  (This is roughly what Owen referred to.)\n\nI don't think we should worry about padding to DFS block boundaries in the first implementation, but rather leave that as a subsequent optimization.\n\nI'm +1 for Owen's pro[osed format.", " +1 for Owen's proposal.\n\n An unrelated issue: the 'append' method in SequenceFile.Writer is passed 2 Writables: key and value. The Writable interface doesn't have a 'getLength' interface. This means one would have to write out the key/value to a temporary buffer to actually figure out it's 'length'. The lengths are particularly relevant here to ensure that the key/value pair can be put into the keyBuffer/valueBuffer without violating the 'configured' maxBufferSize...\n\n To get around this issue: how about making the 'configured' bufferSize the 'lower_bound' instead of the 'upper_bound'? This will ensure we can write out the key/value and then check the buffer size, and if need be go ahead and compress etc. This will save the construction of the temporary buffer for getting the key/value lengths. Related gain: it's far simpler with this scheme to deal with outlier/rouge keys/values which are larger than bufferSize itself.\n\n Logical next step: make this 'bufferSize' configurable per SequenceFile, this will let applications control it depending on the sizes of their keys/values. I propose to introduce a new constructor with this as an argument for SequenceFile.Writer. This will then be written out as a part of the file-header (along with compression details) and the SequenceFile.Reader can pick this up and read accordingly. (Of course there will be a system-wide default if unspecified per file).\n\n Thoughts?\n\nthanks,\nArun\n\n ", "Actually we have an 'oops' here... afaics there isn't a way to even construct\n  <key1-length><key1-bytes><key2-length><key2-bytes><key3-length><key3-bytes> \n  <value1-length><value1-bytes><value2-length><value2-bytes><value3-length><value3-bytes> \nwithout constructing a temporary object from the Writable key/value, since there isn't a way to figure out the key/value length at all.\n\nI feel constructing a temporary object will be huge overhead i.e. introduce an extra copy from Writable to temp-buffer and then from Writable/tempBuffer to keyBuffer/valueBuffer for compression... \n\nAny way to do this without an extra copy?\n\n-*-*-\n\nOne alternative I can think of to avoid the extra copy is slightly convoluted, though this still won't be able to construct Owen's proposal as-is. (Will look very similar to my older proposal)\n\nMaintain 2 auxillary arrays which keep track of actual key/value lengths. The way to maintain lengths is to compute difference in size of keyBuffer/valBuffer before and after insertion of each key/value and then store that difference.\n\nE.g. \n\nint oldKeyBufferLength = keyBuffer.length();\nkey.write(keyBuffer);\nint newKeyBufferLength = keyBuffer.length();\nkeySizes.insert(newKeyBufferLength - oldKeyBufferLength); //save the last key size\n\n// same for 'val'\n\nif((newKeyBufferLength + newValueBufferLength) > minBufferSize) {   //lower_bound instead of 'higher_bound'\n\n  // Compress both keyBuffer and valueBuffer\n  // Write out keySizes array to disk in zero-compressed format\n  // Write out valueSizes array to disk in zero-compressed format\n  // Write out compressedKeyBufferSize in zero-compressed format\n  // Write out compressed keyBuffer\n  // Write out compressedValueBufferSize in zero-compressed format\n  // Write out compressed valueBuffer\n  // Reset keyBuffer, valueBuffer, keySizes and valueSizes\n\n} else {\n  // Done\n  return;\n}\n\n\n-*-*-\n\n\nAppreciate any inputs/alternatives/refinements...\n\nthanks,\nArun", "> Any way to do this without an extra copy?\n\nThere's no way to length-prefix things without knowing the length, and the length is only known after the value is serialized.  I doubt the extra copy will significantly affect overall write performance.\n\nNote that folks who use the raw API, directly passing bytes rather than Writables, should not suffer the extra-copy penalty when adding items to a file.", "I think this is enough of an argument to return to a format with key lengths segregated though.  We proposed interleaved because we thought it would be simpler to code.  Since it clearly will not be, we might as sell segregate lengths.  This has the extra advantage that it will compress better (because we will be grouping like data together).\n\nI think this brings us full circle back to arun's original proposal (but all wiser).  So I'm now proposing:\n\n| sync-marker \n| numPairs (delta compressed int)\n| KeyLenBlockCompressedLen (delta compressed int)\n| KeyBlockCompressedLen  (delta compressed int)\n| ValueLenBlockCompressedLen  (delta compressed int)\n| ValueBlockCompressedLen  (delta compressed int)\n| keyLengths... (gzipped & delta compressed)\n| keys... (gzipped or custom compressed)\n| valueLengths... (gzipped & delta compressed)\n| values... (gzipped or custom compressed)\n\nThis will allow one to scan the keys and skip the values if desired.  It will yield pretty good compression.  Its not that hard to understand if well documented...\n\n--\n\nI like arun's suggestion of considering the target block size the minimum.  That will keep things simpler.\n\nI suggest we proceed this way, unless anyone objects?\n", "Re documentation: perhaps we should add file-format documentation to the javadoc as a part of this change.  This could look something like Lucene's file-formats documentation (except simpler).  That mixes a pseudo-formal BNF-like syntax with commentary:\n\nhttp://lucene.apache.org/java/docs/fileformats.html\n\nThis should link to other javadoc to describe things like the zero-compressed int format.\n\nEric's proposal looks good to me.  I assume zero-compressed ints are meant most places he says 'delta compressed'.  Is that right?  It may make sense to delta compress the lists of key and value lengths, but it probably does not make sense to delta compress numPairs and the block lengths.", "Here's a test run analysing compression of VInts (exported from recordio).\n\nEssentially we get almost 50% savings (either with zlib/gzip) of compressed-VInts v/s uncompressed raw integers on disk (4bytes per int) ... \n\nSounds like a good reason to go ahead with Eric's proposal to compress not only keys/values but also keyLengthsBlock/valueLengthsBlock? I'll go ahead with this for now unless anyone objects...\n\nthanks,\nArun\n\n", "Just to be clear, delta compression means something different to me than zero-compression.  The former represents a lists of integers with their differences, the latter elides leading zeros in integers.  They're not exclusive.  A sorted list of integers is smaller when delta compressed and zero-compressed.  A random list of integers is probably not helped by delta compression, but is helped by zero compression.  If values are in a narrow range, then delta compression may help.  Thus it may be useful for lists of key lengths and value lengths.\n\nYou provide some benchmarks showing the advantage of zero compression for random sequences.  Eric said delta compression, but I think he meant zero compression.  I agree that we should use zero compression everywhere.  The only question is if we should also use delta compression anywhere.\n", "I meant zero compressed.  So we're all on the same page.\n\n(Of course recording key & value lengths rather than offsets is delta compression, so we are actually doing both)\n", "Sounds good... looks like we're all on the same page; I'll get going on this.\n\nAppreciate both of you spending time... I'll also keep in mind Doug's thoughts on documenting the file-format for SequenceFile.\n\nthanks!\nArun", "Issues which I came across while implementing the above proposal...\n\n1. The implementation of the public interface\n\n  SequenceFile.Writer.append(byte[] data, int start, int length. int keyLength)\n\n  as it exists today, does not honour 'deflateValues' i.e. it does not compress 'values' at all. I feel this is contrary to user's expectations since the other 'append' interface does compress values and breaks the abstraction of 'compressed' sequence files. I propose we remedy this now and add necessary support for the same here too. (I understand that it might be a break with existing behaviour, but I feel we should correct this right-away... we need to fix it some time or the other.). \n\n I will also go ahead and add a 'rawAppend' public interface if the existing functionality (just write data to disc with heeding 'deflateValues') is deemed necessary.\n\n\n2. I propose we add a public interface:\n \n  void flush() throws IOException \n\n  to SequenceFile.Writer to let the user explictly compress and flush existing data in key/value buffers. \n\n This api will also be used from existing 'close' (flush remaining data in buffers) and 'append' (flush buffers to dfs after they exceed the configured size) apis internally... the only point of contention is whether I should make this api 'public'.\n\n3. Afaik I can't see a way to 'configure' the default 'minimum buffer size' since the SequenceFile class, as it exists, does not seem to have any access to a 'Configuration' object... \n  (... in my previous life Owen pointed out that making a 'comparator' class implement the 'Configurable' interface ensured that it's 'configure' api would be called by the framework; will this trick work again?!)\n\n  I don't want to hardcode any values for 'minimum buffer size' nor does the idea of adding a new constructor with a 'Configuration' object as one of the params look very appealing... \n\n-*-*-\n\n Thoughts? ", "The \"raw\" append/next interface for SequenceFile is intended to get the raw bytes from the file. Its intended use was for doing things like merging and sorting where the values don't need to be instantiated. So the lack of decompression was deliberate. However, in the switch to block compression, that doesn't make sense. In the new block compression reader and writer, just treat them as a key or value that has already been serialized for you.", "One more thing, the \"append\" isn't appending bytes, but a preserialized key/value pair. The interface is a little unfortunate, because it forces the key and value to be in the same buffer. A more general interface would be something like:\n\nappend(byte[] key, int keyOffset, int keyLength, byte[] value, int valueOffset, int valueLength)\n\nThe advantage of that interface is that if the application has the key and value in different buffers they don't need to be copied into a single buffer before being copied to the SequenceFile.Writer's buffer.", "Sounds like we better be careful here.  \n\nThis raw interface is presumably used mainly by the framework?  So we can probably change it without breaking the universe?\n\nI think we should probably change it to deal with \"serializedRef\" objects or some other new type that points to the buffer and keeps the info on if the data is compressed (and with which class).\n\nOtherwise application code is going to need to deal with tracking the state of each object and  finding the right compress/decompress calls to make.  A frequent scenario will be moving things from a perhaps block compressed file to an item compressed format for sorting.  That should be efficient if possible.\n\nI like this approach because it makes what is going on very explicit, vs the current interface, which is obviously confusing.  The alternative seems to be copious documentation in the classes and all use cases and frequent discussions on the list...\n", "Arun, what does flush do exactly?  Does it create a block boundary?  I'd vote for not expanding the interface until we have a compelling use case.  Although I can see how this could arise.\n\nPS.  I'm all for hardcoding a value of 2MB or something else sane for the block size.  Keeping things simple is often much more important than making everything configurable.  As long as the API lets you set the buffer size on creation, I'm happy.  My experience with such systems is at some point the number of configuration options becomes a weakness, not a strength.  Happy to be out voted on this point. (In this case)\n", "-1 on adding flush to the public api.\n\nI just checked and the only users of SequenceFile.Writer.append(byte[], ...) in both Hadoop and Nutch are in SequenceFile itself. Once I have my RawSequenceInputFormat, it will also be a user of this interface.", "Rebuttals:\n\n1. append\n\n  I like Owen's idea about generalising the interface to:\n  \n  append(byte[] key, int keyOffset, int keyLength, byte[] value, int valueOffset, int valueLength) \n\n  with it's associated 'clarity' for user and advantage that it precludes an extra copy into a single buffer... couple of +1s and I'll take this path.\n\n  @Owen: I understand that this interface currently appends a 'preserialized' key/value pair, but as you point out with 'compressed blocks' this gets worrisome in the long run (things like 'custom compression' will require 'serializedRef' like objects soon enough) ... \n\n How about letting the user pass in the preserialized key/value and then we will still go ahead and honour 'deflateValues' in the append? Honouring 'compress' directives will ensure consistent behaviour with rest of apis (read: append(Writable key, Writable value) and also uncompress in the SequenceFile.Reader.next call will ensure the what-you-store-is-what-you-get contract.\n\n Otherwise a true 'rawAppend' will mean (especially in 'compressed blocks' context) that I will need to create a 'block' with a single key/value pair and write out to disk ...  \n\n Summarising: We can switch to the 'general' append interface and honour 'compress' directives in it... ensuring consistency & clarity. (I also volunteer to fix 'older' append calls in SequenceFile.java; Owen can then \n\n2. flush\n\n  I should have worded things more carefully... I was looking to see if there is a compelling use case for this 'already'. \n  Looks like there isn't... I'll drop this. \n  \n  (Adding a public 'flush' later is infinitely easier than adding now and removing later... :) )\n\n  @Eric: Yep, the 'flush' does create a block boundary, it's used internally in 2 cases for now: (a) sizeof(keyBuffer+valueBuffer) exceeds minBlockSize (b) When the 'close' api is called.\n    \n3. configuration\n \n  I concur with need to keep things simple... I'll just hardcode a 'sane' value for now. \n  (Yes, there is a way via the constructor to set the buffersize on creation.)\n\n  (PS: I do hear bells in my head when I see, as it exists, SequenceFile.Reader gets a 'Configuration' object via the constructor and the 'symmetric' SequenceFile.Writer doesn't... but that's topic for another discussion.)\n", "I suggest adding the binary append API suggested by Owen and deprecating the old binary append API, but making it work back-compatibly.  Thus it should accept pre-compressed (if compression is enabled) values, de-compress them, then call the new append method.  We should update all existing binary appends in Hadoop and prepare a patch for Nutch to do the same.  Then we should file a bug to remove the deprecated method in the next release.\n\nWe unfortunately lose the ability to move individual compressed values around.  If a mapper does not touch values, it would be best to only decompress values on reduce nodes, rather than decompress and recompress them on map nodes, since compression can be computationally expensive.  But I don't see how to avoid this if we want to compress multiple values together.  I think this argues that we might still permit the existing single-value compression, since that might be most efficient for large-valued files that are not touched during maps.\n\nAlso, please add a public SequenceFile.Writer() constructor that accepts a Configuration.  We should probably also deprecate the unconfigured constructor and remove it in the next release.  I agree with Eric that things can be over-configurable, but its easier to make them configurable in the code from the start, and only as-needed add them to hadoop-default.xml, so that folks who have not read the code can tweak them.\n\nI also agree that flush should not be public.", "> I suggest adding the binary append API suggested by Owen and deprecating the old binary append API, but making it work back-compatibly. Thus it should accept pre-compressed (if compression is enabled) values, de-compress them, then call the new append method. \n\n  My hunch is that we do not need to worry about 'pre-compressed' values since as of today both the 'raw' apis do not honour it... is this true?\n\n  In-fact we could take the route in which 'append' compresses whatever data is passed along, thus possibly compressing data twice. With the 'symmetric' call to next (which decompresses) we give back the data which the user passed along in the first place... I did have a chat with Owen about this and we both felt this could work.\n\n> We unfortunately lose the ability to move individual compressed values around. If a mapper does not touch values, it would be best to only decompress values on reduce nodes, rather than decompress and recompress them on map nodes, since compression can be computationally expensive.\n\n  I can see a way around this if it really will make a difference...\n\n  We can take the path that values are decompressed only 'on demand' i.e. a series of calls to SequenceFile.Reader.next(Writable key) does not need to decompress 'valBuffer' (or even valLengthsBuffer). Hence when we read a compressed 'block' we need not decompress values till we see a call to either SequenceFile.Reader.next(Writable key, Writable value) or SequenceFile.Reader.next(DataOutputBuffer buffer).\n\n  Implementing this 'lazy decompression' of values is slightly more complex... worth it?\n\n-*-*-\n\n  PS:   \n\n  1. SequenceFile.Reader.next(DataOutputBuffer buffer) should be changed to \n     SequenceFile.Reader.next(DataOutputBuffer keyBuffer, DataOutputBuffer valBuffer) similar to the 'raw' append api?\n\n  2. Does it make sense to have compression configurable for both keys and values separately? i.e. let user specify (during creation) whether he preferes to compress 'keys' or 'values' or both? Overkill for now? Maybe makes sense once we move to custom compressors for each?\n", "Folks...\n\nI'm happy with everything on this thread now, except for the raw API as discussed.  Could folks please consider my suggestion in:\n\nhttp://issues.apache.org/jira/browse/HADOOP-54#action_12422716\n\nI think this addresses all the concerns about sometimes compressed  data and avoiding the loose of current functionality etc.  I also think that it removes a very dangerous ambiguity that the current & owen's version of raw API permit (what is raw? who can use it...).\n\nPlease let me know what you think of this....\n", "Eric, I don't see how to implement both block compression, which is a huge win, and access to a pre-decompression representation. Especially if what you want to do with the pre-decompression representation is sorting or merging. Therefore, I was (and am) proposing that the \"raw\" access is a little less raw and that the byte[] representation is always decompressed. Am I missing something? This is an semantic change to the \"raw\" SequenceFile API, but I think it is required to get block-level compression.\n\nOn a slight tangent, I think that the SequenceFile.Reader should not decompress the entire block but just enough to get the next key/value pair.", "I completely agree that you should incrementally decompress.  The right answer might be just enough for the next entry or a small buffer,  should performance test that.\n\nMy point on raw is that you can return a reference tuple in an object:\n\n   <raw bytes,is compressed flag, compressor class> used in a reference\n\nThen you read the bytes, decompressed if they come from a block compressed or an uncompressed file, compressed if they come from an item compressed file.\n\nThen you pass this reference to the target sequence file's raw write method.  The target then compresses or decompresses as needed.\n\nSince you package all of this up behind an API, folks will not get confused into using this essentially internal API to do the wrong thing  and it will efficiently passed item compressed objects from one such stream to another if given the chance.\n\nThis may be worth considering, since sorts and merges may often operate on item compressed values and this will avoid a lot of spurious decompression/compression.\n\nPS we probably should only bother doing this for values.\n", "My point is that the raw bytes are useless except in their original context.\n\nSay my value is compressed as the byte stream: 12, 34, 56, 78\nIf I'm merging 100 files, I can't write 12, 34, 56, 78 to the output file and expect it to work, because naturally the compressed bytes depend on the state of the compressor.\n\nSo your reference tuple, would need to be:\n\n<raw bytes, compressor class, compressor state>\n\nwhere the compressor state is some compressor specific data. In the case of gzip, it is the last 32k of decompressed byte or whatever.\n\nAnd that assumes that no one ever tries to use a compression algorithm that uses partial bytes.\n\nIt looks to me like you'd add a lot of complexity for very little gain. You'd only win if you had large compressed values that you didn't really need to look at or use for anything. (For example, if you wanted to take a table that was url -> html document and generate the number of urls in each domain.) ", "The point is that you can carry object compressed data through the system compressed.  Block compressed data clearly needs to be uncompressed.\n\nIn a lot of situations this makes block compression undesirable.  We don't want to loose an important current performance optimization to add block compression if we can avoid it.", "Regarding 'incremental decompress' in SequenceFile.Reader:\n\nMaybe I'm miss something here - but isn't 1 decompress (of the whole block) followed by n reads of keys (or values) going to lead to the same amortized cost as m (where m < n) decompress and n reads? In that case I don't think the complexity of managing this complicated beast (maintaining status of how much is compressed, might have to decompress multiple times to get large values etc.) is worth it...\n", "Sorry for being dense. I missed the fact that you wanted to preserve key-value pair compression as an option.\n\nI'd propose spliting the classes like:\n\nSequenceFile.Writer  // uncompressed\nSequenceFile.RecordCompressWriter  extends Writer\nSequenceFile.BlockCompressWriter  extends Writer\n\nThey would have the current interface, with the following new functions:\n\n  void append(byte[] key, int keyOffset, int keyLength, byte[] value, int valueOffset, int valueLength);\n  boolean canAppendCompressed();\n  void appendCompressed(byte[] key, int keyOffset, int keyLength, \n                                                 byte[] value, int valueOffset, int valueLength);\n\nwhen we add custom compressors, we can add the compressor to the constructors.\n\n The Reader should have methods like:\n\n   boolean next(DataOutputStream key, DataOutputStream value);\n   boolean canReadCompressed();\n   void readCompressed(DataOutputStream key, DataOutputStream value);\n\nwhen we add custom compressors, we can add a getter for them like:\n   StreamCompressor getCompressor();\n\nAs an implementation, I'd consider having SequenceFile.Reader be a bridge to the class that is doing the reading based on the how it is compressed.\n\nThoughts?", "Ok, after talking it over with Eric, here is what is hopefully a last pass at this.\n\nAll in SequenceFile:\n\npublic static class Writer {\n  ... current stuff ...\n  /**\n   * Append an uncompressed representation of the key and a raw representation of the value as the\n   * next record.\n   */\n  public void appendRaw(byte[] key, int keyOffset, int keyLength, RawValue value);\n}\n\npublic static class RecordCompressWriter extends Writer {\n   ... constructor and some overriding methods ...\n}\n\npublic static class BlockCompressWriter extends Writer {\n   ... constructor and some overriding methods ...\n}\n\npublic static class Reader {\n  ... current stuff ...\n  /**\n   * Read the next key into the key buffer and return the value as a RawValue.\n   * @param key a buffer to store the uncompressed serialized key in as a sequence of bytes\n   * @returns NULL if there are no more key/value pairs in the file\n   */\n  public RawValue readRaw(DataOutputStream key);\n}\n\npublic static interface RawValue {\n\n   // writes the uncompressed bytes to the outStream\n   public void writeUncompressedBytes(DataOutputStream outStream);\n\n   // is this raw value compressed (using zip)?\n   public boolean canWriteCompressed();\n\n   // write the (zip) compressed bytes. note that it will NOT compress the bytes if they are not \n   // already compressed\n   // throws IllegalArgumentException if the value is not already compressed\n   public void writeCompressedBytes(DataOutputStream outStream);\n\n   // when we add custom compressors, we would add:\n   public boolean canWriteCompressed(Class compressorClass);\n   public void writeCompressedBytes(Class compressorClass, DataOutputStream outStream);\n}\n", "I mostly have minor naming quibbles.\n\nThe Writer method should be named just 'next', not 'nextRaw'.\n\nThe new Writer subclasses should not be public, but rather should be created by a factory method.\n\nThe RawValue class might better be named 'ValueBytes', and it's methods can simply be called writeCompressed(), writeUncompressed(), etc.\n\nFinally, a substantive remark: we should not allocate a new RawValue for each key read.  So the new Reader methods should be:\n\npublic ValueBytes createValueBytes();\npublic void next(DataOutputStream key, ValueBytes value);\n\n", "I'm not set on the name nextRaw, but I think the semantics are different enough that it deserves a different name, but whatever people want.\n\nI think the 3 Writer classes should be public, because they are actually doing different things and will likely end up with different parameter lists on the constructors. I'd hate to make the factory take the super set of all parameters that any of the Writers want.\n\nValueBytes is fine.\n\nI wasn't really intending to create a RawValue/ValueBytes for each iteration and your interface makes that clearer, so I like that. *smile*", "Addendum:\n\nI spoke to Owen to confirm that it makes sense to implement 'lazy decompression' of 'values' in block compressed files i.e. a series of calls to:\nSequenceFile.Reader.next(Writable key)\nwill not decompress 'value' blocks until a call to either\nSequenceFile.Reader.next(Writable key, Writable val) or\nSequenceFile.Reader.getCurrentValue(Writable val)  {explained below}\n\nGoing along the same trajectory it makes sense to add a 'getCurrentValue' api to the Reader, enabling the user to look at the 'key' and only then decide if he wants to fetch the 'value' (lazy decompression of 'value' holds here too; with associated better performance).\n\nThoughts?\n\n", "+1", "+1", "+1", "+1\n\nHairong is doing a patch today that will add next(key) and getCurrentValue(value) to Reader because she needed it for a patch she is working on. (She hasn't filed the jira yet, or I'd go ahead and link it.)", "An implementation detail which I would like to bring to everyone's attention: \n\nWith the 'raw' values now being a concrete object (implementing the 'ValueBytes' interface) we now have a situation in the 'base' sort of SequenceFile.Sorter where we will potentially have to store millions of 'rawValue' objects (assuming a decent sized SequenceFile with 'small' records). \n\nAs it exists today the 'sort' implementation in SequenceFile.Sorter uses 'io.sort.mb' no. of bytes to 'segment' input for sorting (and later merges them). \n\nOwen suggested we also use 'no. of records' to augment the above in order to prevent situations where we might have to store millions of 'ValueBytes' objects in memory... thus we have a limit on the no. of records (i.e. ValueBytes objects) we use along with 'io.sort.mb' to segment input for sorting.\n\nThoughts?\n\n", "Here's the patch for block compressed SequenceFiles.\n\nMain attractions:\na) Three different writers for SequenceFiles: Writer, RecordCompressWriter, BlockCompressWriter (with a factory to create the different writers).\nb) The 'raw' apis are significantly different (as per previous discussions here)\nc) Fixed SequenceFile.Sorter to take advantage of new 'raw' apis and some minor tweak/enhancements there.\n\nAs per Owen's suggestion, I've only uploaded the patch... once I get some feedback, I'll go ahead and change the 'status' to 'Patch Available'.", "I have a couple of comments. I had more, but jira ate them.\n\n(It really sucks that jira throws your comment away if your login times out. *sigh*)\n\n1. The Writer class should have the unneeded compression stuff taken out.\n2. The Writer.compressed and blockCompressed fields should be taken out and replaced with methods.\n3. The sync bytes in the block compressed writer should be written before the block rather than after (except for the first block). The goal is to get them between blocks, you don't really want one at the end (or beginning) of the file.\n4. The CompressedBytes class should be private.\n5. The private Writer constructor on line 307 is not used.\n6. The static field VERSION_4 should be renamed to BLOCK_COMPRESS_VERSION and it should be marked final.\n7. I'd rename the \"byte[] version\" to \"syncBlock\" and make a new field \"byte version\" that will contain just the last byte, which is the file version.\n8. We really need to move to the Text class instead of UTF8. This has a couple of changes:\n  A.  in writeFileHeader, the \"new UTF8(...).write(out);\" should be \"Text.writeString(out, ...);\"\n  B. in init, reading the class names strings is the reverse: keyClass = Text.readString(in);\n  C. we have to support the UTF8 string encodings for old file versions, so you'll need to switch behavior based on the version we are reading.", "Owen, appreciate your time... \n\nResponses inline:\n\n> 1. The Writer class should have the unneeded compression stuff taken out.\n> 3. The sync bytes in the block compressed writer should be written before the block rather than after (except for the first block). The goal is to get them between blocks, you don't really want one at the end (or beginning) of the file.\n> 4. The CompressedBytes class should be private.\n> 5. The private Writer constructor on line 307 is not used.\n> 6. The static field VERSION_4 should be renamed to BLOCK_COMPRESS_VERSION and it should be marked final.\n> 8. We really need to move to the Text class instead of UTF8. This has a couple of changes:\n  A. in writeFileHeader, the \"new UTF8(...).write(out);\" should be \"Text.writeString(out, ...);\"\n  B. in init, reading the class names strings is the reverse: keyClass = Text.readString(in);\n  C. we have to support the UTF8 string encodings for old file versions, so you'll need to switch behavior based on the version we are reading.\n\nAll done and incorporated into latest patch.\n\n> 2. The Writer.compressed and blockCompressed fields should be taken out and replaced with methods.\n\nNot clear - let's discuss this.\n\n> 7. I'd rename the \"byte[] version\" to \"syncBlock\" and make a new field \"byte version\" that will contain just the last byte, which is the file version.\n\nI don't agree about renaming it to 'syncBlock' since it isn't a sync block. I don't mind doing the \"byte version\" field, but the advantages aren't very clear.\n\n_*_*_\n\nI've attached a new patch (SequenceFilesII.patch) which incorporates Owen's suggestions and also fixes the fallout of the latest SequenceFile in other parts of MR universe.\n\nAlso, I've a first cut of the SequenceFile Formats' documentation up:\nhttp://wiki.apache.org/lucene-hadoop/SequenceFile\n", "Oops... \n\n> 1. The Writer class should have the unneeded compression stuff taken out. \n\nI've kept this around for this version (deprecated) to ensure existing applications don't break; I plan to get rid of this one hadoop release later.\n", "Please find SequenceFile.final.patch which incorporates all of Owen's feedback...", "I am not convinced we want folks to encourage folks to add new SequenceFile.Writer subclasses.  Thus we should change the (new) protected fields to package private.  This will also simplify the javadoc.  I'd also prefer that these subclasses were not public either, making everyone use the factory.  This would expose far less of the implementation and further simplify the javadoc.  Owen has disputed this earlier.  Owen?\n\nI note that when you updated uses of SequenceFile.Writer you consistently used the factory in favor of explicitly constructing the subclasses.  Thanks!  That makes hiding the subclasses easy.  We can always make them public later, if needed, but, once they're public, it is hard to remove them.  If we think the factory method has too many confusing parameters, then we could use a typesafe enumeration, e.g. something like:\n\nwriter = SequenceFile.createWriter(fs, conf, path, Compress.BLOCK);\n", "+1 on using an enumeration to represent the compression method. It's not necessary but makes code much more readable.", "Doug - here's the latest patch.\n\nIt incorporates all your comments (remove protected, type-safe enum and non-public subclasses). I agree we can make the subclasses 'public' at a later date if need be, hence I've made them package private.", "I think this is nearly ready.\n\nA minor improvement: the typesafe enumeration instances should probably have a toString() method, to facilitate debugging.\n\nRunning the TestSequenceFile unit test caused my 515MB Ubuntu box to swap horribly and it didn't complete.  I grabbed a stack trace and saw:\n\n    [junit]     at java.util.zip.Inflater.init(Native Method)\n    [junit]     at java.util.zip.Inflater.<init>(Inflater.java:75)\n    [junit]     at java.util.zip.Inflater.<init>(Inflater.java:82)\n    [junit]     at org.apache.hadoop.io.SequenceFile$CompressedBytes.<init>(SequenceFile.java:231)\n    [junit]     at org.apache.hadoop.io.SequenceFile$CompressedBytes.<init>(SequenceFile.java:227)\n    [junit]     at org.apache.hadoop.io.SequenceFile$Reader.createValueBytes(SequenceFile.java:1195)\n    [junit]     at org.apache.hadoop.io.SequenceFile$Sorter$SortPass.run(SequenceFile.java:1459)\n    [junit]     at org.apache.hadoop.io.SequenceFile$Sorter.sortPass(SequenceFile.java:1413)\n    [junit]     at org.apache.hadoop.io.SequenceFile$Sorter.sort(SequenceFile.java:1386)\n    [junit]     at org.apache.hadoop.io.SequenceFile$Sorter.sort(SequenceFile.java:1406)\n    [junit]     at org.apache.hadoop.io.TestSequenceFile.sortTest(TestSequenceFile.java:178)\n\nSince sorting should not do any inflating, the Inflater should probably not be created in this case.  So maybe we should lazily initialize this field?\n\nMore generally, before we commit this we should ensure that performance is comparable to what it was before.  Creating a new ValueBytes wrapper per entry processed when sorting looks expensive to me, but this may in fact be insignificant.  If it is significant, then we might replace the ValueBytes API with a compressor API, where the bytes to be compressed are passed explicitly.\n", "Doug - here's another patch incorporating the fix (lazy initialization of the inflater) plus your other suggestions.\n\nI've also attached results of a quick test run of SequenceFile-v3 v/s SequenceFile-v4 (2 runs of 10k and 100k records respectively). Within that framework it seems reasonable to assume that creating one ValueBytes object per-record doesn't show  up blatantly (of course we can further iterate on this); IAC there is a safety net for extreme corner cases (http://issues.apache.org/jira/browse/HADOOP-54#action_12425953).\n\nPS: The existing TestSequenceFile.java in trunk doesn't pass along the 'compress' flag to mergeTest - I ran the tests after fixing it; the patch reflects that too. Just fyi.", "It would also be good to see the number of objects (and bytes) allocated during the test. Is there an easy way to get that?", "Benchmarks should be run w/o the -check option and with the -fast option, no?", "Here's a patch with a minor tweak to the Sorter (to reuse ValueBytes objects across spills).\n\nI've also attached results of another set of benchmark runs (w/o -check and with -fast) - I've also run it against instrumented code to show no. of ValueBytes objects and total bytes allocated for raw values (note: doesn't imply all of them are allocated at the same time; just total of bytes allocated in the CompressedBytes.reset/UncompressedBytes.reset calls over lifetime of each test case).", "A minor quible is that when you are basically implementing an Enum, we should probably use the name \"valueOf(String)\" instead of \"getCompressionType(String)\" to be forward compatible with the java 1.5 signature for that functionality.\n\nI'd also like to see some performance numbers for straight reads and writes of the seq3 and seq4 block compressed files.", "Here's the latest patch incorporating Owen's quibble about the enum, also I have added a -rwonly option to TestSequenceFile.java and attached results of just read/write tests on seq3 and seq4.", "I'm ok with the final patch, but I think that we should just go ahead and use the 1.5 enums. I made the trivial edit to do that and re-rolled the patch.", "Great... thanks Owen!\n\nPS: We should probably send a separate mail to hadoop-dev@ announcing the patch to build.xml to spare everyone a surprise...", "I just committed this.\n\nOwen, when you converted things to use the Java 1.5 enum you also threw out most of the javadoc on that class.  I restored & improved this before I committed."], "derived": {"summary": "SequenceFile will optionally compress individual values. But both compression and performance would be much better if sequences of keys and values are compressed together.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "SequenceFile should compress blocks, not individual entries - SequenceFile will optionally compress individual values. But both compression and performance would be much better if sequences of keys and values are compressed together."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.\n\nOwen, when you converted things to use the Java 1.5 enum you also threw out most of the javadoc on that class.  I restored & improved this before I committed."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-55", "title": "map-reduce job overhead is too high", "status": "Closed", "priority": "Minor", "reporter": "Yoram Arnon", "assignee": "Sameer Paranjpye", "labels": [], "created": "2006-02-23T08:14:20.000+0000", "updated": "2009-07-08T16:51:38.000+0000", "description": "map-reduce job overhead is too high. Running a null map-reduce job, with no input, no intermediate data and no output, on a several-hundred node cluster, takes several minutes, rather than several seconds.\nEnhanced distribution, launching and monitoring mechanisms should reduce the overhead to levels that match human online patience for short jobs.", "comments": ["\n   [[ Old comment, sent by email on Wed, 31 May 2006 10:02:17 -0700 ]]\n\nThis may actually turn out to be high-ish prioority \n\n", "I think the performance gains over the last 1.5 years have mostly fixed this. There is always more to do, but..."], "derived": {"summary": "map-reduce job overhead is too high. Running a null map-reduce job, with no input, no intermediate data and no output, on a several-hundred node cluster, takes several minutes, rather than several seconds.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "map-reduce job overhead is too high - map-reduce job overhead is too high. Running a null map-reduce job, with no input, no intermediate data and no output, on a several-hundred node cluster, takes several minutes, rather than several seconds."}, {"q": "What updates or decisions were made in the discussion?", "a": "I think the performance gains over the last 1.5 years have mostly fixed this. There is always more to do, but..."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-56", "title": "hadoop nameserver does not recognise ndfs nameserver image", "status": "Closed", "priority": "Critical", "reporter": "Yoram Arnon", "assignee": "Sameer Paranjpye", "labels": [], "created": "2006-02-23T11:13:31.000+0000", "updated": "2009-07-08T16:41:47.000+0000", "description": "hadoop nameserver does not recognise ndfs image\nThus, upgrading from ndfs to hadoop dfs results in total data loss.\nThe upgrade should be seemless, with the new server recognising all previous version that are not end-of-life'd.", "comments": ["Works for me, although the paths need to renamed a bit.\n\nIf I unpack the attached, and move the directory named 'ndfs' to be /tmp/hadoop/dfs, then I can find the file /user/dcutting/foo/test.\n", "As I understand the failure, we brought up a new nameserver on an old cluster (so no file information).  The nameserver saw all the blocks of the old FS and decided they must all belong to deleted files.  It then proceeded to remove them!!\n\nThis is bad.  We should have enough interlocks in place that in this situation the name server haults and asks for help.  This is not only a risk during upgrades!  Other operator failures could cause huge data lose.", "I see two cases:\n\n1. A namenode has been misconfigured, and comes up knowing about no files.  When datanodes report their blocks, the namenode tells them those blocks belong to no files, so the datanode removes them.  This is bad.\n\n2. A datanode has been moved to a new cluster, or is rebooted after a long downtime.  It comes up and tells the namenode about the blocks that it has.  The namenode tells it that those blocks belong to no files, so the datanode removes them.  This is good.\n\nSo perhaps the thing to fix is that the namenode should not automatically create an empty filesystem if it is misconfigured.  Perhaps instead we should have a separate \"format\" command that creates an empty filesystem, and one must run this the first time before starting dfs.  How does that sound?  It makes the out-of-box experience a little more awkward, but I think it might be warranted in this case.", "I'd perhaps request one more feature: tag each block somehow with a GUID related to the DFS filesystem it is associated with.  So, if we're adding a filesystem \"format\" can assign the GUID to the newly-created image. And a datanode can find out what the GUID is of the nameserver it's working with before it enumerates blocks, merely ignoring any blocks that have the wrong GUID.\n\nThis solves the problem of multiple folks writing with different DFS installs to the same drive, as well as the misconfigured namenode problem, above.", "A format command sounds useful.\n\nGUID and other extensions to block meta data to allow more error checking also sound like a good idea, but I feel like we need to thrash out the use cases  Not sure I follow the intended behavior in bryan's example.  Maybe we should fork that discussion into an enhancement proposal?\n", "I agree with Eric: a filesystem GUID could be a useful long-term feature, and may help solve this problem, but we need a simpler solution short-term.  I think adding a format command would address this issue, and also still be useful if we later add a filesystem guid.", "Format is definitely useful.\nBut I don't think it is reasonable to add GUID or any user information anywhere except\nfile description on the namenode. Otherwise it will be hard to support this information.\n- The problem of different DFS installs writing to the same drive can be solved by introducing\nDFS cluster name, and including it as a final component of dfs.name.dir and dfs.data.dir\n- DFS probably needs an application version number, which should be compared when the\nfile system starts uploading information from the check pointed image.\nAnd the conversion method, which if specified would provide conversion between the versions,\nand if not the system should refuse loading the image. Sooner or later conversion of data will\nbecome an issue, it would be good to have the mechanism in place by that time.\n", "The advantage of the GUID over enhancing the configuration is that it prevents anyone from \"screwing up\" - the defaults will never cause data loss. Without it, even with \"format\", it's possible for someone to bring up a set of datanodes on a shared set of machines and start clobbering data. With a GUID, this is no longer an accident that can happen. The original issue that spawned this discussion was with this kind of situation - settings in a config file lead to data \"cleanup\" that wasn't desired. With a GUID, this risk goes away. Without it, especially with default data directories like /tmp/, it's very easy for two different people to clobber each others data by running datanode instances on the same machine(s).\n\nI understand there's a big push back against complexity. But hadoop is a component likely to be used in a lot of situations, by users who might or might not have complete control of their cluster. The DFS layer is supposed to provide data reliability, so it seems appropriate to put in guards against bad end-user behavior/misconfigurations, if it's not going to be a big cost in performance (it shouldn't - what, an extra string during the initial chat between namenode/datanode?), or storage (it shouldn't add more than a few extra bytes to the filename of each block - or a whole GUID subdir, rather than/in addition to the suggested named paths).\n\nAn much weaker alternative to prevent only the one worst-case I'm highlighting, would be for a datanode to shutdown with an error if *none* of the blocks on in a datanode's storage directory are from live files in the DFS. I think that is a far less powerful fix, with the only benefit being that it doesn't require changing the behavior of virtually any of the existing code.", "Sorry Bryan, I misinterpreted GUID as a group/user id.\nSo, yes a global unique id is for identifying particular data instances of the file system.\nIt can be generated by \"format\". And there shouldn't be any substantial overhead to support this.\n\nWhat I am trying to propose is to use the version number of the file system (as a software application),\nhard coded (rather than configured) and accessible via the getVersion() method, as a part of GUID.\nThe version alone would have been sufficient to avoid the original problem\nwith data loss, but GUID provides even better consistency.\nAn additional advantage of versioning is that when the internal data layout for the name\nor data nodes is changed, one can provide conversion methods from one version to another.\n", "Sorry -  yes, I meant a globally-unique identifier, not a group/user id.\n\nSticking a GUID+version into the namenode metadata sounds helpful. I don't really see a reason to stick the version into the block filename, though. Especially if all \"upgrade\" related commands are going to be issued by the namenode.", "Duplicated by HADOOP-19"], "derived": {"summary": "hadoop nameserver does not recognise ndfs image\nThus, upgrading from ndfs to hadoop dfs results in total data loss. The upgrade should be seemless, with the new server recognising all previous version that are not end-of-life'd.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "hadoop nameserver does not recognise ndfs nameserver image - hadoop nameserver does not recognise ndfs image\nThus, upgrading from ndfs to hadoop dfs results in total data loss. The upgrade should be seemless, with the new server recognising all previous version that are not end-of-life'd."}, {"q": "What updates or decisions were made in the discussion?", "a": "Duplicated by HADOOP-19"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-57", "title": "hadoop dfs -ls / does not show root of file system", "status": "Closed", "priority": "Minor", "reporter": "Yoram Arnon", "assignee": null, "labels": [], "created": "2006-02-23T11:17:35.000+0000", "updated": "2009-07-08T16:41:47.000+0000", "description": "hadoop dfs -ls / does not show root of file system - it shows the user's home directory.\nIt's thus impossible to learn the contents of the root file system via the shell.", "comments": ["Here is a small patch that enables seeing the root file system.  The getDFSPath() function in DistributedFileSystem returns empty string in case of an ls on /. The patch below modifies it to return /.\n\n264d263\n<       path.append(DFSFile.DFS_FILE_SEPARATOR);\n266a266\n>         path.append(DFSFile.DFS_FILE_SEPARATOR);\n268,270d267\n<         if(i!=0){\n<            path.append(DFSFile.DFS_FILE_SEPARATOR);\n<       }\n", "here is a better fix:\n\n268a269,271\n>       if (path.toString().equals(\"\")){\n>           return DFSFile.DFS_FILE_SEPARATOR;\n>       }\n\nThis one solves the problem and defaults to /user/username in case of non-absolute path.", "I forgot to mention the file:\n\nthe file with changes is DistributedFileSystem.java.\n\n", "Fixed.  Thanks!"], "derived": {"summary": "hadoop dfs -ls / does not show root of file system - it shows the user's home directory. It's thus impossible to learn the contents of the root file system via the shell.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "hadoop dfs -ls / does not show root of file system - hadoop dfs -ls / does not show root of file system - it shows the user's home directory. It's thus impossible to learn the contents of the root file system via the shell."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed.  Thanks!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-58", "title": "Hadoop requires configuration of hadoop-site.xml or won't run", "status": "Closed", "priority": "Minor", "reporter": "Michael Stack", "assignee": null, "labels": [], "created": "2006-02-24T03:43:08.000+0000", "updated": "2006-08-03T17:46:29.000+0000", "description": "On a new install, I would expect '${HADOOP_HOME}/bin/start-all.sh\" to bring up a basic instance, one that is using local filesystem (Or if not, then uses a DFS homed in localhost:/tmp) and that has all four daemons running on localhost.  Currently this is not the case.  Hadoop complains 'java.lang.RuntimeException: Not a host:port pair: local'.  It doesn't like the 'local' default value for mapred.job.tracker and fs.default.name properties.\n\nRevision: 379930\n\n", "comments": ["Suggested patch that  makes localhost and DFS the default.  Local filesystem might be better as default but looks like more work needed making 'local' work again.", "I don't think we want things to use DFS and TaskTracker/JobTracker by default, since this slows things down and uses more resources than needed when running on a single node.\n\nThe javadoc provides a recommended configuration for \"pseudo-distributed\" use:\n\nhttp://lucene.apache.org/hadoop/docs/api/overview-summary.html\n\nWith that in place, bin/start-all.sh works fine, no?", "Feels like the 'pseudo-distributed' additions to hadoop-site.xml should be default config. in hadoop-default.xml but then I suppose that would preclude 'standalone operation'.\n\nThanks for pointer to the doc. It clarifies how things are meant to work.\n\nPlease close this issue.", "I'm closing this.  The default configuration supports standalone, in-process operation, and there's a well-documented way to achieve standalone multi-process operation (aka \"pseudo distributed')."], "derived": {"summary": "On a new install, I would expect '${HADOOP_HOME}/bin/start-all. sh\" to bring up a basic instance, one that is using local filesystem (Or if not, then uses a DFS homed in localhost:/tmp) and that has all four daemons running on localhost.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Hadoop requires configuration of hadoop-site.xml or won't run - On a new install, I would expect '${HADOOP_HOME}/bin/start-all. sh\" to bring up a basic instance, one that is using local filesystem (Or if not, then uses a DFS homed in localhost:/tmp) and that has all four daemons running on localhost."}, {"q": "What updates or decisions were made in the discussion?", "a": "I'm closing this.  The default configuration supports standalone, in-process operation, and there's a well-documented way to achieve standalone multi-process operation (aka \"pseudo distributed')."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-59", "title": "support generic command-line options", "status": "Closed", "priority": "Minor", "reporter": "Doug Cutting", "assignee": "Hairong Kuang", "labels": [], "created": "2006-02-24T07:30:38.000+0000", "updated": "2007-01-03T00:36:32.000+0000", "description": "Hadoop commands should all support some common options.  For example, it should be possible to specify the namenode, datanode, and, for that matter, any config option, in a generic way.\n\nThis could be implemented with code like:\n\npublic interface Tool extends Configurable {\n  void run(String[] args) throws Exception;\n}\n\npublic class ToolBase implements Tool extends Configured {\n  public final void main(String[] args) throws Exception {\n    Configuration conf = new Configuration();\n    ... parse config options from args into conf ...\n    this.configure(conf);\n    this.run();\n  }\n}\n\npublic MyTool extends ExcecutableBase {\n  public static void main(String[] args) throws Exception {\n    new MyTool().main(args);\n  }\n}\n\nThe general command line syntax could be:\n\nbin/hadoop [generalOptions] command [commandOptions]\n\nWhere generalOptions are things that ToolBase handles, and only the commandOptions are passed to Tool.run().  The most important generalOption would be '-D', which would define name/value pairs that are set in the configuration.  This alone would permit folks to set the namenode, datanode, etc.", "comments": ["I'd like to see this combined with HADOOP-59.\n\nWe should duplicate as little code as possible.  Thus the try/catch and logging code should all be in a base class, and each main should be a one-line call to this base-class method.  And, while we're at it, we can add generic command line options!\n\nSo I think we need a patch for HADOOP-59 that also includes logging and return codes, then a new version of this patch that builds on that, making Nutch tools extend a ToolBase class, implementing a run(String[]) method and a one-line main().  Does that sound like a good plan?", "Michel,\n\nI think that -D should define Hadoop configuration properties, not JVM system properties.  And I don't think Hadoop configurations should by default include all of the JVM's system properties.  Finally, please include good user-level javadoc on all public & protected items, attach Apache's license at the top of each file, try to bundle things into a single patch file, etc.  These kinds of things make it much easier for me to commit a patch.  Ideally all that I need to do is read the patch to see that it looks reasonable, apply it with 'patch -p 0 < patchFile', run unit tests and commit.  Including new unit tests is also a good idea.  If I need to clean up  javadoc, licenses, formatting, etc. before I can commit then I will be less inclined to process a contribution promptly.\n\nDoug\n", "This looks like it has been addressed by the refactoring done in HADOOP-59. Does this need to stay open?", "I am thinking to design the ToolBase as follows:\n\npublic class ToolBase implements Tool extends Configured { \n  public final void main(String[] args) throws Exception { \n    Configuration conf = new Configuration(); \n    String [] commandOptions = parseGeneralOptions( conf, args );\n    this.configure( conf ); \n    this.run( commandOptions ); \n  } \n} \n\nParseGeneralOptions parses the arguments, looks for general options, and sets configuration accordingly. It returns command-specific options.\n\nI plan to support 4 general options:\n\n-fs <local | namenode:port>             specify file system \n-jt <local | jobtracker:port>                 specify job tracker\n-config <configuration file>                specify a file that contains application-specific configuration\n-Dname=value                                     set a porperty to be value", "This patch provides a general framework to support general command options. Tool provides an interface and ToolBase processes general command options.\n\nGeneral command options supported are\n-conf <configuration file>      specify an application configuration file\n-D <property=value>               use value for given property\n-fs <local|namenode:port>    specify a name node\n-jt <local|jobtracker:port>       specify a job tracker\n\nThe syntax to run bin/hadoop becomes\nbin/hadoop command [general options] [command options and arguments]\n\nThe patch also includes changes to four tools, DFSShell, DFSck, JobClient, and CopyFiles to inherit from ToolBase. \n\nExamples using general options are\nbin/hadoop dfs -fs darwin:8020 -ls /data\nbin/hadoop dfs -Dfs.default.name=darwin:8020 -ls /data\nbin/hadoop dfs -conf hadoop-site.xml -ls /data\nbin/hadoop job -jt darwin:50020 -status job_0001\nbin/hadoop distcp -Dfs.default.name=darwin:8020 -Dmapred.job.tracker=darwin:50020 srcurl dsturl\nbin/hadoop fsck -fs dawin:8020 /data\n\nLast, the patch includes a junit test for general options.", "In the patch submitted above, the processing of general options are implemented using commons CLI with a fix. The fix has not committed yet. So I attached the jar here.", "Any reason we need to segregate general options from command options?  \nThat seems a little confusing.\n", "The analogy is with the cvs and svn command line programs, which support both general options, determining which servers to talk to, etc., then command-specific options.  The documentation then can be organized similarly, describing general options, then each command and its options.  The set of commands is extensible (any class with a main() can be specified to bin/hadoop), and each command need only provide documentation for its options.", "I just committed this.  Thanks, Hairong!", "I'm sorry that I didn't pay paid attention to this issue earlier. \n\nI think that it is confusing having the cli options segregated. For example, I have a patch for distcp (aka CopyFiles) that adds a \"-i\" option to ignore read errors. With this setup, the user needs to specify the -i *last*.  It implies that the user need to remember what is a generic option versus what is a command option. (Ok, raise your hand if you can tell me the difference between \"cvs -d foo co bar\" and \"cvs co -d foo bar\". The sad thing is that I do know and I'm sure a few of you do too. But it certainly does confuse non-experts.)\n\nFurthermore, each application needs to handle --help themselves (and as a side effect adding a new generic option means updating the usage string in each application).\n\nI haven't used the commons cli library, but I would have prefered something less inhertiance-based, more like::\n\nParser cliParser = new GenericOptionsParser();\ncliParser.addOption(\"i\", false, \"ignore read errors\");\ncliParser.parse(args, conf);\nboolean ignoreErrors = cliParser.hasOption(\"i\");\n", "Owen, what you say makes good sense.  If you feel strongly about it, please submit a new bug.", "\nI know this is very old bug.. \n\nOwen Wrote:\n\n> I haven't used the commons cli library, but I would have prefered something less inhertiance-based, more like::\n\nI think it does not need to be inheritence based. All we need is to make static member parseGenericOptions() public. This does not address other issues oven brings up.  Then it can be used like:\n\n   String[] commandArgs = ToolBase.processGenericOptions(conf, argv); // In that case it need not be called ToolBase :)\n   // rest as before.\n\n> Parser cliParser = new GenericOptionsParser();\n> cliParser.addOption(\"i\", false, \"ignore read errors\");\n> cliParser.parse(args, conf);\n> boolean ignoreErrors = cliParser.hasOption(\"i\");\n\nAbove can also be supported without changing much I guess.. with another static function.\n", "\n> This does not address other issues oven brings up.\n\nSorry Owen!"], "derived": {"summary": "Hadoop commands should all support some common options. For example, it should be possible to specify the namenode, datanode, and, for that matter, any config option, in a generic way.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "support generic command-line options - Hadoop commands should all support some common options. For example, it should be possible to specify the namenode, datanode, and, for that matter, any config option, in a generic way."}, {"q": "What updates or decisions were made in the discussion?", "a": "> This does not address other issues oven brings up.\n\nSorry Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-60", "title": "Specification of alternate conf. directory", "status": "Closed", "priority": "Minor", "reporter": "Michael Stack", "assignee": null, "labels": [], "created": "2006-02-28T01:56:17.000+0000", "updated": "2006-08-03T17:46:29.000+0000", "description": "Currently, hadoop configuration must be done by making edits and addition to ${HADOOP_HOME}/conf.  Allowing specification of an alternate configuration directory will allow keeping configuration and binary distinct.  Benefits include: Binary can be made read-only; or binary is blanket-updateable with configuration undisturbed.", "comments": ["HADOOP_CONF_DIR patch.  Passes HADOOP_CONF_DIR setting to subscripts and forwards the environment variable over the ssh chasm (if pertinent config. made to remote ssh_config).", "I should have committed this yesterday, before I made a bunch of other changes to these scripts.  Now it no longer applies!\n\nMichael, can you please regenerate this against the current sources?  Thanks!", "Looking at your patch, I would change the setting line from:\nHADOOP_CONF_DIR=${HADOOP_CONF_DIR:=$HADOOP_HOME/conf}\nto:\nHADOOP_CONF_DIR=${HADOOP_CONF_DIR:-$HADOOP_HOME/conf}\nbecause you don't need the redundant assignment.\n\nYou also need to surround all of the file/directory names with quotes so that you support spaces in directory names.\n\n", "Version of patch that will go against 382230.\n\nThanks for the feedback Owen.  I integrated your comments into this version of the patch.", "Version 3 is version 2 plus edit of hadoop-env.template modifying defaults for HADOOP_SSH_OPTS.", "I just committed this.  I had to add a few more quotes to get things to work correctly with spaces in directory names, removed a spurious variable setting on the 'nohup' line of hadoop-daemon.sh, and updated a comment.\n", "Fixed!"], "derived": {"summary": "Currently, hadoop configuration must be done by making edits and addition to ${HADOOP_HOME}/conf. Allowing specification of an alternate configuration directory will allow keeping configuration and binary distinct.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Specification of alternate conf. directory - Currently, hadoop configuration must be done by making edits and addition to ${HADOOP_HOME}/conf. Allowing specification of an alternate configuration directory will allow keeping configuration and binary distinct."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  I had to add a few more quotes to get things to work correctly with spaces in directory names, removed a spurious variable setting on the 'nohup' line of hadoop-daemon.sh, and updated a comment."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-61", "title": "Bashless Hadoop Start Script", "status": "Resolved", "priority": "Minor", "reporter": "Jeff Ritchie", "assignee": null, "labels": [], "created": "2006-03-01T03:26:20.000+0000", "updated": "2011-08-11T18:53:56.000+0000", "description": "This is for the people who want to try to get hadoop running without cygwin.  The code needs some work and could be improved.\nAttachment to follow.", "comments": ["hadoop.java is in the 'default' package.\nhadoop.xml is the properties file for specifying 'enviroment varibles'\n\nI've only tested tasktracker and dfs but the rest should work (except maybe the jar).", "But Cygwin isn't just for the scripts, since 06' it is also required within the code for a few commands.", "Yes Harsh, you are right!.\nI remember , Cygwin is required for du, df ....commands", "Uma - yep, those among other things for proper functionality.\n\nBut FWIW, all daemons can be launched via their jars as well. But wrappers like this or what HADOOP-1525 has to offer probably does make sense going forward. Leaving this (seemingly) stale ticket open for some adoption in future.", "Out of date"], "derived": {"summary": "This is for the people who want to try to get hadoop running without cygwin. The code needs some work and could be improved.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Bashless Hadoop Start Script - This is for the people who want to try to get hadoop running without cygwin. The code needs some work and could be improved."}, {"q": "What updates or decisions were made in the discussion?", "a": "Out of date"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-62", "title": "can't get environment variables from HADOOP_CONF_DIR", "status": "Closed", "priority": "Minor", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-03-02T08:49:53.000+0000", "updated": "2006-08-03T17:46:29.000+0000", "description": "The bin/hadoop script doesn't use the HADOOP_CONF_DIR variable to find hadoop-env.sh.", "comments": ["This patch fixes the problem by setting the HADOOP_CONF_DIR earlier in the script.", "Shouldn't we also make similar changes in the other scripts that source hadoop-env.sh?\n\nAlso, wouldn't it be cleaner to assign & default HADOOP_CONF_DIR in a separate line, rather than as a side-effect the first time its used?\n\nFinally, why kill the comment about CLASSPATH?\n", "HADOOP-60 does the above Owen patch and addresses the first Doug comment.", "> Shouldn't we also make similar changes in the other scripts that source hadoop-env.sh?\n\nof course. I just missed the fact that they had similar mistakes in them.\n\n> Also, wouldn't it be cleaner to assign & default HADOOP_CONF_DIR in a separate line, rather than as \n> a side-effect the first time its used?\n\nI was just trying to copy your style, since it was your script.\n\n> Finally, why kill the comment about CLASSPATH?\n\nAfter the conditional assignment was removed, it seemed like a \"add one to x\" type comment.\nA better comment would be:\n\n# The configuration directory needs to be first in the classpath so that the config files are found by the \n# classloader.\n", "It's not my script, it's *our* script!  Which do you think is the better style?  I don't think I wrote that line anyway..."], "derived": {"summary": "The bin/hadoop script doesn't use the HADOOP_CONF_DIR variable to find hadoop-env. sh.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "can't get environment variables from HADOOP_CONF_DIR - The bin/hadoop script doesn't use the HADOOP_CONF_DIR variable to find hadoop-env. sh."}, {"q": "What updates or decisions were made in the discussion?", "a": "It's not my script, it's *our* script!  Which do you think is the better style?  I don't think I wrote that line anyway..."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-63", "title": "problem with webapp when start a jobtracker", "status": "Closed", "priority": "Minor", "reporter": "Hairong Kuang", "assignee": "Hairong Kuang", "labels": [], "created": "2006-03-03T10:38:35.000+0000", "updated": "2009-07-08T16:51:40.000+0000", "description": "there exist two issues with starting up webapp\n\n1. webapp is not be able to be loaded from a jar file.\n2. web.xml can not be parsed properly using java 1.4", "comments": ["Is this still an issue? Can this be closed?", "I believe the issue is still there. But hadoop startup script gets around the problem. Let's clost it.", "As stated, Hadoop startup scripts work around this issue."], "derived": {"summary": "there exist two issues with starting up webapp\n\n1. webapp is not be able to be loaded from a jar file.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "problem with webapp when start a jobtracker - there exist two issues with starting up webapp\n\n1. webapp is not be able to be loaded from a jar file."}, {"q": "What updates or decisions were made in the discussion?", "a": "As stated, Hadoop startup scripts work around this issue."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-64", "title": "DataNode should be capable of managing multiple volumes", "status": "Closed", "priority": "Minor", "reporter": "Sameer Paranjpye", "assignee": "Milind Barve", "labels": [], "created": "2006-03-07T07:05:03.000+0000", "updated": "2009-07-08T16:41:49.000+0000", "description": "The dfs Datanode can only store data on a single filesystem volume. When a node runs its disks JBOD this means running a Datanode per disk on the machine. While the scheme works reasonably well on small clusters, on larger installations (several 100 nodes) it implies a very large number of Datanodes with associated management overhead in the Namenode.\n\nThe Datanod should be enhanced to be able to handle multiple volumes on a single machine.", "comments": ["I am not convinced, w/o benchmarks, that this is required.  Multiple datanodes in the same JVM (as currently implemented) share a single TCP connection to the namenode.  However each currently sends separate heartbeats to the namenode.  Thus the primary impact of the proposed change would be that these heartbeats could be combined into a single RPC.  The processing on the server would be the same, only spread over fewer RPC calls.  So this change is primarily warranted if heartbeat RPC overhead dominates namenode performance.  Even if that's proven to be the case, then we can achieve a similar effect much more simply by increasing the heartbeat interval.\n", "Heartbeat overhead aside, a second motivator for this change is the simplification of block placement logic. The namenode wouldn't have to track the datanode-machine mapping to ensure that replicas of a block don't end up on the same box. I don't know if this is currently done, but if not it could be a problem. Also, if block placement decisions are to be made based on some measure of load (number of blocks in being read/written, CPU usage etc.) the load needs to be tracked at a machine level. It's simpler to note have the namenode compute the aggregate load per machine.", "Noticed there's an unanswered question in the comment above.\nHow well does hadoop handle multiple volumes?\nSince it starts one datanode per volume, is there a risk (even though it's a small risk) that a file with replication level 3 might end up with one block on one node, if said node have >=3 volumes?", "The block placement algorithm does check that copies are not placed on nodes on the same host.", "I have profiled the namenode, and most action in namenode happens as a response to heartbeat and blockreport messages. Therefore I think it is important to enable the datanode to handle multiple volumes. this also relates to hadoop-50, which needs handling multiple directories. The scheme I have in mind is for datanode to load-balance among volumes (that corrspond to multiple disks) and then within a volume, block-placement will be done within multiple directories according to block-id. I am currently preparing a proposal on this issue.", "Proposal:\n\nIn the configuration (e.g. hadoop-site.xml), site-admin can specify a comma separated list of volumes as a value corresponding to key \"dfs.data.dir\". These volumes are assumed to be mounted on different disks. Thus the total disk capacity for the datanode is assumed to be a sum of disk capacities of these volumes, in addition, taking into account the /dev/sda* or /dev/hda* mapping of these volumes (i.e. not counting the same /dev/* twice.)\n\nNew blocks are created round-robin in these volumes. The policy for block-allocation is controlled by a separable piece of code, so that different policies can be substituted at runtime later. Mapping of datablocks to volume-id is kept in memory of datanode. When the datanode comes up again, it discovers this mapping by reading specified volumes. Later, when datanode is also periodically checkpointed, this mapping is stored in the checkpoint as well.\n\nEach volume is further automatically split into multiple subdirectories (the number of these directories is configurable, and should be a power of 2, so that the last x bits of a block-id is used to determine which subdirectory the block is stored in. this is the scheme used in Mike's patch for hadoop-50.\n\nIf a datanode is re-configured with different number (or locations) of volumes for dfs.data.dir, the blocks stored in earlier locations are considered by the datanode to be lost (when in future, the datanode is checkpointed, it will try to recover those \"lost\" blocks). If one of the volumes is read-only, it will currently be considered to be dead only with-respect-to that volume. i.e. it will still continue to store blocks in read-write volumes, but blocks in the read-only volumes will be considered lost, since they cannot be deleted.)\n\nPlease comment on this proposal asap, so that I can go ahead with implementation.\n", "Why do datanodes need to checkpoint? What's the value of storing out the mapping, vs. re-enumerating them at startup time? The namenode doesn't keep track of what nodes have which blocks, why should a storage node keep track any more rigorously within its own state? I'd argue that all of that complexity is needless - the cost of maintaining a consistent state is way too high for little benefit.\n\nPlease make it very easy to change the block-allocation code. The default behaviors of the current code have been causing troubles on my very heterogenous cluster for a very long time - uniform distribution only really actually makes sense if the same amount of space is available on each drive. For all other cases, doing this leads immediately to unnecessary failures.\n\nI'm not sure about the \"blocks considered lost on read-only volumes\" bit, but, if that implies that the blocks become unavailable, then I think the approach is too heavy-handed. Those blocks might be the only copies, and ignoring them means that cluster might not be able to find a live copy of a block anywhere else. Please clarify what a \"lost\" block is.", "Can we map effectively map volumes to devices on Windows? Will 'df' under cygwin produce a comprehensible mapping of paths to devices? Maybe this should be left out of the implementation?\n\nCode for monitoring disk capacity on the datanode will need to be updated to run 'df' on all volumes considered.  Round robin placement needs to account for differences in capacity on the various volumes.\n\nHow does this interact with Konstantin's storage id implementation? We will now need to have 1 storage-id across multiple volumes.\n\nDo we need to use the last x-bits of a block to map it to a directory? Maybe we should use a simple round robin scheme here as well. The amount of state is small enough to keep in a hastable, no?\n\nDo we ever need to checkpoint datanodes? Seems like that is a separable discussion. In any case, it seems like the less state we keep in side files the better it is.\n\nWe should include a mechanism to make read-only volumes visible on the namenode, as part of the health/status page, so that admins can be alerted in a timely manner.", "About check-pointing datanodes. I agree that it is a needless complexity. I was confused about this as well. But as Konstantin pointed out to me, the datanode checkpointing proposal is NOT checkpointing datanodes' state, but checkpointing datanodes' blockreport in the namenode checkpoint. Thanks konstantin.\n\nAs the proposal (and the implementation) currently stands, if dfs.data.dir is read-only, the datanode reports to be dead, since block-delete etc operations cannot be carried out on it. The namenode treats that datanode as dead, and tries to re-replicate its blocks on other data nodes. The same behavior will continue, except the datanode will not report itself to be dead if at least one volume specified in the dfs.data.dir list is read-write. However, it will not report blocks contained in read-only volumes.\n\nStorage-ID continues to be one per datanode. Putting blocks in different volumes is datanode-internal.\n\nThe DF.java contains code to detect mount. This will be considered to be the differentiation between different disks. Even if it is not right, it does not preclude correct operation of datanode, only performance is affected. Performance will be maximized if all volumes specified in dfs.data.dir are located on different local disks.\n\nMaking read-only mounts visible on namenode is an orthogonal issue. My proposal specifies a backward-compatible way of dealing with it.\n\nUsing the last x bits to map a block on local directory will minimize datanode's state as well as keep the directory size minimal (since block-ids are random). Consider it an implicit hashtable on disk.", "dfs.data.dir is currently used to specify the location of temporary files written by dfs client (data is written to disk, then an entire dfs block is streamed to the datanodes). Rather than trying to support a multiple-volume behaviour there too, let's separate the client config from the datanode config, using 'client.tempdata.dir'. Try to make the change backwards compatible.\n\nread-only drives are hard to maintain except by totally ignoring them, since data can not be deleted from them. If a file is deleted, then a blockid is reclaimed for another file, bad things might happen if that blockid is served by some read-only volume. If it's the last copy of a block, *and* the volume is read-only and on its way to be dead, then that block is unfortunately lost.\n\nround robin is a bit harsh as an allocation scheme. allocation proportional to free space would work better IMO.", "It sounds like its a different issue, but, I'm still worried about the \"treat read-only as dead\" option.\n\nI regularly see old drives throw out enough IDE errors to get remounted read-only. Often times, a reboot will bring the drive back online just fine - and bring back the blocks that weren't deleted from it. If that block number was reused in the meantime, we'll have a problem anyway, regardless of whether we treat that block as readable once the initial read-only condition is encountered. Meanwhile, we've possibly missed an opportunity to save a block from early death, since a copy of it is still actually available.\n\nI'll probably open up another issue around this later one, once the current one is closed, just to clarify. In the meantime, is there a realistic issue with block numbers being reused, such that an old block coming online would pervert things? Should a datanode maybe be periodically requesting the CRCs for its blocks, and checking to see if they still match? This generally falls into the space of \"a good idea\" anyway, since, barring reading a block, there's no way to tell if its disk has gone bad.", " = I believe there was a misunderstanding on the datanode checkpointing issue.\nHADOOP-306 proposes to checkpoint only the list of datanodes, effectively DatanodeInfo.\nIt was not meant to store the datanode block reports.\nThe block map is not and should not be checkpointed.\n\n= DF on Windows will return the drive letter, which can be used to distinguish disks.\nIt will work only for local disks though. Mounted (mapped network) drives on Windows won't work.\n\n= I agree storageID should be the same per node. It will need to be stored separately on each drive.\nOtherwise, if only one drive stores the id and gets corrupted we will not be able to restore the\nstorage id for other drives. Also, the storage files on each drive should be locked when the datanode\nstarts to prevent from running multiple data nodes with the same blocks.\n\n= It is a good idea that the number of directories is a power of 2.\nBut I do not support the idea to reserve any number of bits of block-id to determine block locations, for 2 reasons.\na) Block replicas can have different locations on different data nodes.\nb) The block id is issued by the namenode, and it is not good if the namenode will need to\nknow about a datanode storage setup.\nInstead, we can partition bit representation of the block id into a number of parts consistent\nwith the number of directories and e.g. XOR them. The result will represent the directory name.\nI think this will be random enough.\n\n= I don't think the datanode can be even start on a read-only disk.\nThe storage file won't open.", "for more discussion on the read-only disk issue, see the discussion in HADOOP-163", "Thanks for your inputs Yoram, Konstantin, Bryan, Sameer.\n\nHere is my modified proposal:\n\n1. The config parameter dfs.data.dir could have a list of directories separated by commas.\n2. Another config parameter (client.buffer.dir) will contain comma-separated list of directories for buffering blocks until they are sent to datanode. DFS client will manage the in-memory map of blocks to these directories.\n3. Datanode will maintain a map in memory of blockid's mapped to storage locations.\n4. Datanode will choose appropriate location to write a block based on a separate block-to-volume placement strategy. Information about volumes will be made available to this strategy with DF.\n5. Datanode will try to report correct available diskspace by appropriately taking into account the space reported by DF on each volume. If the mount point is same for more than one volume, then the available disk space will not be counted twice.\n6. Storage-ID will be unique per data node, and will be stored in each of the volumes at top levels.\n7. Each volume will further be separated into a shallow directory hierarchy, with maximum of N blocks per directory. This block to directory mapping will also be maintained in a hashtable by datanode. as a directory fills up, new directory will be created as a sibling, upto a maximum of N siblings. Then second level of directories will start. The parameter N can be specified as a config variable \"dfs.data.numdir\".\n8. Only if all the volumes specified in dfs.data.dir are read-only, the datanode will shutdown. Otherwise, it will log the readonly directories, and treat them as if they were never specified in dfs.data.dir list. This behavior is consistent with current state of implementation.\n\nIf there are any other issues to think about, please comment.\n", "We could consider the datanode doing some kind of best effort if one volume is read-only, by copying that volume's data locally to the other volumes before shutting down the bad volume. Best effort only - if there isn't enough space for everything then just copy what you can and move on. It may decrease the number of blocks that need to be replicated over the network.", "This proposition looks good to me.\nThe only thing that seems excessive is the dynamic data structures for maintaining\nblockid-to-directory mapping.\nThe alternative is to do a static mapping based on blockids and the number of directories.\nSuppose that the maximal number of entries per directory is N. We should define a function\n      dirName( blockId, N, dirLevel )\nwhich returns a local directory name for each level of the directory tree.\nSo the datanode needs to store  only the current hight  of the directory tree H.\nThen for a given  blockId, its path is determined by\n      / dirName(blockId,N,0) / dirName(blockId,N,1) / ... / dirName(blockId,N,H)\nAnd when the datanode needs to add a new directory level it will not need\nto rename anything in the existing directory tree.\nI see a disadvantage of this approach, that the directories should be\nre-structured if the maximal number of entries per directory is changed.\nBut the same is applicable for the dynamic approach, at least when N is decreased.\nWe might consider hardcoding N rather than having it configurable.\n", "Since we only expect to have around 10k blocks per node, storing the table in memory should not be a problem.  With even 100k blockids per nodem, at 100bytes of RAM per blockid, a datanode would only require 10MB.  So optimizing this seems premature.", "Patch attached.", "I just committed this.  Thanks, Milind!", "I forgot to name the bug in the commit message.  So, for the record, the commit was:\n  http://svn.apache.org/viewvc?view=rev&revision=440508"], "derived": {"summary": "The dfs Datanode can only store data on a single filesystem volume. When a node runs its disks JBOD this means running a Datanode per disk on the machine.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "DataNode should be capable of managing multiple volumes - The dfs Datanode can only store data on a single filesystem volume. When a node runs its disks JBOD this means running a Datanode per disk on the machine."}, {"q": "What updates or decisions were made in the discussion?", "a": "I forgot to name the bug in the commit message.  So, for the record, the commit was:\n  http://svn.apache.org/viewvc?view=rev&revision=440508"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-65", "title": "add a record I/O framework to hadoop", "status": "Closed", "priority": "Minor", "reporter": "Sameer Paranjpye", "assignee": null, "labels": [], "created": "2006-03-07T07:12:40.000+0000", "updated": "2006-08-03T17:46:29.000+0000", "description": "Hadoop could benefit greatly from a simple record I/O framework that enables the specification of simple record types and enables the generation of code for serialization/deserialization in multiple target languages. The framework would handle a small well understood set of primitive types and simple compositions of these (structs, vectors, maps) . It would be possible to leverage this framework to express I/O in MapReduce computations and to use this as the basis for Hadoops RPC implementation. This would make interfacing with code in languages other than Java much easier.", "comments": ["I have attached a patch for Hadoop record I/O framework. It contains a translator for generating Java or C++ code from the record definitions, and a Java and C++ runtime library for serializing and deserializing records in binary, CSV, and XML formats. Generated Java code for records implements the WritableComparable interface, so these records could be used as keys and values for the Hadooop map-reduce framework. See the package.html file in org.apache.hadoop.record for full description.", "I have committed this.  It looks great!  Thanks, Milind!", "Looking a bit more closely, I see that some of the packages and classes are missing javadoc.  Can you please supply an additional patch which adds this?  In many cases, javadoc comments are only needed on base classes and interfaces (e.g., JType, InputArchive, OutputArchive) as they will automatically propagate to subclasses & implementations.  Also, the JavaCC-generated code will in many places lack javadoc, but a package.html explaining that the package contains only generated code would be useful.", "I realized that some of the classes which were earlier package-private had to be made public, and therefore are lacking in class comments. I will add comments and will submit a patch."], "derived": {"summary": "Hadoop could benefit greatly from a simple record I/O framework that enables the specification of simple record types and enables the generation of code for serialization/deserialization in multiple target languages. The framework would handle a small well understood set of primitive types and simple compositions of these (structs, vectors, maps).", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "add a record I/O framework to hadoop - Hadoop could benefit greatly from a simple record I/O framework that enables the specification of simple record types and enables the generation of code for serialization/deserialization in multiple target languages. The framework would handle a small well understood set of primitive types and simple compositions of these (structs, vectors, maps)."}, {"q": "What updates or decisions were made in the discussion?", "a": "I realized that some of the classes which were earlier package-private had to be made public, and therefore are lacking in class comments. I will add comments and will submit a patch."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-66", "title": "dfs client writes all data for a chunk to /tmp", "status": "Closed", "priority": "Major", "reporter": "Sameer Paranjpye", "assignee": "Doug Cutting", "labels": [], "created": "2006-03-07T07:55:56.000+0000", "updated": "2009-07-08T16:41:48.000+0000", "description": "The dfs client writes all the data for the current chunk to a file in /tmp, when the chunk is complete it is shipped out to the Datanodes. This can cause /tmp to fill up fast when a lot of files are being written. A potentially better scheme is to buffer the written data in RAM (application code can set the buffer size) and flush it to the Datanodes when the buffer fills up.\n", "comments": ["It looks to me like the temp file is only in fact used when the connection to the datanode fails.  Normally the block is streamed to the datanode as it is written.  But if the connection to the datanode fails then an application exception is not thrown, instead the temp file is used to recover, by reconnecting to a datanode and trying to write the block again.\n\nData is bufferred in RAM first, just in chunks much smaller than the block.  I don't think we should buffer the entire block in RAM, as this would, e.g., prohibit applications which write lots of files in parallel.\n\nWe could get rid of the temp file and simply throw an application exception when we lose a connection to a datanode while writing.  What is the objection to the temp file?", "So the problem with /tmp is that this can fill up and cause failures.  This is very config / install specific.  We almost never use /tmp because it gets blown out by something sometime, always when you least expect it.  Maybe we should throw by default and provide some config to do something else, such as provide a a file path for temp files?  This could be in /tmp if you chose, or map reduce could default to its temp directory where it is storing everything else.\n\nPerformance is clearly not an issue if this is truly an exceptional case.", "Eric: I agree.  We should probably change this use a temp directory under dfs.data.dir instead of /tmp.", "It doesn't make a lot of sense to buffer the entire block in RAM. On the other hand, an application ought to be able to control the buffering strategy to some extent. Most stream implementations have a setBufferSize() or equivalent method that allow programmers to do this. The default buffer size is reasonably small so that many files can be opened without worrying too much about buffering.\n\nBesides the issues with filling up /tmp (32MB is a pretty large chunk to be writing there), it's unclear that the scheme adds a lot of value, it may even be detrimental. If a connection to a Datanode fails then why not try to recover by re-connecting and throw an exception if that fails. If it's just the connection that has failed the client should be able to reconnect pretty easily. If the Datanode is down for the count the odds are low (or are they?) that it'll come back by the time the client finishes writing the block and the write will fail anyway, so why write to a temp file. If it's common for Datanodes to bounce and come back then Datanode stability is an problem that we should be working on. In that case, the temp file is only a workaround and not a real solution, it might even be masking the problem in many cases.\n", "It's hard to resume writing a block when a connection fails, since you don't know how much of the previous write succeeded.  Currently the block is streamed over TCP connections.  We could instead write it as a series of length-prefixed buffers, and query the remote datanode on reconnect about which buffers it had recieved, etc.  But that seems like reinventing a lot of TCP.\n\nIf the datanode goes down then currently the entire block is in a temp file so that it can instead be written to a different datanode.  Thus if datanodes die during, e.g., a reduce, then the reduce task does not have to restart.  But if reduce tasks are running on the same pool of machines as datanodes, then, when a node fails, some reduce tasks will need to be restarted anyway.  So I agree that this may not be helping us much.  I think throwing an exception when the connection to the datanode fails would be fine.", "Here's a minimally-tested patch that removes the use of temp files.", "I just committed a fix for this and some problems that it was hiding.", "There is a method on File to delete the file when the jvm exits. Here is a patch that calls the method on the temporary block files. Apparently there is a bug in the jvm under windows so that the files are only deleted if they are closed. For linux or solaris users, this should make sure that no block files end up being dropped by the application.", "Should we worry about the space this consumes?  For each block a client writes there will be memory allocated containing the path name to the temporary file that cannot be gc'd.  If that's 100 bytes, then 1M blocks would generate 100MB, which would be a big leak.  But writing 1M blocks means writing 32TB from a single JVM, which would take around a month (at current dfs speeds).  If we increase the block size (as has been discussed) then the rate is slowed proportionally.  So I guess we don't worry about this \"leak\"?"], "derived": {"summary": "The dfs client writes all the data for the current chunk to a file in /tmp, when the chunk is complete it is shipped out to the Datanodes. This can cause /tmp to fill up fast when a lot of files are being written.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "dfs client writes all data for a chunk to /tmp - The dfs client writes all the data for the current chunk to a file in /tmp, when the chunk is complete it is shipped out to the Datanodes. This can cause /tmp to fill up fast when a lot of files are being written."}, {"q": "What updates or decisions were made in the discussion?", "a": "Should we worry about the space this consumes?  For each block a client writes there will be memory allocated containing the path name to the temporary file that cannot be gc'd.  If that's 100 bytes, then 1M blocks would generate 100MB, which would be a big leak.  But writing 1M blocks means writing 32TB from a single JVM, which would take around a month (at current dfs speeds).  If we increase the block size (as has been discussed) then the rate is slowed proportionally.  So I guess we don't worry about this \"leak\"?"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-67", "title": "Added statistic/reporting info to DFS", "status": "Closed", "priority": "Trivial", "reporter": "Barry Kaplan", "assignee": "Doug Cutting", "labels": [], "created": "2006-03-08T06:18:53.000+0000", "updated": "2009-07-08T16:41:49.000+0000", "description": "The DatanodInfo, DFSFileInfo, and DFSClient were recently changed to package level protection, this hampers the ability to get some useful reporting data that can be used for DataNode and DFS health/performance.", "comments": ["my implementation...", "I just fixed this.  There is now a public API for DFS statistics."], "derived": {"summary": "The DatanodInfo, DFSFileInfo, and DFSClient were recently changed to package level protection, this hampers the ability to get some useful reporting data that can be used for DataNode and DFS health/performance.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Added statistic/reporting info to DFS - The DatanodInfo, DFSFileInfo, and DFSClient were recently changed to package level protection, this hampers the ability to get some useful reporting data that can be used for DataNode and DFS health/performance."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just fixed this.  There is now a public API for DFS statistics."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-68", "title": "\"Cannot abandon block during write to <file>\" and \"Cannot obtain additional block for file <file>\" errors during dfs write test", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-03-08T08:11:00.000+0000", "updated": "2009-07-08T16:41:49.000+0000", "description": "In the namenode's log file, when trying to run my writer benchmark, I get a bunch of messages like:\n\n060307 112402 Server handler 2 on 9020 call error: java.io.IOException: Cannot a\nbandon block during write to /user/oom/random/.part001389.crc\njava.io.IOException: Cannot abandon block during write to /user/oom/random/.part\n001389.crc\n        at org.apache.hadoop.dfs.NameNode.abandonBlock(NameNode.java:188)\n        at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAcces\nsorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:585)\n        at org.apache.hadoop.ipc.RPC$1.call(RPC.java:208)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:200)\n\nand \n\n060307 112402 Server handler 1 on 9020 call error: java.io.IOException: Cannot a\nbandon block during write to /user/oom/random/part001695\njava.io.IOException: Cannot abandon block during write to /user/oom/random/part0\n01695\n        at org.apache.hadoop.dfs.NameNode.abandonBlock(NameNode.java:188)\n        at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAcces\nsorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:585)\n        at org.apache.hadoop.ipc.RPC$1.call(RPC.java:208)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:200)\n\nand\n\n060307 112402 Server handler 2 on 9020 call error: java.io.IOException: Cannot obtain additional block for file /user/oom/random/part001274\njava.io.IOException: Cannot obtain additional block for file /user/oom/random/pa\nrt001274\n        at org.apache.hadoop.dfs.NameNode.addBlock(NameNode.java:160)\n        at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAcces\nsorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:585)\n        at org.apache.hadoop.ipc.RPC$1.call(RPC.java:208)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:200)\n\n", "comments": ["This has been fixed when fixing other bugs."], "derived": {"summary": "In the namenode's log file, when trying to run my writer benchmark, I get a bunch of messages like:\n\n060307 112402 Server handler 2 on 9020 call error: java. io.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "\"Cannot abandon block during write to <file>\" and \"Cannot obtain additional block for file <file>\" errors during dfs write test - In the namenode's log file, when trying to run my writer benchmark, I get a bunch of messages like:\n\n060307 112402 Server handler 2 on 9020 call error: java. io."}, {"q": "What updates or decisions were made in the discussion?", "a": "This has been fixed when fixing other bugs."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-69", "title": "Unchecked lookup value causes NPE in FSNamesystemgetDatanodeHints", "status": "Closed", "priority": "Major", "reporter": "Bryan Pendleton", "assignee": null, "labels": [], "created": "2006-03-08T08:51:29.000+0000", "updated": "2009-07-08T16:41:49.000+0000", "description": null, "comments": ["Check for null value after looking up blocks (ie, no known node has the block). Should prevent the NPE. The correct value (no hints for unfound blocks) is returned.", "Bump: Any reason this patch hasn't been applied? It looks like it's still possible for non-found blocks to return null, causing an NPE in this function, rather than a more graceful recovery.", "Committed.  Sorry, this fell off my radar.  Thanks for the reminder.  I fixed something related in:\n\nhttp://svn.apache.org/viewcvs.cgi/lucene/hadoop/trunk/src/java/org/apache/hadoop/dfs/FSNamesystem.java?rev=382545&r1=377317&r2=382545\n\nso I hadn't seen this problem.\n"], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Unchecked lookup value causes NPE in FSNamesystemgetDatanodeHints"}, {"q": "What updates or decisions were made in the discussion?", "a": "Committed.  Sorry, this fell off my radar.  Thanks for the reminder.  I fixed something related in:\n\nhttp://svn.apache.org/viewcvs.cgi/lucene/hadoop/trunk/src/java/org/apache/hadoop/dfs/FSNamesystem.java?rev=382545&r1=377317&r2=382545\n\nso I hadn't seen this problem."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-70", "title": "the two file system tests TestDFS and TestFileSystem take too long", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-03-09T07:30:48.000+0000", "updated": "2009-07-08T16:41:49.000+0000", "description": "Running \"ant test\" takes hours and uses the conf directory, which forces it to run on the real cluster.\n\nI propose that we split rename the test classes from\n\nsrc/test/org/apache/hadoop/dfs/TestDFS.java to LongTestDFS.java\nsrc/test/org/apache/hadoop/fs/TestFileSystem.java to LongTestFileSystem.java\n\nand then we set up a new ant target \"long-test\" that runs all tests that match \"**/LongTest*.java\".", "comments": ["TestFileSystem only takes 4 seconds for me, but TestDFS is too slow.  So I'm +1 for moving TestDFS to a test-long target.  Then we should make the \"nightly\" target depend on test-long.\n", "TestFileSystem connects to the default FileSystem in the conf directory. Would it be ok to make TestFileSystem always use local? Otherwise it can be really slow (10 minutes before I killed it) or fail if the cluster happens to be down.\n\nI'd like to have \"ant test\" take minutes and not depend on the cluster being available, so that I can use it during development.\n", "I'm okay with having it always run local when run as a unit test, but not from the command line.  From the command line it should use the configured filesystem, no?  But unit tests should probably be config-independent, explicitly testing different config options.", "That sounds reasonable. \n\nI think we need to go the next step and create a conf directory that is used for junit tests. How about \na conf/junit that contains hadoop-site.xml and mapred-defaults.xml. I'll change the test classpath to include the real conf directory to pick up the hadoop-default.xml so that it doesn't need to be duplicated.\n\nMaybe instead of a \"test-long\" target it should be a \"cluster-test\" and it uses the real conf directory.\n\nIn all cases, the command line version of the test should use the real conf directory.", "This patch creates a config directory for the junit tests that forces mapreduce and dfs to \"local\". It also renames TestDFS to ClusterTestDFS and creates a new ant target \"cluster-test\" that runs with the real config directory instead of the junit config.", "I just committed this.\n\nI changed things so that the test config files are in src/test rather than conf/junit.  That's the way Nutch does something similar, and it also requires a less classpath manipulation, since src/test is already on the test classpath (although I had to reorder things, to make sure it came ahead of conf/).  I also changed the conf files in src/test to be empty, forcing defaults to be used for unit tests.  Finally, to implement the test-cluster task, instead of copying the test task I parameterized the test task a bit and call it.", "The unit tests should not be using /tmp for their data, because it won't be cleaned up and could conflict between users. By setting the config files to put the directories under build/test/data/run you are sure that it will be cleared at least the next time you run the test and each user has their own.", "Okay, I set the test configuration to write things to build/test instead of /tmp.  But, if temp really makes things fragile, then perhaps we should really change the default, rather than change the test, no?  Personally I slightly prefer /tmp for unit tests, since it is much faster when you're developing in an nfs-mounted home directory...", "I vote for changing the default.  If using /tmp makes sense in your setup, you should be free to change the default.  We should making doing so globally easy.  But we should steer free of using /tmp unless asked to.  It is one of those things that just bites folks over and over and over again."], "derived": {"summary": "Running \"ant test\" takes hours and uses the conf directory, which forces it to run on the real cluster. I propose that we split rename the test classes from\n\nsrc/test/org/apache/hadoop/dfs/TestDFS.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "the two file system tests TestDFS and TestFileSystem take too long - Running \"ant test\" takes hours and uses the conf directory, which forces it to run on the real cluster. I propose that we split rename the test classes from\n\nsrc/test/org/apache/hadoop/dfs/TestDFS."}, {"q": "What updates or decisions were made in the discussion?", "a": "I vote for changing the default.  If using /tmp makes sense in your setup, you should be free to change the default.  We should making doing so globally easy.  But we should steer free of using /tmp unless asked to.  It is one of those things that just bites folks over and over and over again."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-71", "title": "The SequenceFileRecordReader uses the default FileSystem rather than the supplied one", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-03-09T08:18:06.000+0000", "updated": "2009-07-08T16:51:41.000+0000", "description": "The mapred.TestSequenceFileInputFormat test was failing when run with a conf directory that pointed to a dfs cluster. The reason was that SequenceFileRecordReader was using the default FileSystem from the config, while the test program was assuming the \"local\" file system was being used.", "comments": ["Here is a patch that adds the FileSystem as an argument to the constructor.", "Doug,\n   This one seems to have dropped off your radar. The fix isn't mandatory any more, because we created a custom config for the junit tests. However, it still seems cleaner to use the provided FileSystem rather than fetching the default one.", "This looks good, although it makes an incompatible changes to a public APIs used by Nutch.  So we should probably continue to support the old method signatures too, or else make a co-ordinated change to Nutch.", "This patch also fixes the TextInputFormat and the framework's handling of the output path too. I will file a follow-up but to get deprecate the current FileSystem.get(Configuration) method and replace it like FileSystem.getDefault(Configuration). There are many place still left in the code where the default file system is gotten. However, with this patch, I was able to run map/reduce jobs (including a unit test) with the input and output going to non-default file systems. ", "+1\n\nhttp://issues.apache.org/jira/secure/attachment/12359029/non-default-fs-input.patch applied and successfully tested against trunk revision r544740.\n\nTest results:   http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/257/testReport/\nConsole output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/257/console", "Owen, this will break InputFormats and OutputFormats that use the FileSystem passed to getRecordReader() and getRecordWriter(), since null is now passed.  Is that intended?  Is it now an error to touch that parameter?  If so, we should at least provide some warning in the release notes, no?  It'd be nicer to have a new API and then deprecate this one...", "*sigh*\n\nI guess it is a bit too aggressive. The point of doing that is that any OutputFormats who use that FileSystem are broken in exactly the way fixed by this patch. Furthermore, the parameter has been named 'ignored' for many releases. (Only OutputFormats were given nulls, because InputFormats don't take a FileSystem. I think I removed the InputFormat FileSystem parameter a long time ago.)", "This patch no longer applies to trunk, and I still think we need to make it more back-compatible.", "Updated to trunk and continued passing in the default fs.", "Realized that one of the changes wasn't needed.", "Sorry for the lateness on this one. It had fallen off my radar as being bounced. We need this one.", "+1", "I just committed this."], "derived": {"summary": "The mapred. TestSequenceFileInputFormat test was failing when run with a conf directory that pointed to a dfs cluster.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "The SequenceFileRecordReader uses the default FileSystem rather than the supplied one - The mapred. TestSequenceFileInputFormat test was failing when run with a conf directory that pointed to a dfs cluster."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-72", "title": "hadoop doesn't take advatage of distributed compiting in TestDFSIO", "status": "Closed", "priority": "Major", "reporter": "Konstantin Shvachko", "assignee": null, "labels": [], "created": "2006-03-10T04:39:51.000+0000", "updated": "2009-07-08T16:51:41.000+0000", "description": "TestDFSIO runs N map jobs, each either writing to or reading from a separate file of the same size, \nand collects statistical information on its performance. \nThe reducer further calculates the overall statistics for all maps. \nIt outputs the following data:\n- read or write test\n- date and time the test finished   \n- number of files\n- total number of bytes processed\n- overall throughput in mb/sec\n- average IO rate in mb/sec per file\n\n__Results__\nI run 7 iterations of the test one after another on a cluster of ~200 nodes. \nThe file size is the same in all cases 320Mb. \nThe number of files tried is 1,2,4,8,16,32,64.\nThe log file with statistics is attached.\nIt looks like we don't have any distributed computing here at all.\nThe total execution time increases proportionally to the total size of data both for writes and reads.\nAnother thing is that the io ratio for read is higher than the write rate just gradually.\nFor comparison I attach time measuring for the same ios performed on the same cluster but sequentially in a simple loop.\nThis is the summary:\n\nFiles\tmap/red time\tsequential time\n 1\t\t49\t\t\t  34 \n 2\t\t86\t\t\t  69\n 4\t\t158\t\t\t131\n 8\t\t299\t\t\t266\n16\t\t569\t\t\t532\n32\t\t1131\n64\t\t2218\n\nThis doesn't look good, unless there is something wrong with my test (attached) or the cluster settings.\n", "comments": ["Did you look at the web ui to see how many map tasks were used to execute this?  My suspicion is that only a single map task is used.  A SequenceFile cannot be split into chunks smaller than 2k.  With less than 64 files, your single input file is probably less than 2k.  You could instead use a text input file, which can be split into smaller chunks, or you could use a custom getSplits() implementation that actually parses the input file, or you could use a much larger number of files.", "TestDFSIO is modified to create one input file for each map task.\nThat way it works in parallel.\nEverything is getting very slow when the the number of writes is close to or \nlarger than the size of the cluster.\n", "This was caused by a misunderstanding."], "derived": {"summary": "TestDFSIO runs N map jobs, each either writing to or reading from a separate file of the same size, \nand collects statistical information on its performance. The reducer further calculates the overall statistics for all maps.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "hadoop doesn't take advatage of distributed compiting in TestDFSIO - TestDFSIO runs N map jobs, each either writing to or reading from a separate file of the same size, \nand collects statistical information on its performance. The reducer further calculates the overall statistics for all maps."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was caused by a misunderstanding."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-73", "title": "bin/hadoop dfs -rm works only for absolute paths", "status": "Closed", "priority": "Trivial", "reporter": "Stefan Groschupf", "assignee": "Sameer Paranjpye", "labels": [], "created": "2006-03-10T07:47:30.000+0000", "updated": "2009-07-08T16:41:48.000+0000", "description": "every dfs command like -du works with relative paths but remove only works only for absolute paths. ", "comments": [], "derived": {"summary": "every dfs command like -du works with relative paths but remove only works only for absolute paths.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "bin/hadoop dfs -rm works only for absolute paths - every dfs command like -du works with relative paths but remove only works only for absolute paths."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-74", "title": "hash blocks into dfs.data.dirs", "status": "Closed", "priority": "Major", "reporter": "Doug Cutting", "assignee": "Konstantin Shvachko", "labels": [], "created": "2006-03-11T06:33:10.000+0000", "updated": "2009-07-08T16:41:49.000+0000", "description": "When dfs.data.dir has multiple values, we currently start a DataNode for each (all in the same JVM).  Instead we should run a single DataNode that stores block files into the different directories.  This will reduce the number of connections to the namenode.  We cannot hash because different devices might be different amounts full.  So the datanode will need to keep a table mapping from block id to file location, and add new blocks to less full devices.", "comments": [], "derived": {"summary": "When dfs. data.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "hash blocks into dfs.data.dirs - When dfs. data."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-75", "title": "dfs should check full file availability only at close", "status": "Closed", "priority": "Minor", "reporter": "Doug Cutting", "assignee": "Milind Barve", "labels": [], "created": "2006-03-11T06:35:35.000+0000", "updated": "2009-07-08T16:41:50.000+0000", "description": "Currently it appears that dfs checks that the namenode knows about all blocks in a file as each block is written.  It would be more efficient to only check that all blocks are stored somewhere when the file is closed.", "comments": ["Added a patch that removes that check (whether the previous blocks have been replicated or not) after adding every block to the file in construction. Now it checks that all file blocks have been replicated only at close of a file.", "Doug, can this be committed for the 0.3 release?", "I just committed this.  Thanks, Milind!"], "derived": {"summary": "Currently it appears that dfs checks that the namenode knows about all blocks in a file as each block is written. It would be more efficient to only check that all blocks are stored somewhere when the file is closed.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "dfs should check full file availability only at close - Currently it appears that dfs checks that the namenode knows about all blocks in a file as each block is written. It would be more efficient to only check that all blocks are stored somewhere when the file is closed."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Milind!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-76", "title": "Implement speculative re-execution of reduces", "status": "Closed", "priority": "Minor", "reporter": "Doug Cutting", "assignee": "Sanjay Dahiya", "labels": [], "created": "2006-03-11T06:57:00.000+0000", "updated": "2013-05-02T02:29:02.000+0000", "description": "As a first step, reduce task outputs should go to temporary files which are renamed when the task completes.\n", "comments": ["I've tried to implement speculative reduces and it seems to be working, however I'd like you to take a look at it since I'm not familiar with some of the inner workings of hadoop.\n\nAs suggested it writes output to a temporary name and the first one to finish moves it to the correct output name.\nThe patch adds a String tmpName to getRecordWriter in OutputFormatBase\nand a close method. Basically the OutputFormatBase keeps track of the tmpName and the final name\nonce close is called it moves the tmp to the final.\n\nThis means the current output formats doesn't have to be changed.\n\nThis patch would ideally be complemented by a better tasktracker selection, I've seen instances where there's two final reduce tips and then a speculative reduce is assigned to the same node that is already running the other task.\n\nA speculative reduce will be started if finishedReduces / numReduceTasks >= 0.7\n\nThat's about it, looking forward to hear your input", "As one should've suspected there seems to be a problem with the patch.\nIt doesn't clean up after the tasks properly, but I do not currently have the time to look at this.\nStill, feedback is welcome or if someone wants to use any of it in their own patch, go ahead.\n", "Here is a list of code level changes, I will test this stuff meanwhile\n\n- Adding extra jobConf configuration - runSpeculativeReduces. \n\n- TaskInProgress maintains a list of nodes where it has already ran ( or is running ), this will be used to not schedule a speculative instance where the task is already running or has failed in past. [TIP already contains a list of nodes where it task failed ]. \n\n- Another option is if *any* reduce task is already assigned to this TT and is still running, then its not assigned a speculative task. [comments?]\n\n- TIP.hasSpeculative task , now checks for reduce tasks as well. currently it checks for only map tasks. The exact condition(timeouts) in which reduce task should be executed speculatively is open for discussion. using johan's conditions(finishedReduces / numReduceTasks >= 0.7 ) for testing till then. \n\n- JobInProgress.findNewTask - looks for speculative tasks (TIP.hasSpeculativeTask()) and whether the task ran on same task tracker. \n\n- If speculative execution of reduce is enabled then ReduceTask.run() creates a temp file name for reduce output. When reduce task finishes it checks if the output file is already written by some other reduce instance else it renames its output to final output. otherwise temp output is deleted. \n\n- TaskTracker.TIP.cleanup() also cleans up the reduce task temp file if it is killed in between. \n\n- JobTracker.pollForTaskWithClosedJob(), TIP.shouldCloseForClosedJob() - return true if a speculative reduce task finished first, which ultimately goes down to TT and kills/cleans up the task.\n\nThe exact condition(timeouts) in which reduce task should be executed speculatively is open for discussion. \n\ncomments? ", "I think it is better for now to just use JobConf.setSpeculativeExecution for both maps and reduces.\n\nmachinesWhereFailed is a list of machines where the task has failed. To find where it is currently running, you need to use recentTasks. Currently, the recentTasks is a set of task ids that are running. You should probably make it a map from task id to task tracker id. \n\nDon't block speculative reduces based on other reduces running. That would make a perpetually busy cluster never run speculative reduces.\n\n*** Please create a library that lets you create files off to the side and when you call commit moves them into place. Speculative reduces need it, but that functionality is useful other places, such as side effect-based maps. The class should also have an abort method that cleans up.\n\nThe defaults for the map speculative execution don't look too unreasonable, so just use them for now.", "This patch is up for review. \n\nHere is the list of changes included in this patch - \n\nReplaced recentTasks to a Map, added a new method in TaskInProgress hasRanOnMachine, which looks at this Map and hasFailedOnMachines(). This is used to avoid scheduling multiple reduce instances of same task on the same node. \n\nAdded a PhasedRecordWriter, which takes a RecordWriter, tempName, finalName. Another option was to create a PhasedOutputFormat, this seems more natural as it works with any existing OutputFormat and RecordWriter. Records are written to tempName and when commit is called they are moved to finalName. \n\nReduceTask.run() - if speculative execution is enabled then reduce output is written to a temp location using PhasedRecordWriter. After task finishes the output is written to a final location. \nIf some other speculative instance finishes first then TaskInProgress.shouldCloseForClosedJob() returns true for the taskId. On TaskTracker the task is killed by Process.destroy() so cleanup code is in TaskTracker instead of Task. The cleanup of Maps happen in Conf, which is probably misplaced. We could refactor this part for both Map and Reduce and move cleanup code to some utility classes which, given a Map and Reduce task track the files generated and cleanup if needed. \n\nAdded an extra attribute in TaskInProgress - runningSpeculative, to avoid running more than one speculative instances of ReduceTask. Too many Reduce instances for same task could increase load on Map machines, this needs discussion. I can revert this change back to allow some other number of instances of Reduces (MAX_TASK_FAILURES?). \n\ncomments", "Regarding the PhasedRecordWriter class, I think this should provide the option of creating a file, write to them and then call commit or abort. The current implementation binds it to a RecordWriter, but this should cater to things like Sequence files also (for e.g., RandomWriter should be able to use this functionality).", "The PhasedRecordWriter won't handle all of the cases, because RecordWriters can write multiple files. Furthermore, they are user code and it would be better to minimize required changes to them.\n\nA better approach would be to have a PhasedFileSystem that takes a base FileSystem and use that to commit/abort the changes. Then the framework could pass the PhasedFileSystem to the createRecordReader call and it would catch all of the files that the RecordWriter created. When the PhasedFileSystem gets a create call, it creates it in the base FileSystem with a mutated name. When the changes are commited, the files are all renamed. If the changes are aborted, the mutated filenames are deleted.\n\n", "PhasedFileSystem sounds good, I will work on this approach. thanks Devaraj, Owen for review. ", "Looking at FileSystem implementation here is a concern \n\nFileSystem is an abstract class, extended by Local/Distributed FileSystem. ideal way to have a PhasedFileSystem would be if FileSystem was an interface, we could implement it and take a FileSystem in the constructor of PhasedFIleSystem. some thing like  - \nPhasedFileSystem implements FileSystem{\n   public PhasedFileSystem(FileSysten fs);\n   // channel all methods to fs \n  // implement commit() / abort()\n}\nIn this case we could use Phased functionality with both Local/Distributed FileSystem, still maintaining interface compatibility. Doing this with base abstract class will cause some dummy objects. as in next option \n\nNext option is to have something like \n\nPhasedFileSystem extends FileSystem{\n   private FileSystem fs ; \n   protected PhasedFileSystem(FileSysten fs) { \n    super(fs.getConf()); // not used, just dummy for base class creation\n   \tthis.fs = fs ; \n   }\n   protected PhasedFileSystem(conf){ super(conf); throw NotSupportedException; } \n   \n   public static getNamed(...) { \n     return new PhasedFileSystem(FileSystem.getNamed..) ;\n   }\n   //  All other methods channel calls to fs\n   commit() ;\n   abort(); \n}\n\nLast option is to add extra methods to FileSystem itself, which will not be used in most cases but will be available in derived FileSystems. This doesnt sound too good. \n \nOption 2 is something that will work well for us, even though its not a good design.\n\nComments?", "1. This patch doesn't apply cleanly anymore. The TaskTracker.java has a conflicting change.\n\n2. I agree that it would be nice if FileSystem was an interface, but that is way out of scope for this bug. \n\nYou should derive PhasedFileSystem from FileSystem. It should have a \"base\" FileSystem that does the real work and a Map<String,String> that maps \"final\" filenames to \"temporary\" filenames. \n\nPhasedFileSystem will need to implement the abstract \"Raw\" methods from FileSystem. The read operations, can just be sent directly to the base FileSystem. createRaw should create a temporary name, put it in the map and create the file using the base FileSystem. The other \"modification\" methods (renameRaw, deleteRaw) should throw UnsupportedOperationException.\n\nTo ensure that the files are cleaned up correctly, I'd suggest that the temporary files be stored under:\n\n<system dir>/<job id>/<tip id>/<task id>/<unique file id>\n\n(in dfs clearly) so that when the job is finished, we just need to delete the job directory to clean up any remains of failed tasks that didn't clean up properly.\n\nthere should also be:\n  public void commit() throws IOException {...}\n  public void abort() throws IOException {...}\n  public void close() throws IOException { abort(); }\n\nFor close, it should lock the tip directory, move the files into place, put a DONE touch file in the tip directory and delete the task id directory.", "Since the dfs locking doesn't really work reliably, I would drop the tip id part of the directory structure, don't do any locking, and if the file is already in place, just skip that file.", "Here is update patch for review\n\nchanges in this patch \n - Moved temp files to (mapred.system.dir)/<jobid> \n-  2 new variables in TIP - \n        -    runningTasks ( to track currently running instances of attempts for same task, earlier this was done using a boolean) \n        -    hasCompletedTask ( for reduces if a attempts succeeds then all subsequent failures for same TIP should be ignored, existing setup tries to schedule another task for that.) \n- Added a phaseFileSystem, which takes a jobid, tipid, taskid and creates all files in (mapred.system.dir)/<jobid>/<tipid>/<taskid>, these are moved to their final location on commit or deleted. \n- Task Constructor needs tip id now, its passed in RPC as well. \n- Task.localizeConfiguration adds tip id to conf. \n- Minor change in FSDirectory to add exception message in log if rename fails. \n\n+Planning to add an example for using PhasedFileSystem - changing RandomWriter to use PhasedFileSystem, as Devaraj suggested in a separate patch on the same issue. \n\nthanks owen for review. ", "One important point that needs to be reviewed in this patch is extra checks for speculative reduce tasks in TIP. We may want to inherit some of this in spec maps too or structure it better. looking for comments on that. ", "1. Noticed incorrect Javadoc/comments in some places. Please go through the patch carefully and fix them.\n2. The commit and abort APIs of PhasedFileSystem needs to invoke the baseFS's close() method.\n3. The runningTasks variable seems redundant and can be removed, and the code updating/reading runningTasks can be changed to update/read recentTasks instead.", "One more comment - need to document somewhere how exactly (what config params) a user of the PhasedFileSystem (a map method) can access the JobId, TaskId and TIPId when he wants to create an instance of PhasedFileSystem. Better yet, add a new constructor in PhasedFileSystem that takes a JobConf object. Inside that constructor, you can get the JobId, TaskId and TIPId values and proceed. The user doesn't have to bother about details in this case.", "> it would be nice if FileSystem was an interface [ ...]\n\nSometimes that would be nice, and sometimes it would not be.  The FileSystem API has evolved considerably without breaking back-compatibility.  That's harder to do with an interface.\n", "Changes in this patch update \n\n1. PhasedFileSystem has extra constructor that takes an JobConf\n2. PhasedFileSystem now handles file overwrite flag properly. \n\n3. commit/abort can be done for a single file or for all files created by this FS. \n4. The method for commit is split in 2 function one is pvt with an extra boolean variable to avoid concurrent modification while committing in a iterator loop. \n 4. reduceProgress calculation is changed, earlier it was dividing the progress by reduces.length twice, once while update status and then while checking for spec tasks, which was incorrect. \n\nThanks Devaraj for the review. ", "ops attached wrong file. updating.", "Updated patch, earlier patch was causing PiEstimator test to fail. It was closing fs before ReduceTask, and Piestimator reduce task's close opens a new file causing it to fail. It is fixed now. TextInputFormat test still fails but I guess thats due to HADOOP-696. ", "The patch failed with Streaming test when run twice in succession, because output file created in first run still exists in second run and PhasedFileSystem was not handling an exception while rename. \nThis patch fixes the issue. ", "New patch, fixed some java doc warnings. ", "Conflicts with latest trunk. will submit an updated patch", "Patch updated with latest trunk. ", "The propsed PhasedFileSystem needs to support mkdirs in order to work with MapFileOutputFormat.\n\nIt will break less user OutputFormats if we also support the read operations:\nexists\nopenRaw\ngetLength\nisDirectory\nlistPathsRaw\nsetWorkingDirectory\ngetWorkingDirectory\nby passing the request to the underlying FileSystem.\n\nIt is confusing to use the TaskInProgress.hasSucceededTask for reduces and TaskInProgress.completes for maps. I think it would be better to use the completes for both.\n\nThanks for putting the generic types into activeTasks, but it should look like:\n\nMap<String, String> activeTasks = new HashMap();\n\nThe declared type should use the interface and the constructor doesn't need the generic types.\n\nThe logic in TaskInProgress.isRunnable is pretty convoluted although I think if you use completes for reduces, it doesn't need to change.\n\nTaskInProgress.hasRanOnMachine should be hasRunOnMachine.\n\nfindNewTask should add a new condition instead of continue:\n\n\n               } else if (specTarget == -1 &&\n                          task.hasSpeculativeTask(avgProgress)) {\n+                if(task.hasRanOnMachine(taskTracker)){\n+                  continue ;\n+                }\n                 specTarget = i;\n               }\n\nshould be:\n\n               } else if (specTarget == -1 &&\n                              task.hasSpeculativeTask(avgProgress) &&\n                              !task.hasRanOnMachine(taskTracker)) {\n                 specTarget = i;\n               }\n\nThe patch always creates a PhasedFileSystem even when it won't be used because there is no speculative execution.\n\n@@ -298,7 +305,14 @@\n\n     } finally {\n       reducer.close();\n-      out.close(reporter);\n+      if( runSpeculative ){\n+        out.close(reporter);\n+        pfs.commit();\n+        pfs.close();\n+       }else{\n+        out.close(reporter);\n+        fs.close();\n+      }\n     }\n\n\"out.close(reporter);\" should be lifted out of the branch. And it is usually better to not close the file system because they are cached and may be used in another context. So I'd drop the else clause all together.\n\n\n", "This patch removes the part of killing a reduce task if another speculative instance is running due to issue reported in HADOOP-737. Now the task is left running till job finishes. \nAlso includes Owen's review comments\n\nthanks Owen for the review. ", "I see a couple more points:\n  1. The code in TaskInProgress starting at line 445 is duplicated between the then/else branches and should be re-written to use a single test.\n  2. The directory ${system.dir}/${jobid}/${tipid}/${taskid} should always be deleted when a task completes or fails regardless of whether speculative execution was running so that non-speculative execution tasks can use the PhasedFileSystem too.\n  3. Equivalently, ${system.dir}/${jobid} should be always be deleted when a job completes or fails.\n  4. Why did you remove the \"/ reduces.length\" in computing the avgProgress in JobInProgress.java line 357? That seems wrong.\n  5. The JobInProgress.findNewTask should not break if it finds a speculative task. That would make it run a speculative task rather than a regular task that comes after it in the task list.", "  4. Why did you remove the \"/ reduces.length\" in computing the avgProgress in JobInProgress.java line 357? That seems wrong. \n\n\nJobInProgress.java:313 already divides the progressDelta of a task by no of tasks before adding it to reduce progress. That averages it out over reduces, or am I missing something there? ", "I just committed this.\n\nI made one change, making PhasedFileSystem package-private rather than public.  It's easier to make something public later than to make it non-public once it is public, and we should strive to minimize our public APIs.\n\nThanks, Sanjay!"], "derived": {"summary": "As a first step, reduce task outputs should go to temporary files which are renamed when the task completes.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Implement speculative re-execution of reduces - As a first step, reduce task outputs should go to temporary files which are renamed when the task completes."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.\n\nI made one change, making PhasedFileSystem package-private rather than public.  It's easier to make something public later than to make it non-public once it is public, and we should strive to minimize our public APIs.\n\nThanks, Sanjay!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-77", "title": "hang / crash when input folder does not exists.", "status": "Closed", "priority": "Critical", "reporter": "Stefan Groschupf", "assignee": null, "labels": [], "created": "2006-03-13T10:05:40.000+0000", "updated": "2009-07-08T16:51:41.000+0000", "description": "The jobtracker hangs and jobtracker info server throws Internal Server Error when until task initialization a exception will be thrown. \nFuture jobs will no processed and also the job info server does not show any information since it throws a a http 500.\n\nThis is a show blocker especially when hadoop is shell script driven. \n\n\n060312 235707 TaskInProgress tip_6jd6g8 has failed 4 times.\n060312 235707 Aborting job job_hsg7y8\n060312 235708 Server connection on port 50020 from  XX.100.XXX.2: exiting\n060312 235709 Server connection on port 50020 from  XX.100.XXX.2: starting\n060312 235710 parsing jar:file:/home/myuser/nutch/lib/hadoop-0.1-dev.jar!/hadoop-default.xml\n060312 235710 parsing jar:file:/home/myuser/nutch/lib/hadoop-0.1-dev.jar!/mapred-default.xml\n060312 235710 parsing file:/home/myuser/nutch/conf/hadoop-site.xml\n060312 235710 parsing jar:file:/home/myuser/nutch/lib/hadoop-0.1-dev.jar!/hadoop-default.xml\n060312 235710 parsing jar:file:/home/myuser/nutch/lib/hadoop-0.1-dev.jar!/mapred-default.xml\n060312 235710 parsing file:/home/myuser/nutch/conf/hadoop-site.xml\n060312 235710 parsing jar:file:/home/myuser/nutch/lib/hadoop-0.1-dev.jar!/hadoop-default.xml\n060312 235710 parsing jar:file:/home/myuser/nutch/lib/hadoop-0.1-dev.jar!/mapred-default.xml\n060312 235710 parsing /u1/hadoop-data/tmp/hadoop/mapred/local/jobTracker/job_2p6ywq.xml\n060312 235710 parsing file:/home/myuser/nutch/conf/hadoop-site.xml\n060312 235711 parsing jar:file:/home/myuser/nutch/lib/hadoop-0.1-dev.jar!/hadoop-default.xml\n060312 235711 parsing jar:file:/home/myuser/nutch/lib/hadoop-0.1-dev.jar!/mapred-default.xml\n060312 235711 parsing /u1/hadoop-data/tmp/hadoop/mapred/local/jobTracker/job_2p6ywq.xml\n060312 235711 parsing file:/home/myuser/nutch/conf/hadoop-site.xml\n060312 235712 job init failed\njava.io.IOException: Not a file: /user/myuser/segments/20060312214035/crawl_fetch/part-00001/data\n        at org.apache.hadoop.mapred.InputFormatBase.getSplits(InputFormatBase.java:99)\n        at org.apache.hadoop.mapred.JobInProgress.initTasks(JobInProgress.java:127)\n        at org.apache.hadoop.mapred.JobTracker$JobInitThread.run(JobTracker.java:208)\n        at java.lang.Thread.run(Thread.java:595)\nException in thread \"Thread-20\" java.lang.NullPointerException\n        at org.apache.hadoop.mapred.JobInProgress.kill(JobInProgress.java:437)\n        at org.apache.hadoop.mapred.JobTracker$JobInitThread.run(JobTracker.java:212)\n        at java.lang.Thread.run(Thread.java:595)\n060312 235713 Server connection on port 50020 from  XX.100.XXX.2: exiting\n\n...\n\n\n60312 235715 parsing file:/home/myuser/nutch/conf/hadoop-site.xml\n060312 235755 /jobtracker.jsp: \njava.lang.NullPointerException\n        at org.apache.hadoop.mapred.JobInProgress.finishedMaps(JobInProgress.java:205)\n        at org.apache.hadoop.mapred.jobtracker_jsp.generateJobTable(jobtracker_jsp.java:67)\n        at org.apache.hadoop.mapred.jobtracker_jsp._jspService(jobtracker_jsp.java:130)\n        at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:94)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427)\n        at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:475)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567)\n        at org.mortbay.http.HttpContext.handle(HttpContext.java:1565)\n        at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:635)\n        at org.mortbay.http.HttpContext.handle(HttpContext.java:1517)\n        at org.mortbay.http.HttpServer.service(HttpServer.java:954)\n        at org.mortbay.http.HttpConnection.service(HttpConnection.java:814)\n        at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981)\n        at org.mortbay.http.HttpConnection.handle(HttpConnection.java:831)\n        at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244)\n        at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)\n        at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)\n060313 014526 /jobtracker.jsp: \n", "comments": ["This patch solves the problem for me.", "I have applied this patch.  Thanks, Stefan."], "derived": {"summary": "The jobtracker hangs and jobtracker info server throws Internal Server Error when until task initialization a exception will be thrown. Future jobs will no processed and also the job info server does not show any information since it throws a a http 500.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "hang / crash when input folder does not exists. - The jobtracker hangs and jobtracker info server throws Internal Server Error when until task initialization a exception will be thrown. Future jobs will no processed and also the job info server does not show any information since it throws a a http 500."}, {"q": "What updates or decisions were made in the discussion?", "a": "I have applied this patch.  Thanks, Stefan."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-78", "title": "rpc commands not buffered", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-03-14T06:48:27.000+0000", "updated": "2006-08-03T17:46:31.000+0000", "description": "Calls using Hadoop's RPC framework get sent across the network byte by byte.", "comments": ["The problem is that the Connection object was doing:\n\nout = new BufferedOutputStream(new FilteredOutputStream(sock.getOutputStream()) { \n// definition of write(byte[], int offset, int length)\n});\n\nThe problem is that the write definition called super.write(byte[], int, int), which did a loop calling write(byte) for each byte. So I changed the definition to call out.write(byte[], int,int), which will send the array in a single call.", "Thanks for catching this, Owen!  This can make a big performance difference!"], "derived": {"summary": "Calls using Hadoop's RPC framework get sent across the network byte by byte.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "rpc commands not buffered - Calls using Hadoop's RPC framework get sent across the network byte by byte."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for catching this, Owen!  This can make a big performance difference!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-79", "title": "listFiles optimization", "status": "Closed", "priority": "Major", "reporter": "Konstantin Shvachko", "assignee": "Konstantin Shvachko", "labels": [], "created": "2006-03-14T07:53:30.000+0000", "updated": "2009-07-08T16:41:49.000+0000", "description": "In FSDirectory.getListing() looking at line\nlisting[i] = new DFSFileInfo(curName, cur.computeFileLength(), cur.computeContentsLength(), isDir(curName));\n\n1. computeContentsLength() is actually calling computeFileLength(), so this is called twice,\nmeaning that file length is calculated twice.\n2. isDir() is looking for the INode (starting from the rootDir) that has actually been obtained\njust two lines above, note that the tree is locked by that time.\n\nI propose a simple optimization for this, see attachment.\n\n3. A related question: Why DFSFileInfo needs 2 separate fields len for file length and\ncontentsLen for directory contents size? It looks like these fields are mutually exclusive,\nand we can use just one, interpreting it one way or another with respect to the value of isDir.", "comments": ["This looks fine to me.  I simplified FSDirectory.isDir() a bit more & committed this.\n\nDid you find this to be a bottleneck in benchmarks?  BTW, I have had some success profiling Hadoop daemons using Sun's built-in sampling profiler.  I simply set HADOOP_OPTS to  '-agentlib:hprof=cpu=samples,interval=20' before starting a daemon.  Then, when I stop that daemon, it dumps profile data to a text file.\n\nAnd, finally, yes, DFSFileInfo could re-use the length field for both purposes.  But this class is only used for interchange, right?, so making it small will only serve to make RPC's a bit faster and won't save a lot of memory.", "No, it was not really a bottleneck.\nInteresting about the profiling. Is it JMX or something else?\nYes, DFSFileInfo is for reporting only, so it does not save space.\nIt might save some code though, since when you have one field you \nprobably don't need two different finctions to extract it."], "derived": {"summary": "In FSDirectory. getListing() looking at line\nlisting[i] = new DFSFileInfo(curName, cur.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "listFiles optimization - In FSDirectory. getListing() looking at line\nlisting[i] = new DFSFileInfo(curName, cur."}, {"q": "What updates or decisions were made in the discussion?", "a": "No, it was not really a bottleneck.\nInteresting about the profiling. Is it JMX or something else?\nYes, DFSFileInfo is for reporting only, so it does not save space.\nIt might save some code though, since when you have one field you \nprobably don't need two different finctions to extract it."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-80", "title": "binary key", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-03-14T15:18:51.000+0000", "updated": "2006-08-03T17:46:31.000+0000", "description": "I needed a binary key type, so I extended BytesWritable to be comparable also.", "comments": ["Overall this looks good.  A couple of questions:\n\n1. Why call setSize(0) in read()?  This looks like a no-op.  Am I missing something?\n\n2. Why bother to use md5 for hashCode()?  That could be expensive.  Why not implement this like java.util.Arrays.hashCode() and UTF8.hashCode():\n\n  public int hashCode() {\n    int hash = 1;\n    for (int i = 0; i < size; i++)\n      hash = (31 * hash) + (int)bytes[i];\n    return hash;\n  }\n\n", "> 1. Why call setSize(0) in read()? This looks like a no-op. Am I missing something?\n\nYeah, that is a little subtle and I should have commented it better. Basically, when I do the second setSize, it may call setCapacity, which will copy the current data. If the current size is 0, then it won't copy anything. It won't change the user visible behavior, but will save a useless copy.\n\n> 2. Why bother to use md5 for hashCode()?  That could be expensive.  Why not implement this like \n> java.util.Arrays.hashCode() and UTF8.hashCode():\n\nYeah, I considered doing something lighter than md5, but using md5 prevents pathological cases from doing bad things. We also use md5 a lot around here, so it is a really useful default for us, but it might make sense to have a lighter hash alternative. However, since in map/reduce the hash function is only used for partitioning the map output, it seemed better to use a known good hash function than taking a chance on a fast but sloppy hash function.\n", "This updated patch handles the case of shrinking the buffer's capacity. It also adds testing code for that situation into the unit test.", "If we think that the default java hash function is bad, then we should probably switch it in all of the places where we hash, e.g., in UTF8, here, etc.  For now I will: (a) add a static hashBytes() method to WritableComparable, use it in BytesWritable().  Then, if we someday decide to change the hash function then we'll have a single place to do it.  Allocating a new digester per call to hashCode() seems expensive (although I have not benchmarked this).\n\nMapReduce does use the hash function by default for partitioning, but HashMaps will also use it, so MapReduce is not the only client.", "I just committed this as r386219.", "Doug committed this."], "derived": {"summary": "I needed a binary key type, so I extended BytesWritable to be comparable also.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "binary key - I needed a binary key type, so I extended BytesWritable to be comparable also."}, {"q": "What updates or decisions were made in the discussion?", "a": "Doug committed this."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-81", "title": "speculative execution is only controllable from the default config", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-03-15T15:26:04.000+0000", "updated": "2009-07-08T16:51:41.000+0000", "description": "The application's JobConf is not consulted when checking whether speculative execution should be used.", "comments": ["This patch does:\n\nChanges the JobInProgress and TaskInProgress to keep the JobConf, which is the job specific configuration,\nrather than the system-wide configuration.\nAdds set/get speculative execution methods to the JobConf, so that applications don't need to get the string literal right.\nOn a side note, I also made all of the fields in TaskInProgress private rather than package-local.", "I committed this in 386224.  Thanks, Owen!"], "derived": {"summary": "The application's JobConf is not consulted when checking whether speculative execution should be used.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "speculative execution is only controllable from the default config - The application's JobConf is not consulted when checking whether speculative execution should be used."}, {"q": "What updates or decisions were made in the discussion?", "a": "I committed this in 386224.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-82", "title": "JobTracker loses it: NoSuchElementException", "status": "Closed", "priority": "Minor", "reporter": "Michael Stack", "assignee": null, "labels": [], "created": "2006-03-16T01:15:11.000+0000", "updated": "2006-08-03T17:46:31.000+0000", "description": "On a number of occasions, JobTracker goes into a loop that it never recovers from.  Over and over it prints the below to the jobtracker log.\n\n060304 124522 Server handler 5 on 8010 call error: java.io.IOException: java.util.NoSuchElementException\njava.io.IOException: java.util.NoSuchElementException\n   at java.util.TreeMap.key(TreeMap.java:433)\n   at java.util.TreeMap.firstKey(TreeMap.java:287)\n   at java.util.TreeSet.first(TreeSet.java:407)\n   at org.apache.hadoop.mapred.TaskInProgress.getTaskToRun(TaskInProgress.java:428Timed out.org.apache.hadoop.fs.ChecksumException: Checksum error:/2/hadoop/nara/data/tmp/task_r_m80hob/all.1 at 1554810368 at\norg.apache.hadoop.fs.FSDataInputStream$Checker.verifySum(FSDataInputStream.java:122)\nat org.apache.hadoop.fs.FSDataInputStream$Checker.read(FSDataInputStream.java:98)\nat org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:158)\nat java.io.BufferedInputStream.fill(BufferedInputStream.java:218) at\njava.io.BufferedInputStream.read(BufferedInputStream.java:235) at\norg.apache.hadoop.fs.FSDataInputStream$Buffer.read(FSDataInputStream.java:210)\nat java.io.DataInputStream.readInt(DataInputStream.java:353) at\norg.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:367) at\norg.apache.hadoop.io.SequenceFile$Sorter$SortPass.run(SequenceFile.java:557)\nat org.apache.hadoop.io.SequenceFile$Sorter.sortPass(SequenceFile.java:523)\nat org.apache.hadoop.io.SequenceFile$Sorter.sort(SequenceFile.java:511)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:254) at\norg.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:666)\n\nI added debug to TIP#getTaskToRun so i could tell which TIP had empted its allotment of tasks.  Below is extract from jobtracker log that shows sequence of events for TIP tip_fizr7m that lead up to JT losing it:\n\n060314 203637 Adding task 'task_m_4d6ht0' to tip tip_fizr7m, for tracker 'tracker_41791' on ia109314.archive.org \n060314 204758 Task 'task_m_4d6ht0' has been lost.\n060314 204811 Adding task 'task_m_fb0wf0' to tip tip_fizr7m, for tracker 'tracker_70065' on ia109314.archive.org\n060314 210118 Task 'task_m_fb0wf0' has been lost.\n060314 210119 Adding task 'task_m_irar47' to tip tip_fizr7m, for tracker 'tracker_82285' on ia109324.archive.org\n060314 211541 Taskid 'task_m_irar47' has finished successfully.\n060314 211541 Task 'task_m_irar47' has completed.\n060314 211543 Adding task 'task_m_qo1g69' to tip tip_fizr7m, for tracker 'tracker_97839' on ia109306.archive.org\n060314 213004 Taskid 'task_m_qo1g69' has finished successfully.\n060314 213004 Task 'task_m_qo1g69' has completed.\n060314 213005 Adding task 'task_m_t0lnzk' to tip tip_fizr7m, for tracker 'tracker_57273' on ia109314.archive.org\n060314 214118 Task 'task_m_t0lnzk' has been lost.\n\nSo, we lose two, complete two, then lose a third.\n\nTIP should have been done on first completion.\n\nTIP accounting is off.\n\n\n", "comments": ["Patch that stops completes going < 0.", "I just committed this.  Thanks, Michael!"], "derived": {"summary": "On a number of occasions, JobTracker goes into a loop that it never recovers from. Over and over it prints the below to the jobtracker log.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "JobTracker loses it: NoSuchElementException - On a number of occasions, JobTracker goes into a loop that it never recovers from. Over and over it prints the below to the jobtracker log."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Michael!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-83", "title": "infinite retries accessing a missing block", "status": "Closed", "priority": "Major", "reporter": "Yoram Arnon", "assignee": "Konstantin Shvachko", "labels": [], "created": "2006-03-16T02:59:24.000+0000", "updated": "2009-07-08T16:41:50.000+0000", "description": "A file in the DFS got corrupted - the reason for that is unknown, but might be justified.\n\nwhen accessing the file, I get an infinite stream of error messages from the client - attached below.\nThe client aparently increments an error counter, but never checks it.\n\nCorrect behaviour is for the client to retry a few times, then abort.\n\n\n060315 105436 No node available for block blk_2690692619196463439\n060315 105436 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block\n060315 105446 No node available for block blk_2690692619196463439\n060315 105446 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block\n060315 105456 No node available for block blk_2690692619196463439\n060315 105456 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block\n060315 105506 No node available for block blk_2690692619196463439\n060315 105506 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block\n060315 105516 No node available for block blk_2690692619196463439\n060315 105516 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block\n060315 105526 No node available for block blk_2690692619196463439\n060315 105526 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block\n060315 105536 No node available for block blk_2690692619196463439\n060315 105536 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block\n060315 105546 No node available for block blk_2690692619196463439\n060315 105546 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block\n060315 105556 No node available for block blk_2690692619196463439\n060315 105556 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block\n060315 105606 No node available for block blk_2690692619196463439\n060315 105606 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block\n060315 105616 No node available for block blk_2690692619196463439\n060315 105616 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block\n060315 105626 No node available for block blk_2690692619196463439\n060315 105626 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block\n060315 105636 No node available for block blk_2690692619196463439\n060315 105636 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block\n060315 105646 No node available for block blk_2690692619196463439\n060315 105646 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block\n060315 105657 No node available for block blk_2690692619196463439\n060315 105657 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block\n060315 105707 No node available for block blk_2690692619196463439\n060315 105707 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block\n060315 105717 No node available for block blk_2690692619196463439\n060315 105717 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block\n060315 105727 No node available for block blk_2690692619196463439\n060315 105727 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block\n060315 105737 No node available for block blk_2690692619196463439\n060315 105737 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block\n060315 105747 No node available for block blk_2690692619196463439\n060315 105747 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block\n060315 105757 No node available for block blk_2690692619196463439\n060315 105757 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block\n060315 105807 No node available for block blk_2690692619196463439\n060315 105807 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block\n060315 105817 No node available for block blk_2690692619196463439\n", "comments": ["In order to prevent the client from looping infinitely retrying to locate missing block the following line\n-                   /**\n                    if (failures >= MAX_BLOCK_ACQUIRE_FAILURES) {\n                        throw new IOException(\"Could not obtain block \" + blocks[targetBlock]);\n                    }\n-                   **/\nin DFSClient.blockSeekTo(long) should be uncommented.\nIt is also desired to reduce the value of the MAX_BLOCK_ACQUIRE_FAILURES constant\nwhich is set to 10 now. In current settings it will take at least 100 seconds to detect missing block.\nSo additionally to uncommenting the lines above I'd set\n-   static int MAX_BLOCK_ACQUIRE_FAILURES = 10;\n+   static int MAX_BLOCK_ACQUIRE_FAILURES = 3;\nand reduce sleeping time between retries to 3 seconds from current 10\n                    LOG.info(\"Could not obtain block from any node:  \" + ie);\n                    try {\n-                       Thread.sleep(10000);\n+                       Thread.sleep(3000);\n                    } catch (InterruptedException iex) {\n                    }\n", "Okay, I've commited this.\n\nNote that this situation can occur while a dfs system is starting up.  If one lanuches the namenode prior to the datanodes then the namenode will know about files but not yet know wherre their blocks are, until the datanodes check in for the first time.  I recently changed the bin/start-all.sh script to start the namenode first, so that datanodes would not all initially report problems connecting to the namenode.  I knew there was a reason that it was done in the other order, but could not remember what the reason was.  Now I remember!  I switched that back and added a comment.\n\nAlso, next time, can you please attach a patch file, so that I don't have to manually make your suggested changes?  Thanks!\n"], "derived": {"summary": "A file in the DFS got corrupted - the reason for that is unknown, but might be justified. when accessing the file, I get an infinite stream of error messages from the client - attached below.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "infinite retries accessing a missing block - A file in the DFS got corrupted - the reason for that is unknown, but might be justified. when accessing the file, I get an infinite stream of error messages from the client - attached below."}, {"q": "What updates or decisions were made in the discussion?", "a": "Okay, I've commited this.\n\nNote that this situation can occur while a dfs system is starting up.  If one lanuches the namenode prior to the datanodes then the namenode will know about files but not yet know wherre their blocks are, until the datanodes check in for the first time.  I recently changed the bin/start-all.sh script to start the namenode first, so that datanodes would not all initially report problems connecting to the namenode.  I knew there was a reason that it was done in the other order, but could not remember what the reason was.  Now I remember!  I switched that back and added a comment.\n\nAlso, next time, can you please attach a patch file, so that I don't have to manually make your suggested changes?  Thanks!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-84", "title": "client should report file name in which IO exception occurs", "status": "Closed", "priority": "Minor", "reporter": "Yoram Arnon", "assignee": "Konstantin Shvachko", "labels": [], "created": "2006-03-16T03:01:31.000+0000", "updated": "2009-07-08T16:41:49.000+0000", "description": "A file in the DFS got corrupted somehow.\nThe client gets an exception accessing a block in the file:\n\n060315 105907 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block\n060315 105917 No node available for block blk_2690692619196463439\n\nIt could (and should) report the file in which the error occurred, together with the block information.", "comments": ["This patch does file name and offset reporting for the failed block.\nI also removed 2 unused variables.\nShould be ok to include into the release.", "I just committed this. I removed the unused code rather than just commenting it out, and also removed some spurious empty comments inserted by the patch.\n\nThanks, Konstantin!"], "derived": {"summary": "A file in the DFS got corrupted somehow. The client gets an exception accessing a block in the file:\n\n060315 105907 Could not obtain block from any node:  java.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "client should report file name in which IO exception occurs - A file in the DFS got corrupted somehow. The client gets an exception accessing a block in the file:\n\n060315 105907 Could not obtain block from any node:  java."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this. I removed the unused code rather than just commenting it out, and also removed some spurious empty comments inserted by the patch.\n\nThanks, Konstantin!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-85", "title": "a single client stuck in a loop blocks all clients on same machine", "status": "Closed", "priority": "Major", "reporter": "Yoram Arnon", "assignee": "Sameer Paranjpye", "labels": [], "created": "2006-03-16T05:44:09.000+0000", "updated": "2009-07-08T16:41:49.000+0000", "description": "I was running a set of clients, running cp from one dfs to another dfs using fuse, one file per cp process.\nOne client got stuck in a loop because of a bad block in one of the files (separate bug filed).\nThe other clients, running in parallel in the background, all stopped making progress on their respective files.", "comments": ["Fixing HADOOP-83 resolves this issue.", "Fixing HADOOP-83 resolves this issue."], "derived": {"summary": "I was running a set of clients, running cp from one dfs to another dfs using fuse, one file per cp process. One client got stuck in a loop because of a bad block in one of the files (separate bug filed).", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "a single client stuck in a loop blocks all clients on same machine - I was running a set of clients, running cp from one dfs to another dfs using fuse, one file per cp process. One client got stuck in a loop because of a bad block in one of the files (separate bug filed)."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixing HADOOP-83 resolves this issue."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-86", "title": "If corrupted map outputs, reducers get stuck fetching forever", "status": "Closed", "priority": "Major", "reporter": "Michael Stack", "assignee": "Doug Cutting", "labels": [], "created": "2006-03-17T01:58:09.000+0000", "updated": "2006-08-03T17:46:32.000+0000", "description": "In our rack, there is a machine that reliably corrupts map output parts.  When reducers try to pickup the map output, Server#Handler checks the checksum, notices corruption, moves the bad map output part aside and throws a ChecksumException.  Undeterred, the reducer comes back again minutes later only this time it gets a FileNotFoundException out of Server#Handler (Because the part was moved aside).  And so it goes till the cows come home.\n\nDoug applied a patch that in map output  file, when it notices a fatal exception, it logs a severe error on the TaskTracker#LOG. Then in TT, if a severe logging has occurred, TT does a soft restart (TT stays up but closes down all services and then goes through init again).  This patch was committed (after I suggested it was working), only, later, I noticed the severe log flag is not cleared across TT restart so TT goes into a cycle of continuous restarts.  \n\nA further patch that clears the severe flag was posted to the list.  This improves things but has issues too in that on revival, the TT continues to be plagued by reducers looking for parts no longer available for a period of ten minutes or so until the JobTracker gets around to updating them about change in where to go get map outputs.  During this period, the TT gets restarted 5-10 times -- but eventually comes back on line (There may have been too much damage done during this period of flux making it so the job will fail).\n\nThis issue covers implementing a better solution.  \n\nSuggestions include having the TT stay down a period to avoid the incoming reducers or somehow examining the incoming reducer request, checking its list of tasks to see if it knows anything of the reducers' request and rejecting it with a non-severe error if not a map of the currently running TT.  A little birdie (named DC) suggests a better soln. is probably an addition to intertrackerprotocol so either the TT or the reducer updates JT when corrupted map output.", "comments": ["I just committed something that should help tremendously in fixing this.  One can now get the RPC server context in code that runs under a server.  This will let us, when we encounter an error in MapOutputFile, instead of using LOG.severe(), to get a pointer to the TaskTracker instance and call a method that will propagate the error back (over the InterTrackerProtocol) to the JobTracker, where it belongs.", "Here's a completely untested patch.  It does compile!\n\nI don't think we need to add a new method to the InterTrackerProtocol, rather we just need to get the failure propagated in the next heartbeat to the TaskTracker.\n", "Testing over w/e.", "+1 on patch.  It works for me.\n\nHere is failure over on a tasktracker:\n\n060318 190019 Moving bad file /0/hadoop/tmp/part-24.out/task_m_4eop89 to /0/bad_files/task_m_4eop89.-1553185447\n060318 190019 Can't read map output:/0/hadoop/tmp/part-24.out/task_m_4eop89\norg.apache.hadoop.fs.ChecksumException: Checksum error: /0/hadoop/tmp/part-24.out/task_m_4eop89 at 1598464\n    at org.apache.hadoop.fs.FSDataInputStream$Checker.verifySum(FSDataInputStream.java:122)\n    at org.apache.hadoop.fs.FSDataInputStream$Checker.read(FSDataInputStream.java:98)\n    at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:158)\n    at java.io.BufferedInputStream.read1(BufferedInputStream.java:254)\n    at java.io.BufferedInputStream.read(BufferedInputStream.java:313)\n    at java.io.DataInputStream.read(DataInputStream.java:80)\n    at org.apache.hadoop.mapred.MapOutputFile.write(MapOutputFile.java:129)\n    at org.apache.hadoop.io.ObjectWritable.writeObject(ObjectWritable.java:117)\n    at org.apache.hadoop.io.ObjectWritable.write(ObjectWritable.java:64)\n    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:229)\n060318 190019 Reporting output lost:task_m_4eop89\n\nReducers must be getting the message fast because after above  no one else comes looking for the corrupted part (No FileNotFoundExceptions in TT log).\n\nMeanwhile over on jobtracker I see following sequence....\n\n060318 184356 Adding task 'task_m_4eop89' to tip tip_ccbgb1, for tracker 'tracker_14754' on ia109307.archive.org\n...\n060318 184716 Taskid 'task_m_4eop89' has finished successfully.\n...\n060318 184716 Task 'task_m_4eop89' has completed.\n...\n060318 190028 Task 'task_m_4eop89' has been lost.\n...\n060318 190029 Adding task 'task_m_7mdx9h' to tip tip_ccbgb1, for tracker 'tracker_61554' on ia109333.archive.org\n...\n060318 190329 Taskid 'task_m_7mdx9h' has finished successfully.\n...\n060318 190329 Task 'task_m_7mdx9h' has completed.\n...\n\ni.e. Task completes successfully.  Subsequently a message comes in that its been lost and a new task is scheduled which in turn completes.\n\n", "I just committed the fix for this.  Thanks, Michael, for helping to debug it."], "derived": {"summary": "In our rack, there is a machine that reliably corrupts map output parts. When reducers try to pickup the map output, Server#Handler checks the checksum, notices corruption, moves the bad map output part aside and throws a ChecksumException.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "If corrupted map outputs, reducers get stuck fetching forever - In our rack, there is a machine that reliably corrupts map output parts. When reducers try to pickup the map output, Server#Handler checks the checksum, notices corruption, moves the bad map output part aside and throws a ChecksumException."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed the fix for this.  Thanks, Michael, for helping to debug it."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-87", "title": "SequenceFile performance degrades substantially compression is on and large values are encountered", "status": "Closed", "priority": "Major", "reporter": "Sameer Paranjpye", "assignee": "Doug Cutting", "labels": [], "created": "2006-03-17T06:49:48.000+0000", "updated": "2006-08-03T17:46:32.000+0000", "description": "The code snippet in quesiton is:\n\n     if (deflateValues) {\n        deflateIn.reset();\n        val.write(deflateIn);\n        deflater.reset();\n        deflater.setInput(deflateIn.getData(), 0, deflateIn.getLength());\n        deflater.finish();\n        while (!deflater.finished()) {\n          int count = deflater.deflate(deflateOut);\n          buffer.write(deflateOut, 0, count);\n        }\n      } else {\n  \nA couple of issues with this code:\n\n1. The value is serialized to the 'deflateIn' buffer which is an instance of 'DataOutputBuffer', this grows as large as needed to store the serialized value and stays as large as the largest serialized value encountered. If, for instance a stream has a single 8MB value followed by several 8KB values the size of the buffer stays at 8MB. The problem is that the *entire* 8MB buffer is always copied over the JNI boundary regardless of the size of the value. We've observed this over several runs where compression performance degrades by a couple of orders of magnitude when a very large value is encountered. Shrinking the buffer fixes the problem.\n\n2. Data is copied lots of times. First the value is serialized into 'deflateIn'. Second, the value is copied over the JNI boundary in *every* iteration of the while loop. Third, the compressed data is copied piecemeal into 'deflateOut'. Finally, it is appended to 'buffer'.\n\n\nProposed fix:\n\n1. Don't let big buffers persist. Allow 'deflateIn' to grow to a *persistent* maximum reasonable size, say 64KB. If a larger value is encountered, grow the buffer in order to process the value, then shrink it back to the maximum size. To do this, we add a 'reset' method which takes a buffer size.\n\n2. Don't use a loop to deflate. The maximum size of the output can be determined by 'maxOutputSize = inputSize * 1.01 + 12'. This is the maximum output size that zlib will produce. We allocate a large enough output buffer and compress everything in 1 pass. The output buffer, of course, needs to shrink as well.\n\n\n", "comments": ["There's a lot of commented-out code added by this patch.  Can you remove that, or is it important that it remain?  You also add a new public append() method, but nothing calls it outside of this file.  So it probably doesn't need to be public.  But it replicates a lot of the logic from another append() method.  Can't we somehow implement this with the old append method, or define the old public append method in terms of this new private method?  Replicating logic is not good.  Finally, there are some spurious whitespace changes in your patch.", "The commented out code is spurious and should be cut. As for the multiple append methods, it's hard to implement\nthe new append methods functionality in terms of the old method, the reverse is doable with a small modification.\nThe old append method expects takes a single array which contains both the key and value. So if the new methods signature was changed to:\n\nappend(byte[] key, int keystart, int keylen, byte[] val, int valstart, int vallen) \n\nthe old one could be implemented in terms of the new private one. I dislike methods with this many parameters but don't have a better idea.", "Doug, any issues with the new patch?", "Here's a simpler version.  This copies less and never passes large, unused buffers over JNI.  It doesn't require changes to any other APIs, nor does it assume that items are typically smaller than 64k.", "I just committed this.  I was unable to measure a performance difference, but it is less code and uses less memory, so it seems a safe change in any case.", "It is less code and memory and is more elegant than the other fix. We were unable to measure a performance difference either, although I'm at a loss to explain why. It looks like under the hood the streaming implementation is pretty close to the old implementation. \n\nInternally, the DeflaterOutputStream passes a bytearray to the Deflater and compresses it in small chunks of 512 bytes (compressed), making a JNI call for each such chunk. So, in theory we should see poor performance for large values.\n\nLarge unused buffers don't persist however, which is nice. All in all, it appears to work great.\n\n\n"], "derived": {"summary": "The code snippet in quesiton is:\n\n     if (deflateValues) {\n        deflateIn. reset();\n        val.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "SequenceFile performance degrades substantially compression is on and large values are encountered - The code snippet in quesiton is:\n\n     if (deflateValues) {\n        deflateIn. reset();\n        val."}, {"q": "What updates or decisions were made in the discussion?", "a": "It is less code and memory and is more elegant than the other fix. We were unable to measure a performance difference either, although I'm at a loss to explain why. It looks like under the hood the streaming implementation is pretty close to the old implementation. \n\nInternally, the DeflaterOutputStream passes a bytearray to the Deflater and compresses it in small chunks of 512 bytes (compressed), making a JNI call for each such chunk. So, in theory we should see poor performance for large values.\n\nLarge unused buffers don't persist however, which is nice. All in all, it appears to work great."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-88", "title": "Configuration: separate client config from server config (and from other-server config)", "status": "Closed", "priority": "Minor", "reporter": "Michel Tourn", "assignee": "Arun Murthy", "labels": [], "created": "2006-03-17T09:15:54.000+0000", "updated": "2007-10-03T18:06:12.000+0000", "description": "servers = JobTracker, NameNode, TaskTracker, DataNode\nclients =  runs JobClient (to submit MapReduce jobs), or runs DFSShell (to browse )\n\nServer machines are administered together.\nSo it is OK to have all server config together (esp file paths and network ports).\nThis is stored in hadoop-default.xml or hadoop-mycluster.xml\n\nClient machines:\nthere may be as many client machines as there are MapRed developers.\nthe temp space for DFS needs to be writable by the active user.\nSo it should be possible to select the client temp space directory for the machine and for the user.\n(The global /tmp is not an option as discussed elsewhere: partition may be full)\n\nCurrent situation: \nBoth the server and the clients have a copy of the server config: hadoop-default.xml\nBut the XML property  \"dfs.data.dir\" is being used as a LOCAL directory path \non both the server machines (Data nodes) and the client machines.\n\nEffect:\nException in thread \"main\" java.io.IOException: No valid local directories in property: dfs.data.dir\n at org.apache.hadoop.conf.Configuration.getFile(Configuration.java:286)\n at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.newBackupFile(DFSClient.java:560)\n ...\n at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:267)\n\n\nCurrent Workaround:\nOn the client use hadoop-site.xml to override dfs.data.dir\n\nOne proposed solution:\n\nFor the purpose of JobClient operations, use a different property in place of dfs.data.dir.\n(Ex: dfs.client.data.dir) \nOn the client, set this property in hadoop-site.xml so that it will override hadoop-default.xml \n\nAnother proposed solution:\n\nHandle the fact that the world is made of a federation of independant Hadoop systems.\nThey can talk to each other (as peers) but they are administered separately.\nEach Hadoop system should have its own separate XML config file.\nClients should be able to specify the Hadoop system they want to talk to.\nAn advantage is that clients can then easily sync their local copy of a given Hadoop system config:\n just pull its config file\n\nIn this view of the world, a Job client is also a kind of independant (serverless) Hadoop system\nIn this case the client config file may have its own dfs.data.dir, which is \nseparate from the dfs.data.dir in the server config file.\n\n", "comments": ["I'd also separate the dfs config from the map-reduce config - they have nothing in common, and dfs has a life of its own in that it could support apps other than map-reduce.\n\nTaking this one step further, I'd separate name node config from data node config, and job tracker config from task tracker config. While useful to have them all bunled up when running on a single node, they're typically running on distinct nodes in a real system, and definitely in different processes, so separate configs make sense.\n\nAs for the client config, it should be really really easy:\n config file, rather than directory\n config file can reside anywhere, including the same directory as the application, and can have any name\n no reliance on environment variables\n specify the config file on the command line (client -f <file>) allowing concurrent clients for multiple hadoop clusters\n as few knobs as possible, simple to configure manually", "This was fixed by HADOOP-785."], "derived": {"summary": "servers = JobTracker, NameNode, TaskTracker, DataNode\nclients =  runs JobClient (to submit MapReduce jobs), or runs DFSShell (to browse )\n\nServer machines are administered together. So it is OK to have all server config together (esp file paths and network ports).", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Configuration: separate client config from server config (and from other-server config) - servers = JobTracker, NameNode, TaskTracker, DataNode\nclients =  runs JobClient (to submit MapReduce jobs), or runs DFSShell (to browse )\n\nServer machines are administered together. So it is OK to have all server config together (esp file paths and network ports)."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was fixed by HADOOP-785."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-89", "title": "files are not visible until they are closed", "status": "Closed", "priority": "Critical", "reporter": "Yoram Arnon", "assignee": "Dhruba Borthakur", "labels": [], "created": "2006-03-17T10:16:35.000+0000", "updated": "2009-07-08T16:41:50.000+0000", "description": "the current behaviour, whereby a file is not visible until it is closed has several flaws,including:\n1. no practical way to know if a file/job is progressing\n2. no way to implement files that never close, such as log files\n3. failure to close a file results in loss of the file\n\nThe part of the file that's written should be visible.", "comments": ["This patch makes a file visible in the file system as soon as it is created by an application. However, the data blocks are associated with a file when the file gets closed.\n\nIf a DFS client A has created a file and is writing data to it, another DFS client will see the file size as zero until A closes the file.", "I see 2 problems with this patch:\n# It does not close any of the 3 issues raised in the description above.\nYou can see files in the ls, but you cannot see file progress and you still loose all accumulated data if the file is not closed.\n# Implementation-wise I would expect that when files are made visible we will get rid of the pending create structure, \nbut it is still there, so now you will have to synchronize pending create data with the corresponding inode.\n\nI'd propose to make files both visible and readable while they are still being created. Namely, we should let\nclients read portions of the file that have been written and replicated on data-nodes.", "This patch does not change APIs and disk formats. The idea is that files that are created by clients appear immediately in the namespace. Other clients can see the file and get its attributes. This is a big change in semantics and I would rather do this in the beginning of a release than towards the end of the release cycle. This is a step in implementing full-fledged \"appends\" for HDFS files (HADOOP-1700).\n\nRegarding \"removing pending creates\" structures, I agree that they should move into a new kind of inode. I can do it after you introduce the concept of class-hierarchy of inodes (HADOOP-1687).\n\nPlease let me know if the above sounds good to you and makes you agree that this patch is commit-able.\n\n\n", "1. Contents of new blocks that are appended to a file is visible to clients as soon as the datanode reports the block to the namenode. This means that data is visible to clients even when the block metadata is not yet persisted on disk. This approach lets us avoid a fs-transaction into the edit log for every new block allocation.\n\n2. The block allocation for a file is persisted in the edit log when the file is closed.\n\n3. A new API FSDataOutputStream.sync() allows an application to make data persistent on disk even before the file is closed. The invocation of this API causes a transaction to be logged into the edits log to record the blocks that are currently allocated to the file. An application that is recording data to a log file will periodically invoke this API to ensure that the contents of the log file persist even if the application dies before closing the file.\n\n4. The FsShell utility has a new command that is invoked as \"bin/hadoop dfs -tail [-f] <filename>\". When the \"-f\" option is used, the FsShell utility will periodically poll for changes to the filesize. When a filesize change is detected, it will re-open the file and will display the new contents that were added to the file.\n", "This patch implements 1, 2 and 4 of the above.", "merged patch with latest trunk.", "This patch does not apply anymore.\n\nWith HADOOP-1700 on the horizon does it make sense to introduce tail -f here?\nTail -f works until the client that is writing to the file is alive. If it dies before closing the file all information\nreported by tail -f does not exist in the system anymore. So in a way tail -f reports an illusive data that is not \nguaranteed to be present in the future.\nThis patch has a lot of important internal changes, like removing pendingCreates etc.\nBut introducing tail -f at this point may cause confusion among potential users.\n", "Merged patch with latest trunk. Konstantin's view was that user's might get confused if they can view data in a file that gets thrown out if the namenode restarts. Keeping with his view, i have removed the \"tail\" command from this patch.\n\nA follow-on patch will introduce a new API \"flush\" that will persist blocks before file is closed. ", "+1\nMy only concern is that in order to add or remove blocks you reallocate the array of blocks\nso that it's length always equals the number of blocks in the file.\nI tested this patch with small files (1 or 2 blocks) and did not see any degradation in performance.\nSince our average file size is 1.5 blocks this is good. \nIMO we should still test it for bigger files to make sure there is no slow-down.", "1. merged patch with latest trunk.\n2. Re-introduced the \"tail\" command from dfs shell.", "Marking it Patch Available to trigger automatic tests,", "> 2. Re-introduced the \"tail\" command from dfs shell.\n\nWhat is the point of reviews and discussions then?", "-1 overall.  Here are the results of testing the latest attachment \nhttp://issues.apache.org/jira/secure/attachment/12366109/tail4.patch\nagainst trunk revision r577010.\n\n    @author +1.  The patch does not contain any @author tags.\n\n    javadoc +1.  The javadoc tool did not generate any warning messages.\n\n    javac +1.  The applied patch does not generate any new compiler warnings.\n\n    findbugs +1.  The patch does not introduce any new Findbugs warnings.\n\n    core tests +1.  The patch passed core unit tests.\n\n    contrib tests -1.  The patch failed contrib unit tests.\n\nTest results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/785/testReport/\nFindbugs warnings: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/785/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/785/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/785/console\n\nThis message is automatically generated.", "The contrib test failure si nto related to this patch, being tracked by HADOOP-1924", "Dhruba convinced me that it is worth committing the patch with the tail functionality in it.\nThere are 2 types of failures that lead to a loss of entire file data. \n# The name-node  failure, and \n# the client failure.\n\nIf the name-node dies the length of an incomplete file will be set to 0, which correspond to the current behavior, when we just loose the entire file.\nIf the client dies the name-node automatically closes all files created by the client as long as it detects the client lease expiration.\nThe last one is the most common case of failure, and the code provides protection for from loosing data in the case.", "I just committed this.", "Integrated in Hadoop-Nightly #243 (See [http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/243/])", "This feature alters user-facing behavior. Until now, a file's existence was an indication that the task producing the file is complete. Please mark it such in release notes. Thanks.", "The CHANGES.txt file lists this issue under the section titled \"NEW FEATURES\". So, this should figure prominently in the release notes.", "It is actually HADOOP-1708 that introduced the feature that \"files appear in namespace as soon as they are created\". I changed the release notes (CHANGES.txt) to ensure that this change appears in the INCOMPATIBLE section rather than in NEW FEATURES section.", "That was what was decided in the last group meeting between you, me, Owen,\nSameer, etc. That's why I introduced the changes to FsShell.java.\n\n\n\n\n"], "derived": {"summary": "the current behaviour, whereby a file is not visible until it is closed has several flaws,including:\n1. no practical way to know if a file/job is progressing\n2.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "files are not visible until they are closed - the current behaviour, whereby a file is not visible until it is closed has several flaws,including:\n1. no practical way to know if a file/job is progressing\n2."}, {"q": "What updates or decisions were made in the discussion?", "a": "That was what was decided in the last group meeting between you, me, Owen,\nSameer, etc. That's why I introduced the changes to FsShell.java."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-90", "title": "DFS is succeptible to data loss in case of name node failure", "status": "Closed", "priority": "Major", "reporter": "Yoram Arnon", "assignee": "Konstantin Shvachko", "labels": [], "created": "2006-03-17T10:49:53.000+0000", "updated": "2009-07-08T16:41:49.000+0000", "description": "Currently, DFS name node stores its log and state in local files.\nThis has the disadvantage that a hardware failure of the name node causes a total data loss. \nSeveral approaches may be used to address this flaw:\n1. replicate the name server state files using copy or rsync once in a while, either manually or using a cron job.\n2. set up secondary name servers and a protocol whereby the primary updates the secondaries. In case of failure, a secondary can take over.\n3. store the state files as distributed, replicated files in the DFS itself. The difficulty is that it becomes a bootstrap problem, where the name node needs some information, typically stored in its state files, in order to read those same state files.\n\nsolution 1 is fine for non critical systems, but for systems that need to guarantee no data loss it's insufficient.\nSolutions 2 and 3 both seem valid; 3 seems more elegant in that it doesn't require an extra protocol, it leverages the DFS and allows any level of replication for robustness. Below is a proposition for  solution 3.\n\n1.\tThe name node, when it starts up, needs some basic information. That information is not large and can easily be stored in a single block of DFS. We hard code the block location, using block id 0. Block 0 will contain the list of blocks that contain the name node metadata - not the metadata itself (file names, servers, blocks etc), just the list of blocks that contain it. With a block identified by 8 bytes, and 32 MB blocks, we can fit 256K block id's in block 0. 256K blocks of 32MB each can hold 8TB of metadata, which can map a large enough file system, so a single block of block_ids is sufficient.\n2.\tThe name node writes his state basically the same way as now: log file plus occasional full state. DFS needs to change to commit changes to open files while allowing continued writing to them, or else the log file wouldn't be valid on name server failure, before the file is closed. \n3.\tThe name node will use double buffering for its state, using blocks 0 and 1. Starting with block 0, it writes its state, then a log of changes. When it's time to write a new state it writes it to node 1. The state includes a generation number, a single byte starting at 0, to enable the name server to identify the valid state. A CRC is written at the end of the block to mark its validity and completeness. The log file is identified by the same generation number as the state it relates to. \n4.\tThe log file will be limited to a single block as well. When that block fills up a new state is written. 32MB of transaction logs should suffice. If not, we could set aside a set of blocks, and set aside a few locations in the super-block (block 0/1) to store that set of block ids.\n5.\tThe super-block, the log and the metadata blocks may be exposed as read only files in reserved files in the DFS: /.metadata/* or something.\n6.\tWhen a name nodes starts, it waits for data nodes to connect to it to report their blocks. It waits until it gets a report about blocks 0 and 1, from which it can continue to read its entire state. After that it continues normally.\n", "comments": ["I like the idea.  We have something very similar running in some of our other apps.\n\nI'm a bit concerned about using two blocks.  What if the name server only finds one of them?  I'd use block one to stage new versions and then support an operation to make it block zero \"atomically\".  Then there should be no confusion.  You still need to worry about generations of block zero though.\n\nInteresting problem space.", "adding functionality for atomic transfers of blocks is non trivial, and unnecessary in this case.\nboth block 0 and 1 are replicated as many times as required to achieve any desired level of reliability.\nthe generations part is straightforward - read both blocks, verify they're valid via their CRC. If just one is valid - select it, the other one hasn't been written successfully; if they're both valid, they'll have consecutive generation numbers (with rollover, so 0 follows 255), so select the larger one. Noteworty is the fact that a state file with its corresponding log file are completely equivalent to the next generation state file at the time it's created, so the name node is safe in any type of failure while writing the new state: until it's valid and safe the old state+log are there.", "\nSince we have virtually no 'metadata' for a file we could have a complete description of the filesystem by adding a couple of fields to org.apache.hadoop.dfs.Block and then have the DataNodes write small block info files to the same directory as the actual block data.\n\nI actually did this one day as an experiment.\n\nclass Block ...\n\n    long blkid;\n    long len;\n    UTF8 path;// the full path name of this block in the NameNode  \n    long position;// the starting position of this block in the file  \n\nThe NameNode can recover it's state by just waiting for the DataNodes to report the blocks they have.\n\nThe ugly part is what happens during a directory rename. Every DataNode needs to know that the blocks have changed and then re-write it. It could be a LOT of blocks.\n\nOne adds this (below) to FSNamesystem:\n\n   //\n    // Keeps a Vector for every named machine.  The Vector contains\n    // blocks that have recently been part of a rename and are thought to live\n    // on the machine in question.\n    //\n    TreeMap recentRenameSets = new TreeMap();\n\nThen NameNode.getBlockWork has this code at it's beginning:\n\n        // check to see of there are blocks that need to be renamed.\n        //\n        Block blocks[] = namesystem.blocksToRename(new UTF8(sender));\n        if (blocks != null) {\n        \tBlockCommand cmd = new BlockCommand(blocks);\n        \tcmd.setRenameBlocks();\n        \treturn cmd;\n        }\n\nI don't think I'm formally proposing this scheme, but I'd like to talk about it before I go off half cocked and write the wrong thing.\n\nWe are very concerned here about losing our DFS if something ever happens to the NameNode. I have the assignment to fix it, somehow, in the next couple of weeks.\n", "I thought about this for a while, and I think this is how I would attack the problem.\n\nFirst, I introduce the concept of a 'precious file' in dfs. A precious file is a file that can be recovered even if all NameNode state is lost. This is probably not a new object but just a boolean in FSDirectory.INode to indicate that the file is precious (and a boolean in FileUnderConstruction). \n\nA precious file is composed of PreciousBlock's, instead of Block's like a normal dfs file: \n\npublic class PreciousBlock extends Block {\n\t\n\tUTF8   pathName; // absolute dfs path\n\tlong   timestamp;// milliseconds since last change\n\tint    sequence; // which block this is. ie 0th, first, etc.\n\tint    total;    // count of blocks for the file\n\tshort  replication; \n\n\tpublic void write(DataOutput out) ...\n      public void readFields(DataInput in) ... etc.\n}\n\nWhenever a DataNode creates, or replicates, a block for a file that is precious it also serializes the PreciousBlock to a file in it's data directory(s).\n\nYou would see something like this in a datanode directory;\n\nblk_2241143806243395050\nblk_385073589724654571\nblk_8416156569406441156\nprecious_385073589724654571\n\nThis would mean that block #385073589724654571 is part of a precious file. The file precious_385073589724654571 contains a serialization of PreciousBlock.\n\nFiles stored this way can be recovered with out any NameNode state at all. The NameNode could simply wait until the DataNodes report their blocks and reconstruct all the precious files. If a precious file is widely replicated it becomes almost impossible to ever lose. The purpose of the timestamp is for the case where a file is renamed, or deleted and recreated, and a datanode didn't get the message.\n\nThe next part is easy. The NameNode just writes its image and edits to precious files in the dfs. The NameNode can easily keep several old versions of its state, or any other good tricks. Old NameNode images could be rotated like log files, etc. \n\nRecovery from loss, or corruption, of NameNode state is relatively simple. The NameNode recovers the precious files and then just reads the latest reliable image/edits. \n", "\nI changed my mind. Now I'm looking for the smallest possible solution.\n\nI just want a config to enable writing the edits to more than one place. The default config works exactly the same way as the existin code.\n\nAll the other details of backing up the NameNode image and edits can be handled outside of the code.\n\nHere's the patch. \n ", "seems like all the extra copies will be on the same node, right?\nso if it dies, so does the filesystem...\nperhaps the bug should be cloned, with the patch resolving a bug, but leaving the current bug open until it's more fully addressed.", "The low-tech thing I've done that's saved me when the namenode dies is simply to have a cron entry that rsyncs the namenode's files to another node every few minutes.  If the namenode dies, you lose at most a few minutes of computation.  It's not perfect, and its not a long-term solution, but it is a pretty effective workaround in my experience.", "I've done the same, alternating between two backup nodes.\nit's band-aid, until a real solution is devised.", "What we plan is to securely backup the 'snapshot' (the image file) after the NameNode is started. So that takes care of that part.  Now it's all about saving the edit log in a safe way. I don't like the idea of the backup being even one minute old.\n\nThe failure mode is the loss of a hard drive on the NameNode server. Simply writing, ALL the time, the edit log to more than one path, and therefore to more than one drive, goes a long way towards making the data secure. If the node dies you would still have to retreive one of the edit files from one of the drives, but at least one of the drives should still work (there are 3 on our namenode). Someone needs to pull the drives and mount them on another machine before recovery can happen, but hey, it's going to be a rare event.\n\nIf you are using Solaris, as we are, then sun nfs is available (or so they tell me). We mount an nfs drive, and write the edit log to that drive also. In this case recovery can happen by copying the image, and the edits, from nfs to another node, changing DNS for the name of the namenode, and starting a new namenode. \n\nI feel, at least for us, that this IS a real solution, and not just a band-aid. ", "Alan,  I didn't mean to argue that cron+rsync was an ideal solution.  Yes, streaming the edits to multiple locations is the plan.  I personally don't think we should rely on NFS, but rather we should develop our own network protocol and service by which edits are sync'd, but I might be convinced otherwise.", "Attached a patch that allows multiple locations to store the filesystem image and the edits log. Thus, dfs.name.dir can now contain multiple directories separated by comma.\nUpon startup, the namenode scans all provided locations, and determines the most recent image and the most recent edits log. Recents all images, and writes the new image in all the locations, alongwith a timestamp file.\nNamenode format command allows one to selectively format among all the specified locations.\nOne could use a more reliable disks (including NFS mounted disk) to store namesystem state.\n", "Patch submitted.", "I just committed this.  Thanks, Milind!\n"], "derived": {"summary": "Currently, DFS name node stores its log and state in local files. This has the disadvantage that a hardware failure of the name node causes a total data loss.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "DFS is succeptible to data loss in case of name node failure - Currently, DFS name node stores its log and state in local files. This has the disadvantage that a hardware failure of the name node causes a total data loss."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Milind!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-92", "title": "Error Reporting/logging in MapReduce", "status": "Closed", "priority": "Minor", "reporter": "Mahadev Konar", "assignee": "Mahadev Konar", "labels": [], "created": "2006-03-17T12:48:15.000+0000", "updated": "2009-07-08T16:51:41.000+0000", "description": "Currently Mapreduce does not tell you which machine failed to execute the task. Also, it would be nice to have features wherein there is a log report with each job, saying the number of tasks it ran (reporting which one failed and on which machine, listing any error information it can)  with  the start/end/execute time of each task. ", "comments": ["specifically, I'm hoping that we can create a well known file in the output directory with a log of all interesting job output.\n\nPerhaps we should put it in a subdirectory /INFO/ of the output directory, so that when the directory is used for future input, it is not confused with data?  This will also allow us to add other things later.", "It could be done with a logs directory in output_dir. So, the output_dir/logs/tasks/  would contain all the information per task (a file per task). This file would contain information like -- machine name, start time, end time, result status, error messages. Also, there should be a file per job  output_dir/logs/job_id.log. This file would contains job specific data -- no of mapreduce tasks that were running at some time, number of machines it was running on, and the start time and end time of the job. This log could also contain information on which of the task failed, so that the user can take a look at the respective task log to know why it failed.", "Another approach to this would be to expose this through the JobClient API.  Job-related events can be reported to the job client.  Events can be queued in the job tracker and the JobClient can retrieve them as it polls for job status.  Then the JobClient can decide where to log them.  By default they can be logged to standard error.\n\nThe events I think one might care about are:\n  - task start (task_id, type & host)\n  - task completion (task_id)\n  - task failure (task_id, error message)\n\nThe jobtracker already tracks most of this, so I don't think this places a huge new burden on the jobtracker.\n\nI don't like polluting the job's output directory with log data since it would require changes to the InputFormat implementations and other code to make them skip this specially named sub-directory (unless the name begins with a dot, which the fs code already ignores).\n\nIn any case, we could add an option to JobClient to log to the job's output fs.\n", "here is patch that reports the machine on which a specific task failed. Clicking on the jobid takes you to a page where  the information is displayed as before (with a little more parsing). But each of the task's is made clickable to show all the attempts that were made to execute this task. On clikcing on this task, you can see the machine on which a particular task attempt was executed and what the error was.", "I am including a patch that improves upon the job tracker interface. The tipid is made clickable so that it shows all the attempts for a particular task with the machine name. I found it useful, so am uplodaing it again.", "I took this patch for a spin.  +1 on commit.", "I've been using it today too. +1 on commit.", "This mostly looks good.  I committed it with the following changes:\n\n1. TaskStatus is not a public class, so a public method in another public class should not return it.  For now, I just made the method package-private, since the jsp pages are compiled in the same package.  Longer-term we should probably copy this information into the TaskReport, the public version of a TaskStatus, or something.  This information should be available to other applications through a public API, and through an RPC in the JobSubmissionProtocol.\n\n2. I renamed JobTracker.getallTaskStatus to be getTaskStatuses, like the JobInProgress method, and also using correct camel-case.\n\n3. Rather than add a new TaskStatus contructor, leaving an old one that is no longer called, I removed the old one.  We don't need dead code around.  This is package-private, so we can be sure that no code outside of this package uses the old constructor.\n\n4. The patch removed some inter-method whitespace.  I restored it.\n"], "derived": {"summary": "Currently Mapreduce does not tell you which machine failed to execute the task. Also, it would be nice to have features wherein there is a log report with each job, saying the number of tasks it ran (reporting which one failed and on which machine, listing any error information it can)  with  the start/end/execute time of each task.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Error Reporting/logging in MapReduce - Currently Mapreduce does not tell you which machine failed to execute the task. Also, it would be nice to have features wherein there is a log report with each job, saying the number of tasks it ran (reporting which one failed and on which machine, listing any error information it can)  with  the start/end/execute time of each task."}, {"q": "What updates or decisions were made in the discussion?", "a": "This mostly looks good.  I committed it with the following changes:\n\n1. TaskStatus is not a public class, so a public method in another public class should not return it.  For now, I just made the method package-private, since the jsp pages are compiled in the same package.  Longer-term we should probably copy this information into the TaskReport, the public version of a TaskStatus, or something.  This information should be available to other applications through a public API, and through an RPC in the JobSubmissionProtocol.\n\n2. I renamed JobTracker.getallTaskStatus to be getTaskStatuses, like the JobInProgress method, and also using correct camel-case.\n\n3. Rather than add a new TaskStatus contructor, leaving an old one that is no longer called, I removed the old one.  We don't need dead code around.  This is package-private, so we can be sure that no code outside of this package uses the old constructor.\n\n4. The patch removed some inter-method whitespace.  I restored it."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-93", "title": "allow minimum split size configurable", "status": "Closed", "priority": "Major", "reporter": "Hairong Kuang", "assignee": "Doug Cutting", "labels": [], "created": "2006-03-18T03:59:07.000+0000", "updated": "2009-07-08T16:51:41.000+0000", "description": "The current default split size is the size of a block (32M) and a SequenceFile sets it to be SequenceFile.SYNC_INTERVAL(2K). We currently have a Map/Reduce application working on crawled docuements. Its input data consists of 356 sequence files, each of which is of a size around 30G. A jobtracker takes forever to launch the job because it needs to generate 356*30G/2K map tasks!\n\nThe proposed solution is to let the minimum split size configurable so that the programmer can control the number of tasks to generate.", "comments": ["With such big input files the default logic should split things into dfs block-sized splits.  Smaller splits should only be used if this would result in fewer than mapred.map.tasks splits.  What value do you have for mapred.map.tasks in your mapred-default.xml?  Let's make sure that is working before we add a new min.split.size feature.  I don't oppose the feature, but it should be generating 356*30G/32M splits, not 356*30G/2K splits as you claim.  That's still a lot of splits.  If it is too many then we should add the feature you're adding.\n\nNote that, as a workaround, it is also easy to implement this w/o patching by defining an InputFormat that subclasses InputFormatBase and specifies a different minSplitSize.  But making that a long is a good idea.\n\nSo, in summary, can you please confirm that the actual number of splits that you object to is 356*30G/32M splits, not 356*30G/2K?  Thanks.", "From what I've seen, it is always 32M fragments, but that is still 300k input splits/maps, which is a lot. We'd like to be able to drop that by an order of magnitude. (I think in this case that the input splitter never finished, so we don't know.)", "Doug, you are right. The number of splits we got was 356*30G/32M, but still too many.", "Updated patch", "Okay, I have applied this.\n\nFor the record, patches are easier to apply if they are made from the root of the project.  Also, new config properties should generally be added to hadoop-default.xml.  Finally, the cast added in SequenceFileInputFormat was not required."], "derived": {"summary": "The current default split size is the size of a block (32M) and a SequenceFile sets it to be SequenceFile. SYNC_INTERVAL(2K).", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "allow minimum split size configurable - The current default split size is the size of a block (32M) and a SequenceFile sets it to be SequenceFile. SYNC_INTERVAL(2K)."}, {"q": "What updates or decisions were made in the discussion?", "a": "Okay, I have applied this.\n\nFor the record, patches are easier to apply if they are made from the root of the project.  Also, new config properties should generally be added to hadoop-default.xml.  Finally, the cast added in SequenceFileInputFormat was not required."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-94", "title": "disallow more than one datanode running on one computing sharing the same data directory", "status": "Closed", "priority": "Major", "reporter": "Hairong Kuang", "assignee": null, "labels": [], "created": "2006-03-18T09:24:31.000+0000", "updated": "2009-07-08T16:41:50.000+0000", "description": "Currently dfs disallows more one datanode to run on the same computer if they are started up using the same hadoop conf dir. However, this does not prevent more than one data node gets started, each using a different conf dir (strickly speaking, a different pid file). If every machine has two such datanodes running, namenode will be busy on deleting and replicating blocks or eventually lead to block loss.\n\nSuggested solution: put pid file in  the data directory and disallow configuration.\n\n", "comments": ["I agree that we should put some sort of a lock file in the data directory, but I don't think we should move the existing pid file, since that is managed by the generic daemon start/stop code.  Rather we could use the nio file locking code to create an exclusive lock on a file.  This will automatically be unlocked by the kernel if/when the jvm exits."], "derived": {"summary": "Currently dfs disallows more one datanode to run on the same computer if they are started up using the same hadoop conf dir. However, this does not prevent more than one data node gets started, each using a different conf dir (strickly speaking, a different pid file).", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "disallow more than one datanode running on one computing sharing the same data directory - Currently dfs disallows more one datanode to run on the same computer if they are started up using the same hadoop conf dir. However, this does not prevent more than one data node gets started, each using a different conf dir (strickly speaking, a different pid file)."}, {"q": "What updates or decisions were made in the discussion?", "a": "I agree that we should put some sort of a lock file in the data directory, but I don't think we should move the existing pid file, since that is managed by the generic daemon start/stop code.  Rather we could use the nio file locking code to create an exclusive lock on a file.  This will automatically be unlocked by the kernel if/when the jvm exits."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-95", "title": "dfs validation", "status": "Closed", "priority": "Major", "reporter": "Hairong Kuang", "assignee": null, "labels": [], "created": "2006-03-18T09:31:35.000+0000", "updated": "2009-07-08T16:41:49.000+0000", "description": "Dfs needs a validation operation similiar to fsck, so that we get to know the files that are corrupted and which data blocks are missing.\n\nDfs namenode also needs to log more specific information such as which block is replication or is deleted. So when something goes wrong, we have a clue what has happened.", "comments": ["I posted a similar complaint in HADOOP-35, including a patch to make it possible to report on individual file health from the dfs -ls command. I agree that something needs to be added, if only to facilitate debugging of the various block-loss problems I've been seeing.", "we're thinking to have a name-server call that:\n1. checks internal consistency\n2. for each block, verifies that it actually exists on some data node\n3. (optional, perhaps later) ask each data node to actually read its blocks and check their validity", "I vote for a dedicated thread on each name node using 1% of disk IO to validate blocks continuously.  Low cost and uncovering bad blocks is high value.  This would also let you collect disk stats reliability to determine if the disks were bad.", "This map-reduce test reads all blocks of all files, and detects which of them are missing or corrupted.\nSee HADOOP-101 discussion.\n", "Posting here a similar performance test, which is related to HADOOP-72\nJust because these two tests have too much in common.\nTestDFSIO measures performance of the cluster for reads and writes.", "Is this issue still pertinent? We have an fsck utility and TestDFSIO has been committed via a separate JIRA issue.", "I think Eric's comment (http://issues.apache.org/jira/browse/HADOOP-95#action_12371188) is still important. But, probably a new issue to bring up. Ongoing validation/balancing/etc. are still problems - the \"fsck\" utility just covers the top-level problem.", "This was fixed by HADOOP-101"], "derived": {"summary": "Dfs needs a validation operation similiar to fsck, so that we get to know the files that are corrupted and which data blocks are missing. Dfs namenode also needs to log more specific information such as which block is replication or is deleted.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "dfs validation - Dfs needs a validation operation similiar to fsck, so that we get to know the files that are corrupted and which data blocks are missing. Dfs namenode also needs to log more specific information such as which block is replication or is deleted."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was fixed by HADOOP-101"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-96", "title": "name server should log decisions that affect data: block creation, removal, replication", "status": "Closed", "priority": "Critical", "reporter": "Yoram Arnon", "assignee": "Hairong Kuang", "labels": [], "created": "2006-03-22T01:09:50.000+0000", "updated": "2009-07-08T16:41:50.000+0000", "description": "currently, there's no way to analyze and debug DFS errors where blocks disapear.\nname server should log its decisions that affect data, including block creation, removal, replication:\n- block <b> created, assigned to datanodes A, B, ...\n- datanode A dead, block <b> underreplicated(1), replicating to datanode C\n- datanode B dead, block <b> underreplicated(2), replicating to datanode D\n- datanode A alive, block <b> overreplicated, removing from datanode D\n- block <removed> from datanodes C, D, ...\n\nthat will enable me to track down, two weeks later, a block that's missing from a file, and to debug the name server.\n\nextra credit:\n- rotate log file, as it might grow large\n- make this behaviour optional/configurable", "comments": ["the plan is to add a log line for each change in the name space and each change in block placement or replication. What we get is effectively a trace of program execution for DFS changes.\nthe log will go to a new log object, to enable switching this (extensive) logging on or off.\nname space changes will be logged at level fine, block commit changes at finer, and block pending changes at finest.\nIn order to facilitate tracing of multiple concurrent operations, each line will include the thread id of the name server's thread. For that we derive a logging class, that places the thread id right after the date/time.\n\nwe log in the following methods of class name node, and in methods of class nameSystem called by them:\ncreate (startFile)\nabandonFileInProgress (abandonFileInProgress )\nAbandonBlock (AbandonBlock )\nreportWrittenBlock (blockReceived)\naddBlock (getAdditionalBlock)\nComplete (completeFile)\nrename (renameTo)\ndelete (delete)\nMkdirs (Mkdirs)\nsendHeartbeat (getHeartbeat)\nblockReport (processReoprt)\nblockReceived (blockReceived)\nerrorReport\ngetBlockWork (pendingTransfer, blocksToInvalidate)\n", "This sounds like a great plan!\n\nHadoop's current log formatter has an option to log thread id's.  The namenode code can simply turn this on.   Also, one can configure the JVM to rotate logs using:\n\nhttp://java.sun.com/j2se/1.4.2/docs/api/java/util/logging/FileHandler.html\n\nIn any case, we should add a timestamp to the log file names generated by bin/hadoop-daemon.sh, where standard out and error are logged, regardless of JVM log configuration.", "OK.\nwe'll use the current log formatter, and just turn on the output of thread id when we're debugging.\n", "One thing that really helped us was to be able to specify the duration to keep log files.  So you could configure the system to keep up to N seconds of logs (think one month).  This way logs don't grow without bound, but you can be confident how much history will be available.  Also, logs gzip pretty well.  It would be nice to zip closed logs automatically.\n\nDon't know how much of this you can get for free from the existing logs packages.  Should investigate this.\n\nAre we just logging on the name node, or are data nodes logging all events too.  Seems like that would be desirable as well.  Using the same mechanisms of course.\n", "This patch adds the following features\n1. NameNode adds a static field \"stateChangeLog\" that keeps tracks of all the namenode state changes\n2. Various logging statements are added to NameNode, FSNamesystem, FSDirectory. Basically namesapce (dir) changes are logged at the fine level and block changes are logged at the finer level\n3. initFileHandler is added to LogFormatter. All logs are directed to a log file instead of stdout. All logs are rolled and capped in size. Log file names end with .log.\n4. In hadoop-daemon.sh, stdout is redirected to a .out file.\n5. namenode logging levels, log file max size, and number of generations are configurable.\n6. A JUnit test program is added to test the correctness of namespace change logging.", "Overall I like this.  A few nits, however:\n\n1. It would be nice if, before it switches logging to a file, the name of that file were logged.  That way, when folks upgrade they can figure out where there logs are written.  This will also help in debugging configuration issues.\n\n2. When I run bin/start-all.sh on my Linux box, the log files end up in my connected directory, in HADOOP_HOME, not in HADOOP_HOME/logs.  When I add a print statement, it shows the correct directory for logDir, but that's not where the files are written.  I have not tested this on Windows, but it would be good to check that it with a simple configuration (i.e., a hadoop-site.xml that only specifies localhost for the jobtracker and namenode) that, on Windows and Linux, the files are written where expected.  Perhaps we could even add a unit test for this.\n\nThanks!", "Also, one of Owen's patches has made applying this patch require manual steps.  So if you make a new patch, please first update your tree and merge with Owen's changes.  Thanks!", "Yes, it makes sense to log the log file name. I will make the change. If HADOOP_LOG_DIR is set, the log directory is set to be HADOOP_LOG_DIR. Otherwise, if HADOOP_HOME is set,  the log directory is set to be HADOOP_HOME/logs. Otherwise, it is set to be the user's home directory. I do not know why you saw log files in HADOOP_HOME. I will take a look.", "Here is patch that includes the changes that Doug suggested. I tested it on a RH linux machine, it worked fine. But it seems that it has a problem on cygwin in that File.exists() returns true for a nonexistent directory. Doug, could you test it? If I am able to figure out what went wrong on cygwin, I will resubmit the patch tomorrow.", "The Cygwin problem is solved.", "I just committed this.\n\nI made one additional change.  You removed the 'cd $HADOOP_HOME' line in hadoop-daemon.sh.  I re-added this to hadoop-daemons.sh, so that, when starting remote daemons, they are always run from HADOOP_HOME rather than the users home directory, which is more likely to be NFS mounted.  The CWD of a daemon is used for core dumps, java profiler output, etc. and it is generally best if it is not NFS mounted.\n\nThanks, Hairong!", "Hi Doug,\n\nThanks for commiting this patch.\n\nThe reason that I removed the line 'cd $HADOOP_HOME' is that I had difficulty starting hadoop from any directory except for \"HADOOP_HOME\". In my configuration, the log dir & pid dir are relative to \"HADOOP_HOME\".  But \"HADOOP_HOME\" is relative to the current directory \".\". If the script changes the current directory, it is not able to get the log dir and pid dir right.\n\nAn alternative fix is to set HADOOP_HOME to be its absolute path.", "I'm now having troubles with things under cygwin.  'bin/hadoop-daemon.sh start namenode' works, but 'bin/start-dfs.sh' silently fails to start any daemons.", "I figured out the problem under Cygwin & fixed it.  HADOOP_HOME wasn't quoted in hadoop-daemon.sh, so things failed when the path contained a space (as my HADOOP_HOME does under cygwin).  When testing under Cygwin one should always install in a path that contains a space to test these cases."], "derived": {"summary": "currently, there's no way to analyze and debug DFS errors where blocks disapear. name server should log its decisions that affect data, including block creation, removal, replication:\n- block <b> created, assigned to datanodes A, B,.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "name server should log decisions that affect data: block creation, removal, replication - currently, there's no way to analyze and debug DFS errors where blocks disapear. name server should log its decisions that affect data, including block creation, removal, replication:\n- block <b> created, assigned to datanodes A, B,."}, {"q": "What updates or decisions were made in the discussion?", "a": "I figured out the problem under Cygwin & fixed it.  HADOOP_HOME wasn't quoted in hadoop-daemon.sh, so things failed when the path contained a space (as my HADOOP_HOME does under cygwin).  When testing under Cygwin one should always install in a path that contains a space to test these cases."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-97", "title": "DFSShell.cat returns NullPointerException if file does not exist", "status": "Closed", "priority": "Major", "reporter": "Konstantin Shvachko", "assignee": "Konstantin Shvachko", "labels": [], "created": "2006-03-22T04:27:07.000+0000", "updated": "2009-07-08T16:41:50.000+0000", "description": "DFSShell.cat crashes with a NullPointerException if file to be displayed does not exist.\nThe bug is fixed in the attached patch, and the general exception handling in main() is added.", "comments": ["I just committed this.  I also changed the error message to print to standard error rather than standard output, and changed the exit code to be 1 when there's an error."], "derived": {"summary": "DFSShell. cat crashes with a NullPointerException if file to be displayed does not exist.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "DFSShell.cat returns NullPointerException if file does not exist - DFSShell. cat crashes with a NullPointerException if file to be displayed does not exist."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  I also changed the error message to print to standard error rather than standard output, and changed the exit code to be 1 when there's an error."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-98", "title": "The JobTracker's count of the number of running maps and reduces is wrong", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-03-23T16:13:39.000+0000", "updated": "2009-07-08T16:51:42.000+0000", "description": "When a heatbeat comes in from a task tracker, the job tracker just adds the number of currently running maps and reduces. The jobs from the previous heartbear are never subtracted. This causes the scheduling to misjudge the \"loading\" levels of the task trackers.", "comments": ["This patch consolidate all of the places where status was put in to or out of the taskTrackers status map, so that\nthe counts of maps and reduces are maintained consistently. \n\nI also added the cluster status to the webapp so that you can get a quick view of how busy the cluster is.", "This looks great.  I just committed it.  Thanks, Owen."], "derived": {"summary": "When a heatbeat comes in from a task tracker, the job tracker just adds the number of currently running maps and reduces. The jobs from the previous heartbear are never subtracted.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "The JobTracker's count of the number of running maps and reduces is wrong - When a heatbeat comes in from a task tracker, the job tracker just adds the number of currently running maps and reduces. The jobs from the previous heartbear are never subtracted."}, {"q": "What updates or decisions were made in the discussion?", "a": "This looks great.  I just committed it.  Thanks, Owen."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-99", "title": "task trackers can only be assigned one task every heartbeat", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-03-23T16:30:16.000+0000", "updated": "2009-07-08T16:51:42.000+0000", "description": "Task trackers only call pollForNewTask once per a heartbeat (10 seconds) rather than when a task finishes. Especially for quick maps this means that the cluster is under utilized, because each map finishes in a cycle and the task tracker never gets a second, third or fourth task to run.", "comments": ["This is a duplicate to Hadoop-305"], "derived": {"summary": "Task trackers only call pollForNewTask once per a heartbeat (10 seconds) rather than when a task finishes. Especially for quick maps this means that the cluster is under utilized, because each map finishes in a cycle and the task tracker never gets a second, third or fourth task to run.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "task trackers can only be assigned one task every heartbeat - Task trackers only call pollForNewTask once per a heartbeat (10 seconds) rather than when a task finishes. Especially for quick maps this means that the cluster is under utilized, because each map finishes in a cycle and the task tracker never gets a second, third or fourth task to run."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is a duplicate to Hadoop-305"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-100", "title": "Inconsistent locking of the JobTracker.taskTrackers field", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-03-24T01:08:27.000+0000", "updated": "2009-07-08T16:51:41.000+0000", "description": "The JobTracker is using an inconsistant lock for protecting taskTrackers, which is the list of current task trackers. Some of the routines lock the JobTracker and others lock the taskTrackers field.", "comments": ["I made the table private instead of package local.\n\nI also wrapped each of the references to the table with a lock on the table.\n\nThere is still a lingering issue with the webapp's use of JobTracker.taskTrackers(). If a status update comes in while the webapp is iterating through the taskTrackers, it will get a ConcurrentModificationException.", "I just committed this.  Thanks, Owen."], "derived": {"summary": "The JobTracker is using an inconsistant lock for protecting taskTrackers, which is the list of current task trackers. Some of the routines lock the JobTracker and others lock the taskTrackers field.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Inconsistent locking of the JobTracker.taskTrackers field - The JobTracker is using an inconsistant lock for protecting taskTrackers, which is the list of current task trackers. Some of the routines lock the JobTracker and others lock the taskTrackers field."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-101", "title": "DFSck - fsck-like utility for checking DFS volumes", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": "Andrzej Bialecki", "labels": [], "created": "2006-03-24T02:50:45.000+0000", "updated": "2009-10-13T21:25:35.000+0000", "description": "This is a utility to check health status of a DFS volume, and collect some additional statistics.", "comments": ["Looks like a good utility for collecting meta-data statistics.\n\nIt does not detect missing blocks, though.\nThe missing block problem is when the namenode \"thinks\" a block\nis stored on one or several datanodes, while it is not.\nIn order to detect this the utility needs to actually read blocks from\neach of the specified locations. Is there an API call for that?\nAnd it does not have an option to convert the system into a consistent state,\nwhich is usually provided by fscks.\n\nIt would be very useful to integrate this code with the DFSShell.report(),\nproviding more reporting options for the system.", "It does detect missing blocks, that's the whole point. However, you need to restart the namenode first to be sure you get the latest block reports from datanodes.\n\nCurrent failure modes for DFS involve blocks that are completely missing. The only way to \"fix\" them would be to recover chains of blocks and put them into lost+found - and here we could do better than fsck, because we know the file name they belong to.", "Oops, forgot to comment on another point you make: yes, it would be great if we could add an API call to request a datanode to check the status of a given block without sending it to the client, e..g read it physically off the disk and report success/failure. We can do this now only by actually retrieving the block, which is too costly.\n\nThis is the first version of the tool, any contributions are of course welcome.", "The dfsck is done at the client side and hence does not lock the file system. It has a potential inconsitency problem. What if a client deletes a directory when  tool is checking it?\n\nAnother concern is the performance. The tool needs to issue a RPC to the namenode for each dir/file in the system. Any benchmark?", "I think not locking the fs is a feature.  It should probably, if it gets file-not-found error from the namenode, backup and recheck to see if the file still exists, and, if it doesn't check whether its parent still exists, etc., attempting to ignore errors that are only the result of changes to the FS while checking it.  In the short-term, this tool is superior to anything else we currently have, and I'd vote for adding it as a 'bin/hadoop dfs -check' command.  As for performance, as a single-threaded client process it shouldn't be able to overwhelm the namenode.\n", "This is a good and awaited feature, filed previously as bug hadoop-95. I vote to check it in, because as you say, it's much better than anything we have, and of critical importance.\n\nRegarding performance, clearly the nameserver will not be overwhelmed, but the operation may take a very long time to execute. It's one thing to traverse a million entries in memory (for a modest 32TB FS), but another matter to execute a hundred thousand RPC calls from a single client. Also, when we change the open command to not return the entire list of blocks, in the interest of shortening the time of opening a file, especially when reading just a few blocks from a very large file, the implementation will need to change.\n\nLastly, there's extensibility. We'll want to test for things that are available only on the name server, like blocks that are not used by any file.\n\nWouldn't it be better to request the server to execute this code internally, and report results either to the client or to a local file?", "I like that this does not use anything more than the client API to check the server.  That keeps the server core lean and mean.  The use of RPC's effectively restricts the impact of the scan on the FS.\n\nA datanode operation that streams through a block without transferring it over the wire won't correctly check checksums using our existing mechanism.  To check file content we could instead simply implement a map-reduce job that streams through all the files in the fs.  This would not take much code: nothing additional in the core.  MapReduce should handle the locality, so that most data shouldn't go over the wire.\n\nBTW, blocks not used by any file are not known to the name node, are they?  When they're reported by a datanode the datanode is told to remove them.\n", "Wow, lots of comments, let me address some of them:\n\n* re: locking. I also see this as an advantage, fsck can run in parallel with normal operations. If someone else deletes a file, no big deal - the name is removed from the namesystem, so if we suddenly detect missing blocks we could always check if a file with this name still exists in the namesystem.\n\n* re: performance. Sure, we could parallelize this, which should speed things up (currently it's rather slow, checking ~1TB takes > 2 hours), but then it would put a higher load on the namenode. Perhaps we could make this an option, e.g. start a configurable pool of fsck threads in parallel.\n\n* re: blocks not in use by any file. I think this is already handled internally by namenode<->datanode protocol (for good and for bad), i.e. namenode detects orphaned blocks and tells datanodes to remove them. See FSNamesystem:924 .\n\n* handling the reverse situation (missing blocks in existing files) should be straightforward, with the use of /lost+found directory: for each corrupted file a directory would be created there, and remaining chains of consecutive blocks would be stored in that directory.\n\n* re: checking blocks through streaming: +1, I like the concept, could you perhaps implement it? ;) Also, what happens if a mapred task tries to retrieve a missing/corrupted block? I think currently this hangs the task, due to a missing break in the while loop in DFSClient:354", "Just posted a map-reduce test that checks all blocks of all files.\nSee HADOOP-95.\nThe infinite loop in DFSClient is fixed now, so it works.\n\nWith respect to some of the previous comments.\nRestarting the cluster (a big one) just to check its consistency is not an exciting option.\nThis means that we will have to wait up to 55 minutes before missing blocks will\nbe detected by examining just the namenode data.\n\nA drawback of the map-reduce test is that we cannot force the system to check all replicas \nof the block. So corrupted block is reported only if all of its replicas are bad.\nBut yes this is better than nothing.", "Regarding the restarting time - yes, but I couldn't find any other way to force datanodes to update their block reports with the namenode, perhaps we should extend BlockCommand to support this (the namenode can't call datanodes, it can just return BlockCommands when datanodes call in).", "Updated version. Added options to treat inconsistencies: ignore, move to /lost+found, delete.", "Andrzei, for your benchmark experiment (~1TB takes > 2 hours), how many directories and files were there in your dfs? For the performance of your dfsck, I think the number of files and dirs matters rather than the size of the data in dfs.", "There were around 40,000 directories, and 200,000 files.", "Added as 'bin/hadoop fsck'.", "Attaching test plan for Fsck."], "derived": {"summary": "This is a utility to check health status of a DFS volume, and collect some additional statistics.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "DFSck - fsck-like utility for checking DFS volumes - This is a utility to check health status of a DFS volume, and collect some additional statistics."}, {"q": "What updates or decisions were made in the discussion?", "a": "Attaching test plan for Fsck."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-102", "title": "Two identical consecutive loops in FSNamesystem.chooseTarget()", "status": "Closed", "priority": "Major", "reporter": "Konstantin Shvachko", "assignee": "Konstantin Shvachko", "labels": [], "created": "2006-03-24T10:03:45.000+0000", "updated": "2009-07-08T16:41:51.000+0000", "description": "I think this was meant to be the way I corrected.\nOtherwise there is no difference in two loops except for double\nbracketing in the if statement.", "comments": ["I just committed the patch.  Thanks, Konstantin."], "derived": {"summary": "I think this was meant to be the way I corrected. Otherwise there is no difference in two loops except for double\nbracketing in the if statement.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Two identical consecutive loops in FSNamesystem.chooseTarget() - I think this was meant to be the way I corrected. Otherwise there is no difference in two loops except for double\nbracketing in the if statement."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed the patch.  Thanks, Konstantin."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-103", "title": "introduce a common parent class for Mapper and Reducer", "status": "Closed", "priority": "Minor", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-03-25T06:07:04.000+0000", "updated": "2009-07-08T16:51:39.000+0000", "description": "I'd like to a base class that implements the default (empty) bodies for configure and close so that Mapper and Reducer classes that derive from it, which is optional, don't have to implement those methods.", "comments": ["This patch introduces a UserTask base class that will implement the trivial configure and close methods needed for Mapper and Reducer classes. It also modifies all of the examples and built-in map and reduce classes to use the new base class. (Note that this new base class is completely optional, user code does not need to change, but it removes a lot of null function bodies that would otherwise be needed.)", "Here is a new patch that renames the base class to MapReduceBase.\n", "I just committed this.  Thanks, Owen!"], "derived": {"summary": "I'd like to a base class that implements the default (empty) bodies for configure and close so that Mapper and Reducer classes that derive from it, which is optional, don't have to implement those methods.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "introduce a common parent class for Mapper and Reducer - I'd like to a base class that implements the default (empty) bodies for configure and close so that Mapper and Reducer classes that derive from it, which is optional, don't have to implement those methods."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-104", "title": "Reflexive access to non-public class with public ctor requires setAccessible (with some JVMs)", "status": "Closed", "priority": "Major", "reporter": "Michel Tourn", "assignee": "Hairong Kuang", "labels": [], "created": "2006-03-25T07:07:06.000+0000", "updated": "2006-09-13T03:53:24.000+0000", "description": "Multiple times I have hit this problem which prevents the NameNode from starting.\nThe only fix I had so far was to loose all my DFS data...\n\nException in thread \"main\" java.lang.RuntimeException: java.lang.IllegalAccessException: \nClass org.apache.hadoop.io.WritableFactories can not access a member of \nclass org.apache.hadoop.dfs.Block with modifiers \"public\"\n        at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:49)\n        at org.apache.hadoop.io.ArrayWritable.readFields(ArrayWritable.java:81)\n        at org.apache.hadoop.dfs.FSDirectory.loadFSEdits(FSDirectory.java:374)\n        at org.apache.hadoop.dfs.FSDirectory.loadFSImage(FSDirectory.java:347)\n        at org.apache.hadoop.dfs.FSDirectory.<init>(FSDirectory.java:258)\n        at org.apache.hadoop.dfs.FSNamesystem.<init>(FSNamesystem.java:151)\n        at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:82)\n\n\nAccording to this, \nhttp://forum.java.sun.com/thread.jspa?threadID=704100&messageID=4082902\nthis is a known issue when using\nreflexive access to a non-public class with public ctor \n(class org.apache.hadoop.dfs.Block is such a class)\n\nThis problem may not occur with all JVM releases. \n(I build on 1.5.0-b64 and run on 1.5.0_05-b05)\n\nThis problem only occured for me when \nI upgrade code or change XML configuration AND \nhave existing files in the DFS.\nThis problem does not occur when I just stop / restart the NameServer.\n\nIn any case, the attached patch fixes it by calling setAccessible\nbefore constructing the instance with reflection.\n\n\n", "comments": ["class Block registers a factory in its static initializer.\n\nSo another thing which may occur here is:\nclass loading/initialization order is non-deterministic.\n\nIf class Block loads before this is called:\n  WritableFactories.newInstance(Block.class)\nthen things work as expected, the Block factory is called.\n\nElse \nWritableFactories does not find a registered factory for Block (yet),\ntries to construct the instance reflectively,\nand fails as explained in the bug.\n\nIn this case an alternate fix is to make sure that all\nclasses that have a factory method declared as a static initializer\nare loaded before they are needed.\n"], "derived": {"summary": "Multiple times I have hit this problem which prevents the NameNode from starting. The only fix I had so far was to loose all my DFS data.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Reflexive access to non-public class with public ctor requires setAccessible (with some JVMs) - Multiple times I have hit this problem which prevents the NameNode from starting. The only fix I had so far was to loose all my DFS data."}, {"q": "What updates or decisions were made in the discussion?", "a": "class Block registers a factory in its static initializer.\n\nSo another thing which may occur here is:\nclass loading/initialization order is non-deterministic.\n\nIf class Block loads before this is called:\n  WritableFactories.newInstance(Block.class)\nthen things work as expected, the Block factory is called.\n\nElse \nWritableFactories does not find a registered factory for Block (yet),\ntries to construct the instance reflectively,\nand fails as explained in the bug.\n\nIn this case an alternate fix is to make sure that all\nclasses that have a factory method declared as a static initializer\nare loaded before they are needed."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-105", "title": "DFS commands either do not support some popular unix commands or have inconsistent behaviors", "status": "Closed", "priority": "Major", "reporter": "Runping Qi", "assignee": "Sameer Paranjpye", "labels": [], "created": "2006-03-25T09:50:17.000+0000", "updated": "2009-07-08T16:41:51.000+0000", "description": "Several issues.\n\n1. DFS commandline does not support rmdir\n2. When executing a command like dfs -rmdir PATH, it goes through silently, without warning that -rmdir is not supported\n3. DFS -rm PATH works even though PATH is a non-empty directory, which is inconsistent with unix convention.\n\n", "comments": [], "derived": {"summary": "Several issues. 1.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "DFS commands either do not support some popular unix commands or have inconsistent behaviors - Several issues. 1."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-106", "title": "Data blocks should be record-oriented.", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": "Sameer Paranjpye", "labels": [], "created": "2006-03-26T04:21:10.000+0000", "updated": "2011-06-19T19:48:13.000+0000", "description": "If data blocks were starting and ending on data record boundaries, and not in random places within a file, it would give some important advantages:\n\n* it would be possible to avoid \"fishing\" for the beginning of first record in a split (see SequenceFile.Reader.sync()).\n\n* it would make recovering from DFS errors much more successful and easier - in most cases missing blocks could be just skipped and the remaining parts combined together.", "comments": ["My intuition is it makes more sense to do this the other way around and have records aligned to blocks.  This keeps the FS implementation trivial.  Just pad near the end of a block.  This way you keep a good seperation of APIs too.  Fairly straight forward to change the record model to do that.  Only issues are with huge records.  You have a couple of options there.  The simplest is to disallow them...", "I agree with Eric. This is backwards. You want the sequence file to pad out to block boundaries."], "derived": {"summary": "If data blocks were starting and ending on data record boundaries, and not in random places within a file, it would give some important advantages:\n\n* it would be possible to avoid \"fishing\" for the beginning of first record in a split (see SequenceFile. Reader.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Data blocks should be record-oriented. - If data blocks were starting and ending on data record boundaries, and not in random places within a file, it would give some important advantages:\n\n* it would be possible to avoid \"fishing\" for the beginning of first record in a split (see SequenceFile. Reader."}, {"q": "What updates or decisions were made in the discussion?", "a": "I agree with Eric. This is backwards. You want the sequence file to pad out to block boundaries."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-107", "title": "Namenode errors \"Failed to complete filename.crc  because dir.getFile()==null and null\"", "status": "Closed", "priority": "Major", "reporter": "Igor Bolotin", "assignee": "Doug Cutting", "labels": [], "created": "2006-03-28T02:04:46.000+0000", "updated": "2009-07-08T16:41:51.000+0000", "description": "We're getting lot of these errors and here is what I see in namenode log: \n\n060327 002016 Removing lease [Lease.  Holder: DFSClient_1897466025, heldlocks: 0, pendingcreates: 0], leases remaining: 1\n060327 002523 Block report from member2.local:50010: 91895 blocks.\n060327 003238 Block report from member1.local:50010: 91895 blocks.\n060327 005830 Failed to complete /feedback/.feedback_10.1.10.102-33877.log.crc  because dir.getFile()==null and null\n060327 005830 Server handler 1 on 50000 call error: java.io.IOException: Could not complete write to file /feedback/.feedback_10.1.10.102-33877.log.crc by DFSClient_1897466025\njava.io.IOException: Could not complete write to file /feedback/.feedback_10.1.10.102-33877.log.crc by DFSClient_1897466025\n        at org.apache.hadoop.dfs.NameNode.complete(NameNode.java:205)\n        at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:585)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:237)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:216)\n\nI can't be 100% sure, but it looks like these errors happen with checksum files for very small data files. \n", "comments": ["It looks like your write to a file takes too long.\nThe client has 1 minute to complete one block write until\nthe lease issued for that client expires. When the lease expires the\nnamenode thinks the block is abandoned. If your files are small,\nconsisting of only 1 block, then the file will be considered abandoned\nas well. So the namenode removes the file before the client reports\nits completion.\nLease duration is not configurable, so you cannot control that.\nBut you can retry everything starting from file creation when you\nreceive that exception.\nIs it true that your writes take longer than a minute?\n", "A connection to a datanode is opened for the checksum file when a file is opened.  Then lots of data is written to the main file, and only a little to the parallel checksum file.  So the checksum file might not get touched in up to a minute.\n\nThe last block of every file (checksum & main) is tee'd to a temporary local file, so that if the network connection dies then attempts can be made to re-transmit it to another datanode.\n\nThis patch changes things so that connections to datanodes are not initiated until the block is complete.  All writes are initially to the local, temporary file and only copied to a datanode when the block is complete.", "This is correct - I tryed to write log files directly to DFS and depending on activity it could take pretty long time between calls. As a workaround - I changed my code to write to local output first and move to DFS only after file is closed (startLocalOutput/completeLocalOutput) and the problem went away.\n\nThe question is whether this behavior is expected? I though that with buffered output the blocks should not be requested until flush. If the block is requested too soon and the output takes a while for whatever reason - it's practically guaranteed to hit this problem, right? Also I never saw this happening with data files, only checksum files.", "Just tested the patch and now it works as expected. \nThanks!", "I just committed a fix for this."], "derived": {"summary": "We're getting lot of these errors and here is what I see in namenode log: \n\n060327 002016 Removing lease [Lease. Holder: DFSClient_1897466025, heldlocks: 0, pendingcreates: 0], leases remaining: 1\n060327 002523 Block report from member2.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Namenode errors \"Failed to complete filename.crc  because dir.getFile()==null and null\" - We're getting lot of these errors and here is what I see in namenode log: \n\n060327 002016 Removing lease [Lease. Holder: DFSClient_1897466025, heldlocks: 0, pendingcreates: 0], leases remaining: 1\n060327 002523 Block report from member2."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed a fix for this."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-108", "title": "EOFException in DataNode$DataXceiver.run", "status": "Closed", "priority": "Major", "reporter": "Igor Bolotin", "assignee": null, "labels": [], "created": "2006-03-29T08:15:19.000+0000", "updated": "2009-07-08T16:41:51.000+0000", "description": "This morning - after upgrade of the system - something got wrong and we started to get lot of exceptions.\nSituation didn't change after removing everything and creating new file system. \n\nMultiple exceptions on all data nodes:\n060328 145922 108 DataXCeiver\njava.io.EOFException\n        at java.io.DataInputStream.readFully(DataInputStream.java:178)\n        at java.io.DataInputStream.readLong(DataInputStream.java:380)\n        at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:448)\n        at java.lang.Thread.run(Thread.java:595)\n\nNo errors on the name node.\n\nDFS clients report following:\n060328 150923 task_r_2twzsl  Error while writing.\n060328 150923 task_r_2twzsl java.net.SocketTimeoutException: Read timed out\n060328 150923 task_r_2twzsl     at java.net.SocketInputStream.socketRead0(Native Method)\n060328 150923 task_r_2twzsl     at java.net.SocketInputStream.read(SocketInputStream.java:129)\n060328 150923 task_r_2twzsl     at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)\n060328 150923 task_r_2twzsl     at java.io.BufferedInputStream.read1(BufferedInputStream.java:256)\n060328 150923 task_r_2twzsl     at java.io.BufferedInputStream.read(BufferedInputStream.java:313)\n060328 150923 task_r_2twzsl     at java.io.DataInputStream.readFully(DataInputStream.java:176)\n060328 150923 task_r_2twzsl     at java.io.DataInputStream.readLong(DataInputStream.java:380)\n060328 150923 task_r_2twzsl     at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.internalClose(DFSClient.java:776)\n060328 150923 task_r_2twzsl     at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.endBlock(DFSClient.java:751)\n060328 150923 task_r_2twzsl     at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.close(DFSClient.java:814)\n060328 150923 task_r_2twzsl     at org.apache.hadoop.fs.FileSystem.createNewFile(FileSystem.java:202)\n\n", "comments": ["Found the difference - looks like it happens only when using hadoop with patch from HADOOP-107", "This problem didn't happen to us anymore after upgrading Hadoop. \nAlso, based on the description - this one looks like duplicate of HADOOP-128, so I'd like to suggest to close it.\n", "Duplicate of HADOOP-128"], "derived": {"summary": "This morning - after upgrade of the system - something got wrong and we started to get lot of exceptions. Situation didn't change after removing everything and creating new file system.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "EOFException in DataNode$DataXceiver.run - This morning - after upgrade of the system - something got wrong and we started to get lot of exceptions. Situation didn't change after removing everything and creating new file system."}, {"q": "What updates or decisions were made in the discussion?", "a": "Duplicate of HADOOP-128"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-109", "title": "Blocks are not replicated when...", "status": "Closed", "priority": "Major", "reporter": "Konstantin Shvachko", "assignee": "Konstantin Shvachko", "labels": [], "created": "2006-03-29T12:43:07.000+0000", "updated": "2009-07-08T16:41:51.000+0000", "description": "When the block is under-replicated the namenode places it into\nFSNamesystem.neededReplications list.\nWhen a datanode D1 sends getBlockwork() request to the namenode, the namenode\nselects another node D2 (which it thinks is up and running) where the new replica of the\nunder-replicated block will be stored.\nThen namenode removes the block from the neededReplications list and places it to\nthe pendingReplications list, and then asks D1 to replicate the block to D2.\nIf D2 is in fact down, then replication will fail and will never be retried later, because\nthe block is not in the neededReplications list, but rather in the pendingReplications list,\nwhich namenode never checks.\n", "comments": ["This was fixed as a part of HADOOP-940."], "derived": {"summary": "When the block is under-replicated the namenode places it into\nFSNamesystem. neededReplications list.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Blocks are not replicated when... - When the block is under-replicated the namenode places it into\nFSNamesystem. neededReplications list."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was fixed as a part of HADOOP-940."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-110", "title": "new key and value instances are allocated before each map", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-03-30T02:11:17.000+0000", "updated": "2009-07-08T16:51:42.000+0000", "description": "Each time map is called with a new key and value rather than reusing the old ones.", "comments": ["The reason that new objects are allocated during map is so that new objects will be available for the combiner.  So fixing HADOOP-110 is easy, but HADOOP-2 must be fixed first, since it is the underlying problem.", "Moves the creation of the map input objects outside of the loop.", "I just committed this.  Thanks, Owen."], "derived": {"summary": "Each time map is called with a new key and value rather than reusing the old ones.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "new key and value instances are allocated before each map - Each time map is called with a new key and value rather than reusing the old ones."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-111", "title": "JobClient.runJob() should return exit status for a job.", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": "Owen O'Malley", "labels": [], "created": "2006-03-30T19:49:38.000+0000", "updated": "2009-07-08T16:51:42.000+0000", "description": "JobClient.runJob() doesn't return any values. Any information about the exit status of a job is discarded (or appears only in logs).\n\nIt should be possible to return the exit status, so that JobClient users can determine whether a job was successfully completed or not. This is also important when using cmd-line tools in shell scripts - currently they don't return any exit codes, because it's not possible to determine the outcome of a job submitted through JobClient. As a consequence, it's difficult to automate repetitive jobs using shells scripts.\n\nIt would be also nice to have an exit message in case of errors, for human consumption.\n\nI propose to implement one of the following:\n\n* change the return type of this method from void to int\n\n* or, better yet, to put the exit code and optional exit messages inside the JobConf instance under pre-defined keys.", "comments": ["This was fixed by HADOOP-1326."], "derived": {"summary": "JobClient. runJob() doesn't return any values.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "JobClient.runJob() should return exit status for a job. - JobClient. runJob() doesn't return any values."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was fixed by HADOOP-1326."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-112", "title": "copyFromLocal should exclude .crc files", "status": "Closed", "priority": "Minor", "reporter": "Monu Ogbe", "assignee": "Doug Cutting", "labels": [], "created": "2006-03-30T23:25:35.000+0000", "updated": "2009-07-08T16:41:50.000+0000", "description": "Doug Cutting says: \"The problem is that when copyFromLocal \nenumerates local files it should exclude .crc files, but it does not. \nThis is the listFiles() call on DistributedFileSystem:160.  It should \nfilter this, excluding files that are FileSystem.isChecksumFile().\n\nBTW, as a workaround, it is safe to first remove all of the .crc files, \nbut your files will no longer be checksummed as they are read.  On \nsystems without ECC memory file corruption is not uncommon, but I have \nseen very little on clusters that have ECC.\"\n\nOriginal observations:\n\nHello Team,\n\nI created a backup of my DFS database:\n\n# bin/hadoop dfs -copyToLocal /user/root/crawl /mylocaldir\n\nI now want to restore from the backup using:\n\n# bin/hadoop dfs -copyFromLocal /mylocaldir/crawl /user/root\n\nHowever I'm getting the following error:\n\ncopyFromLocal: Target /user/root/crawl/crawldb/current/part-00000/.data.crc\nalready exists\n\nI get this message with every permutation of the command that I've tried, and\neven after totally deleting all content in the DFS directories.\n\nI'd be grateful for any pointers.\n\nMany thanks,\n\n\n\n", "comments": ["I just committed a fix for this.  DistributedFileSystem.copyFromLocal() no longer attempts to copy CRC files.", "I cannot see what exactly was committed here, but now regular dfs -cp doesn't\ncopy crc files, which causes e.g. dfs -cat complain about it.\nI presume that changes were made to FileUtil.copyContents() which is called in\nseveral places, which sometimes do and sometimes do not need crc files.\nIn case of dfs copy the crc files are needed.", "The changes made are listed at:\n\nhttp://svn.apache.org/viewcvs?rev=390218&view=rev\n\nThe described problem was fixed: a -copyToLocal (a.k.a -get) followed by a -copyFromLocal (a.k.a. -put) no longer fails complaining about a .crc file.  If this is failing again then this bug should be re-opened.  Otherwise I think it should remain closed.\n\nIf there is a problem with 'dfs -cp' then I think that is a separate bug, no?\n", "Given the lack of further comments, I will re-close this.  If there are other, related bugs, please report them as new bugs rather than re-opening this again."], "derived": {"summary": "Doug Cutting says: \"The problem is that when copyFromLocal \nenumerates local files it should exclude. crc files, but it does not.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "copyFromLocal should exclude .crc files - Doug Cutting says: \"The problem is that when copyFromLocal \nenumerates local files it should exclude. crc files, but it does not."}, {"q": "What updates or decisions were made in the discussion?", "a": "Given the lack of further comments, I will re-close this.  If there are other, related bugs, please report them as new bugs rather than re-opening this again."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-114", "title": "Non-informative error message", "status": "Closed", "priority": "Trivial", "reporter": "Rod Taylor", "assignee": "Doug Cutting", "labels": [], "created": "2006-03-31T00:52:51.000+0000", "updated": "2009-07-08T16:51:42.000+0000", "description": "060330 105006 mapred.child.heap.size is deprecated. Use mapred.child.heap.size instead. Meantime, interpolated child.heap.size into child.java.opt: -Xmx200m\n\nThe instructions inform you to use the deprecated option.", "comments": ["This is actually (ungrammatically) instructing you to use a new option.  I think the typo is that \"interpolated\" is meant to be \"interpolate\", meaning that you should include your current value of mapred.child.heap.size into mapred.child.java.opt, a more general option.  Can someone propose a better way to say this?", "I couldn't say about the accuracy of the rest, but this phrasing specifically is very confusing \"mapred.child.heap.size is deprecated. Use mapred.child.heap.size instead.\"\n\nIf mapred.child.heap.size is deprecated, then why in the world would it suggest that I use it?\n\nShould be it hadoop.child.heap.size?", "Correct log message.\n\nThis issue actually belongs back in hadoop.  Perhaps someone can move it there (and apply the patch there).", "I just committed this.  Thanks, Michael!"], "derived": {"summary": "060330 105006 mapred. child.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Non-informative error message - 060330 105006 mapred. child."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Michael!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-113", "title": "Allow multiple Output Dirs to be specified for a job", "status": "Closed", "priority": "Major", "reporter": "Rod Taylor", "assignee": null, "labels": [], "created": "2006-03-31T01:34:35.000+0000", "updated": "2009-07-08T16:51:42.000+0000", "description": "Allow a single job to create multiple outputs. 2 additional simple functions only\n\nThis allows for more complex branching of the process to occur either with multiple steps of the same type or allow different actions to take place on each output directory depending on the required actions.\n\n\nFor my specific use, it allows me to run multiple Generate Outputs instead of a single Generate Output as submitted in NUTCH-171(http://issues.apache.org/jira/browse/NUTCH-171)", "comments": ["We should probably instead add a Configuration.getFiles() method, used by this and by getInputDirs().  This should be implemented in terms of Configuration.getStrings().   And we should add a Configuration.addFile() method that's used by this and by addInputDir().  This should be implemented in terms of a Configuration.addString() method.  Otherwise we end up copying the same code around in too many places.\n\nHowever I'm not yet convinced that this feature is the best way to achieve your goal.  I've commented on NUTCH-171 that an alternate mechanism might better achive your goal.  If that or something like it makes sense (specifying, for a job, the maximum number of its maps & reduces that should be run on a single node at once, so that a job can use less than the entire cluster, permitting other jobs to pass it) then we should start a new Hadoop bug for that.", "\nIs the intention to have one mapper fork into multiple reducers, saving the file io of doing independent map passes?\n\nmapper1 -> output a -> reducer 1\n                 -> output b -> reducer 2 \n\ninstead of\n\nmapper1 -> output a -> reducer 1\nmapper 2 -> output b -> reducer 2\n\nwherein the second example, the map input file is read twice instead of once?\n\nthat could be useful. i not sure how much it would really speed things up.", "The way that Hadoop would deal with this these days is to have a pair of static methods in the specific OutputFormat that supports multiple directories. So there would be no need to add them into JobConf."], "derived": {"summary": "Allow a single job to create multiple outputs. 2 additional simple functions only\n\nThis allows for more complex branching of the process to occur either with multiple steps of the same type or allow different actions to take place on each output directory depending on the required actions.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Allow multiple Output Dirs to be specified for a job - Allow a single job to create multiple outputs. 2 additional simple functions only\n\nThis allows for more complex branching of the process to occur either with multiple steps of the same type or allow different actions to take place on each output directory depending on the required actions."}, {"q": "What updates or decisions were made in the discussion?", "a": "The way that Hadoop would deal with this these days is to have a pair of static methods in the specific OutputFormat that supports multiple directories. So there would be no need to add them into JobConf."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-115", "title": "permit reduce input types to differ from reduce output types", "status": "Closed", "priority": "Major", "reporter": "Runping Qi", "assignee": "Runping Qi", "labels": [], "created": "2006-04-01T00:30:32.000+0000", "updated": "2009-07-08T16:51:42.000+0000", "description": "When map tasks write intermediate data out, they always use SequencialFile RecordWriter with key/value classes from the job object.\n\nWhen the reducers write the final results out, its output format is obtained from the job object. By default, it is TextOutputFormat, and no conflicts.\nHowever, if one wants to use SequencialFileFormat for the final results, then the key/value classes are also obtained from the job object, the same as the map tasks' output. Now we have a problem. It is impossible for the map outputs and reducer outputs use different key/value classes, if one wants the reducers generate outputs in SequentialFileFormat.\n\nA simple fix would be to add another two attributes to JobConf class: mapOutputLeyClass and mapOutputValueClass. That allows the user to have different key/value classes for the intermediate and final outputs.\n\n", "comments": ["New attibutes in the JobConf should denote output from reduce task not from map as suggest a the name mapOutputValueClass.", "This is the way it is supposed to work.  From the MapReduce paper:\n\n  map (k1,v1) ! list(k2,v2)\n  reduce (k2,list(v2)) ! list(v2)\n\n  I.e., the input keys and values are drawn from a different\n  domain than the output keys and values. Furthermore,\n  the intermediate keys and values are from the same domain\n  as the output keys and values.\n\nI am closing this bug.  If someone feels strongly that we should extend the MapReduce model in this direction, then we can re-open it.  But, as it stands, things work as intended.", "Let's reopen this. I've had discussions with Runping today, and it seems to me that:\n\n1. It is basically free with respect to the framework.\n2. It allows more applications to be written using the framework rather than working around the framework.\n3. It is less clear that we should allow the user to change the key type in the reduce, but since the current API does allow them to change the value (if not the type), I think we should be consistent and allow a type change too.\n\nI propose:\n\n1. Add {set,get}MapOutput{Key,Value}Class functions in JobConf.\n2. The default values for getMapOutput{Key,Value}Class are the values from getOutput{Key,Value}Class.\n3. Always check the types in the output collector rather that the OutputFormat, so that even text output files are check for type correctness.\n\nWe should include a javadoc comment for setMapOutputKeyClass will warn that changing the key in the reduce will mean that your output is NOT sorted.", "+1\n\nBut MapOutput seems confusing.  Shouldn't these be called FinalOutput or ReduceOutput?\n", "Ah!  Are you suggesting that getOutput* describes the final classes output from reduce always and if you don't set the new variables MapOutput* it also controls the map?  That is clear enough and backwards compatible.  I just was not looking at it the right way!\n", "+1\n\nMy original post about the issue gives a simple case that would benefit from this: http://www.mail-archive.com/hadoop-user%40lucene.apache.org/msg00073.html\n\nAs already said, this would add more transformational power to Hadoop and make certain cases more straightforward.\n\n", "\" I.e., the input keys and values are drawn from a different\n  domain than the output keys and values. Furthermore,\n  the intermediate keys and values are from the same domain\n  as the output keys and values. \"\n\nThe Google MapReduce paper states the Web-Link graph example where this is not the case:\n\n\"Reverse Web-Link Graph: The map function outputs\n(target; source) pairs for each link to a target\nURL found in a page named source. The reduce\nfunction concatenates the list of all source URLs associated\nwith a given target URL and emits the pair:\n(target; list(source))\".\n\n", "I've implemented the above case with the existing code, and it's pretty simple... the output is actually always list(source), or perhaps a new type that encapsulates the list, and it just means that the map actually outputs a list of length 1, and the combiner/reducer concatenates lists.", "I should have addressed the combiner before. *smile* Of course the combiner input and output has to match the map output types. So, it looks like:\n\nmap: k1,v1 -> seq(k2,v2)\ncombine: k2,seq(v2) -> seq(k2,v2)\nreduce: k2, seq(v2) -> seq(k3,v3)\n\nSo the only extra code is to set/get the types for k2/v2 (or equivalent k3/v3), although I would recommend adding a type check in the reduce collector. It is completely upward compatible.\n\nAs for user confusion, I've already had to explain this restriction (k2==k3 and v2==v3) far more times than I'd like.\n\nOn a side note, we could hack around the problem by defining an OutputFormat that uses SequenceFileWriter, but doesn't open the file until the first key/value pair is written and takes the types from the first instances. But that breaks when someone puts the type check into the reduce collector.", "(1) adds JobConf.set/getMapKey/ValueClass\n(2) changes in ReduceTask so that the Map phase classes are used in append & sort\n(3) explicit key and value class specification in OutputFormat.getRecordWriter so that they are not automatically pulled out of JobConf\n\nTests run ok, but I haven't added tests for different Map/Reduce key/value classes.\n\nOwen's suggestion on automatically typing the OutputFormat when the first record is written would cause changes in fewer classes, but I had this already working...\n\n", "Your patch adds {set,get}MapOutputComparatorClass(), which aren't needed, because the map outputs are the only ones that are compared. \n\nI don't think you are handling the combiners.\n\nDid you need to change the interfaces to explicitly pass around the key/value types to the getRecordWriter()? Shouldn't getRecordWriter only be called for the reduce outputs?", "You're right about the getMapOutputComparatorClass and the needless interface change.\n\nAutomatic/deferred SequenceFile typing based on the first record doesn't seem feasible because SequenceFile.Writer.append(byte[]...) is used here and there.\n\nI already had a second go at this with less changes, but I'll keep it to myself until I can put some unit tests together.\n\nAt least I'm learning about how things work under the hood.\n\n\n\n", "Patch including\n\nTestReduceTask\n- generates a bunch of SequenceFiles and reduces them by running a single ReduceTask\n- two test methods, one where input is just copied to output and one where the Reducer swaps keys and values\n- Reducer checks that all generated key-value pairs are reduced by key\n- checks that the resulting output file contains what it's supposed to\n\nJobConf\n- the necessary set/getMapOutputKey/ValueClass methods\n- getOutputComparator uses MapKeyClass if one is specified\n\nReduceTask\n- append and sort phases get the classes from getMapOutput.. methods\n\nThis should take care of the Reduce part of the problem. MapTask should be also adjusted accordingly, but since I haven'twritten  a test for that I haven't done it yet.\n\nOwen, I didn't get your comment on handling the combiners - doesn't the combiner just use the map OutputCollector underneath and as you put it \n\nmap: k1,v1 -> seq(k2,v2)\ncombine: k2,seq(v2) -> seq(k2,v2) \n\nthe outputs are exactly the same, even if the combiner is technically a Reducer?\n\n\n\n\n\n", "Runping is going to take a look at fixing the patch.", "My patch is attached.\n\n \n\nChange highlights:\n\nJobConf.java:\n    Added set/getMapOutputKey/ValueClass methods\n    Modified getOutputKeyComparatorClass \n\nMapTask.java:\n    call getMapOutputKey/ValueClass instead of getOutputKey/ValueClass\n\nMapReduce.java:\n    call getMapOutputKey/ValueClass instead of getOutputKey/ValueClass\n\nMapOutputFile.java:\n    call getMapOutputKey/ValueClass instead of getOutputKey/ValueClass\n\n \n\nI've run a job with a combiner, with UTF8 as the  map output value class, and  CrawledDoc as the output value class.\n\nThe job completed successfully.\n\n", "Your patch has some strange formatting, and some spurious whitespace changes.  I fixed most of these.  It would also be best to have a unit test that uses this feature.  But I'm tired of this issue, and it should do no harm, so I committed it."], "derived": {"summary": "When map tasks write intermediate data out, they always use SequencialFile RecordWriter with key/value classes from the job object. When the reducers write the final results out, its output format is obtained from the job object.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "permit reduce input types to differ from reduce output types - When map tasks write intermediate data out, they always use SequencialFile RecordWriter with key/value classes from the job object. When the reducers write the final results out, its output format is obtained from the job object."}, {"q": "What updates or decisions were made in the discussion?", "a": "Your patch has some strange formatting, and some spurious whitespace changes.  I fixed most of these.  It would also be best to have a unit test that uses this feature.  But I'm tired of this issue, and it should do no harm, so I committed it."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-116", "title": "cleaning up /tmp/hadoop/mapred/system", "status": "Closed", "priority": "Major", "reporter": "raghavendra prabhu", "assignee": "Doug Cutting", "labels": [], "created": "2006-04-04T12:13:58.000+0000", "updated": "2009-07-08T16:51:42.000+0000", "description": "Clean up directories\n\nsubmit_ which contain job.xml and job.jar files as and when the job is finished", "comments": ["This tries to delete the submit_ (which contains job.xml and job.jar file)\n\nTested with mapred \"local\" option", "These files should not be removed by the JobClient, since that may exit while the job is still running.  Instead they should be removed by the LocalJobRunner or the TaskTracker, when the job is done.  I'm working on a patch to fix this.", "Fixed in svn revision 392353."], "derived": {"summary": "Clean up directories\n\nsubmit_ which contain job. xml and job.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "cleaning up /tmp/hadoop/mapred/system - Clean up directories\n\nsubmit_ which contain job. xml and job."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed in svn revision 392353."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-117", "title": "mapred temporary files not deleted", "status": "Closed", "priority": "Blocker", "reporter": "raghavendra prabhu", "assignee": "Doug Cutting", "labels": [], "created": "2006-04-04T12:46:44.000+0000", "updated": "2018-04-11T14:26:19.000+0000", "description": "I found out that JobConf.java\n\nCreated interchanged names with parent being file and file being parent directory\n\nAs a result files were not getting deleted", "comments": ["Fixes the reversal of directory and child file\n\nAs a result, deletion is proper", "There is also one more issue\n\nTwo instances of directories /tmp are found now. One in root directory and other in current directory(where application is run)\n\nAs a result it is looking at the wrong folder to delete (different /tmp). WIll try to fix this problem if more users confirm that delete is not happening\n\n", "Are you sure you are running the latest hadoop? There was an issue, which resulted in such behaviour on Cygwin, but it's been fixed in rev. 389633.", "I just fixed this."], "derived": {"summary": "I found out that JobConf. java\n\nCreated interchanged names with parent being file and file being parent directory\n\nAs a result files were not getting deleted.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "mapred temporary files not deleted - I found out that JobConf. java\n\nCreated interchanged names with parent being file and file being parent directory\n\nAs a result files were not getting deleted."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just fixed this."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-118", "title": "Namenode does not always clean up pendingCreates", "status": "Closed", "priority": "Critical", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-04-05T00:30:38.000+0000", "updated": "2009-07-08T16:41:50.000+0000", "description": "In some failure modes, the pending creates list is not cleaned up and prevents that file from ever being created.\n\nWhen I try to create the file after the first job was killed (hours previously), I get:\n\n060404 084619 Cannot start file because pendingCreates is non-null. src=/user/oom/rand/part000118\n060404 084619 Server handler 0 on 8020 call error: java.io.IOException: Cannot create file /user/oom/rand/part000118 on client DFSClient_-1656137458\njava.io.IOException: Cannot create file /user/oom/rand/part000118 on client DFSClient_-1656137458\n        at org.apache.hadoop.dfs.NameNode.create(NameNode.java:147)\n        at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:585)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:237)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:216)\n", "comments": ["This patch \n  1. registers DFS Clients onto a list to be close when the jvm exits. \n  2. makes closing a DFSClient call abandonFileCreate on each file that was not finished.\n  3. it throws an exception if the client is used after it is closed.\n  4. removes the close done by the JobClient\n  5. adds the lease holder to the abandonFileInProgress message. This allows  the host's lease to be cleaned up as well as detecting attempts to abandon other clients files.", "I just committed this.  Thanks, Owen!"], "derived": {"summary": "In some failure modes, the pending creates list is not cleaned up and prevents that file from ever being created. When I try to create the file after the first job was killed (hours previously), I get:\n\n060404 084619 Cannot start file because pendingCreates is non-null.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Namenode does not always clean up pendingCreates - In some failure modes, the pending creates list is not cleaned up and prevents that file from ever being created. When I try to create the file after the first job was killed (hours previously), I get:\n\n060404 084619 Cannot start file because pendingCreates is non-null."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-119", "title": "ReduceTask.configure() is called twice", "status": "Closed", "priority": "Major", "reporter": "Darek Zbik", "assignee": null, "labels": [], "created": "2006-04-05T04:27:00.000+0000", "updated": "2009-07-08T16:51:43.000+0000", "description": "ReduceTask.configure() is called twice for each created reduce task \nFirst call is done indirect from \norg.apache.hadoop.mapred.JobConf.newInstance()\ncalled in ReduceTask.run(). Second call was in ReduceTask.run() just after creating a new instance. I suggest to remove the second call. For all new instances created in case of map task there are no directly xxx.configure();", "comments": ["One line patch tested in my codes.", "This seems to have been fixed already."], "derived": {"summary": "ReduceTask. configure() is called twice for each created reduce task \nFirst call is done indirect from \norg.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "ReduceTask.configure() is called twice - ReduceTask. configure() is called twice for each created reduce task \nFirst call is done indirect from \norg."}, {"q": "What updates or decisions were made in the discussion?", "a": "This seems to have been fixed already."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-120", "title": "Reading an ArrayWriter does not work because valueClass does not get initialized", "status": "Closed", "priority": "Major", "reporter": "Dick King", "assignee": "Cameron Pope", "labels": [], "created": "2006-04-05T07:54:37.000+0000", "updated": "2007-11-05T18:11:57.000+0000", "description": "If you have a Reducer whose value type is an ArrayWriter it gets enstreamed alright but at reconstruction type when ArrayWriter::readFields(DataInput in) runs on a DataInput that has a nonempty ArrayWriter , newInstance fails trying to instantiate the null class.", "comments": ["This patch appears to fix the problem.  I don't know whether you will object to the exception modification on ClassNotFoundException .\n\n-dk\n", "After thinking about it for a bit, the problem with this patch is that this is going to encode the typename in each and every record. So if your value type is ArrayWriter<UTF8>, you are going to spend an extra 2+strlen(\"org.apache.hadoop.io.UTF8\") bytes per a record. That's a fair amount of overhead.\n\nWe also have to be a careful with the serialization of ArrayWritable because it is used in the DFS name node logs.\n\nI'm not sure what the right solution is. Probably for right now, I would derive a subclass of ArrayWritable that is specific for your type. It isn't pretty, but it is guaranteed to be safe.\n\npublic class UTF8Array extends ArrayWritable {\n  public UTF8Array() {\n     super(UTF8.class);\n  }\n}", "I realized that, of course, but I'm not expecting a whole lot of small ArrayWritable's .\n\nOwen O'Malley's solution has the problem that ArrayWritables can themselves contain ArrayWritables.  We might, for example, want to have a two-component ArrayWritable where there are some integers and some UTF8s,  so we need a class\n\n  public class ArrayOfArrayOfSomeIntsAndSomeUTF8s extends ArrayWritable\n  {\n     ... filling this in gives me a headache\n  }\n\n-dk\n", "Regarding the class name encoding: you could use the same trick that we use in Nutch, org.apache.nutch.crawl.MapWritable, which uses a sort of dictionary encoding, and as long as you use only the \"standard\" types the overhead is just 1 byte. For non-standard types, the type name is put into a dictionary (once), and henceforth only 1 byte is used, too.", "The big deal is probably not so much the space it takes but the TIME it takes to look up a class.\n\nOwen, I'll stop by at 11 to talk about this.  I think we need to generalize the notion of an input ot output class to an input or output _descriptor_ .\n\nGotta run now -- literally.\n\n-dk\n", "Regarding the dictionary approach ... remember the life of a dictionary is the lifetime of a DFS, not merely a particular run or a DFS file.  Also it does violate the apparent convention that the callER knows the type of the object.  Now maybe that should be rethought, but we should do so explicitly, not back into it.\n\n-dk\n", "Dictionary is stored together inside the file, so I don't think it's a problem - you can always read it later and restore the data.\n\nCaller may be unaware of exact types stored in a Map-like structure, but it should not prevent him from knowing (and using) portions of data that he knows about, so I think this doesn't violate the convention... ok, maybe bends it a little ;)", "Hello all,\n\nwhat is current thinking about this bug and the right way to fix it?\n\nWhat about adding a simple javadoc comment in this class to warn users that it must be derived to be used as a Reducer value? ", "Hello,\nThis bug is still not fixed in the latest versions of Hadoop. I wrote a fix similar to the one submitted. But it hasnt been updated in the respository.\nIs there some issue with this fix ?", "Could anyone commit this patch ?", "-1, could not apply patch.\n\nThe patch command could not apply the latest attachment http://issues.apache.org/jira/secure/attachment/12324953/hadoop-120-fix.patch as a patch to trunk revision r536583.\n\nConsole output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/133/console\n\nPlease note that this message is automatically generated and may represent a problem with the automation system and not the patch.", "This patch no longer applies to trunk.  It's also not formatted according to Hadoop's conventions.  Finally, this is not an acceptable approach.  Rather, one could use the workaround suggested by Owen, or we could add a new Writable class that writes the name of the class in each instance, but it would not be back-compatible to make this change to ArrayWritable.", "I ran into this issue this morning. I'm new to Hadoop so forgive me if I failed to understand the big picture in submitting this patch.\n\nTo address, I added some documentation to ArrayWritable indicating that it needs to be subclassed to be used as input to Reducers. I also added a check for valueClass being undefined when calling readFields() - as a relatively new Hadoop user, it took me a while to figure out what the NPE I got in ReflectionUtils was and what to do with it. \n\nIt looks like some DFS datastructures depend on ArrayWritable being the way that it is, so it seems that changing the serialization format would break stuff. It might make sense to create a different ArrayWritable that serialized type information for Hadoop users specifically for MapReduce applications, but this class should probably document its contract with the outside world and throw a specific exception when it's violated. I've also added a unit test to verify the exceptions thrown or not thrown are as expected.", "Patch to document ArrayWritable and throw a specific exception when readFields is undefine.", "+1\n\nhttp://issues.apache.org/jira/secure/attachment/12365371/hadoop-120-workaround.patch applied and successfully tested against trunk revision r573708.\n\nTest results:   http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/715/testReport/\nConsole output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/715/console", "I think we can do this more simply by prohibiting the creation of ArrayWritable's whose valueClass is null.  I'll attach a patch.", "Here's the new version that prohibits creation of ArrayWritable's with a null valueClass.", "Yes, that's definitely a better fix and if anyone is using ArrayWritable's default constructor, it should be easy to change.", "-1 overall.  Here are the results of testing the latest attachment \nhttp://issues.apache.org/jira/secure/attachment/12365761/HADOOP-120-3.patch\nagainst trunk revision r575472.\n\n    @author +1.  The patch does not contain any @author tags.\n\n    javadoc +1.  The javadoc tool did not generate any warning messages.\n\n    javac +1.  The applied patch does not generate any new compiler warnings.\n\n    findbugs +1.  The patch does not introduce any new Findbugs warnings.\n\n    core tests +1.  The patch passed core unit tests.\n\n    contrib tests -1.  The patch failed contrib unit tests.\n\nTest results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/754/testReport/\nFindbugs warnings: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/754/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/754/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/754/console\n\nThis message is automatically generated.", "I just committed this.  Thanks, Cameron.", "Integrated in Hadoop-Nightly #241 (See [http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/241/])"], "derived": {"summary": "If you have a Reducer whose value type is an ArrayWriter it gets enstreamed alright but at reconstruction type when ArrayWriter::readFields(DataInput in) runs on a DataInput that has a nonempty ArrayWriter , newInstance fails trying to instantiate the null class.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Reading an ArrayWriter does not work because valueClass does not get initialized - If you have a Reducer whose value type is an ArrayWriter it gets enstreamed alright but at reconstruction type when ArrayWriter::readFields(DataInput in) runs on a DataInput that has a nonempty ArrayWriter , newInstance fails trying to instantiate the null class."}, {"q": "What updates or decisions were made in the discussion?", "a": "Integrated in Hadoop-Nightly #241 (See [http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/241/])"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-123", "title": "mini map/reduce cluster for junit tests", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Milind Barve", "labels": [], "created": "2006-04-07T02:34:07.000+0000", "updated": "2009-07-08T16:51:42.000+0000", "description": "I would like a single process map-reduce pseduo-distributed cluster that can be used for unit tests. We should add a unit test with a word count map/reduce.", "comments": ["I have code for this already. A couple of small changes are needed to make it work. Some config fixes, a fix to override the places where the code gets the hostname, etc.", "alan,\n\nif you could pass me the patch, i could make the required modifications, test, and submit.\n\nmilind", "I'm putting together the patch today.", "I put my changes into the latest trunk, and built a patch. It doesn't work yet, but the outline is there.\n\nNotes:\n\nThere is an added conf to override the hostname for nodes (DataNode and TaskTracker)\n\nThere is a DNS hack so that java resolves all names in the form localhostN to 127.0.0.1 (MyNameService)\nThis requires the system property 'sun.net.spi.nameservice.provider.1' to be set in the command line \nor in the ant script.\n\ndfs sim = RunDfsSim.java\nmapred sim = RunMapRedSim.java\n\nThe dfs sim works pretty good. The MapRed sim is broken. It's always config issues. \nGenerally you want for the TaskTrackers to use their local config's and avoid loading the\ndefault configs (which are all for localmode).\n\nI changed the build xml so that the 'test-cluster' target runs all tests that start with 'ClusterTest'.\nThe ClusterTestDFS.java test is working but needs to be refactored to use RunDfsSim.java\n\n'new JobConf(conf)' used to simply drop the conf on the floor, so there is a fix to Configuration for that.\n\nI had to add code to close the editlog in FSNamesystem or ClusterTestDFS.java fails on the second pass.\n\nOops, the multiple editlogs patch, in FSDirectory, is also in this patch, but the default case changes nothing.\n\nThere is a small change to the test-cluster target of build.xml to support the sims.\n\nTell me what you think.\n\n\n\n\n\n\n\n", "I have added a patch that now includes a mini-map-reduce cluster in unit tests. There are three unit tests added that use the mini-map-reduce cluster. Main changes (other than test additions) are: Now jobtracker, and tasktrackers can be cleanly shut down (earlier ctrl-c was the only way). Class path changes in test.classpath (job-tracker and task-tracker need webapps, and a temporary overriding mechanism to overriding src/test/hadoop-site.xml). Multiple task-trackers can be spawned on the same node with their respective mapred.local.dir, therefore unless explicitly overridden in hadoop-site.xml, the task uses task-tracker's mapred.local.dir.\n\nThe three new unit tests take 15 seconds, 115 seconds, and 119 seconds respectively, on my linux box. They test mini MR cluster's bringup-shutdown, a map-reduce job with local file system, and a map-reduce job with mini-DFS cluster.\n\nBy the way, Hadoop now satisfies an important requirement of being a parallel programming infrastructure. I have included a map-reduce job that estimates the value of PI using monte-carlo method :-)\n", "I just committed this.  Thanks, Milind!"], "derived": {"summary": "I would like a single process map-reduce pseduo-distributed cluster that can be used for unit tests. We should add a unit test with a word count map/reduce.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "mini map/reduce cluster for junit tests - I would like a single process map-reduce pseduo-distributed cluster that can be used for unit tests. We should add a unit test with a word count map/reduce."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Milind!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-124", "title": "don't permit two datanodes to run from same dfs.data.dir", "status": "Closed", "priority": "Critical", "reporter": "Bryan Pendleton", "assignee": "Konstantin Shvachko", "labels": [], "created": "2006-04-08T01:45:55.000+0000", "updated": "2009-07-08T16:41:51.000+0000", "description": "DFS files are still rotting.\n\nI suspect that there's a problem with block accounting/detecting identical hosts in the namenode. I have 30 physical nodes, with various numbers of local disks, meaning that my current 'bin/hadoop dfs -report\" shows 80 nodes after a full restart. However, when I discovered the  problem (which resulted in losing about 500gb worth of temporary data because of missing blocks in some of the larger chunks) -report showed 96 nodes. I suspect somehow there were extra datanodes running against the same paths, and that the namenode was counting those as replicated instances, which then showed up over-replicated, and one of them was told to delete its local block, leading to the block actually getting lost.\n\nI will debug it more the next time the situation arises. This is at least the 5th time I've had a large amount of file data \"rot\" in DFS since January.", "comments": ["Did you potentially start identically configured datanodes as a different user?\n\nRight now the only \"lock\" preventing this is the pid file used by the nutch-daemon.sh script.  Perhaps the datanode should lock each directory in dfs.data.dir?  That should prevent this, no?\n\nI suppose this could also happen if the datanode lost its connection to the namenode, but the namenode had not yet timed out the datanode.  Then the datanode would reconnect and blocks might be doubly-reported.  To fix this, perhaps the namenode should refuse to represent more than one copy of a block from a given IP?  If a second is reported, the first should be forgotten?\n\n", "It seems like it would help to have the datanode generate a unique identifier the first time it is run, and save that in the data directory. On datanode restarts it uses the unique identifier from the data directory. The namenode would be able to complain about multiple instance of the same datanode.", "I have certainly seen some weirdness with my cluster where a stop-all seemed to think it had succeeded but there were still datanode and tasktracker instances running. Locking at the directory level seems like a good hedge against that sort of problem. As a bonus - it'll prevent blocks from getting removed if someone mistakenly starts up a second set of datanodes against a different namenode, as long as the datanode daemon for the competing DFS instance is running on the machine.\n\nKeeping track of only one copy of a block for a given IP seems like a good start, but might be too simple. What if, by some unfortunate process, a lot of duplicates get stored on a node. If there's no way to detect this, it could be that a bunch of copies of a block end up being duplicated on a node unnecessarily. One way out of that would be to notice if a host is reporting more than one copy of a block, and kick off a knee-jerk fix: 1) Extra-replicate the block 2) Ask the node with dups to remove its copies of the block. Of course, in the case where two datanode instances are somehow servicing from the same directory, the knee-jerk reaction would kick off for all blocks that get stored into that space - so locking would definitely be necessary to make this work.", "This seems like something we should fix in the .2 release, no?", "Changing the summary to better describe what we intend to fix.", "I just talked with Owen, and we came up with the following plan:\n\n(0) store a node id in each dfs.data.dir;\n\n(1) pass the node id to the name node in all DataNodeProtocol calls;\n\n(2) the namenode tracks datanodes by <id,host:port> pairs, only talking to one id from a given host:port at a time.  requests from an unknown host:port return a status that causes the datanode to exit and *not* restart itself.\n\n(3) add a hello() method to DataNodeProtocol, called at datanode startup only.  this erases any entries for a datanode id, replacing them with the new entry.\n\nThus when a second datanode is started it causes any existing datanode running on that host to be forgotten and to exit when it next contacts the namenode.", "There is a problem with shutting down the old datanode when the new one starts.\nThe new datanode must be sure the old one is gone before taking over.\n\nI think a datanode should merely keep a FileLock in the dfs.data.dir while it is running.\nIn this case the new node will not be able to start with the same data directory.\nIs that the problem we are trying to solve here?\n", "I believe we have three problems to address:\n\n1) The namenode needs to know to purge old entries identical entries when a new datanode registers.  Else we get rot.  See doug's suggestions above.\n\n2) You could have 2 or more datanodes on one server.  They need to always be unique.  We should asign a unique ID to each datanode home directory and make sure the datanode is started with a valid home directory as well.  I like the idea of assigning a uniqueID to each datanode home.\n\n3) You concern that two daemons might run on the same datadir.\n\nWe should address all these concerns.  Your suggestion of a startup lock and a uniqueID, plus (hello method) should together handle all of these.\n", "I see exactly 2 problems now.\n1) Two datanodes can be started sharing the same data directory, and reporting the same blocks twice.\nI'm planning to solve this by placing a lock in the data dir.\n2) Starting a replacement datanode.\nIf a datanode fails, or intentionally shut down, the new datanode\nserver can be started on the same node and port or on a different node or port,\nbut connected to the same data directory.\nIn current implementation if the new datanode starts before the namenode removed the old datanode from\nits list identical block will be considered belonging to both nodes, and consequently over-replicated.\nTo solve this second problem the datanode should store a unique id per data directory as proposed.\nAnd the namenode should identify block reports by those ids (only!), rather than by hostname:port.\nEach datanode reporting the same id (no matter which hostname or port it has) should be considered\na replacement to the old node, and the old datanode at this point should be removed from the namenode\nthe same way it is removed when its hearbeats are not reaching the namenode for 10 minutes.", "+1", "Here is an algorithm that imo solves the problem in the most general case.\nThis is the registration part only, since the rest is rather straightforward.\n\nI'm trying to cover two issues.\n1) Data nodes should be banned from reporting the same block copies multiple times\nif they are intentionally or unintentionally started to serve the same data storage.\nThat is why data nodes need to register, and need to keep a persistent storageID.\n2) Name node should be able to recognize registered data nodes, even if it is restarted\nor replaced by a spare name node serving the same name space.\nThat is why name nodes need to keep a persistent namespaceID.\n\nComments are highly appreciated.\n", "This is the patch that fixes the problem.\nDFS_CURRENT_VERSION has been changed to -2,\nsince internal file layouts have changed.\n\nI created a new package for exceptions.\n", "For future development in this direction.\nWe should persistently store on the name node all storage IDs, which the\nname node ever assigned any blocks to.\nWith that knowledge the name node can reject blocks from any newly\nregistered data storages that are not on the name node list.\nIn other words when a data node registers NEW data storage it should not\nreport any blocks from that storage, and the name node can effectively verify \nthat since it never assigned any blocks to this storage.\nThis would prevent us from accidentally connecting data nodes representing\ndifferent clusters (DFS instances).\n", "Eric Baldeschwieler wrote:\n> why not store the cluster in the data node?\n\nWe can alternatively store namespaceID in every data storage belonging to the cluster.\nMay be this is conceptually cleaner. I preferred storing storageIDs in the meta data just \nbecause this gives the name node knowledge of which storages are missing, and lets it \nreport it.", "+1, except the exceptions should not be in a separate package, but in the dfs package.  If you make that change then I'd be happy to commit this, since it fixes a very critical bug.  Thanks!", "I just applied this to trunk and unit tests failed with UnregisteredDatanodeExceptions.", "Yes, that was a MiniDFSCluster incomatibility.\nFixed that, merged with the current version,\nremoved the exception package.\n", "This patch does not apply to the current trunk.  Can you please update your sources to the current trunk, resolving any conflicts, and re-generate the patch?  Thanks!", "This adds a number of new public classes that I'm not certain should be public.  Should user code ever need to access a DataStorage, DatanodeID or DatanodeRegistration, or are these only used internally?  Also, several of these exceptions appear only to be used internally, but I'm not certain about all of them.  Would you object if I simply make all of these new classes package-private?  Then, if we need, we can reveal more later as needed.", "I just committed this, modified to make most of the new classes package-private rather than public.  Thanks, Konstantin!", "Yes, I think it is ok to make the classes unaccessible from the outside of the project.\nExceptions are different, they are returned to a client in the form of IOException right now, \nbut if the client wants to distinguish between them then we need to keep them public."], "derived": {"summary": "DFS files are still rotting. I suspect that there's a problem with block accounting/detecting identical hosts in the namenode.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "don't permit two datanodes to run from same dfs.data.dir - DFS files are still rotting. I suspect that there's a problem with block accounting/detecting identical hosts in the namenode."}, {"q": "What updates or decisions were made in the discussion?", "a": "Yes, I think it is ok to make the classes unaccessible from the outside of the project.\nExceptions are different, they are returned to a client in the form of IOException right now, \nbut if the client wants to distinguish between them then we need to keep them public."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-125", "title": "LocalFileSystem.makeAbsolute bug on Windows", "status": "Closed", "priority": "Minor", "reporter": "Peter Sutter", "assignee": "Doug Cutting", "labels": [], "created": "2006-04-08T06:43:59.000+0000", "updated": "2006-08-03T17:46:35.000+0000", "description": "\nLocalFileSystem.makeAbsolute() has a bug when running on Windows (which is very useful for the development phase of a Hadoop task on one's laptop).\n\nProblem:  if a pathname such as /tmp/hadoop... is given in a config file, when the jobconf file is created, it is put into the relative directory named: currentdir/tmp/hadoop..., but when hadoop tries to open the file, it looks in c:/tmp/hadoop..., and the job fails.\n\nCause: while Unix has two kinds of filespecs (relative and absolute), WIndows actually has three:\n\n(1) relative to current directory (subdir/file)\n(2) relative to current disk (/dir/subdir/file)\n(3) absolute (c:/dir/subdir/file)\n\nSo when a config file specifies a directory with what-is-on-unix an absolute path (/tmp/hadoop...), the makeAbsolute() method will not work correctly. Basically, File.isAbsolute() will return false for cases (1) and (2) above, but true for case (3), which is not expected by the code below. \n\nThe solution would be to code explicit detection of all three casses for Windows in the code below from LocalFileSystem:\n\n    private File makeAbsolute(File f) {\n      if (f.isAbsolute()) {\n        return f;\n      } else {\n        return new File(workingDir, f.toString());\n      }\n    }\n\nIm happy to explain if this explanation is confusing... ", "comments": ["I just fixed this.", "Doug,\n   Doesn't your fix break on unix if the user has a directory named \"\\\\foo\"? Certainly it would be a strange name, but it is legal. I was also curious why you took out setting the System property. It was convienent to be able to set the working directory on the local file system and be able to use File.getAbsolutePath(). Could we extend the FileSystem interface to add makeAbsolute?", "> Doesn't your fix break on unix if the user has a directory named \"\\\\foo\"?\n\nPerhaps.  I did not test this case.  I was just despately trying to get Nutch to work correctly standalone & pseudo-distributed on Windows and linux.  There may still be bugs.\n\nChanging the global system directory caused strange behaviour on Windows, and also seems like a bad idea.  Isn't the point of a default directory for job and fs instances to insulate them from the global system default directory?  We do currently cache the LocalFs instance, which may be a bug, but changing the process's CWD affects other applications in the JVM (like even our own datanode & namenode) which might not expect the CWD to move under them.\n\n> Could we extend the FileSystem interface to add makeAbsolute?\n\nThe long-term fix that I'd like to see is that we stop using java.io.File for file names.  We could use String, we could use our own FileName class, or we could perhaps use java.net.URI.  The advantage of URI is that it can also naturally include the namenode host and port.  The disadvantage is that URI does not support tree operations like getParent().  I'll log a bug for this."], "derived": {"summary": "LocalFileSystem. makeAbsolute() has a bug when running on Windows (which is very useful for the development phase of a Hadoop task on one's laptop).", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "LocalFileSystem.makeAbsolute bug on Windows - LocalFileSystem. makeAbsolute() has a bug when running on Windows (which is very useful for the development phase of a Hadoop task on one's laptop)."}, {"q": "What updates or decisions were made in the discussion?", "a": "> Doesn't your fix break on unix if the user has a directory named \"\\\\foo\"?\n\nPerhaps.  I did not test this case.  I was just despately trying to get Nutch to work correctly standalone & pseudo-distributed on Windows and linux.  There may still be bugs.\n\nChanging the global system directory caused strange behaviour on Windows, and also seems like a bad idea.  Isn't the point of a default directory for job and fs instances to insulate them from the global system default directory?  We do currently cache the LocalFs instance, which may be a bug, but changing the process's CWD affects other applications in the JVM (like even our own datanode & namenode) which might not expect the CWD to move under them.\n\n> Could we extend the FileSystem interface to add makeAbsolute?\n\nThe long-term fix that I'd like to see is that we stop using java.io.File for file names.  We could use String, we could use our own FileName class, or we could perhaps use java.net.URI.  The advantage of URI is that it can also naturally include the namenode host and port.  The disadvantage is that URI does not support tree operations like getParent().  I'll log a bug for this."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-126", "title": "\"hadoop dfs -cp\" does not copy crc files", "status": "Closed", "priority": "Major", "reporter": "Konstantin Shvachko", "assignee": "Konstantin Shvachko", "labels": [], "created": "2006-04-08T10:47:11.000+0000", "updated": "2009-07-08T16:41:51.000+0000", "description": "DFSShell.copy() uses FileUtil.copyContents() which works with \"Raw\" files, and does not copy crc files.\nTry\nhadoop dfs -cp a b\nThen either\nhadoop dfs -cat b\nhadoop dfs -copyToLocal b c\nThe last two complain about crc files\nIn fact DFSShell.copy() should use the same FileSystem methods as copyToLocal and copyFromLocal do.\n", "comments": ["This fixes the problem.", "I just committed this patch.  Thanks, Konstantin!"], "derived": {"summary": "DFSShell. copy() uses FileUtil.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "\"hadoop dfs -cp\" does not copy crc files - DFSShell. copy() uses FileUtil."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this patch.  Thanks, Konstantin!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-127", "title": "Unclear precedence of config files and property definitions", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": "Doug Cutting", "labels": [], "created": "2006-04-10T17:47:40.000+0000", "updated": "2007-10-03T18:06:13.000+0000", "description": "The order in which configuration resources are read is not sufficiently documented, and also there are no mechanisms preventing harmful re-definition of certain properties, if they are put in wrong config files.\n\nFrom reading the code in Hadoop Configuration.java, JobConf.java and Nutch NutchConfiguration.java I _think_ this is what's happening.\n\nThere are two groups of resources: default resources, loaded first, and final resources, loaded at the end. All properties (re)-defined in files loaded later will override any previous definitions:\n\n* default resources: loaded in the order as they are added. The following files are added here, in order:\n\n    1. hadoop-default.xml (Configuration)\n    2. nutch-default.xml  (NutchConfiguration)\n    3. mapred-default.xml (JobConf)\n    4. job_xx_xxx.xml       (JobConf, in JobConf(File config))\n\n* final resource: which always come after default resources, i.e. if any value is defined here it will always override those set in default resources (NOTE: including per job settings!!!). The following files are added here, in reversed order:\n\n    2. hadoop-site.xml (Configuration)\n    1. nutch-site.xml    (NutchConfiguration)\n\n(i.e. hadoop-site.xml will take precedence over anything else defined in any other config file).\n\nI would appreciate checking that this is indeed the case, and suggestions how to ensure that you cannot so easily shoot yourself in the foot if you define wrong properties in hadoop-site or nutch-site ...", "comments": ["I think you have it right.  Some guidelines:\n\nFolks should only define things in the -site files if they want to force them for all code.\n\nFolks should not edit the -default files.\n\nNon-default settings that may be overridden by application code should be put in mapred-default.xml.\n\nApplication settings are set in the job.\n\nStrictly speaking, it doesn't much matter whether you put something in a nutch- or hadoop- file, although the intent is to keep things that are specific to Hadoop in hadoop- files and things specific to Nutch in nutch- files.\n\n\n", "<quote>Folks should only define things in the -site files if they want to force them for all code. </quote>\n\nI should have read this earlier, it would have saved me some time.\n\nActually, the fact that properties defined in hadoop-final.xml override EVERYTHING, included properties defined in job config files, is something very important that should be well documented, because it's not the intuitively expected behaviour (which, to me, was:\n - hadoop-default.xml, mapred-default.xml overrided by\n - hadoop-final.xml, overrided by\n - job config files\n\nI've searched the wiki (afterwards, unfortunately) and it's very well documented there. However, the comments included in hadoop-default.xml and other delivered config files are not clear about this. Maybe they should be detailed, or just link to the wiki page.\n\n", "I believe this was fixed by HADOOP-785."], "derived": {"summary": "The order in which configuration resources are read is not sufficiently documented, and also there are no mechanisms preventing harmful re-definition of certain properties, if they are put in wrong config files. From reading the code in Hadoop Configuration.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Unclear precedence of config files and property definitions - The order in which configuration resources are read is not sufficiently documented, and also there are no mechanisms preventing harmful re-definition of certain properties, if they are put in wrong config files. From reading the code in Hadoop Configuration."}, {"q": "What updates or decisions were made in the discussion?", "a": "I believe this was fixed by HADOOP-785."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-128", "title": "Failure to replicate dfs block kills client", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-04-11T00:19:13.000+0000", "updated": "2009-07-08T16:41:51.000+0000", "description": "When the datanode gets an exception, which is logged as:\n\n060407 155835 13 DataXCeiver\njava.io.EOFException\n        at java.io.DataInputStream.readFully(DataInputStream.java:178)\n        at java.io.DataInputStream.readLong(DataInputStream.java:380)\n        at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:462)\n        at java.lang.Thread.run(Thread.java:595)\n\nIt closes the user's connection to the data node, which causes the client to get an IOException from:\n\n        at java.io.DataInputStream.readFully(DataInputStream.java:178)\n        at java.io.DataInputStream.readLong(DataInputStream.java:380)\n        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.internalClose(DFSClient.java:883)\n ", "comments": ["This patch changes the client so that:\n   1. it has replication * 1 minute timeout for the block replicas to be written.\n   2. improved logging, including the filename and remote hostname when things fail\n   3. \n\nIt patches the DataNode so that:\n   1. Failures downstream (from the mirror nodes) never propagate back upstream.\n   2. Improved logging including filenames and remote host names.\n   3. the changes involve a lot of whitespace changes because of block changes, so i'll include a separate upload that ignores whitespaces.", "These are the diffs to DataNode.java ignoring whitespaces.", "The read and write block functionality needs to be factored out of the huge if/then/else. I'll open a new bug for that.", "I forgot the default value for the retries value.", "I just committed this.\n\nI note that you increased the timeout in the client, presumably to account for timeouts down the replication chain.  But shouldn't we then also increase the timeout in the datanode when it connects to the next link in the chain?  It didn't look like you added that.\n\n+1 for refactoring this (in another patch).  The logic of this is hard to follow!\n"], "derived": {"summary": "When the datanode gets an exception, which is logged as:\n\n060407 155835 13 DataXCeiver\njava. io.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Failure to replicate dfs block kills client - When the datanode gets an exception, which is logged as:\n\n060407 155835 13 DataXCeiver\njava. io."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.\n\nI note that you increased the timeout in the client, presumably to account for timeouts down the replication chain.  But shouldn't we then also increase the timeout in the datanode when it connects to the next link in the chain?  It didn't look like you added that.\n\n+1 for refactoring this (in another patch).  The logic of this is hard to follow!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-129", "title": "FileSystem should not name files with java.io.File", "status": "Closed", "priority": "Major", "reporter": "Doug Cutting", "assignee": "Doug Cutting", "labels": [], "created": "2006-04-11T03:05:09.000+0000", "updated": "2006-08-03T17:46:35.000+0000", "description": "In Hadoop's FileSystem API, files are currently named using java.io.File.  This is confusing, as many methods on that class are inappropriate to call on Hadoop paths.  For example, calling isDirectory(), exists(), etc. on a java.io.File is not the same as calling FileSystem.isDirectory() or FileSystem.exists() passing that same file.  Using java.io.File also makes correct operation on Windows difficult, since java.io.File operates differently on Windows in order to accomodate Windows path names.  For example, new File(\"/foo\") is not absolute on Windows, and prints its path as \"\\\\foo\", which causes confusion.\n\nTo fix this we could replace the uses of java.io.File in the FileSystem API with String, a new FileName class, or perhaps java.net.URI.  The advantage of URI is that it can also naturally include the namenode host and port.  The disadvantage is that URI does not support tree operations like getParent().\n\nThis change will cause a lot of incompatibility.  Thus it should probably be made early in a development cycle in order to maximize the time for folks to adapt to it.", "comments": ["I think we should change this to a Hadoop-specific class, e.g. FileName (not a simple String - too limiting). FileName-s could only be used when holding a reference to a valid instance of FileSystem - this way operations like getParent() could always consult FileSystem-specific routines to resolve DFS names to real names in case of LocalFileSystem.\n\nI also propose that this class should be versioned, and contain some File-like metadata - for now I'm thinking specifically about creation / modification time.", ">  I think we should change this to a Hadoop-specific class, e.g. FileName.\n\nWhy not URI?  What required methods are missing from URI?  Conversely, what URI methods do you think might cause problems?\n\nPartially answering my own question, with URIs we'd have to check the schema host and port matched the fs when implementing each FS method.  In other words, given that we need a FileSystem instance to do anything, the schema, host and port fields of the URI are usually redundant and force us to perform error checking.  However these same fields would be useful when specifying MapReduce input and output directories, in command lines, etc., permitting one to easily specify non-default FileSystem implementations.\n\nNote that I don't think URI buys us interoperability with other systems.  So we should only use it if we think it will make writing Hadoop easier: if it consists of code that we'd need to mostly need to write anyway.\n\nA side-benefit of URI is that it provides standards-defined filename syntax.  We don't have to figure out how to, e.g., escape things, or how backslashes and colons should be treated, etc.  We can simply point to a standard.\n\n> I also propose that this class should be versioned, and contain some File-like metadata - for now I'm thinking specifically about creation / modification time.\n\nThis works so long as files are write-once.  But if they can be appended to or overwritten then this information could get stale.", "URI actually *can* compute parent directory.  For example:\n\nURI subDir = new URI(\"/foo/bar/baz/\");\nURI parent = subDir.resolve(\"..\");\n\nParent.toString() returns \"/foo/bar/\".\n\nSo I think that URI has the features we want for filenames and not much else.  Am I missing something?\n\nIt might also be useful to implement a URLStreamHandler, so that one can create \"hdfs:\" urls and use them whereever java accepts URLs, e.g., in classloaders, etc.  But the URL class doesn't support relative path name resolution, the primary feature we require for names.\n\nUnless there are objections, I'll start exploring replacing the uses of java.io.File with java.net.URI.\n\nMy thinking is that we remove rather than deprecate the old methods.  This makes the change incompatible, but I think we really want to get rid of the use of java.io.File.  I'm willing to update Nutch & unit tests as required, but this may break others' code.  Should we instead deprecate these in Hadoop 0.2 and then remove them in 0.3?  Thoughts?", "Working through this more, I'm now leaning away from URI and towards a new class.  It will be easier to replace with a new class, since the API can be made to resemble File.  For example, we have a lot of code that calls 'new File(dir, name)' to construct a file in a subdirectory.  The idiom for doing that with URI's is slightly more complicated, and would require a utility method somewhere.  Similarly for file.getParentFile(), etc.\n\nSo now I'm leaning towards a class named \"Path\" that's mostly a drop-in replacement for File, except it doesn't support FS operations like exists(), mkdir(), delete(), etc.", "It could contain a URI... ", "Does it make sense to create class that would extend File and override unsupported operations to throw UnsupportedOperationException?", "> Does it make sense to create class that would extend File and override unsupported operations to throw UnsupportedOperationException?\n\nI'm not sure what advantages that would have.  Is the idea to detect errors at runtime rather than at compile time?  I've just about finished a patch adding a new class.  I'll post it later today.", "Here's a patch that replaces uses of java.io.File in Hadoop's FileSystem and MapReduce API's with a new class named Path.  I left some existing File-based methods, now deprecated, sufficient for Nutch to run w/o alteration.  I'd like to remove the deprecated methods after the 0.2 release.\n\nI believe that the only incompatible change is that dfs.data.dir and mapred.local.dir, when lists of directories, must now be comma-separated and may no longer be space-separated.  This is in order to make things work better on Windows.\n\nI have tested this in standalone and pseudo-distributed operation on both Linux and Windows, with unit tests and with the Nutch crawler.\n\nBarring objections, I will apply this tomorrow.", "I just committed this.  It was a big change.  I hope I haven't broken anything!"], "derived": {"summary": "In Hadoop's FileSystem API, files are currently named using java. io.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "FileSystem should not name files with java.io.File - In Hadoop's FileSystem API, files are currently named using java. io."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  It was a big change.  I hope I haven't broken anything!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-130", "title": "Should be able to specify \"wide\" or \"full\" replication", "status": "Closed", "priority": "Minor", "reporter": "Bryan Pendleton", "assignee": "Konstantin Shvachko", "labels": [], "created": "2006-04-11T04:47:43.000+0000", "updated": "2009-07-08T16:41:52.000+0000", "description": "Should be able to specify that a file be \"fully\" or \"widely\" replicated, rather than an explicit replication count. This would be useful for job configuration and jar files, and probably other files whose use is wide enough to necessitate reducing latency to access them.\n\nThe current implementation will also complain if you specify replication that is wider than the system's maximum replication value, and has no facility to enable \"full\" replication should the number of datanodes exceed the current maximum settable value of 32k.\n", "comments": ["This issue has been discussed several times.  Most recently in HADOOP-170.\n\nReplicating to every node rarely seems useful and will require more mechanism.\nReplicating to squareroot(number of active reader nodes) seems most correct.\n\nHaving rack aware placement and several options will eventually be useful.\n\nI suggest putting in the API and just setting replication to 10 (or 3 if number of nodes <30), which will yield a good speedup on even large clusters and will not cause problems in general."], "derived": {"summary": "Should be able to specify that a file be \"fully\" or \"widely\" replicated, rather than an explicit replication count. This would be useful for job configuration and jar files, and probably other files whose use is wide enough to necessitate reducing latency to access them.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Should be able to specify \"wide\" or \"full\" replication - Should be able to specify that a file be \"fully\" or \"widely\" replicated, rather than an explicit replication count. This would be useful for job configuration and jar files, and probably other files whose use is wide enough to necessitate reducing latency to access them."}, {"q": "What updates or decisions were made in the discussion?", "a": "This issue has been discussed several times.  Most recently in HADOOP-170.\n\nReplicating to every node rarely seems useful and will require more mechanism.\nReplicating to squareroot(number of active reader nodes) seems most correct.\n\nHaving rack aware placement and several options will eventually be useful.\n\nI suggest putting in the API and just setting replication to 10 (or 3 if number of nodes <30), which will yield a good speedup on even large clusters and will not cause problems in general."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-131", "title": "Separate start/stop-dfs.sh and start/stop-mapred.sh scripts", "status": "Closed", "priority": "Minor", "reporter": "Chris A. Mattmann", "assignee": "Doug Cutting", "labels": [], "created": "2006-04-11T09:48:05.000+0000", "updated": "2009-07-08T16:41:51.000+0000", "description": "Hadoop needs start-dfs/mapred.sh scripts, and stop-dfs/mapred.sh scripts, to independently start mapred, or independently start dfs. This way, users can use a single subsystem of the full Hadoop component library.", "comments": ["patch that separates out start/stop-dfs.sh and start/stop-mapred.sh scripts, and makes start-all.sh and stop-all.sh call them.", "I just committed this, fixing a typo in start-mapred.sh.  Thanks, Chris.\n"], "derived": {"summary": "Hadoop needs start-dfs/mapred. sh scripts, and stop-dfs/mapred.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Separate start/stop-dfs.sh and start/stop-mapred.sh scripts - Hadoop needs start-dfs/mapred. sh scripts, and stop-dfs/mapred."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this, fixing a typo in start-mapred.sh.  Thanks, Chris."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-132", "title": "An API for reporting performance metrics", "status": "Closed", "priority": "Major", "reporter": "David Bowen", "assignee": null, "labels": [], "created": "2006-04-14T03:12:36.000+0000", "updated": "2006-08-03T17:46:35.000+0000", "description": "I'd like to propose adding an API for reporting performance metrics.  I will post some javadoc as soon as I figure out how to do so.  The idea is for the API to be sufficiently abstract that various different implementations can be plugged in.  In particular, there would be one that just writes the metric data to a file, and another that sends metrics to Ganglia.  It would also be possible to plug in an implementation that can support high-frequency (say, per-second) sending of fairly large amounts of data (up to hundreds of metrics) across the network.\n\nI'd be very interested in people's thoughts about what the requirements should be for such an API.\n\n- David Bowen\n", "comments": ["Here is the proposed javadoc.  Comments on the API are welcome also.\n", "Looks good.  Would be sweet having job progress show in Ganglia.  Metrics would run on every slave?\n\nI'd suggest that MetricsRecord and guages need descriptions (Adding to Record would be easy enough -- harder to guage going by your API so far).  Would come in handy in an admin page listing available records and their guages.  if they had descriptions, then Record could be roughly mapped to jmx MBean and gauge to jmx Attribute.", "A nit: the code should be in package org.apache.hadoop.metrics, not org.hadoop.metrics.", "Here is an updated API, incorporating the feedback I've received so far.  The main\nchanges are \n\n   (1) Ganglia support is included - this doesn't really affect the API, but the javadoc describes the configuration options which might be of interest.\n\n   (2) The SPI (service provider interface) is now in a separate package, and it does most of the implementation work so that implementation packages (like the file and ganglia sub-packages) can be quite small.\n\n  (3) The unbuffered option has been removed.  This means that developers can use the API freely in inner-loops without the concern that metric reporting might be configured to emit data on every update.  The data will just be stored in an internal table, and the table will only be sent periodically to the metrics server.\n\n", "+1 Looks good to me!\n\nI think there's a typo in the overview, where you should have setMetric() you instead have setGauge().\n\n", "\nOK, here is the source code, in the form of a patch.\n\nNOTE: this requires javac.version 1.5.  I.e. you currently have to say \"ant -Djavac.version=1.5\".  Is this acceptable in general for Hadoop development? \nThe 1.5 (or 5.0 as they call it) for-loop syntax is very convenient.\n\nA couple of changes since yesterday's javadoc: \n\n(1) there is now a NullContext which is used by default and does nothing; formerly the FileContext was used by default and it wrote to stdout if a file was not specified.\n\n(2) there is now an Updater interface for the callbacks, instead of using Runnable.  This is so that the MetricsContext can be passed as an argument to the callback.\n\n", "We should still stick with JDK 1.4.  Most developers are now using JDK 1.5, but many folks still use JDK 1.4 in production.  So, if it's not too much pain, please back-port this to 1.4.  Thanks!", "OK, here is an updated version of the patch with the code made uglier and less robust so as to accommodate people who perversely insist on running with JDK 1.4.  Also, I added a note to CHANGES.txt.\n", "Woops, forgot to update CHANGES.txt before modifying it. ", "I just committed this.  Thanks, David!"], "derived": {"summary": "I'd like to propose adding an API for reporting performance metrics. I will post some javadoc as soon as I figure out how to do so.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "An API for reporting performance metrics - I'd like to propose adding an API for reporting performance metrics. I will post some javadoc as soon as I figure out how to do so."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, David!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-133", "title": "the TaskTracker.Child.ping thread calls exit", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-04-14T04:09:39.000+0000", "updated": "2009-07-08T16:51:43.000+0000", "description": "The TaskTracker.Child.startPinging thread calls exit if the TaskTracker doesn't respond. Calling exit in a mutli-threaded program is really problematic. In particular, it prevents cleanup/finally clauses from running. We need to move to a model where it uses Thread.interrupt(), which means we need to check the interrupt flag in place in the map loop and reduce loop and stop masking the InterruptExceptions.", "comments": ["We can't always rely on cleanup/finally stuff to run.  JVMs can exit unexpectedly.  We hope it doesn't happen often, but we must be able to handle that situation.  If we need to, e.g., clean up temp files, we do that on startup.\n\nThe reason this was added was to handle the case where the tasktracker has exited and the child is somehow hung.  We must not leave stray, hung, JVMs around.  Thread.interrupt() is not reliable enough.  When a thread is hung, it will not recieve an interrupt.  I've seen this frequently when fetching, where socket read()  requests hang indefinitely, despite the socket having a short read timeout.\n\nSo I'd be happy to have this first try to exit more gracefully, but, after a time, it should still call exit().  The child processes do not have a pid file.  Once their parent has died, nothing tracks them, so they must reliably exit fairly quickly when their parent dies.", "Ok, I can see wanting to try gently first and then pull out the exit sledgehammer if it doesn't work.\n\nChecking back with the taskTracker once a second seems too frequent.\n\nI'd also propose changing the ping message to return a boolean that means the task should exit, so that IOException can mean that there was a communication error. If there is a communication error, it would make sense to retry a few times.", "Sure, that would be safer, but recall that this communication is all on the same host.  A tasktracker shouldn't have more than a handful of children, so per second pings should not be a great burden.  And communications problems to localhost seem unlikely.  I've seen nodes with loads over 100, timing out all sorts of requests from other hosts, and I've never seen \"Parent died\" logged when a tasktracker was really still alive.  But, still, it shouldn't hurt to try a few times.", "That said, it's possible that the exit() somehow swallows the \"Parent died\" message and all that we see is the \"Child died with nonzero exit status\" message.  We do see those more than we'd like.  I don't see how the message could get swallowed, however.  The parent has a thread reading the standard error of the child to EOF.  The logged message should get flushed to standard error before the child process exits.  And the kernel shouldn't throw away the last buffer written when a process exits.  So I doubt that's happening.", "I've been assuming that the \"Child died with nonzero exit status\" with no other error messages is coming from the failure of a ping. I will change that message to include the actual exit status and change the ping-failure exit code to 123 or something.", "Sounds like a good plan.", "The results are interesting. Based on a 45 minute run (on 195 linux nodes) of my random writer, I got:\n\n226 instaces of exit == 143\n11 instances of exit == -113\n  2 instances of exit == 65   // exception thrown on the ping\n  0 instances of exit == 66  // task tracker not recognizing the task\n\nSo there is a lot of dying going on that I don't understand. Tomorrow, I'll make a patch for my code that  changes ping from \"void ping(string)\" to \"boolean ping(string)\" where false means that the task is unknown and exceptions get a second chance. Do you happen to recognize either 143 or -113? Doing a quick search in eclipse, I didn't see them.", "Searching around a bit, I found a few folks who suggest using -Xrs if you're seeing exit code 143, since this can be caused by signals.  So you might try adding -Xrs to mapred.child.java.opts and see if that helps any.", "The 143's could be normal.  We call process.destroy() to stop a task. This probably sends a SIGTERM, and SIGTERM causes an exit 143, according to http://scv.bu.edu/SCV/FAQ/batchcode.txt.\n\n", "Here is a patch that allows retries for communication problems on the ping from the Task to the TaskTracker. It changes the interface of ping from returning nothing to returning a boolean. A false return value means the child should immediately go away. Exceptions are treated as temporary problems and given 3 chances. This patch also changes the exit code to be either 65 (asked to kill self) or 66 (ping exception).", "I just committed this.  Thanks, Owen!"], "derived": {"summary": "The TaskTracker. Child.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "the TaskTracker.Child.ping thread calls exit - The TaskTracker. Child."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-134", "title": "JobTracker trapped in a loop if it fails to localize a task", "status": "Closed", "priority": "Major", "reporter": "Runping Qi", "assignee": "Owen O'Malley", "labels": [], "created": "2006-04-14T06:58:16.000+0000", "updated": "2009-07-08T16:51:42.000+0000", "description": "\nThe symptoms:\n\n    When I ran  jobs on a big cluster, I noticed that some jobs got stucked. Some map tasks never got started. When I look at the log of the task tracker responsible for the tasks, I saw the following exceptions:\n\n060413 160702 Lost connection to JobTracker [kry1040/72.30.116.100:50020].  Retrying...\njava.io.IOException: No valid local directories in property: mapred.local.dir\n        at org.apache.hadoop.conf.Configuration.getFile(Configuration.java:282)\n        at org.apache.hadoop.mapred.JobConf.getLocalFile(JobConf.java:127)\n        at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.localizeTask(TaskTracker.java:391)\n        at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.<init>(TaskTracker.java:383)\n        at org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:270)\n        at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:336)\n        at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:756)\n\nThe reason for the exception is that the directory hadoop/mapred/local has \"wrong\" owner, thus the task tracker cannot access to it.\nThis caused the task tracker stucked into the following loops:\n\n            while (running) {\n                boolean staleState = false;\n                try {\n                    // This while-loop attempts reconnects if we get network errors\n                    while (running && ! staleState) {\n                        try {\n                            if (offerService() == STALE_STATE) {\n                                staleState = true;\n                            }\n                        } catch (Exception ex) {\n                            LOG.log(Level.INFO, \"Lost connection to JobTracker [\" + jobTrackAddr + \"].  Retrying...\", ex);\n                            try {\n                                Thread.sleep(5000);\n                            } catch (InterruptedException ie) {\n                            }\n                        }\n                    }\n                } finally {\n                    close();\n                }\n                LOG.info(\"Reinitializing local state\");\n                initialize();\n            }\n\nIssue 1:\n    Method offerService() must catch and handle the exceptions that may be thrown from new TaskInProgress() call, and report back to the job tracker if it cannot run the task. This way, the task can be assigned to other task tracker.\n\nIssue 2:\n    The taskTracker should check whether it can access to the local dir at the initialization time, before taking any tasks.\n\n\nRunping\n", "comments": ["Ok, this patch fixes the problem. In particular,\n  1. adds the hostname to the task tracker names.\n  2. moves exception-raising code out of the constructor for TaskInProgress\n  3. pulls the task start up code into a separate procedure so that I can make sure it doesn't throw any exceptions\n  4. records failures during initialization of tasks\n  5. moves the stringifyException into the utils package.", "This is the previous patch updated to reflect the Path changes that went in today.", "I just committed this.  Thanks, Owen!"], "derived": {"summary": "The symptoms:\n\n    When I ran  jobs on a big cluster, I noticed that some jobs got stucked. Some map tasks never got started.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "JobTracker trapped in a loop if it fails to localize a task - The symptoms:\n\n    When I ran  jobs on a big cluster, I noticed that some jobs got stucked. Some map tasks never got started."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-135", "title": "Potential deadlock in JobTracker.", "status": "Closed", "priority": "Major", "reporter": "Konstantin Shvachko", "assignee": "Owen O'Malley", "labels": [], "created": "2006-04-14T07:28:03.000+0000", "updated": "2009-07-08T16:51:43.000+0000", "description": "org.apache.hadoop.mapred.JobTracker.RetireJobs.run()\nlocks resources in this order\n                synchronized (jobs) {\n                    synchronized (jobInitQueue) {\n                        synchronized (jobsByArrival) {\n\norg.apache.hadoop.mapred.JobTracker.submitJob()\nlocks resources in a different order\n        synchronized (jobs) {\n            synchronized (jobsByArrival) {\n                synchronized (jobInitQueue) {\n\nThis potentially can lead to a deadlock.\nUnless there is common lock on top of it in which case these\nthree locks are redundant.", "comments": ["Reverse the order of locking for the two fields so that they are always acquired in the same order.", "I just committed this.  Thanks, Owen."], "derived": {"summary": "org. apache.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Potential deadlock in JobTracker. - org. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-136", "title": "Overlong UTF8's not handled well", "status": "Closed", "priority": "Minor", "reporter": "Dick King", "assignee": "Hairong Kuang", "labels": [], "created": "2006-04-15T03:11:52.000+0000", "updated": "2006-08-30T23:33:51.000+0000", "description": "When we feed an overlong string to the UTF8 constructor, two suboptimal things happen.\n\nFirst, we truncate to 0xffff/3 characters on the assumption that every character takes three bytes in UTF8.  This can truncate strings that don't need it, and it can be overoptimistic since there are characters that render as four bytes in UTF8.\n\nSecond, the code doesn't actually handle four-byte characters.\n\nThird, there's a behavioral discontinuity.  If the string is \"discovered\" to be overlong by the arbitrary limit described above, we truncate with a log message, otherwise we signal a RuntimeException.  One feels that both forms of truncation should be treated alike.  However, this issue is concealed by the second issue; the exception will never be thrown because UTF8.utf8Length can't return more than three times the length of its input.\n\nI would recommend changing UTF8.utf8Length to let its caller know how many characters of the input string will actually fit if there's an overflow [perhaps by returning the negative of that number] and doing the truncation accurately as needed.\n\n-dk\n\n", "comments": ["The UTF8 class was originally written to be compatible with Sun's DataOutput.writeUTF:\n\nhttp://java.sun.com/j2se/1.4.2/docs/api/java/io/DataOutput.html#writeUTF(java.lang.String)\n\nLong-term it would be good to support strings with more than 2^16 characters and to support 4-byte UTF8 characters, but this will be hard to do back-compatibly.\n\nShort term, would you or someone else like to submit a patch improving UTF8's handling of long strings?  Some unit tests demonstrating these problems would also be welcome.  Thanks!\n\n", "I need a fix for this. (Serialization of long UTF8 strings)\n\nI have two proposals.\nI am not addressing 4-byte UTF8 characters.\nWhat would others recommend here?\n\n\nOption 1. \n\nAn alternate encoding for potentially long Strings. \nCode must explicitely choose to write and read back the \"large\" version.\n\nTo share as much code as possible, \njust add an optional argument to the UTF8 constructors: boolean large\nIf large mode:\n the length encoding is a VarLenShort (see below)\nelse:\n the length encoding is a short (the current format)\n\nNote about static methods in class o.a.h.io.UTF8:\nThis change requires instance state (for the boolean large flag)\nSo the static versions of the UTF8 methods would ignore this change.\nThis should not be a problem since the code mentions that \nthe static methods are deprecated and will go away,\n   \n   \n   \nOption 2. A semi-backward-compatible change.\n\nIn fact this is the same change as Option 1, \nexcept that we always assume large = true\n\nin UTF8 change this:\n  int bytes = in.readUnsignedShort();\nto this:\n  int bytes = in.readVarLenShort(); \n(and similarly for writes)\n\nThis is word-level variable-length encoding:\nif the highest bit of the length word (16th bit) is set, \nthen there is an extension word for the length. \nTotal payload: 30 bits worth of length, which is enough.\n\nFor short enough Strings, the length encoding is unchanged \nThis is why it is semi-backwards-compatible.\n\nWhat inputs are currently accepted:\n Unicode strings, clipped at 0xffff/3=21845 characters.\n\nWhat would be backwards compatible:\n Strings of encoded length <= 32767 bytes\n This includes: \n  o content with average character length less than 32767/21845 = 1.5 bytes\n  o in partic. all single-byte UTF-8 (ASCII, iso-8859)\n \n", "What about simply introducing a new UTF8_String class that does the right thing for the complete range of characters and deprecating the use of the current class?", "I would vote for option 2 above.  I'm having difficulty imagining an existing file that has the following three properties:\n\n1: too large to convert if need be with a one-off\n\n2: has UTF8's larger than 32767 bytes encoded but smaller than 21845 characters\n\n3: hasn't already been trashed by a UTF8 that was truncated.\n\nI woul dlike to go after the 4-byters as well.  There's no compatibility issue here.\n\n-dk\n", "I think we should aim for 100% back-compatibility.  Perhaps if the 16th bit is set, and the next byte is invalid UTF-8 (e.g., a null) then we can assume the fourth byte is part of the length, otherwise the third and fourth are payload data.  That would permit up to 2^23 bytes.  In a subsequent release we could drop this \"feature\", permitting up to 2^31 bytes.  Could something like that work?\n", "OK, given the need for 100% backward-compatibility and given that I will only use this format internally:\nI will go with Sameer's suggestion: \nhave a separate class org.apache.hadoop.io.LargeUTF8.java\n\nI will just make its length field 4-bytes, rather than var-length:\nOtherwise this would complicate things like the ad-hoc offset computations in UTF8.Comparator.\n\n\nShould I make a TestLargeUTF8 based on TestUTF8?\n\n---\n\nConcerning 4-bytes-long UTF-8 characters:\nit seems that this situation does not occur in \"Java-modified-UTF8\"\n\nThe 4-byte chars are represented as 3+3.\nSee Modified UTF-8 in:\nhttp://java.sun.com/developer/technicalArticles/Intl/Supplementary/\n\n", "Here is the patch, two new files:\n\norg.apache.hadoop.io.TestLargeUTF8\norg.apache.hadoop.io.LargeUTF8\n\nThe only difference with the UTF8 string format is that \nthe length is stored on 4 bytes rather than 2 bytes.\n\nTestLargeUTF8 tests serialization of larger strings up to 1MB\n\n\n", "If we can't fix the UTF8 class back-compatibly, then I'd prefer we provide a replacement class and deprecate UTF8.  Long-term we don't want two classes around that do almost the same thing.  We could name the new class Text or Chars.  It could use a variable number of bytes to indicate its length, using the 1 byte for lengths less than 127, two-bytes for lengths less than 2^14, etc., then encode the content with UTF-8.  Note that the length should be the number of bytes of content, not the number of encoded characters.  Does this sound reasonable?\n", "The static methods for converting integers and longs to zero-encoded representation (similar to what you have suggested) already exists in the record IO Util class.", "Just to verify, which length-encoding scheme are we using for class Text (aka LargeUTF8)\n\na) The \"UTF-8/Lucene\" scheme? (highest bit of each byte is an extension bit, which I think is what Doug is describing in his last comment) or \nb) the record-IO scheme in o.a.h.record.Utils.java:readInt\n\nEither way, note that:\n\n1. UTF8.java and its successor Text.java need to read the length in two ways: \n  1a.  consume 1+ bytes from a DataInput and \n  1b. parse the length within a byte array at a given offset \n(1.b is used for the \"WritableComparator optimized for UTF8 keys\" ).  \n\no.a.h.record.Utils only supports the DataInput mode.\nIt is not clear to me what is the best way to extend this Utils code when you need to support both reading modes\n\n2 Methods like UTF8's WritableComparator are to be low overhead, in partic. there should be no Object allocation. \nFor the byte array case, the varlen-reader utility needs to be extended to return both: \n the decoded length and the length of the encoded length.\n (so that the caller can do offset += encodedlength)\n   \n3. A String length does not need (small) negative integers.\n\n4. One advantage of a) is that it is standard (or at least well-known and natural) and there are no magic constants  (like -120, -121 -124)\n\n", "FYI:\nsome info on Java-modified UTF-8 \n(this was previously posted)\nSee Modified UTF-8 in: \nhttp://java.sun.com/developer/technicalArticles/Intl/Supplementary/ \n\nAs far as i understand:\nThe bottom-line is that supplementary UTF-8 characters:\no would be encoded as 4+ bytes in non-Java programs\no but they are already encoded as two Java char-s (i.e. two-bytes) when our converter code sees them.\no and so the conversion to UTF-8 proceeds on these two chars independently.\no So all the existing Java UTF-8 code that only handles 1..3-bytewide chars is already compliant with Java-modified UTF-8.\n\nWhat do the java-i18n experts think?\n\n---\nEarlier comment:\n\nConcerning 4-bytes-long UTF-8 characters: \nit seems that this situation does not occur in \"Java-modified-UTF8\" \n\nThe 4-byte chars are represented as 3+3. \nSee Modified UTF-8 in: \nhttp://java.sun.com/developer/technicalArticles/Intl/Supplementary/ \n", "this was fixed in 0.5"], "derived": {"summary": "When we feed an overlong string to the UTF8 constructor, two suboptimal things happen. First, we truncate to 0xffff/3 characters on the assumption that every character takes three bytes in UTF8.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Overlong UTF8's not handled well - When we feed an overlong string to the UTF8 constructor, two suboptimal things happen. First, we truncate to 0xffff/3 characters on the assumption that every character takes three bytes in UTF8."}, {"q": "What updates or decisions were made in the discussion?", "a": "this was fixed in 0.5"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-137", "title": "Different TaskTrackers may get the same task tracker id, thus cause many problems.", "status": "Closed", "priority": "Critical", "reporter": "Runping Qi", "assignee": "Owen O'Malley", "labels": [], "created": "2006-04-15T07:10:22.000+0000", "updated": "2009-07-08T16:51:43.000+0000", "description": "In the TaskTracker#Initialize method, the following line assigns task tracker name (id):\n\nthis.taskTrackerName = \"tracker_\" + (Math.abs(r.nextInt()) % 100000);\n\nFor a fair size cluster, it is possible that different task trackers to get the same id, causing name conflict.\nI encountered this problem with a cluster of 274 nodes. Once such conflict happens, a lot of strange things may happen.\nFor example, a reducer task tried to copy from a machine (task tracker) a map output file that was actually produced \non another machine.\n", "comments": ["\nA simple fix for the issue is to add hostname as part of task track names. \nI believe Owen has actually done this in trying to fix another issue, but the patch\nis not committed yet.\n\n", "This was fixed in the patch submitted for HADOOP-134", "Fixed with patch for HADOOP-134"], "derived": {"summary": "In the TaskTracker#Initialize method, the following line assigns task tracker name (id):\n\nthis. taskTrackerName = \"tracker_\" + (Math.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Different TaskTrackers may get the same task tracker id, thus cause many problems. - In the TaskTracker#Initialize method, the following line assigns task tracker name (id):\n\nthis. taskTrackerName = \"tracker_\" + (Math."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed with patch for HADOOP-134"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-138", "title": "stop all tasks", "status": "Closed", "priority": "Trivial", "reporter": "Stefan Groschupf", "assignee": "Doug Cutting", "labels": [], "created": "2006-04-15T08:59:28.000+0000", "updated": "2009-07-08T16:51:43.000+0000", "description": "When a tasktracker runs X tasks it require X heartbeats to stop all jobs.\nStop all tasks with one heartbeat improve the availability and free resources faster.   ", "comments": ["May be something like that would work?", "I just committed this.  Thanks, Stefan!"], "derived": {"summary": "When a tasktracker runs X tasks it require X heartbeats to stop all jobs. Stop all tasks with one heartbeat improve the availability and free resources faster.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "stop all tasks - When a tasktracker runs X tasks it require X heartbeats to stop all jobs. Stop all tasks with one heartbeat improve the availability and free resources faster."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Stefan!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-139", "title": "Deadlock in LocalFileSystem lock/release", "status": "Closed", "priority": "Major", "reporter": "Igor Bolotin", "assignee": "Doug Cutting", "labels": [], "created": "2006-04-15T10:39:58.000+0000", "updated": "2006-08-03T17:46:36.000+0000", "description": "LocalFileSystem lock/release methods marked synchronized and inside they lock file channel - this produces deadlock situation. Let's see how it happens: \n1. First thread locks the file and starts some long-running process.\n2. Second thread tries to lock the file and it blocks inside channel lock method. It  keeps LocalFileSystem instance \"locked\" as well. \n3. First thread finished it's processing and tries to release lock - it blocks because LocalFileSystem instance is \"locked\" by second thread - both threads are waiting to each other. \n", "comments": ["Attached is proposed patch. I removed entire lock/release method synchronization and replaced it with critical section synchronization for sets accesses - leaving channel lock/release calls outside of synchronized statement. ", "I just committed this.  Thanks, Igor!\n\nThe patch did not apply directly, due to other changes made to Hadoop since you contributed it.  So I had to apply parts of it manually.  Please check that it still looks correct.  Thanks!"], "derived": {"summary": "LocalFileSystem lock/release methods marked synchronized and inside they lock file channel - this produces deadlock situation. Let's see how it happens: \n1.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Deadlock in LocalFileSystem lock/release - LocalFileSystem lock/release methods marked synchronized and inside they lock file channel - this produces deadlock situation. Let's see how it happens: \n1."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Igor!\n\nThe patch did not apply directly, due to other changes made to Hadoop since you contributed it.  So I had to apply parts of it manually.  Please check that it still looks correct.  Thanks!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-140", "title": "General documentation", "status": "Closed", "priority": "Minor", "reporter": "Teppo Kurki", "assignee": null, "labels": [], "created": "2006-04-15T13:58:58.000+0000", "updated": "2006-12-15T23:02:11.000+0000", "description": "Getting a grasp of how Hadoop works is a little hard, because one first has to get a grip on the whole MapReduce thing and then figure out how it is carried out in Hadoop. Judging from the mailing list a little more general documentation would help a lot.\n\n", "comments": ["Xdoc document that describes how Hadoop carries out MapReduce that I came up with trying to understand everything. \n\nMaybe someone could read it and see that it is not completely off the mark?\n\n\n\n", "This looks mostly correct and is a very useful introduction.\n\nCan you please post it to the wiki: http://wiki.apache.org/lucene-hadoop/\n\nThanks!", "http://wiki.apache.org/lucene-hadoop/HadoopMapReduce\n\nThis really doesn't solve the issue, but unless someone is picking this task up maybe  this should be closed for now?", "Thanks for adding this to the wiki!"], "derived": {"summary": "Getting a grasp of how Hadoop works is a little hard, because one first has to get a grip on the whole MapReduce thing and then figure out how it is carried out in Hadoop. Judging from the mailing list a little more general documentation would help a lot.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "General documentation - Getting a grasp of how Hadoop works is a little hard, because one first has to get a grip on the whole MapReduce thing and then figure out how it is carried out in Hadoop. Judging from the mailing list a little more general documentation would help a lot."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks for adding this to the wiki!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-141", "title": "Disk thrashing / task timeouts during map output copy phase", "status": "Closed", "priority": "Major", "reporter": "Peter Sutter", "assignee": "Owen O'Malley", "labels": [], "created": "2006-04-18T05:21:17.000+0000", "updated": "2009-07-08T16:51:43.000+0000", "description": "\nMapOutputProtocol connections cause timeouts because of system thrashing and transferring the same file over and over again, ultimately leading to making no forward progress(medium sized job, 500GB input file, map output about as large as the input, 10 node cluster).\n\nThere are several bugs behind this, but the following two changes improved matters considerably.\n\n(1) \n\nThe buffersize in MapOutputFile is currently hardcoded to 8192 bytes (for both reads and writes). By changing this buffer size to 256KB, the number of disk seeks are reduced and the problem went away. \n\nIdeally there would be a buffer size parameter for this that is separate from the DFS io buffer size.\n\n(2)\n\nI also added the following code to the socket configuration in both Server.java and Client.java. No linger is a minor good idea in an enivronment with some packet loss (and you will have that when all the nodes get busy at once), but 256KB buffers is probably excessive, especially on a LAN, but it takes me two hours to test changes so I havent experimented.\n\nsocket.setSendBufferSize(256*1024);\nsocket.setReceiveBufferSize(256*1024);\nsocket.setSoLinger(false, 0);\nsocket.setKeepAlive(true);\n", "comments": ["reword that first sentance: \n\nReduce progress grinds to a halt with lots of MapOutputProtocol timeouts and transferring the same file over and over again because of system thrashing.\n", "It would be good to find which of these changes actually made the difference for you.  Each TaskTracker's map output server only accepts up to mapred.tasktracker.tasks.maximum connections at once.  Since this is typically around 2, I would be surprised if a small buffer size results in lots of seeks, since the OS should perform readaheads in its buffer cache.\n\nWhat OS are you using?  What do you have mapred.tasktracker.tasks.maximum set to?\n\nIf on linux, what does 'iostat -x 1 10' show when things are slow?  How about 'sar -n DEV 1 10'?\n", "\nAs it turns out, my changes did not fix the problem, just changed the timing.\n\nThe thrashing was occucring because one reducer was in the merge phase, and the other reducer was in the file copy phase.  The particular file that was failing, was being copied from the local system.  I have the concurrent merges set to 24 and the task count set to 4.\n\nI added logging statements, and the file was clearly being received in full by MapOutputFile, yet ReduceTaskRunner was getting a timeout on that file about 1 minute and 20 seconds later, request it again and again, and each time receive the file yet get a timeout just over a minute later.\n\nI did find two interesting bug in RPC.java while trying to track this down (which im filing separately), but for now I am completely stumped.\n\nAt the moment the cluster is otherwise busy, so I cant do any more experiments until perhaps tomororw. Any suggestions would be very welcome. We are using Linux, and I'll try the commands you suggested when Im able to recreate it, but for now this does not look like a disk or TCP problem, it really looks like an RPC scheduling problem. ", "Some timeouts during the copy phase may not be bad.  If too many nodes are transferring from a given node, then it may time out additional requests.  And if a one node is already transferring from a another node for one task, then attempts by a second task to transfer may timeout (due to the shared connection pool).  These should not affect overall performance too much, especially if the timeout is relatively short.", "\nA few timeouts would be fine. The problem is when the same files timeout over and over again, and progress ceases completely. \n\nI was able to make the problem go away by increasing the number of mappers by 6X, making the map output files 1/6th as large, so I have given up on finding the problem.\n\nSo here is the summary:\n\n- with 700MB map output files (18 mappers), original code: the job would never progress past reduce progress of  17% or 18%.\n- with 700MB map output files (18 mappers), large buffers: the job completed in 27 hours\n- with 120MB map output files (106 mappers), and large buffers: the job completed in 6 hours\n\nIm happy to share logs that include the timeouts and extended logging information on MapOutputFile.java if anyone is interested, but i wont post them here because they are several hundred megabytes.\n\nOtherwise I will continue to use the workaround of smaller map output files.", "Paul,\n   Is this still happening with the http map output transfer or can I close this?", "\n   [[ Old comment, sent by email on Wed, 2 Aug 2006 13:47:05 -0700 ]]\n\nClose it out! The new shuffle path is really great.\n\n\n"], "derived": {"summary": "MapOutputProtocol connections cause timeouts because of system thrashing and transferring the same file over and over again, ultimately leading to making no forward progress(medium sized job, 500GB input file, map output about as large as the input, 10 node cluster). There are several bugs behind this, but the following two changes improved matters considerably.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Disk thrashing / task timeouts during map output copy phase - MapOutputProtocol connections cause timeouts because of system thrashing and transferring the same file over and over again, ultimately leading to making no forward progress(medium sized job, 500GB input file, map output about as large as the input, 10 node cluster). There are several bugs behind this, but the following two changes improved matters considerably."}, {"q": "What updates or decisions were made in the discussion?", "a": "[[ Old comment, sent by email on Wed, 2 Aug 2006 13:47:05 -0700 ]]\n\nClose it out! The new shuffle path is really great."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-142", "title": "failed tasks should be rescheduled on different hosts after other jobs", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-04-18T05:25:02.000+0000", "updated": "2009-07-08T16:51:43.000+0000", "description": "Currently when tasks fail, they are usually rerun immediately on the same host. This causes problems in a couple of ways. \n  1.The task is more likely to fail on the same host. \n  2.If there is cleanup code (such as clearing pendingCreates) it does not always run immediately, leading to cascading failures.\n\nFor a first pass, I propose that when a task fails, we start the scan for new tasks to launch at the following task of the same type (within that job). So if maps[99] fails, when we are looking to assign new map tasks from this job, we scan like maps[100]...maps[N], maps[0]..,maps[99].\n\nA more involved change would avoid running tasks on nodes where it has failed before. This is a little tricky, because you don't want to prevent re-excution of tasks on 1 node clusters and the job tracker needs to schedule one task tracker at a time.", "comments": ["\nIdeally, it will be the best if the tasktracker can diagnose whether the failure was task specific or is a general case. \nIf it is a general case, the tasktracker should generate some alter and stop polling for tasks before the problem is corrected.\n\nHere is a case I encountered. For some reason, the tmp dir of the DFS was not writable:\n\n060417 144314 task_r_3e1f4h  Error running child\n060417 144314 task_r_3e1f4h java.io.FileNotFoundException: /export/crawlspace2/k\nryptonite/hadoop/dfs/data/tmp/client-5535743708351505322 (Read-only file system)\n060417 144314 task_r_3e1f4h     at java.io.FileOutputStream.open(Native Method)\n060417 144314 task_r_3e1f4h     at java.io.FileOutputStream.<init>(FileOutputStr\neam.java:179)\n060417 144314 task_r_3e1f4h     at java.io.FileOutputStream.<init>(FileOutputStr\neam.java:131)\n060417 144314 task_r_3e1f4h     at org.apache.hadoop.dfs.DFSClient$DFSOutputStre\nam.<init>(DFSClient.java:576)\n060417 144314 task_r_3e1f4h     at org.apache.hadoop.dfs.DFSClient.create(DFSCli\nent.java:127)\n060417 144314 task_r_3e1f4h     at org.apache.hadoop.dfs.DistributedFileSystem.c\nreateRaw(DistributedFileSystem.java:83)\n060417 144314 task_r_3e1f4h     at org.apache.hadoop.fs.FSDataOutputStream$Summe\nr.<init>(FSDataOutputStream.java:43)\n060417 144314 task_r_3e1f4h     at org.apache.hadoop.fs.FSDataOutputStream.<init\n>(FSDataOutputStream.java:132)\n060417 144314 task_r_3e1f4h     at org.apache.hadoop.fs.FileSystem.create(FileSy\nstem.java:201)\n060417 144314 task_r_3e1f4h     at org.apache.hadoop.fs.FileSystem.create(FileSy\nstem.java:168)\n060417 144314 task_r_3e1f4h     at com.yahoo.yst.crawl.aggregation.CrawledDocOut\nputFormat.getRecordWriter(CrawledDocOutputFormat.java:39)\n060417 144314 task_r_3e1f4h     at org.apache.hadoop.mapred.ReduceTask.run(Reduc\neTask.java:265)\n060417 144314 task_r_3e1f4h     at org.apache.hadoop.mapred.TaskTracker$Child.ma\nin(TaskTracker.java:709)\n\nIn this case,  all reduce tasks will fail. Thus, it does not make sense to get any new reduce tasks.\n\nIn some other case, if the map.local.dir is not writable, then both map tasks and reduce rasks will fail. \nIn this case, it should stop polling for new tasks.\n", "This patch does three things:\n   1. When a task fails, it sets the following task to be the first to be checked for assignment to a TaskTracker.\n   2. Tasks prefer not to run on TaskTrackers where they have failed before.\n   3.  Speculative tasks will not run on TaskTrackers where they have failed.", "I just committed this.  Thanks, Owen!"], "derived": {"summary": "Currently when tasks fail, they are usually rerun immediately on the same host. This causes problems in a couple of ways.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "failed tasks should be rescheduled on different hosts after other jobs - Currently when tasks fail, they are usually rerun immediately on the same host. This causes problems in a couple of ways."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-143", "title": "exception call stacks are word wrapped in webapp", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-04-18T22:22:03.000+0000", "updated": "2009-07-08T16:51:43.000+0000", "description": "The exception call stacks in the webapp word wrap, which makes them much harder to read. It is particularly unfortunate in the remote exceptions, which use a blank line to separate the local from the remote call stacks.", "comments": ["Here is a patch that adds a \"<pre>\" and \"</pre>\" around the diagnostic info from the tasks.", "This was commited by Doug as Revision 394988."], "derived": {"summary": "The exception call stacks in the webapp word wrap, which makes them much harder to read. It is particularly unfortunate in the remote exceptions, which use a blank line to separate the local from the remote call stacks.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "exception call stacks are word wrapped in webapp - The exception call stacks in the webapp word wrap, which makes them much harder to read. It is particularly unfortunate in the remote exceptions, which use a blank line to separate the local from the remote call stacks."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was commited by Doug as Revision 394988."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-144", "title": "the dfs client id isn't relatable to the map/reduce task ids", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-04-18T22:49:25.000+0000", "updated": "2009-07-08T16:41:51.000+0000", "description": "From the dfs logs you can't tell which map/reduce tasks where involved, which makes debugging harder.", "comments": ["This patch sets the task id in the task's local job conf and the dfs client will use the \"mapred.task.id\" from the Configuration, if it is defined.", "So instead of DFS client names like: DFSClient_2114239722\nFor map/reduce tasks, I get client names like: DFSClient_task_r_dgi5u\n\nThey are still random strings, but at least they are tied to the map/reduce logs.", "I just committed this.  Thanks, Owen!"], "derived": {"summary": "From the dfs logs you can't tell which map/reduce tasks where involved, which makes debugging harder.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "the dfs client id isn't relatable to the map/reduce task ids - From the dfs logs you can't tell which map/reduce tasks where involved, which makes debugging harder."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-145", "title": "io.skip.checksum.errors property clashes with LocalFileSystem#reportChecksumFailure", "status": "Resolved", "priority": "Major", "reporter": "Michael Stack", "assignee": "Owen O'Malley", "labels": [], "created": "2006-04-18T23:16:49.000+0000", "updated": "2014-07-18T04:51:14.000+0000", "description": "Below is from email to the dev list on Tue, 11 Apr 2006 14:46:09 -0700.\n\nChecksum errors seem to be a fact of life given the hardware we use.  They'll often cause my jobs to fail so I have been trying to figure how to just skip the bad records and files.  At the end is a note where Stefan pointed me at 'io.skip.checksum.errors'.   This property, when set, triggers special handling of checksum errors inside SequenceFile$Reader: If a checksum, try to skip to next record.  Only, this behavior can conflict with another checksum handler that moves aside the problematic file whenever a checksum error is found.  Below is from a recent log.\n\n060411 202203 task_r_22esh3  Moving bad file /2/hadoop/tmp/task_r_22esh3/task_m_e3chga.out to /2/bad_files/task_m_e3chga.out.1707416716\n060411 202203 task_r_22esh3  Bad checksum at 3578152. Skipping entries.\n060411 202203 task_r_22esh3  Error running child\n060411 202203 task_r_22esh3 java.nio.channels.ClosedChannelException\n060411 202203 task_r_22esh3     at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:89)\n060411 202203 task_r_22esh3     at sun.nio.ch.FileChannelImpl.position(FileChannelImpl.java:276)\n060411 202203 task_r_22esh3     at org.apache.hadoop.fs.LocalFileSystem$LocalFSFileInputStream.seek(LocalFileSystem.java:79)\n060411 202203 task_r_22esh3     at org.apache.hadoop.fs.FSDataInputStream$Checker.seek(FSDataInputStream.java:67)\n060411 202203 task_r_22esh3     at org.apache.hadoop.fs.FSDataInputStream$PositionCache.seek(FSDataInputStream.java:164)\n060411 202203 task_r_22esh3     at org.apache.hadoop.fs.FSDataInputStream$Buffer.seek(FSDataInputStream.java:193)\n060411 202203 task_r_22esh3     at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:243)\n060411 202203 task_r_22esh3     at org.apache.hadoop.io.SequenceFile$Reader.seek(SequenceFile.java:420)\n060411 202203 task_r_22esh3     at org.apache.hadoop.io.SequenceFile$Reader.sync(SequenceFile.java:431)\n060411 202203 task_r_22esh3     at org.apache.hadoop.io.SequenceFile$Reader.handleChecksumException(SequenceFile.java:412)\n060411 202203 task_r_22esh3     at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:389)\n060411 202203 task_r_22esh3     at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:209)\n060411 202203 task_r_22esh3     at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:709)\n\n(Ignore line numbers.  My code is a little different from main because I've other debugging code inside in SequenceFile.  Otherwise I'm running w/ head of hadoop).\n\nThe SequenceFile$Reader#handleChecksumException is trying to skip to next record but the file has been closed by the move-aside.\n\n\nOn the list there is some discussion on merit of moving aside file when bad checksum found.  I've trying to test what happens if we leave the file in place but haven't had a checksum error in a while.  \n\nOpening this issue so place to fill in experience as we go.\n", "comments": ["Long ago."], "derived": {"summary": "Below is from email to the dev list on Tue, 11 Apr 2006 14:46:09 -0700. Checksum errors seem to be a fact of life given the hardware we use.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "io.skip.checksum.errors property clashes with LocalFileSystem#reportChecksumFailure - Below is from email to the dev list on Tue, 11 Apr 2006 14:46:09 -0700. Checksum errors seem to be a fact of life given the hardware we use."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-146", "title": "potential conflict in block id's, leading to data corruption", "status": "Closed", "priority": "Major", "reporter": "Yoram Arnon", "assignee": "Konstantin Shvachko", "labels": [], "created": "2006-04-19T05:07:03.000+0000", "updated": "2009-07-08T16:41:52.000+0000", "description": "currently, block id's are generated randomly, and are not tested for collisions with existing id's.\nwhile ids are 64 bits, given enough time and a large enough FS, collisions are expected.\nwhen a collision occurs, a random subset of blocks with that id will be removed as extra replicas, and the contents of that portion of the containing file are one random version of the block.\nto solve this one could check for id collision when creating a new block, getting a new id in case of conflict. This approach requires the name node to keep track of all existing block id's (rather than just the ones who have reported in), and to identify old versions of a block id as in valid (in case a data node dies, a file is deleted, then a block id is reused for a new file).\nAlternatively, one could simply use sequential block id's. Here the downsides are: \n1. migration from an existing file system is hard, requiring compaction of the entire FS\n2. once you cycle through 64 bits of id's (quite a few years at full blast), you're in trouble again (or run occasional/background compaction)\n3. you must never lose the high watermark block id.\n\n\nsynchronized Block allocateBlock(UTF8 src) {\n        Block b = new Block();\n        FileUnderConstruction v = (FileUnderConstruction) pendingCreates.get(src);\n        v.add(b);\n        pendingCreateBlocks.add(b);\n        return b;\n    }\n\n\nstatic Random r = new Random();\n\n    /**\n     */\n    public Block() {\n        this.blkid = r.nextLong();\n        this.len = 0;\n    }", "comments": ["I'd vote for sequential allocation.  It will take a *really* long time to cycle through all ids.  Migration should not be expensive, since it just requires renaming block files, not copying them.  The high-watermark block id can be logged with the block->name table.\n\nHere's one way to migrate: initially the high-water-mark id is zero.  So all blocks in the name table are out-of-range, and hence need renaming.  Renaming can be handled like other blockwork: the namenode can give datanodes rename commands.  While a block is being renamed it must be kept in side tables, so that, e.g., requests to read files whose blocks are partially renamed can still be handled.\n", "I have attached a patch that fixes block-id conflicts for randomly generated block-ids. Now it checks after generating a block ID whether it already exists in the file system.\n", "I just committed this.  Thanks, Konstantin!", "Oops.  I meant, \"Thanks Milind! \"  Sorry.  I wasn't paying attention."], "derived": {"summary": "currently, block id's are generated randomly, and are not tested for collisions with existing id's. while ids are 64 bits, given enough time and a large enough FS, collisions are expected.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "potential conflict in block id's, leading to data corruption - currently, block id's are generated randomly, and are not tested for collisions with existing id's. while ids are 64 bits, given enough time and a large enough FS, collisions are expected."}, {"q": "What updates or decisions were made in the discussion?", "a": "Oops.  I meant, \"Thanks Milind! \"  Sorry.  I wasn't paying attention."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-147", "title": "MapTask removed mapout files before the reduce tasks copy them", "status": "Closed", "priority": "Major", "reporter": "Runping Qi", "assignee": null, "labels": [], "created": "2006-04-19T07:53:06.000+0000", "updated": "2009-07-08T16:51:39.000+0000", "description": "I was running a job on a cluster of 138 nodes. The job had 1050 map tasks and 128 reduce tasks. It stucked at the reduce stage.\nAll the reduce tasks were trying to copy file from a map task with the following status show on the web interface:\n\nreduce > copy > task_m_ehz5q1@node1262.foo.com:60040\n\nHowever, the log on the machine node1262 (where the map task task_m_ehz5q1 ran) showed that the map task finished even before the \nreduce tasks copied the map output files:\n\n060417 103554 Server connection on port 60050 from 72.30.117.220: starting\n060417 103554 task_m_ehz5q1  Client connection to 0.0.0.0:60050: starting\n060417 103554 task_m_ehz5q1 1.0% /user/runping/runping/proj/part-00039:0+71\n060417 103554 Task task_m_ehz5q1 is done.\n060417 103554 parsing file:/local/hadoop/conf2/hadoop-default.xml\n\n......................\n\n060417 103613 parsing file:/local/hadoop/conf2/hadoop-site.xml\n060417 103623 task_m_ehz5q1 done; removing files.\n060417 103633 parsing file:/local/hadoop/conf2/hadoop-default.xml\n060417 103633 parsing file:/local/hadoop/conf2/mapred-default.xml\n060417 103633 parsing file:/local/hadoop/conf2/hadoop-site.xml\n\n...........................................\n\n060417 190241 SEVERE Can't open map output:/local/hadoop/mapred/local/task_m_ehz5q1/part-32.out\njava.io.FileNotFoundException: /local/hadoop/mapred/local/task_m_ehz5q1/part-32.out\n        at org.apache.hadoop.fs.LocalFileSystem.openRaw(LocalFileSystem.java:115)\n        at org.apache.hadoop.fs.FSDataInputStream$Checker.<init>(FSDataInputStream.java:46)\n        at org.apache.hadoop.fs.FSDataInputStream.<init>(FSDataInputStream.java:228)\n        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:154)\n        at org.apache.hadoop.mapred.MapOutputFile.write(MapOutputFile.java:116)\n        at org.apache.hadoop.mapred.MapOutputFile.write(MapOutputFile.java:116)\n        at org.apache.hadoop.io.ObjectWritable.writeObject(ObjectWritable.java:117)\n        at org.apache.hadoop.io.ObjectWritable.write(ObjectWritable.java:64)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:231)\n060417 190241 Unknown child with bad map output: task_m_ehz5q1. Ignored.\n060417 190241 Server handler 2 on 60040 caught: java.io.FileNotFoundException: /local/hadoop/mapred/local/task_m_ehz5q1/part-32.out\njava.io.FileNotFoundException: /local/hadoop/mapred/local/task_m_ehz5q1/part-32.out\n        at org.apache.hadoop.fs.LocalFileSystem.openRaw(LocalFileSystem.java:115)\n        at org.apache.hadoop.fs.FSDataInputStream$Checker.<init>(FSDataInputStream.java:46)\n        at org.apache.hadoop.fs.FSDataInputStream.<init>(FSDataInputStream.java:228)\n        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:154)\n        at org.apache.hadoop.mapred.MapOutputFile.write(MapOutputFile.java:116)\n        at org.apache.hadoop.io.ObjectWritable.writeObject(ObjectWritable.java:117)\n        at org.apache.hadoop.io.ObjectWritable.write(ObjectWritable.java:64)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:231)\n060417 190241 parsing file:/local/hadoop/conf2/hadoop-default.xml\n060417 190241 parsing file:/local/hadoop/conf2/mapred-default.xml\n\nAnd the above exceptions repeated for many (not sure whether it is tru for all the reduce task) other reduce tasks.\n\nAnother strange thing noticed from the logs.\n\nOn another machine's log, I saw:\n\n060417 190528 parsing file:/local/hadoop/conf2/hadoop-site.xml\n060417 190528 task_r_24d8k4 copy failed: task_m_ehz5q1 from node1262.foo.com/72.30.117.220:60040\njava.io.IOException: timed out waiting for response\n        at org.apache.hadoop.ipc.Client.call(Client.java:305)\n        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:141)\n        at org.apache.hadoop.mapred.$Proxy2.getFile(Unknown Source)\n        at org.apache.hadoop.mapred.ReduceTaskRunner.prepare(ReduceTaskRunner.java:110)\n        at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:66)\n060417 190528 task_r_24d8k4 0.11523809% reduce > copy > task_m_ehz5q1@node1262.foo.com:60040\n060417 190528 task_r_24d8k4 Copying task_m_epatk8 output from node1387.foo.com.\n                               \nwhich is expected. However, before this line, \nI saw another copy activity in the log:\n\n060417 103608 parsing file:/local/hadoop/conf2/hadoop-site.xml\n060417 103608 task_r_a4yl3t Copying task_m_ehz5q1 output from node1262.foo.com.\n060417 103608 parsing file:/local/hadoop/conf2/hadoop-default.xml\n\nAnd the task task_r_a4yl3t does not belong to the concerned job, \naccording to the Web interface. That is strange.\n\nAnd I checked a few other machines where some reduce tasks ran, \nand I saw the same thing.\n\nI suspect there was a conflict in job ID. If two jobs had the same ID, \nwhen one closes, it may also mark the other as \"closed\" too, thus trggering map tasks\nto clean up prematurely.\n\nA simple way to avoid potential jobid conflict is to use sequential numbers.\n\n\n\n", "comments": ["The fix to HADOOP-150 addresses conflicts in job and task ids."], "derived": {"summary": "I was running a job on a cluster of 138 nodes. The job had 1050 map tasks and 128 reduce tasks.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "MapTask removed mapout files before the reduce tasks copy them - I was running a job on a cluster of 138 nodes. The job had 1050 map tasks and 128 reduce tasks."}, {"q": "What updates or decisions were made in the discussion?", "a": "The fix to HADOOP-150 addresses conflicts in job and task ids."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-148", "title": "add a failure count to task trackers", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-04-19T12:04:58.000+0000", "updated": "2009-07-08T16:51:43.000+0000", "description": "Adds a count of failures that have occurred on each TaskTracker in the TaskTrackerStatus. The webapp displays the list of failures for each TaskTracker. In addition, the TaskTracker that has the most failures is listed.", "comments": ["This patch adds the failure counts to the TaskTrackerStatus and the webapp.", "I have applied this patch.  Thanks, Owen."], "derived": {"summary": "Adds a count of failures that have occurred on each TaskTracker in the TaskTrackerStatus. The webapp displays the list of failures for each TaskTracker.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "add a failure count to task trackers - Adds a count of failures that have occurred on each TaskTracker in the TaskTrackerStatus. The webapp displays the list of failures for each TaskTracker."}, {"q": "What updates or decisions were made in the discussion?", "a": "I have applied this patch.  Thanks, Owen."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-149", "title": "TaskTracker#unJar trashes file modes", "status": "Closed", "priority": "Minor", "reporter": "Michael Stack", "assignee": null, "labels": [], "created": "2006-04-20T00:59:20.000+0000", "updated": "2006-08-03T17:46:36.000+0000", "description": "Last Changed Rev: 395069\n\nThe unJar'ing of the job 'jar', trashes any file modes I've lovingly set at zip time.  This is a bit of a pain when my job wants to run external scripts and I want to bundle the scripts up in the jar itself for distribution out to slaves.\n\nI ain't sure how to address the issue though. Nought about unix file modes in JarEntry nor ZipEntry.  I tried the ant 1.6.5 task unjar and unzip tasks thinking they'd respect file modes but they do same as TaskTracker#unJar (Commons zip makes mention of unix file modes but I haven't tried it).  Perhaps support for jobs as tar(.gz) bundles?  (But again, couldn't use ant to untar. It does same as unzip/unjar trashing file permissions).", "comments": ["Yes, this is a pain.  I think the root of the problem is that file modes are not cross-platform, so Java doesn't have an API to access them.\n\nAs a work-around, can you, instead of directly invoking your scripts, invoke '/bin/bash myScript.sh'?  That way you don't require things to be executable.", "That'll do.  Thanks.\n\nI'd suggest that you can close this issue.  The suggested workaround -- a /bin/bash or /usr/bin/env prefix -- will likely work for most."], "derived": {"summary": "Last Changed Rev: 395069\n\nThe unJar'ing of the job 'jar', trashes any file modes I've lovingly set at zip time. This is a bit of a pain when my job wants to run external scripts and I want to bundle the scripts up in the jar itself for distribution out to slaves.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "TaskTracker#unJar trashes file modes - Last Changed Rev: 395069\n\nThe unJar'ing of the job 'jar', trashes any file modes I've lovingly set at zip time. This is a bit of a pain when my job wants to run external scripts and I want to bundle the scripts up in the jar itself for distribution out to slaves."}, {"q": "What updates or decisions were made in the discussion?", "a": "That'll do.  Thanks.\n\nI'd suggest that you can close this issue.  The suggested workaround -- a /bin/bash or /usr/bin/env prefix -- will likely work for most."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-150", "title": "tip and task names should reflect the job name", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-04-20T01:34:39.000+0000", "updated": "2009-07-08T16:51:43.000+0000", "description": "The tip and task names should be related to the job id. I'd propose:\n\njob name: job_<random 32bit/base36>\ntip: tip_<job id>_[mr]_<fragment #>\ntask: task_<tip id>_<attempt #>\n\nso examples would be:\njob_abc123\ntip_abc123_m_00034\ntask_abc123_m_00034_1\n \n", "comments": ["+1\n\nDoes the job name need to be random, even?  Could we allocate these sequentially too?\n\nAlso, we don't even need to name the taskTracker's with a random number.  We can use hostname:port, where the port is one used by one of the tasktracker's daemons (map output or umbilical).  That would make these more meaningful too.\n\nAnother random note: the umbilical and map output RPC servers could actually be run on the same port.  The way RPC is currently implemented, all public methods on the object served are available to be called remotely.  This is perhaps unsafe.  Perhaps RPC should be changed so that a server names the interface whose methods calls should be restricted to...", "Here is a patch that changes the names to match the given pattern.", "Ok, I forgot the \"task_\" in the prefix and I changed the job ids from random to sequential within a job tracker.", "A minor correction...", "I just committed this.  Thanks, Owen!"], "derived": {"summary": "The tip and task names should be related to the job id. I'd propose:\n\njob name: job_<random 32bit/base36>\ntip: tip_<job id>_[mr]_<fragment #>\ntask: task_<tip id>_<attempt #>\n\nso examples would be:\njob_abc123\ntip_abc123_m_00034\ntask_abc123_m_00034_1.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "tip and task names should reflect the job name - The tip and task names should be related to the job id. I'd propose:\n\njob name: job_<random 32bit/base36>\ntip: tip_<job id>_[mr]_<fragment #>\ntask: task_<tip id>_<attempt #>\n\nso examples would be:\njob_abc123\ntip_abc123_m_00034\ntask_abc123_m_00034_1."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-151", "title": "RPC code has socket leak?", "status": "Closed", "priority": "Major", "reporter": "Peter Sutter", "assignee": "Doug Cutting", "labels": [], "created": "2006-04-20T05:18:25.000+0000", "updated": "2006-08-03T17:46:36.000+0000", "description": "In RPC.java, the field named CLIENT should be neither static, nor a field of RPC. It should be (a) a private nonstatic field of InvocationHandler(),and (just further down), (b) a local variable in the RPC.call() method below.  The comment above the declaration was a bit of giveaway: \n\n   //TODO mb@media-style.com: static client or non-static client?\n  private static Client CLIENT;\t\n\n  private static class Invoker implements InvocationHandler {\n    private InetSocketAddress address;\n\n    public Invoker(InetSocketAddress address, Configuration conf) {\n      this.address = address;\n      CLIENT = (Client) conf.getObject(Client.class.getName());\n      if(CLIENT == null) {\n          CLIENT = new Client(ObjectWritable.class, conf);\n          conf.setObject(Client.class.getName(), CLIENT);\n      }\n    }\n\n    public Object invoke(Object proxy, Method method, Object[] args)\n      throws Throwable {\n      ObjectWritable value = (ObjectWritable)\n        CLIENT.call(new Invocation(method, args), address);\n      return value.get();\n    }\n  }\n\n  /** Construct a client-side proxy object that implements the named protocol,\n   * talking to a server at the named address. */\n  public static Object getProxy(Class protocol, InetSocketAddress addr, Configuration conf) {\n    return Proxy.newProxyInstance(protocol.getClassLoader(),\n                                  new Class[] { protocol },\n                                  new Invoker(addr, conf));\n  }\n\n  /** Expert: Make multiple, parallel calls to a set of servers. */\n  public static Object[] call(Method method, Object[][] params,\n                              InetSocketAddress[] addrs, Configuration conf)\n    throws IOException {\n\n    Invocation[] invocations = new Invocation[params.length];\n    for (int i = 0; i < params.length; i++)\n      invocations[i] = new Invocation(method, params[i]);\n    CLIENT = (Client) conf.getObject(Client.class.getName());\n    if(CLIENT == null) {\n        CLIENT = new Client(ObjectWritable.class, conf);\n        conf.setObject(Client.class.getName(), CLIENT);\n    }\n    Writable[] wrappedValues = CLIENT.call(invocations, addrs);\n    \n    if (method.getReturnType() == Void.TYPE) {\n      return null;\n    }\n\n    Object[] values =\n      (Object[])Array.newInstance(method.getReturnType(),wrappedValues.length);\n    for (int i = 0; i < values.length; i++)\n      if (wrappedValues[i] != null)\n        values[i] = ((ObjectWritable)wrappedValues[i]).get();\n    \n    return values;\n  }. \n", "comments": ["What's the problem with a static CLIENT?  What problems does this cause?  The client has the connection pool, so one potential problem is that, a large request or response will delay other requests or responses to/from the same host.  Is that the issue you're seeing?  If so, can you provide more details about the circumstances where this occurs?\n", "\nIf thre is only one Client object, then you're right, its OK that its static. \n\nThe fact that the object is also stored in the conf - combined with the comment - made it look like a coding error (since its not accessed by anything but those parts of the code).\n\n", "You're right, that code got uglified by the configuration stuff.  There should only need to be a single Client for RPC.  Each Client keeps a cache of TCP connections to other host:port pairs expecting the same Writable class for requests and responses.  All RPCs use Invocation as the request class and ObjectWritable as the response class, so they can generally share connections.\n\nThe problem is that there's no longer a global configuration.  So a global client could be lazily constructed the first time a call is made with the invoker's configuration.  Or we could create a new client for each configuration.  But, you're right, we shouldn't do both, as the current code does.  Changing the global CLIENT shouldn't break anything, but it's also silly and wasteful.  The only thing the client reads from the configuration is the RPC timeout.  It might almost be better to remove the configuration from Client altogether and always explicitly pass the timeout as a parameter to call().\n\nI wonder if this could even be leading to socket leaks?  Hmm.  A client doesn't close it's connections until it's either explicitly closed (which these are not) or until the remote server dies.  So we should probably fix this somehow.", "I just fixed this.  I restored the original behaviour, where a single client is used for all calls, regardless of the configuration.  This results in the use of a single connection pool for all RPC calls, and hence closes a potential socket leak.  Long-term we might consider other approaches to pooling connections.  Today it seemed more important to close the leak than to, e.g., permit different ipc timeouts for different jobs."], "derived": {"summary": "In RPC. java, the field named CLIENT should be neither static, nor a field of RPC.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "RPC code has socket leak? - In RPC. java, the field named CLIENT should be neither static, nor a field of RPC."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just fixed this.  I restored the original behaviour, where a single client is used for all calls, regardless of the configuration.  This results in the use of a single connection pool for all RPC calls, and hence closes a potential socket leak.  Long-term we might consider other approaches to pooling connections.  Today it seemed more important to close the leak than to, e.g., permit different ipc timeouts for different jobs."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-152", "title": "Speculative tasks not being scheduled", "status": "Closed", "priority": "Minor", "reporter": "Bryan Pendleton", "assignee": null, "labels": [], "created": "2006-04-21T00:32:31.000+0000", "updated": "2009-07-08T16:51:43.000+0000", "description": "The criteria for starting up a speculative task includes a check that the \"average progress\"-\"progress\" > the speculative gap, currently 0.2.\n\nI don't know if this is the right metric, but it doesn't seem to be correctly calculated. I've regularly seen the \"average progress\" with values of less than 0.01, while the \"progress\" value is showing in the range .90-1.0, and yet, still no speculative tasks are started up. This has caused at least one long-running task to run about 10% longer while overloaded hosts catch up.", "comments": ["*bump*\n\nIs anyone else seeing this problem? My cluster is pretty unevenly loaded, and, without speculative execution, I'm waiting for very long times for tasks to timeout on short jobs. Speculative execution is enabled, so there's no reason that, say, two maps out of ~1900 should be holding up execution. I suspect the \"progress\" accounting being done in the Job isn't being done correctly.\n\nBut, even then, perhaps we need more metrics - with the current metrics, if one of the job units happens to be running really slowly on a given node, but might run faster on other nodes, it might never get executed on another node because the progress on the slow node might be reported as close enough to done so as to not trip the speculative execution.", "This was fixed when we fixed the thresholds for launching speculative tasks as part of HADOOP-76."], "derived": {"summary": "The criteria for starting up a speculative task includes a check that the \"average progress\"-\"progress\" > the speculative gap, currently 0. 2.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Speculative tasks not being scheduled - The criteria for starting up a speculative task includes a check that the \"average progress\"-\"progress\" > the speculative gap, currently 0. 2."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was fixed when we fixed the thresholds for launching speculative tasks as part of HADOOP-76."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-153", "title": "skip records that fail Task", "status": "Closed", "priority": "Major", "reporter": "Doug Cutting", "assignee": "Sharad Agarwal", "labels": [], "created": "2006-04-21T00:40:52.000+0000", "updated": "2013-05-02T02:29:16.000+0000", "description": "MapReduce should skip records that throw exceptions.\n\nIf the exception is thrown under RecordReader.next() then RecordReader implementations should automatically skip to the start of a subsequent record.\n\nExceptions in map and reduce implementations can simply be logged, unless they happen under RecordWriter.write().  Cancelling partial output could be hard.  So such output errors will still result in task failure.\n\nThis behaviour should be optional, but enabled by default.  A count of errors per task and job should be maintained and displayed in the web ui.  Perhaps if some percentage of records (>50%?) result in exceptions then the task should fail.  This would stop jobs early that are misconfigured or have buggy code.\n\nThoughts?", "comments": ["+1\n\nThis would be a cool feature to have. Perhaps the exceptions should also be made visible at the jobtracker. An extension to manage exceptions in RecordWriter.write() could record the offending key(s) and skip them the next time the task is run.", "+1\n\nThis would be a generalization of the checksum handler that tries to skip records when 'io.skip.checksum.errors' is set.  In a recent note on the list, I speculate that there are a class of IOExceptions that should be treatable in the same manner -- skipped if a flag is set (Search subject line ''Corrupt GZIP trailer' and other irrecoverable IOEs.').", "sounds good.  The acceptable % should probably be configurable.  I'd be inclined to use something more like 1%.  You could turn the feature off by configuring 0%, which should arguably be the default.\n", "\n+1\n\nExceptions in the map and reduce functions that are implemented by the user should be handled by the user within the functions.\nIn the current implementation of sequencial file record reader, it is hard to skip to the next record if exception happens during record reading.\n\n", "Am starting to look at this issue. I am tending to handle this by only keeping track of how RecordReader.next behaves. This will take care of all cases - Java, Streaming, and Pipes in a generic manner. I am not very much in favor of providing hooks in the applications. Is that a requirement? Any other requirement? Thoughts welcome.", "> I am tending to handle this by only keeping track of how RecordReader.next behaves.\n\nI'm not sure what you mean by that.\n\nThere are two kinds of places where user code might throw per-record exceptions:\n\n# under RecordReader#next() or RecordWriter#write().  Depending on the RecordReader/RecordWriter implementation and the exception, it may or may not be possible to call next() or write() again.  Either are likely to leave streams mid-object.  A reader of binary input might get badly out of sync, and a writer of binary output might generate badly corrupt data.  To address this correctly, we either need to change to contracts of next() and write(), or we need to add new methods to re-sync these to object boundaries.\n\n# under Mapper#map() or Reducer#reduce().  Exceptions here can be ignored without causing anything worse than data loss.  We can safely proceed without worrying about corruption.\n", "Doug, I was just trying to summarize the high level requirement. Thanks for the detailed inputs.", "A major use case is that the user implementation of the map or reduce(or third party libraries) can throw StackOverflow or OOM. In the latter case there is no guarantee that the program will be stable even if it catches the OOM. So a method will be good if the framework keeps track of the records, terminates the execution of the task  and skips them in the rescheduled run. \n", "Here is a proposal that tries to address the scenarios discussed in this jira:\n0) Define the concept of a _failed record number_ that is set by Tasks and propagated to the JobTracker on task failures. This becomes part of the TIP object at the JobTracker.\n1) Define an API in the RecordReader to do with getting the record boundary. On getting an exception in RecordReader.next, the task starts from the beginning of the last successfully read record till the boundary and reads the next record from that point (ignoring the boundary bytes). Applies to maps.\n2) Define an API in RecordWriter to do with writing record boundary along with every write(k,v). The record boundary can default to the sync bytes. Tasks fail when they get an exception while writing a bad record. With (0), in the subsequent retries, the records can be skipped. This applies to outputs of maps and reduces.\n3) Define an API in RecordReader to do with whether we want to have recovery while reading records on not (useful for e.g. if the RecordReader has side effects in the next() method that would affect the reading of the subsequent record if there was an exception for the current record). \n\nIn cases of applications throwing exceptions in the map/reduce methods, the exception is caught by the Task method, which invoked the map/reduce method. The task attempt is killed after noting the record number of the failed invocation. With the above point (0), these records are not fed to the m/r methods in the subsequent retries.\n\nThe recovery/skip above is done on a best effort basis. That is, the worst case is that tasks fail!\n\nThe above strategies should at least allow configuring the max % of records that will keep the recovery/skip cycle going before the job is declared as having failed. Also, the \"skip records on re-execution\" should probably be configurable on a per job basis by the user (since the exceptions could have been caused due to reasons other than incorrect input).\n\nThoughts?", "I should have thought about Pipes and Streaming apps. In order to reliably reexecute Pipes Map tasks, we probably need to change the corresponding protocol to tell the sequence number of the record for which it is emitting the (keys,values). It's not clear how to handle the Streaming tasks since the protocol for Streaming can't be tweaked in a generic manner.", "This sounds like roughly the right direction.\n\nMaybe we should extend diagnostic info to contain a record number (or offset?) and use that to record the error that was thrown? For errors in RecordReader.next, you are right that it probably makes sense to skip ahead to the next sync marker/newline. Since the meta data is equally at risk as the record data. So having a \"gotoSync\" in RecordReader would help.\n\nMappers and Reducers should be able to tolerate killer records, just like the reader.\n\nWe should think about what is required for the RecordWriter to recover from a failed write... Of course I suspect there aren't any good answers until HDFS has truncate and append.", "bq. Mappers and Reducers should be able to tolerate killer records, just like the reader.\nAs I mentioned earlier, for the reader, I have an API in RecordReader for querying whether the RecordReader is side-effect free. If the RecordReader is not side-effect free (for e.g., if the next method implementation has issues to do with proceeding further if the current record read throws an exception), then the task is declared as having failed (and the offending record is skipped in the task retry).\nIf we were to support the same strategy for Mapper/Reducer, we need to consider a similar problem. Also, since Mapper/Reducer can throw exceptions due to problems not necessarily to do with bad input, we probably need from the user, the info whether to continue with map/reduce method invocations in the event of exceptions. In addition, the user can also mention the fatal exceptions which should result in the task failure (e.g. OOM).\nMakes sense?", "Honestly, i think bringing extra complexity (especially RecordReader API change, which will break lots of legacy code) is not justified to solve this issue. To be more explicit, the framework does not need to \"continue from where it was\" once a task throws an exception. I think we can just do step 0 (as defined above) and re-execute the task from the beginning, but this time skipping the problematic record. It is expected that resuming the execution from where we left and completely re-executing the task will not make a big difference, since the number of tasks is assumed to be high enough. \n\nI propose \n1.  Define the concept of a failed record number that is set by Tasks and propagated to the JobTracker on task failures. This becomes part of the TIP object at the JobTracker.\n2. Define an API in JobConf and a configuration item in hadoop-site to [dis]allow skipping records. (can be merged with 3 below)\n3. Define an API in JobConf and a configuration item in hadoop-site to set max number(or percentage) of records that can be skipped \n4. Do not change RecordReader/Writer interfaces. \n5. The recovery/skip above is done on a best effort basis. That is, the worst case is that tasks fail!\n \n\nSo this functionality will be completely transparent to the application code, except for setting the configuration. If the user code is not side-effect free, than the user has to deal with that, because already the tasks can be executed more than once for various reasons(for example speculative execution).\n\nAny thoughts ? ", "The *order* of the number of exceptions that the framework intends to handle affects the design a great deal. What is the scope of the problem we intend to handle here? I see there being two cases:\n# the number of exceptions is small compared to the number of tasks in a job. In this scenario Enis' strategy makes a lot of sense. In general we assume that tasks are fine grained enough that re-executing a handful of them is not a significant burden in terms of job runtime and throughput.\n# the number of exceptions is _O(num tasks)_. In this scenario, re-execution could cause job runtime to double (or worse) since every task could in principle be executed two or more times. If we set out to handle this case then we'll need to keep enough state to enable each task to skip the offending record(s) *and* start where they left off i.e. not re-process any previously handled records\n\nPerhaps we should attempt to resolve case 1) with task re-execution for now since it represents a useful incremental step towards a more sophisticated solution. It may prove to be sufficient. One could in principle argue that if the number of exceptions is _O(num tasks)_ the problem is better handled at the application level.", "The tricky bit will be identifying the failed record number, no?  The naive approach would be to have the child report after each record has been processed, so that the parent can then know, when it crashes, which record it was on.  But that would probably be too expensive.\n\nSo rather we might have the child report in its TaskStatus the range of record numbers it intends to process before it next reports.  Then, on failure, the parent knows that skipping that range will skip the offending record.  The child can adaptively choose a range that keeps the status update time within reason.  It would start by reporting range 0-1, then use the time required to process that entry to determine how big of a range to send the next time and still provide timely updates.  If a particular record takes a long time, then it can update status before the range is exhausted, providing a new range.\n\nWould this be acceptable, to skip a few more than just the offending record?\n", "bq. The tricky bit will be identifying the failed record number, no? The naive approach would be to have the child report after each record has been processed, so that the parent can then know, when it crashes, which record it was on. But that would probably be too expensive.\n\nIn the original mapreduce paper, the record number is kept in a global variable which is then passed to the master through an UDP package. The master decides that the record is malformed if it sees more than one report for the same record number. \n\nI think we may be able catch the exception in the Child process and make an IPC to the TT(which in turn reports this to JT). There may be situations in which the IPC will fail, for those tasks we can adopt the above strategy of reporting the record number in the subsequent reexecution of the task to find out exactly which record the computation fails. \n", "Enis, agree that for the Java tasks case we could get the offending record immediately in the Child process. The problem here is that with things like Pipes apps (where the Java task spawns another child process from within), the record number at which the exception happened is tricky to get since the exception was really encountered in the Pipes process (this doesn't include the exception that we might encounter while reading the input since that happens in the Java parent task and we can catch those immediately).", "hey folks - we are having a discussion on a similar jira (covering a smaller subset of issues) - 3144. we are actually hitting this problem (corrupted records causing OOM) and have a simple workaround specific to our problem.\n\nbut i am a little intrigued by the proposal here. for the recordreader issues - why not, simply, let the record reader skip the bad record(s). as the discussions here mentions - there have to be additional api's in the record reader to be able to skip problematic records. If the framework trusts record readers to be able to skip bad records - why bother re-executing? why not allow them to detect and skip bad records on the very first try. if TT/JT want to keep track and impose a limit on the bad records skipped - they could ask the record reader to report the same through an api.\n\nthe exceptions from map/reduce functions are different  - if they make the entire task unstable due to OOM issues then a re-execution makes sense. but if we separate the two issues - we may have a more lightweight  way of tolerating pure data corruption/validity issues (as we are trying to in 3144).", "Yes, if you assume that record readers have the necessary logic to be able to skip bad records on the very first try, it would work. However, in this issue, we were trying to address the problem more from the framework perspective. That is, even though recordreaders might not have the logic to determine whether a particular record is corrupt, the framework can do it with some help from the reader.", "> why not, simply, let the record reader skip the bad record(s) [ ... ] ?\n\nIf the record reader can identify bad records and skip them, then the framework need not get involved: the RecordReader iteself can catch exceptions that it knows might be thrown by bad records and then try to read the next.", "> That is, even though recordreaders might not have the logic to determine whether a particular record is corrupt, the framework can do it with \nsome help from the reader\n\n> If the record reader can identify bad records and skip them, then the framework need not get involved\n\nin the beginning of this jira -  it was mentioned that problems from recordreader.next() were also covered in this jira. I take it from these comments - that this is not the case any more. it seems to me that if a recordreader can skip a record reliably (which is the support required from record readers from this jira) - then it will also be able to avoid throwing exceptions (since quite obviously, it can catch any exceptions and invoke the logic to skip to the next record within the next() body).\n\njust wanted to make sure since this jira was mentioned as something that might preempt 3144 (putting basic corruption detection code in the linerecordreader)  - but that doesn't seem like the case ..", "bq. in the beginning of this jira - it was mentioned that problems from recordreader.next() were also covered in this jira. I take it from these comments - that this is not the case any more. it seems to me that if a recordreader can skip a record reliably (which is the support required from record readers from this jira) - then it will also be able to avoid throwing exceptions (since quite obviously, it can catch any exceptions and invoke the logic to skip to the next record within the next() body).\n\nI didn't mean that (not sure about Doug). Today, the way map tasks work is that the recordreader's next method is invoked for each record. So if the implementation of next can handle corrupt records, then the framework doesn't need to get involved. However, if the implementation of next is not capable of handling bad records, then it is most likely going to throw an exception, and, the proposal in this jira for handling bad input starts with that assumption. The framework catches that exception and the recordreader's new interface (look at points 1, 2, and, 3 in http://issues.apache.org/jira/browse/HADOOP-153?focusedCommentId=12574404#action_12574404) comes handy to recover from that. For cases like OOM where continuing the current task execution is not safe, the framework notifies the JobTracker about these bad records, and upon the next retry these records are skipped (point 0 in the previously mentioned url) \n\nAt this point of time, since we don't have a patch for this jira, I don't see this jira preempting HADOOP-3144. Although it'd be nice if we have a patch for this jira to handle the general case.", "i am missing something .. whatever the framework can do using the recordreader's new interfaces - the recordreader can do itself. If the recordreader can define record boundaries and skip to the next record - then it can catch any and all exceptions that it might throw and ignore the bad record and move to the next one without involving the framework.\n\nsorry if i am being totally obtuse here - just not getting it ..", "coversely, if the recordreader can avoid throwing exceptions - then u don't really need new api's from the recordreader. u can just void calling map()/reduce() on specific records that had a problem.\n\none small implementation request - one of the things i have noticed (based on some profiling) is that a lot of processing overhead (in the map side)  in hadoop comes from function calls that are invoked for every processed record. it would be nice if this jira does not lead to more of the same. considering that this is not a normal case - it might make sense to have different code paths for the first and subsequent retries (so that in the normal case - we don't incur overhead looking for records to be skipped). ", "In our user case, when re-execute, we do not want to skip those killer records. Instead, in our user-supplied mapper we want to be able to identify them such that we can take a different code path to specialize them. So, we would like to see during re-executing, the records are marked by a flag indicating they are killer records in the previous runs. Or any alternatives that could give us the same information would help.", "Yiping, the problem is that if the recordreader cannot read (deserialize) we can't do much. We could write the offending records out to a well known location (some place in the dfs for example) that you can later inspect. \n\nJoydeep, I buy your argument that all these can be done in the recordreader implementation. So, maybe we should re-scope this jira to only consider failed invocations of map/reduce methods.", "Devaraj,\n\nAfter some discussion, we think that output the killer records somewhere and let the good records pass is an acceptable solution. It would be better if they are collected onto DFS and put into a flat directory such that we can run a job against it easily. And we also want to know what is the roadmap for this ticket?", "I suggest a transactional emit() interface to be supplement to this proposal.\n\nWhen an exception happens, there might already be some partial output be emitted from the mapper. In some use cases, such partial output should be considered garbage and harmful. So I suggest a transactional emit mode, such that the Collector should buffer the output records from mapper and until the mappers calls a commit() operation, they are actually submitted. At the beginning of the map() function, the mapper should call a start() operation which discards any records remaining in the Collector. ", "There is a problem with respect to pipes, in that multiple input records can be in the subject process's stomach when the crash occurs so there's no principled way to say how many of the records preceding the one that was read just before the exception should be regarded as having been suspect.  I get that the plan includes that being a configuration variable, but perhaps there could be a way for the pipes interface to say that a particular record is beyond reproach?\n\n-dk\n", "So here are some thoughts, after some discussion with others, on how to handle app level faults. Comments welcome.\n\nJava Maps\nIn this case, we can immediately know which record couldn't be processed and depending on the type of exception that the method threw we can decide to continue or not (the user can tell us which exceptions are fatal; we could also have a couple of defaults like OOM). If we decide to not continue, the task can be reexecuted in the *same tasktracker slot* and this time that record is skipped. In order to know which record should be skipped in the reexecution, the task as part of the progress/ping RPC tells the TaskTracker the record number of the last successfully processed record and the set of bad record numbers is passed to the task upon reexecution and the task simply skips those for processing. \n\nStreaming \nIn this case, the Java parent notifies the TaskTracker what the last successfully processed record is. The \"last successfully processed\" record in this case refers to the record that was sent to the streaming child process just before the crash was detected. The same TaskTracker then reexecutes the task and this time, the Java task skips that record assuming that that was the one on which the process crashed. If the process crashes even now, it gets reexecuted and this time the Java parent skips the last 2 records. This could go on with every reexecution skipping the last 2*exec-count number of records (where exec-count represents the number of reexecutions). This will give us a range within which the faulty record exists. Upon the first successful reexecution, the TaskTracker passes the range on to the JobTracker and the user can then debug his input and/or the program that processes the input. An alternative strategy is to do a binary-search for the offending record. \n\nPipes \nThe exact same thing as Streaming applies here too. The one point to note here is this that if we enable the user to tell us whenever it can successfully process a record (similar to the status/progress calls to the Java parent) it would substantially help in the reexecution w.r.t skipping records.", "\nRegarding skipping records wrt streaming and pipe: \nWhen a task is re-executed, it can skip N records starting at the detected fail position and processes the rest.\nAt the end, the tasl can process thos N skipped records on a \"best-effort\" basis.\n\n\n", "Based on above comments, I am  summarizing what IMO could be an *overall* approach:\n\nThere could be following error scenarios:\nA) The framework can catch the exception thrown by map/reduce function (only applicable for java Tasks). The framework can decide to keep moving after skipping the record, OR if the exception seems to be FATAL like OutOfMemory making the task process unstable, the framework can decide to forgo that Task execution. Re-execution should skip that record.\nB) The task process crashes due to bad record.\n\nFor supporting the above error scenarios, here is what could be done:\n1. Each task would have 2 lists - SKIPPED_RECORDS and SKIPPED_RANGES. Perhaps these could be maintained in DFS.\n\n2 Have Task periodically send to the TaskTracker, the record range which it would going to process next. Range -> R(i, j). If the Task crashes, then the last received range is written to SKIPPED_RANGES.\n\n3. Have Task periodically send the skipped records. This is the list for which it has caught exception and skipped since the last send. The TaskTracker will write these to the SKIPPED_RECORDS list.\n\n4. Whenever a Task starts, it will look for SKIPPED_RECORDS and SKIPPED_RANGES from any previous run and will skip those while executing. At the end Task will try to run SKIPPED_RANGES on best-effort basis.\n\n5. Have some job level thresholds like TOLERABLE_SKIPPED_PERCENT. When all the remaining task are trying to execute SKIPPED_RANGES, check for this threshold and if cumulative SKIPPED_RANGES for all remaining tasks are less than this threshold, then finish the job gracefully.\n\n6. Executing SKIPPED_RANGES: execute by dividing a range into half. If a particular range succeeds then try from another range. In first pass, all the ranges are trimmed to half. In second pass, all ranges are trimmed to 1/4, and so on. This will continue till TOLERABLE_SKIPPED_PERCENT is not met.\n\n7. Pipes: For identifying record range, the protocol has to be modified to figure out the processed records.\n\n8. Streaming: The streaming process can write the processed record no to the stderr as a framework counter. See HADOOP-1328.\nFor streaming process which does not support this feature, we can fall back to the mechanism in which the record range sent always start from the beginning (as we are not sure which ones have been processed yet). Range -> R(0, j). This range is then tried in the end on best effort basis, as described in 6.\nSome optimizations could be done to this approach like instead of starting from begin, have it start based on some job configured no N. For eg. Range -> R(i-N, j). N is the expected no of records in the streaming process' stomach before they are processed. Users can define N for their jobs  based on the buffers used in their process. The framework then tries to tune the value of N based on the crashes it encounters in further executions. The algorithm for this can become little complex; and there may not be that much payoff. So I think initially lets have it always skip from start, and optimize this behavior later.\n\nThoughts?", "I am attaching the patch I have developed several weeks ago for this issue. It is just an illustrative example, but I guess the final patch might rely heavily on this. \nIt captures several of the above items : \n1. Tasks have an associated badrecords list, which are kept in the JT's TaskInProgress objects. I do not think we should skip \"ranges\" of possibly valid records in favor of skipping bad ones. \n2. bad records are pushed / pulled via Reporter / UmblicalProtocol / TaskStatus\n3. configurable max number of skippable records per map and reduce. Percentages do not apply here, since we do not know the absolute number of records to compute the percentage. \n4. No support for pipes / streaming. \n  \nFor the pipes case, I think we should first run the tasks normally, until an error occurs. Upon the subsequent run of the same task, then the pipes  application should run in \"debug\" mode. In debug mode, the application should send back the key(or recordID) before it is passed to the mapper/reducer. \n\nFor streaming, again the application should run in debug mode on second trial. In this mode, the framework should send the key/value pairs one-by-one, meaning that it will not send another pair over the pipe, before the previous are consumed by the application. But as far as I know, there is no mechanism to detect this. So we might opt for delaying this to another issue. \n", "Instead of keeping bad records ids in JobTracker's TIP object, I would prefer keeping it in FileSystem along with other job files. This would allows us to be more scalable as no of bad records could potentially be high.", "bq. Instead of keeping bad records ids in JobTracker's TIP object, I would prefer keeping it in FileSystem along with other job files. This would allows us to be more scalable as no of bad records could potentially be high.\nI think the number of bad records *supposed to be* low. If the job fails in lots of records, then there should be some problem with the implementation itself. \nI guess the number of bad records per job should be on the order of hundreds max. While working with nutch, there were just several records which caused StackOverflow by the third-party html parsers ,out of millions of urls.  \nCan you give us an example percentage of bad records that you see in the jobs at Y!  ", "I assume there would be use cases in which a higher bad records percentage could be acceptable say jobs related to log mining, crawling, indexing, analysis for research purposes etc.\nThat said, even 1 % of acceptable bad records could turn out to be a big no given that hadoop jobs are generally suitable for huge data. Piggybacking on jobtracker's memory for this may not be a good idea. Transferring the whole list over rpc would not be ideal.\nAlso, the need to persist may arise due to HADOOP-3245.", "Had an offline discussion with Eric and Devaraj, and we came up with following:\n- Let this issue handle the case of crashes and hangups. For the case of catching the exception for Java tasks, filed another Jira -> HADOOP-3700\n- Design gets impacted based on the assumption of how frequent the failures would be. At this point of time, design for INFREQUENT failures. This would simplify the design. Also, bad records can be maintained by the Jobtracker (as pointed out by Enis), as no of bad records are expected to be quite low.\n- Failing Fast the bad jobs is very crucial to avoid wasting Grid resources. Thresholds should be define in such a way that we identify bad jobs early enough, say maximum of 10% of the maps can fail. Also, we need to make sure that we execute failed task VERY FAST.\n- Apart from bad data, Task crashes could be due to bad user code (like Out of memory) or bad nodes. To isolate these cases, on failure, reexecute on another node as now.  If it fails AGAIN, then reexecute a third time, this time in the special mode where we report every record completion to the Task tracker.\n- For the case of Streaming, streaming would have to write the processed record count to the stderr as a framework counter, to take advantage of this feature.\n\n\n\n\n", "Sharad,\n\nI would suggest bad records to be written onto DFS. The reason is not for expecting high percentage of bad records, but for debugging/analysis purpose. As user would expect to collect those bad boys and analyze them.", "Here is the initial patch, intended for early feedback. It does the core work of maintaining the failed ranges and skipping those on further executions. Few test cases have also been included.\nAlthough the basic functionality is working, the patch is still *incomplete*. Things remaining are:\n- Capture offsets along with record sequence no. Once we have all the offsets for the skipped records, actual records data can be written anytime later on.\n- Write the skipped records information (only sequence nos and offsets) to job history.\n- Support for Pipes and Streaming.\n- Introduce thresholds to limit the no of failing tasks execution (failing fast for really bad user code)\n- Execute the skipped ranges on best effort basis. Perhaps this can be done as a separate Jira later on, if we find value in it after seeing real use cases.", "Attaching the patch. It adds the support for streaming and pipes. Also it includes a test case for streaming.\nThe work is in progress. Other items as mentioned in above comment are remaining.", "The basic skip functionality including streaming/pipes is working in the last submitted patch. \nFor the remaining items, had an offline discussion with Devaraj:\n1.  Writing the skipped records: Saving the offsets seems to be non-trivial and it will make sense only for FileSplits and map tasks. So we are thinking of the strategy to write the skipped records during Task execution itself.\nTask would write the skipped records locally to the disk while it executes. If the task attempt is last, it will flush the skipped records to the DFS, after it pases thru all the seen bad ranges.  One drawback is that if there is a new bad range in the last attempt and task fails, the records for the range in which last attempt has failed, are not written. However, user can increase the no of attempts and can get to this bad range.\n2.  When this special (skipping) mode kicks off (after the 2nd attempt by default), disable the task speculation, as in this special mode it is already expected to run slower than normal.\n3.  Define a user configurable no MAX_SKIPPED_RECORDS-> acceptable skipped records in the neighborhood of a bad record. By default this no would be Long.MAX_VALUE. If skipped range is  greater than MAX_SKIPPED_RECORDS, the task will try to narrow down the skipped range by doing a binary search during task re-executions till MAX_SKIPPED_RECORDS threshold is met or all task attempts are exhausted.\n\nthoughts/suggestions ?", "I am concerned that this further complicates the already complicated JobTracker and TaskTracker.  Ideally we could layer this as a user library.  Even if that's not possible today, we should structure it so that we might transition it to such an implementation.  So, rather than adding more JobConf methods, perhaps its configuration should be through static accessor methods on a SkipBadRecords class?\n\nMight it be possible to implement this through the filesystem?  A MapRunnable  implementation could, when it catches exceptions, record the bad record location to a task/attempt directory.  Then, on startup, the MapRunnable could list task/* files, to find all bad record locations generated by prior invocations.  Could that work?  It shouldn't actually generate that many files, since most tasks should not have errors.  It would generate one extra listFiles call to the namenode per task attempt, which doesn't seem too bad.", "Seems I haven't clearly mentioned, this Jira we scoped for handling Task crashes/hangs as that seems to be more pressing requirement from users. For handling java exceptions, have another JIRA - HADOOP-3700. I should rename the subject of this Jira appropriately.\n\n>I am concerned that this further complicates the already complicated JobTracker and TaskTracker.\nSince we are handling task crashes, the tasktracker and jobtracker have to get involved at some level, no? btw, In the current patch, there are minimal changes to the jobtracker/tasktracker. Most of the logic is in RecordReader wrapper and SortedRanges ( a new class). We get a good working functionality (incorporates test cases) for skipping the bad ranges.\n\n>rather than adding more JobConf methods, perhaps its configuration should be through static accessor methods on a SkipBadRecords class?\n+1. It would avoid polluting the JobConf.\n\n> Might it be possible to implement this through the filesystem?\nYes we can use filesystem to write the bad record sequence nos. But since those are assumed to be very small, we agreed to store in Jobtracker's TIP. When TIP instantiates MapTask, it passes that to it.\nAs Yiping has suggested in one of his comment to write the actual bytes (key,values) of the skipped records, we are trying to address that. My earlier comment #1.\n\n\n\n", "> Since we are handling task crashes, the tasktracker and jobtracker have to get involved at some level, no?\n\nYou're right, at least the TaskTracker needs to be involved, if it's to capture crashed processes, and the JobTracker if it's to handle machine crashes.  Sigh.\n\nA few issues with the patch:\n - InterTrackerProtocol's version needs to change since the format of a Task and TaskStatus have changed.\n - UmbilicalProtocol#reportNextRecordRange() needs javadoc.  This javadoc is not published, but our protocols should be well-documented.\n - You add a method to the Reporter interface: are we sure no users implement this?  Also, this is the wrong place for this method.  It has nothing to do with reporting status.  Unfortunately, getInputSplit() was added to Reporter at some point in the past, setting a bad example.  These should be added to the context (once we have contexts).\n - in MapRunner, autoIncrPrcossedRecord is a typo.\n\n", "I'm nervous about adding extra NN operations per task.  That is going to make scaling to large clusters more expensive.  I'd really like to find another solution if possible.  We just took out a bunch of NN work per task due to performance problems!  (task promotion stuff)\n", "> I'm nervous about adding extra NN operations per task.\ncurrent solution doesn't  add any NN operations. It works based on storing failed ranges in TIP.", "Incorporated Doug's review comments.\n- moved configuration methods out of JobConf\n- changed InterTrackerProtocol's version\n- added javadoc to UmbilicalProtocol#reportNextRecordRange()\n- keeping the Reporter interface unchanged\n- typo in MapRunner fixed", "We can have this Jira to address the base skipping functionality. I have created two dependent Jiras HADOOP-3828 and HADOOP-3829 which are the incremental features over this one.", "disabling the speculation if skip mode is active", "-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12386874/153_4.patch\n  against trunk revision 679845.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 9 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    -1 findbugs.  The patch appears to introduce 3 new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2953/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2953/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2953/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2953/console\n\nThis message is automatically generated.", "Fixed findbugs warnings.", "+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12387079/153_5.patch\n  against trunk revision 680577.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 9 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2968/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2968/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2968/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2968/console\n\nThis message is automatically generated.", "Sigh.  Sorry it's taken so long for me to get back to this.  The patch has now gone stale!  My bad.\n\nIt looks much better.\n\nThe configuration parameters controlling skipping should all start with \"mapred.skip\".  Private constants should probably be defined for these, since they're referred to more than once.  That way the compiler will check that they're all spelled correctly.  The term \"isSkipModeKickedOff\" might instead be just \"inSkipMode\" or simply \"isSkipping\", and \"skip.mode.kicked.off\" might be \"mapred.skip.on\" or somesuch.\n\nIt  would also be good to add javadoc to the new public Task methods and to SortedRange's public methods.  We don't (yet) publish the javadoc for Task, but it is a pretty central class and deserves to be well documented.  In general, public methods in public classes should have javadoc.  Sometimes classes (like Task) which didn't used to be public are made public, and lots of their methods are then missing javadoc, but that's not an excuse to continue the practice.  And it never hurts to have javadoc, even for non-public methods in non-public classes...", "updated to trunk\nincorporated Doug's comments.\nchanged SortedRanges.Range to store (startIndex,length) instead of (startIndex,endIndex)", "cleaned up Test classes. Removed redundant test cases.\nother minor clean up/better variable names.", "+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12387626/153_7.patch\n  against trunk revision 682978.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 9 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3016/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3016/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3016/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3016/console\n\nThis message is automatically generated.", "Doug, could you please review this one last time? Thanks!", "+1 This looks good to me.", "no changes, other than wrapping the lines exceeding 80 chars.", "Sharad, sorry for the late comment - one small change I would like to see - the updates to the counters MAP_PROCESSED_RECORDS and REDUCE_PROCESSED_RECORDS should happen in the test/skip mode only.", "Incorporated Devaraj's comment.", "I just committed this. Thanks, Sharad!", "Integrated in Hadoop-trunk #581 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/581/])"], "derived": {"summary": "MapReduce should skip records that throw exceptions. If the exception is thrown under RecordReader.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "skip records that fail Task - MapReduce should skip records that throw exceptions. If the exception is thrown under RecordReader."}, {"q": "What updates or decisions were made in the discussion?", "a": "Integrated in Hadoop-trunk #581 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/581/])"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-154", "title": "fsck fails when there is no file in dfs", "status": "Closed", "priority": "Trivial", "reporter": "Lei Chen", "assignee": null, "labels": [], "created": "2006-04-21T03:11:25.000+0000", "updated": "2009-07-08T16:41:52.000+0000", "description": "First type in command\n    \n     hadoop namenode -format\n\nThen \n\n     hadoop fsck /\n\nThen is the following exception:\n\nException in thread \"main\" java.lang.ArithmeticException: / by zero\n        at org.apache.hadoop.dfs.DFSck$Result.toString(DFSck.java:563)\n        at java.lang.String.valueOf(String.java:2577)\n        at java.io.PrintStream.print(PrintStream.java:616)\n        at java.io.PrintStream.println(PrintStream.java:753)\n        at org.apache.hadoop.dfs.DFSck.main(DFSck.java:435)\n\nPossible Solution: Check whether totalBlocks equal to 0 in line 563 of DFSck.java", "comments": ["Fixed. Thank you!"], "derived": {"summary": "First type in command\n    \n     hadoop namenode -format\n\nThen \n\n     hadoop fsck /\n\nThen is the following exception:\n\nException in thread \"main\" java. lang.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "fsck fails when there is no file in dfs - First type in command\n    \n     hadoop namenode -format\n\nThen \n\n     hadoop fsck /\n\nThen is the following exception:\n\nException in thread \"main\" java. lang."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fixed. Thank you!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-155", "title": "Add a conf dir parameter to the scripts", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Milind Barve", "labels": [], "created": "2006-04-21T03:19:14.000+0000", "updated": "2006-11-10T21:29:39.000+0000", "description": "We'd like a conf_dir parameter on the startup scripts (ie. \"-c <confdif>\"). In particular, it would be nice if it propagated down from hadoop-daemons.sh to slaves.sh to hadoop-daemon.sh using the command line rather than using the ssh -o SendEnv=HADOOP_CONF_DIR, which is not supported in many environments.", "comments": ["Duplicate of HADOOP-260"], "derived": {"summary": "We'd like a conf_dir parameter on the startup scripts (ie. \"-c <confdif>\").", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add a conf dir parameter to the scripts - We'd like a conf_dir parameter on the startup scripts (ie. \"-c <confdif>\")."}, {"q": "What updates or decisions were made in the discussion?", "a": "Duplicate of HADOOP-260"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-156", "title": "Reducer  threw IOEOFException", "status": "Closed", "priority": "Major", "reporter": "Runping Qi", "assignee": "Owen O'Malley", "labels": [], "created": "2006-04-21T04:10:48.000+0000", "updated": "2009-07-08T16:51:46.000+0000", "description": "A job was running with all the map tasks completed.\nThe reducers were appending the intermediate files into the large intermediate file.\njava.io.EOFException was thrown when the record reader tried to read the version number\nduring initialization. Here is the stack trace:\n\njava.io.EOFException \n    at java.io.DataInputStream.readFully(DataInputStream.java:178) \n    at java.io.DataInputStream.readFully(DataInputStream.java:152) \n    at org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:251) \n    at org.apache.hadoop.io.SequenceFile$Reader.(SequenceFile.java:236) \n    at org.apache.hadoop.io.SequenceFile$Reader.(SequenceFile.java:226) \n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:205) \n    at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:709) \n\nAppearantly, the intermediate file was empty. I suspect that one map task\ngenerated empty intermidiate files for all the reducers, since all the reducers\nfailed at the same place, and failed at the same place during retries.\n\nUnfortunately, we cannot know which map task generated the empty files,\nsince the exception does not offer any clue.\n\nOne simple enhancement is that the record reader should catch IOException and re-throw it with additional \ninformation, such as the file name.\n\n", "comments": ["i've tackled the same problem with hadoop 0.3.2 after setting mapred.reduce.tasks from 'nodes' to 'nodes * mapred.tasktracker.tasks.maximum(==2)' like sugested in the wiki.\n\njava.io.EOFException at java.io.DataInputStream.readFully(DataInputStream.java:178) at java.io.DataInputStream.readFully(DataInputStream.java:152) at org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:263) at org.apache.hadoop.io.SequenceFile$Reader.(SequenceFile.java:247) at org.apache.hadoop.io.SequenceFile$Reader.(SequenceFile.java:237) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:203) at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:847)  ", "I see the same problem with 0.6.2. But I suspect maps are producing correct data. From the logs it appears the map output was not empty for all reduces as some other reduce tasks read output and were successfully finished. For the map outputs which were not available when reduces asked for it, i can see consistently that task tracker assumes maps are done and deletes the files. after this reduces ask for the map data and the failures cascades.\n\nHere is a common pattern -\n\n2006-09-15 15:15:59,564 INFO org.apache.hadoop.mapred.TaskRunner: task_0001_m_000341_0 done; removing files.\n\nFor this task - before it got cleaned up a reduce task copied data successfully and finished. \n\n2006-09-15 02:00:50,537 INFO org.apache.hadoop.mapred.TaskRunner: task_0001_r_000524_0 done copying task_0001_m_000341_0 output.\n\nBut after the cleanup another reduce task tries to copy and fails -\n\n2006-09-15 15:16:01,794 WARN org.apache.hadoop.mapred.TaskTracker: Http server (getMapOutput.jsp): java.io.FileNotFoundException: /***/hadoop/mapred/local/task_0001_m_000341_0/part-94.out\n\nOn reduce task we get same EOFException. \n", "Also the task tracker running the map task in question was *not* reported lost on jobtracker and hence reinitialized causing map output to cleanup. ", "ok, the EOFException in the case I posted earlier is not a critical issue. it happened after the job was aborted on job tracker, some task trackers cleaned up map outputs while reduce tasks were still running on others. \n\nHowever there are genuine EOFExceptions as well which occur in sort phase on reduce tasks, which may be due to malformed map output. e.g. following \n\n2006-09-15 08:04:13,900 INFO org.apache.hadoop.mapred.TaskTracker: task_0001_r_000511_0 0.33333334% reduce > sort\n\n2006-09-15 08:04:13,996 INFO org.apache.hadoop.mapred.TaskRunner: task_0001_r_000511_0 06/09/15 08:04:13 WARN mapred.TaskTracker: Error running child\n2006-09-15 08:04:13,996 INFO org.apache.hadoop.mapred.TaskRunner: task_0001_r_000511_0 java.io.EOFException\n2006-09-15 08:04:13,996 INFO org.apache.hadoop.mapred.TaskRunner: task_0001_r_000511_0  at java.io.DataInputStream.readFully(DataInputStream.java:178)\n2006-09-15 08:04:13,996 INFO org.apache.hadoop.mapred.TaskRunner: task_0001_r_000511_0  at java.io.DataInputStream.readFully(DataInputStream.java:152)\n2006-09-15 08:04:13,996 INFO org.apache.hadoop.mapred.TaskRunner: task_0001_r_000511_0  at org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:952)\n2006-09-15 08:04:13,996 INFO org.apache.hadoop.mapred.TaskRunner: task_0001_r_000511_0  at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:937)\n2006-09-15 08:04:13,996 INFO org.apache.hadoop.mapred.TaskRunner: task_0001_r_000511_0  at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:928)\n2006-09-15 08:04:13,996 INFO org.apache.hadoop.mapred.TaskRunner: task_0001_r_000511_0  at org.apache.hadoop.io.SequenceFile$Sorter$SortPass.run(SequenceFile.java:1594)\n2006-09-15 08:04:13,996 INFO org.apache.hadoop.mapred.TaskRunner: task_0001_r_000511_0  at org.apache.hadoop.io.SequenceFile$Sorter.sortPass(SequenceFile.java:1523)\n2006-09-15 08:04:13,996 INFO org.apache.hadoop.mapred.TaskRunner: task_0001_r_000511_0  at org.apache.hadoop.io.SequenceFile$Sorter.sort(SequenceFile.java:1496)\n2006-09-15 08:04:13,996 INFO org.apache.hadoop.mapred.TaskRunner: task_0001_r_000511_0  at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:240)\n2006-09-15 08:04:13,996 INFO org.apache.hadoop.mapred.TaskRunner: task_0001_r_000511_0  at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1165)\n", "I suspect this was one of the race conditions and that part of the code has changed completely since this was filed."], "derived": {"summary": "A job was running with all the map tasks completed. The reducers were appending the intermediate files into the large intermediate file.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Reducer  threw IOEOFException - A job was running with all the map tasks completed. The reducers were appending the intermediate files into the large intermediate file."}, {"q": "What updates or decisions were made in the discussion?", "a": "I suspect this was one of the race conditions and that part of the code has changed completely since this was filed."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-157", "title": "job fails because pendingCreates is not cleaned up after a task fails", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-04-21T23:03:19.000+0000", "updated": "2009-07-08T16:41:52.000+0000", "description": "When a task fails under map/reduce, if the client doesn't abandon the files in progress (usually because it was killed), the lease on the name node lasts 1 minute. During that minute, I see 3 backup copies of the task fail because pendingCreates is non-null.", "comments": ["This patch improves the failures reporting.\n\n1. I created org.apache.hadoop.ipc.RemoteException class that includes the class name of the exception that was the cause.\n2. The ipc client throws this RemoteException rather than java.rmi.RemoteException.\n3. The DFSClient.create waits and retries if the file is already being created.\n4. Killed tasks do not complain when they have non-zero exit codes from their process.\n5. Improved the error message when tasks are killed for not updating their progress.\n6. Dfs' ClientProtocol.addBlock now takes the client name rather than the client machine.\n7. Problems renewing dfs leases are now logged.\n8. More details in the exception messages when DfsClient.create fails.\n9. addBlock now checks to make sure it is the same client that owns the lease who is adding to the file.\n10. FileUnderConstruction now records who is creating the file.\n11. Some new exception classes defined for problems that DFSClient wants to catch", "I just committed this.  Thanks, Owen!"], "derived": {"summary": "When a task fails under map/reduce, if the client doesn't abandon the files in progress (usually because it was killed), the lease on the name node lasts 1 minute. During that minute, I see 3 backup copies of the task fail because pendingCreates is non-null.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "job fails because pendingCreates is not cleaned up after a task fails - When a task fails under map/reduce, if the client doesn't abandon the files in progress (usually because it was killed), the lease on the name node lasts 1 minute. During that minute, I see 3 backup copies of the task fail because pendingCreates is non-null."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-158", "title": "dfs should allocate a random blockid range to a file, then assign ids sequentially to blocks in the file", "status": "Closed", "priority": "Major", "reporter": "Doug Cutting", "assignee": "Konstantin Shvachko", "labels": [], "created": "2006-04-22T00:06:18.000+0000", "updated": "2009-07-08T16:41:52.000+0000", "description": "A random number generator is used to allocate block ids in dfs.  Sometimes a block id is allocated that is already used in the filesystem, which causes filesystem corruption.\n\nA short-term fix for this is to simply check when allocating block ids whether any file is already using the newly allocated id, and, if it is, generate another one.  There can still be collisions in some rare conditions, but these are harder to fix and will wait, since this simple fix will handle the vast majority of collisions.\n", "comments": ["Using the formulas in:\n\nhttp://en.wikipedia.org/wiki/Birthday_paradox#Generalisation\n\nI think it is actually very unlikely that, with 64-bit block ids and a decent random number generator, we are actually seeing collisions.  It seems more likely that the symptoms ascribed to duplicate block id allocations are actually the result of other bugs.  Still, it would be more comfortable to not rely on random block id allocation long-term.", "Block id collisions have largely beed addressed by the fix in HADOOP-146. The namenode first checks for the presence of a randomly generated blockid before assigning it to a block.\n\nLonger term, we should implement a scheme where the namenode allocates large blockid ranges to files. When a file is created the namenode generates a 5-byte integer, a range-id, for the file, checking for collisions and re-generating the range-id if necessary. Blocks for the new file are then assigned 8-byte block ids sequentially, where the high 5-bytes are the range-id and the low 3-bytes correspond to the block number within the file. Blocks in a file then get the ids rangeid-0, rangeid-1, ..., rangeid-(N-1), where N is the number of blocks in the file. This lets us assign upto a trillion ranges and lets each file grow to 0.5, 1, 2 ... petabytes depending on whether the block size is 32, 64, 128 ... MB. The scheme has the additional benefit of saving some memory per block at the namenode.\n\nThere is the scenario of a file being deleted while a node holding some of its blocks is down, having the files range-id re-assigned to another file and seeing collisions when the node later comes back. To get around this, the namenode tags each block in a file with the creation time of the file. When a collision occurs the timestamps will be different, the most recent timestamp wins and old blocks are discarded.\n\n", "Why must the file-id part of the block id be random?  Can't that be sequential?", "It can be sequential. In that case, the namenode would need to determine the lowest unused file-id at startup and start file-id assignments from that point. \n\nEven sequential allocation of file-ids should probably do the collision check because you don't need a trillion files in the system before you wrap around, you only need a trillion file creation events. If you're doing the collision check in both schemes the random file-id assignment keeps things simpler.\n\nThe possibility of collision with sequential assignment of file-ids is very remote, but why expose ourselves? I'm probably being paranoid so ignore me on this one if you want.\n\n\n\n", "It could, but that would make upgrading an existing file system harder: one would have to \"compress\" the id's before upgrading, then keep track of the high-water-mark file id. The upgrade wouldn't be as smooth as when selecting 'clean' ranges randomly, and would require metadata conversion. Blocks on individual datanodes may even need to change their ids.\nOR, one could copy/move the entire filesystem to a new, clean one.\nThe selected method allows for a seamless upgrade, requiring no conversion, and would work equally well with a 2^64 id address space.", "I would think that random allocation would make collisions more likely, not less.  We always know which block ids are used by complete files.  The concern is only about block ids which have been recently allocated to a file, but the file is somehow not yet complete.  So, with sequential allocation, a collision can only happen if the probe key (the next block id to allocate) wraps all the way around before a file is completed, while with random allocation it can happen much more frequently.  We simply have to make sure that probe key increments are logged to the edits file along with other file system changes.  Am I missing something?", "Yes, random assignment of file-ids makes collisions more likely. However, collisions are possible even with sequential assignment, and if they are possible they need to be detected. Since, collision detection code is needed with both random and sequential assignment, random assignment makes the system simpler because the namenode doesn't have to track the 'high watermark' file-id.\n\nDon't think recently assigned file-ids that belong to incomplete files are a concern, since the namenode will be aware of all file-ids used, whether they belong to incomplete files or not.\n\nWrap around before a file completes is not the only collision scenario. In the sequential assignment scheme, suppose, the first million files in the system get the file-ids, 0-999999. These files archival data of some kind, so are never deleted. Life goes on, lots of files are created and removed, at any given time there are only a few million files total (complete + incomplete) in the system. At some point, the system will have gone through a trillion file creation events, the file-ids will wrap and start to collide with the first million files.\n", "> At some point, the system will have gone through a trillion file creation events [ ... ]\n\nWe generally aim for a block created per drive no more than every 100 milliseconds, so that transfer dominates seek.  With 10,000 nodes, each with four drives, that would give a maximum block creation rate of 400k/second (assuming a replication level of one).  At that rate it would take 100,000 years to exhaust all 64-bit block ids.  I wonder what version Hadoop will have then?\n"], "derived": {"summary": "A random number generator is used to allocate block ids in dfs. Sometimes a block id is allocated that is already used in the filesystem, which causes filesystem corruption.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "dfs should allocate a random blockid range to a file, then assign ids sequentially to blocks in the file - A random number generator is used to allocate block ids in dfs. Sometimes a block id is allocated that is already used in the filesystem, which causes filesystem corruption."}, {"q": "What updates or decisions were made in the discussion?", "a": "> At some point, the system will have gone through a trillion file creation events [ ... ]\n\nWe generally aim for a block created per drive no more than every 100 milliseconds, so that transfer dominates seek.  With 10,000 nodes, each with four drives, that would give a maximum block creation rate of 400k/second (assuming a replication level of one).  At that rate it would take 100,000 years to exhaust all 64-bit block ids.  I wonder what version Hadoop will have then?"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-159", "title": "name node at 100% cpu, making redundant replications", "status": "Closed", "priority": "Blocker", "reporter": "Yoram Arnon", "assignee": "Sameer Paranjpye", "labels": [], "created": "2006-04-22T05:56:24.000+0000", "updated": "2009-07-08T16:41:52.000+0000", "description": "some hours after adding some new nodes to the cluster, the name node went into a state where it's consuming 100% cpu.\nThe log file keeps logging messages of the forms\n060421 155049 Obsoleting block blk_8093115169359854355\n060421 155049 Pending transfer (block blk_-6965677235456960523) from node1383:50010 to 2 destinations\n060421 155049 Block report from node1283:50010: 2140 blocks.\n060421 155049 Redundant addStoredBlock request received for block blk_-6836937139917042917 on node node1143:50010\n\nmany DFS operations time out, making useful work impossible.\n\nrestarting dfs solved the problem for a while, but it came back within an hour.\n", "comments": ["Is this problem still seen? Some of the committed patches appear to have resolved this issue, no?", "It was observed just twice, in the same timeframe.\n\nI'm not aware of any patches that attempt to address this. Logging was added though.\nLet's wait a while before closing, unless there's a specific fix.", "Looks like HADOOP-178 addresses this. Let's monitor and re-open if it manifests again."], "derived": {"summary": "some hours after adding some new nodes to the cluster, the name node went into a state where it's consuming 100% cpu. The log file keeps logging messages of the forms\n060421 155049 Obsoleting block blk_8093115169359854355\n060421 155049 Pending transfer (block blk_-6965677235456960523) from node1383:50010 to 2 destinations\n060421 155049 Block report from node1283:50010: 2140 blocks.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "name node at 100% cpu, making redundant replications - some hours after adding some new nodes to the cluster, the name node went into a state where it's consuming 100% cpu. The log file keeps logging messages of the forms\n060421 155049 Obsoleting block blk_8093115169359854355\n060421 155049 Pending transfer (block blk_-6965677235456960523) from node1383:50010 to 2 destinations\n060421 155049 Block report from node1283:50010: 2140 blocks."}, {"q": "What updates or decisions were made in the discussion?", "a": "Looks like HADOOP-178 addresses this. Let's monitor and re-open if it manifests again."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-160", "title": "sleeping with locks held", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-04-23T07:01:20.000+0000", "updated": "2006-08-03T17:46:37.000+0000", "description": "I ran findbugs and it reported 7 cases of sleeping with locks held. Part of what is killing jobs is unreasonably slow responses from the servers (and task tracker in particular), and this may be contributing to that.", "comments": ["This patch removes the locking of the TaskTracker while the thread sleeps to wait for completion of the task in TaskTracker.taskFinished. It also moves startNewTask to just lock the tip instead of the TaskTracker while the new task is launched.", "I just committed this patch.  Thanks, Owen."], "derived": {"summary": "I ran findbugs and it reported 7 cases of sleeping with locks held. Part of what is killing jobs is unreasonably slow responses from the servers (and task tracker in particular), and this may be contributing to that.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "sleeping with locks held - I ran findbugs and it reported 7 cases of sleeping with locks held. Part of what is killing jobs is unreasonably slow responses from the servers (and task tracker in particular), and this may be contributing to that."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this patch.  Thanks, Owen."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-161", "title": "dfs blocks define equal, but not hashcode", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Milind Barve", "labels": [], "created": "2006-04-23T07:04:00.000+0000", "updated": "2009-07-08T16:41:52.000+0000", "description": "Findbugs reports that dfs.Block defines equals but not hashcode, which is problematic if it is ever put into a hash table.", "comments": ["Added a hashCode method to Block. The hash algorithm is from \"Effective Java\" by Joshua Bloch.", "Please disregard the last patch. Since equals compares blockIds only, hashCode should also consider only blockIds. Correct patch is attached here.", "I just committed this.  Thanks, Milind!"], "derived": {"summary": "Findbugs reports that dfs. Block defines equals but not hashcode, which is problematic if it is ever put into a hash table.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "dfs blocks define equal, but not hashcode - Findbugs reports that dfs. Block defines equals but not hashcode, which is problematic if it is ever put into a hash table."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Milind!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-162", "title": "concurrent modification exception in FSNamesystem.Lease.releaseLocks", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-04-25T02:54:19.000+0000", "updated": "2009-07-08T16:41:52.000+0000", "description": "FSNameSystem.Lease.releaseLocks iterates through the creates set, calling InternalReleaseCreate on each element, which changes the creates set in the Lease. This causes a ConcurrentModificationException if you have more than two files that are owned by the lease that times out.", "comments": ["This patch moves the lease finding/updating code out of internalReleaseCreate and into abandonFileInProgress. releaseLocks clears the set itself.", "I just committed this.  Thanks, Owen."], "derived": {"summary": "FSNameSystem. Lease.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "concurrent modification exception in FSNamesystem.Lease.releaseLocks - FSNameSystem. Lease."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-163", "title": "If a DFS datanode cannot write onto its file system. it should tell the name node not to assign new blocks to it.", "status": "Closed", "priority": "Major", "reporter": "Runping Qi", "assignee": "Hairong Kuang", "labels": [], "created": "2006-04-25T04:52:02.000+0000", "updated": "2009-07-08T16:41:53.000+0000", "description": "I observed that sometime, if a file of a data node is not mounted properly, it may not be writable. In this case, any data writes will fail. The name node should stop assigning new blocks to that data node. The webpage should show that node is in an abnormal state.\n\n", "comments": ["Alternately, the datanode daemon should simply exit if it cannot write to its configured data directories.", "\nExiting is an option. However, the datanode may still be able to read, thus to serve the existing blocks.\n", "Good point.  So perhaps a read-only node should report itself at 100% of capacity?  Then the namenode should never allocate blocks to it.", "I envision 24x7 systems where the datanode is automatically restarted upon failure by init or another HA component. When a partition/FS fails, it will likely remain in a failed state after restart, so reporting up and stopping to serve would be better than simply exitting, which would lead to thrashing. At the end of the day, outside intervention would be required, so the most important part is diagnosing the error and reporting it as such. reporting 100% full would not generate the same kind of attention by a correction system/person.", "But should a datanode with no disk or a read-only disk still send heartbeats to the namenode?  I think not.  So it should exit the datanode daemon loop.  I think more than that is hard to specify at this point.  You're talking about what we should do when we have a system that automatically restarts, and a system that monitors, etc.  We don't have those systems in Hadoop today, so they're hard to code to!  In the meantime, do you think it would be better to enter some zombie state, not sending heartbeats or otherwise participating in namenode network protocols, but looping sending out SOS over channels TBD?", "what I'm suggesting is close to your suggestion:\nif you're read-only, behave as though you're 100% full, serve only read requests, but don't mislead: report you're read-only, not that you're 100% full. Namenode will avoid new block allocations to the node, but its log will contain an error that could trigger external corrective action.\n", "I plan to take a simple approach to this problem. If a data node finds out that it can not write to its disk. It reports to the name node and aborts. The name node logs the error and alerts it on the http UI.\n\nI will use the existing data node protocol \"errorReport\" for the error reporting but with a minor change. In addition to the parameter error message, the rpc will also send an error code so that the name node does not need to parse the error message to figure out which action to take.", "Sounds good - except, what \"aborts\"? The idea of the datanode staying operating, but reporting error and not accepting further blocks is probably better, but maybe you meant \"abort the block write\". The node's blocks should probably not be counted by the namenode, but still available as a source for replication. Also, staying up means that there are fewer timeouts - it used to be that, when writing large volumes into DFS, if one or more of your nodes was full, your writer would hit a periodic timeout as connections to the (full and constantly restarting) datanode were refused. Hitting a timeout because some fraction of all resources is overused is, of course, much *much* slower than continuing to stream. Further - if the datanode periodically re-tests if the error condition has lifted, it can more immediately begin contributing to the cluster productivity again.", "We are trying to deal with the case that the node is misconfigured / broken.  Trying to operate in these situations is hard.  Simpler to fail fast, IMO.  This leverages the designed strengths of HDFS.  Our goal is to get the information to the operator so they can diagnose and fix the problem and seal the problem off from the cluster.\n\nThis is distinct from the case that the node is simply full.  That would not trigger this condition.", "In this patch, if a data node finds that its data directory becomes not readable or writable, it logs the error and reports the problem to its namen ode and shut down itself. When the name node receives the error report, it lots the error and removes the data node info.\n\nA data node detects disk problem at startup time, when it receives a r/w request, after it receives a command from its name node, and before it sends out a block report. A data node will not start up if its data dir is not readable or writable. ", "This looks great!  I just committed it.  Thanks, Hairong!"], "derived": {"summary": "I observed that sometime, if a file of a data node is not mounted properly, it may not be writable. In this case, any data writes will fail.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "If a DFS datanode cannot write onto its file system. it should tell the name node not to assign new blocks to it. - I observed that sometime, if a file of a data node is not mounted properly, it may not be writable. In this case, any data writes will fail."}, {"q": "What updates or decisions were made in the discussion?", "a": "This looks great!  I just committed it.  Thanks, Hairong!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-165", "title": "long response times from task trackers under load", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": null, "labels": [], "created": "2006-04-25T23:34:02.000+0000", "updated": "2009-07-08T16:51:44.000+0000", "description": "we are seeing very slow response times from the task tracker. I put in some instrumentation to measure how long each call took for the RPC.Sever code to run the method (so it does not include serialization/deserialization time). The top of the list (in ms) looks like:\n\n7581 progress node1192\n7022 ping node1192\n5393 ping node1162\n4854 progress node1162\n4749 progress node1194\n4709 ping node1194\n3813 ping node1100\n3486 ping node1190\n3266 progress node1190\n3187 progress node1265\n3078 ping node1203\n2972 progress node1203\n2947 progress node1240\n2889 progress node1100\n2875 ping node1116\n2843 ping node1189\n2772 ping node1183\n2737 ping node1110\n2727 progress node1183\n2710 ping node1123\n2563 ping node1304\n2527 progress node1144\n2479 ping node1137\n2476 progress node1304\n2430 ping node1240\n2416 ping node1144\n2377 progress node1176\n2339 ping node1109\n2321 progress node1114\n2311 ping node1157\n2185 ping node1265\n2185 ping node1109\n2172 ping node1114\n2145 progress node1109\n2127 ping node1176\n2083 progress node1189\n2076 ping node1229\n2073 progress node1188\n2072 progress node1123\n2048 ping node1161\n2003 progress node1110\n1989 ping node1180\n1963 ping node1114\n", "comments": ["Can you please try placing 'Server.LOG.setLevel(Level.FINE);' in the tasktracker and 'Client.LOG.setLevel(Level.FINE);' in the child.  That should provide some details about where the delay(s) are.\n", "I guess I should have made it clear that most of the times aren't this bad. This is the head of a long list. For example, the tail of the ping times look like (count, time):\n\n      2 134\n      1 123\n      1 116\n      1 108\n      1 101\n      1 79\n      1 62\n      1 56\n      1 55\n      1 37\n      1 30\n      1 15\n      1 12\n      2 4\n     11 3\n     90 2\n    312 1\n   9836 0\n\nso by far most of the pings take 0 or 1 ms to execute.", "Did fixing HADOOP-160 help this any?", "This has been fixed primarily by moving the pulling part of the shuffle from the task tracker to the reduce's jvm. Previously, we saw task trackers burning 100% cpu and now it is much better."], "derived": {"summary": "we are seeing very slow response times from the task tracker. I put in some instrumentation to measure how long each call took for the RPC.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "long response times from task trackers under load - we are seeing very slow response times from the task tracker. I put in some instrumentation to measure how long each call took for the RPC."}, {"q": "What updates or decisions were made in the discussion?", "a": "This has been fixed primarily by moving the pulling part of the shuffle from the task tracker to the reduce's jvm. Previously, we saw task trackers burning 100% cpu and now it is much better."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-166", "title": "IPC is unable to invoke methods that use interfaces as parameter", "status": "Closed", "priority": "Minor", "reporter": "Stefan Groschupf", "assignee": "Doug Cutting", "labels": [], "created": "2006-04-26T06:12:25.000+0000", "updated": "2006-08-03T17:46:37.000+0000", "description": "Methods of the implementation class are searched via method name and call parameters that can be implementations of the interfaces defined in the method signature.\n", "comments": ["A test that shows the problem and a patch suggestion.", "The problem isn't that the parameter is an interface, but rather that the declared class in the implementation is not exactly the same class as the instance passed.  One way to fix this would be to add the list of parameter types of the client method when writing the invocation, since those should match the server implementation, but that would make each RPC call bigger.  Alternately one could, as your patch suggests, search for an appropriate method.  However, your search code is not  quite correct.  The search should be ordered from the implementation class through its superclasses, to find the most specific implementing method.  And searching adds a cost to each RPC, so we'd probably want to cache the results of searches.\n\nSo I think I probably prefer adding the parameter types to the invocation.  This can be easily fixed in ObjectWritable by having it write two class names for Writable parameters: both the declared parameter type and the instance type.", "Make sense.\nHere an update as you suggested.\nI renamed the declaredClass into instanceClass in the readObject method, so the change looks much larger as it is, but I found using two variables with the same name not that senseful. \n\nLooks that overall better?\n", "I just committed a fix for this.  Thanks for your help, Stefan!"], "derived": {"summary": "Methods of the implementation class are searched via method name and call parameters that can be implementations of the interfaces defined in the method signature.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "IPC is unable to invoke methods that use interfaces as parameter - Methods of the implementation class are searched via method name and call parameters that can be implementations of the interfaces defined in the method signature."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed a fix for this.  Thanks for your help, Stefan!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-167", "title": "reducing the number of Configuration & JobConf objects created", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-04-26T10:52:22.000+0000", "updated": "2006-08-03T17:46:37.000+0000", "description": "Currently, Configuration and JobConf objects are created many times during executing a job. In particular, the Task Tracker creates a lot of them. They both clutter up the logs and parse the xml config files over and over again.", "comments": ["This patch removes a lot of the extra JobConfs from the TaskTracker. In particular, it does not read the config files for each map output as it is transfered to a reduce. It also adds a new constructor for JobConf(Class) and makes the old JobConf() constructor read the mapred-defaults.xml. Unless Nutch makes heavy use of the JobConf(Configuration) and JobConf(Configuration,Class) constructors, I think we should depriciate them. There really isn't any advantage to having a separate Configuration object.", "Avoiding the multiple config-loading messages is a good thing.\nThis could also be controlled with a verbosity  / logging level setting.\n\nPlease don't remove the JobConf(Configuration) constructor.\n\nThis is the only mechanism available to programatically change your Configuration.\nWe rely on this in two ways: \n1. to select from multiple XML config files that correspond to multiple Hadoop systems.\n2. to make some properties (paths) user-dependant.\n\nEx for 1.\n    config_ = new Configuration();\n    config_.addFinalResource(getHadoopAliasConfFile());\n    jobConf_ = new JobConf(config_);\n\nIn fact, it is important for Hadoop to maintain this property:\n ALL uses of a JobConf must be configurable at the outset by the caller by passing in a Configuration object.\n Common examples of such top-level Hadoop entry points: \n     Job submission, MapRed in local mode, DFS client calls.\n\nIn general we should make sure that we don't FORCE \na long lifetime for a 'cached' JobConf object:\nThere are applications that need to use new JobConf-s along the way:\n1. bec. they must first discover properties of the Hadoop cluster (list files, then submit job)\n2. bec. they talk to multiple Hadoop systems (import / export files)\n", "Your example is exactly the same as:\n\njobConf = new JobConf();\njobConf.addFinalResource(getHadoopAliasConfFile());\n\njust without reading the xml files an extra time.", "Can we stop the extra reads caused by addFinalResource() and 'new JobConf(Configuration)' by re-using the hash table instead of re-reading the files?  addFinalResource could simply read that single file, rather than re-read everything.  And 'new JobConf(Configuration)' could clone the contents of the configuration rather than re-reading it, no?  Or even use nested Properties...\n\nOne feature that's currently supported is that Configuration.write() only writes things that differ from the defaults.  This isn't essential, but it's nice.  The way it distinguishes is that defaults are always in a nested properties and non-defaults are always in the top-level properties.", "I just committed this.  Thanks, Owen!"], "derived": {"summary": "Currently, Configuration and JobConf objects are created many times during executing a job. In particular, the Task Tracker creates a lot of them.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "reducing the number of Configuration & JobConf objects created - Currently, Configuration and JobConf objects are created many times during executing a job. In particular, the Task Tracker creates a lot of them."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-168", "title": "JobSubmissionProtocol and InterTrackerProtocol don't include \"throws IOException\" on all methods", "status": "Closed", "priority": "Minor", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-04-26T12:13:37.000+0000", "updated": "2009-07-08T16:51:44.000+0000", "description": "Timeouts for RPC's are thrown as IOExceptions so unless the method is declared as throwing IOException in the Procotol interface, the Java library wraps the exception in an UndeclaredThrowableException.", "comments": ["This patch adds \"throws IOException\" to the RPC protocol methods that didn't have it.", "I just committed this.  Thanks, Owen!"], "derived": {"summary": "Timeouts for RPC's are thrown as IOExceptions so unless the method is declared as throwing IOException in the Procotol interface, the Java library wraps the exception in an UndeclaredThrowableException.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "JobSubmissionProtocol and InterTrackerProtocol don't include \"throws IOException\" on all methods - Timeouts for RPC's are thrown as IOExceptions so unless the method is declared as throwing IOException in the Procotol interface, the Java library wraps the exception in an UndeclaredThrowableException."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-169", "title": "a single failure from locateMapOutputs kills the entire job", "status": "Closed", "priority": "Critical", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-04-26T12:24:43.000+0000", "updated": "2009-07-08T16:51:44.000+0000", "description": "Any communication failure in locateMapOutputs kills both the reduce task and the entire job.", "comments": ["This patch just catches the exception, logs it, and ignores it. This patch depends on the patch for hadoop-168. (Otherwise, you'd need to catch UndeclaredThrowableException instead of IOException.)", "I just committed this patch.  Thanks, Owen."], "derived": {"summary": "Any communication failure in locateMapOutputs kills both the reduce task and the entire job.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "a single failure from locateMapOutputs kills the entire job - Any communication failure in locateMapOutputs kills both the reduce task and the entire job."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this patch.  Thanks, Owen."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-170", "title": "setReplication and related bug fixes", "status": "Closed", "priority": "Major", "reporter": "Konstantin Shvachko", "assignee": "Konstantin Shvachko", "labels": [], "created": "2006-04-27T04:25:48.000+0000", "updated": "2009-07-08T16:41:48.000+0000", "description": "Having variable replication (HADOOP-51) it is natural to be able to\nchange replication for existing files. This patch introduces the functionality.\nHere is a detailed list of issues addressed by the patch.\n\n1) setReplication() and getReplication() methods are implemented.\n2) DFSShell prints file replication for any listed file.\n3) Bug fix. FSDirectory.delete() logs delete operation even if it is not successful.\n4) Bug fix. This is a distributed bug.\nSuppose that file replication is 3, and a client reduces it to 1.\nTwo data nodes will be chosen to remove their copies, and will do that.\nAfter a while they will report to the name node that the copies have been actually deleted.\nUntil they report the name node assumes the copies still exist.\nNow the client decides to increase replication back to 3 BEFORE the data nodes\nreported the copies are deleted. Then the name node can choose one of the data nodes,\nwhich it thinks have a block copy, to replicate the block to new data nodes.\nThis setting is quite unusual but possible even without variable replications.\n5) Logging for name and data nodes is improved in several cases.\nE.g. data nodes never logged that they deleted a block.\n\n", "comments": ["We should start setting a high replication count in MapReduce for submitted job files, that are read by every node.  But how should we use this?  Can we simply do something like setReplication(Integer.MAX_VALUE) and have a file replicated as high as the fs thinks is useful (once per node, once per rack, once per nodes/10, whichever it chooses).", "This probably has to be called smartReplication().\n\nRight now even if we the data blocks are placed \"smart\" by the namenode\nwe will still not see the desired locality because of the crc files, which are\ncompletely independent on the data files.", "It's really JobTracker, not the fs, that knows how high to set the replication count since the JobTracker will know the number of mapper tasks. The JobTracker also knows when the job has finished and can set the replication count back to the original number. I guess in most cases the job files will just get deleted anyway at the end of the job.\n\nWhen you say \"submitted job files\", you are just talking about the job conf and jar file that correspond to a job, correct?", ">  This probably has to be called smartReplication(). \n\nWhat's the matter with Integer.MAX_VALUE?\n\nThis is one of the most important applications for variable replications counts.  We have them now, but they're not yet easy to use in the most obvious and needed application.  That's why I'm asking about this.\n\n> we will still not see the desired locality because of the crc files\n\nThe .crc files are very tiny, much less than 1%.  If all of the data is read locally but .crc files, then throughput will be much faster, switches will not be the bottleneck.\n", "Can you please add proper javadoc to the new public methods in FileSystem.java?  Thanks.\n\nAlso, as mentioned above, an easy way to indicate wide replication would be a welcome addition.  I'll certainly commit this without that, but I'd really like to be able to use this feature, and cannot yet.", "the optimal replication factor for distributing a file in 2 hops is sqrt(cluster size). The worst case delivery time is 2*sqrt(n) times a single transfer time.\n\nreplicating to all the nodes of the dfs cluster has two disadvantages:\n1. it's slower than replicating to sqrt(n), with a worst case of n.\n2. if the dfs cluster is large, but multiple smaller map-reduce clusters are using it concurrently, there's little value in over-replication to every node of the cluster when it's required by a subset.\n\n+1 on asking the job tracker for the size of the cluster and replicating based on that size.", "Java documentation is added for new public methods.\n", "> +1 on asking the job tracker for the size of the cluster and replicating based on that size.\n\nI think you mean \"namenode\", not \"job tracker\".  Is that right?  If so, this sounds fine.  I don't care that we get it exactly right the first time, just that the API have some standard way to say \"replicate highly\": we can improve the implementation later, but we should only have to change client applications once.", "- Replication can be set to maximum if the number of nodes is available.\nThat is block will replicated on all almost all nodes.\n- Replication can be set to n/10, whatever n you choose.\n- Currently cannot achieve replication one per rack, since there is no notion of racks, which well known.\nHow exactly do you want to use  Integer.MAX_VALUE?", "I just committed this.  Thanks, Konstantin!", "\nIt will be more effective if after assigning a map task to a specific jobtracker node, the jobtrackercan request the name node to replicate the blocks of the split for the maptask on to the jobtracker node."], "derived": {"summary": "Having variable replication (HADOOP-51) it is natural to be able to\nchange replication for existing files. This patch introduces the functionality.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "setReplication and related bug fixes - Having variable replication (HADOOP-51) it is natural to be able to\nchange replication for existing files. This patch introduces the functionality."}, {"q": "What updates or decisions were made in the discussion?", "a": "It will be more effective if after assigning a map task to a specific jobtracker node, the jobtrackercan request the name node to replicate the blocks of the split for the maptask on to the jobtracker node."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-171", "title": "need standard API to set dfs replication = high", "status": "Closed", "priority": "Major", "reporter": "Doug Cutting", "assignee": "Konstantin Shvachko", "labels": [], "created": "2006-04-27T06:24:07.000+0000", "updated": "2009-07-08T16:41:47.000+0000", "description": "There should be a standard way to indicate that files should be highly replicated, appropriate for files that all nodes will read.  This should be settable both on file creation and for already-existing files.  Perhaps specifying a particular replication value, like Short.MAX_VALUE, or zero, can be used to signal this.  The level should not be constant, but should be relative to the cluster size and network topography.  If more nodes are added or if nodes are deleted, the actual replication count should increase or decrease.\n\nInitially, all that is needed is an API to specify this.  It could initially be implemented with a constant (e.g., 10) or with something related to the number of datanodes (sqrt?), and needn't auto-adjust as the cluster size changes  That is only  the long-term goal.\n\nWhen JobClient copies job files (job.xml & job.jar) into the job's filesystem, it should specify this replication level.\n", "comments": ["This bug follows from the discussion in HADOOP-170.", "So my concrete question is: how should I change JobClient?  It currently uses fs.copyFromLocalFile() to copy the job.jar and fs.create() to write the job.xml.  Both of these files should be replicated highly.  This code should not have to explicitly query the FileSystem to ask how many nodes it has, etc.\n\nIf my Short.MAX_VALUE suggestion were implemented, then these calls might become something like:\n\nfs.create(\"job.xml\", Short.MAX_VALUE);\n\nand\n\nfs.copyFromLocalFile(localJobJar, remoteJobJar, Short.MAX_VALUE);\n\nDo someone have another proposal for how we should achieve this?", "One alternative to 'fs.copyFromLocalFile(localJobJar, remoteJobJar, Short.MAX_VALUE)' might be:\n\nfs.copyFromLocalFile(localJobJar, remoteJobJar);\nfs.setReplicatoin(remoteJobJar, Short.MAX_VALUE);\n\nIn other words, we indicate this after file creation.  Or if folks don't like using Short.MAX_VALUE this way, then this could be something like:\n\nfs.create(\"job.xml\"); \nfs.setReplicateHighly(\"job.xml\");\nfs.copyFromLocalFile(localJobJar, remoteJobJar);\nfs.setReplicateHighly(remoteJobJar);\n\nOne issue with this is, what would getReplication() return for these?  And would setReplication(f, getReplication(f)) be a no-op?  An advantage of using a sentinel value like Short.MAX_VALUE is that it doesn't add a lot of special cases to the existing, numeric API.", "We can implement\nshort highReplicationHint()\nwhich would ask namenode before anything what it thinks an appropriate\nreplication for highly accessed files would be.\nThen copyFromLocalFile() would use that value to create files.\nThe size of the cluster does not vary often. 10% variation due to node\nfailure and come back is not a big deal with respect to sqrt or /10.", ">  We can implement short highReplicationHint()\n\nThat would work, but it doesn't seem like the simplest API.  But if folks feel strongly I can live with it.\n\n> Then copyFromLocalFile() would use that value to create files.\n\nI don't think we want copyFromLocal to always do that.  I think we'd end up with something like:\n\nfs.setReplication(fs.highReplicationHint());\n\nThat's okay.  If the filesystem grows or shrinks a lot then it might become inadequate.  But whatever, if most folks prefer this, I'd be willing to go along.\n\nPerhaps we need to distingush in the API between the requested replication count and the actual replication count.  Then one can set the requested level to Small.MAX_VALUE and query the actual value to see how the fs interprets this.\n", "highReplicationHint is not the right description IMO.  One goal is to optimize on distribution.  Call it distributionHint().\n\nAnother might be everyRackHint().  Different semantics, different effect.", "There are two different issues here, I think\n1) asking for a replication hint.\nThis is what highReplicationHint() is for.\nWe can provide a parameter (strategy) making it look like\n    short highReplicationHint( enum strategy )\nThis one is easy since it just returns a number based on the number of registered data nodes.\nBut it works for the problem of choosing replication for submitted job files (.jar, .xml).\n2) dynamically maintaining replication of a file based on a chosen strategy and cluster size.\nThis is hard. \nDo we really need that now? \nAre submitted job files removed after the job is done?"], "derived": {"summary": "There should be a standard way to indicate that files should be highly replicated, appropriate for files that all nodes will read. This should be settable both on file creation and for already-existing files.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "need standard API to set dfs replication = high - There should be a standard way to indicate that files should be highly replicated, appropriate for files that all nodes will read. This should be settable both on file creation and for already-existing files."}, {"q": "What updates or decisions were made in the discussion?", "a": "There are two different issues here, I think\n1) asking for a replication hint.\nThis is what highReplicationHint() is for.\nWe can provide a parameter (strategy) making it look like\n    short highReplicationHint( enum strategy )\nThis one is easy since it just returns a number based on the number of registered data nodes.\nBut it works for the problem of choosing replication for submitted job files (.jar, .xml).\n2) dynamically maintaining replication of a file based on a chosen strategy and cluster size.\nThis is hard. \nDo we really need that now? \nAre submitted job files removed after the job is done?"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-172", "title": "rpc doesn't handle returning null for a String[]", "status": "Closed", "priority": "Blocker", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-04-28T00:15:15.000+0000", "updated": "2006-08-03T17:46:38.000+0000", "description": "The job tracker gets errors in returning the result from pollForTaskWithClosedJob\n\n060427 100434 Served: pollForTaskWithClosedJob 0\ndeclaredClass = [Ljava.lang.String;\ninstance class = org.apache.hadoop.io.NullWritable\n        at org.apache.hadoop.io.ObjectWritable.writeObject(ObjectWritable.java:95)\n        at org.apache.hadoop.io.ObjectWritable.write(ObjectWritable.java:65)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:230)\n", "comments": ["Doug fixed this this morning."], "derived": {"summary": "The job tracker gets errors in returning the result from pollForTaskWithClosedJob\n\n060427 100434 Served: pollForTaskWithClosedJob 0\ndeclaredClass = [Ljava. lang.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "rpc doesn't handle returning null for a String[] - The job tracker gets errors in returning the result from pollForTaskWithClosedJob\n\n060427 100434 Served: pollForTaskWithClosedJob 0\ndeclaredClass = [Ljava. lang."}, {"q": "What updates or decisions were made in the discussion?", "a": "Doug fixed this this morning."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-173", "title": "optimize allocation of tasks w/ local data", "status": "Closed", "priority": "Major", "reporter": "Doug Cutting", "assignee": "Doug Cutting", "labels": [], "created": "2006-04-28T03:58:09.000+0000", "updated": "2009-07-08T16:51:45.000+0000", "description": "When a job first starts, all task trackers ask the job tracker for jobs at once.  With lots of task trackers, the job tracker gets very slow.  The first type of task that the job tracker attempts to find is one with some of its input data stored on the same node as the task tracker.  This case currently loops through tasks blindly, which, on average, requires numHosts/(replication*2) iterations to find a match (I think).  This could be optimized by adding a table mapping from host to task.\n", "comments": ["This patch optimizes the jobtracker's allocation of tasks to nodes that have local data.  I have tested it, but not yet on a large cluster.", "I committed this."], "derived": {"summary": "When a job first starts, all task trackers ask the job tracker for jobs at once. With lots of task trackers, the job tracker gets very slow.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "optimize allocation of tasks w/ local data - When a job first starts, all task trackers ask the job tracker for jobs at once. With lots of task trackers, the job tracker gets very slow."}, {"q": "What updates or decisions were made in the discussion?", "a": "I committed this."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-174", "title": "jobclient kills job for one timeout", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-04-29T04:01:14.000+0000", "updated": "2009-07-08T16:51:45.000+0000", "description": "The launching application (via JobClient) checks the status once a second. If any timeouts or errors occur, the user's job is killed. ", "comments": ["This patch puts a retry loop in to the JobClient.\n\nIt also changes the type of the exception thrown for rpc timeouts to SocketTimeoutException.", "I just committed this.  Thanks, Owen!"], "derived": {"summary": "The launching application (via JobClient) checks the status once a second. If any timeouts or errors occur, the user's job is killed.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "jobclient kills job for one timeout - The launching application (via JobClient) checks the status once a second. If any timeouts or errors occur, the user's job is killed."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-175", "title": "Utilities for reading SequenceFile and MapFile", "status": "Closed", "priority": "Minor", "reporter": "Andrzej Bialecki", "assignee": "Owen O'Malley", "labels": [], "created": "2006-04-29T04:57:24.000+0000", "updated": "2009-04-23T19:24:57.000+0000", "description": "Most data in Hadoop is stored in SequenceFile-s and MapFile-s. Sometimes there is a need to examine such files, but no specialized utilities exist ro read them.\n\nThese two classes provide a functionality to examine individual records in such files, and also to dump the content of such files to a plain text output.", "comments": ["+1\n\nInstead of putting these in in util, why not have them as the main() for MapFile and SequenceFile?\n\nAlso, it might be good to integrate these into bin/hadoop.", "Ok, I'll rework the patch along these lines.", "Also, we could add a generic dumper that sniffs the magic number of a file and dumps it accordingly.  If it's a file that begins with {'S', 'E', 'Q' , 3} then it's a sequence file, if its a directory with sequence files named \"index\" and \"data\", then its a map file, if none of the first 100 bytes are less than 32, then its text, etc.", "Actually, I may have misled you by the poor choice of class names ... come to think of that, the subject is not too clear either. These two utilities started as readers of SequenceFile and MapFile, but now they read SequenceFileOutputFormat and MapFileOutputFormat ... so if anything I think their place is in the main methods of these classes.\n\nI could add similar main() methods to SequenceFile.Reader and MapFile.Reader while I'm here ...", "> I could add similar main() methods to SequenceFile.Reader and MapFile.Reader  ...\n\nYes, please!  Also, the OutputFormat versions could call routines from these, no?  They would mostly be directory iterators, right, but could use the primitive MapFile & SequenceFile to dump records.", "This was fixed by HADOOP-2113."], "derived": {"summary": "Most data in Hadoop is stored in SequenceFile-s and MapFile-s. Sometimes there is a need to examine such files, but no specialized utilities exist ro read them.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Utilities for reading SequenceFile and MapFile - Most data in Hadoop is stored in SequenceFile-s and MapFile-s. Sometimes there is a need to examine such files, but no specialized utilities exist ro read them."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was fixed by HADOOP-2113."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-176", "title": "comparators of integral writable types are not transitive for inequalities", "status": "Closed", "priority": "Major", "reporter": "Dick King", "assignee": null, "labels": [], "created": "2006-04-29T06:25:32.000+0000", "updated": "2006-09-08T21:19:43.000+0000", "description": "Consider the following code from IntWritable.java :\n\n    public int compare(byte[] b1, int s1, int l1,\n                       byte[] b2, int s2, int l2) {\n      int thisValue = readInt(b1, s1);\n      int thatValue = readInt(b2, s2);\n      return thisValue - thatValue;\n    }\n\nIf a Java Runtime subtracts 20 from -(2^31 - 10) it gets a huge positive number, not the negative value that the comparator should return.\n\nLongWritable does this right, of course.\n\nThat last line should be\n\n       return (thisValue<thatValue ? -1 : (thisValue==thatValue ? 0 : 1));\n\n-dk\n", "comments": ["I just committed this.  Thanks, Dick."], "derived": {"summary": "Consider the following code from IntWritable. java :\n\n    public int compare(byte[] b1, int s1, int l1,\n                       byte[] b2, int s2, int l2) {\n      int thisValue = readInt(b1, s1);\n      int thatValue = readInt(b2, s2);\n      return thisValue - thatValue;\n    }\n\nIf a Java Runtime subtracts 20 from -(2^31 - 10) it gets a huge positive number, not the negative value that the comparator should return.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "comparators of integral writable types are not transitive for inequalities - Consider the following code from IntWritable. java :\n\n    public int compare(byte[] b1, int s1, int l1,\n                       byte[] b2, int s2, int l2) {\n      int thisValue = readInt(b1, s1);\n      int thatValue = readInt(b2, s2);\n      return thisValue - thatValue;\n    }\n\nIf a Java Runtime subtracts 20 from -(2^31 - 10) it gets a huge positive number, not the negative value that the comparator should return."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Dick."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-177", "title": "improvement to browse through the map/reduce tasks", "status": "Closed", "priority": "Minor", "reporter": "Mahadev Konar", "assignee": "Mahadev Konar", "labels": [], "created": "2006-05-01T05:16:03.000+0000", "updated": "2009-07-08T16:51:45.000+0000", "description": "The Jobtracker webbapp currently shows all the maps and reduce tasks on a single page. This sometimes causes the browser to crash with 1000's of maps/recudes running.", "comments": ["This patch allows to browse through the map/reduce tasks with a maximum of 2000 tasks been shown on a single page. ", "This is great!  I just committed it.  Thanks!"], "derived": {"summary": "The Jobtracker webbapp currently shows all the maps and reduce tasks on a single page. This sometimes causes the browser to crash with 1000's of maps/recudes running.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "improvement to browse through the map/reduce tasks - The Jobtracker webbapp currently shows all the maps and reduce tasks on a single page. This sometimes causes the browser to crash with 1000's of maps/recudes running."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is great!  I just committed it.  Thanks!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-178", "title": "piggyback block work requests to heartbeats and move block replication/deletion startup delay from datanodes to namenode", "status": "Closed", "priority": "Major", "reporter": "Hairong Kuang", "assignee": "Hairong Kuang", "labels": [], "created": "2006-05-02T01:09:16.000+0000", "updated": "2009-07-08T16:41:53.000+0000", "description": "Currently each datanode sends at least two messages to namenode within a heartbeat interval. One is a heartbeat message and another is block work request. By piggybacking the block work request to a heartbeat can greatly cut the number of messages between a datanode and the namenode.\n\nSecondly each datanode waits for a configurable \"StartupPeriod\" before it sends a block work request in order to avoid uneccessary block replication at startup time. But if the namenode starts much later than datanodes, this scheme does not work. Furthermore, the namenode has more information to decide when to send block work to datanodes. For example, all datanodes send block reports etc. It is more resonable to move the startup delay from datanodes to the namenode ", "comments": ["I made the changes described in the issue report. In addition, I made the granularity of locking on receivedBlockList to be smaller, i.e. the code synchronizes on receivedBlockList only when reading/writing to the list. Also there seemed to be a bug on line 174 in the patch when calculating waittime. So I changed \"now\" to be System.currentTimeMillis().", "I just committed this.  Thanks, Hairong!"], "derived": {"summary": "Currently each datanode sends at least two messages to namenode within a heartbeat interval. One is a heartbeat message and another is block work request.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "piggyback block work requests to heartbeats and move block replication/deletion startup delay from datanodes to namenode - Currently each datanode sends at least two messages to namenode within a heartbeat interval. One is a heartbeat message and another is block work request."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Hairong!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-179", "title": "task tracker ghosts remain after 10 minutes", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-05-02T01:40:36.000+0000", "updated": "2009-07-08T16:51:45.000+0000", "description": "I had a bunch of TaskTrackers time out because they were in the middle of job cleanup and the JobTracker restarted them by responding to emitHeartbeat with UNKNOWN_TASKTRACKER. Afterwards, I ended up with both the new and the restarted TaskTrackers on the list:\n\nnode1100_1234, 0-10 seconds since heartbeat\nnode1100_4321,  >10,000 seconds since heartbeat", "comments": ["This bug is fixed in the most part by my patch to HADOOP-180, which among other things made the task trackers choose names for themselves based on the host and port number. Because when task trackers are reinitialized for missing a heartbeat, they will almost always get the same name, they will not leave behind \"ghosts\"."], "derived": {"summary": "I had a bunch of TaskTrackers time out because they were in the middle of job cleanup and the JobTracker restarted them by responding to emitHeartbeat with UNKNOWN_TASKTRACKER. Afterwards, I ended up with both the new and the restarted TaskTrackers on the list:\n\nnode1100_1234, 0-10 seconds since heartbeat\nnode1100_4321,  >10,000 seconds since heartbeat.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "task tracker ghosts remain after 10 minutes - I had a bunch of TaskTrackers time out because they were in the middle of job cleanup and the JobTracker restarted them by responding to emitHeartbeat with UNKNOWN_TASKTRACKER. Afterwards, I ended up with both the new and the restarted TaskTrackers on the list:\n\nnode1100_1234, 0-10 seconds since heartbeat\nnode1100_4321,  >10,000 seconds since heartbeat."}, {"q": "What updates or decisions were made in the discussion?", "a": "This bug is fixed in the most part by my patch to HADOOP-180, which among other things made the task trackers choose names for themselves based on the host and port number. Because when task trackers are reinitialized for missing a heartbeat, they will almost always get the same name, they will not leave behind \"ghosts\"."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-180", "title": "task tracker times out cleaning big job", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-05-02T01:42:42.000+0000", "updated": "2009-07-08T16:51:45.000+0000", "description": "After completing a big job (63,920 maps, 1880 reduces, 188 nodes), lots of the TaskTrackers timed out because the task cleanup is handled by the same thread as the heartbeats.", "comments": ["This patch fixes the timeouts by creating a synchronized queue (we really should go to java 1.5 soon) of tasks that need to be cleaned up and a daemon thread that does it in the background.\n\nIt also fixes some race conditions in the TaskTracker on the tasks variable. (Some references where locking the TaskTracker and others were locking the TaskTracker.TaskInProgress.)\n\nI also changed the rpc logging a little to include both client and server time measurements.", "I just committed this.  Thanks, Owen!"], "derived": {"summary": "After completing a big job (63,920 maps, 1880 reduces, 188 nodes), lots of the TaskTrackers timed out because the task cleanup is handled by the same thread as the heartbeats.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "task tracker times out cleaning big job - After completing a big job (63,920 maps, 1880 reduces, 188 nodes), lots of the TaskTrackers timed out because the task cleanup is handled by the same thread as the heartbeats."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-182", "title": "lost task trackers should not update status of completed jobs", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-05-02T01:48:33.000+0000", "updated": "2009-07-08T16:51:44.000+0000", "description": "When a Task Tracker is lost (by not sending a heartbeat for 10 minutes), the JobTracker marks the tasks that were active on that node as failed. There are two issues:\n   1. No task from a completed or failed job should be modified.\n   2. No reduces should be marked as failed, since their output is in dfs and therefore not only on the dead node.", "comments": ["Note that marking completed reduces of running jobs as failed is _really_ bad because if the file already exists, I believe the tip will fail, thereby killing the job.", "This patch fixes:\n  1. subtracting the progress for failed tasks\n  2. completed/failedTask methods now update the status\n  3. only fail maps and reduces that are currently running\n  4. remove a few unused imports and a method"], "derived": {"summary": "When a Task Tracker is lost (by not sending a heartbeat for 10 minutes), the JobTracker marks the tasks that were active on that node as failed. There are two issues:\n   1.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "lost task trackers should not update status of completed jobs - When a Task Tracker is lost (by not sending a heartbeat for 10 minutes), the JobTracker marks the tasks that were active on that node as failed. There are two issues:\n   1."}, {"q": "What updates or decisions were made in the discussion?", "a": "This patch fixes:\n  1. subtracting the progress for failed tasks\n  2. completed/failedTask methods now update the status\n  3. only fail maps and reduces that are currently running\n  4. remove a few unused imports and a method"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-183", "title": "adjust file replication factor when loading image and edits according to replication.min and replication.max", "status": "Closed", "priority": "Minor", "reporter": "Hairong Kuang", "assignee": "Hairong Kuang", "labels": [], "created": "2006-05-02T02:11:55.000+0000", "updated": "2009-07-08T16:41:52.000+0000", "description": "Currently in dfs, when namenode starts, a file's replication factor is loaded either from image or edits. The replication factor may be smaller than replication.min or greater than replication.max.", "comments": ["The patch adjusts a file's replication factor when loaded from dfs image or edits. If the replication factor is smaller than replication.min, it is set to be replication.min. If it is bigger than replication.max, it is set to be replication.max.", "I just committed this.  Thanks, Hairong!"], "derived": {"summary": "Currently in dfs, when namenode starts, a file's replication factor is loaded either from image or edits. The replication factor may be smaller than replication.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "adjust file replication factor when loading image and edits according to replication.min and replication.max - Currently in dfs, when namenode starts, a file's replication factor is loaded either from image or edits. The replication factor may be smaller than replication."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Hairong!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-184", "title": "hadoop nightly build and regression test on a cluster", "status": "Closed", "priority": "Minor", "reporter": "Mahadev Konar", "assignee": "Mahadev Konar", "labels": [], "created": "2006-05-02T04:38:42.000+0000", "updated": "2006-08-03T17:46:39.000+0000", "description": "create a jar file for the tests and have  filesystem and mapreduce tests on the cluster", "comments": ["filtered out owen's code for Example Driver to util ProgramDriver.java. Now both the AlltestDriver.java and ExampleDriver.java use this class. Changed the build file to generate a jar file for the test's as well. Changed the MapredLoadTest.java to make it a junit test and run it on a cluster as well.\nSo the tests can be run as \nbin/hadoop jar hadoop-0.2-test.jar  mapredtest args\nbin/hadoop jar hadoop-0.2-test.jar testfilesystem args\non a cluster.", "I'm having trouble applying this patch:\n\n% patch -p 0 < ~/Desktop/nightly_build.patch patching file src/test/org/apache/hadoop/test/AllTestDriver.java patching file src/test/org/apache/hadoop/fs/TestFileSystem.java\npatching file src/test/org/apache/hadoop/mapred/TestMapRed.java\npatching file src/java/org/apache/hadoop/util/ProgramDriver.java\npatching file src/examples/org/apache/hadoop/examples/ExampleDriver.java\npatch: **** malformed patch at line 810: Index: build.xml\n", "attaching the updated patch. should work now..\n", "I just committed this.  Thanks, Mahadev.", "My patch broke ant package.. It did not do a compile-test before. It does it now. The hadoop-test.jar is made available with the package now.", "Thanks.  I committed this."], "derived": {"summary": "create a jar file for the tests and have  filesystem and mapreduce tests on the cluster.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "hadoop nightly build and regression test on a cluster - create a jar file for the tests and have  filesystem and mapreduce tests on the cluster."}, {"q": "What updates or decisions were made in the discussion?", "a": "Thanks.  I committed this."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-185", "title": "tasks are lost during pollForNewTask", "status": "Closed", "priority": "Critical", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-05-02T10:57:38.000+0000", "updated": "2009-07-08T16:51:44.000+0000", "description": "There is the potential for \"losing\" tasks that are assigned by the JobTracker to a TaskTracker, but that fail during returning the result (usually due to a RPC timeout). In this case, the Job becomes \"wedged\" in that the tasks will never run and never time out.", "comments": ["This patch adds a new thread to the JobTracker that every three minutes checks the list of launched tasks that haven't been reported yet. If any of them are older than 10 minutes, they are marked as failed.", "Might it be cleaner to make the launchingTasks field private to ExpireLaunchingTasks, and have access to it through syncronized methods?  That would make it much easier to assure oneself that it is always accessed in a thread safe manner, as access would be more localized, no?", "Ok, this patch pulls the set of launching tasks into the runnable object that checks for expired tasks.", "I just committed this.  Thanks, Owen!"], "derived": {"summary": "There is the potential for \"losing\" tasks that are assigned by the JobTracker to a TaskTracker, but that fail during returning the result (usually due to a RPC timeout). In this case, the Job becomes \"wedged\" in that the tasks will never run and never time out.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "tasks are lost during pollForNewTask - There is the potential for \"losing\" tasks that are assigned by the JobTracker to a TaskTracker, but that fail during returning the result (usually due to a RPC timeout). In this case, the Job becomes \"wedged\" in that the tasks will never run and never time out."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-186", "title": "communication problems in the task tracker cause long latency", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-05-02T11:21:01.000+0000", "updated": "2009-07-08T16:51:44.000+0000", "description": "The Task Tracker's offerService loop has no protection from exceptions, so that any communication problems with the Job Tracker, such as RPC timeouts, cause the TaskTracker to sleep 5 seconds and start again at the top of the loop. ", "comments": ["This patch adds catches around each of the routines that may throw in the offerService loop, with the exception of the emitHeartbeat. (I left one of them unguarded so that if the connection was down, it will get back to TaskTracker.run().) It also moves the time of the previous heartbeat to the top of the cycle rather than the bottom, which means that under load the task tracker stays much closer to a 10 second cycle than before.", "I should add that there are other, as yet undiscovered sources of long latency in the TaskTracker. We've seen pings and progress calls that take 5 minutes to complete. This patch does not solve that problem at all.", "I just committed this.  Thanks, Owen!"], "derived": {"summary": "The Task Tracker's offerService loop has no protection from exceptions, so that any communication problems with the Job Tracker, such as RPC timeouts, cause the TaskTracker to sleep 5 seconds and start again at the top of the loop.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "communication problems in the task tracker cause long latency - The Task Tracker's offerService loop has no protection from exceptions, so that any communication problems with the Job Tracker, such as RPC timeouts, cause the TaskTracker to sleep 5 seconds and start again at the top of the loop."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-187", "title": "simple distributed dfs random data writer & sort example applications", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-05-02T12:30:38.000+0000", "updated": "2009-07-08T16:51:44.000+0000", "description": "These are the examples/benchmark programs that I've been using to test Hadoop map/reduce with. The first is a program that runs 10 maps/node and each map writes 1 gig of random data to a dfs file as a SequenceFile of BytesWritable/BytesWritable.\n\nThe second uses the identity map and reduce to sort the data and write it out to dfs.", "comments": ["I just committed this.  Thanks, Owen."], "derived": {"summary": "These are the examples/benchmark programs that I've been using to test Hadoop map/reduce with. The first is a program that runs 10 maps/node and each map writes 1 gig of random data to a dfs file as a SequenceFile of BytesWritable/BytesWritable.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "simple distributed dfs random data writer & sort example applications - These are the examples/benchmark programs that I've been using to test Hadoop map/reduce with. The first is a program that runs 10 maps/node and each map writes 1 gig of random data to a dfs file as a SequenceFile of BytesWritable/BytesWritable."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-188", "title": "more unprotected RPC calls in JobClient.runJob allow loss of job due to timeout", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-05-02T23:16:03.000+0000", "updated": "2009-07-08T16:51:45.000+0000", "description": "I fixed one of the RPC calls in JobClient.runJob, but I missed a couple of others.", "comments": ["This patch puts the try block around the entire body of the loop, so that if any IOExceptions are thrown in the runJob loop, the user gets a log message and a few retries before their job is killed.", "I just committed this.  Thanks, Owen."], "derived": {"summary": "I fixed one of the RPC calls in JobClient. runJob, but I missed a couple of others.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "more unprotected RPC calls in JobClient.runJob allow loss of job due to timeout - I fixed one of the RPC calls in JobClient. runJob, but I missed a couple of others."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-189", "title": "Add job jar lib, classes, etc. to CLASSPATH when in standalone mode", "status": "Closed", "priority": "Major", "reporter": "Michael Stack", "assignee": "Doug Cutting", "labels": [], "created": "2006-05-03T00:44:04.000+0000", "updated": "2009-07-08T16:51:45.000+0000", "description": "Currently, in standalone mode,  hadoop is unable to launch other than the most basic of job jars where 'basic' is a job jar with nought but class files at top level of the jar with Main-Class pointing at entry point.  If the job jar has dependencies on jars under the job jar lib or there are job jar plugins in the classes dir, etc.,  these dependencies are not loaded and the job fails launch.", "comments": ["Tell me if this fixes things for you.  Thanks!", "The patch seems to have no effect.  When my map goes to run, unless the classes I want to instantiate are in the root of the job jar, they are not found (ClassNotFoundException).  Nothing under the job jar /lib nor /classes directory is ever found.  \n\nTracing, I see that any new class will have null for ClassLoader -- i.e. the \"bootstrap class loader\" -- never the URLClassLoader created by this patch. \n\nSeems like this patch runs too late in the game to be of use?   In order to create a job, I need to be able to refer to the implementing class (and possibly configurations) BEFORE LocalJobRunner has had a chance to do its job jar unrolling machinations.  To be able to refer to implementing classes, they need to be on the immediate CLASSPATH.  \n\nSeems like any messings w/ classloading populating the CLASSPATH needs to happen before we run Main-Class. This would be most useful I'd imagine.  Would a java program that did nothing but read hadoop config. for hadoop local and system dir and then unrolled the job jar finishing by emitting Main-Class and CLASSPATH additions for the hadoop script to exec online #156 be too ugly?  It would have the advantage of resemblng how job jars are unrolled out on slaves.\n\nI can work up such a patch if amenable.\n\n", "> Tracing, I see that any new class will have null for ClassLoader\n\nFor this patch to work your, e.g., Mapper class needs to only be in the job jar, not also on the classpath built by bin/hadoop.  If you are able to instantiate classes in the job jar, and they show up with the bootstrap class loader, then that means the jar file is somehow on the bootstrap classpath, which it should not be after the changes to 'bin/hadoop jar' made in the patch.  Is that how you are starting things?\n\nAnother potential problem could be that the URLClassloader is not correctly constructed.  The urls added must be either a jar file or end in slashes.  It might be useful to log the urls added to the path in LocalJobRunner to check this.", "Here's a patch I've actually tested!", "Ok.  This patch works really well (Looks to be spin on unbundling before invocation of Main-Class).  Thanks for all your work on this.", "I just committed this.", "I just reverted this, since it broke distributed operation.", "Okay, this time it works in both standalone and distributed modes."], "derived": {"summary": "Currently, in standalone mode,  hadoop is unable to launch other than the most basic of job jars where 'basic' is a job jar with nought but class files at top level of the jar with Main-Class pointing at entry point. If the job jar has dependencies on jars under the job jar lib or there are job jar plugins in the classes dir, etc.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Add job jar lib, classes, etc. to CLASSPATH when in standalone mode - Currently, in standalone mode,  hadoop is unable to launch other than the most basic of job jars where 'basic' is a job jar with nought but class files at top level of the jar with Main-Class pointing at entry point. If the job jar has dependencies on jars under the job jar lib or there are job jar plugins in the classes dir, etc."}, {"q": "What updates or decisions were made in the discussion?", "a": "Okay, this time it works in both standalone and distributed modes."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-190", "title": "Job fails though task succeeded if we fail to exit", "status": "Closed", "priority": "Major", "reporter": "Michael Stack", "assignee": null, "labels": [], "created": "2006-05-03T04:28:56.000+0000", "updated": "2006-08-03T17:46:39.000+0000", "description": "This is an odd case.  Main cause will be programmer error but I suppose it could happen during normal processing. Whichever, would be grand if hadoop was better able to deal.\n\nMy map task completed 'successfully' but because I had started threads inside in my task that were not set to be of daemon type that under certain circumstances were left running,  my child stuck around after reporting 'done' -- the JVM wouldn't go down while non-daemon threads still running.  After ten minutes, TT steps in,  kills the child and does cleanup of the successful output.  Because JT has been told the task completed successfully, reducers keep showing up looking for the output now removed -- until the job fails.\n\nBelow is illustration of the problem using log output:\n\n....\n060501 090401 task_0001_m_000798_0 0.99491096% adding http://www.score.umd.edu/a\num.jpg 24891 image/jpeg\n060501 090401 task_0001_m_000798_0 1.0% adding http://www.score.umd.edu/album.jp\n24891 image/jpeg\n060501 090401 Task task_0001_m_000798_0 is done.\n...\n060501 091410 task_0001_m_000798_0: Task failed to report status for 608 seconds\nKilling.\n....\n060501 091410 Calling cleanup because was killed or FAILED task_0001_m_000798_0\n060501 091410 task_0001_m_000798_0 done; removing files.\n\nThen, subsequently....\n\n060501 091422 SEVERE Can't open map output:/1/hadoop/tmp/task_0001_m_000798_0/pa\n-12.out\njava.io.FileNotFoundException: LocalFS\n...\n\nand on and on.", "comments": ["Here's a suggested patch.  If task has been marked 'done', don't remove the output (I haven't tested this patch -- the condition is awkward to manufacture).", "I applied this patch.  Thanks, Michael."], "derived": {"summary": "This is an odd case. Main cause will be programmer error but I suppose it could happen during normal processing.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Job fails though task succeeded if we fail to exit - This is an odd case. Main cause will be programmer error but I suppose it could happen during normal processing."}, {"q": "What updates or decisions were made in the discussion?", "a": "I applied this patch.  Thanks, Michael."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-191", "title": "add hadoopStreaming to src/contrib", "status": "Closed", "priority": "Major", "reporter": "Michel Tourn", "assignee": "Doug Cutting", "labels": [], "created": "2006-05-03T05:10:00.000+0000", "updated": "2006-08-03T17:46:40.000+0000", "description": "This is a patch that adds a src/contrib/hadoopStreaming directory to the source tree.\nhadoopStreaming is a bridge to run non-Java code as Map/Reduce tasks.\nThe unit test TestStreaming runs the Unix tools tr (as Map) and uniq (as Reduce)\n\n\nTO test the patch: \nMerge the patch. \nThe only existing file that is modified is trunk/build.xml\ntrunk>ant deploy-contrib\ntrunk>bin/hadoopStreaming : should show usage message\ntrunk>ant test-contrib    : should run one test successfully\n\nTO add src/contrib/someOtherProject:\nedit src/contrib/build.xml\n\n\n\n", "comments": ["The usage message:\n\nhadoop-trunk>bin/hadoopStreaming\n\nUsage: hadoopStreaming [options]\nOptions:\n  -input   <path>     DFS input file(s) for the Map step\n  -output  <path>     DFS output directory for the Reduce step\n  -mapper  <cmd>      The streaming command to run\n  -reducer <cmd>      The streaming command to run\n  -files   <file>     Additional files to be shipped in the Job jar file\n  -cluster <name>     Default uses hadoop-default.xml and hadoop-site.xml\n  -config  <file>     Optional. One or more paths to xml config files\n  -inputreader <spec> Optional. See below\n  -verbose\n\nIn -input: globbing on <path> is supported and can have multiple -input\nDefault Map input format: a line is a record in UTF-8\n  the key part ends at first TAB, the rest of the line is the value\nCustom Map input format: -inputreader package.MyRecordReader,n=v,n=v\n  comma-separated name-values can be specified to configure the InputFormat\n  Ex: -inputreader 'StreamXmlRecordReader,begin=<doc>,end=</doc>'\nMap output format, reduce input/output format:\n  Format defined by what mapper command outputs. Line-oriented\nMapper and Reducer <cmd> syntax:\n  If the mapper or reducer programs are prefixed with noship: then\n  the paths are assumed to be valid absolute paths on the task tracker machines\n  and are NOT packaged with the Job jar file.\nUse -cluster <name> to switch between \"local\" Hadoop and one or more remote\n  Hadoop clusters.\n  The default is to use the normal hadoop-default.xml and hadoop-site.xml\n  Else configuration will use $HADOOP_HOME/conf/hadoop-<name>.xml\n\nExample: hadoopStreaming -mapper \"noship:/usr/local/bin/perl5 filter.pl\"\n           -files /local/filter.pl -input \"/logs/0604*/*\" [...]\n  Ships a script, invokes the non-shipped perl interpreter\n  Shipped files go to the working directory so filter.pl is found by perl\n  Input files are all the daily logs for days in month 2006-04\n", "Most of the changes to the top-level build.xml don't seem to be required, and a number are spurious whitespace and comment changes.  It seems to build fine with only the new targets added.\n\nAlso, is the new bin/ script required?  Won't 'bin/hadoop jar build/hadoop-streaming.jar ...' suffice?  (You'll need to set the \"Main-Class\" attribute in the jar's manifest.)\n", ">top-level build.xml : \nOK, I can remove the unnecessary changes.\nWhich contrib targets would you keep in?\nI mimicked deploy-contrib, test-contrib, clean-contrib on Nutch plugins.\n(It is true that for now the new targets are not required since the nightly target does not call them.)\n\n>bin/hadoop jar build/hadoop-streaming.jar ...\nLooks cleaner. I'll try to do it this way\n\n", "I'm okay with all the contrib targets.  But all of the other changes to that file seem spurious.  The new properties are unused and the directory created will be created by another build script anyway.", "Updated patch:\n\n1. top-level build.xml has 3 contrib targets and no other changes.\n\n2. script hadoopStreaming is gone. \nnew Usage message: \nbin/hadoop jar build/hadoop-streaming.jar [options]\n\n3. removed some spurious exec permissions on  source files\n", "I just committed this.  It looks great!  Thanks, Michel.", "An update to hadoop-streaming.\n\n", "This patch depends on the LargeUTF8 patch:  http://issues.apache.org/jira/browse/HADOOP-136\n\n\nAdded a few more configurable options.\n\nmichel@cdev2004> bin/hadoop jar build/hadoop-streaming.jar -info\nUsage: $HADOOP_HOME/bin/hadoop jar build/hadoop-streaming.jar [options]\nOptions:\n  -input    <path>     DFS input file(s) for the Map step\n  -output   <path>     DFS output directory for the Reduce step\n  -mapper   <cmd>      The streaming command to run\n  -combiner <cmd>      Not implemented. But you can pipe the mapper output\n  -reducer  <cmd>      The streaming command to run\n  -file     <file>     File/dir to be shipped in the Job jar file\n  -cluster  <name>     Default uses hadoop-default.xml and hadoop-site.xml\n  -config   <file>     Optional. One or more paths to xml config files\n  -dfs      <h:p>      Optional. Override DFS configuration\n  -jt       <h:p>      Optional. Override JobTracker configuration\n  -inputreader <spec>  Optional.\n  -jobconf  <n>=<v>    Optional.\n  -cmdenv   <n>=<v>    Optional. Pass env.var to streaming commands\n  -verbose\n\nFor more details about these options:\nUse $HADOOP_HOME/bin/hadoop jar build/hadoop-streaming.jar -info\n"], "derived": {"summary": "This is a patch that adds a src/contrib/hadoopStreaming directory to the source tree. hadoopStreaming is a bridge to run non-Java code as Map/Reduce tasks.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "add hadoopStreaming to src/contrib - This is a patch that adds a src/contrib/hadoopStreaming directory to the source tree. hadoopStreaming is a bridge to run non-Java code as Map/Reduce tasks."}, {"q": "What updates or decisions were made in the discussion?", "a": "This patch depends on the LargeUTF8 patch:  http://issues.apache.org/jira/browse/HADOOP-136\n\n\nAdded a few more configurable options.\n\nmichel@cdev2004> bin/hadoop jar build/hadoop-streaming.jar -info\nUsage: $HADOOP_HOME/bin/hadoop jar build/hadoop-streaming.jar [options]\nOptions:\n  -input    <path>     DFS input file(s) for the Map step\n  -output   <path>     DFS output directory for the Reduce step\n  -mapper   <cmd>      The streaming command to run\n  -combiner <cmd>      Not implemented. But you can pipe the mapper output\n  -reducer  <cmd>      The streaming command to run\n  -file     <file>     File/dir to be shipped in the Job jar file\n  -cluster  <name>     Default uses hadoop-default.xml and hadoop-site.xml\n  -config   <file>     Optional. One or more paths to xml config files\n  -dfs      <h:p>      Optional. Override DFS configuration\n  -jt       <h:p>      Optional. Override JobTracker configuration\n  -inputreader <spec>  Optional.\n  -jobconf  <n>=<v>    Optional.\n  -cmdenv   <n>=<v>    Optional. Pass env.var to streaming commands\n  -verbose\n\nFor more details about these options:\nUse $HADOOP_HOME/bin/hadoop jar build/hadoop-streaming.jar -info"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-192", "title": "Trivial JRE 1.5 versus 1.4 bug", "status": "Closed", "priority": "Blocker", "reporter": "David Bowen", "assignee": null, "labels": [], "created": "2006-05-04T00:23:46.000+0000", "updated": "2009-07-08T16:51:46.000+0000", "description": "Long.valueOf(long) is in 1.5 but not in 1.4.x.  Use new Long(long) instead.", "comments": ["Here's the fix.", "I just committed this.  Thanks, David!"], "derived": {"summary": "Long. valueOf(long) is in 1.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Trivial JRE 1.5 versus 1.4 bug - Long. valueOf(long) is in 1."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, David!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-193", "title": "DFS i/o benchmark.", "status": "Closed", "priority": "Major", "reporter": "Konstantin Shvachko", "assignee": "Konstantin Shvachko", "labels": [], "created": "2006-05-04T08:57:17.000+0000", "updated": "2017-08-10T18:20:29.000+0000", "description": "DFS i/o benchmark is a map-reduce based test that measures performance of the cluster for reads and writes.\nThis is an evolved version of HADOOP-72, and HADOOP-95 test.\n\nThis test writes into or reads from a specified number of files.\nFile size is specified as a parameter to the test.\nEach file is processed in a separate map task.\nThe unique reducer then collects stats.\nFinally, the following information is displayed\n\n# read or write test\n# date and time the test finished\n# number of files processed\n# total number of bytes processed\n# throughput in mb/sec (total number of bytes / sum of processing times)\n# average i/o rate in mb/sec per file\n# standard i/o rate deviation\n\nI  included the test into the AllTestDriver.", "comments": ["I just committed this.  Thanks, Konstantin."], "derived": {"summary": "DFS i/o benchmark is a map-reduce based test that measures performance of the cluster for reads and writes. This is an evolved version of HADOOP-72, and HADOOP-95 test.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "DFS i/o benchmark. - DFS i/o benchmark is a map-reduce based test that measures performance of the cluster for reads and writes. This is an evolved version of HADOOP-72, and HADOOP-95 test."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Konstantin."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-194", "title": "Distributed checkup of the file system consistency.", "status": "Closed", "priority": "Major", "reporter": "Konstantin Shvachko", "assignee": "Konstantin Shvachko", "labels": [], "created": "2006-05-04T09:11:48.000+0000", "updated": "2006-08-03T17:46:40.000+0000", "description": "This is a map-reduce based test that checks consistency of the file system\nby  reading all blocks of all files, and detecting which of them are missing or corrupted.\nSee HADOOP-95 and HADOOP-101 for related discussions.\n\nThis could be an alternative to the sequential checkup in dfsck.\nIt would be nice to integrate distributed checkup with dfsck, but I don't yet see how.\n\nThis test reuses classes defined in HADOOP-193.\n", "comments": ["Should be applied after HADOOP-193.", "I just committed this.  Thanks, Konstantin."], "derived": {"summary": "This is a map-reduce based test that checks consistency of the file system\nby  reading all blocks of all files, and detecting which of them are missing or corrupted. See HADOOP-95 and HADOOP-101 for related discussions.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Distributed checkup of the file system consistency. - This is a map-reduce based test that checks consistency of the file system\nby  reading all blocks of all files, and detecting which of them are missing or corrupted. See HADOOP-95 and HADOOP-101 for related discussions."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Konstantin."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-195", "title": "improve performance of map output transfers", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-05-05T00:50:50.000+0000", "updated": "2009-07-08T16:51:45.000+0000", "description": "The data transfer of the map output should be transfered via http instead rpc, because rpc is very slow for this application and the timeout behavior is suboptimal. (server sends data and client ignores it because it took more than 10 seconds to be received.)", "comments": ["The RPC mechanism is designed to be able to support large values.  Timeouts are for inactivity on a request, not for completion.  So perhaps there is just a bug in the RPC mechanism that can be fixed.  Or perhaps this really is an inappropriate use of it.  I'd like to hear more about the failures you're seeing before we decide.", "\nI've had problems with timeouts during the copy phase, but its not clear to me that the problem is with the RPC mechanism. It may be TCP, in which case going to HTTP wouldnt help. \n\nSince the copy phase is the moment when the switch is busiest, my recommendation is to saturate your switch with traffic, and experiment with various transfer methods, like HTTP, etc.\n\nI did tests to saturate our (cheap) switch with traffic from the nodes and measure packet loss. Though all the nodes were sending data at near a gigabit, the packet loss rate ranged from 0.5% to 2%, sometimes peaking at 5%. \n\nTCP behaves badly in such an environment, even with low latency. This is probably an opportunity to retune TCP, but ever since our workaround this problem has gone to the back burner.\n\nWe worked around the problem by dramatically increasing the number of mappers, which reduces the size of the map output file. For probablistic problems like this, the proobability of failure increases with the duration of the transfer, and when files reach a certain size the failure is likely to happen on every transfer, and no forward progress can be made.\n\nThe problem is an open question. I spent a lot of time trying to debug the RPC mechanisms, and I wasnt able to find a clear culprit.\n", "Looking at the logs of my sort benchmark on 188 nodes, each reduce is fetching and processing 1 gig of data from ~64k maps. After all of the maps are done, a reduce takes 8 hours to run. 7 of those hours are in fetching the map outputs.\n\nTiming of calls to getFile that complete (average ~15k bytes):\n   Max: 76 seconds\n   Avg: 385 ms\n   Mean: 45 ms\n   Distribution (count, int(log_2(ms))):\n     149 0\n   41449 1\n   96305 2\n  101595 3\n13060775 4\n71232197 5\n 9675008 6\n 4569403 7\n 5688185 8\n 5196811 9\n 3878971 10\n 4267733 11\n 1855209 12\n  411456 13\n   70594 14\n   24182 15\n       1 16\n\nTimeouts from getFile: 29120\n\nSo the reduce prepare is being dominated by calls to getFile (64k * 385 = 6.8 hours).\n\nFor a first pass, I'll try increasing the number of threads serving data to 20 (from 2) and try the parallel rpc call to fetch 5 files at a time.\n\nThoughts?", "So you're seeing nearly a 50% timeout rate.  That's not good.  It would be nice to know more about how these occur.  For example, if they're occurring because the client connection  to that host is busy, then increasing the number of serving threads alone won't help.  You'd also need to disable or enhance connection pooling on the client.  Parallel RPC calls will probably increase the timeout rate, but might still improve things, since (hopefully) one of the requests will find an available server and keep the clients network interface busy.\n\nDisabling pooling for these calls should not be hard.  Add an option to RPC.getProxy() to disable connection pooling, and store this as a new field in the Invoker.  Then add an option to Client.call() that it passes to getConnection() to bypass the connection cache.  Then, in a finally clause of Client.call(), close the connection when noCacheConnections was passed.  You could even bypass the connection's thread and simply read the response directly under Client.call(), since there will be no other calls multiplexed over the connection.\n\nEnhancing pooling might be better yet.  One could, e.g., remove a connection from the pool while a request is outstanding, and then add it back into the pool only if there were no io errors, if there's not already a pooled connection to that host, and perhaps if the pool's not too big.  So each call could have a dedicated connection.\n\nThe ability to multiplex requests and responses over a single connection is still probably an important feature for, e.g., Nutch's distributed search, where you might have a hundred clients all broadcasting queries to a hundred backends.  In this case you don't want to have 10,000 connections, but rather just 100, and you don't want to force fast queries to wait for slower queries to complete.  So dedicated connections per call should be an option, not mandated.", "No, I'm seeing a 0.01% timeout rate. There are 64k*2k = 128m getFile calls. Losing 29k of them is sub-optimal, but not a serious drain.", "In that case then, perhaps increasing the the number of serving threads to N*2*tasks_per_node, and using parallel calls to N hosts (where N = 5 to start?) will help throughput, as you suggest.", "\nglad to hear you're working on a sort benchmark for hadoop.\n\ncompare:\n- 1 gigabyte in 7 hours on 188 nodes, to\n- 1 terabyte in 7 minutes on 80 itaniums\n\nhttp://research.microsoft.com/barc/SortBenchmark/\n\ncleaning up the sort path will benefit us all greatly!", "> 1 gigabyte in 7 hours on 188 nodes\n\nNote that the total size of the data sorted in this case is several terabytes.  Owen said 1G per reduce task, and I think he has a few thousand reduce tasks.  (Owen?)  So, you're right, we're not yet down to 7 minutes/terabyte, but things still aren't quite as bad as you state.\n\nGoogle's MapReduce paper reports a terabyte sort time of 891 seconds using a cluster of 1800 dual Xeon nodes.\n\nThe Indy test you cite (TB in 7 minutes on 80 Itaniums) also has fancier switches and disk arrays than Google's cluster or Owen's cluster.  In particular, it's not clear that it would easily scale to sorting 10 terabytes in 7 minutes on 800 Itaniums, since 800-port switches are harder to find.", "The code that I'm testing with is in the examples directory.\n\nrandomwriter generates 1g/map and 10 maps/node for a total of 1880 g of data.\nthe sequence file format splits that into 63920 maps and I use 1880 reduces.\n\n", "\nooops, i was wrong by a few orders of magnitude. \nbut i was also right by a couple orders of magnitude\n\nthe best feature of the map/reduce approach is that you only have to make one thing fast, and every program you write is fast.\n\nand that one thing is sort.\n\nso again,.. im very pleased that we're taking a look at the sort path!\n\nonce the copy phase is fixed, the next step is for the Yahoo guys to contribute David Cossock's sort ;)", "\nI checked with Robert Ramey who held the PennySort record for a while, because the PennySort runs on a single commodity meachine, so we should all feel comfortable with the comparison.\n\nHis sort does about 1GB/minute on an ordinary machine.\n\nSo,  1880GB on 188 machines should be = 10 minutes sort time + ?? copy time (copy should be faster than sort).\n\nIf Owen's example is taking 1 hour sort time plus 7 hours copy time, its apparent that the biggest win is in improving the copy, and the next important task is improving the sort.\n\nDavid Cossock's sort at Yahoo is very fast, so that is one option if Yahoo is interested. \n\nIf  Yahoo isnt interested in contributing that, Robert Ramey may be open to doing consulting work in this area. Once the copy times are improved, we'd be interested in helping sponsor sort performance improvements.\n\n", "Owen,\n \nHere are a few things you can try for your sort performance issues:\n \n(1) DONT NAGLE\n \nNagle's algorithm can add 200ms to a TCP transaction, and could be accounting for a chunk of that average 385 milliseconds you're seeing. I think Nagle is enabled by default. You could try disabling Nagle with setTcpNoDelay on the RPC-server-side socket.\n\nWith 64,000 files, one Nagle per file could waste hours.\n\nNote that by disabling Nagle, you'll get some runt packets (for example, the header and at the boundary of each socket write). This should not be a material performance issue, and you can reduce it arbitrarily by increasing the size of the buffer (right now it's a hardcoded 8KB buffer).\n \nIt would be great Java allowed the use of TCP_CORK and the use of sendfile(), since you could avoid runts and avoid a few buffer copies because sendfile does it all in the kernel. That's what HTTP servers do, which was your original suggestion.\n \nOther TCP tuning could help. My intuition is that these wildly varying transfer times are related to packet loss, and I expect some level of tuning to help this but I haventhad time to look into it. Disabling linger should help when packets are lost in the closing handshake.  I'm also an advocate of much larger buffers, although that isn't an issue for your particular case. Another minor issue (and not causing your particular performance issue) is that we should turn on keepalives with a short retry, since without keepalives TCP does not _always_ detect hung sessions, and the current code that ignores socket timeouts can leave us with a collection of zombie sessions. But of course, that's not an issue for your situation.\n\n(2) BIGGER FILES\n \nModern disks transfer at over 50MB/s, yet seek no faster than the drives used by the ancient sumerians.\n \n15KB takes less than 300 microseconds to transfer to or from a SATA disk. Since a seek takes probably more than 10 milliseconds, you're at least 97% seek bound with a 15KB file.  Its actually worse: how many disk seeks does it take to create a file and then delete it later? (I dont know) \n \nTransferring even 1MB whenever you touch disk is 30-50% seek-bound, because the 20ms it takes to transfer 1MB is in the same ballpark as the seek time of the drive (I get 10-20ms when I measure SATA seek). \n\nYou might as well transfer 4MB or 16MB each time you touch disk, then your code will still work a few years from now.\n \nIncreasing drive transfer speed is due to increasing data density. The more bits per inch the more bits per second at a given rotational speed. Our grandfathers got away with using 4KB and 8KB block sizes in their JCL because data densities were just really low back then. \n \n(3) LESS THRASHING\n \nWe're moving terabytes, yet the sort path writes map output to disk and reads it back in 5 times before the data reaches the reducer:\n \n1 - mapper output, \n2 - copy individual map output files to reducer, \n3 - concatenate little files into big file, \n4 - write merge file when sort buffer is full, \n5 - write merged data to disk.\n \nSteps 3 and 5 are the first opportunity for a speedup because they exist purely for programmer convenience. The sorter can read directly from a list of files instead of one file, and the reducer could call the merger directly to collect the input records instead of storing it to disk as an intermediate step.\n \nStep 2 permits the mapper-to-reducer copy to be restarted if it fails, but strictly speaking that data only needs to go to disk if the sort buffer doesn't have enough space left to hold the entire file (since it would be easy to back out of a failed transfer as long as the sort buffer doesn't fill up). Also, you could just start the sort early when you encountered a file that would fit in the sort buffer, but there's just not enough room left in the sort buffer. \n \nStep 4 only needs to occur when the data is too large to fit into RAM.\n \nWhether or not step 1 needs to occur could be a matter of debate.\n \n(4) BIG BUFFERS IN MERGE PHASE\n \nThe default 4KB buffer size is really bad for the merge phase (see suggestion 2). Since the merger is reading from a bunch of merge files, and from the perspective of the OS, you are performing one read at a time from a randomly-chosen merge file, its unlikely for the OS to figure out that you really want to sequentialize. \n\nIt's a little daredevil to rely on Linux doing 1000-to-1 sequentialization anyway; you might as well just use a big buffer and be done with it -I'm not sure of the benefit of the small buffer.\n \n(5) OTHER IMPROVEMENTS\n \nThe DFS write of the data coming out of the reducer currently writes a local copy of data to disk, but this could be eliminated by using a 32MB RAM buffer instead of a disk file for recoveries from connection failures. \n \nIn general, we'd be better off using NIO instead of stream IO because of fewer buffer copies and better control of buffering and endian-ness. Stream IO does at least two buffer copies of the data, one in the kernel because of use of the disk cache, and at least one in Java.  Yes, I realize that this would be a major rewrite and less important than the others mentioned. \n \nIt would be interesting to calculate the number of buffer copies in the current sort path above considering that you get two buffer copies when you write the disk and two when you read the disk, and you have 5 extra visits to disk plus the network transfer. \n", "Good list paul!\n\nSome very simple changes should have a big impact on sort behavior as you observe. We'll start working on that once it becomes the bottleneck.\n\nOne simple way to increase the file sizes is to reduce the number of reduces significantly and increase the DFS block size to 64 or 128meg.\n\nWe'll play with these (if I can convince owen).  I think we should bump the hadoop default block size to 128m, this is still small enough to replicate quickly, but will reduce #map jobs significantly when you just want to scan data.  Reduce the number of reduces as well and we'll have significantly larger transactions.\n\nAll that said, I think we are probably uncovering things in the RPC layer (and server threading) more than basic network issues, since we're running on a decent network and not even beginning to approach saturating it.  But we'll certainly play with \"setTcpNoDelay.\"  It will be interesting to see if that moves things along.", "Has anybody tried to use the APR (Apache portable runtime) with a JNI wrapper like tomcat? With this wrapper you could use OS features like sendfile, epoll, random number generator and so on. I haven't used it myself, just saw some performance test with JBoss web which is using this.\n\nThis is might bit off topic, but Java NIO has been mentioned before. I've played around with Java NIO some weeks ago to see where it could be usefull in Nutch/hadoop. With my simple tests I found no significant performance improvements in file IO. I guess the tests were just too simple (serializing/deserializing Java objects to/from disk) to give useful results.\n\nI also tested the network throughput with a multiplexed socket compared to the one-thread-per-client design. With NIO the throughput was almost independent from the number of concurrent connections while the threading overhead became very significant with 100+ threads. \n\nMy testbed was a simple server with two IO thread and a few worker thread and bunch of clients that sent messages (serialized Java objects) to the server. On the server side one IO thread read messages from the socket and put them into a blocking queue and the other IO thread read outgoing messages from another blocking queue and sent them. The worker thread pulled messages from the in-queue, work on them (in my test they just copied the message) and put their result on the out-queue. This way the server could handle a few 1000 connections without problem. This design or something similar might be useful for the namenode or distributed search as mentioned before.", "\neric,\n\nmost of my suggestions relate to the copy phase of the sort path, not the sort itself. once that is working, i can make sort suggestions (although my best sort suggestion is for you guys to talk with david cossock about sorts).\n\nthis whole area is critical. on that cluster, owen's 2TB should sort in 10 minutes, and the data should be copied in less than that time, for a total run time of <20 minutes. \n\npleased that yahoo has resources to apply. \n\npaul", "\ndominek,\n\nbuffer copies are nowhere near a bottleneck in hadoop, yet. right now we have lots of wins just from getting our buffering right.\n\nreducing buffer copies only matters when buffer copies are a bottleneck. you would have to use a profiler to see how much time was being spent in your serialization/deserialization code, for example. if your code is the bottleneck, then reducing buffer copies might not matter. how long are your requests? if they are small, its not likely to matter. if they are gigabytes, then it could matter a lot.\n\nother questions about your use of NIO:\n- did you try using the native endian-ness with NIO? or the default? (the default is evil Sun endian-ness)\n- are you using direct buffers, or indirect? (indirect buffers still cost you a buffer copy in user space)\n- are you using memory mapping, or buffered io? (buffered io costs you a buffer copy in kernel space)\n\nof course, an honest-to-god unbuffered read is so much better than memory mapping. someone who is more of a unix guy could help you figure out which linux filesystem supports real unbuffered io, and how to make that happen from java. when you're memory mapped, its hard to coerce the system into doing the multimegabyte double-buffered reads that you really want to do if you are interested in performance. you might have to use JNI to make that io fast. but again, its only worthwhile if you know where the bottlenecks are. windows nt is popular among sort people because its so easy to get an honest-to-god unbuffered io.\n\nbut again, none of that matters if you're not moving much data, or if you dont have a buffer copy bottleneck.\n\nusing the JNI interface you mention sounds interesting. of course, if we're going to go non-pure-java, we might as well use owen's idea of an http server to serve up the map output data, since that server will already be tuned. we're using lighttpd here and getting super good performance (for a different application of course).\n\nim super glad there's an interest in performance here!\n\npaul", "\nOwen,\n\nMike Ovsiannikov had a great suggestion.  Can you check netstat for TIME_WAIT sessions during the slowdown? Maybe the system is running out of sockets (ports), 64,000 connects per reducer is a lot of connections.\n\nIf you are accumulating zillions of TIME_WAIT sessions, you might want to try to experiment with the following:\n\nnet.ipv4.tcp_tw_reuse\nnet.ipv4.tcp_tw_recycle\n\nThat histogram that you have.. can you show that as a time-series? Does it go really fast at first, and then bog down?\n\nIt might also be good to instrument the time it takes to connect versus transfer versus close, etc.\n\nPaul", "paul,\n\nthe file IO test were really pretty simple. To be honest I don't remember my actual setup anymore but only that I saw not much difference between normal stream IO and NIO but the data was by far not gigabytes.\n\nI used default endian-ness, direct buffers and buffered io. From what I read before memory mapping gives almost no performance gain against buffered io for streaming io. It makes if you've random access within a limited region of a file. In both cases you've buffered IO because of the OS's file system buffer. From what I know about OSs there is no copy to kernel space and the file system buffer of a modern OS is hardly to beat performance wise. Unbuffered IO would actually decrease the performance because the file system cannot change the write order and do other tricks to reduce seeks. In general I don't think there is much space for file IO performance improvements in hadoop except using e.g. APR through JNI.\n\nTo improve the sorting performance I'd start by looking at the algorithm itself, because there seem to be better algorithms out there. This huge difference in performance cannot be caused by suboptimal implementation. Bottlenecks are file and network IO so the goal is to reduce those. I haven't used Nutch/hadoop for some time now and so I'm not up to date with the current code. \n\nThis is a really interesting problem, could be a nice project for Google's Summer of Code.", "\ndominek,\n\nif you're not moving gigabytes of data, its unlikely that you need to worry about one or two buffer copies.\n\nif an application does move gigabytes of data, and you have some other use for the CPU while you're moving data, you definitely dont want that data flowing through the file cache because you are incurring a buffer copy and the data volumes are too large to get any benefit from the file cache. \n\nbut it feels like there are a few too many if's for our discussion to be of much importance ;) its definitely a fun area of work!\n\npaul\n\n", "Here are two dumps of \"netstat -ees\" taken ~5 hours apart on one of the worker nodes during reduce.  Let me know if you see anything interesting.", "paul,\n\ni agree, the less you copy the data the better (faster). But moving data in RAM is a few orders of magnitude faster then disk writes (factor 1000-10000 or so). So when optimizing one should start to reduce the slowest operations as much as possible and that are disk IO and network IO. \n\nI had a short look at the sorter implementation in SequenceFile.java. If I understood the code correctly the sorter reads as big chunks as possible from the input file, sort them in-memory and writes the sorted chunk to a temp file. When the whole input file has been sorted to several sorted chunk files one or more merge runs are started that merge those chuck files to one sorted file which has the complete input file sorted in total order. Before you got the final sorted file the mapper writes a temp file, the sorter reads it and writes several sorted temp files and finaly you run one or more merge phases that have to read those sorted chunks and write the final sorted output.\n\nI'd split the whole sorting. The mapper output collector does not have to write one big output file, it could also write several pre sorted chunks like the sort phase in the current sorter does. The merging phase can be done on the fly when transfering the sorted map output to the reducers. This way you only have to write a few sorted chunks (the number depends on the buffer size of the output collector and the temp data size produced by the mapper). This reduces disk IO and also disk space needed for temp data.\n\nAnother possible optimization could be the creation of index files when you've small keys and large values. The collector would dump the values in one big file and put the keys along with the index of the value in sorted index chunks like before. When merging the partial sorted output on the fly while transfering keys and values are joined together again.\n\nI'm not sure if I've the time to implement this and run performance comparisons against the current implementation, but I think this could speed up the sorting quite a lot.", "\nowen,\n\ni took a look at the netstat, and nothing jumped out at me, but i'll show this to a couple other people to see what they think.\n\ni'm posting an excel spreadsheet with the before, after, and delta numbers in columns\n\npaul", "Since there is obviously interest in my benchmark, here is an update:\n\nI reran my sort test yesterday with:\n   1. fewer reduces (2/node) (hadoop-202)\n   2. the map ids replaced with integers (hadoop-200)\n   3. the number of server threads for map output serving set to 20\n\nI sorted 1760 gig of data on 179 nodes in 18.6 hours, which is much better than before.\n\nI had 20 reduce tasks fail and reexecute themselves (last original reduce finished in ~16.5 hours)\n\n2 of those tasks were assigned to the same node and were the only two tasks running for the last hour, which clearly shows that we need speculative reduces.", "btw, here is the timeline:\n\n060508 145202 Running job: job_0002\n060508 145203  map 0%  reduce 0%\n060508 145316  map 1%  reduce 0%\n060508 145435  map 2%  reduce 0%\n060508 145553  map 3%  reduce 0%\n060508 145704  map 4%  reduce 0%\n060508 145822  map 5%  reduce 0%\n060508 145934  map 6%  reduce 0%\n060508 150045  map 7%  reduce 0%\n060508 150203  map 8%  reduce 0%\n060508 150323  map 9%  reduce 0%\n060508 150435  map 10%  reduce 0%\n060508 150551  map 11%  reduce 0%\n060508 150704  map 12%  reduce 0%\n060508 150824  map 13%  reduce 0%\n060508 150942  map 14%  reduce 0%\n060508 151055  map 15%  reduce 0%\n060508 151211  map 16%  reduce 0%\n060508 151326  map 17%  reduce 0%\n060508 151446  map 18%  reduce 0%\n060508 151606  map 19%  reduce 0%\n060508 151731  map 20%  reduce 0%\n060508 151847  map 21%  reduce 0%\n060508 152006  map 22%  reduce 0%\n060508 152127  map 23%  reduce 0%\n060508 152246  map 24%  reduce 0%\n060508 152407  map 25%  reduce 0%\n060508 152526  map 26%  reduce 0%\n060508 152645  map 27%  reduce 0%\n060508 152806  map 28%  reduce 0%\n060508 152926  map 29%  reduce 0%\n060508 153054  map 30%  reduce 0%\n060508 153112  map 30%  reduce 1%\n060508 153214  map 31%  reduce 1%\n060508 153336  map 32%  reduce 1%\n060508 153457  map 33%  reduce 1%\n060508 153616  map 34%  reduce 1%\n060508 153736  map 35%  reduce 1%\n060508 153903  map 36%  reduce 1%\n060508 154023  map 37%  reduce 1%\n060508 154143  map 38%  reduce 1%\n060508 154303  map 39%  reduce 1%\n060508 154424  map 40%  reduce 1%\n060508 154544  map 41%  reduce 1%\n060508 154705  map 42%  reduce 1%\n060508 154825  map 43%  reduce 1%\n060508 154944  map 44%  reduce 1%\n060508 155104  map 45%  reduce 1%\n060508 155223  map 46%  reduce 1%\n060508 155345  map 47%  reduce 1%\n060508 155505  map 48%  reduce 1%\n060508 155624  map 49%  reduce 1%\n060508 155745  map 50%  reduce 1%\n060508 155906  map 51%  reduce 1%\n060508 160027  map 52%  reduce 1%\n060508 160146  map 53%  reduce 1%\n060508 160305  map 54%  reduce 1%\n060508 160427  map 55%  reduce 1%\n060508 160546  map 56%  reduce 1%\n060508 160712  map 57%  reduce 1%\n060508 160833  map 58%  reduce 1%\n060508 160953  map 59%  reduce 1%\n060508 161102  map 59%  reduce 2%\n060508 161115  map 60%  reduce 2%\n060508 161236  map 61%  reduce 2%\n060508 161403  map 62%  reduce 2%\n060508 161523  map 63%  reduce 2%\n060508 161642  map 64%  reduce 2%\n060508 161804  map 65%  reduce 2%\n060508 161925  map 66%  reduce 2%\n060508 162044  map 67%  reduce 2%\n060508 162203  map 68%  reduce 2%\n060508 162323  map 69%  reduce 2%\n060508 162445  map 70%  reduce 2%\n060508 162607  map 71%  reduce 2%\n060508 162725  map 72%  reduce 2%\n060508 162844  map 73%  reduce 2%\n060508 163005  map 74%  reduce 2%\n060508 163124  map 75%  reduce 2%\n060508 163246  map 76%  reduce 2%\n060508 163406  map 77%  reduce 2%\n060508 163527  map 78%  reduce 2%\n060508 163646  map 79%  reduce 2%\n060508 163814  map 80%  reduce 2%\n060508 163936  map 81%  reduce 2%\n060508 164055  map 82%  reduce 2%\n060508 164215  map 83%  reduce 2%\n060508 164336  map 84%  reduce 2%\n060508 164456  map 85%  reduce 2%\n060508 164617  map 86%  reduce 2%\n060508 164736  map 87%  reduce 2%\n060508 164857  map 88%  reduce 2%\n060508 165017  map 89%  reduce 2%\n060508 165144  map 90%  reduce 2%\n060508 165303  map 91%  reduce 2%\n060508 165355  map 91%  reduce 3%\n060508 165424  map 92%  reduce 3%\n060508 165544  map 93%  reduce 3%\n060508 165704  map 94%  reduce 3%\n060508 165825  map 95%  reduce 3%\n060508 165947  map 96%  reduce 3%\n060508 170106  map 97%  reduce 3%\n060508 170224  map 98%  reduce 3%\n060508 170336  map 99%  reduce 3%\n060508 170454  map 100%  reduce 3%\n060508 171720  map 100%  reduce 4%\n060508 173342  map 100%  reduce 5%\n060508 175008  map 100%  reduce 6%\n060508 180613  map 100%  reduce 7%\n060508 182222  map 100%  reduce 8%\n060508 183822  map 100%  reduce 9%\n060508 185446  map 100%  reduce 10%\n060508 191056  map 100%  reduce 11%\n060508 192715  map 100%  reduce 12%\n060508 194330  map 100%  reduce 13%\n060508 195923  map 100%  reduce 14%\n060508 201521  map 100%  reduce 15%\n060508 203111  map 100%  reduce 16%\n060508 204638  map 100%  reduce 17%\n060508 210211  map 100%  reduce 18%\n060508 211742  map 100%  reduce 19%\n060508 213258  map 100%  reduce 20%\n060508 214824  map 100%  reduce 21%\n060508 220344  map 100%  reduce 22%\n060508 221856  map 100%  reduce 23%\n060508 223424  map 100%  reduce 24%\n060508 224715  map 100%  reduce 25%\n060508 225618  map 100%  reduce 26%\n060508 230309  map 100%  reduce 27%\n060508 230842  map 100%  reduce 28%\n060508 231336  map 100%  reduce 29%\n060508 231800  map 100%  reduce 30%\n060508 232215  map 100%  reduce 31%\n060508 232613  map 100%  reduce 32%\n060508 233012  map 100%  reduce 33%\n060508 233410  map 100%  reduce 34%\n060508 233806  map 100%  reduce 35%\n060508 234202  map 100%  reduce 36%\n060508 234557  map 100%  reduce 37%\n060508 234948  map 100%  reduce 38%\n060508 235344  map 100%  reduce 39%\n060508 235744  map 100%  reduce 40%\n060509 000132  map 100%  reduce 41%\n060509 000533  map 100%  reduce 42%\n060509 000932  map 100%  reduce 43%\n060509 001347  map 100%  reduce 44%\n060509 001822  map 100%  reduce 45%\n060509 002355  map 100%  reduce 46%\n060509 003007  map 100%  reduce 47%\n060509 003723  map 100%  reduce 48%\n060509 004651  map 100%  reduce 49%\n060509 010213  map 100%  reduce 50%\n060509 011506  map 100%  reduce 51%\n060509 012217  map 100%  reduce 52%\n060509 012730  map 100%  reduce 53%\n060509 013027  map 100%  reduce 54%\n060509 013259  map 100%  reduce 55%\n060509 013606  map 100%  reduce 56%\n060509 013913  map 100%  reduce 57%\n060509 014153  map 100%  reduce 58%\n060509 014350  map 100%  reduce 59%\n060509 014557  map 100%  reduce 60%\n060509 014854  map 100%  reduce 61%\n060509 015056  map 100%  reduce 62%\n060509 015356  map 100%  reduce 63%\n060509 015638  map 100%  reduce 64%\n060509 015947  map 100%  reduce 65%\n060509 020204  map 100%  reduce 66%\n060509 020543  map 100%  reduce 67%\n060509 020728  map 100%  reduce 68%\n060509 021001  map 100%  reduce 69%\n060509 021200  map 100%  reduce 70%\n060509 021330  map 100%  reduce 71%\n060509 021502  map 100%  reduce 72%\n060509 021703  map 100%  reduce 73%\n060509 022007  map 100%  reduce 74%\n060509 022301  map 100%  reduce 75%\n060509 022609  map 100%  reduce 76%\n060509 022940  map 100%  reduce 77%\n060509 023324  map 100%  reduce 78%\n060509 023642  map 100%  reduce 77%\n060509 023723  map 100%  reduce 76%\n060509 023746  map 100%  reduce 77%\n060509 023800  map 100%  reduce 76%\n060509 023820  map 100%  reduce 77%\n060509 024342  map 100%  reduce 78%\n060509 024559  map 100%  reduce 77%\n060509 024602  map 100%  reduce 78%\n060509 024845  map 100%  reduce 79%\n060509 025201  map 100%  reduce 80%\n060509 025453  map 99%  reduce 80%\n060509 025523  map 100%  reduce 80%\n060509 025645  map 100%  reduce 81%\n060509 030103  map 100%  reduce 82%\n060509 030454  map 100%  reduce 83%\n060509 030844  map 100%  reduce 84%\n060509 031401  map 100%  reduce 85%\n060509 031924  map 100%  reduce 86%\n060509 032352  map 100%  reduce 87%\n060509 032927  map 100%  reduce 88%\n060509 033737  map 100%  reduce 89%\n060509 034300  map 100%  reduce 90%\n060509 035312  map 100%  reduce 91%\n060509 040142  map 100%  reduce 92%\n060509 041755  map 100%  reduce 93%\n060509 043246  map 100%  reduce 94%\n060509 050725  map 100%  reduce 95%\n060509 060826  map 100%  reduce 96%\n060509 065234  map 100%  reduce 97%\n060509 074655  map 100%  reduce 98%\n060509 075706  map 100%  reduce 99%\n060509 081125  map 100%  reduce 100%\n060509 092825 Job complete: job_0002\nJob ended: Tue May 09 09:28:25 PDT 2006\nThe job took 66983 seconds.\n", "As to why we lost the 20 reduces, here is the breakdown:\n\nno progress update to task tracker for 20 min: 11\nrpc timeout on progress: 4\nlost task tracker (no heartbeat to job tracker for 10 min): 3\nexit 65 (caused by ping failures): 4\n\none task had exit 65 and lost task tracker\none task had exit 65 and no progress update\n\nso it task tracker latency is still a big concern.", "\nOwen,\n\nAre you still using 64,000 mappers? If so, wouldnt your average map output file size be around 80KB?\n\nI'd suggest doing a /= 10 or /=50 on those mappers. \n\nIf you had 1880 mappers and 376 reducers, your map output files would be 2.8MB each, which might be better then 80KB.\n\nYou might try 1800 mappers and 350 reducers, so that you have spare capacity on your nodes for failed mappers or reducers (giving you 3MB map output files).\n\nHas anyone measured the map-task creation overhead? Does anyone know the file creation/deletion overhead on Linux? Each of those little files is created, written, read, and deleted twice in the currnet code, and each time as that tiny filesize).\n\nPaul\n\nPaul", "> Are you still using 64,000 mappers? If so, wouldnt your average map output file size be around 80KB?\n\nYes.  But this comes from having a map task per input file block, which permits map tasks to be placed on nodes where their data is local.  Simply reducing the number of map tasks will defeat that important optimization.  Better to instead increase the dfs block size to 128m, as Eric suggested.  This would increase the map outputs to 340k (with 376 reducers).  But, once we move to a larger cluster, with 1000 or more reducers, then the map outputs will again become small.  So optimizing for small map outputs will remain important, even as we increase the dfs block size.", "\nFantastic! I wasnt aware of that optimization.\n\n- Has the map task creation overhead been measured?\n- Why not go to gigabyte blocks? ", "By \"map task creation\" do you mean task startup costs?  We have not yet explicitly measured that, but it does involve starting a JVM.  Currently the shuffle is still the slowest step, and until that changes, that's where the attention is.\n\nGigabyte blocks could prove useful.  We have not yet experimented much with larger sized blocks.", "Ok, last night's run with the parallel fetches (upto 5 at a time) finished in 14.13 hours. The input data and cluster were the same as monday's run.\n\nNone of the reduces failed, which helped.\n\nAfter I take the kids to school and get into the office, I'll take a closer look at the logs.", "\nOwen,\n\nA couple of small ideas:\n\n- Could you fit the tempfiles in a RAM disk? This would just be a hack to determine whether the disk physics of small files are a factor here, both on the mapper end and the reducer end. Note that you need 2X the space on the reducer end, because it keeps two copies of the data around in small-file-form. \n\n- If small files are shown to be a problem (as I am guessing), and (as Doug suggests) we want to optimize for that case, perhaps the best thing to do would be to send the map output data directly to the reducer, and have the reducers write them to disk in some log structured format, maintaining a list of segments that were abandoned mid-stream and are to be ignored in the processing step. This way you'd have all sequential disk access\n\nThanks. Sorry for the volume of responses here, but its an area of great interest to us.\n\nPaul", "The problem with such approaches is that they add a lot of complexity and break down in some cases.  They are particularly challenged by node failures and graceful handling of those is a key requirement to scaling up.  I think we have a lot of good optimizations we can do without changing the model.  We should do those first and gain some experience operating them before radically departing from it.\n\nCan you give us some data on the workload challenges you are facing?  In our current benchmark we are not to a place where these issues are the logical ones to tackle.  Maybe you could publish an alternate that expresses the demands of your workload?\n", "Here is a chart of the data transfers from Monday's run (blue) and Tuesday's run (red). The parallel fetches seem help most in the early part of the curve. You can also see the substantial rework that was caused by losing the tasks on Monday.", "\nOwen,\n\nLovely chart, thanks for posting it. \n\nAre you sure that the big valley in sequential mode was the re-execution of tasks?? if not, that looks really promising area for further measurement to see what else is going on at that time.\n\nIt is interesting that parallel sessions dont seem to increase the transfer rate by much. Either approach gets 50-60k files/minute, which means about 1000 files a second, which is only about 5 files received per second per node, or 10 files per second total per node (in+out).\n\nDoes this suggest that the bottleneck is not TCP or anything in the network, since parallel sessions should absorb these type of issues??\n\nIf so, the problem must be somewhere else, possibly disk or RPC code. Admittedly, 10 files per second processed by a node is so feeble that it seems hard to imagine it is a spindle problem, but  keeping an open mind:\n\n- Are you using a single spindle for your temporary files, or are you spreading them across all the available spindles on each node? Example:\n\n<property>\n  <name>mapred.local.dir</name>\n<value>/data/tmp/hadoop/mapred/local,/data1/tmp/hadoop/mapred/local,/data2/tmp/mapred/local,/data3/tmp/mapred/local</value>\n  <description>The local directory where MapReduce stores intermediate\n  data files.  May be a space- or comma- separated list of\n  directories on different devices in order to spread disk i/o.\n  </description>\n</property>\n\n- Another way to test whether spindle contention is the issue would be to use a RAM disk for the temporary files. Yes its a silly thing to do, but it would be a way to test the question if you have enough RAM.\n", "Everything we're now seeing is consistent with the inter-rack switches being the primary bottleneck.  With 188 nodes sharing a 1Gb/s backbone, there's only 600KB/s per node.  We're seeing 10 80kB files transferred per second, or 800kB/second, slightly higher, since some files are already on the same rack.\n\nInstead of caching temp files in RAM we can instead try to transfer files soon after they are generated and to process them on the remote end soon after they are recieved.  That way we can benefit from the kernel's cache, getting performance similar to what we'd see if we cached them ourselves.", "\nI dont have a 188 node cluster, so i wrote a simulator to test the impact of file sizes and buffer sizes on performance for a single node in Owen's test. The program has a simiulated map step, to create the output files using the configured buffer size, and a copy step to copy the files. \n\nThe idea is to isolate filesystem/disk performance issues from any interaction with RPC, TCP, switches, etc.\n\nResults show an 8-10X speedup with larger files and buffers:\n\nConfigration \"sort2\":\n(32MB DFS blocks, 4KB buffer, 320 mappers/node, 356 reducers total, 10GB total data): \n- map phase: 48 minutes and 45 seconds\n- copy phase: 70 minutes and 38 seconds\n\nConfiguration \"big\":\n(1GB DFS blocks, 1MB buffer, 10 mappers/node, 356 reducers total, 10GB total data)::\n- map phase: 6 minutes and 24 seconds\n- copy phase, 7 minutes and 56 seconds\n\nThat final copy phase was only running at 30MB/sec, so it should be easy to move that across the network if those 188 nodes were on one big switch. Obviously, this is about half the speed of the bare drive, so there is another 2X improvement possible and still be able to fit within gigabit network limitations.\n\nLike Owen's sort test, this test generate 10GB of data per node, and the node I'm using has 4GB of RAM. The program is attached, along with outout according to Owens' last configuration and a run with larger files and buffers. \n\nThe program also has a configuration called \"sort1\" that had the original configuraiton, but would take too long to run so i didint run it.", "The network core has a lot more bandwidth than that.  Each rack has a 1Gbit uplink, although this is a crufty enough network (oldest gear we have) that we may find some odd outliers.  But it can be assumed that we have at least 30mBits of bandwidth simultaneously from ever node to every other.  And the write tests are behaving as if we do.  So we can focus on finding other explainations.  We're also starting benchmarking on a newer set of nodes with ~100mBits simultaneously all to all.", "I wrote a network bandwidth tester that just uses Java sockets to connect all nodes to all nodes. My application waits until all of the servers are up and starts sending data (10g/node) using Java's sockets. On my cluster, which is currently at 202 nodes, it took an average of 1423 seconds (median of 1630) to finish the transfer between nodes. That is substantially faster than Hadoop's shuffle (7 hours?) and means that we have a long way to go in terms of shuffle optimization.\n\n", "Here's a patch that ought to address a lot of the concerns with the copy phase. The reduce task now launches a small number of threads (default 5) that fetch map outputs in parallel. The difference between this and the parallel RPC is that the parallel fetches are independent of each other, as soon as one copy finishes the next one starts. The parallel RPC send N requests and wait for them all to complete before sending the next N.\n\nRan Owen's benchmark with this code for a total runtime of 8 hours 53 minutes. Sorting 2000GB on 200 nodes, with 10 parallel fetchers per reduce. The (map+copy) finished in approximately 2.5 hours, so that the bulk of the time was spent in the reduce phase. We saw 41 failures during sort and reduce, which pushed out the total runtime.\nTasktracker latency causing pings and progress reports from the child to fail appears to be the biggest concern at this point.\n\nThe copy phase could be speeded further by improving the hit rate at the beginning of the copy. At this time a reduce probes for a random subset of the map outputs\nand fetches as many as are available, the fetcher threads are mostly idle early on, and they don't need to be. Having the job tracker furnish a list of maps that have completed since the last query from a tasktracker would go a long way towards addressing this.\n\n\nThe number of parallel copiers per reduce is controlled by the config variable \"mapred.reduce.parallel.copiers\" the default is 5 \n", "Some more implementation notes:\n\n1) The ReduceTaskRunner queries for map output locations concurrently with the copies. Discovered outputs\nare pushed into a queue from which they are pulled by the copiers.\n2) Added a new RPC method 'callRaw' which bypasses RPC's connection sharing mechanism. Requests are sent down a socket owned exclusively by the caller. The caller is free to do what he pleases with the connection once the request is complete. This implementation closes the connection immediately so that we don't get lots of idle threads in the tasktrackers.\n3) Early runs showed some load imbalances in the copies with lots of reduces pounding a single tasktracker. I've tried to address this by\n  a) ensuring that a reduce only copies 1 output from a given host at any time \n  b) introducing a backoff if a copy from some host fails\nthis seems to works fine for single node and small clusters because the number outputs is also small enough that a lot of parallelism isn't needed\n4) Some tasktrackers were running out of memory when 100s of clients connected to them at once. This happened because not enough memory was available to create the threads needed to handle the concurrent connections. This caused tasktrackers to go into a bad state and not service any more client requests while still sending heartbeats to the jobtracker. Tasktrackers now handle OutOfMemory errors gracefully. Of course, this condition no longer manifested once the load balancing code mentioned in 3) was introduced.\n\n\n\n\n\n", "This is great!\n\nThe RPC.callRaw() method really seems like it belongs in Client.java, and should mostly be code that's shared with non-raw calls.  Otherwise we duplicate code: if we change the format of calls on the wire or the way errors are handled, etc. this code will break.\n\nSo instead we might:\n\n1. Make RPC.Invocation public.\n2. Add a boolean 'newConnection' to Client.call(), adding the signature:\n    Writable call(Writable, InetAddress, newConnection);\n    This method can be implemented by constructing a Connection instance, but never calling connection.start(), moving the body of Connection.run() to a new method, Connection.getResponse() that can be called from Client.call() when newConnection is specified.\n\nDoes this make sense?\n\nThis way, e.g., if we want to specify a connect timeout for RPC connections (first creating the socket, then explicitly connecting), change buffer sizes, etc. we can do it in one place.\n\nIf there's agreement to clean this up (or that it doesn't need cleaning up!) then I can commit this as-is, and we can clean that up as a subsequent step, so that the patch doesn't grow too huge, the codebase doesn't move, etc.", "I'd agree that the callRaw stuff belongs in Client.java, I've copied the call dispatch code from Client.sendParam, but that's a violation of abstraction. Really wanted to get this off the ground so I didn't mess too much with the Client code. The approach you outline is the cleaner way to do it.\n\nor\n\nOwen's testing the parallel fetch against an HTTP server by having jetty serve the map outputs and it appears to be working a lot better than the RPC server. If we go that direction, we might decide we don't need 'callRaw' at all since this is the only use case so far.\n\nEither way I can commit to cleaning this up.", "So you'd prefer we commit this as-is & clean it up later?  Or should we hold off a bit & wait for Owen's HTTP-based map output collector?", "Talked to Owen. The agreement is that you should go ahead and commit this and we'll have it cleaned up in a subsequent patch.", "I just committed this.  Thanks, Sameer!"], "derived": {"summary": "The data transfer of the map output should be transfered via http instead rpc, because rpc is very slow for this application and the timeout behavior is suboptimal. (server sends data and client ignores it because it took more than 10 seconds to be received.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "improve performance of map output transfers - The data transfer of the map output should be transfered via http instead rpc, because rpc is very slow for this application and the timeout behavior is suboptimal. (server sends data and client ignores it because it took more than 10 seconds to be received."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Sameer!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-196", "title": "Fix buggy uselessness of Configuration( Configuration other) constructor", "status": "Closed", "priority": "Major", "reporter": "alan wootton", "assignee": null, "labels": [], "created": "2006-05-05T04:49:48.000+0000", "updated": "2006-09-08T21:19:43.000+0000", "description": "The constructor \npublic Configuration(Configuration other) ", "comments": ["The constructor: \n\n\tpublic Configuration(Configuration other)\n\t\nsets the defaultResources, and the finalResources but does not use them \nbecause it then sets the properties to a clone of the other.properties.\n\nIf someone then calls addFinalResource (as does JobConf) then the\n'other' properties are just lost. This becomes a problem if you attempt to use: \n\tpublic JobConf(Class exampleClass)  \n\nMy fix (there are several possible ways to fix this) is to just remember\nthe other.properties and then overlay then whenever getProps() loads the resources.\n\n", "Some mistakes in this submission (my first one, and I can't seem to edit them away).\n\nLet me restate the bug.\n\nThis constructor\n\n  public JobConf(Configuration conf) {\n    super(conf);\n    initialize();\n  }\n\ndoes not work as it should. The conf that is passed in gets lost.\n\nThe patch fixes this bug.\n\n- alan wootton, shopping.com", "Is there a reason why the patch was not submitted? I have encountered the same problem and patched it myself in a very similar way before finding out it had already been reported. Currently Configuration(conf) does not behave as expected.\n\nLorenzo Thione \nPowerset, Inc.", "> Is there a reason why the patch was not submitted?\n\nNot a good one.  I think it just fell off my radar.\n\nIt would be good to add a unit test for this.  I think the use case is roughly:\n\nConfiguration c1 = new Configuration();\nc1.set(\"foo\", \"bar\");\nConfiguratoin c2 = new Configuration(c1);\nassertEquals(c2.get(\"foo\"), \"bar\");\n\nIs that right?\n\nThe provided patch would fix this, but by setting all the values except \"foo\" twice: once when reloaded from resources and once when reloaded from overrides.  Perhaps it would be better to put all resource-loaded properties in a nested Properties instance that's inherited via 'new Properties(Properties)' and only store values set directly in a top-level Properties instance.  Or perhaps we should just set most of the values twice and not worry about it.  Thoughts, anyone?", "Attached patch fixes problem with current implementation, dynamically set properties are now preserved. Extension to testcase is also provided.", "I just committed this.  Thanks, Sami!"], "derived": {"summary": "The constructor \npublic Configuration(Configuration other).", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Fix buggy uselessness of Configuration( Configuration other) constructor - The constructor \npublic Configuration(Configuration other)."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Sami!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-198", "title": "adding owen's examples to exampledriver", "status": "Closed", "priority": "Minor", "reporter": "Mahadev Konar", "assignee": "Mahadev Konar", "labels": [], "created": "2006-05-05T07:33:24.000+0000", "updated": "2006-08-03T17:46:40.000+0000", "description": "owen's sorter and randomwriter are not added to the examples.jar file", "comments": ["Added owen's examples to exampledriver. Also, made the ClusterStatus contructors public since the WritableFactories throw an exception if non public fields are accessed out of the package.", "I just committed this.  I changed your fix to the WritableFactory problem slightly, as I'd rather avoid making more things public to work around this.  Please confirm that things work for you the way I fixed it.  Thanks!"], "derived": {"summary": "owen's sorter and randomwriter are not added to the examples. jar file.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "adding owen's examples to exampledriver - owen's sorter and randomwriter are not added to the examples. jar file."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  I changed your fix to the WritableFactory problem slightly, as I'd rather avoid making more things public to work around this.  Please confirm that things work for you the way I fixed it.  Thanks!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-199", "title": "reduce copy progress not updating", "status": "Closed", "priority": "Minor", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-05-07T10:26:54.000+0000", "updated": "2009-07-08T16:51:38.000+0000", "description": "I'm running with the svn head from Friday and my patch for hadoop-180 and I'm not getting progress updates until reduces finish. I'm worried that it may be related to my changes for hadoop-182. I'll track it down, but I wanted to let everyone know.", "comments": ["Good catch, Mahadev. Here is the patch.", "I just committed this.  It probably warrants a 0.2.1 release, so I've earmarked it that way, for Friday."], "derived": {"summary": "I'm running with the svn head from Friday and my patch for hadoop-180 and I'm not getting progress updates until reduces finish. I'm worried that it may be related to my changes for hadoop-182.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "reduce copy progress not updating - I'm running with the svn head from Friday and my patch for hadoop-180 and I'm not getting progress updates until reduces finish. I'm worried that it may be related to my changes for hadoop-182."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  It probably warrants a 0.2.1 release, so I've earmarked it that way, for Friday."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-200", "title": "The map task names are sent to the reduces", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-05-07T10:54:51.000+0000", "updated": "2009-07-08T16:51:38.000+0000", "description": "As each reduce is created, it is given the entire set of potential map names. For my large sort jobs with 64k maps, this means that each reduce task is given a two dimensional array that is 5 tasks/map * 64k maps = 320k strings. Since the reduce task is passed from the job tracker to the task tracker and down to the task runner, passing the entire list is very expensive. I suspect that this is the cause of the slow downs that I see in the task trackers heart beats when the reduce tasks are being launched.\n\nI propose that the ReduceTask be changed to just get the count of maps, with ids from 0 .. maps -1.\n  public ReduceTask(String jobFile, String taskId, int maps, int partition);\nThen we need to change the protocol for finding map outputs:\n  MapOutputLocation[] locateMapOutputs(String jobId, int[] mapIds, int partition);\n", "comments": ["+1 and another and another...", "+1 this sounds good to me.", "Here is the patch. It does dramatically lower the slugishness when the reduces are launching.", "I just committed this.  Thanks, Owen!"], "derived": {"summary": "As each reduce is created, it is given the entire set of potential map names. For my large sort jobs with 64k maps, this means that each reduce task is given a two dimensional array that is 5 tasks/map * 64k maps = 320k strings.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "The map task names are sent to the reduces - As each reduce is created, it is given the entire set of potential map names. For my large sort jobs with 64k maps, this means that each reduce task is given a two dimensional array that is 5 tasks/map * 64k maps = 320k strings."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-201", "title": "hadoop dfs -report throws exception", "status": "Closed", "priority": "Minor", "reporter": "Johan Oskarsson", "assignee": "Doug Cutting", "labels": [], "created": "2006-05-08T18:21:43.000+0000", "updated": "2009-07-08T16:41:53.000+0000", "description": "Running hadoop dfs -report throws the lovely exception below.\nChanging org.apache.hadoop.dfs.DatanodeInfo back to being a public class solves the problem.\n\n\n~/hadoop$ bin/hadoop dfs -report\n060508 104801 parsing file:/home/hadoop/hadoop/conf/hadoop-default.xml\n060508 104801 parsing file:/home/hadoop/hadoop/conf/hadoop-site.xml\n060508 104801 No FS indicated, using default:xxx:9000\n060508 104801 Client connection to 10.0.0.12:9000: starting\nTotal raw bytes: 2763338170368 (2573.55 Gb)\nUsed raw bytes: 1548564473694 (1442.21 Gb)\n% used: 56.03%\n\nTotal effective bytes: 145953744375 (135.93 Gb)\nEffective replication multiplier: 10.609967427182013\n-------------------------------------------------\n060508 104801 Client connection to 10.0.0.12:9000 caught: java.lang.RuntimeException: java.lang.IllegalAccessException: Class org.apache.hadoop.io.WritableFactories can not access a member of class org.apache.hadoop.dfs.DatanodeInfo with modifiers \"public\"\njava.lang.RuntimeException: java.lang.IllegalAccessException: Class org.apache.hadoop.io.WritableFactories can not access a member of class org.apache.hadoop.dfs.DatanodeInfo with modifiers \"public\"\n        at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:49)\n        at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:226)\n        at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:163)\n        at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:211)\n        at org.apache.hadoop.io.ObjectWritable.readFields(ObjectWritable.java:60)\n        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:170)\nCaused by: java.lang.IllegalAccessException: Class org.apache.hadoop.io.WritableFactories can not access a member of class org.apache.hadoop.dfs.DatanodeInfo with modifiers \"public\"\n        at sun.reflect.Reflection.ensureMemberAccess(Reflection.java:65)\n        at java.lang.Class.newInstance0(Class.java:344)\n        at java.lang.Class.newInstance(Class.java:303)\n        at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:45)\n        ... 5 more\n060508 104801 Client connection to 10.0.0.12:9000: closing\n", "comments": ["I just committed a fix for this.  It will be in the 0.2.1 bugfix release later this week."], "derived": {"summary": "Running hadoop dfs -report throws the lovely exception below. Changing org.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "hadoop dfs -report throws exception - Running hadoop dfs -report throws the lovely exception below. Changing org."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed a fix for this.  It will be in the 0.2.1 bugfix release later this week."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-202", "title": "sort should use a smaller number of reduces", "status": "Closed", "priority": "Trivial", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-05-09T05:27:55.000+0000", "updated": "2009-07-08T16:51:45.000+0000", "description": "We should see better performance with fewer reduces. I'll change the default number of reduces to be equal to the capacity of the cluster.", "comments": ["I just committed this.  Thanks, Owen."], "derived": {"summary": "We should see better performance with fewer reduces. I'll change the default number of reduces to be equal to the capacity of the cluster.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "sort should use a smaller number of reduces - We should see better performance with fewer reduces. I'll change the default number of reduces to be equal to the capacity of the cluster."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-203", "title": "remove deprecated java.io.File methods", "status": "Closed", "priority": "Minor", "reporter": "Doug Cutting", "assignee": "Doug Cutting", "labels": [], "created": "2006-05-09T05:38:30.000+0000", "updated": "2006-12-15T23:02:15.000+0000", "description": "Now that the 0.2 release is out, we should remove the deprecated FileSystem methods that use java.io.File, since using org.apache.hadoop.fs.Path is less error-prone.", "comments": ["This is subsumed by HADOOP-655."], "derived": {"summary": "Now that the 0. 2 release is out, we should remove the deprecated FileSystem methods that use java.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "remove deprecated java.io.File methods - Now that the 0. 2 release is out, we should remove the deprecated FileSystem methods that use java."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is subsumed by HADOOP-655."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-204", "title": "Need to tweak a few things in the metrics package to support the Simon plugin", "status": "Closed", "priority": "Major", "reporter": "David Bowen", "assignee": "David Bowen", "labels": [], "created": "2006-05-10T03:02:56.000+0000", "updated": "2006-10-14T02:07:37.000+0000", "description": "(1) added an extra metrics.jar target to the build.xml so that I can build a stand-alone library containing only the\nmetrics package and its subpackages.\n\n(2) added serialversionUIDs to a bunch of classes to make Eclipse happy\n\n(3) made AbstractMetricsContext.createRecord final, and added a protected newRecord that subclasses can use\nto customize record creation without breaking the parent class.\n\n(4) minor fix to how errors in callbacks are handled\n\n(5) constructor in MetricsRecordImpl made protected rather than package private so that it can be subclassed\n\n(6) extended Util.parse(String serverSpecs, int defaultPort) to handle the case of a null serverSpecs by defaulting to localhost\n\n", "comments": ["I just committed this.  I changed the name of the file generated by the \"metrics.jar\" task to be hadoop-metrics-X.X.jar, where X.X is the hadoop version.  Should this artifact be included with Hadoop releases?  If so then we should make sure that it is bundled into the .tar.gz file built by the \"tar\" target.  Or do we always expect its consumers to build it themselves?", "\n   [[ Old comment, sent by email on Tue, 16 May 2006 12:58:57 -0700 ]]\n\nThanks Doug.  It would be potentially useful to include the jar file in \nHadoop releases so that there is a well-defined place for people to get \nit if they wish to use the metrics API separately from Hadoop.  Then \nagain, no-one is asking for it yet.\n\n- David\n\n"], "derived": {"summary": "(1) added an extra metrics. jar target to the build.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Need to tweak a few things in the metrics package to support the Simon plugin - (1) added an extra metrics. jar target to the build."}, {"q": "What updates or decisions were made in the discussion?", "a": "[[ Old comment, sent by email on Tue, 16 May 2006 12:58:57 -0700 ]]\n\nThanks Doug.  It would be potentially useful to include the jar file in \nHadoop releases so that there is a well-defined place for people to get \nit if they wish to use the metrics API separately from Hadoop.  Then \nagain, no-one is asking for it yet.\n\n- David"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-205", "title": "the job tracker does not schedule enough map on the cluster", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Mahadev Konar", "labels": [], "created": "2006-05-10T03:56:48.000+0000", "updated": "2009-07-08T16:51:46.000+0000", "description": "The job tracker during my big sort runs only has 80-120 maps running at a time with 2 tasks/node and 179 nodes and a large queue of map tasks. It seems to be caused by the load balancing using the current load rather than the required load to run all of the queued tasks.", "comments": ["This patch allows map/reduce jobs to be scheduled using the pending load rather than the current load. Also, I have made changes in the JobInProgress to keep a running count of number of running/finished maps/reduces.", "I just committed this.  Thanks, Mahadev!"], "derived": {"summary": "The job tracker during my big sort runs only has 80-120 maps running at a time with 2 tasks/node and 179 nodes and a large queue of map tasks. It seems to be caused by the load balancing using the current load rather than the required load to run all of the queued tasks.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "the job tracker does not schedule enough map on the cluster - The job tracker during my big sort runs only has 80-120 maps running at a time with 2 tasks/node and 179 nodes and a large queue of map tasks. It seems to be caused by the load balancing using the current load rather than the required load to run all of the queued tasks."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Mahadev!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-207", "title": "Patch to HADOOP-96 uses long deprecated call", "status": "Closed", "priority": "Critical", "reporter": "Bryan Pendleton", "assignee": "Hairong Kuang", "labels": [], "created": "2006-05-10T06:52:58.000+0000", "updated": "2006-12-15T23:02:15.000+0000", "description": "System.getenv() was deprecated in Java 1.4. My mixed Java 1.4/Java 1.5 cluster won't start up with this code in place. Should probably change the scripts to pass the necessary environment variables in using -D or explicit arguments.", "comments": ["This patch allows hadoop logging works with java1.4. It also makes the .log name consistent with .log name.", "The 1.4 compatibility issue could be worthy of fixing in 0.2.1 (which I can't release until svn writes are re-enabled... checking... and it looks like they just were!).  What exactly is the problem on your mixed cluster?  Should I try to squeeze this into the 0.2.1 release?\n\nI'm concerned about the other changes on Win32.  Have you tested that the new Java command-line parameters work correctly if they contain spaces?  In particular, we should test that these work correctly when either the username or the log directory path contain spaces.\n\nThanks!", "If you run Hadoop 0.2.0 on jdk 1.4.2 it gets an exception that stops the servers at start up.", "Is that because I mistakenly compiled 0.2.0 with JDK 1.5, or is there something more going on?  How does it fail?  What's the minimal patch?  (I'll start trying this now and will soon find out, since I'm about to start building & testing release 0.2.1 using JDK 1.4, but any hints are welcome.)", "Assuming you have the trivial program:\n\npublic class test {\n  public static void main(String[] args) {\n    System.out.println(\"Home is \" + System.getenv(\"HOME\"));\n  }\n}\n\nCompiling under 1.5 produces no warnings.\nCompiling under 1.4.2 produces a depreciated warning for getenv.\n\nRunning either the 1.5 or 1.4.2 class file under jvm 1.5 works fine.\nRunning the 1.4 binary under the jvm 1.4.2 throws:\nException in thread \"main\" java.lang.Error: getenv no longer supported, use properties and -D instead: HOME\n        at java.lang.System.getenv(System.java:691)\n        at test.main(test.java:3)\n", "I just committed this.  The patch did not correctly handle paths with spaces on Win32.  I fixed that by converting them to space-free DOS paths.  Thanks, Hairong."], "derived": {"summary": "System. getenv() was deprecated in Java 1.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Patch to HADOOP-96 uses long deprecated call - System. getenv() was deprecated in Java 1."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  The patch did not correctly handle paths with spaces on Win32.  I fixed that by converting them to space-free DOS paths.  Thanks, Hairong."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-208", "title": "add failure page to webapp", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-05-11T05:31:52.000+0000", "updated": "2009-07-08T16:51:46.000+0000", "description": "I'd like a webapp page that just lists the failures in a job so that I can find them more easily. I also want to put back the job detail page.", "comments": ["This patch:\n   1. pulls the task tracker list off the root page and puts it on its own page\n   2. creates a job failure page that lists the task attempts that have failed\n   3. puts back a generic job page that covers both map and reduce", "Don't commit this one yet, I messed up the links for the next/prev. I'll generate a new patch.", "This patch replaces the other one and fixes the targets of the links for next/prev on the task lists.", "I just committed this.  It looks great!  Thanks, Owen!"], "derived": {"summary": "I'd like a webapp page that just lists the failures in a job so that I can find them more easily. I also want to put back the job detail page.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "add failure page to webapp - I'd like a webapp page that just lists the failures in a job so that I can find them more easily. I also want to put back the job detail page."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  It looks great!  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-209", "title": "Add a program to recursively copy directories across file systems", "status": "Closed", "priority": "Major", "reporter": "Milind Barve", "assignee": null, "labels": [], "created": "2006-05-11T06:05:18.000+0000", "updated": "2006-08-03T17:46:41.000+0000", "description": "A useful feature would be a simple command to copy directories recursively across filesystems. The source and destination path should be specified using a filesystem-neutral URI, such as:\n\nhadoop cp dfs://namenode1:port1/path/to/srcdir file:///path/to/local/destination/dir\n\n\"cp\" command would invoke a map-reduce program to copy files recursively.\n\nI willl attach a patch as soon as svn is up and running.", "comments": ["Here is a patch for recursively copying directories across multiple file-systems. Consists of addition of a cp command to bin/hadoop, a mapreduce program to copy files, and a unit test that tests all combinations of filesystems (i.e. 4 combinations of local, and dfs).", "I just committed this.  Thanks, Milind!"], "derived": {"summary": "A useful feature would be a simple command to copy directories recursively across filesystems. The source and destination path should be specified using a filesystem-neutral URI, such as:\n\nhadoop cp dfs://namenode1:port1/path/to/srcdir file:///path/to/local/destination/dir\n\n\"cp\" command would invoke a map-reduce program to copy files recursively.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add a program to recursively copy directories across file systems - A useful feature would be a simple command to copy directories recursively across filesystems. The source and destination path should be specified using a filesystem-neutral URI, such as:\n\nhadoop cp dfs://namenode1:port1/path/to/srcdir file:///path/to/local/destination/dir\n\n\"cp\" command would invoke a map-reduce program to copy files recursively."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Milind!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-210", "title": "Namenode not able to accept connections", "status": "Closed", "priority": "Major", "reporter": "Mahadev Konar", "assignee": "Devaraj Das", "labels": [], "created": "2006-05-11T07:09:22.000+0000", "updated": "2009-07-08T16:41:53.000+0000", "description": "I am running owen's random writer on a 627 node cluster (writing 10GB/node).  After running for a while (map 12% reduce 1%) I get the following error on the Namenode:\n\nException in thread \"Server listener on port 60000\" java.lang.OutOfMemoryError: unable to create new native thread\n        at java.lang.Thread.start0(Native Method)\n        at java.lang.Thread.start(Thread.java:574)\n        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:105)\n\nAfter this, the namenode does not seem to be accepting connections from any of the clients. All the DFSClient calls get timeout. Here is a trace for one of them:\njava.net.SocketTimeoutException: timed out waiting for rpc response\n\tat org.apache.hadoop.ipc.Client.call(Client.java:305)\n\tat org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:149)\n\tat org.apache.hadoop.dfs.$Proxy1.open(Unknown Source)\n\tat org.apache.hadoop.dfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:419)\n\tat org.apache.hadoop.dfs.DFSClient$DFSInputStream.(DFSClient.java:406)\n\tat org.apache.hadoop.dfs.DFSClient.open(DFSClient.java:171)\n\tat org.apache.hadoop.dfs.DistributedFileSystem.openRaw(DistributedFileSystem.java:78)\n\tat org.apache.hadoop.fs.FSDataInputStream$Checker.(FSDataInputStream.java:46)\n\tat org.apache.hadoop.fs.FSDataInputStream.(FSDataInputStream.java:228)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:157)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:43)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:105)\n\tat org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:785).\n\n\nThe namenode then has around 1% CPU utilization at this time (after the outofmemory exception has been thrown). I have profiled the NameNode and it seems to be using around a maixmum heap size of 57MB (which is not much). So, heap size does not seem to be a problem. It might be happening due to lack of Stack space? Any pointers?\n", "comments": ["I'd guess you're out of file handles or threads (both which can appear as OutOfMemoryError).  Each DFS client JVM and each datanode keeps a connection open to the namenode with a corresponding thread.  The number of threads per process in some older kernels was limited, but more recent kernels have mostly removed that limit, and the scheduler now also supports large numbers of threads effectively.  But you may need to change some limits.  Use 'ulimit -n' to see how many file handles you are permitted, and increase that to at least 4x the number of nodes in your cluster.  You may need to change some kernel options to increase the number of threads:\n\nhttp://www.kegel.com/c10k.html#limits.threads\n\nYou can monitor the number of open file handles with 'lsof', and the number of threads with 'ps'.\n\nI spent some time trying to get Hadoop's IPC to use non-blocking IO a while back (and hence far fewer threads).  The problem is that, since IPC requests include objects, we cannot start processing a request until we've recieved the complete request, and requests can be bigger than a single packet.  Moreover, the end of one request and the beginning of the next can be combined in a packet.  So it's easy to accumulate buffers for many connections using just a single thread, the problem is knowing when a buffer has a complete request that should be dispatched to a worker thread.  So we'd need to length-prefix requests, or break them into length-prefixed chunks.  This may be required for effective operation of very large clusters, or perhaps Linux kernel threads are now up to the task.  We'll soon see.\n", "The file handles are fine at 32768.\nThe kernel is 2.6.9, so it should be fine too.\n\nThe problem seems to be that the default thread stack size is 512k, which is more than a gig of stack for his 2036 threads. Mahadev is going to take the stack size on the Listener threads down to 128k, which should take the pressure off.", "\nReducing stack size may relieve the problem temporally, but will not solve the problem completely.\nIt seems to me that the problem is due to the fact that a thread is created per RPC connection. A better solution is to use a thread pool and a connection queue. This way, it is easier to manage the resource limits.\n\n", "> Reducing stack size may relieve the problem temporally, but will not solve the problem completely.\n\nThis remains to be seen.  What you say is possible, but it is also possible that, e.g., 2GB heap may gracefully  handle 10k or more connection threads.  We need to determine this.\n\nIt's a question of constants.  We know we need to allocate some buffer memory per connection, perhaps a few K, but perhaps more in some cases (e.g., block reports).  With a thread, we need some stack space, but less buffer space; probably more memory on the whole.  But there's no point in optimizing this if we can handle as many threads as we need with the amount of memory we have.", "It seems clear to me that before we get to a 2000 node Hadoop cluster, we will be using select to manage the incoming connections. Even with the 128k stack the 8000 threads would need 1 gig of ram, which is too much on our current hardware. The servers already have thread pools, but they just also have a thread per a socket.", "Let's not argue the point in the abstract. \n\nIf someone does submit a patch that reduces the overhead of having many RPCs/connections without complicating the programming model or tanking performance, I assume it would be acceptable, right?\n\nIf someone feels they can achieve these aims, I'd encourage them to sign up / implement something.  Then we can test it.\n\nOtherwise, let's let it lie.\n\n", "> Let's not argue the point in the abstract.  If someone [...] it would be acceptable, right?\n\nIs that a question for me?  I can't answer in the abstract.  Show me code & I'll give an opinion.  Other committers can too, folks can cause me to change my opinion, etc.  Heck, if someone convincingly demonstrates that the thread-per-connection model has reached the end of its tether, then I might implement it myself.\n\nHaving explored this a few times now, I currently think some sort of chunked encoding for requests is required.  We could also chunk responses, which might solve some other issues.", "I'm exploring possible solutions to this problem, kicking off a discussion...\n\na) Procrastinate\n\n  Get a really beefy 64-bit namenode. \n  Run it with lots of RAM in 64-bit (assuming none of the code needs changes and the JVM works) or with (almost) full 4GB virtual address space in 32-bit mode.\n\nb) Thread pool\n   \n  i) Create one thread-per (persistent) connection for the datanodes and then use a thread-pool to handle incoming client-connections. It would ensure that only during times of very high memory usage incoming client connections are penalized while the datanodes themselves have a persistent connection to the namenode.\n\n  ii) Everyone (clients & datanodes) go through the (possibly separate) thread-pool(s).\n\nc) Selectors\n\n Owen: I'm not very clear how selects will help could you please chime in (I'm only casually acquainted with Selectors)?\n\nthanks,\nArun", "Threads are already pooled, with a single thread per client JVM.\n\nThe solution is either to not cache connections, using a new connection per request, or to use selectors, so that a single thread can efficiently handle requests on all connections.  In the latter case, we need to alter the request protocol so that incoming requests can be buffered until they are complete, and then dispatched to a worker thread.  Currently a request cannot be parsed except by a readFields method, so there's no way for generic server code to tell when one request ends and the next begins.  So we can simply first write requests to a buffer on the client, then send them length-prefixed.", "I think we need to break the one thread per connection model.  otherwise our servers will not scale, so \"selectors\" are needed.\n\nAlso we probably need to break the very long connection caching model and the invarient of one connection per VM.  Connection setup is nearly free and serializing requests from different threads creates race conditions and other failure cases.\n", "It's looking clear to me that we need to change the RPC server to use java.nio.channels.ServerSocketChannel instead of the current connection/thread model.\nIs that what we're talking about?\n\nWho is the nio expert here? :-)", "Alan, yes, changing Server.java to use nio's Selector is what's under discussion, using a single thread on the server to buffer requests until they are complete, then dispatching each request to a worker thread.  Client.java must also be modified to buffer request objects so that they can be written preceded by their size, permitting Server.java to determine when each request has fully arrived.", "I am implementing this. For now I am using nio only for client accepts and subsequent reads from the client. The handler threads write the output/response directly by themselves to the clients concerned. Clients are disconnected if they don't communicate within a certain timeout. The thing is that time intervals could potentially be different for different protocols (e.g., dfs datanodes' heartbeats and client leases). So for now I am assuming a maximum timeout for the IPC communication (read from the conf file) and that is applicable for all RPC protocol communication. The servers keep track of when a client last communicated with it (either through TCP connect or through TCP write). Comments?", "Attached is the patch for doing selector-based RPC communication.", "This looks great to me & passes my tests.\n\nOne improvement I'd like to see is for Client.java to not serialize the call twice.  This could easily be done with Hadoop's DataOutputBuffer: write the call to a DataOutputBuffer, write the length, then write the data from the buffer.\n\nThanks!", "Thanks Doug for the comment. I have updated the patch accordingly.", "I just committed this.  Thanks, Devaraj!", "I'm having problems with this patch seeming to cause servers to stop serving requests. Usually, I can do a bit of work, but when I try to submit a job it, the job never seems to show up in the webapp.", "I reverted this for now, since it (for unknown reasons) seemed to break distributed operation.", "I think I will need to work with Owen to have a quick resolution on this.", "This patch was tested by Owen.", "I just committed this.  Thanks, Devaraj!"], "derived": {"summary": "I am running owen's random writer on a 627 node cluster (writing 10GB/node). After running for a while (map 12% reduce 1%) I get the following error on the Namenode:\n\nException in thread \"Server listener on port 60000\" java.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Namenode not able to accept connections - I am running owen's random writer on a 627 node cluster (writing 10GB/node). After running for a while (map 12% reduce 1%) I get the following error on the Namenode:\n\nException in thread \"Server listener on port 60000\" java."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Devaraj!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-211", "title": "logging improvements for Hadoop", "status": "Closed", "priority": "Minor", "reporter": "Sameer Paranjpye", "assignee": "Sameer Paranjpye", "labels": [], "created": "2006-05-12T06:16:09.000+0000", "updated": "2006-08-03T17:46:41.000+0000", "description": "Here's a proposal for some impovements to the way Hadoop does logging. It advocates 3 \nbroad changes to the way logging is currently done, these being:\n\n- The use of a uniform logging format by all Hadoop subsystems\n- The use of Apache commons logging as a facade above an underlying logging framework\n- The use of Log4J as the underlying logging framework instead of java.util.logging\n\nThis is largely polishing work, but it seems like it would make log analysis and debugging\neasier in the short term. In the long term, it would future proof logging to the extent of\nallowing the logging framework used to change while requiring minimal code change. The \npropos changes are motivated by the following requirements which we think Hadoops \nlogging should meet:\n\n- Hadoops logs should be amenable to analysis by tools like grep, sed, awk etc.\n- Log entries should be clearly annotated with a timestamp and a logging level\n- Log entries should be traceable to the subsystem from which they originated\n- The logging implementation should allow log entries to be annotated with source code \nlocation information like classname, methodname, file and line number, without requiring\ncode changes\n- It should be possible to change the logging implementation used without having to change\nthousands of lines of code\n- The mapping of loggers to destinations (files, directories, servers etc.) should be \nspecified and modifiable via configuration\n\n\nUniform logging format:\n\nAll Hadoop logs should have the following structure.\n\n<Header>\\n\n<LogEntry>\\n [<Exception>\\n]\n.\n.\n.\n\nwhere the header line specifies the format of each log entry. The header line has the format:\n'# <Fieldname> <Fieldname>...\\n'. \n\nThe default format of each log entry is: '# Timestamp Level LoggerName Message', where:\n\n- Timestamp is a date and time in the format MM/DD/YYYY:HH:MM:SS\n- Level is the logging level (FATAL, WARN, DEBUG, TRACE, etc.)\n- LoggerName is the short name of the logging subsystem from which the message originated e.g.\nfs.FSNamesystem, dfs.Datanode etc.\n- Message is the log message produced\n\n\nWhy Apache commons logging and Log4J?\n\nApache commons logging is a facade meant to be used as a wrapper around an underlying logging\nimplementation. Bridges from Apache commons logging to popular logging implementations \n(Java logging, Log4J, Avalon etc.) are implemented and available as part of the commons logging\ndistribution. Implementing a bridge to an unsupported implementation is fairly striaghtforward\nand involves the implementation of subclasses of the commons logging LogFactory and Logger \nclasses. Using Apache commons logging and making all logging calls through it enables us to\nmove to a different logging implementation by simply changing configuration in the best case.\nEven otherwise, it incurs minimal code churn overhead.\n\nLog4J offers a few benefits over java.util.logging that make it a more desirable choice for the\nlogging back end.\n\n- Configuration Flexibility: The mapping of loggers to destinations (files, sockets etc.)\ncan be completely specified in configuration. It is possible to do this with Java logging as\nwell, however, configuration is a lot more restrictive. For instance, with Java logging all \nlog files must have names derived from the same pattern. For the namenode, log files could \nbe named with the pattern \"%h/namenode%u.log\" which would put log files in the user.home\ndirectory with names like namenode0.log etc. With Log4J it would be possible to configure\nthe namenode to emit log files with different names, say heartbeats.log, namespace.log,\nclients.log etc. Configuration variables in Log4J can also have the values of system \nproperties embedded in them.\n\n- Takes wrappers into account: Log4J takes into account the possibility that an application\nmay be invoking it via a wrapper, such as Apache commons logging. This is important because\nlogging event objects must be able to infer the context of the logging call such as classname,\nmethodname etc. Inferring context is a relatively expensive operation that involves creating\nan exception and examining the stack trace to find the frame just before the first frame \nof the logging framework. It is therefore done lazily only when this information actually \nneeds to be logged. Log4J can be instructed to look for the frame corresponding to the wrapper\nclass, Java logging cannot. In the case of Java logging this means that a) the bridge from \nApache commons logging is responsible for inferring the calling context and setting it in the \nlogging event and b) this inference has to be done on every logging call regardless of whether\nor not it is needed.\n\n- More handy features: Log4J has some handy features that Java logging doesn't. A couple\nof examples of these:\na) Date based rolling of log files \nb) Format control through configuration. Log4J has a PatternLayout class that can be \nconfigured to generate logs with a user specified pattern. The logging format described\nabove can be described as \"%d{MM/dd/yyyy:HH:mm:SS} %c{2} %p %m\". The format specifiers\nindicate that each log line should have the date and time followed by the logger name followed\nby the logging level or priority followed by the application generated message.\n", "comments": ["I suggest we use iso8601 time format.\n\nhttp://www.cl.cam.ac.uk/~mgk25/iso-time.html\n\nThis would suggest yyyy-MM-ddTHH:mm:SS , such as 2006-05-11T23:47:03\n\nThe T is a literal and no one ever likes it.  Change it for all I care, but standards are ok.  This also suggests UTC, which I think is a good default, but also allows for local time, with a distinct notation 2006-05-11T23:47:03-08.  We could support that as a config option if folks care.\n\nThis format is also directly sortable, which is nice and avoids localization issues (MM-dd or dd-MM).\n\n", "I'm +1 for switching to commons-logging and to log4j by default.  But I think we shouldn't mandate a format.  It should be possible to embed Hadoop in other systems with other logging standards, and get it to comply with those standards.  So most of what you suggest about log formats I think should be couched in the terms \"by default\", right?\n", "I'm +1 for using a time format that is sortable. I've been using the sed, awk, grep tools for merging logs files to see trends across time.\n\ncommons-logging and log4j sound good.", "Yes, the suggestions about formats are meant to be defaults. This is one more reason for using Log4J, it gives you a fair amount of freedom with specifying formats in configuration. ", "Even log4j should be a default.  Hadoop code should only reference the commons-logging api, right?  BTW, Sun's logging API also gives you complete freedom in formatting, although you have to write some Java classes, not just configure it with formatting strings as you can with log4j.", "Wasn't really thinking in terms of making the logging implementation configurable, but there's no reason that can't be done. Hadoop code won't be invoking any part of log4j or whatever else directly.\n\nSun's logging does give you complete freedom in formatting, I was pointing out that it's not as flexible as log4j where a lot can be achieved in configuration.\n\nWe can have the logging implementation used be specified in configuration. Do you see a lot of people making use of that feature though? My instinct is that they won't...", "What real advantage do we get  from all of this flexibility?\n\nOne of our goals for attacking the logging system is to allow us to easily process the logs with the system.  To do that will require building readers that can deal with the log format and organization.  I'd hate to loose that in the interest of complete generality.\n\nJust curious what use case we are after with complete reformability and abstract logging.\n\n", "A lot of exceptions are currently being logged at the *info* level and most of them should probably be at the *warn* level. Especially, once we log the level it would be good to find the exceptions by grepping for WARN.", "The semantics I use for levels is something like:\n\nSEVERE: if this is a production system, someone should be paged, red lights should flash, etc.  Something is definitely wrong and the system is not operating correctly.  Intervention is required.  This should be used sparingly.\n\nWARN: in a production system, warnings should be propagated & summarized on a central console.  If lots are generated then something may be wrong.\n\nINFO, FINE, FINER, etc. are used for debugging.  INFO is the level normally logged in production, FINE, FINER, etc. are typically only used when developing.\n\nIs that consistent with the way others use these?\n", "At least one place where level should be info and not warn is when we create a file on dfs. It tries to do mkdirs whether directory exists or not. If it already exists, it warns that there is an error creating the directory. I think the warning should be when the directory could not be created because of other factors (such as permissions), otherwise it should be info or even fine.", "At the very least switching to commons logging will make it easier when configuring hadoop within other applications. Currently this is one of the few libraries I use that I can't configure to use my standard log4j settings.", "True, user errors that cause no harm don't deserve warnings in a central log.  The trick is to propogate the issue back to the user...", "I needed to get this working with log4j so I quickly ripped out the java.util.logging and replaced it with apache commons. The part I am unsure of is how much configuration of logging do you wish to do from hadoop itself. My opinion is let people configure the log4j the way they want it and hadoop should only choose whatever is available, and log to console if nothing is there (essentially what apache-commons does).", "On default Log4J configuration format - \n\nSome points - \n1. Logging caller class information like - originating method/line no, File name are known to be expensive operations. Also they may not be supported by All JVMs. Do we want these in default config ? \n2. For namenode I added an option %X{client}, this enables logging client identification along with msg but needs the code to supply that information using MDC.put(\"client\", clientName)\n3. We could do a seperate logger per client but thats a bad idea as there will be too many loggers. \n4. We can have logger hierarchy based on packages/classes and/or some logical hierarchy like - namenode.block.allocation, namenode.block.removal etc. Any comments on what type of log hierarchy you would like to see for the modules you worked on ? \n5. We can use MDC for categorizing logs on some criterion other than clients - like blocks, file system. Anythng you would like to see here ? \n\nA log4J format to start with - default is a RollingFile ( based on size, can make it based on time also ). Pls let me know your requirements if any and I will keep updating this file, once this done migrate codebase to log4j. \n-------------------------------------------------\n# remove DEBUG if not needed. \nlog4j.rootLogger=DEBUG, RFA\nlog4j.threshhold=ALL\n\n# Rolling File appender configuration\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${HANDDOP_HOME}/hadoop.log\n\n# change file size \nlog4j.appender.RFA.MaxFileSize=1MB\nlog4j.appender.RFA.MaxBackupIndex=10\n\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n\n\n# logically seperated logger configurations\nlog4j.logger.namenode=RFA\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p %X{client} %c{2} (%F:%M(%L)) - %m%n\n\n", "Here's attached the patch for switching to commons logging with log4j as the logging framework.\n\nNotes:\n\na) Hopefully this can be incorporated asap into hadoop svn since any further commits will entail us to patch anew.\n\nb) The conf/log4j.properties file uses DailyRollingFileAppender which rolls over at midnight. Another choice is to use RollingFileAppender which will rollover every 1MB and maintain 30 backups, which is currently commented out.\n\nc) There are some parts of code for e.g. those configuring the log-file, loog-levels etc. in the code which we have had to comment out since they aren't supported by Commons Logging. They should now be configured via the .properties file.\n\nd) The framework as in the patch creates 4 separate logfiles i.e. \n  > $HADOOP_HOME/logs/namenode.log\n  > $HADOOP_HOME/logs/datanode.log\n  > $HADOOP_HOME/logs/jobtracker.log\n  > $HADOOP_HOME/logs/tasktracker.log\n\n   This is done by passing -Dhadoop.log.file=<logfilename>.log via the bin/hadoop startup script and referenced in log4j.properties.\n\nthanks,\nArun\n\nPS: This will have to be committed *before* the libhdfs patch, and after that I will go ahead and create a 2 line patch for TestDFSCIO.java (switch java.util.logging to commons logging).", "Unfortunately this patch no longer applies cleanly.  Can you please update your source tree and re-generate this patch?  I try to process patches in the order they are submitted, but frequently there are conflicts in the queue.  Thanks!", "Also, we should not yet remove LogFormatter, but only deprecate it, as user code (e.g., Nutch) may use this class.  A good test for back-compatibility of changes to Hadoop's public APIs is to check that Nutch still compiles and runs correctly with an updated Hadoop jar.\n\nIf we deprecate LogFormatter in 0.3 then we can remove it in Hadoop 0.4, but we should always give folks at least one release to remove dependencies on deprecated features.\n\nThanks again!", "One more thing: please don't just comment out obsolete code; delete it.  Thanks.\n\nhttp://wiki.apache.org/lucene-hadoop/HowToContribute", "Doug, \n\n Please find the patch for commons-logging/log4j against the latest snapshot. \n\n I have incorporated all your comments i.e. kept LogFormatter.java, removed dead-wood etc.\n\n Please try and apply this patch on a priority basis since it touches quite a bit of the code-base and potentially any commit will necessiate another patch! :)\n\nthanks,\nArun\n\nPS: We have removed hadoop/conf from the build-time classpath in build.xml since we don't want hadoop's log4j.properties to be picked up by the build-tools. We feel hadoop/conf isn't necessary for building at all. Please correct us if we are wrong.\n", "Minor point: we will need commons-logging-1.0.4.jar and log4j-1.*.*.jar in lib/\n\nthanks,\nArun", "I can apply this by reverting to revision 410692, then use 'svn up' to merge in subsequent changes.  But I'm still having trouble building and running unit tests (at least on Windows, which I'm using today, since I'm on the road).  The change to build.xml causes hadoop-default.xml not to be found.  When I fix that, Jasper fails to be able to compile the jsp pages, since it uses log4j.  'ant clean test' reports:\n\nBuildfile: build.xml\n  [taskdef] log4j:ERROR setFile(null,true) call failed.\n  [taskdef] java.io.FileNotFoundException: \\ (The system cannot find the path s\necified)\n  [taskdef]     at java.io.FileOutputStream.openAppend(Native Method)\n  [taskdef]     at java.io.FileOutputStream.<init>(FileOutputStream.java:177)\n  [taskdef]     at java.io.FileOutputStream.<init>(FileOutputStream.java:102)\n  [taskdef]     at org.apache.log4j.FileAppender.setFile(FileAppender.java:289)\n\nSo I'm (sadly) not quite able to commit this yet.", "Okay, I've worked out the configuration problems.  Now it appears that some things that were previously logged at level=FINE are now logged at INFO, when they should be DEBUG.  I'm working on fixing that...", "With some trepidation, I just committed this.\n\nThere were a number of problems with this patch.  It changed all level=fine log messages into level=info, rather than level=debug.  The build needed to be repaired as well, since logging is performed there, but the standard configuration is not appropriate during build, so I added a build/test log4j configuration.  Finally, the changes to bin/hadoop did not name the log files correctly: the correct log file name should be normally set in bin/hadoop-daemon.sh and used in bin/hadoop.  I fixed all of these.\n\nIn the future, we should not try to make such large changes in the last days before a release.  Such changes should be made early in the release cycle.  I suspect we will still encounter more problems with this as I now try to make a release and test things with Nutch for back-compatibility, on Windows, etc.\n", "Apologise for all the troubles... we assumed fine->info, finer->debug, finest->trace mappings; we should have run this through you once. \n\nNext time please throw out the patch if we screw up and let us scramble to fix the mess we created... appreciate your patience. Thanks!", "We seemed to have missed out getMapOutput.jsp in the earlier patch... here's the fix. Thanks!", "New extensions to hadoop logging - \n - Rolling based on both time and size. \n - compress\n - Move rolled over files to DFS\n\nLog4J 1.3 has a better way of defining rollover policies and actions, I have a working implementation of above  but they depend on Log4J 1.3. Also the properties file will change to an XML format as the .properties format doesn't support all the configurations yet. Are we willing to move to 1.3 and XML log4j properties ?", "According to the tomcat 5.5 docs, XML configuration files don't allow you to use naming convention for logs within tomcat. As someone who uses tomcat and hadoop I would prefer not using the xml log. \n\nYou can (and should) be more picky about which packages to include in the logging. Tomcat 5.5 uses defines loggers by Engine and Host names. For example, for a default Catalina localhost log, add this to the end of the log4j.properties above. Note that there are known issues with using this naming convention (with square brackets) in log4j XML based configuration files, so we recommend you use a properties file as described until a future version of log4j allows this convention.\n\n", "I'm looking at support logging features like (cap on time/size, gzip) and archiving log files into DFS. Log4j 1.3 with XML configurations makes it real easy to implement all these with the RollingPolicies and Triggers separated from appenders. properties file format doesn't allow for specifying RollingPolicies externally for existing Appenders. \nAre you embedding Tomcat within Hadoop or using Hadoop from a webapp? Is it possible to make tomcat use its own properties file or configure Log4J for the webapp separately in the webapp's class loader? ", "I am using Hadoop within tomcat, my guess is there is a way to make hadoop use its own log properties that is separate from tomcat's, but it will be rather annoying to have a separate log4j.properties on a library by library basis.", "From what I understand, you are using hadoop client in tomcat. We need a light weight client for embedding in other apps for these use cases. That client can use apps logging configuration. \nThis logging is primarily targeted at Name node, data node, and trackers which generate logs on an ongoing basis. These log configurations need to be separated from Hadoop client in any case. ", "Is the client code logging?  If so we should file another bug to make this independent of all of the server logging for sure.  As barry explains his rig, I think he is actually running the hadoop servers inside tomcat as well.  This is a more complicated issue.  It may well make sense to support this, don't know enough about the environment to understand the pros and cons.  An obvious pro is that he has fewer processes to shepherd.  A con is that it isn't something anyone else has worked through or made work.  \n", "Barry, I've filed a new enhancement bug (ability to run datanodes in tomcat) to serve as an umbrella for this issue.  I think you've filed another bug on jeti related to this as well.  It would be good to tie all the tomcat issues together.  I'd be curious to know what others on the list (who know more about java/tomcat) think about this proposal, but I think we should move the discussion off this bug.\n\n\nhttp://issues.apache.org/jira/browse/HADOOP-353 - Run datanode (or other hadoop servers) inside tomcat", "A patch for rolling hadoop logs from all nodes to a well defined directory structure in DFS. Log files can be optionally compressed with gzip or zip. Log files are rolled over after a default 10MB or at midnight. The files are rolled over in DFS at a configurable path in a directory structure e.g. - \n<archive path>/year/month/day/logfile_index.log.gz  \n\nThis patch depends on Log4J 1.3, so we may not want to commit it in main trunk yet. It can be used if one doesnt mind using log4j's XML configuration file format. Log4J's DailyRollingFileAppendr will be deperecated in future versions so we will have to use this inevitably. \n\nPlease remember to upgrade the log4j.jar file to 1.3 version and remove the old version or this patch will not compile. \n"], "derived": {"summary": "Here's a proposal for some impovements to the way Hadoop does logging. It advocates 3 \nbroad changes to the way logging is currently done, these being:\n\n- The use of a uniform logging format by all Hadoop subsystems\n- The use of Apache commons logging as a facade above an underlying logging framework\n- The use of Log4J as the underlying logging framework instead of java.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "logging improvements for Hadoop - Here's a proposal for some impovements to the way Hadoop does logging. It advocates 3 \nbroad changes to the way logging is currently done, these being:\n\n- The use of a uniform logging format by all Hadoop subsystems\n- The use of Apache commons logging as a facade above an underlying logging framework\n- The use of Log4J as the underlying logging framework instead of java."}, {"q": "What updates or decisions were made in the discussion?", "a": "A patch for rolling hadoop logs from all nodes to a well defined directory structure in DFS. Log files can be optionally compressed with gzip or zip. Log files are rolled over after a default 10MB or at midnight. The files are rolled over in DFS at a configurable path in a directory structure e.g. - \n<archive path>/year/month/day/logfile_index.log.gz  \n\nThis patch depends on Log4J 1.3, so we may not want to commit it in main trunk yet. It can be used if one doesnt mind using log4j's XML configuration file format. Log4J's DailyRollingFileAppendr will be deperecated in future versions so we will have to use this inevitably. \n\nPlease remember to upgrade the log4j.jar file to 1.3 version and remove the old version or this patch will not compile."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-212", "title": "allow changes to dfs block size", "status": "Closed", "priority": "Critical", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-05-12T22:45:28.000+0000", "updated": "2009-07-08T16:41:53.000+0000", "description": "Trying to change the DFS block size, led the realization that the 32,000,000 was hard coded into the source code. I propose:\n  1. Change the default block size to 64 * 1024 * 1024.\n  2. Add the config variable dfs.block.size that sets the default block size.\n  3. Add a parameter to the FileSystem, DFSClient, and ClientProtocol create method that let's the user control the block size.\n  4. Rename the FileSystem.getBlockSize to getDefaultBlockSize.\n  5. Add a new method to FileSytem.getBlockSize that takes a pathname.\n  6. Use long for the block size in the API, which is what was used before. However, the implementation will not work if block size is set bigger than 2**31.\n  7. Have the InputFormatBase use the blocksize of each file to determine the split size.\n\nThoughts?", "comments": ["Are there two issues here?\n\nI can see a need to change the default block size for a DFS. In my case I'd like to write unit tests with small block sizes to check dfs code for bugs. \n\nI don't see the need for files to have their own sizes. Does this not introduce another 'moving part' to the dfs, and even more possibilities for bugs?  ", "As you point out, it is possible to just make configuration variable and use it everywhere. The problem is that you become very sensititve to differences in the configuration between nodes. It seemed less error prone to leave the client in charge of block size and consistently use their setting.\n\nUnder the hood, dfs currently supports variable block sizes within a file, but I certainly do _not_ want to expose that to the user-visible APIs.", "Variable block sizes within a file? I don't see that at all. I can see some checks being made to ensure that the size being sent matches what a datanode thinks is the correct size, but don't see any way, at all, that all Block's don't have a max size of 32 mb everywhere.\n\nIsn't adding a 'blocksize' param to the create method of DFSClient exposing variable block sizes to the client? \n\nAre we talking about the same thing?", "Sure, look at DFSInputStream.blockSeekTo. You can't generate files that look like that, but the infrastructure supports them.\n\nI'm only trying to expose setting the block size for a given file when it is being created. I don't want to expose changing it _within_ a file.", "Ok, I get it now. Even though it's currently impossible for any block, except the last block of a file, to be anything other than 32mb it looks like the system would support it.\n\nWe need to remove all references to BLOCK_SIZE. \n\nI see some problems. FSSataSet doesn't know which file it's working with. It always uses BLOCK_SIZE.  DFSClient.DFSOutputStream.write() has same problem. \n\nI'll vote yes.\n\n\n\n", "Ok, here is the patch.\n\nChanges dfs block size from a compile time constant to a parameter that is set when a file is created.\n\n1. FileSystem.getBlockSize becomes getDefaultBlockSize\n2. A new method FileSystem.getBlockSize(path) finds the blocksize of a file.\n3. Block size is added to FileSystem.create\n4. InputFormatBase uses the block size of each file rather than the global constant.\n5. I followed the convention of using DfsPath to cache meta information values associatied with the dfs file.\n6. FileUnderConstruction records the block size\n7. Removed check to make sure that the block size was shorter than the global value.\n8. Add a new value", "Overall, this looks great and is much needed.  Unfortunately I'm getting some null pointer exceptions running unit tests with this patch.  I've not yet tried to debug these...", "Doug, the problem is occuring much earlier before NPE:\n\n060515 123400 DIR* FSDirectory.mkdirs: failed to create directory /srcdat\n\nLooks like your DFS config has a root directory not writable for you. Because the test is trying to create dfs://localhost:65314/srcdat. I am using org.apache.hadoop.dfs.MiniDFSCluster for testing copying across dfs.\n", "Milind, I don't think these 'failed to create directory' messages are the problem.  That unit test succeeeds w/o this patch and fails with it.  In either case the unit test prints these messages.  I think these messages are because the directories already exist, so new attempts to create them fail, but I have not yet looked closely at that.", "Ok, I found the problem. I'll create a new patch.", "This adds an additional check for null on a file's block list.", "I just committed this.  Thanks, Owen!"], "derived": {"summary": "Trying to change the DFS block size, led the realization that the 32,000,000 was hard coded into the source code. I propose:\n  1.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "allow changes to dfs block size - Trying to change the DFS block size, led the realization that the 32,000,000 was hard coded into the source code. I propose:\n  1."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-213", "title": "add/change TestDFSIO command line arguments", "status": "Resolved", "priority": "Minor", "reporter": "Yoram Arnon", "assignee": "Konstantin Shvachko", "labels": [], "created": "2006-05-13T02:05:54.000+0000", "updated": "2009-07-08T16:41:54.000+0000", "description": "TestDFSIO would benefit from the follwing changes on the command line:\n\n1. allow a \"-dfs <dfs>\" option, to specify which dfs to test. Just like the hadoop dfs command.\n2. allow a \"-jt <job tracker>\" option, to specify a job tracker. Just like the hadoop job command.\n3. allign the file size units with the dfs block size. Using MB (2^20) when the dfs uses 10^6 skews the results a bit.\n4. add a \"-replication <replication>\" option, to specify files' replication factor.", "comments": ["I think this was fixed with introduction of generic options in hadoop command."], "derived": {"summary": "TestDFSIO would benefit from the follwing changes on the command line:\n\n1. allow a \"-dfs <dfs>\" option, to specify which dfs to test.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "add/change TestDFSIO command line arguments - TestDFSIO would benefit from the follwing changes on the command line:\n\n1. allow a \"-dfs <dfs>\" option, to specify which dfs to test."}, {"q": "What updates or decisions were made in the discussion?", "a": "I think this was fixed with introduction of generic options in hadoop command."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-215", "title": "New method for obtaining report of NameNode and JobTracker internals", "status": "Closed", "priority": "Minor", "reporter": "alan wootton", "assignee": null, "labels": [], "created": "2006-05-13T03:48:55.000+0000", "updated": "2009-07-08T16:51:46.000+0000", "description": "Many weeks ago we (at shopping.com research) decided we wanted to be able to get reports from the internals of JobTracker and NameNode. The hadoop web server provides some of this, but we wanted a more structured output, and easier extensibility.\n\nSo, we decided to use xml, and I wrote it.\n\nThere is a very thin interface to ClientProtocol, and JobSubmissionProtocol like this:\n\npublic XmlReporter getXmlReport(String classname, String question);\n\nThe implementation (in JobTracker and NameNode ) looks like this:\n\npublic XmlReporter getXmlReport(String classname, String question)\n    {\n\t\tXmlReporter reporter = XmlReporter.getInstance( classname, this,  question);\n\t\treporter.report();\n\t\treturn reporter;\n    }\n\nThe idea being that you pass in some xml (question), an XmlReporter (classname) is instanciated and passed back. \n\nAn XmlReporter object consists of nothing more that two org.w3c.dom.Document objects (one in the question, the other is the answer). The Writable interface, for the RPC, simply serializes the dom tree to a string, and then parses it back to a dom tree.\n\nAnyway, here it is. I would like for it to either make it into the code, or for me to find anoher way. \n\n", "comments": ["I think we are abandoning this. Please close this issue.", "Alan asked to close this."], "derived": {"summary": "Many weeks ago we (at shopping. com research) decided we wanted to be able to get reports from the internals of JobTracker and NameNode.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "New method for obtaining report of NameNode and JobTracker internals - Many weeks ago we (at shopping. com research) decided we wanted to be able to get reports from the internals of JobTracker and NameNode."}, {"q": "What updates or decisions were made in the discussion?", "a": "Alan asked to close this."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-216", "title": "Task Detail web page missing progress", "status": "Closed", "priority": "Trivial", "reporter": "Bryan Pendleton", "assignee": "Doug Cutting", "labels": [], "created": "2006-05-13T03:58:34.000+0000", "updated": "2009-07-08T16:51:46.000+0000", "description": "You can see progress on the job view web page, but not the individual task page.", "comments": ["Here's the change which adds the progress display to the task details page.", "I just committed this.  Thanks, Bryan!"], "derived": {"summary": "You can see progress on the job view web page, but not the individual task page.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Task Detail web page missing progress - You can see progress on the job view web page, but not the individual task page."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Bryan!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-217", "title": "IllegalAcessException when creating a Block object via WritableFactories", "status": "Closed", "priority": "Major", "reporter": "Hairong Kuang", "assignee": "Hairong Kuang", "labels": [], "created": "2006-05-13T07:56:39.000+0000", "updated": "2009-07-08T16:41:53.000+0000", "description": "When I ran the dfs namenode, I received an error message listed below. Changing Block class to be public will be able to fix the problem.\n\njava.lang.RuntimeException: java.lang.IllegalAccessException: Class org.apache.hadoop.io.WritableFactories can not access a member of class org.apache.hadoop.dfs.Block with modifiers \"public\"\njava.lang.RuntimeException: java.lang.IllegalAccessException: Class org.apache.hadoop.io.WritableFactories can not access a member of class org.apache.hadoop.dfs.Block with modifiers \"public\"\n        at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:49)\n        at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:226)\n        at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:163)\n        at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:211)\n        at org.apache.hadoop.ipc.RPC$Invocation.readFields(RPC.java:88)\n        at org.apache.hadoop.ipc.Server$Connection.run(Server.java:154)\nCaused by: java.lang.IllegalAccessException: Class org.apache.hadoop.io.WritableFactories can not access a member of class org.apache.hadoop.dfs.Block with modifiers \"public\"\n        at sun.reflect.Reflection.ensureMemberAccess(Reflection.java:65)\n        at java.lang.Class.newInstance0(Class.java:344)\n        at java.lang.Class.newInstance(Class.java:303)\n        at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:45)\n        ... 5 more\n", "comments": ["This does not appear to hamper normal DFS operation in any way. Looks like the Block constructor is registered as a writable factory but is not public. Does it need to be? Should Block objects ever be constructed this way?\n\nThis needs further investigation, making the constructor public just addresses the symptom, not the problem IMO. Need to figure out what part of the code is trying to create blocks through a writable factory, and should it be doing that or something different?\n\n", "I think this has been fixed, no?\n\nThe problem is that, when we modified Hadoop to use a classloader the writable factory stuff partially broke.  In particular, one can now obtain a reference to a Class instance when the static initializers of that class have not yet been run.  In particular, the RPC code calls something like 'WritableFactories.newInstance(ClassLoader.findClass(\"foo.bar.Baz\")), and foo.bar.Baz's static initializer, which registers the writable factory, has not yet run, and the call to newInstance fails as above.  The work-around we've been using thus far is to, in the RPC client code, add a 'static { new Baz(); }' to force the static initializers of Baz to run.  Long-term we should find a better solution.\n\nWe'd like to be able to pass objects in RPCs that are not public classes.  This means that the RPC code needs to be able to construct an instance.  The WritableFactory mechanism was created to solve this, but, as described above, it mysteriously broke when we stopped using the bootstrap classloader.", "This is a general problem with new tools breaking because they get passed back objects back from RPC that haven't been loaded by the application.", "The exception occurs when we restart dfs after the namenode is reformated. The namenode starts as an empty file system so the Block class is loaded. When the name node receives block reports containing blocks from the previous file system, it throws the exception.\n\nTo avoid this, we can either make the Block class public or explicitly load the class in NameNode.", "I just committed this.  Thanks!"], "derived": {"summary": "When I ran the dfs namenode, I received an error message listed below. Changing Block class to be public will be able to fix the problem.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "IllegalAcessException when creating a Block object via WritableFactories - When I ran the dfs namenode, I received an error message listed below. Changing Block class to be public will be able to fix the problem."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-218", "title": "Inefficient calls to get configuration values in TaskInprogress", "status": "Closed", "priority": "Minor", "reporter": "Mahadev Konar", "assignee": "Mahadev Konar", "labels": [], "created": "2006-05-15T05:07:07.000+0000", "updated": "2009-07-08T16:51:47.000+0000", "description": "Each time a pollforNewTask in called on the JobTracker, the taskinprogress makes a call to hasSpeculativeTask() which current does a conf.getSpeculativeExecution() each time its called. The fix would be to store it in the TaskInProgress as soon as it is created and make only a single call to conf.getSpeculativeExecution().", "comments": ["Here is a patch, that gets the configuration value once and stores it in the TaskInProgress.", "I just committed this.  Thanks, Mahadev!"], "derived": {"summary": "Each time a pollforNewTask in called on the JobTracker, the taskinprogress makes a call to hasSpeculativeTask() which current does a conf. getSpeculativeExecution() each time its called.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Inefficient calls to get configuration values in TaskInprogress - Each time a pollforNewTask in called on the JobTracker, the taskinprogress makes a call to hasSpeculativeTask() which current does a conf. getSpeculativeExecution() each time its called."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Mahadev!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-219", "title": "SequenceFile#handleChecksumException NPE", "status": "Closed", "priority": "Trivial", "reporter": "Michael Stack", "assignee": "Doug Cutting", "labels": [], "created": "2006-05-16T01:20:43.000+0000", "updated": "2006-08-03T17:46:42.000+0000", "description": "The SequenceFile#handleChecksumException assumes the conf data member has been set.  It will not be set if we use the 'Reader(FileSystem fs, Path file, int bufferSize, long start, long length)' constructor.  The latter is used by ReduceTask Sorter:\n\n\njava.lang.NullPointerException\n\tat org.apache.hadoop.io.SequenceFile$Reader.handleChecksumException(SequenceFile.java:407)\n\tat org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:400)\n\tat org.apache.hadoop.io.SequenceFile$Sorter$MergeStream.next(SequenceFile.java:837)\n\tat org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue.merge(SequenceFile.java:881)\n\tat org.apache.hadoop.io.SequenceFile$Sorter$MergePass.run(SequenceFile.java:766)\n\tat org.apache.hadoop.io.SequenceFile$Sorter.mergePass(SequenceFile.java:702)\n\tat org.apache.hadoop.io.SequenceFile$Sorter.sort(SequenceFile.java:528)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:253)\n\tat org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:787)\n", "comments": ["If no conf, default to throwing the checksum error.", "I just committed a fix for this.  Instead of checking for a null configuration, I tried to make sure that a non-null configuration is always available.  Thanks, Michael!"], "derived": {"summary": "The SequenceFile#handleChecksumException assumes the conf data member has been set. It will not be set if we use the 'Reader(FileSystem fs, Path file, int bufferSize, long start, long length)' constructor.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "SequenceFile#handleChecksumException NPE - The SequenceFile#handleChecksumException assumes the conf data member has been set. It will not be set if we use the 'Reader(FileSystem fs, Path file, int bufferSize, long start, long length)' constructor."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed a fix for this.  Instead of checking for a null configuration, I tried to make sure that a non-null configuration is always available.  Thanks, Michael!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-220", "title": "Add -dfs and -jt command-line parameters to specify namenode and jobtracker.", "status": "Closed", "priority": "Major", "reporter": "Milind Barve", "assignee": "Milind Barve", "labels": [], "created": "2006-05-16T05:18:08.000+0000", "updated": "2006-08-03T17:46:42.000+0000", "description": "Most hadoop commands accept -df and -jt commandline parameters. Make the cp command to accept those as well.", "comments": ["Ideally these options could be added to all commands through a common main() routine, as described in HADOOP-59.", "Indeed a lot of code is duplicated for parsing command-line options for hadoop commands. Until that gets fixed, here is a patch to add -dfs and -jt commands to \"hadoop cp\".\n", "Please disregard earlier patch. I am attaching another patch here that fixes hadoop-220, hadoop-228, and hadoop-229.\n", "I just committed this.  Thanks, Milind."], "derived": {"summary": "Most hadoop commands accept -df and -jt commandline parameters. Make the cp command to accept those as well.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add -dfs and -jt command-line parameters to specify namenode and jobtracker. - Most hadoop commands accept -df and -jt commandline parameters. Make the cp command to accept those as well."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Milind."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-221", "title": "make the number of map output configurable", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-05-16T10:32:21.000+0000", "updated": "2009-07-08T16:51:49.000+0000", "description": "The number of map output server threads in each task tracker is currently set to the number of task slots. Since the optimum setting depends on the network, it would be nice to have more threads serving map output files than reduces.", "comments": ["This functionality was superceded by the fix for HADOOP-254 (transfering map output files via http)."], "derived": {"summary": "The number of map output server threads in each task tracker is currently set to the number of task slots. Since the optimum setting depends on the network, it would be nice to have more threads serving map output files than reduces.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "make the number of map output configurable - The number of map output server threads in each task tracker is currently set to the number of task slots. Since the optimum setting depends on the network, it would be nice to have more threads serving map output files than reduces."}, {"q": "What updates or decisions were made in the discussion?", "a": "This functionality was superceded by the fix for HADOOP-254 (transfering map output files via http)."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-222", "title": "Set replication from dfsshell", "status": "Closed", "priority": "Trivial", "reporter": "Johan Oskarsson", "assignee": "Johan Oskarsson", "labels": [], "created": "2006-05-16T18:16:12.000+0000", "updated": "2009-07-08T16:41:55.000+0000", "description": "Added ability to set replication for a directory/file from the command line.\nNot heavily tested...", "comments": ["This is very useful.\nSome things that I have noticed.\n- Usage info might be useful.\nRight now it just throws ArrayIndexOutOfBoundsException if you do not specify the path.\nOr some other exception if you forget to provide the replication number. Try\n    hadoop dfs -setrep\n    hadoop dfs -setrep 3\n    hadoop dfs -setrep /user\n- I think the 2 commands should be combined into one like\n    hadoop dfs -setrep [-R] <path>\n- Should we use abbreviations like \"setrep\" or full names?\nsee HADOOP-226 (4) about necessity for a general DFSShell convention.", "Yeah, I know  there are issues like that, this is mostly a quick hack I needed to set the replication in order to save space.\n\nMy question is, should I fix them or should I wait for a complete DFSShell rehaul with Commons CLI?\nI currently do not have time to rewrite the DFSShell myself, but perhaps if there's been no attempt made in a few weeks I might give it a try.\n\nYou're absolutely right that they should be combined into one, also related to the whole issue with proper command line parsing etc.\nSame thing with -ls and -lsr I suppose.\n\nThanks for the comments.", "Other things that I would love to see added to DFSShell (to make it more shell-like). They are not such a big deal to implement, but they would increase the usefulness of this class tremendously (note: this is orthogonal to the issue of using Commons CLI, which I'd love to see too):\n\n* globbing and multi-argument cp/mv/rm/ls, using the standard shell metacharacters\n\n* interactive mode: ability to issue several commands in the same session, while keeping track of the context (such as pwd)\n\n* scripts: nothing fancy, just the ability to execute several commands recorded in a text file.\n\nThe scriptability and interactive mode, and a host of other useful features we can throw in cheaply by using Rhino (http://www.mozilla.org/rhino/shell.html).", "Johan, I would vote for your patch if you fix it.\n", "I finally had some time to fix the issues in the previous patch.\nPosting new patch that shouldn't throw any nasty exceptions if the user doesn't provide all the needed input", "New version with error handling etc", "+1", "I just committed this.  Thanks, Johan!"], "derived": {"summary": "Added ability to set replication for a directory/file from the command line. Not heavily tested.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Set replication from dfsshell - Added ability to set replication for a directory/file from the command line. Not heavily tested."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Johan!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-225", "title": "tasks are left over when a job fails", "status": "Closed", "priority": "Minor", "reporter": "Yoram Arnon", "assignee": "Owen O'Malley", "labels": [], "created": "2006-05-18T00:51:27.000+0000", "updated": "2009-07-08T16:51:48.000+0000", "description": "when jobs are stopped or otherwise fail, tasks are often left around.\nthe job tracker shows that there are map or reduce (mostly reduce) tasks running, when no job is running.\nthese accumulate over time.\neventually there are so many of those, that the job tracker can't launch new tasks, requiring a restart of the MR cluster.", "comments": ["Has anyone seen this recently or can we close it?", "Not sure:\n- I haven't been monitoring this as much lately.\n- JT crashes too frequently to get a decent statistic\n\nLet's check in a few days\n\nYoram \n\n", "We haven't seen this in a long time."], "derived": {"summary": "when jobs are stopped or otherwise fail, tasks are often left around. the job tracker shows that there are map or reduce (mostly reduce) tasks running, when no job is running.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "tasks are left over when a job fails - when jobs are stopped or otherwise fail, tasks are often left around. the job tracker shows that there are map or reduce (mostly reduce) tasks running, when no job is running."}, {"q": "What updates or decisions were made in the discussion?", "a": "We haven't seen this in a long time."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-226", "title": "DFSShell problems. Incorrect block replication detection in fsck.", "status": "Closed", "priority": "Major", "reporter": "Konstantin Shvachko", "assignee": null, "labels": [], "created": "2006-05-18T05:10:51.000+0000", "updated": "2009-07-08T16:41:56.000+0000", "description": "1. We need to adjust Dfsck to the new per-file replication feature.\nfsck checks block replication based on the configured global replication parameter.\nWhich is now just the default. The actual file replication is returned in DFSFileInfo.\nSo at least the reporting is screwed by that, although I didn't check what will happen with\nother options -move and -delete.\n\n2. fsck throws NullPointerException if you type\nbin/hadoop fsck -files /doc\ninstead of\nbin/hadoop fsck /doc -files\n\n3. Unfortunately, there are several commands that throw different kinds of Exceptions\nrather than at least printing the usage info, when some of its arguments are missing or\nmisplaced. ArrayIndexOutOfBoundsException is one them. Try\nbin/hadoop dfs -mv\nbin/hadoop dfs -cp\nbin/hadoop dfs -rm\n\n4. In general the shell is growing and getting more sophisticated.\nShould we work out a general convention on how the parameters should be structured, named,\nshort/long version of the keywords, help, etc.", "comments": ["This sounds like more than one bug.  (1) should be a separate bug.  (2) and (3) are minor: the error message is simply not very informative.  (4) calls for use of something like http://jakarta.apache.org/commons/cli/.  And 2-4 are somewhat related to HADOOP-59.", "I like Commons CLI. I think we should use it.\nExcept that we have sort of just one command, which is hadoop,\nwith hierarchical options, arguments, and sub-options.\nYes HADOOP-59 has other related issues.\nI agree we should combine all of them in one new issue specifically\ndedicated to DFSShell.\n", "We should not try to parse all command line options in one place.  A generic class should parse out -D, -conf, -fs, etc. options, build a configuration, then the per-command class should parse the rest of the command line.  That's what HADOOP-59 is about.  Commons CLI could be used to implement the generic option parser and/or each command's option parser.", "Here's a fix to make \"fsck\" count over/under replicated blocks according to the per-file value, rather than the global one.", "I just committed this.  Thanks Bryan!\n\nPlease submit new issues for the other aspects of this that remain unfixed."], "derived": {"summary": "1. We need to adjust Dfsck to the new per-file replication feature.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "DFSShell problems. Incorrect block replication detection in fsck. - 1. We need to adjust Dfsck to the new per-file replication feature."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks Bryan!\n\nPlease submit new issues for the other aspects of this that remain unfixed."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-227", "title": "Namespace check pointing is not performed until the namenode restarts.", "status": "Closed", "priority": "Major", "reporter": "Konstantin Shvachko", "assignee": "Dhruba Borthakur", "labels": [], "created": "2006-05-18T06:39:19.000+0000", "updated": "2013-06-23T13:55:23.000+0000", "description": "In current implementation when the name node starts, it reads its image file, then\nthe edits file, and then saves the updated image back into the image file.\nThe image file is never updated after that.\nIn order to provide the system reliability reliability the namespace information should\nbe check pointed periodically, and the edits file should be kept relatively small.\n", "comments": ["Does anyone have a clever idea of how to take a snapshot of a running, very active, and very large, NameNode? Without things timing out like crazy? We're ok with the idea of just stopping everything once in a while and restarting the namenode. Still.... ", "It looks as if this is on a back burner. I would like to raise the issue again, because the edits file can become large very quickly depending on the frequency of file operations. I generate an edits file of more than 2.5Gb every 24 hours. Of course, the namenode server could be restarted periodically (this what I have to do right now), but this is rather interruptive to clients. What about rotating the edits file periodically and resolving the older edits file with the image file in a separate thread? ", "Proposal for Checkpointing the Namesystem State in DFS\n------------------------------------------------------\n\nCurrently, the namesystem state in memory consists of a tree\nwhere the leaf nodes are files and internal nodes are directories.\nThis is serialized onto a disk file once when the namenode starts\nup. During the namenode operation, the state changes are made in\nmemory, but the on-disk copy of the image is not modified. Instead,\na transaction log (called edits log) for the namesystem changes\nis stored on the disk. When the namenode starts up again, it merges\nthe on-disk image and the edits log, and writes out the updated image.\n\nWhile the namenode is in operation, the edits log keeps on growing,\nirrespective of the size of the namesystem. For example, if a single file\nis constantly created and deleted, the namesystem state will be of\nconstant size, however, the edits log will keep on growing.\n\nTherefore it is necessary to periodically checkpoint the current state\nof the namesystem, and to purge the edits log.\n\nImage File Format on Disk\n-------------------------\n\nThe image on the disk consists of a header followed by a list of\npath entries, followed by known datanode entries. The header consists\nof DFS version number, namespace ID, and number of path entries.\nEach path entry corresponds to either a file or a directory. A file entry\ncontains the full path of the file, it's replication factor, number of\nblocks, and a list of block-entries for blocks belonging to that file.\nEach block entry consists of blockID and length of the block.\nA directory entry consists of full path of the directory, replication\nfactor (which, for a directory is always 0), followed by 0. This last\n0 distinguishes between files and directories.\nThe datanodes section of the image begins with number of known datanodes\nfrom the datanode map that the FSNameSystem maintains, followed by\nserialized form of each DatanodeDescriptor in that map.\n\nEdits Log File Format on Disk\n-----------------------------\n\nThe edits log contains a list of namespace transactions. Each transaction\nbegins with a transaction op-code that signifies type of the transaction.\nThere are seven types of transactions, and each type contains different\nproperties associated with it. These types of transactions and their\nassociated properties are listed\nbelow:\n\n1. Add File : Full path of the file, replication factor, list of blocks\n2. Rename : Old path of the file, new path of the file\n3. Delete : Path of the file deleted\n4. Make Directory : Path of the created directory\n5. Set Replication : Path of the file, new replication factor\n6. Add Datanode : datanode descriptor of the node\n7. Remove Datanode : datanode ID of the node\n\nWhen should we Checkpoint ?\n---------------------------\n\nCheckpointing decision could be based on elapsed time (e.g. every hour)\nor based on number of transactions (e.g. every 100,000 changes to the\nnamesystem). Since the later is approximately reflected in the size of\nthe edits log, this decision could also be based on the size of the\nedits log. This is preferred since, the cpu and/or memory requirements\nof checkpointing are determined by the size of the image as well as\nsize of the edits log. Also, this choice ensures that an idle\nnamenode is not checkpointed unnecessarily based on elapsed time.\n\nHow should we Checkpoint ?\n--------------------------\n\nThere are a number of choices here. We describe each choice, and its pros\nand cons below.\n\n1. Lock the entire namespace in the main namenode thread, while we save\nthe entire image on disk.\n\nThis would disable all namenode operations while we are checkpointing\nthe image. That would include processing of heartbeats also, and would\ncause datanodes to consider namenode to be dead, and cause cascading\nDFS crash.\n\n2. While saving an INode (i.e. path), only lock those nodes in the tree\nfrom root to that node.\n\nThis would require extensive changes in the simple locking model used\nin the namenode. Currently all the operations that the namenode\nperforms are fairly inexpensive. Therefore the simple locking model\nthat locks the entire namespace durung the transaction suffices. With\nthe new fine-grained locking model, one would need to acquire multiple\nlocks for each namspace operation, thus incurring additional overhead\nfor normal operations, which is the common case.\n\n3. Lock the entire namespace while we make an in-memory copy of the\nnamespace. Hand the copy over to a checkpointing thread, unlock the\nnamespace.\n\nThis would certainly be faster that option 1, since it does not involve\nwriting the namespace to disk while it is locked. However, it would\nrequire double the amount of virtual memory to hold the namespace\nwhile checkpointing is in progress.\n\n4. Lock the namespace. Rename the edits log. Start a new edits log.\nUnlock the namespace. Fork off a separate process, which loads old image\nand old edits log, and saves new image.\n\nThis method suffers from the same problem of requiring double the virtual\nmemory as does option 3. In addition, the forked process doubles the\nsystem resources such as open files and sockets.\n\n5. Lock the namespace. Rename the edits log. Start a new edits log.\nUnlock the namespace. Start a new thread, which merges old image on disk\nwith old edits log on disk to create a new image.\n\nOne observation that makes this proposal attractive is that the current\nin-memory image of the namesystem can be recreated by merging the old\nimage on the disk with the current edits log. On the face of it, this\nmethod would also suffer from extensive virtual memory requirements,\nhaving to load an entire disk image into memory. However, upon closer\ninspection, merging an on-disk image with on-disk edits log can be\nachieved with very small memory footprint, if we change the way the image\nis stored on disk. These changes and their rationale is explained below.\n\nEach entry in the on-disk image corresponds to a path. The only\nrequirement on the order of these entries for successfully loading\nthe image is that the entries corresponding to directories be before\nthe entries for files within those directories.\n\nIf we store the image where entries are sorted by their 'path' field,\nclearly entries for directories would be earlier than entries for files\nwithin them. With the sorted image, checkpointing process would involve\nin-memory sorting of edit log on the 'path' field, and then merging\nthe path-related ttransaction one path at a time before writing the\nfinal path record on the new disk image.\n\nAlmost all the path-related transactions in the edits log correspond to\na single path. The only exception to this is the 'rename' transaction,\nwhich corresponds to two paths, one old and one new. For files, this\ntransaction could be split into a pair of transactions that corresponds\nto one path each. 'rename old-path new-path' can be split into\n'delete old-path' and 'create new-path' for the sake of transaction\nlogging. Even for directories that are empty, one can do a similar\nsplit. However, for directories that contains files and/or subdirectories\nit becomes complicated, because each file/subdirectory under the\nrenamed directory needs to have a pair of log-entries corresponding\nto a 'delete' and 'create'. This will increase the edits log size\nby a factor proportional to the number of files in the renamed directory.\n\nOne approach to handle the directory rename operation while periodically\ncheckpointing, is to apply the rename operations both on the on-disk\nimage, and on the edits log entries previous to the rename operation.\nAfter renaming, though, the image will need to be sorted again according\nto the path names. This could be very expensive, since the on-disk image\nof a large filesystem (>1PB) could be a few GB.\n\nThe edits log would typically be a few Megabytes at the time of\nperiodic checkpointing, typically much smaller than the image. We take\nadvantage of this size difference, and an observation about the directory\nrename operation, to propose a solution to this problem.\n\nA rename operation \"rename srcPath dstPath\" in the edits log can be\nmoved to the end of the edits log, by applying other rename operations\non the edits log entries timestamped after the rename entry. That is,\n\nrename srcPath dstPath\n\noperation can be removed, and \n\nrename srcPath dstPath\nrename tmpPath srcPath\n\ncan be appended at the end of the edits log, if we apply the following\nrename operations to all edits log entries that occur after the rename\ndirectory operations:\n\nrename srcPath tmpPath\nrename dstPath srcPath\n\nThis way, we can manipulate the edits log, so that all the rename\ndirectory transactions are moved to the end. Then, we remove these\nrename operations into a separate file called the rename-table.\n\nWe change the definition of the on-disk image, so that it consists not\nonly of the sorted list of all paths in the namesystem, but also this\nrename-table. When the namenode starts up, it loads the list of paths\ninto memory to form a filesystem tree, and the applies the rename\noperations from the rename-table, to get the final image.\n\nWith this modification for handling directory-renames, our periodic\ncheckpointing algorithm becomes:\n\nId = On disk image\nRd = Rename table on the disk\nEd = Edits log on the disk\n\n1. Load Ed into memory as a list of edits entries\n2. Scan from the end of the edits log in order to find a directory rename\n   By applying the transformation described above, move these renames\n   towards the end. Do this for all directory renames.\n3. Append these rename operation to Rd.\n4. Sort the remaining edits log in memory.\n5. Merge Id (which is sorted), and sorted Ed, to get a new Id.\n\nThe namenode startup procedure is also modified, to be:\n\n1. Load Id, form a root directory tree.\n2. For each entry in Rd, apply rename operations on the image.\n3. Merge edits log, if exists.\n4. Store back the image as a sorted list of paths.\n5. Delete renames-table, and edits log.\n", "A checkpointing approach you don't seem to evaluate is making it very cheap to clone the in-memory  tree, specifically, by always copying nodes between the root and each edit.  That way one can checkpoint by just grabbing the pointer to the root, and then writing the shadow tree in the background.  No merging required, no complex re-sorting of operations, etc.", "Well, the sort-merge approach does not seem performant anyway if the number of renames is large. So, I am adding the copy-on-write (Dhruba's comment on HADOOP-334) to the proposal. This proposal suggests adding a clone member to every node. While checkpointing is in progress, change to a node is instead made to a clone. When periodic checkpointing is finished, the checkpointer resets the clones by making them actual nodes. I will write up a better description and post as RFC soon.\n", "Complex rename relocation stuff could be avoided if we used (unique) files ids to identify files.\nIn this case file name is just an attribute of the file. Renaming does not change the file id.\nFile hierarchy is based on ids rather than file names.\nAnd if we need to sort, we sort by file ids rather than their names.\n\nI like the merging approach. It is simple in general (not in details though) and does not\ninvolve introducing additional structures in the name-node, which will be hard to support,\nespecially if we plan to replace global locking by something more elaborate.\nAnd best of all it can work as a separate component.", "Proposal for Copy-On-Write FileSystem Tree For Periodic Checkpointing\n\nWe propose that the hadoop namenode image be checkpointed to disk after\nevery fixed (configurable) number of transactions.\n\nThe checkpointing method we propose:\n\n1. Does not introduce extensive changes in the simple locking model\n   currently used in the namesystem (FSNamesystem).\n2. Does not fork a heavyweight process to perform checkpointing.\n3. Does not lock the entire namesystem during checkpointing.\n4. Does not change the image or transaction log format in any way.\n5. Does not significantly increase garbage collection activity.\n\nThis proposal is based on making the filesystem tree copy-on-write\n*only during checkpointing*. We keep track of the number of outstanding\ntransactions in the main namenode thread. When this number reaches the\nconfigured (dfs.checkpoint.interval) number (say 10 million), the\nnamenode thread that was performing the transaction (in a synchronized\nmethod) performs the following actions:\n\n1. Close the transaction log 'edits.N', where N is the current\n   generation number. (Current fsedits is considered equivalent to\n   'edits.0', and current fsimage is considered to be 'fsimage.0').\n3. Creates a new transaction log 'edits.<N+1>'.\n4. Wakes up a checkpointing thread to dump a new image.\n5. Release namesystem lock.\n\nThis checkpointing thread:\n\n1. Acquires global namesystem lock.\n2. Sets a namenode-global boolean volatile variable\n'checkpointingInProgress' to true.\n3. Releases global lock.\n4. Starts traversing the filesystem tree in breadth-first manner, and\nwriting it to the disk in a file called 'fsimage.<N+1>' and removes\nfsimage.N, and edits.N.\n5. After writing the image, reacquires the global namesystem lock.\n6. Applies the changes on the shadow nodes to actual nodes.\n7. Set checkpointingInProgress to false.\n8. Releases the global namesystem lock.\n9. Sleep waiting for notification to do checkpointing again. \n\nStep 6 operation will become clear, when we describe how the namenode\nserver threads change the namesystem tree *while* checkpointing is in\nprogress.\n\nNamenode server threads always acquire the global namesystem lock\nbefore making any changes to the filesystem tree. Therefore all the\nsteps described below occur in critical-section.\n\n1. Check if checkpointingInProgress is false.\n2. If it is false, perform the requested namesystem changes, exactly as\nthey are performed currently.\n3. If it is true, locate the node of the filesystem tree that needs to\n   be changed.\n  3.1 If its member named 'shadow' of type 'Inode' is non-null,\n      perform the requested changes to that node.\n  3.2 Otherwise, create a new shadow Inode, clone all the fields from\n      original Inode there, assign it to the 'shadow' field of original\n      Inode. And perform the requested changes to the shadow Inode.\n      Append the original node in a list called 'changedNodes'.\n      \nStep 6 of the checkpointing node consists of traversing the\n'changedNodes' list, and replacing the fields of original node, with\nit's shadow node, and resetting the shadow reference to null.\n\nWith this checkpointing scheme, the namenode startup procedure remains\nunchanged, except that now the namenode looks for a valid image.N with\nmaximum N in the dfs.name.dir(s).\n", "Minor correction:\n\nIn the comment above, instead of\n\nStep 6 of the checkpointing *node* consists of \n\nPlease read:\n\nStep 6 of the checkpointing *thread* consists of \n\n", "The design looks pretty simple and clean.\nI still like the merging approach better. It is stand-alone!\nThere is no need to change anything in the name-node code.\nIt is useful as a maintenance utility for merging edits and images externally.\nDoes not lock name-node.\nAt some point the name-node data structures should be revised substantially\nand this copy-on-write effort will most probably be a wasted effort.\nDoes it make sense to invest more effort in designing a simpler merge algorithm?\n\nIf we still choose to do that:\n- Should we use \"standard name\" for current image and edits files (without .N)?\nMeaning before checkpointing edits is renamed to edits.N and new edits is re-created.\n- Do we need to keep all old images? Looks like just the last one is required.\nThis is periodic checkpointing, not a backup procedure.\n- If the node crashes in the middle of the checkpoint it is left with the old image,\nold edits, and new edits files. Are we going to apply both old and new edits during startup?", "Merging of fsimage with the edits can be done using O(sqrt( number of files )) memory.\n\nSuppose the number of files in fsimage (sorted by path name) is N.\nI divide fsimage into blocks so that each block has B=sqrt(N) namespace entries.\nThe number of such blocks will be also M=sqrt(N).\nFor each block we store in memory the path name of the first entry of the block, and the block offset.\nI then start reading the edits file. For every operation in edits I read an appropriate block from\nfsimage using the table in-memory, look for the appropriate entry, and perform operation on the \ncorresponding file. Update operations are performed in place, remove just leaves the free space \nin the block. When a new entry needs to be added current block is split into two new blocks each \ncontaining half of the records of the original block, and is stored in the end of the fsimage file.\nThe in-memory table is also updated to reflect new keys and new block offsets.\nThis algorithm needs to keep in memory the table of size M and one block of size B.\nThe total size of memory used is M + B = O(sqrt(N)).\n\nIf we need to tighten the memory requirement then we can divide N into smaller number \nof blocks (reduce M) and read a part of the block each time (reduce B).\nThe price is more disk IOs, which seems acceptable, for the name-node disk usage is not critical.\n", "Sounds a lot like a BTree and comes with all of the issues.  Lots of IO and complexity.  Reimplementing that seems like a bad idea.  perhaps you can find a good java BTree, but this seems like a big, heavy piece of code.\n\nWhy do we  need to do this?", "Here is a patch on the current Hadoop trunk .\n\nThis patch do automatic checkpoints without locking the filesystem.\n\nWhen it is time to do a checkpoint, edit logs stream are closed and new edit logs are opened, a thread is created that create a fake FSNamesystem that will merge previously written logs into fsimage. At the end, new edit logs are renamed to their old names.\n\nIt  will consume as much memory during the chekpointing as the current running instance of the FSNamesystem.\n\nThe auto checkpointing feature is disabled by default. So applying the patch \"as is\" is almost safe. (It does not break current image and logs format and loading philosophy) \n\nNonetheless, I can understand that you, the Hadoop dev team,  does not want to integrate this huge hacky patch as a part of the hadoop distribution...\n", "Right patch with a right name for the unit test case", "Ouch ! I'm tired today sorry, here is the right patch !", "Thanks Philippe, this is a refined effort that uses just the existing code to upload the image and merge it with the edits.\nUnfortunately, it doubles the memory consumption during checkpointing, which is what this issue all about imo.\n\n> Sounds a lot like a BTree and comes with all of the issues.\nIt's not a tree, there is no balancing, and I didn't mentioned trees even once.\nThe only issue I can associate with BTrees is splitting the block into 2.\n\n> Why do we need to do this?\nI am advocating to revive Milind's proposal #5 of the initial design.\nOur goal is to minimize memory overhead used for checkpointing and to provide uninterrupted access to the name-node during checkpointing.\nWe are not considering blocking approaches here so far, which makes minimizing memory our main requirement.\n\nThe copy-on-write approach potentially leads to a linear memory increase and requires additional name-node data structures.\nProposal #5 is an attempt to separate checkpointing from the name-node regular operation process.\nIt takes the image file and the edits file and merges them whether the name-node is present or not.\nIt does it with lots of IOs BUT in constant space.\n\nI was trying to come up with a simpler algorithm for the stand-alone checkpointing.\nIt uses more space but does not require external sorting or unintuitive file entry renaming (as #5).\nAnd it can be adapted to use constant space for the price of more ios.\nGiving up IOs imo is the right tradeoff here, since disk is not used by the name-node and as mostly idle during its regular operation.", "The Backup Namenode Proposal\n--------------------------------------------\nThe idea is to create a backup namenode, download the fsimage and the edits file\nto the backup namenode, merge them into a single image and then upload the newly\ncreated image into the primary namenode.\n\nThis approach has the following advantages:\n    1. No additional memory or CPU requirement for the primary namenode.\n    2. Good scalability, backup namenodes can be plugged into the network\n       on demand.\n    3. Address space separation of primary namenode and backup namenode, thus\n       better fault tolerance.\n\nThe namenode when invoked with the \"-backupmode\" command line option functions as the backup namenode. No additional scripts needed. One can run the backup namenode and the primary namenode on the same physical machine.\n\nThe backup namenode downloads the fsimage and the edits from the primary namenode through a http-get message. The primary namenode rolls the edit file on disk, send starts logging new transactions into the new editlog file. The backup namenode merges the downloaded fsimage and edit into a new image file. It then uploads the new image file to the primary namenode. The primary namenode replaces the old fsimage and the old editlog with the new uploaded fsimage.\n", "I like this proposal. It is simple, clean, reliable.\nWe need a backup namenode anyway for a production deployment.\n", "I like this proposal too :)\n\nMuch cleaner that my hacky patch !", "Here is a much detailed writeup on the Backup NameNode proposal. \"Secondary NameNode\" and \"Backup NameNode\" refer to the same node in this writeup. Please review and comment.\n\nConfiguration\n-------------\nThere will be an additional file named \"masters\" in the configuration directory (similar to the \"slaves\" file) that will list the node names where Secondary NameNode should be run. The start-dfs.sh script will start the Secondary-NameNode appropriately.\n\nThe configuration file will have a the following new definitions:\n    * fs.checkpoint.dir      : Location where the Secondary NameNode can download the\n                                        fsImage and edits file.\n    * fs.checkpoint.period   : Time (in seconds) between two checkpoints.\n    * fs.checkpoint.size     : Size (in MB) of edit log that triggers a checkpoint.\n\nThe Secondary NameNode will use \"org.apache.hadoop.dfs.NameNode.Alternate\" property to log its debug and informational messages.\n\nPrimary NameNode\n--------------------------\nThe Primary NameNode will add the following new RPCs to the ClientProtocol:\n\n    * getEditLogSize()\n        This call returns the size of the current edit log file. This call fails\n        if the NameNode is in SafeMode or there are more than one edit log file.\n\n    * rollEditLog()\n        This call closes the current edit log and opens a new edit log file.\n        The names of the edit files are either \"edits\" or \"edits.new\". To keep\n        complexity to a minimum, there will be a max of two edit log\n        files \"edits\" and \"edits.1\".\n        This call returns an error if any of the following conditions occur:\n        - NameNode is in SafeMode\n        - Both \"edits\" and 'edits.new\" are already pre-existing\n\n    * rollFsImage()\n        This call does the following steps (atomically):\n        - removes fsImage\n        - copies fsImage.tmp to fsImage\n        - removes edits\n        - moves edit.new to edits\n        This call fails if any of the files fsImage, fsImage.new or edits\n        does not exist. It also fails if the dfs is in SafeMode.\n\nThe NameNode will have two additional servlets:\n    * putFsImage.class\n        This servlet causes all the incoming data to be stored in a file\n        named fsImage.tmp in the dfs.name.dir directory. If this file already\n        exists, then this call returns error.\n\n    * getFile.class?param=pathname\n        This servlet retrieves the contents of the specified file.\n\nThe Primary NameNode at startup time deletes fsImage.tmp (if it exists). The NameNode loads the fsImage, then loads the edits and then loads edits.1.  Then it writes the merged fsImage, deletes edits and edits.1.\n\n\nSecondary NameNode\n-------------------------------\nThe Secondary NameNode periodically pings the NameNode with the getCurrentEditLogSize() RPC. This call returns the size of the current edit log. The Secondary NameNode initiates a checkpoint if either the size of the edit log exceeds the size specified in the fs.checkpoint.size or if the time since last checkpoint completion has exceeded fs.checkpoint.period.\n\nThe Secondary NameNode issues the rollEditLog() RPC to instruct the Primary NameNode to start logging edits into edits.1.  The Secondary NameNode then uses the getFile servlet to fetch the contents of fsImage and edits. It puts them in the fs.checkpoint.dir and, reads them into memory, merges them and writes it back to fsImage.tmp. The Secondary NameNode than uploads the fsImage.tmp file to the Primary NameNode using the putFsImage servlet.\n\nOnce the above steps are successful, the Secondary NameNode issues the rollFsImage() RPC. A checkpoint is complete when this RPC completes successfully.\n\nIf any of the RPC calls returns an error, the Secondary NameNode discards all processing that it might have done, logs an error message, and waits for the normal trigger to start the next checkpoint.\n\nIssues\n------\n1. The emphasis is on simplicity. For this reason, the NameNode restricts that there can be only two outstanding edits file at any time: edits and edits.1. This ensures that there cannot be more than one Secondary NameNode for a Primary NameNode.\n\n2. The fact that rollFsImage() fails if either edits or edits.1 are non-existent means that the system is protected against spurious checkpoint if the NameNode restarts when the Secondary NameNode was doing a merge. This check can be made more explicit by returning a cookie with the rollEditLog() command and enforcing that rollFsImage() supplies the same cookie. (The Primary NameNode resets the cookie if it restarts).\n\n\n", ">    * fs.checkpoint.period : Time (in seconds) between two checkpoints.\nThis is the maximal time between the checkpoints, right?\n\nWe should introduce new NamenodeProtocol for primary to secondary name-node communication.\n\nI'd go in the other direction: the primary node checks the edits log size each time it adds an entry.\nWhen it reaches the checkpoint.size or if the checkpoint was not done longer than checkpoint.period\nthe primary rolls the edits log and sends a command to the secondary node to create a new checkpoint.\n\nI think we should think about supporting multiple secondary nodes at least at the design stage.\nIn that case we will need to further propagate the checkpoints.\nOr do we just write the latest image into hdfs?\n", "I agree that we can introduce a new Protocol called the SecondaryNamenodeProcotol. \n\nI would still persist with the proposal that the primary namenode is just a \"slave\" as far as periodic checkpointing is concerned. All the \"intelligence\" of when to create the checkpoint, how to create it, etc.etc remains with the SecondaryNamenode. In the case when we support multiple Secondary namenodes doing their own periodic checkpointing according to their own schedules, the primary Namenode would otherwise have to do lots of schedule management for each of these periodic-checkpointers.\n\n", "An addition to the above proposal. There will be an additional parameter named dfs.namenode.secondary.configfile that contains the absolute pathname of the \"masters\" file.\n\nThe namenode will allow SecondaryNamenodeProcotol connections only from nodes listed in the \"masters' file. Also, the webUI can query the namenode to list the identities of the Secondary Namenodes.", "My idea of supporting multiple secondary nodes was that the primary node always deals with ONE secondary node, which in turn\nbecomes the \"primary\" node for next secondary node, and so on. The order of the nodes is defined by how the secondary nodes\nare listed in the config file. That way each name-node need to know and speak to only one secondary, which substantially\nsimplifies the logic. The primary decides when the new check point should be created and initiates the chain of checkpoints.\nI think we want to avoid heartbeat processing from secondary nodes and minimize inter-name-node communication traffic.\n\nI'd prefer to have all configuration in config file rather than configurable paths to other files, containing edditional configuration parameters.\nDon't like the last proposal linking \"masters\" in the config. This will make configration even more complicated.", "Konstantin proposal is essentially a push model where the primary Namenode drives the scehduling policies of the periodic checkpointing. Also, he mentioned about supporting cascading secondaries.\n\nI am going ahead the pull model: the Namenode is a very passive entity as far as periodic checkpointing is concerned. The scheduling policies are maintained only by the secondary namenode. The secondary namenode polls the primary periodically (say every 5 minutes) to determine the size of the current edit log.\n\nThe secondary would use HTTP-GET method to transfer fsmage and edits. Al alternative that was discussed was to use HDFS itself to transfer the image. However using HDFS has the disadvantage that the secondary would have to poll the primary to determine when the upload to HDFS was complete (HDFS does not have streaming RPC and has a fixed timeout for an RPC).", "First draft of periodic chekpointing code. Code review comments appreciated.", "The second draft of the periodic checkpointing code. This includes a unit test case to test checkpointing as part of nightly test run.", "Comments on Periodic Checkpointing Patch - v2\n----------------------------------------\n\nfs.checkpoint.period should be in seconds, not milliseconds.\nCode still contains debugging printfs. Log messages are not descriptive\nenough.\nTransferFsImage.java has windows-style crlf line-endings.\nTestCheckpoint does not test periodic checkpointing. Instead it does the\nsame thing as TestRestartDFS.\nNewly added methods in namesystem should not be public.\nFSImage.java has several whitespace-only-changes.\nIn FSEditLog.java, getEditLogSize checks to see if all edit logs have the same\nlength. However, this may not be true. If one of the local or remote fs which\nstores edits is full (or has exceeded quotas), the edits log will be of\ndifferent sizes. In that case getEditLogSize should return maximum\namong all edits.\nSecondaryNamenode.java does not use Logging to print errors, instead\nuses System.err.\nprintUsage is called once with an empty string.\nprintUsage prints [report] instead of [-geteditsize].\nIt should be possible to run the checkpointer as a cron job. There is\nno option for the secondaryNamenode to exit after finishing checkpointing.\ndefault masters files is not added. It should contain localhost.\nhadoop-daemons.sh usage contains [--file configfile]. It should be\ncalled [--hosts hostlistfile].\n", "I am trying to come up with the default values for the following configurable parameters:\n\n1.\tThe size of the edit log that can cause the next checkpoint.\n2.\tThe time period from one checkpoint to the other.\n\nThe next periodic checkpoint occurs whenever at least one of the above conditions are met.\n\nAssuming that a transaction takes 200 bytes in the edits log and the rate of 100 transactions per second, the edit log will increase at the rate of about 70MB per hour. Thus I am proposing that the default values for periodic checkpoints be \n1.\tedit log size = 64KB\n2.\ttime = 1hour\n\nComments appreciated.\n\n", "64KB seems aggressive, it only allows about 300 transactions before a checkpoint happens. Checkpointing should be frequent enough that when the namenode restarts it should be able to merge the edits into the namespace in a fairly short period of time (10-15 seconds perhaps?).\n\nIt might be useful to monitor how many edits can be merged into the image in 10 seconds (on a 2Ghz CPU say) and use something around that as the default.", "Sorry, a typo. My earlier calculations should have resulted in:\n\n1. edit log size = 64MB (not KB)\n2. time = 1 hour", "My theory is that a checkpoint time is actually dominated by the time to read and write the image to disk and the time to transfer the the image back and forth between the namenode and the secondary namenode. The actual CPU time needed to do the merge of the edit log with the fsimage may be relatively small.\n\nMy back-of-the-envelope calculation shows that sequential disk IO rates are at best  2GB per minute. Assuming a 2 GB image:\n\n1. NameNode reads image  = 1min\n2. Secondary NameNode stores image = 1 min (maybe in parallel with Step 1)\n3. Secondary NameNode stores new image = 1min\n4. NameNode stores image = 1 min\n\nThus a total of 3 to 4 minutes can be used to do a checkpoint (ignoring network issues).", "\nIs it possible to eleminate 1. and 2. above? Since secondary holds the new image from the last checkpoint, at the start of checkpoint secondary can exchange a token with the primary to confirm its image is the latest and merge with that image.\n", "Yes, it is possible to optimize. But it will not be part of my current implementation.", "This incorporates most review comments.\n\nThe main changes in this round of changes were in the following areas:\n\n1. Handle error cases when one namedir is bad.\n2. Use the fstime file to locate the latest and greatest edits file and image file \n3. Change the saveFSImage() function to use the code from periodic checkpoint. This reduces code   complexity because two different code paths need not be maintained. \n4. Uses Enums for image file names.\n5. \"format\"ing creates an image and an empty editslog.\n6. The TestCheckpoint simulates various namenode failure cases.\n", "\nregd 64MB default size, It need not be tailored so that it takes one hour to reach the limit.. there is already other variable to do checkpoints every one hour.  As Sameer pointed out its size could be determined by how it affects NameNode start up time. Since namenode start time depends more on the image size, edits log size could be larger. A small value could have more burden on NameNodes that are already busy.  In a big deployment, default many not matter at all since it will be manually set.\n", "Are you saying that the default editlogsize should be 128MB or so?", "\nHow much time does it take to merge 512MB? I think as long as time to merge edit.log is in the order of 30-60 sec, that should be fine. That way if a node is lightly loaded it will checkpoint every hour and edit.log would still be much smaller and if it busy, we won't add extra checkpointing load.\n\nAnother thing, to implement 'image file token' to avoid steps 1. and 2. above, we don't need to store any extra state on disk. This would just be a runtime property. Every time primary gets a new image from secondary, it also exchanges a 'token/etag'. If primary restarts, first check point will have steps 1. and 2.\n\n", "The namenode is typically bottlenecked on CPU whereas a new checkpoint-upload is IO bound. A periodic checkpoint acquires the fsnamesystem lock and switches/renames files. The overhead of doing this operation once every 360000 namenode transactions (in our hour) should be minimal. I would like to make 64MB the default editlogsize and once deployed, I will observe a real life cluster and then determine if we need to change the default size.\n\nReagrding the optimization of 'image token': I would defer it till I see measureable difference in performance on a real cluster. My goal is to keep the periodic checkpoint \"as-simple-as-possible\" while achieving the goal of making the \"namenode-restart faster\".", "\nYes, the optimization should be/should be done later.\n", "This has been merged with the latest trunk. Please review.", "This is the final patch for periodic checkpointing. Review comments from Milind that were incorporated here were:\n\n[Minor] hadoop-config.sh added export command wrongly indented\n\n[Reconsider Design] Why do we prevent periodic checkpointing if the namenode is in safemode ? Periodic checkpointing does not alter namespace, so should be allowed in safemode.\n\n[Minor] Clearly mark ErrorSimulation functions in SecondaryNamenode.java as used only from the testcase, so as to avoid confusion while reading code.\n\n", "Please incorporate periodiccheckpoint6.patch. Thanks.", "+1, because http://issues.apache.org/jira/secure/attachment/12349479/periodiccheckpoint6.patch applied and successfully tested against trunk revision r499156.", "+1 code reviewed.", "bin/slaves.sh caused secondary namenodes to start on all datanodes (only if HADOOP_SLAVES defined in hadoop-env.sh)", "+1\n\nI've run sort, smalljobs, nnbench, and dfsio benchmarks with this patch.  All pass.", "I just committed this.  Thanks, Dhruba!"], "derived": {"summary": "In current implementation when the name node starts, it reads its image file, then\nthe edits file, and then saves the updated image back into the image file. The image file is never updated after that.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Namespace check pointing is not performed until the namenode restarts. - In current implementation when the name node starts, it reads its image file, then\nthe edits file, and then saves the updated image back into the image file. The image file is never updated after that."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Dhruba!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-228", "title": "hadoop cp should have a -config option", "status": "Closed", "priority": "Minor", "reporter": "Yoram Arnon", "assignee": "Milind Barve", "labels": [], "created": "2006-05-18T07:12:00.000+0000", "updated": "2006-08-03T17:46:42.000+0000", "description": "hadoop cp should have a -config option to enable overriding of default parameters.\nit  would perhaps be good to rename the command as well to something like dcp or distcp, since it's not a simple command, but rather an entire map-recude job", "comments": ["Added -config option.\nChanged command name from \"cp\" to \"distcp\".\nPart of patch attached to hadoop-220."], "derived": {"summary": "hadoop cp should have a -config option to enable overriding of default parameters. it  would perhaps be good to rename the command as well to something like dcp or distcp, since it's not a simple command, but rather an entire map-recude job.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "hadoop cp should have a -config option - hadoop cp should have a -config option to enable overriding of default parameters. it  would perhaps be good to rename the command as well to something like dcp or distcp, since it's not a simple command, but rather an entire map-recude job."}, {"q": "What updates or decisions were made in the discussion?", "a": "Added -config option.\nChanged command name from \"cp\" to \"distcp\".\nPart of patch attached to hadoop-220."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-229", "title": "hadoop cp should generate a better number of map tasks", "status": "Closed", "priority": "Minor", "reporter": "Yoram Arnon", "assignee": "Milind Barve", "labels": [], "created": "2006-05-18T07:22:37.000+0000", "updated": "2006-08-03T17:46:42.000+0000", "description": "hadoop cp currently assigns 10 files to copy per map task.\nin case of a small number of large files on a large cluster (say 300 files of 30GB each on a 300 node cluster), this results in long execution times.\nbetter would be to assign files per task such that the entire cluster is utilized: one file per map, with a cap of 10000 maps total, so as not to over burden the job tracker.", "comments": ["Number of maps is now computed as follows:\n\nnumMaps = max(1, min(numFiles,  numNodes*10, totalBytes/256MB, 10000)).\n\nPlus added reporting status for every file (or every 32MB - approx 10 seconds) so that tasks dont timeout while copying huge files.\n\nThis fix is part of the patch attached to hadoop-220."], "derived": {"summary": "hadoop cp currently assigns 10 files to copy per map task. in case of a small number of large files on a large cluster (say 300 files of 30GB each on a 300 node cluster), this results in long execution times.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "hadoop cp should generate a better number of map tasks - hadoop cp currently assigns 10 files to copy per map task. in case of a small number of large files on a large cluster (say 300 files of 30GB each on a 300 node cluster), this results in long execution times."}, {"q": "What updates or decisions were made in the discussion?", "a": "Number of maps is now computed as follows:\n\nnumMaps = max(1, min(numFiles,  numNodes*10, totalBytes/256MB, 10000)).\n\nPlus added reporting status for every file (or every 32MB - approx 10 seconds) so that tasks dont timeout while copying huge files.\n\nThis fix is part of the patch attached to hadoop-220."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-230", "title": "improve syntax of the hadoop dfs command", "status": "Closed", "priority": "Minor", "reporter": "Yoram Arnon", "assignee": "Mahadev Konar", "labels": [], "created": "2006-05-18T22:56:40.000+0000", "updated": "2009-07-08T16:42:04.000+0000", "description": "the hadoop dfs syntax (hadoop dfs -option value -cmd arg) is clunky: options must appear before the command, the command looks like just another option.\nI propose a more standard syntax:\n1. the command (ls, mv, du etc.) always comes first\n2. no '-' for the command\n3. options may appear anywhere, including between the command and its arguments\n\nallowed syntax would be:\nhadoop dfs ls -dfs <dfs> /path\nhadoop dfs ls /path -dfs <dfs>\n\nother commands may benefit from a similar syntax change:\nhadoop job [-status] job_0002 -jt <jt>\netc.", "comments": ["this bug is pretty old. given that people have already adapted to the dfs syntax and changing the current syntax would break all the scripts I am closing the bug as wont fix."], "derived": {"summary": "the hadoop dfs syntax (hadoop dfs -option value -cmd arg) is clunky: options must appear before the command, the command looks like just another option. I propose a more standard syntax:\n1.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "improve syntax of the hadoop dfs command - the hadoop dfs syntax (hadoop dfs -option value -cmd arg) is clunky: options must appear before the command, the command looks like just another option. I propose a more standard syntax:\n1."}, {"q": "What updates or decisions were made in the discussion?", "a": "this bug is pretty old. given that people have already adapted to the dfs syntax and changing the current syntax would break all the scripts I am closing the bug as wont fix."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-231", "title": "DFSShell Improvement wish list", "status": "Closed", "priority": "Major", "reporter": "Konstantin Shvachko", "assignee": "Mahadev Konar", "labels": [], "created": "2006-05-19T02:42:31.000+0000", "updated": "2009-07-08T16:42:13.000+0000", "description": "This is to discuss general DFSShell improvements, standardization, conventions, etc.\nHere are links to assorted ideas on this issue from previous issues.\nWe need to generalize them at some point.\n\nHADOOP-59\nHADOOP-226\nHADOOP-222#action_12412326\nHADOOP-230\n", "comments": ["closing the bug since HADOOP-230 is not longer valid."], "derived": {"summary": "This is to discuss general DFSShell improvements, standardization, conventions, etc. Here are links to assorted ideas on this issue from previous issues.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "DFSShell Improvement wish list - This is to discuss general DFSShell improvements, standardization, conventions, etc. Here are links to assorted ideas on this issue from previous issues."}, {"q": "What updates or decisions were made in the discussion?", "a": "closing the bug since HADOOP-230 is not longer valid."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-232", "title": "jar files sent to task tracker should override existing jar", "status": "Closed", "priority": "Minor", "reporter": "Yoram Arnon", "assignee": "Milind Barve", "labels": [], "created": "2006-05-19T05:17:33.000+0000", "updated": "2009-07-08T16:51:48.000+0000", "description": "jar files sent to task tracker are appended to list, rather than prepended to it.\nthis results in the original jar getting executed, although a new one was sent  - not the intent.", "comments": ["Patch attached.", "This is a security issue.  Typically there should not be multiple versions of a resource on the classpath.  If there are, what's to be trusted, the one local to the machine, or the one included in the job?  We don't want folks to be able to, e.g. include new versions of Hadoop's internal classes in a job.  Nor do we wish them to be able to override non-overrideable job settings, which this would permit.\n\nCan you elaborate on the case where you needed this feature?", "debugging the hadoop distcp command required it.\nthe command is executed by the dfs client, which is in this case a map task.\nthe class is part of the hadoop release however, so it already resided on the tasktracker's nodes. We wanted to actually override it using a newly sent jar, without upgrading and restarting the map-reduce cluster.\nwe saw that as a general enough case to warrant a change to the task tracker.\n\nI guess that an alternate solution would be to unbundle the 'external' programs from the main jar file.\n", "Yes, if you think this cp command needs to be updatable separately from the core then its code should not be in the core build, but rather in the examples, contrib or perhaps a (new) apps build.", "If the CopyFiles class is not put into core hadoop, I think it should not be part of the bin/hadoop command as well. Because all the classes invoked from bin/hadoop are in core hadoop. We should provide a separate script called distcp (which was my original proposal) in a top-level opt (denoting optional) directory. The source and class file should go to examples. Comments ?", "No longer necessary."], "derived": {"summary": "jar files sent to task tracker are appended to list, rather than prepended to it. this results in the original jar getting executed, although a new one was sent  - not the intent.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "jar files sent to task tracker should override existing jar - jar files sent to task tracker are appended to list, rather than prepended to it. this results in the original jar getting executed, although a new one was sent  - not the intent."}, {"q": "What updates or decisions were made in the discussion?", "a": "No longer necessary."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-233", "title": "add a http status server for the task trackers", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-05-19T07:04:40.000+0000", "updated": "2009-07-08T16:51:47.000+0000", "description": "I'd like to have:\n  1. a jetty server on each task tracker to serve up logs and status\n  2. the jetty server on the job tracker serve up logs", "comments": ["This patch generalizes the jetty server and splits it so that the task tracker and job tracker can have different jsp scripts registered. In particular,\n\nIn src/webapps:\n  index.html -> job/index.html\n  mapred/*.jsp -> job/*.jsp\n  n/a -> task/\n  n/a -> static/\n\nWe can put things like hadoop-logo.jpg in static and they will be available in either server under http://<host>:<port>/static/\n\nThe logs for each server are available under http://<host>:<port>/logs/\n\nThe default ports are:\n   job tracker: 50030\n   task tracker: 50060\n\n", "I finally figured out how to apply this:\n\nsvn mv src/webapps/mapred src/webapps/job\nsvn mv src/webapps/index.html src/webapps/job\npatch -p 0 < task-tracker-webapp.patch\nsvn add src/webapps/task\n\nBut then it doesn't compile.  I think you forgot to include the new class that starts a jetty server, StatusHttpServer.  There doesn't appear to be any of the 'static' stuff (mentioned above) in the patch.  Is that also missing from the patch?\n", "Ok, here is an updated patch. You'll still need to move the src/webapps/mapred to src/webapps/job and copy the hadoop-logo.jpg into src/webapps/static. \n\n", "Why do you copy the jsp files into the build directory?  That way they wind up in the jar, but only the .class files, .html and .jpg should need to be in the jar, since the .jsp files are pre-compiled, no?", "I just committed this.  I changed build.xml to not copy jsp files into the build directory.\n\nThanks, Owen!"], "derived": {"summary": "I'd like to have:\n  1. a jetty server on each task tracker to serve up logs and status\n  2.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "add a http status server for the task trackers - I'd like to have:\n  1. a jetty server on each task tracker to serve up logs and status\n  2."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  I changed build.xml to not copy jsp files into the build directory.\n\nThanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-234", "title": "Hadoop Pipes for writing map/reduce jobs in C++ and python", "status": "Closed", "priority": "Major", "reporter": "Sanjay Dahiya", "assignee": "Owen O'Malley", "labels": [], "created": "2006-05-19T16:28:42.000+0000", "updated": "2013-05-02T02:29:06.000+0000", "description": "MapReduce C++ support\n\nRequirements \n\n1. Allow users to write Map, Reduce, RecordReader, and RecordWriter functions in C++, rest of the infrastructure already present in Java should be reused. \n2. Avoid users having to write both Java and C++ for this to work. \n3. Avoid users having to work with JNI methods directly by wrapping them in helper functions. \n4. The interface should be SWIG'able.", "comments": ["It would be best to avoid deserializing & reserializing objects in Java before passing them to C.  So you might instead always use BytesWritable for the input and output keys and values in Java, and implement a subclass of SequenceFileInputFormat that uses SequenceFile.next(DataOutputBuffer) to copy the raw binary content into the BytesWritable.  Then your mapper and reducer implementations could simply get the bytes from the BytesWritable and pass it to the native method, without re-serializing.  Does this make sense?", "I agree with Doug that if you are going to throw away the advantages of actually having meaningful types on the C++ that the keys and values on the Java side should be BytesWritable. \n\nThat said, I think it would be much less error-prone for the users and easier to understand and debug if you followed the Hadoop API much closer. Define a Writable and WritableComparable interfaces in C++. The Record IO classes will support them with a minor change to the code generator. ", "I agree that  keys/values should be BytesWritable in case we are not using typed data (Records). But I am trying to understand how to to avoid serialization/de-serialization multiple times between Java and C++ and still use Record IO. \nSo when we define a new record format and generate classes to work with the record, the generated classes contain Reader and Writer for the record. These read/write 'Record' objects from streams. One way to implement this would be to modify the class generation and make Reader extend SequenceInputFile and return(optionally) a ByetWritable rather than a Record. The idea is that the Record know how to read itself and its size etc. so we let it read from the input but it reads in BytesWritable rather than the record type in Java. we pass this BytesWritable to C++ and do deserialization only once to get the right type. \nWith this in place we should be able to avoid the need for serialization/de-serialization and users will not need to write extra code per type of record. ", "I think we can mostly achieve what we want by using the SequenceFile.next(DataOutputBuffer) method to read the raw bytes of each entry, regardless of the declared types.  Thus the SequenceFile can still be created with the correct Java key and value classes, but when we actually read entries in Java we won't use instances of those classes, but rather just read the raw content of the entry into a byte-array container like BytesWritable that is passed to C.  Then the C code can deserialize the Record instance from the byte-array container.  So a C-based mapper should specify the real input key and value classes, but its InputFormat implementation will ignore that when entries are read, passing raw bytes to C.", "This can be done by providing just one C++ class in record IO. Currently, C++ version for record IO generates methods for each class that read from and write to InStream and OutStream interfaces, that contain only read and write methods. Creation of concrete classes that implement these interfaces is outside of the code generation. One could proovide a byteStream class in C++ that provides these interfaces. The construction of BytesOutStream happens in C++ and after serializing C++ record, this stream goes to Java via JNI, which is then converted to BytesWritable and written to the sequencefile. BytesInStream is created in Java tied with the sequencefile, and supplies bytes to the C++ record.", "I am not sure if I am following correctly here, I am new to the codebase and so I took some time to document the MapReduce flow in the system. There are some things I am not clear about (bold italics in attachment). would be great if you could clarify. It may help others new to the project. \n\nIn this specific case, RecordReader.next() is called by the framework (MapTask) and next() method knows  the boundry of the next Record in the File. In case it is a complex RecordIO type the boundry wont be known to anyone except the generated object, which contains code for reading and writing the object. This is the part which is not clear how a generic SequenceFile would know size/boundries of RecordIO objects. If the generated code contains code for reading the next record in a ByteWritable in addition to Record, this will be easy to implement. \nI think I am missing something here, can you please clarify ? ", "Updated ... ", "\n   [[ Old comment, sent by email on Mon, 22 May 2006 17:28:16 +0530 ]]\n\nHi Doug -\nI was also looking for a way to avoid serialization and de- \nserialization, but I am still not clear how do we use existing record  \nIO with this (without modifying generated classes)\nWhen we define a new record format and generate classes to work with  \nthe record, the generated classes contain Reader and Writer for the  \nrecord. These read/write 'Record' objects from streams. One way to  \nimplement this would be to modify the class generation and make  \nReader extend SequenceInputFile and return(optionally) a ByetWritable  \nrather than a Record. With this in place we should be able to avoid  \nthe need for serialization/de-serialization and users will not need  \nto write extra code per type of record. Or am I missing something here ?\n\n~Sanjay\n\n\n\n\n\n", "This is my current plan of approach.", "+1  This proposal looks good.  I note that the contexts could, with the addition of a nextKey() method, be turned into iterators, permitting alternate map and reduce control structures, like MapRunnable.  This may prove to be a feature.", "Here is a first pass of this patch. I still need to add unit tests.", "A few quick comments:\n\n- the pipes package should have a package.html file\n- only Submitter.java should be public, right?\n- one must 'chmod +x' the configure scripts\n- the create-c++-configure task fails for me on Ubuntu w/ 'Can't exec \"aclocal\"'\n- 'bin/hadoop pipes' should print a helpful message\n- a README.txt file in src/examples/pipes telling how to run things would go a long way\n\n", "+1\n\nhttp://issues.apache.org/jira/secure/attachment/12357433/pipes-2.patch applied and successfully tested against trunk revision r538318.\n\nTest results:   http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/146/testReport/\nConsole output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/146/console", "I just committed this.  Thanks, Owen!", "Integrated in Hadoop-Nightly #91 (See http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/91/)"], "derived": {"summary": "MapReduce C++ support\n\nRequirements \n\n1. Allow users to write Map, Reduce, RecordReader, and RecordWriter functions in C++, rest of the infrastructure already present in Java should be reused.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Hadoop Pipes for writing map/reduce jobs in C++ and python - MapReduce C++ support\n\nRequirements \n\n1. Allow users to write Map, Reduce, RecordReader, and RecordWriter functions in C++, rest of the infrastructure already present in Java should be reused."}, {"q": "What updates or decisions were made in the discussion?", "a": "Integrated in Hadoop-Nightly #91 (See http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/91/)"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-235", "title": "LocalFileSystem.openRaw() throws the wrong string for FileNotFoundException", "status": "Closed", "priority": "Major", "reporter": "Benjamin Reed", "assignee": null, "labels": [], "created": "2006-05-20T01:46:44.000+0000", "updated": "2006-08-03T17:46:43.000+0000", "description": "openRaw should throw f.toString() on an error, not toString().", "comments": ["I agree that this change helps a lot.", "I just committed this.  Thanks, Ben!"], "derived": {"summary": "openRaw should throw f. toString() on an error, not toString().", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "LocalFileSystem.openRaw() throws the wrong string for FileNotFoundException - openRaw should throw f. toString() on an error, not toString()."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Ben!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-236", "title": "job tracker should refuse connection from a task tracker with a different version number", "status": "Closed", "priority": "Major", "reporter": "Hairong Kuang", "assignee": "Sharad Agarwal", "labels": [], "created": "2006-05-20T02:17:34.000+0000", "updated": "2009-07-08T16:51:48.000+0000", "description": "After one mapred system upgrade, we noticed that all tasks assigned to one task tracker failed. It turned out that for some reason the task tracker was not upgraded.\n\nTo avoid this, a task tracker should reports its version # when it registers itsself with a job tracker. If the job tracker receives an inconsistent version #, it should refuse the connection.", "comments": ["We need a Tasktracker registration mechanism that validates a tasktrackers version when it attempts to first contact the jobtracker. A related issue is that the out of date tasktracker should then gracefully shut down and not try to keep pinging the jobtracker.\n\nThe jobtracker may optionally report mimatched tasktracker that tried to connect to it.", "Here is an approach:\n1. Pass additional argument -> String buildVersion in InterTrackerProtocol.heartbeat method\n2. VersionInfo.getRevision() can be used to figure out the buildVersion for jobtracker and tasktracker\n3. In the JobTracker's heartbeat method, verify the buildVersion with the Jobtracker's. If buildVersion does not match, throw an Exception which would result in TaskTracker to shutdown.\n\nOne other approach could be to include buildVersion in TaskTrackerStatus class. That would avoid changing the InterTrackerProtocol.heartbeat method signature. But the drawback would be that it would increase the JobTracker memory footprint as TaskTrackerStatus objects are stored in Jobtracker's memory.", "On relook, seems like buildVersion check at the initialContact is sufficient  (similar to the Namenode/Datanode model).\nAttaching the patch for review.", "+1 Patch looks good.", "-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12383007/236_v1.patch\n  against trunk revision 662813.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    -1 tests included.  The patch doesn't appear to include any new or modified tests.\n                        Please justify why no tests are needed for this patch.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2547/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2547/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2547/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2547/console\n\nThis message is automatically generated.", "Not included any test case as it is difficult to write for this functionality.", "The revision isn't strong enough, if you make a change and restart the cluster, an old task tracker would still agree. It really should be:\n\n{code}\nVersionInfo.getVersion() + \" from \" + VersionInfo.getRevision() + \" by \" + \nVersionInfo.getUser() + \" on \" + VersionInfo.getDate()\n{code}", "Attaching the new version of patch. Incorporated Owen's recommendation.", "-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12383449/236_v2.patch\n  against trunk revision 663487.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    -1 tests included.  The patch doesn't appear to include any new or modified tests.\n                        Please justify why no tests are needed for this patch.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed core unit tests.\n\n    -1 contrib tests.  The patch failed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2588/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2588/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2588/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2588/console\n\nThis message is automatically generated.", "the test case failures are unrelated to this patch.", "I just committed this. Thanks, Sharad!"], "derived": {"summary": "After one mapred system upgrade, we noticed that all tasks assigned to one task tracker failed. It turned out that for some reason the task tracker was not upgraded.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "job tracker should refuse connection from a task tracker with a different version number - After one mapred system upgrade, we noticed that all tasks assigned to one task tracker failed. It turned out that for some reason the task tracker was not upgraded."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this. Thanks, Sharad!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-237", "title": "Standard set of Performance Metrics for Hadoop", "status": "Closed", "priority": "Major", "reporter": "Milind Barve", "assignee": "Milind Barve", "labels": [], "created": "2006-05-20T05:42:11.000+0000", "updated": "2006-08-04T22:22:31.000+0000", "description": "I am starting to use Hadoop's shiny new Metrics API to publish performance (and other) Metrics of running jobs and other daemons.\n\nWhich performance metrics are people interested in seeing ? If possible, please group them according to modules, such as map-reduce, dfs, general-cluster-related etc. I will follow this process:\n\n1. collect this list\n2. assess feasibility of obtaining metric\n3. assign context/record/metrics names\n4. seek approval for names\n5. instrument the code.\n", "comments": ["Here are some quick ideas for what could be interesting statistics to monitor:\n\nMap Input:\n  records/second\n  bytes/second\n\nMap Output Transferred to Reduce Node;\n  records/second\n  bytes/second\n\nReduce Output:\n  bytes/second\n  records/second\n\nJob Tracker:\n  maps tasks launched\n  map tasks completed\n  reduce tasks launched\n  reduce tasks completed\n\nDFS Datanode\n  bytes/second written\n  bytes/second read\n  blocks/second read\n  blocks/second written\n  blocks replicated\n  blocks removed\n\nDFS NameNode\n  files created\n  files renamed\n  files listed\n  files opened\n  files removed\n\nIs this the sort of thing you had in mind?\n", "Yes, this sort of list is exactly what I had in mind.", "Here are a few I would like to add to the list:\n\nJob Tracker\n  jobs submitted\n  jobs completed\n\n\nTask Tracker\n  total tasks completed\n  number of maps running\n  number of reduces running\n", "Attached a patch that uses hadoop metrics API and a properties file that defaults to null context but has examples of file and ganglia output contexts.\n\nAll metrics requested in the bug, except for output-bytes for reduce tasks have been included. The output-bytes required a change to recordwriter interface, and therefore has been postponed.\n\nI have tested it with file and null context. Have not tested it with ganglia.", "This mostly looks good to me.\n\nThe indentation is non-standard for Hadoop (using four spaces instead of two), many lines exceed 80 columns, the new package imports section is not separated by a blank line and in NameNode.java I think you add some unused fields.\n\nMore importantly, there seems to be a lot of duplicated code.  It looks like we should probably add a MetricsReporter base class whose constructor constructs the context, record, etc.  This should log rather than ignore exceptions.  Then it can have reportMetric(String, int) and reportMetric(String, long) methods.", "okay. I will make the necessary modifications and will resubmit the patch. For low overhead, though I would have at least two instances of MetricsReporter, one for each context, dfs, and mapred.", "\nThis code is not using the metrics API as intended, in that it calls the update method after each metric modification.  The API is record-oriented, so update copies the whole record to the client library.  \n\nI don't think that this will cause significant, observable problems with the metric data, but it could be a significant performance issue.\n\nThe preferred model would be to replace per-metric methods like\n        void mapInput(long numBytes) \n        void mapOutput(long numBytes) \n\nwith something like\n       void mapIO(long numBytesInput, long numBytesOutput)\n\nand have this only call the update method once.\n\n\n      ", "But then, mapIO needs to be called everytime eiither numBytesInput or numBytesOutput change (they change in different places). So, I don't see a performance difference. Maybe having a different record for each metric is a better solution performance wise, because it will avoid the record-copy overhead, but then it will increase record update overhead, by sending two separate packets.", "\nOK, maybe this is no big deal since the records are small.  The idea of a record was to be a bunch of things that should be updated simultaneously, but maybe using it for a small number of things that are updated independently is OK.  Splitting the record into two would cost a bit of extra space in the client library (since the overhead of an extra record in a hash table outweighs the savings of  4 bytes per record) and would not save much in the cost of an update.\n\n", "I have attached an updated metrics patch. I have addressed most concerns expressed by Doug and David, except for removing duplicate code. Because the alternative is an overkill. It would not allow localized code updates, compiler cannot inline so increases overhead, because of multiple exit points in some classes, it will leak metrics records and in any case the duplicate code is small and in private classes (3-lines each in 6 classes). Please let me know if you have any more concerns.", "I still feel the duplicate could should be removed.  Performance should not motivate this: HotSpot should be able to inline regardless.  Nearly identical code is repeated in eight places, differing only in a constant string, which can easily be made a paramter.  If local modifications are required, then the common implementation can be subclassed or not used, but currently there are effectively no local modifications.\n\nAlso, you're still ignoring rather than logging exceptions.\n\nThis code will become the prototypical use of the metrics API.  We should make sure that it looks good.  If boilerplate code is required for prototypical use, then that boilerplate should become utility methods and/or classes in the metrics package, and the metrics package-level javadoc should encourage this use.  Insertion of metrics code should be minimally invasive.", "agreed. will re-do the patch soon.\n", "Hopefully this patch addresses alll your concerns, Doug.", "sorry forgot to merge with trunk before diffing.", "repatched after merging with trunk.", "Milind, sorry to be a stickler here, but I think this still needs a bit more work.\n\nThe MetricsUtil class should be in the metrics package and just be named Metrics.  The create() method should be renamed createRecord(). This would make canonical use look something like:\n\nMetricsRecord record = Metrics.createRecord(\"dfs\", \"datanode\");\n...\nMetrics.report(record, \"bytes-read\", bytesRead);\n\nAlso, the javadoc in this class should be improved.  It should reference the (very good) javadoc of MetricsRecord, MetricsContext, etc.  This will be most folks first point of contact with the metrics package.  Parameters and return types should be well documented (you can mostly cut and paste these from MetricsRecord).", "Doug, I have incorporated the suggested changes, and attached the patch.", "I just committed this.\n\nIt would be great to get some, e.g., Ganglia screenshots up on the wiki demoing this stuff.\n\nAn extra thanks to Milind for being patient and persistent on this one!\n"], "derived": {"summary": "I am starting to use Hadoop's shiny new Metrics API to publish performance (and other) Metrics of running jobs and other daemons. Which performance metrics are people interested in seeing ? If possible, please group them according to modules, such as map-reduce, dfs, general-cluster-related etc.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "Standard set of Performance Metrics for Hadoop - I am starting to use Hadoop's shiny new Metrics API to publish performance (and other) Metrics of running jobs and other daemons. Which performance metrics are people interested in seeing ? If possible, please group them according to modules, such as map-reduce, dfs, general-cluster-related etc."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.\n\nIt would be great to get some, e.g., Ganglia screenshots up on the wiki demoing this stuff.\n\nAn extra thanks to Milind for being patient and persistent on this one!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-238", "title": "map outputs transfers fail with EOFException", "status": "Closed", "priority": "Blocker", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-05-21T13:23:21.000+0000", "updated": "2009-07-08T16:51:48.000+0000", "description": "My patch on Friday unintentionally included part of my explorations with the file transfers, which broke map output transfers.", "comments": ["This makes the map output location use the \"real\" task tracker port rather than the http port.", "I just committed this."], "derived": {"summary": "My patch on Friday unintentionally included part of my explorations with the file transfers, which broke map output transfers.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "map outputs transfers fail with EOFException - My patch on Friday unintentionally included part of my explorations with the file transfers, which broke map output transfers."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-239", "title": "job tracker WI drops jobs after 24 hours", "status": "Closed", "priority": "Minor", "reporter": "Yoram Arnon", "assignee": "Sanjay Dahiya", "labels": [], "created": "2006-05-23T00:28:51.000+0000", "updated": "2013-05-02T02:29:00.000+0000", "description": "The jobtracker's WI, keeps track of jobs executed in the past 24 hours.\nif the cluster was idle for a day (say Sunday) it drops all its history.\nMonday morning, the page is empty.\nBetter would be to store a fixed number of jobs (say 10 each of succeeded and failed jobs).\n\nAlso, if the job tracker is restarted, it loses all its history.\nThe history should be persistent, withstanding restarts and upgrades.", "comments": ["More please.", "This info should be logged, so that it is durable in the face of jobtracker restarts too", "How about providing an additional JSP for job history. Current JSP's are using live data (not persistant), they will be significantly changed if we read from log file as well as live jobtracker in them. It will be cleaner if we dump the job log in a file and read the file in a separate JSP, linked from current page. Comments ? ", "+1 to the general idea of logging the data and reporting from the log.\n\nIt would be good to expand a bit on your idea, so it is clear what you are suggesting.", "I meant on JobTracker we log the job transition states in a structured ( XML?) persistent file. This is different from the standard log file and can be parsed and displayed by a JSP page (something like JobHistory.jsp) . It includes only completed and failed jobs. So from the user interface perspective running jobs are displayed as they are currently but failed and completed jobs are displayed from this log.\n\nWe can add extra information in this log than what we have currently as a postmortem analysis like time spent by different phases or by different hosts (?). \n\nThis will require unique job ids across jobtracker restarts otherwise it will be difficult to track jobs in history with same id. \nThis log file can contain a configurable numbers of days of history, which can be browsed by time. Optionally this history can itself reside in HDFS. Does this make sense ?", "if a job tracker fails while running a job, you'd want the failed job's information shown in the history, so you probably want to log a job in the file as soon as it's launched, rather than when it's completed.\n\nThere are several links that can be drilled down into from the job information - tasks, failed tasks, task trackers etc. Take care to store all that information in the log file so we do not lose any current functionality.\n\ncreating a file per day may be good, as it will enable a simple way of keeping a configurable length of history.", "I agree, but to display the status of running job we can use the existing infrastructure rather than the log file as it already provides the most current status. We will log as things happen but use it for display when job fails/completes or job tracker restarts. \nYes, the current functionality should be the minimum thats available in the log.", "+1 to the gist of this (Sanjay's latest suggestions and yoram's point about startup).\n\nPutting the log in HDFS is interesting, but perhaps a distraction short term.\n\nI think it would be worth trying to use the actual log infrastructure to store this information.  Rolling, compression, removal after a fixed time, no lost state when the sever fails...  all of this sounds like logging.\n", "repost, lost tabs in last post - \n\nHere is a first cut at the information in job history. While displaying in JSP we can show it in different views like by hosts or by jobs  -  \n\n\n- jobid \n - jobName\n - User\n - jobconf ( job.xml ) \n - start time \n - finish time\n - Status \n - total maps \n - total reduces\n - finished maps ( if make -k )\n - finished reduces ( if make -k )\n - Available task trackers list at job start (to find which hosts never ran any tasks or if hosts added in between. not sure if this is available from else where ?)\n \n - maps \n    - taskid\n        - task attempt\n            - hostname\n            - start time\n            - finish time \n            - error\n\n - reduces \n    - taskid \n        - task attempt\n            - host name \n            - start time\n            - finish time\n            - phases \n                - copy (?)\n                    - start time\n                    - finish time\n                - sort \n                    - start time\n                    - finish time\n                - reduce \n                    - start time\n                    - finish time\n            - status \n            - error ", "looks good.\nwhat would the jobconf be used for?", "thats for looking at job parameters - input/output etc on the web page itself if we keeps lots of jobs in history. currently we have it as a path in a temp directory displayed on the JSP. We can have the same path there or dump the jobConf in the log file. ", "For format of history file on disk - here is a proposal. \n\nSince we want the file to be flushed on every write, to survive jobtracker restarts. it makes sense to use a simple record oriented structure in file. Each log statement appends a record in the file. Since there can be multiple jobs running at any time the records can be intermixed in the log file ( unless we use one history file per job ). \nUsing one history file per job is also a viable option in which case we can separate log files in different directories for different days and delete old files. \n\nIn both cases following simple file format can be used to log history and parse/display in JSPs. \n<recordType> <key=value> <key=value> .... \n\nwhere recordType = {JobInfo, Task, MapAttempt, ReduceTask, ReduceAttempt .... }\nand keys will depend on recordType e.g. for JobInfo keys = {jobId, jobName, submitTime, launchTime ... }\n\ne.g. log while job start up may look like \n\nJobInfo jobId=job_001 jobName=wordCount submitTime=0001\nJobInfo jobId=job_001 launchTime=0002\n...\nJobInfo jobId=job_001 finishTime=0002\n\nWe can provide a proxy class JobHistory, which exposes specific methods for logging different log events and takes care for formatting issues at a central place. \n\ncomments ? \n", "Adding a dependency on Hadoop-263, which adds start/finish times to TaskStatus. This can be used at jobtracker to log these for individual task attempts. The times for individual task attempts is not available on job tracker otherwise. \nUsing JobInProgress.updateTaskStatus() to log start and finish times for individual task attempts to JobHistory. Currently I log start time when TaskStatus.SUCCEDDED or FAILED is reported, to avoid multiple startTime entries in history. ", "If we keep a single history file for jobtracker we will run into a very large history files very soon, specially when there are large number of small tasks. On the other hand if we rollover the file every day then job start and end events for longer jobs or the jobs that start on the day end will be in different log files. We will still be able to see daily activity but drilling into jobs will be a problem as we will have to look up in multiple huge file for job specifc events. \nYoram and I discussed over IM and here is current approach. \n\nWe maintain a master file for all jobs - this file contains only job start/finish events along with no of tasks failed at finish. If the JobTracker dies before finishing a job then we dont log number of failed taks in this file. \n\nFor each job we create a separate history log file and this file contains task/taskattempt start and finish times along with failures if any. \n\nThe master index is rolledover every month, and during rollover we look for all jobs that have not finished and move them to the new file and discard old jobs. The detailed history log for jobs older than a month will get deleted. \n\nThe master index will be used to render the main JSP for job history, clicking on the job will cause corresponding job file to be loaded / parsed and displayed on respective JSPs. \n\nStart time of the jobtracker is used as an extra key to uniquely identify jobs since same jobids are used when jobtracker restarts. \n\nWe will not have any host specific view of tasks in this case. ", "A patch for maintaining job tracker history. This patch - \n1. instruments JobInProgress to add log events to history file. \n2. Jobhistory management with a job master index file and one history file per job,  parser for job history files. \n3. Contains WI for viewing Jobtracker history. \n\nThis patch depends on Hadoop-263, it wont work otherwise. ", "Updating patch with latest trunk. Up for review. ", "Perhaps we should change the string constants in JobInfo.java to be an enum (or two), to get the compiler to do a bit more work for us?  Or, we could use a record i/o record and XML output.  This way serializers and deserializers would be written for us.  That's perhaps overkill, but it's worth thinking about.\n\nIn any case, we need to make sure it's easy for these files to evolve.  Adding and removing fields should be trivial, and processing old files that have obsolete or missing fields should also be natural.  Right now, each new field requires a number of coordinated changes.  Is there any way we can reduce these?", "yes, we can use enums in JobInfo and other classes and use String values to store in files so its still human readable. I will make this change. \nXML may not be a good option as history is written in append mode, at any point of time the file is consistent if JT crashes. In case of XML wither we write whole file again with every change or a JT crash will leave it malformed, thats the only reason I didnt use XML. \nI will think over simplifying evolving this file and get back again. ", "Here are some new changes ( will submit patch in a while ) \n\nMoved code to Java 5\n\nReplaced all data attributes in JobInfo and other objects with HashMap. All key-value pairs are accessed through get(KEY), set(Key, Value) methods. All keys are defined in a single Enum, which acts like a global namespace for all keys used in job history. \n\nAdding new keys/values is now fairly simple, they need to be added to the Enum. after that these can be read/written without making any change in JobHistory implementation. \n\ndoes this sound reasonable?", "> does this sound reasonable?\n\nYes, it sounds great to me!  Thanks!", "Here is an updated patch. ", "This is looking good, but the enum class names are all caps when they should be CamelCase. (So Keys instead of KEYS and Values instead of VALUES.)", "thanks Owen\nI changed the casing for enums, here is the updated patch, ", "I just committed this.  Thanks, Sanjay!"], "derived": {"summary": "The jobtracker's WI, keeps track of jobs executed in the past 24 hours. if the cluster was idle for a day (say Sunday) it drops all its history.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "job tracker WI drops jobs after 24 hours - The jobtracker's WI, keeps track of jobs executed in the past 24 hours. if the cluster was idle for a day (say Sunday) it drops all its history."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Sanjay!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-240", "title": "namenode should not log failed mkdirs at warning level", "status": "Closed", "priority": "Minor", "reporter": "Hairong Kuang", "assignee": "Hairong Kuang", "labels": [], "created": "2006-05-24T00:20:58.000+0000", "updated": "2009-07-08T16:41:55.000+0000", "description": "when a namenode creates a directory, it also recursively tries to creates its parent directories. If they already exist, the lastSuccess is  false and the \"error\" is logged at the warning level.\n\nThe right approach is to set the logging leve lower to fine or simply ignore the the \"error\".", "comments": ["This patch fixes the problem by removing the logging when a namenode \"fails\" to create a parent directory. It also adjusts the logging level of some logs.", "On a second thought, the right approach to this problem should distiguish different cases when create a directory and then log them differently. When the mkdirs function  is called, either the directory is successfully created, or the directory already existed, or the directory is not able to created due to its parent's problem.\n\nCurrently FSDirectory.addNode returns null when a directory exists or a directory is not able to be created. I am thinking to add a PathNotFound exception. When a directory is not able to be created, addNode throws a PathNotFound exception. When a directory exists, it returns null. Otherwise, returns a new node.\n\nWhen the function mkdirs catches a PathNotFound exception, it logs it as a failure and returns. When it gets a null, simply continues.", "Just attached a new patch to this issue. Sorry that I forgot to log in.", "I'd like to apply this now, but it has conflicts.  I'll try to work through them...", "Sorry.  I am unable to get these into 0.3.1, they'll have to wait for 0.4.", "Doug,  I rebuilt the patch and here it is. I hope that you are able to apply it.", "I just committed this.  Thanks, Hairong."], "derived": {"summary": "when a namenode creates a directory, it also recursively tries to creates its parent directories. If they already exist, the lastSuccess is  false and the \"error\" is logged at the warning level.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "namenode should not log failed mkdirs at warning level - when a namenode creates a directory, it also recursively tries to creates its parent directories. If they already exist, the lastSuccess is  false and the \"error\" is logged at the warning level."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Hairong."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-241", "title": "TestCopyFiles fails under cygwin due to incorrect path", "status": "Closed", "priority": "Minor", "reporter": "Konstantin Shvachko", "assignee": "Milind Barve", "labels": [], "created": "2006-05-24T01:22:23.000+0000", "updated": "2006-08-03T17:46:43.000+0000", "description": "Under cygwin TestCopyFiles generates an incorrect url which includes windows style path.\nThis is the result of concatenation of a win path with unix path.\nFile.getPath() should be used to produce a consistent path.\n", "comments": ["First bug was in the test iself, which was using string instead of Path to construct a path. That was fixed. The second bug was that when Namenode was made to stop from MiniDFSCluster, it was correctly closing FSNamesystem, but it was not closing the FSDirectory. As a result the editlog was never getting closed. On Windows, when it was trying to create the MiniDFSCluster again, it could not format the namedir because the editlog was still not closed. Fixed it.", "I just committed this.  Thanks, Milind."], "derived": {"summary": "Under cygwin TestCopyFiles generates an incorrect url which includes windows style path. This is the result of concatenation of a win path with unix path.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "TestCopyFiles fails under cygwin due to incorrect path - Under cygwin TestCopyFiles generates an incorrect url which includes windows style path. This is the result of concatenation of a win path with unix path."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Milind."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-242", "title": "job fails because of \"No valid local directories in property: \" exception", "status": "Closed", "priority": "Major", "reporter": "Yoram Arnon", "assignee": "Owen O'Malley", "labels": [], "created": "2006-05-24T02:13:55.000+0000", "updated": "2009-07-08T16:51:49.000+0000", "description": "when running a fairly large job, of 70+K map tasks, I get many exceptions as shown below, and eventually the job failes when a task fails four times.\nThe exception doesn't really tell us enough information to debug this properly, so the first thing to do would be to add more information (path) to the exception.\nThe path indicated in the config file exists, is writable and valid, though 'path' may be anything.\n\nthe exception:\njava.io.IOException: No valid local directories in property: mapred.local.dir at org.apache.hadoop.conf.Configuration.getLocalPath(Configuration.java:293) at org.apache.hadoop.mapred.JobConf.getLocalPath(JobConf.java:153) at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.localizeTask(TaskTracker.java:523) at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.launchTask(TaskTracker.java:572) at org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:389) at org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:303) at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:418) at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:920)\n\nthe code:\n  public Path getLocalPath(String dirsProp, String path)\n    throws IOException {\n    String[] dirs = getStrings(dirsProp);\n    int hashCode = path.hashCode();\n    FileSystem fs = FileSystem.getNamed(\"local\", this);\n    for (int i = 0; i < dirs.length; i++) {  // try each local dir\n      int index = (hashCode+i & Integer.MAX_VALUE) % dirs.length;\n      Path file = new Path(dirs[index], path);\n      Path dir = file.getParent();\n      if (fs.exists(dir) || fs.mkdirs(dir)) {\n        return file;\n      }\n    }\n    throw new IOException(\"No valid local directories in property: \"+dirsProp);\n  }\n", "comments": ["perhaps we should not record failure until a job has been tried on two nodes?  Then we could collect stats on frequency that a job fails on a given node.\n\nAlso 4 failures seems odd.  Shouldn't we just have a configurable allowable % failure?  That will scale evenly.", "It is 4 failures of the _same_ fragment of work. So it takes map-0000000 failing 4 times before the job is killed. The job tracker does know which machines that each task has failed on and prefers to run other tasks on that node.", "Here is a patch that increases the logging associated with this failure. It also tries to switch the condition around based on the possibility of multiple processes or threads. So it does:\n\nfs.mkdirs(dir) || fs.exists(dir)\n\ninstead of:\n\nfs.exists(dir) || fs.mkdirs(dir)", "I just committed this.  Thanks, Owen.", "My patch didn't fix the problem, so I need to track this one down. ", "I think the fix for HADOOP-277 fixed this one. We've seen it since, but only when the local dir was running out of space."], "derived": {"summary": "when running a fairly large job, of 70+K map tasks, I get many exceptions as shown below, and eventually the job failes when a task fails four times. The exception doesn't really tell us enough information to debug this properly, so the first thing to do would be to add more information (path) to the exception.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "job fails because of \"No valid local directories in property: \" exception - when running a fairly large job, of 70+K map tasks, I get many exceptions as shown below, and eventually the job failes when a task fails four times. The exception doesn't really tell us enough information to debug this properly, so the first thing to do would be to add more information (path) to the exception."}, {"q": "What updates or decisions were made in the discussion?", "a": "I think the fix for HADOOP-277 fixed this one. We've seen it since, but only when the local dir was running out of space."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-243", "title": "WI shows progress as 100.00% before actual completion (rounding error)", "status": "Closed", "priority": "Trivial", "reporter": "Yoram Arnon", "assignee": "Owen O'Malley", "labels": [], "created": "2006-05-24T02:17:46.000+0000", "updated": "2009-07-08T16:51:49.000+0000", "description": "For jobs of over 50000 tasks, the rounding error in the WI is confusing.\nWhen less than 0.005% of the map (or reduce) tasks remain to execute, the WI shows progress as 100.00%, which is misleading.\nRounding down to the nearest .01% would be better.", "comments": ["This patch creates a new method in StringUtils for formating percentages and uses it in all of the progress fields in the web/ui. The method displays the percentage as \"0.00%\" with values rounded down.\n\nTo ensure that complete jobs show up as 100.00% instead of 99.99%, I also needed to set the progress to 1.0f explicitly to avoid floating point approximation problems. To avoid the problem with late updates pushing the progress over 1.0, I made the fields private and forced all users to go through the setters, which guarantee progress stays within 0.0 to 1.0.", "I just committed this.  Thanks, Owen!", "\n   [[ Old comment, sent by email on Tue, 12 Sep 2006 10:10:55 -0700 ]]\n\nMuchos gracias\nYoram\n\n"], "derived": {"summary": "For jobs of over 50000 tasks, the rounding error in the WI is confusing. When less than 0.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "WI shows progress as 100.00% before actual completion (rounding error) - For jobs of over 50000 tasks, the rounding error in the WI is confusing. When less than 0."}, {"q": "What updates or decisions were made in the discussion?", "a": "[[ Old comment, sent by email on Tue, 12 Sep 2006 10:10:55 -0700 ]]\n\nMuchos gracias\nYoram"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-244", "title": "very long cleanup after a job fails", "status": "Closed", "priority": "Major", "reporter": "Yoram Arnon", "assignee": "Sameer Paranjpye", "labels": [], "created": "2006-05-24T02:24:52.000+0000", "updated": "2009-07-08T16:51:48.000+0000", "description": "Eight hours after a job failed (it executed for about 14 hours prior to failing), many task trackers keep throwing the exceptions below:\n\n060523 121732 Server handler 0 on 50040 caught: java.io.FileNotFoundException: LocalFS\njava.io.FileNotFoundException: LocalFS\n        at org.apache.hadoop.fs.LocalFileSystem.openRaw(LocalFileSystem.java:123)\n        at org.apache.hadoop.fs.FSDataInputStream$Checker.<init>(FSDataInputStream.java:46)\n        at org.apache.hadoop.fs.FSDataInputStream.<init>(FSDataInputStream.java:228)\n        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:157)\n        at org.apache.hadoop.mapred.MapOutputFile.write(MapOutputFile.java:116)\n        at org.apache.hadoop.io.ObjectWritable.writeObject(ObjectWritable.java:151)\n        at org.apache.hadoop.io.ObjectWritable.write(ObjectWritable.java:64)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:230)\n060523 121814 task_0006_r_000123_0 copy failed: task_0006_m_046105_0 from node5:50040\njava.net.SocketTimeoutException: timed out waiting for rpc response\n        at org.apache.hadoop.ipc.Client.call(Client.java:305)\n        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:150)\n        at org.apache.hadoop.mapred.$Proxy2.getFile(Unknown Source)\n        at org.apache.hadoop.mapred.ReduceTaskRunner.prepare(ReduceTaskRunner.java:112)\n        at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:67)\n060523 121814 task_0006_r_000123_0 0.13023989% reduce > copy > task_0006_m_046105_0@node5:50040\n060523 121814 task_0006_r_000123_0 Copying task_0006_m_048815_0 output from node6\n060523 121817 SEVERE Can't open map output:/hadoop/mapred/local/task_0006_m_031921_0/part-152.out\njava.io.FileNotFoundException: LocalFS\n        at org.apache.hadoop.fs.LocalFileSystem.openRaw(LocalFileSystem.java:123)\n        at org.apache.hadoop.fs.FSDataInputStream$Checker.<init>(FSDataInputStream.java:46)\n        at org.apache.hadoop.fs.FSDataInputStream.<init>(FSDataInputStream.java:228)\n        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:157)\n        at org.apache.hadoop.mapred.MapOutputFile.write(MapOutputFile.java:116)\n        at org.apache.hadoop.io.ObjectWritable.writeObject(ObjectWritable.java:151)\n        at org.apache.hadoop.io.ObjectWritable.write(ObjectWritable.java:64)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:230)\n060523 121817 Unknown child with bad map output: task_0006_m_031921_0. Ignored.\n060523 121817 Server handler 1 on 50040 caught: java.io.FileNotFoundException: LocalFS\njava.io.FileNotFoundException: LocalFS\n        at org.apache.hadoop.fs.LocalFileSystem.openRaw(LocalFileSystem.java:123)\n        at org.apache.hadoop.fs.FSDataInputStream$Checker.<init>(FSDataInputStream.java:46)\n        at org.apache.hadoop.fs.FSDataInputStream.<init>(FSDataInputStream.java:228)\n        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:157)\n        at org.apache.hadoop.mapred.MapOutputFile.write(MapOutputFile.java:116)\n        at org.apache.hadoop.io.ObjectWritable.writeObject(ObjectWritable.java:151)\n        at org.apache.hadoop.io.ObjectWritable.write(ObjectWritable.java:64)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:230)\n060523 121914 task_0006_r_000123_0 copy failed: task_0006_m_048815_0 from node6:50040\njava.net.SocketTimeoutException: timed out waiting for rpc response\n        at org.apache.hadoop.ipc.Client.call(Client.java:305)\n        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:150)\n        at org.apache.hadoop.mapred.$Proxy2.getFile(Unknown Source)\n        at org.apache.hadoop.mapred.ReduceTaskRunner.prepare(ReduceTaskRunner.java:112)\n        at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:67)\n", "comments": ["Duplicate of HADOOP-225"], "derived": {"summary": "Eight hours after a job failed (it executed for about 14 hours prior to failing), many task trackers keep throwing the exceptions below:\n\n060523 121732 Server handler 0 on 50040 caught: java. io.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "very long cleanup after a job fails - Eight hours after a job failed (it executed for about 14 hours prior to failing), many task trackers keep throwing the exceptions below:\n\n060523 121732 Server handler 0 on 50040 caught: java. io."}, {"q": "What updates or decisions were made in the discussion?", "a": "Duplicate of HADOOP-225"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-245", "title": "record io translator doesn't strip path names", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Milind Barve", "labels": [], "created": "2006-05-24T03:41:58.000+0000", "updated": "2006-08-03T17:46:43.000+0000", "description": "When I run the record translator with a pathname, the path name is not stripped. So for example:\n\n% bin/rcc --language c++ foo/bar/bat.jr\n\ngenerates:\n\nfoo/bar/bat.jr.hh (instead of ./bat.jr.hh)\nand the first line is #ifndef __FOO/BAR/BAT_JR_HH__\n\nthe first was unexpected and the second is clearly wrong.", "comments": ["Fixed. This patch also includes fix for a bug HADOOP-246, plus much needed Javadoc in some parts of recordIO.", "I just committed this.  Thanks!"], "derived": {"summary": "When I run the record translator with a pathname, the path name is not stripped. So for example:\n\n% bin/rcc --language c++ foo/bar/bat.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "record io translator doesn't strip path names - When I run the record translator with a pathname, the path name is not stripped. So for example:\n\n% bin/rcc --language c++ foo/bar/bat."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-246", "title": "the record-io generated c++ has wrong comments", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Milind Barve", "labels": [], "created": "2006-05-24T03:45:07.000+0000", "updated": "2006-08-03T17:46:43.000+0000", "description": "The comments on the namespaces on the closing come out backward:\n\n} // end namespace org\n} // end namespace apache\n} // end namespace hadoop\n} // end namespace record\n} // end namespace test\n", "comments": ["It would also be nice if the namespaces weren't closed off just to be opened again...\n\n} // end namespace org\n} // end namespace apache\n} // end namespace hadoop\n} // end namespace record\n} // end namespace test\nnamespace org {\nnamespace apache {\nnamespace hadoop {\nnamespace record {\nnamespace test {\n", "I have attached a patch to hadoop-245 that includes a fix for this bug as well. Repeated opening and closing of namespaces does not cause any runtime inefficiency. (and probably very small compile-time inefficiency.) However it makes the code generator hairier than it needs to be. That is why I open and close namespaces for each record independently.", "Fix is part of patch in HADOOP-245"], "derived": {"summary": "The comments on the namespaces on the closing come out backward:\n\n} // end namespace org\n} // end namespace apache\n} // end namespace hadoop\n} // end namespace record\n} // end namespace test.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "the record-io generated c++ has wrong comments - The comments on the namespaces on the closing come out backward:\n\n} // end namespace org\n} // end namespace apache\n} // end namespace hadoop\n} // end namespace record\n} // end namespace test."}, {"q": "What updates or decisions were made in the discussion?", "a": "Fix is part of patch in HADOOP-245"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-247", "title": "The Reduce Task thread for reporting progress during the sort exits in case of any IOException", "status": "Closed", "priority": "Critical", "reporter": "Mahadev Konar", "assignee": "Mahadev Konar", "labels": [], "created": "2006-05-24T03:52:00.000+0000", "updated": "2009-07-08T16:51:48.000+0000", "description": "The Reduce task thread for reporting progress during the sort, exits in case of any exception (except InterruptedException). The solution would be to continue the thread in case of an exception.", "comments": ["This patch makes the thread exit on an InterruptedException and continues on a Throwable().", "Modified the patch so it follows 80 character lines.", "I just committed this.  Thanks, Mahadev!"], "derived": {"summary": "The Reduce task thread for reporting progress during the sort, exits in case of any exception (except InterruptedException). The solution would be to continue the thread in case of an exception.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "The Reduce Task thread for reporting progress during the sort exits in case of any IOException - The Reduce task thread for reporting progress during the sort, exits in case of any exception (except InterruptedException). The solution would be to continue the thread in case of an exception."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Mahadev!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-248", "title": "locating map outputs via random probing is inefficient", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Devaraj Das", "labels": [], "created": "2006-05-24T06:25:33.000+0000", "updated": "2013-05-02T02:29:03.000+0000", "description": "Currently the ReduceTaskRunner polls the JobTracker for a random list of map tasks asking for their output locations. It would be better if the JobTracker kept an ordered log and the interface was changed to:\n\nclass MapLocationResults {\n   public int getTimestamp();\n   public MapOutputLocation[] getLocations();\n}\n\ninterface InterTrackerProtocol {\n  ...\n  MapLocationResults locateMapOutputs(int prevTimestamp);\n} \n\nwith the intention that each time a ReduceTaskRunner calls locateMapOutputs, it passes back the \"timestamp\" that it got from the previous result. That way, reduces can easily find the new MapOutputs. This should help the \"ramp up\" when the maps first start finishing.", "comments": ["Propose the following:\n\n1) Modify the \"TaskCompletionEvent[] getTaskCompletionEvents(String jobid, int fromEventId)\" defined in IntertrackerProtocol and JobSubmissionProtocol to have a new argument that will signify how many events we want to fetch. We may get a smaller number depending on how many events got registered for the job. \nSo, it becomes: TaskCompletionEvent[] getTaskCompletionEvents(String jobid, int fromEventId, int maxEvents)\nThis will generally be more scalable. In the case of map-output-fetches, it helps in the way that we do the same thing as we do today (except that the randomness is not there and the TT exactly knows which maps finished).\n\n2) Since the events IDs are numbered in a monotonically increasing sequence for a Job, we don't need to maintain timestamps (as the original comment on this bug suggests).\n\n3) Add a \"boolean isMapTask()\" method to TaskCompletionEvent class that will return true if the event is from a map task, false otherwise.\n\nComments?", "Sounds good. How do we deal with map failures? Do we get multiple completion events when there are re-tries?  ", "Sounds good, Devaraj.\n\nThe events are per a taskid, not a tipid. So different attempts to run \"map 0\" would result in different events.\n\nThat said, however, we probably should make another event \"lost\" or something for tasks that are lost because their output had problems or the task tracker was lost.\n\nWe may also want to flag the \"complete\" events of lost tasks as obsolete so that reduces don't see them and try and fetch their outputs.", "The patch does the following:\n1) Modifies the protocol version for InterTrackerProtocol since there is a major change there in the way map output is fetched. The method locateMapOutputs has been removed.\n2) The getTaskCompletion method in both InterTrackerProtocol and JobSubmission has been changed to take an extra argument - the max no. of events we want to fetch from the JobTracker.\n3) Two more fields are added in TaskCompletionEvents - the real-ID portion of the taskId string (for e.g., if the taskId is task_0001_m_000003_0, the real-ID is 3 within the job), and another boolean field to indicate whether the task is a map or not. By the way, these could have done on the Reduce side also by parsing the taskId string, but I think this is a more general way of doing it and it also is in line with the thought of having \"ID objects\" in the future.\n4) Only 10 events are fetched at a time by the JobClient\n5) A new value OBSELETE has been added to TaskCompletionEvent.Status to signify lost tasks (which were earlier reported as SUCCEEDED). For this, whenever a FAILED/KILLED TaskStatus is got, it is checked whether a SUCCEEDED was earlier reported for the same taskId, and if so that event is marked as OBSELETE.\n6) The number of events probed by the ReduceTaskRunner at any time is equal to max(5*numCopiers, 50).\n\nFeedback appreciated..", "This patch does better failure handling. It saves the location of a failed map output fetch in a hashmap for later retrial. Also, it has logic which makes it prefer a newer location to the saved one.", "+1, because http://issues.apache.org/jira/secure/attachment/12349617/248-initial8.patch applied and successfully tested against trunk revision r499156.", "This is a minor modification of Devaraj's patch that fixes a spelling mistake (OBSELETE) and use TaskInProgress.partition instead of parsing the task id.", "+1, because http://issues.apache.org/jira/secure/attachment/12349653/248-9.patch applied and successfully tested against trunk revision r500023.", "I just committed this.  Thanks, Devaraj!", "I just reverted this, since it was causing things to hang.", "This had a problem introduced unintentionally in the last submission (by Owen, when he corrected the spelling of OBSOLETE, etc.). The problem was that there is a variable called fromEventId which is used to track from which eventId a tasktracker should fetch events from from the jobtracker. This was earlier a IntWritable object, so that set(<somenumber>) could be done on the object and the new value of the 'int' within the object could be seen even when the method invocation returned. This variable was changed to an int and instead \"fromEventId += <somenumber>\" was done. Unfortunately, this would not be visible when the method invocation returned and hence the TaskTracker would get stuck at a particular eventId and would make no forward progress...\nAttached is the new patch which has the IntWritable stuff put back in, and also the method JobClient.listEvents has been modified to take two extra args - fromEventId, numEvents (this method didn't exist when I was earlier working on this issue). The JobSubmissionProtocol version has been changed also to reflect the change in the getTaskCompletionEvents protocol method (missed this in the earlier patch).", "I just committed this.  Thanks, Devaraj!", "-1, because the patch command could not apply the latest attachment (http://issues.apache.org/jira/secure/attachment/12351812/248-fixed1.patch) as a patch to trunk revision r510644. Please note that this message is automatically generated and may represent a problem with the automation system and not the patch. Results are at http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch", "Sorry, ignore Hadoop QA's -1."], "derived": {"summary": "Currently the ReduceTaskRunner polls the JobTracker for a random list of map tasks asking for their output locations. It would be better if the JobTracker kept an ordered log and the interface was changed to:\n\nclass MapLocationResults {\n   public int getTimestamp();\n   public MapOutputLocation[] getLocations();\n}\n\ninterface InterTrackerProtocol {.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "locating map outputs via random probing is inefficient - Currently the ReduceTaskRunner polls the JobTracker for a random list of map tasks asking for their output locations. It would be better if the JobTracker kept an ordered log and the interface was changed to:\n\nclass MapLocationResults {\n   public int getTimestamp();\n   public MapOutputLocation[] getLocations();\n}\n\ninterface InterTrackerProtocol {."}, {"q": "What updates or decisions were made in the discussion?", "a": "Sorry, ignore Hadoop QA's -1."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-249", "title": "Improving Map -> Reduce performance and Task JVM reuse", "status": "Closed", "priority": "Major", "reporter": "Benjamin Reed", "assignee": "Devaraj Das", "labels": [], "created": "2006-05-24T07:50:20.000+0000", "updated": "2010-10-12T12:13:07.000+0000", "description": "These patches are really just to make Hadoop start trotting. It is still at least an order of magnitude slower than it should be, but I think these patches are a good start.\n\nI've created two patches for clarity. They are not independent, but could easily be made so.\n\nThe disk-zoom patch is a performance trifecta: less disk IO, less disk space, less CPU, and overall a tremendous improvement. The patch is based on the following observation: every piece of data from a map hits the disk once on the mapper, and 3 (+plus sorting) times on the reducer. Further, the entire input for the reduce step is sorted together maximizing the sort time. This patch causes:\n\n1)  the mapper to sort the relatively small fragments at the mapper which causes two hits to the disk, but they are smaller files.\n2) the reducer copies the map output and may merge (if more than 100 outputs are present) with a couple of other outputs at copy time. No sorting is done since the map outputs are sorted.\n3) the reducer  will merge the map outputs on the fly in memory at reduce time.\n\nI'm attaching the performance graph (with just the disk-zoom patch) to show the results. This benchmark uses a random input and null output to remove any DFS performance influences. The cluster of 49 machines I was running on had limited disk space, so I was only able to run to a certain size on unmodified Hadoop. With the patch we use 1/3 the amount of disk space.\n\nThe second patch allows the task tracker to reuse processes to avoid the over-head of starting the JVM. While JVM startup is relatively fast, restarting a Task causes disk IO and DFS operations that have a negative impact on the rest of the system. When a Task finishes, rather than exiting, it reads the next task to run from stdin. We still isolate the Task runtime from TaskTracker, but we only pay the startup penalty once.\n\nThis second patch also fixes two performance issues not related to JVM reuse. (The reuse just makes the problems glaring.) First, the JobTracker counts all jobs not just the running jobs to decide the load on a tracker. Second, the TaskTracker should really ask for a new Task as soon as one finishes rather than wait the 10 secs.\n\nI've been benchmarking the code alot, but I don't have access to a really good cluster to try the code out on, so please treat it as experimental. I would love to feedback.\n\nThere is another obvious thing to change: ReduceTasks should start after the first batch of MapTasks complete, so that 1) they have something to do, and 2) they are running on the fastest machines.", "comments": ["I have not yet had a chance to look closely at your patches, but these are clearly optimizations that we badly need.  Thanks!", "What happened to these patches?\n\nI'm especially interested in the jvm reuse since we not only run large jobs but also a lot of small ones where the setup time really kills performance", "Most of these things except for the jvm reuse have been done in other patches. I've only left this bug open because of the jvm reuse issues, but the patch is currently useless because the code has changed so much in the last year. In fact, it would be relatively tricky to get right with the capturing of stdout/stderr from the tasks.", "Wondering if we can have one jvm per job and have tasks as threads spawned from the job jvm? ", "No, threads within a single jvm has no major advantage over multiple jvms and has significant disadvantages. I feel that the best trade off is having each jvm running a single task at a time, but reusing the jvm for the following task.", "In my use case, we'd like to load a significant amount of data into memory at the start of a task run.  Sharing 10Gb of read-only data in a single JVM running on a 8-16 processor machine.  This would allow for lower cost and more throughput, otherwise we are redundantly loading this significant amount of data and therefore limited by memory rather than CPUs. \n\nMy suggestion would be to provide the option and let the user choose based on their needs.  If you're running \"untrusted\" processes on the cluster, you can run each task in an independent JVM.  If you're looking to maximize throughput and memory and you are running trusted processes, you should be able to specify # of threads per Task.\n\nIn my current situation, I'm only able to make use of 4 CPUs on 8 CPU machines because each task requires 10Gb to run, the majority of that being read-only data loaded from distributed cache.", "To be more clear, my suggestion would be to add parameters \"mapred.map.tasks.threads\" and \"mapred.reduce.tasks.threads\" and let people set the number of threads to spawn per task, and let them default to 1.\n\n", "Reducing the number of JVMs spawned should be a high priority.  It is a tremendous amount of overhead for low-cpu usage jobs.", "Another issue is that the JVM is continuously optimizing your code, when you restart the VM you lose that information and it has to be reconstructed in the next run.  CPU bound processes would definitely benefit from having the VM stay around for the length of the job.", "Would this make sense, under some conditions (e.g. tasks code is trusted), to allow the tasks to run directly inside the task tracker jvm?\nI actually had some scenario where the amount of memory available was critically low!\n", "A good discussion on the JVM reuse issue happened on HADOOP-3675. The links :\nhttp://issues.apache.org/jira/browse/HADOOP-3675?focusedCommentId=12619448#action_12619448  through http://issues.apache.org/jira/browse/HADOOP-3675?focusedCommentId=12619775#action_12619775 capture this.", "sorry it was mistake... ", "After a discussion with Owen on the logs issue, it seems the approach where we write everything to three files (stdout/err/log) with an index file containing offsets per task should work. Thoughts?", "> the approach where we write everything to three files (stdout/err/log) with an index file containing offsets per task should work.\n\nSeems like a good idea. Would the Java process write the index file? If so, it would have to count bytes, but that should be doable.\n\nI'm not sure what the cause of the performance problem that led to the stream handling being done outside Java (HADOOP-1553), but is it worth revisiting to see if a Java approach is as fast? It would certainly be simpler.", "bq.  the approach where we write everything to three files (stdout/err/log) with an index file containing offsets per task should work.\n\nThis has problems in some cases : processes (forked by streaming/pipes tasks) that live beyond the life time of the task process pollute the stdout/err/log files, in that they might continue to write to these files even after we mark the end of data from one task. In the current setup they inherit parent task's file descriptors and continue to write to the same files and hence pose no problem.\n\nbq. I'm not sure what the cause of the performance problem that led to the stream handling being done outside Java (HADOOP-1553), but is it worth revisiting to see if a Java approach is as fast? It would certainly be simpler.\n+1", "{quote}\nprocesses (forked by streaming/pipes tasks) that live beyond the life time of the task process pollute the stdout/err/log files\n{quote}\nThis is a general problem with combining tasks into a single JVM and I don't believe there any solutions. Take for example, a Mapper that launches a thread that is not joined in the close method. The output will split across the tasks.\n\n{quote}\nI'm not sure what the cause of the performance problem that led to the stream handling being done outside Java (HADOOP-1553), but is it worth revisiting to see if a Java approach is as fast? It would certainly be simpler.\n{quote}\nI'm confused. Pre HADOOP-1553, the TaskTracker copied the task's stdout and stderr \"by hand\" to a file. Since you only get a new pair of streams when you start a new JVM, backing out HADOOP-1553 wouldn't help. You would still need to guess where the boundary was.", "> You would still need to guess where the boundary was.\n\nI was thinking that you would call System.setOut() and System.setErr() for each new task, so that each stream was redirected to a new file. (Of course, if the task hasn't finished producing output, it will go into the wrong output file. But this is a general problem, as you point out.) But if the overhead for copying the process output to a file in Java is so high then we should do the stream handling outside Java and track offsets.", "I was thinking that you could use an OutputStream with ThreadLocal OutputStream within it, and use these with System.setOut() and System.setErr(). This would allow multiple threads to have their own output and error streams. Not sure how this would work with pipes though.", "Will the System.setOut/setErr approach work for the case where the parent Java task spawns child processes (that do printf(), etc.) ? Will the spawned children also see those as their stdout/err?", "No it doesn't work. It also doesn't work if you use jni code in your application that calls printf. When I was doing HADOOP-1553, I did consider that possibility, but there were too many cases that it does *not* handle. In the case of this patch, it would still not work with threads that weren't finished when the task switch was done. They would be counted as output from the new task instead of the old one.", "\nIf we implement hadoop 2560, then jvm resuse becomes a less urgent issue.\n", "I agree with Runping that HADOOP-2560 has more far reaching effects.", "Sure, but re-using the jvm is the first step for HADOOP-2560. There is a lot of work to implement grouping of maps. In particular, you need to deal with task failures, changes to the shuffle and task event log, interactions with speculative execution and so on. On the other hand, to group mappers you need to reuse the jvm, otherwise you can't join their outputs together.", "\nHADOOP-2560 does not need JVM re-use.\nIt just needs mapper tasks can work with multiple splits.\nEverything else reminds the same.\n\n", "Here is an early untested patch (the major missing part is the handling of the logs, and some cleanup needs to be done). This is up for an early review.\n\nThe major change here is the addition of a new class called JvmManager that does the bookkeeping of the JVMs spawned by the tasktracker.", "This has some fixes. Logging not yet done though.", "The data-structures have been simplified in this patch. Also, this has been tested a fair amount.", "The loadgen benchmark ran in the following way:\n{code}\nbin/hadoop jar hadoop-0.19.0-dev/hadoop-0.19.0-dev-test.jar loadgen \\\n   -D test.randomtextwrite.bytes_per_map=$((100)) \\\n   -D test.randomtextwrite.total_bytes=$((100*100000)) \\\n   -D mapred.compress.map.output=false \\\n   -r 1 \\\n   -outKey org.apache.hadoop.io.Text \\\n   -outValue org.apache.hadoop.io.Text \\\n   -outFormat org.apache.hadoop.mapred.lib.NullOutputFormat \\\n   -outdir fakeout\n{code}\n\n(that is 100K mappers each generating 100 bytes of data), run on 100 nodes with 4 mapper slots each, showed *50%* improvement for the map-phase. ", "\n\nGenerating only 100 bytes, each mapper is essentially doing nothing.\nSo you are basically testing the costs of jvm launches and task assignment.\nThus, I would expect much better improvement (say 10x or more).\nWith 2x improvement, it implies that the cost for task assignment is significant, and in your \ncase, was a bottleneck.\n\nHow long did the shuffling take?\n \n", "Yes, the aim of this benchmark was to see how large jobs with small tasks would be affected. But as discussed in this jira already, there are other cases where jvm reuse could prove useful. The shuffling in both cases took ~30 minutes. ", "Here is a patch that addresses the task logs issue. In short the patch has the following:\n1) Addition of queues in the tasktracker for maps and reduces where a new task is queued up. When a slot becomes free a task is pulled from one of the relevant queues and initialized/launched \n2) When a tasktracker reports with tasks in COMMIT_PENDING status, it could be given a new task. This new task would be queued up at the tasktracker and launched only when the task doing the commit finishes. \n3) JvmManager is the class that keeps track of running JVMs and tasks running on the JVMs. When a JVM exits the appropriate task (if any was being run by the JVM) is failed. \n4) New task state has been added for signifying INITIALIZED and ready to run. Till a task is initialized (like task locallization, distributed cache init, etc.,) a JVM is not given that. \n5) The log handling is done this way:\n   - I have a small indirection file, which I call index.log, in each attempt dir. Let's say that the JVM for attempt_1_1\n     also runs attempt_foo_bar. The index.log file in the directory attempt_foo_bar would look like: \n         LOG_DIR: attempt_1_1\n         STDOUT: <start of stdout for attempt_foo_bar in the stdout file in attempt_1_1 directory> <length>\n         SYSLOG: <similar to above>\n         STDERR: <similar to above>\n   - I create this log.index file at task startup, and have a thread to continously update the lengths of the log files\n     so that one can see the logs of a running task.\n   - I modified tools like TaskLog.Reader to go through this indirection when someone asks for the log files.      \n6) A new class JVMId has been created to create/identify JVM IDs instead of plain integers. (Arun had requested for this offline)\n\nHave tested large sort with this patch.\n\nThe only thing remaining is the handling of the workDir for tasks (that has things like Distributed Cache symlinks). I am going to address that in the next patch very shortly.\n\nWould really appreciate a review on this one.", "I'm spent a fair bit of time on this, it's looking good - some comments:\n\n# JvmManager.getJvmManagerInstance isn't thread-safe.\n# JvmManagerForType.{taskFinished|taskKilled|killJvm} - silently errors are ignored i.e. all the != null checks, at least log them?\n# JvmManagerForType.reapJvm\n#* Minor: Iteration could be cleaner\n#* What if numJvmsSpawned >= maxJvms and all are busy and belong to different jobs? i.e should we at least log that?\n# JVMId should probably have a static counter for the 'id' rather than use hashCode().\n# TaskRunner.exitCode should probably be initialized to -1, definitely not 0.\n# TaskTracker:\n#* Can we make {map|reduce}TasksToLaunch internal to {map|reduce}Launcher? Ditto for numFree{Map|Reduce}Slots? Basically make addToTaskQueue and addFreeSlot methods on the TaskLauncher.\n#* Sanity checks in addFree{Map|Reduce}Slot ?\n#* TaskTracker.initialize should reset numFree{Map|Reduce}Slots to handle the case when the TaskTracker bounces?\n# Minor: TaskTracker.Child - Good time to move it to a different file?\n\nAs an aside: How about removing the {map|reduce}Launcher threads and getting TaskTracker.getTask to pull it off the queue, initialize task and then returning it to the Child? Thoughts?\n\n", "Thanks Arun for the review. Most of them can be addressed without much debate. However, I am not sure about the last comment, i.e., getTask RPC doing the task initialization. Currently, most of the initialization is done in a separate TaskRunner thread and TaskTracker.getTask just returns an initialized task. If we move the initialization to a separate thread, I am worried that if the task initialization takes a long time (since there is communication with the DFS involved as well), then the RPC handlers get blocked (depending on how many JVMs ask for tasks). Also, there is the issue of task cleanup that in my current patch is done in the same TaskRunner thread. That has to be done inline too (or in a separate thread launched when the task finishes) if we remove the taskrunner thread. \nAlso, in the current patch, the JVM is launched (if needed) in the same TaskRunner thread. We'd also need to think about where to launch the JVM in the first place so that it starts making the getTask RPCs.\nFor the above reasons, I propose to keep the existing model but I am happy to discuss this out. Also, I think having launcher threads sets us up logically for the case where the TT queues up many more tasks than there are slots. What do others think?", "Guys, just a quick comment. Unless it has been modified recently, S3FileSystem's store will cache to $hadoop.tmp.dir/s3 the blocks read from S3 and remove them only at JVM shutdown. This is not a major issue if each map task is run under a new JVM but it might become an issue if the JVM is re-used. We often experience problems with disk usage when running an Hadoop client that reads a lot of data from S3 and we had to patch it to get around it.", "Another corner case:\n\nTaskTracker.getTask should validate that the JVM identified by JVMId was _really_ launched by the _current_ JVMManager, currently it just checks the JobId. This probably means that the JVMId.id should be a function of the current timestamp to ensure it's unique across TaskTracker restarts (and hence it invalidates my previous comment about using a static counter in JVMId...).\n\nbq. For the above reasons, I propose to keep the existing model but I am happy to discuss this out. Also, I think having launcher threads sets us up logically for the case where the TT queues up many more tasks than there are slots. What do others think?\n\nActually to be clear, I'm not proposing to get rid of TaskRunner thread, just the TaskLauncher threads; this will ensure we do task-initialization on demand... but I'm happy to just throw this up and get feedback from others! Owen?", "Good point. I hadn't considered the tasktracker restart case. \nThere are some other corner cases:\n1) I need to make sure that the JVM really gets to execute that task it was spawned for. This is required so that things like keep.failed.task.files works. Although I do disable jvm reuse for such cases, the thing is it might still be possible that the spawned JVM gets to run some other queued task. So to handle this, I need to pass the firstTaskId in the getTask RPC which will return the task if it finds it. This will guarantee that when jvm reuse is disabled the JVM executes that and only that task for which it was spawned.\n2) A JVM is spawned but the task which it was spawned for happened to be executed by some other JVM just before the new JVM was fully initialized. This new JVM would just stay in memory since the busy flag is set to true in the JvmRunner initialization. To handle this I need to introduce a JVM_INITIALIZING state to distinguish between busy and initializing JVMs (initializing JVMs are not killed when the TT looks for a JVM to purge).\n\nRegarding the task initialization, it happens on demand even in the patch. It's just that the RPCs are kept away from task init/cleanup. Owen?", "Ignore my second comment on jvm initialization. I handled it a bit differently with the busy flag itself.", "Attached is a patch with all the review comments incorporated (Arun's and Amareshwari's). This patch might have some findbugs warnings (showed up when test-patch was run locally) but it seems unrelated to my patch.", "Pushing through hudson", "This is looking good. +1.\n\nIf we can get the output of 'ant test-patch' this is good to go! *smile*", "Attached is the output of test-patch\n\n[exec]\n     [exec] -1 overall.\n     [exec]\n     [exec]     +1 @author.  The patch does not contain any @author tags.\n     [exec]\n     [exec]     +1 tests included.  The patch appears to include 3 new \nor modified tests.\n     [exec]\n     [exec]     +1 javadoc.  The javadoc tool did not generate any \nwarning messages.\n     [exec]\n     [exec]     +1 javac.  The applied patch does not increase the total \nnumber of javac compiler warnings.\n     [exec]\n     [exec]     -1 findbugs.  The patch appears to introduce 1 new \nFindbugs warnings.\n     [exec]\n     [exec]\n     [exec]\n\nThe findbugs warning is due to the System.exit call I make in the Child.java. The calls are required.\nThe attached patch fixes a javadoc warning and failures in two testcases. It passes all core/contrib tests on my local machine.", "I just committed this. Thanks, Devaraj!", "Integrated in Hadoop-trunk #611 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/611/])", " If this is 1 (the default), then JVMs are not reused (1 task per JVM). If it is -1, there is no limit to the number of tasks a JVM can run (of the same job). One can also specify some value greater than 1. Also a JobConf API has been added - setNumTasksToExecutePerJvm."], "derived": {"summary": "These patches are really just to make Hadoop start trotting. It is still at least an order of magnitude slower than it should be, but I think these patches are a good start.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "Improving Map -> Reduce performance and Task JVM reuse - These patches are really just to make Hadoop start trotting. It is still at least an order of magnitude slower than it should be, but I think these patches are a good start."}, {"q": "What updates or decisions were made in the discussion?", "a": "If this is 1 (the default), then JVMs are not reused (1 task per JVM). If it is -1, there is no limit to the number of tasks a JVM can run (of the same job). One can also specify some value greater than 1. Also a JobConf API has been added - setNumTasksToExecutePerJvm."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-250", "title": "HTTP Browsing interface for DFS Health/Status", "status": "Closed", "priority": "Major", "reporter": "Devaraj Das", "assignee": null, "labels": [], "created": "2006-05-24T12:22:28.000+0000", "updated": "2009-07-08T16:41:46.000+0000", "description": "A web interface to view the DFS health/status (name and data nodes) is to be created. User can connect to the webserver on the namenode and a web page will be displayed. The web page will give some details about that namenode (startup time and the total cluster capacity). The web page will contain a table of 'live' and 'dead' datanodes. Each live datanode will be a link to the complete details of the datanode as given by DatanodeInfo (also see DataNodeReport).", "comments": ["You should be able to use the StatusHttpServer class and add the necessary jsp files for the name node. That will also give you access to the log directories via the web server. It would also make sense to include the free/used disk space on each data node.", "Attaching a patch which implements the DFS HTTP browsing interface. The default port where the webserver (on the namenode) runs on is 50070. Also attached a screenshot of the webpage.", "The screen shot of the webpage", "This looks great.  However the method FSNameSystem.getAddress duplicates the logic of DataNode.createSocketAddr.  We should have a single static method for this, and it should probably take a Configuration parameter.  At a minimum, we should call that method rather than copying it.", "Thanks Doug for the comments. Attached is the revised patch where I call DataNode.createSocketAddr directly.", "I'd like to commit this, but the changes this requires to FSNamesystem.java now conflict with other changes to that file.  I can try to sort this out, but if someone else gets a chance first that would be great.  Thanks.", "Sorry.  I am unable to get these into 0.3.1, they'll have to wait for 0.4.", "Doug, created this patch again against the current trunk.", "Unit tests are now failing for me with this patch.  Can you please look into this?  Thanks!", "The problems here were mostly setup related (admit I should have run the unit tests before submitting the patch!). \nSince I start a StatusHttpServer in FSNameSystem class, the \"build\" directory should be included in the classpath (so webapps resource can be found) and hadoop.log.dir should be set, otherwise, StatusHttpServer cannot be constructed properly [updated patch patches build.xml file with these settings].\n\nAlso, the stop method of StatusHttpServer should be called every time the namenode is stopped, otherwise a subsequent test trying to create a NameNode object may fail since StatusHttpServer fails to open the port. \n\nAnother thing that causes some failures to happen (once in a while) is the sleep/timeout values in MiniDFSCluster.java. A NameNode is started and after sleeping for 1 sec, a datanode is started. But if the namenode hasn't fully come up by then, then RPCs will fail (the ipc.client.timeout of 1 sec seems to be too small). So I changed all 1000 msec to 2000 msec in MiniDFSCluster.java and the tests seem to be passing consistently after that. Updated patch is attached.", "Starting an HTTP server within the namenode may be the cause for a slightly longer namenode startup time.", "I just committed this.  I made two additional changes.  In FSNameSystem.java I moved the initialization of the static variable fsNameSystemObject to before where Jetty is started, since the jsp pages require a non-null value for this and the servlet init was failing with a null pointer exception.  I also added an entry to hadoop-default.xml for the new dfs.info.port parameter."], "derived": {"summary": "A web interface to view the DFS health/status (name and data nodes) is to be created. User can connect to the webserver on the namenode and a web page will be displayed.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "HTTP Browsing interface for DFS Health/Status - A web interface to view the DFS health/status (name and data nodes) is to be created. User can connect to the webserver on the namenode and a web page will be displayed."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  I made two additional changes.  In FSNameSystem.java I moved the initialization of the static variable fsNameSystemObject to before where Jetty is started, since the jsp pages require a non-null value for this and the servlet init was failing with a null pointer exception.  I also added an entry to hadoop-default.xml for the new dfs.info.port parameter."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-251", "title": "progress report failures kill task", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-05-25T04:41:12.000+0000", "updated": "2009-07-08T16:51:47.000+0000", "description": "Communication problems in reporting progress to the task tracker kill the task because the exceptions are not caught. Since reporting progress is not critical, my patch catches the exception, logs it, and ignores it.", "comments": ["I just committed this.  Thanks!"], "derived": {"summary": "Communication problems in reporting progress to the task tracker kill the task because the exceptions are not caught. Since reporting progress is not critical, my patch catches the exception, logs it, and ignores it.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "progress report failures kill task - Communication problems in reporting progress to the task tracker kill the task because the exceptions are not caught. Since reporting progress is not critical, my patch catches the exception, logs it, and ignores it."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-252", "title": "add versioning to RPC", "status": "Closed", "priority": "Major", "reporter": "Yoram Arnon", "assignee": null, "labels": [], "created": "2006-05-25T05:36:37.000+0000", "updated": "2006-08-04T22:22:31.000+0000", "description": "currently, any change to any rpc message breaks the protocol, with mysterious exceptions occurring at run time.\na versioning sceme would have two benefits:\n- intelligent error messages, indicating that an upgrade is required\n- backwards compatibility could be supported.\n\nthe proposal is to add a \"const version\" for each protocol, and a method: int getVersion(int version) that sends the client's version and receives the server's version. This would be the first method invoked on connection. Both sides then either agree on the lowest version number, providing backwards compatibility support, or abort the connection as \"unsupported version\".", "comments": ["One complication is that different RPC protocols share common connections, in order to minimize the number of per-connection threads and sockets.  So the versions would really need to be transmitted with each request, or perhaps with the first request of each protocol over a given connection.", "Associating this with a connection seems like a violation of abstraction to me.  Sending this info with every request is probably more appropriate.  Or for stateful protocols, you can do it in the setup phase.", "having a handshake when setting up a connection is fairly standard.\nBoth sides may either agree on the lowest common denominator, and send messages in a previously supported format, or decide to throw. It's better than a version in each message for two reasons:\n1(minor) a few bytes less per message\n2(major) allows a new component to avoid sending new messages to an old component, that doesn't support it.", "If we had connections dedicated to a protol, your argument would be well founded.  Since we don't, there is no place to hang the tests you describe.  Let's discuss.", "connections are dedicated to a protocol, in that a client connects to a server that's waiting on some port serving a single protocol.\na datanode only serves the datanode protocol. likewise namenode etc.\n\nthe idea is that the test is done after the tcp connection setup, as the first message passed between client and server. It's actually one more method of each protocol, identifying both sides' versions of the protocol. It would be part of the protocol; it's technically sound...", "The way the software is currently layered, it makes sense to version the base IPC connection, and perhaps also, in the RPC layer, to check the version of the proxy class with that of the server instance for compatibility.  In the latter case, we might use the same mechanism a RMI, (serialVersionUID static field, defaulting to a hash of method signatures).", "Maybe you should produce a one pager with a well thought out plan that covers all the corner cases?  Gather a couple of +1s and we can proceed.\n", "I have started thinking about this issue. Here is my proposal so far:\n\nThere are 6 protocols that use RPC in hadoop. One is in test, and 5 in source. These are interfaces that are implemented by servers. We modify these interfaces to inherit from a common interface, called VersionedProtocol. VersionedProtocol has exactly one method:\n\nlong getProtocolVersion()\n\nThe client tries to get a proxy via RPC.getProxy in the setup phase, and passes the required interface class.\nThis method, upon successfully obtaining the proxy, casts it to VersionedProtocol, calls its getProtocolVersion, compares it with the passed in interface's static final long versionID field, and fails if they do not match.\n\nI think we should not use computed-by-default serialVersionUID field, since it can change even if you sneeze at the class :-) Also, since we do not use serialization runtime to calculate this hash, we are not attached to the name as well. This version needs to change in case of any interface changes to the protocol.\n\nThe client, upon version mismatch, prints out an error, and exits cleanly. The server blissfully remains unaware of that.\n\nThe objective of this is not to handle backward compatibility, but to detect and fail fast and cleanly if there is a version mismatch between client and server.\n", "One minor change, the VersionedProtocol base interface contains exactly one method:\n\npublic long getProtocolVersion(String protocolClassName, long clientVersionID);\n\nThis is because servers that implement multiple protocols can return appropriate versionID's corresponding to the proxy.\nLater versions can also adapt their behavior according to the client version. If a server is adaptive, then it can return the less of client version and server version. (negotiation is left to the implementor of these adaptive protocols).\n\nCurrently, the server ignores the clientVersion, and returns its own version for the requested interface.\n\nI will submit a patch shortly.", "Added versioning to RPC mechanism.", "I just committed this.  Thanks, Milind!"], "derived": {"summary": "currently, any change to any rpc message breaks the protocol, with mysterious exceptions occurring at run time. a versioning sceme would have two benefits:\n- intelligent error messages, indicating that an upgrade is required\n- backwards compatibility could be supported.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "add versioning to RPC - currently, any change to any rpc message breaks the protocol, with mysterious exceptions occurring at run time. a versioning sceme would have two benefits:\n- intelligent error messages, indicating that an upgrade is required\n- backwards compatibility could be supported."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Milind!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-253", "title": "we need speculative execution for reduces", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-05-25T13:16:43.000+0000", "updated": "2009-07-08T16:51:46.000+0000", "description": "With my new http-based shuffle (on top of the svn head including sameer's parallel fetch), I just finished sorting 2010 g on 200 nodes in 8:49 with 9 reduce failures. However, the amusing part is that the replacement reduces were _not_ the slow ones. 8 of the original reduces were the only things running for the last hour. The job timings looked like:\n\nJob 0001\n  Total:\n    Tasks: 16551\n    Total: 10056104 secs\n    Average: 607 secs\n    Worst: task_0001_r_000291_0\n    Worst time: 31050 secs\n    Best: task_0001_m_013597_0\n    Best time: 20 secs\n  Maps:\n    Tasks: 16151\n    Total: 2762635 secs\n    Average: 171 secs\n    Worst: task_0001_m_002290_0\n    Worst time: 2663 secs\n    Best: task_0001_m_013597_0\n    Best time: 20 secs\n  Reduces:\n    Tasks: 400\n    Total: 7293469 secs\n    Average: 18233 secs\n    Worst: task_0001_r_000291_0\n    Worst time: 31050 secs\n    Best: task_0001_r_000263_1\n    Best time: 5591 secs\n\nAnd the number of tasks run per a node was very uneven:\n\n#tasks node\n124 node1161\n117 node1307\n117 node1124\n116 node1253\n114 node1310\n111 node1302\n111 node1299\n111 node1298\n111 node1249\n111 node1221\n110 node1288\n110 node1286\n110 node1211\n109 node1268\n108 node1292\n108 node1202\n108 node1200\n107 node1313\n107 node1277\n107 node1246\n107 node1242\n107 node1231\n107 node1214\n106 node1243\n105 node1251\n105 node1212\n105 node1205\n104 node1272\n104 node1269\n104 node1210\n104 node1203\n104 node1193\n104 node1128\n103 node1300\n103 node1285\n103 node1279\n103 node1209\n103 node1173\n103 node1165\n102 node1276\n102 node1239\n102 node1228\n102 node1204\n102 node1188\n101 node1314\n101 node1303\n100 node1301\n100 node1252\n99 node1287\n99 node1213\n99 node1206\n98 node1295\n98 node1186\n97 node1293\n97 node1265\n97 node1262\n97 node1260\n97 node1258\n97 node1235\n97 node1229\n97 node1226\n97 node1215\n97 node1208\n97 node1187\n97 node1175\n97 node1171\n96 node1291\n96 node1248\n96 node1224\n96 node1216\n95 node1305\n95 node1280\n95 node1263\n95 node1254\n95 node1153\n95 node1115\n94 node1271\n94 node1261\n94 node1234\n94 node1233\n94 node1227\n94 node1225\n94 node1217\n94 node1142\n93 node1275\n93 node1198\n93 node1107\n92 node1266\n92 node1220\n92 node1219\n91 node1309\n91 node1289\n91 node1270\n91 node1259\n91 node1256\n91 node1232\n91 node1179\n89 node1290\n89 node1255\n89 node1247\n89 node1207\n89 node1201\n89 node1190\n89 node1154\n89 node1141\n88 node1306\n88 node1282\n88 node1250\n88 node1222\n88 node1184\n88 node1149\n88 node1117\n87 node1278\n87 node1257\n87 node1191\n87 node1185\n87 node1180\n86 node1297\n86 node1178\n85 node1195\n85 node1143\n85 node1112\n84 node1281\n84 node1274\n84 node1264\n83 node1296\n83 node1148\n82 node1218\n82 node1168\n82 node1167\n81 node1311\n81 node1240\n81 node1223\n81 node1196\n81 node1164\n81 node1116\n80 node1267\n80 node1230\n80 node1177\n80 node1119\n79 node1294\n79 node1199\n79 node1181\n79 node1170\n79 node1166\n79 node1103\n78 node1244\n78 node1189\n78 node1157\n77 node1304\n77 node1172\n74 node1182\n71 node1160\n71 node1147\n68 node1236\n68 node1183\n67 node1245\n59 node1139\n58 node1312\n57 node1162\n56 node1308\n56 node1197\n55 node1146\n54 node1106\n53 node1111\n53 node1105\n49 node1145\n49 node1123\n48 node1176\n46 node1136\n44 node1132\n44 node1125\n44 node1122\n44 node1108\n43 node1192\n43 node1121\n42 node1194\n42 node1138\n42 node1104\n41 node1155\n41 node1126\n41 node1114\n40 node1158\n40 node1151\n40 node1137\n40 node1110\n40 node1100\n39 node1156\n38 node1140\n38 node1135\n38 node1109\n37 node1144\n37 node1120\n36 node1118\n34 node1133\n34 node1113\n31 node1134\n26 node1127\n23 node1101\n20 node1131\n\nAnd it should not surprise us that the last 8 reduces were running on nodes 1134, 1127,1101, and 1131. This really demonstrates the need to run speculative reduce runs. \n\nI propose that when the list of reduce jobs running is down to 1/2 the cluster size that we start running speculative reduces. I estimate that it would have saved around an hour on this run. Does that sound like a reasonable heuristic?", "comments": ["Minor correction, the sort took 8:39 instead of 8:49.", "+1\n\nAs before, it must be possible to disable speculative execution for reduces with side-effects.  Also, some of the support needs to be in the output format implementations.  In particular, these should first write to a temporary name, then rename the output when the task completes, but only if the output does not already exist.  We should note that this is the expected behavior for output formats in the interface javadoc, and put as much of the implementation as possible in the base class.\n", "Duplicate of HADOOP-76"], "derived": {"summary": "With my new http-based shuffle (on top of the svn head including sameer's parallel fetch), I just finished sorting 2010 g on 200 nodes in 8:49 with 9 reduce failures. However, the amusing part is that the replacement reduces were _not_ the slow ones.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "we need speculative execution for reduces - With my new http-based shuffle (on top of the svn head including sameer's parallel fetch), I just finished sorting 2010 g on 200 nodes in 8:49 with 9 reduce failures. However, the amusing part is that the replacement reduces were _not_ the slow ones."}, {"q": "What updates or decisions were made in the discussion?", "a": "Duplicate of HADOOP-76"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-254", "title": "use http to shuffle data between the maps and the reduces", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-05-25T23:53:34.000+0000", "updated": "2009-07-08T16:51:46.000+0000", "description": "To speed up the shuffle time, I'll use http (via the task tracker's jetty server) to send the map outputs.", "comments": ["This patch introduces the use of http to shuffle the data from maps to reduces. Using this patch, the shuffle mostly keeps up with the maps during my sort benchmark.\n\nThis patch also allows the task tracker's http servers to try multiple ports until it finds one that works.", "This looks great!\n\nA couple of improvements:\n\n1. in MapOutputLocation.getFile(), shouldn't things be closed in a 'finally' clause?\n2. does MapOutputFile still need to be a Writable?  I don't think so.  We should remove its write & readFields implementations & any other methods that are no longer called.\n3. do we have any way to detect when map outputs are lost or corrupted?  that was a useful mechanism that i'd hate to lose.\n4. Sameer promised that you'd remove RPC.callRaw() in this patch.\n", "Also, we should remove MapOutputProtocol.", "Ok, here is an updated patch that addresses Doug's concerns.\n\n1. Local file system is now used for reading and writing the map output files. Hopefully, this won't hurt our performance too much.\n\n2. Exceptions reading the map outputs call TaskTracker.lostMapOutput so the map is marked as failed. (I tested this by manually changing a character in one of the map output files and making sure that the map reran.)\n\n3. I removed the assumption that the TaskTracker is a singleton.\n\n4. I added set/getAttribute on the StatusHttpServer so that the user can pass objects to the jsp code.\n\n5. I removed more of the dead code (RPC.callRaw, MapOutputFile)", "I just committed this. You rock, Owen."], "derived": {"summary": "To speed up the shuffle time, I'll use http (via the task tracker's jetty server) to send the map outputs.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "use http to shuffle data between the maps and the reduces - To speed up the shuffle time, I'll use http (via the task tracker's jetty server) to send the map outputs."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this. You rock, Owen."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-255", "title": "Client Calls are not cancelled after a call timeout", "status": "Closed", "priority": "Major", "reporter": "Naveen Nalam", "assignee": "Owen O'Malley", "labels": [], "created": "2006-05-26T05:30:14.000+0000", "updated": "2007-01-03T21:40:04.000+0000", "description": "In ipc/Client.java, if a call times out, a SocketTimeoutException is thrown but the Call object still exists on the queue.\n\nWhat I found was that when transferring very large amounts of data, it's common for queued up calls to timeout. Yet even though the caller has is no longer waiting, the request is still serviced on the server and the data is sent to the client. The client after receiving the full response calls callComplete() which is a noop since nobody is waiting.\n\nThe problem is that the calls that timeout will retry and the system gets into a situation where data is being transferred around, but it's all data for timed out requests and no progress is ever made.\n\nMy quick solution to this was to add a \"boolean timedout\" to the Call object which I set to true whenever the queued caller times out. And then when the client starts to pull over the response data (in Connection::run) to first check if the Call is timedout and immediately close the connection.\n\nI think a good fix for this is to queue requests on the client, and do a single sendParam only when there is no outstanding request. This will allow closing the connection when receiving a response for a request we no longer have pending, reopen the connection, and resend the next queued request. I can provide a patch for this, but I've seen a lot of recent activity in this area so I'd like to get some feedback first.", "comments": ["There are three parts to this. The first part is that I'm just about done with converting the map output to use http rather than rpc. So that aspect of this problem will go away.\n\nThe second part is that this behavior happens on all of the rpc's, not just map output transfer. However, sending a message now to potentially prevent a future message is not necessarily a winning game. If you did send a \"cancel call\" message, you'd probably want to remove the call from the server's work queue if it is still waiting to be processed.\n\nIt is tempting to try a strategy where you check the age of a call when you start processing it on the server and reject messages that are too old, but the problem is that it is _not_ 10 seconds from the start of the call, but rather 10 seconds with no data received on the socket, which is hard for the server to estimate. \n\nThe final part is that this characteristic that the server can not assume that the return message from the rpc call was received is a problem. For example, I had a problem with pollForNewTask timing out and dropping tasks. I fixed that by adding a timeout so that after a task is assigned to a task tracker, it it does not show up in a task tracker status message within 10 minutes it is considered lost. However, this applies to _all_ of the rpc messages. You always need to make sure that if the return value of the rpc call were to disappear into thin air that the problem would be detected eventually. There are other instances of this ind of problem that still exist in the code that need to be identified and fixed.", "I think this is, in general, something that we won't fix.  It might be possible to improve things, but we cannot, without elaborate handshake protocols, guarantee that RPC responses are received by clients.  As Owen has indicated, we must instead make our applications tolerant of that.\n\nNote that this is generally a problem for HTTP-based services too.  When someone hits \"stop\" in their browser for a slow request, the server generally continues to compute the request and only discovers that the connection has been closed when it attempts to write the response, if at all.  If the client times out after the server has flushed the response, then there's no way for the server to know this.\n\nYou sugguest that we might queue requests on the client so that only a single request to a particular server is outstanding at a time.  That would not work well for distributed search (Nutch's original IPC application).  In distributed search a front end typically has many queries outstanding.  Each query is broadcast to a number of back end servers.  Different queries take different amounts of time.  We do not want to make fast queries wait for slow queries, as that would make all queries slow and increase the burden on the front end servers.\n\nWould folks object if I resolve this as a WONTFIX bug?", "Well the problem I was seeing is that a getFile RPC request for say 1GB was issued, but then the Call object timedout on the client. Yet the 1GB was still transferred fully to the client and then discarded since there was no waiting Call object. My system got into a situation where 1000s of getFile requests were queued up per node in the server's tcp receive buffers. So you can see how no progress would have ever been made.\n\nAre you suggesting that this is not going to be a problem because all the large response body RPCs will now be done over HTTP?  I can't see how leaving the code as it is would be fine, unless all RPCs are going to be very quick to service and have small response bodies.\n\nYou mentioned that search queries is an example of an RPC that could be broadcast out. What would happen if the queries were taking too long to service and the client side rpc request was already timing out. I could see it get into a similar situation where the servers would become busy processing stale query requests.\n\nSo if all the RPCs will be small response bodies, then it seems fine to keep the connection always open and just read in the response and throw it away. And then why not add a CANCEL-RPC request type that can get sent over whenever the client request has timed out?", "\nDoug,\n\nWe realize all of this will change with all the great copy/sort-path work being done at Yahoo, but here's what we're seeing: When our individual map output files get large, like >1GB, we get endless cascading timeouts.\n\nNaveen found the actual cause of this:\n\n- The map output response capacity of each tasktracker is equal to the _average_ number of concurrent requests sent to each tasktracker. Eg, with two tasks per node, the capacity is two tasks. The average number of requests is two, but that means some nodes get 4 requests and some nodes get 0.\n\n- Tasktrackers queue up over-the-limit requests. When the files are large, the time it takes to complete the previous requests exceeds the timeout interval. This means the client stops waiting for the file, and requests it again.\n\n- Meanwhile, the old request for thr file is eventually processed, and it runs to completion, snce the current clicnt RPC code merrily receives it although nobody is waiting for it.. Since that file is over a gigabyte, it holds up the works for a long time, long enough that waiting tasks timeout, causing more additional requests.\n\nSo what happens is, you get an endless cycle of timed-out requests, that are eventually satisfied, causing useless transfers of >1GB, which cause more timeouts, etc.\n\nObviously, the new efforts at Yahoo will change this considerably, for example _not_ multiplexing these transfers within a single TCP session will allow termination of the TCP session on a timeout, which will prevent large, useless transfers from happening.\n\nBut I just wanted to explain what we were seeing, and why it is definitely uncool to proceed with large transfers when nobody will use the results.\n\nWe'll try out the new Yahoo code, and see what we get. I suspect that this will help a lot.", "I'm going to hijack this bug. Clearly the original context was fixed by moving from the rpc getMapOutput to a jetty servlet. However, we are seeing cases where the dfs servers have trouble keeping up with the rpc calls. \n\nTherefore, I propose that we define a fraction of the ipc.timeout that is the maximum time the rpc calls can take before they are given to the handler.", "This patch has the rpc server handlers discard any call that is older than 60% of the ipc.timeout.\n", "I worry about letting the queued calls grow without bound.  If a synchronized server implementation spends a long time on a single request, while lots of other requests are coming in from other clients, then the queue could end up exhausting memory.  So perhaps we should discard stale requests as new requests are queued rather than when they're dequeued, so that we can limit the size of the queue?", "+1 for Dougs comment\n\nThe thread populating the call queue should add calls to the end of the queue. Then examine calls at the front for staleness and discard calls that are too old.\n\n", "This patch adds a fixed size limit to the number of calls in the rpc server's queue. If the queue is full, the oldest is discarded to make room for the new one.", "I just committed this.  Thanks, Owen!", "This was included in the 0.7.0 release, but mistakenly marked for 0.8.0."], "derived": {"summary": "In ipc/Client. java, if a call times out, a SocketTimeoutException is thrown but the Call object still exists on the queue.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Client Calls are not cancelled after a call timeout - In ipc/Client. java, if a call times out, a SocketTimeoutException is thrown but the Call object still exists on the queue."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was included in the 0.7.0 release, but mistakenly marked for 0.8.0."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-256", "title": "Implement a C api for hadoop dfs", "status": "Closed", "priority": "Major", "reporter": "Arun Murthy", "assignee": "Arun Murthy", "labels": [], "created": "2006-05-27T03:33:13.000+0000", "updated": "2013-09-09T03:39:07.000+0000", "description": "Implement a C api for hadoop dfs to ease talking to hadoop's dfs from native C/C++ applications.", "comments": ["I have attached libhdfs.patch...\n\nIt creates a new sub-dir: hadoop/src/c++/libhdfs and also a port of hadoop/src/test/org/apache/hadoop/fs/TestDFSIO.java called src/test/org/apache/hadoop/fs/TestDFSCIO.java which I have added to src/test/org/apache/hadoop/test/AllTestDriver.java.\n\nthanks,\nArun", "This looks good, but the build still needs some work.\n\nI tried to run 'make' in the source directory, but had to first add $(LDFLAGS) to a number of the targets to stop complaints about finding libjvm.so.  Also, it would be best to add a target to build.xml that builds this.  Builds should not write files in the source tree: it would be better to have all of the compiler outputs placed in the top-level build directory (e.g., in ../../../build/libhdfs, or as specified in an environment variable passed from build.xml).\n\nMost of the above are also valid complaints about librecordio.  I must not have been paying enough attention when I committed that!\n\nThe tests depend on executables in /tmp, but, as far as I can see, there's nothing that puts them there.\n\nThe goal is that one should be able to check things out of subversion and run something like:\n\n  ant test-libhdfs\n\nAnd have the code build and run.  It is safe to assume that folks have gcc, make, etc. installed and available.  Ideally this would work on Cygwin as well as linux, but that's a lower priority.\n\nThanks!", "Doug,\n\n  Appreciate your feedback... some minor queries/responses:\n\n    a) I'm working on getting the whole thing integrated with build.xml/ant and also about putting all outputs in the top-level build directory. librecordio led me down the wrong garden path.\n    \n    b) Can you please elaborate on the issues you had with LDFLAGS? On my system (Gentoo GNU/Linux - 2.6.16.11) make/makeclean works for me i.e. there are no issues building it.  I'm relying on $JAVA_HOME in my Makefile (it's set to /opt/sun-jdk-1.5.0.06 on my box - afaik it's the recommended way?) to point me libjvm.so. This isn't prefect (e.g. $(JAVA_HOME)/jre/lib/i386/server won't work on non i386 architectures??) Is there a better alternative?\n       Since I wasn't clear which system path to dump the .so I'm building (any suggestions?), I just used LD_PRELOAD to run my executable (hdfs_test) - admit I should have clearly pointed this out. \n       For the same reason I had to resort to --rpath tricks to get TestDFSCIO to work (i.e. both hdfs_read & hdfs_write executables have an --rpath for libhdfs.so.1). This was very necessary for getting the hdfs_{read|write} to work with map/reduce. Appreciate any suggestions...\n    \n    c) Does it make sense to just copy the executables from $(HADOOP_HOME)/libhdfs to /tmp/TestDFSCIO from the client (initialization phase) on which the tests are initiated? Admit this completely slipped my mind, glaringly. My bad. \n\nthanks,\nArun\n        ", "Doug,\n\n  Please find attached a new patch incorporating your feedback. Specifically: libhdfs is now integrated with build.xml, fixed TestDFSCIO.java to automatically copy the executables to /tmp/TestDFSCIO on the remoted filesystem and some minor enhancements to the Makefile.\n\nthanks,\nArun", "I just committed this.  I changed the path to 'ld' from '/usr/bin/ld' to simply 'ld', since it's installed in /bin on my Ubuntu system.  Thanks, Arun!"], "derived": {"summary": "Implement a C api for hadoop dfs to ease talking to hadoop's dfs from native C/C++ applications.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Implement a C api for hadoop dfs - Implement a C api for hadoop dfs to ease talking to hadoop's dfs from native C/C++ applications."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  I changed the path to 'ld' from '/usr/bin/ld' to simply 'ld', since it's installed in /bin on my Ubuntu system.  Thanks, Arun!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-257", "title": "starting one data node thread to manage multiple data directories", "status": "Closed", "priority": "Major", "reporter": "Hairong Kuang", "assignee": "Konstantin Shvachko", "labels": [], "created": "2006-05-27T06:06:45.000+0000", "updated": "2009-07-08T16:41:54.000+0000", "description": "If a data node is configured with multiple data directories, current implementation of dfs will start multiple data node threads, each of which manages one data directory and talks to its name node independently. From the name node's point of view, it sees multiple data nodes instead of one.\n\nI feel that a more scalable solution should be to start one data node thread that manages multiple data diretories. But the one data node thread needs to take care of the block allocation problem.", "comments": ["Duplicate of HADOOP-64"], "derived": {"summary": "If a data node is configured with multiple data directories, current implementation of dfs will start multiple data node threads, each of which manages one data directory and talks to its name node independently. From the name node's point of view, it sees multiple data nodes instead of one.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "starting one data node thread to manage multiple data directories - If a data node is configured with multiple data directories, current implementation of dfs will start multiple data node threads, each of which manages one data directory and talks to its name node independently. From the name node's point of view, it sees multiple data nodes instead of one."}, {"q": "What updates or decisions were made in the discussion?", "a": "Duplicate of HADOOP-64"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-258", "title": "Cannot obtain block errors", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Sameer Paranjpye", "labels": [], "created": "2006-05-27T14:16:43.000+0000", "updated": "2009-07-08T16:41:55.000+0000", "description": "I've been seeing a lot of errors from dfs for the last couple days. The exception that I see is:\n\njava.io.IOException: Could not obtain block: blk_2867431916903738534 file=/user/oom/big-rand/part000271 offset=0 at org.apache.hadoop.dfs.DFSClient$DFSInputStream.blockSeekTo(DFSClient.java:532) at org.apache.hadoop.dfs.DFSClient$DFSInputStream.read(DFSClient.java:641) at org.apache.hadoop.fs.FSDataInputStream$Checker.read(FSDataInputStream.java:83) at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:158) at java.io.BufferedInputStream.fill(BufferedInputStream.java:218) at java.io.BufferedInputStream.read1(BufferedInputStream.java:256) at java.io.BufferedInputStream.read(BufferedInputStream.java:313) at java.io.DataInputStream.readFully(DataInputStream.java:176) at java.io.DataInputStream.readFully(DataInputStream.java:152) at org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:264) at org.apache.hadoop.io.SequenceFile$Reader.(SequenceFile.java:248) at org.apache.hadoop.io.SequenceFile$Reader.(SequenceFile.java:238) at org.apache.hadoop.mapred.SequenceFileRecordReader.(SequenceFileRecordReader.java:36) at org.apache.hadoop.mapred.SequenceFileInputFormat.getRecordReader(SequenceFileInputFormat.java:53) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:105) at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:866)\n\nI haven't had my fingers in that code recently, does it ring any bells for anyone?", "comments": ["Just FYI, I recently integrated hadoop 0.5 into my copy of the source, rebuit and installed. I ran into similar \"Could not obtain block\" errors until I realized that I hadn't deployed the (new in hadoop 0.5) .../webapps/datanode/ servlet on my cluster."], "derived": {"summary": "I've been seeing a lot of errors from dfs for the last couple days. The exception that I see is:\n\njava.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Cannot obtain block errors - I've been seeing a lot of errors from dfs for the last couple days. The exception that I see is:\n\njava."}, {"q": "What updates or decisions were made in the discussion?", "a": "Just FYI, I recently integrated hadoop 0.5 into my copy of the source, rebuit and installed. I ran into similar \"Could not obtain block\" errors until I realized that I hadn't deployed the (new in hadoop 0.5) .../webapps/datanode/ servlet on my cluster."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-259", "title": "map output http client does not timeout", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-05-29T13:14:42.000+0000", "updated": "2009-07-08T16:51:46.000+0000", "description": "The new map output http client uses java.net.URLConnection to fetch the data file. However under Java 1.4 there is no way to specify a timeout and it is set to infinite (or if not infinite at least 12 hours).  This causes reduce tasks to get \"stuck\" in the \"reduce > copy\" phase even after the \"Task failed to report status for 600 seconds. Killing.\" message.\n\nI will add the code in the ReduceTaskRunner to make sure that copies in-flight don't get stuck, but this is another point where a switch to java 1.5 would be helpful. Under 1.5 I could set the timeout on the connection and the read would timeout after the given interval and my entire change would be local to MapOutputLocation.copyFile.\n\nFor now, I'll assume that we need to maintain support for 1.4 and make the corresponding fix, but I'm grumbling...", "comments": ["\nMoving to 1.5 is fine for us, in fact I recommend it.", "Ok, this patch adds a timer for the map output copy threads. If they don't make progress often enough. (Progress is defined as receiving a block of data from the http server.) I've been testing it for the last two days and it seems to be working, but if we go to java 1.5 we should probably use the socket timeouts.", "Does Pingable really need to be a public interface in the util package?  The util package could easily get cluttered with stuff that's only used in one place.  Any objection if I make this a package-private interface in mapred, or perhaps even a nested class?", "Ok, I moved the Pingable interface to be an inner interface to MapOutputLocation.", "I just committed this.  Thanks, Owen!"], "derived": {"summary": "The new map output http client uses java. net.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "map output http client does not timeout - The new map output http client uses java. net."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-260", "title": "the start up scripts should take a command line parameter --config making it easy to run multiple hadoop installation on same machines", "status": "Closed", "priority": "Minor", "reporter": "Mahadev Konar", "assignee": "Milind Barve", "labels": [], "created": "2006-05-31T01:42:21.000+0000", "updated": "2006-08-18T11:18:01.000+0000", "description": "--config paramter would allow getting rid of SSH_OPTS.", "comments": ["This patch allows for a --config command line argument to all the scripts. It still works if HADOOP_CONF_DIR env variable is set and --config is not specified.", "I like what this does, but not particularly the way it is done.  Having the same 20 lines repeated in 10 files seems fragile to me.  Can we somehow instead put this logic in a single file that's included in all of the scripts?", "you are right Doug. I will factor it out., so that it looks much cleaner. Thanks for the input.\n", "I have started looking at this issue. The original patch contains duplicate shell-script in all scripts. We could have separated it out as a function and included that file. However, in the meanwhile, the scripts have become portable across bourne-compatible shells, rather than being specific to bash. How do you propose we eliminate repeatition ?", "Added a [--config confdir] optional parameter to all hadoop shell scripts. The scripts now use the portable source command to eliminate duplicate code.", "I just committed this.  Thanks, Milind!", "As far as I can tell, it's still necessary to define a separate PID directory for this to work; the PID variable in hadoop-deamon.sh does not take the configuration directory into account. While having a separate PID directory per configuration is not a problem per se, it seems unnecessary."], "derived": {"summary": "--config paramter would allow getting rid of SSH_OPTS.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "the start up scripts should take a command line parameter --config making it easy to run multiple hadoop installation on same machines - --config paramter would allow getting rid of SSH_OPTS."}, {"q": "What updates or decisions were made in the discussion?", "a": "As far as I can tell, it's still necessary to define a separate PID directory for this to work; the PID variable in hadoop-deamon.sh does not take the configuration directory into account. While having a separate PID directory per configuration is not a problem per se, it seems unnecessary."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-261", "title": "when map outputs are lost, nothing is shown in the webapp about why the map task failed", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-05-31T01:46:39.000+0000", "updated": "2009-07-08T16:51:44.000+0000", "description": "When a task tracker invokes mapOutputLost, it does not include a diagnostic that lets the user (or the web app) know what failed in the map. I want to add a diagnostic message.", "comments": ["This patch adds a diagnostic message to the task to explain why it move from success to failed when a map output file can not be read.", "I just committed this.  Thanks, Owen!"], "derived": {"summary": "When a task tracker invokes mapOutputLost, it does not include a diagnostic that lets the user (or the web app) know what failed in the map. I want to add a diagnostic message.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "when map outputs are lost, nothing is shown in the webapp about why the map task failed - When a task tracker invokes mapOutputLost, it does not include a diagnostic that lets the user (or the web app) know what failed in the map. I want to add a diagnostic message."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-262", "title": "the reduce tasks do not report progress if they the map output locations is empty.", "status": "Closed", "priority": "Major", "reporter": "Mahadev Konar", "assignee": "Mahadev Konar", "labels": [], "created": "2006-05-31T02:01:48.000+0000", "updated": "2009-07-08T16:51:45.000+0000", "description": "The ReduceTaskRunner should report progress even if the number of ouput locations to copy is empty. In case, the last few maps are running on a tasktracker that goes down, all the reduce tasks waiting for these mapoutputs would fail.", "comments": ["here is a patch that reports progress while its sleeping and waiting for more map outputs.", "I just committed this.  Thanks, Mahadev."], "derived": {"summary": "The ReduceTaskRunner should report progress even if the number of ouput locations to copy is empty. In case, the last few maps are running on a tasktracker that goes down, all the reduce tasks waiting for these mapoutputs would fail.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "the reduce tasks do not report progress if they the map output locations is empty. - The ReduceTaskRunner should report progress even if the number of ouput locations to copy is empty. In case, the last few maps are running on a tasktracker that goes down, all the reduce tasks waiting for these mapoutputs would fail."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Mahadev."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-263", "title": "task status should include timestamps for when a job transitions", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Sanjay Dahiya", "labels": [], "created": "2006-06-01T02:12:52.000+0000", "updated": "2013-05-02T02:29:00.000+0000", "description": "It would help users to understand what happened if the task status included information about when the task transitioned:\n\nMap:\n   started\n   finished\n\nReduce:\n   started\n   shuffle finished\n   sort finished\n   finished", "comments": ["Here is what I am thinking for this. \n\nWe add an extra field (timestamp) in org.apache.hadoop.mapred.TaskStatus. The timestamp gets TaskTracker.TaskInprogress.reportProgress(), when a task reports progress. As part of Task the timestamp is updated in Progress.set(), Progress.setStatus() and complete(). The timestamp is sent over to TaskTracker over RPC along with status.\n\nAnother option is to update the timestamp when TaskTracker gets the progress report, without making any change to the Task. But sending timestamp over RPC gives a more accurate timestamp of the last progress update. \n", "This patch displays a timestamp with task status on the webpage. The timestamp is of time when task tracker receives a state change update. Time stamps are also displayed for individual task attempts in addition to state changes. ", "A couple of points need work:\n  1. It only catches the finish time of the task.\n  2. It includes a couple of comment/whitespace changes that are accidental.\n  3. It calls the added field \"timestamp\", which doesn't say what the time represents.\n\nI was expecting to see:\n  2 new fields in TaskStatus:\n     private long timeStarted;\n     private long timeStopped;\n  a new class derived from TaskStatus named ReduceTaskStatus that includes:\n     private long timeShuffleFinished;\n     private long timeSortFinished;\n\nThis patch does point out the high simularity of the TaskStatus and TaskReport. They are basically the same type just slightly filtered by which protocol they are being used on. Why don't you add timeStarted and timeStopped to TaskReport and we can  drop TaskReport in a later patch.\n  ", "A new patch, now shows start, finish times for tasks and shuffle/sort finished time for reduce tasks. \nfew points, which should probably be separate issues. \n- TaskStatus, TeskReport replicate some status data which is present as member variables in TaskTracker.TaskInProgress, TaskInProgress. We can reduce the duplication of data by maintaining objects of TaskStatus and get/set from that directly, using simple composition. \n- The status string passed around needs to be a public static final String defined in TaskStatus and used everywhere else, so it can be compared against reliably wrt future changes. Currently its reduce > copy (...), reduce > sort, reduce > reduce.\n", "Updated the display. now showing start/finish times in separate columns. ", "A couple of points:\n   1. I'd express the start times as absolute times and the others as relative times in hours, minutes and seconds.\n   2. the web ui shouldn't include shuffle & sort time columns for map tasks.\n   3. \"blank\" times should be written as \"&nbsp;\" in the html so that the browsers make the box for the cell.\n   4. usually, when you are creating format objects, you keep them in static fields rather than creating them on each call.\n   5. the start times should be when the task starts running not when the job is submitted\n   6. the shuffle time is never being set", "Thanks Owen for the review, here is an updated patch. \n\nThe earlier patch failed with latest trunk due to some changes in TaskInProgress, so I redid some part of it. Attached patch works with the latest trunk. ", "Ok, looking at the current patch, I have a couple of comments:\n  1. The shuffle & sort times should be set to the current time when a later stage is finished. (This would happen when the shuffle or sort finishes in less than a single reporting cycle.) I'd probably put the relevant check/update in the setters for the times in TaskStatus.\n  2. You need to pull the replicated code for formatting delta times into StringUtils. And if a field (and all of the larger fields) are 0, you shouldn't print the label except for the last one. (eg. 0 time => \"0 secs\")\n  3. Reporting time to milliseconds when the real granularity is 10 seconds is overkill. Just report to seconds.\n  4. You don't need to add the type parameter to the taskdetails.jsp because it is available in TaskStatus.getIsMap()\n  5. You should probably add the times to the TaskReport constructor rather than creating the object and setting it to be consistent with the other fields.", "  4. You don't need to add the type parameter to the taskdetails.jsp because it is available in TaskStatus.getIsMap() \n\nI added this to simplify the html generation, taskdetails.jsp is used for both Map and Reduce tasks, we get TaskStatuses from tracker.getTaskStatuses(). The header is generated even if there is no task status. using an extra parameter simplifies this. \n\nI agree with rest of the comments. \n", "Thanks for the comments, Owen \n\nHere is an updated patch. ", "A couple more things:\n  1. You don't have javadoc on the public methods in TaskStatus.\n  2. For the task details you should always have at least one result, so just test for that case explicitly and put a message about \"no task attempts found\". Please remove the new type parameter.\n  3. Can we factor some more of the duplicated code in the jsp into a single instance?", "Thanks Owen for the comments. \n\nHere is an updated patch, I  also added time difference for start-shuffle-sort phases along with finish time stamps  on JSPs. ", "Could we include the time taken for task even if it fails? This would be quite handy in calcutating the total machine hours each job takes.", "There are still a lot (8?) of public procedures without java doc in this patch.\n\nThe set ? time methods could all be package local rather than public, couldn't they? Restricting their visibility is a good thing.", "I left plain empty get/set methods without comments. Since we are putting javadoc for *all* public methods for new code, i'll add these now. \n\nAlso for failed task time, I will set finish time to the time failure was reported. ", "Here is an updated patch. \nTask finish time is now set to failed time in case task fails. If the task fails at task tracker then failed time is set there and sent over to jobtracker. if the task tracker itself goes down then job tracker sets the failed time to when it discovers task failed. \nA side effect is that in case a task starts and before it could send its first status, task tracker dies, in that case the failed time will be recorded but task start time will not be recorded as task tracker went down before it could report that. We rely on task tracker to report the correct task launch time. Start/finish times for TIPs will be available in any case. \n", "The patch no longer works with current trunk. Here is an updated patch.", "I am uncomfortable using progress status strings to determine which phase a task is in.  Rather we should have a proper API for this.  (This is similar to HADOOP-544, but different.)\n\nPerhaps we should have an enum in ReduceTask named Phase.  TaskUmbilicalProtocol.progress() could get a new 'int phase' parameter that would include the phase.ordinal() value.  Would that work?", "Here is an updated patch, as doug suggested I have added an extra argument to TaskUmbilicalProtocol.progress(String taskid, float progress, String state, int phase). phase is Phase.ordinal(). \nAll classes deriving from TaskUmbilicalProtocol are updated with this argument, but its value is ignored in IsolationRunner and LocalJobRunner. \nIn JobTracker.ExpireLaunchingTasks.run() the phase is not available if its a reduce task, in such case it uses Phase.UNKNOWN. \nAlso both map and reduce tasks use TaskUmbilicalProtocol.progress, so Map tasks return Phase.MAP and reduce tasks return Phase.SHUFFLE, PHASE.SORT or Phase.REDUCE. ", "As of HADOOP-306, we can put enum types into RPC and your change would be much cleaner if the TaskUmbilicalProtocol.progress' phase parameter was really passed as an enum.\n\nFurthermore, we should probably make methods in WritableUtils for reading and writing enums:\n\n  public static <T extends Enum<T>> T readEnum(DataInput in, Class<T> enumType\n                                                                                         ) throws IOException;\n  public static void writeEnum(DataOutput out,  Enum enum) throws IOException;\n\nThe Phase field of Task should be private.\n\nYou can pull the replicated code for setting the phase out of the constructors and do:\n\nclass ReduceTask ...\n  {\n    setPhase(Phase.SHUFFLE); \n  }\n\n  public ReduceTask() {\n    ...\n  }\n\n  public ReduceTask(...) {\n    ...\n  }\n\nthe same can be done for MapTask.\n", "Ok, I will add to WritableUtils, instead of enum.ordinal() I will use string value of enum in DataOutput ( unless someone objects to it)\n", "Here is an updated patch. ", "I just committed this.  Thanks, Sanjay!"], "derived": {"summary": "It would help users to understand what happened if the task status included information about when the task transitioned:\n\nMap:\n   started\n   finished\n\nReduce:\n   started\n   shuffle finished\n   sort finished\n   finished.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "task status should include timestamps for when a job transitions - It would help users to understand what happened if the task status included information about when the task transitioned:\n\nMap:\n   started\n   finished\n\nReduce:\n   started\n   shuffle finished\n   sort finished\n   finished."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Sanjay!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-264", "title": "WritableFactory has no permissions to create DatanodeRegistration", "status": "Closed", "priority": "Blocker", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-06-01T06:16:33.000+0000", "updated": "2009-07-08T16:41:55.000+0000", "description": "The datanode can not come up because the DatanodeRegistration is package local and the factory registration doesn't happen.", "comments": ["This patch forces the namenode to load the DatanodeRegistration class so that the factory is setup before it is needed for RPC.", "I should comment that this is a total hack just to get us flying again...", "There is an idiom for this hack.  See e.g., the static initializer blocks near the top of DataNode, DFSClient, DFSShell, JobTracker, TaskTracker, etc.  It's nice to keep these all uniform, with the same comment, so that, someday, we can easily remove them all at once.", "Ok, after considering it a few minutes, I propose that all objects passed through RPC should be public classes with public methods. That will help when we factor the different servers (jobtracker vs. tasktracker) into separate packages (org.apache.hadoop.mapred.job vs org.apache.hadoop.mapred.task ???) and fundamentally the RPC interfaces are _public_ with a capital P.\n\nHowever, the WritableFactory factory is still very useful for serialization and such. I'd propose that classes that want to register do:\n\nstatic void registerWritableFactory() {\n  WritableFactories.setFactory(Foo.class, new WritableFactory () { ... });\n}\n\nstatic {\n  registerWritableFactory();\n}\n\nThen when WritableFactory wants to create an instance of a class, if no factory is registered, it uses reflection to find a \"registerWritableFactory\" method and calls it, if such a method exists. (We can use setAccessable to make it callable, even if it is not public.)", "I agree with Owen that DatanodeRegistration as well as other classes passed through RPC should be public.\nThe counter-patch :-) with only one word added is attached.\nSince it is related to HADOOP-124 making some classes non-public,\nI would return public status to the exceptions as well.\nJust speculating: if RPC would have to convert RemoteException to the real\nexceptions like LeaseExpiredException then it would crash for the same reason.\nWe do not have that mechanism (for converting exceptions) yet, but one day we will.", "Here is a patch that matches the standard idiom.", "I just committed this.  Thanks, Owen."], "derived": {"summary": "The datanode can not come up because the DatanodeRegistration is package local and the factory registration doesn't happen.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "WritableFactory has no permissions to create DatanodeRegistration - The datanode can not come up because the DatanodeRegistration is package local and the factory registration doesn't happen."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-265", "title": "Abort tasktracker if it can not write to its local directories", "status": "Closed", "priority": "Major", "reporter": "Hairong Kuang", "assignee": "Hairong Kuang", "labels": [], "created": "2006-06-01T07:07:42.000+0000", "updated": "2009-07-08T16:51:46.000+0000", "description": "Currently if a task tracker is not able to write to any of its local directories, it continues to run and all the tasks assigned to it fail.\n\nA task tracker should not start upif it has a problem reading/writing any of its local directories. It should abort if it gets the problem at run time.", "comments": ["In this patch, if a task tracker finds out that any of its local directories becomes not readable or writable, it logs the error. If all of its local directories are not readable/writable, it reports the problem to its job tracker and then aborts. When the job tracker receives the error report, it logs the error. \n\nA task tracker detects disk problem at startup time and before it starts a new task. A task tracker will not start if all its local directories are not readable or writable. \n", "I just committed this.  Thanks, Hairong!"], "derived": {"summary": "Currently if a task tracker is not able to write to any of its local directories, it continues to run and all the tasks assigned to it fail. A task tracker should not start upif it has a problem reading/writing any of its local directories.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Abort tasktracker if it can not write to its local directories - Currently if a task tracker is not able to write to any of its local directories, it continues to run and all the tasks assigned to it fail. A task tracker should not start upif it has a problem reading/writing any of its local directories."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Hairong!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-266", "title": "RPC doesn not handle exceptions correctly", "status": "Closed", "priority": "Minor", "reporter": "Konstantin Shvachko", "assignee": "Owen O'Malley", "labels": [], "created": "2006-06-01T09:10:37.000+0000", "updated": "2008-03-29T03:01:07.000+0000", "description": "1. Examining HADOOP-264 bug I realized that not all rpc server exceptions are actually returned to the client. Particularly, if an exception happens in \norg.apache.hadoop.ipc.Server.Connection.run()\nfor example inside \nparam.readFields(in);\nthen it logged but never returned back to the client.\nClient simply timeouts in this case, which is not exactly what one would expect.\n\n2. On the way back \norg.apache.hadoop.ipc.Client.call()\nrpc client always throws a RemoteException, rather than the exception wrapped into this RemoteException.\n", "comments": ["The code is working correctly and there are substantial problems with changing it."], "derived": {"summary": "1. Examining HADOOP-264 bug I realized that not all rpc server exceptions are actually returned to the client.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "RPC doesn not handle exceptions correctly - 1. Examining HADOOP-264 bug I realized that not all rpc server exceptions are actually returned to the client."}, {"q": "What updates or decisions were made in the discussion?", "a": "The code is working correctly and there are substantial problems with changing it."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-268", "title": "TaskTracker latency is slowing down maps because progress reporting is done inline", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": null, "labels": [], "created": "2006-06-02T01:11:58.000+0000", "updated": "2009-07-08T16:51:47.000+0000", "description": "Moving the progress reporter to a separate thread should free up the user application to run faster.", "comments": ["This was fixed by HADOOP-1462."], "derived": {"summary": "Moving the progress reporter to a separate thread should free up the user application to run faster.", "classification": "performance", "qna": [{"q": "What is the issue described?", "a": "TaskTracker latency is slowing down maps because progress reporting is done inline - Moving the progress reporter to a separate thread should free up the user application to run faster."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was fixed by HADOOP-1462."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-269", "title": "add FAQ to Wiki", "status": "Closed", "priority": "Major", "reporter": "Doug Cutting", "assignee": "Doug Cutting", "labels": [], "created": "2006-06-02T02:04:14.000+0000", "updated": "2006-08-03T17:46:45.000+0000", "description": "Hadoop should have an FAQ in the Wiki.  We can bootstrap this by reviewing the mailing list archives.", "comments": ["I just added an FAQ to the wiki.\n\nhttp://wiki.apache.org/lucene-hadoop/FAQ\n"], "derived": {"summary": "Hadoop should have an FAQ in the Wiki. We can bootstrap this by reviewing the mailing list archives.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "add FAQ to Wiki - Hadoop should have an FAQ in the Wiki. We can bootstrap this by reviewing the mailing list archives."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just added an FAQ to the wiki.\n\nhttp://wiki.apache.org/lucene-hadoop/FAQ"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-270", "title": "possible deadlock when shut down a datanode thread", "status": "Closed", "priority": "Major", "reporter": "Hairong Kuang", "assignee": "Hairong Kuang", "labels": [], "created": "2006-06-02T03:45:09.000+0000", "updated": "2009-07-08T16:41:55.000+0000", "description": "The DataNode class provides a method \"shutdown\" that can be used to notify a data node thread to abort itself gracefully. In addition this method waits for its data receiving server to terminate. This may cause possible deadlock if the method is called by the data receiving server.", "comments": ["The patch moves \"join\" operation to the end of \"offerservice\". In addition, it starts up its data receive server in the beginning of \"offerservice\" instead of in the DataNode constructor.", "I just committed this.  Thanks, Hairong!"], "derived": {"summary": "The DataNode class provides a method \"shutdown\" that can be used to notify a data node thread to abort itself gracefully. In addition this method waits for its data receiving server to terminate.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "possible deadlock when shut down a datanode thread - The DataNode class provides a method \"shutdown\" that can be used to notify a data node thread to abort itself gracefully. In addition this method waits for its data receiving server to terminate."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Hairong!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-271", "title": "add links to task tracker http server from task details and failure pages", "status": "Closed", "priority": "Minor", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-06-02T06:31:08.000+0000", "updated": "2009-07-08T16:51:46.000+0000", "description": "Make the machine name in the list of task attempts and failures into links to the correct task tracker http server.", "comments": ["This patch changes the TaskStatus to contain the task tracker name rather than the machine name. The jsp pages then use this to get both the machine name and http port.", "Don't commit this, yet. If the task tracker has been timed out, it can't find the relevant object and gets a null pointer exception. I'll change the patch to just put in the task tracker name without the link in that case.", "Ok, this patch protects the jsp from not finding the named task tracker by printing the name of the task tracker if it isn't found.", "One more iteration to deal with the fact that Windows does not contain the \"killall\" command. It is unfortunate that Java does not provide a way to send signals to processes or to get pids.", "Updated to apply against the current sources.", "I just committed this.  Thanks, Owen."], "derived": {"summary": "Make the machine name in the list of task attempts and failures into links to the correct task tracker http server.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "add links to task tracker http server from task details and failure pages - Make the machine name in the list of task attempts and failures into links to the correct task tracker http server."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-272", "title": "bin/hadoop dfs -rm <dir> crashes in log4j code", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-06-03T06:03:04.000+0000", "updated": "2006-08-03T17:46:45.000+0000", "description": "When I run \"bin/hadoop dfs -rm out-dir\" I get the following error messages:\n\nlog4j:ERROR setFile(null,true) call failed.\njava.io.FileNotFoundException: /local/owen/hadoop/run/log (Is a directory)\n        at java.io.FileOutputStream.openAppend(Native Method)\n        at java.io.FileOutputStream.<init>(FileOutputStream.java:177)\n        at java.io.FileOutputStream.<init>(FileOutputStream.java:102)\n        at org.apache.log4j.FileAppender.setFile(FileAppender.java:289)\n        at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:163)\n        at org.apache.log4j.DailyRollingFileAppender.activateOptions(DailyRollingFileAppender.java:215)\n        at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:256)\n        at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:132)\n        at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:96)\n        at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:654)\n        at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:612)\n        at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:509)\n        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:415)\n        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:441)\n        at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:468)\n        at org.apache.log4j.LogManager.<clinit>(LogManager.java:122)\n        at org.apache.log4j.Logger.getLogger(Logger.java:104)\n        at org.apache.commons.logging.impl.Log4JLogger.getLogger(Log4JLogger.java:229)\n        at org.apache.commons.logging.impl.Log4JLogger.<init>(Log4JLogger.java:65)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:494)\n        at org.apache.commons.logging.impl.LogFactoryImpl.newInstance(LogFactoryImpl.java:529)\n        at org.apache.commons.logging.impl.LogFactoryImpl.getInstance(LogFactoryImpl.java:235)\n        at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:370)\n        at org.apache.hadoop.conf.Configuration.<clinit>(Configuration.java:54)\n        at org.apache.hadoop.dfs.DFSShell.main(DFSShell.java:307)\nlog4j:ERROR Either File or DatePattern options are not set for appender [DRFA].\nDelete failed\n", "comments": ["I just fixed this."], "derived": {"summary": "When I run \"bin/hadoop dfs -rm out-dir\" I get the following error messages:\n\nlog4j:ERROR setFile(null,true) call failed. java.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "bin/hadoop dfs -rm <dir> crashes in log4j code - When I run \"bin/hadoop dfs -rm out-dir\" I get the following error messages:\n\nlog4j:ERROR setFile(null,true) call failed. java."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just fixed this."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-273", "title": "Add a interactive shell for admistrative access to the DFS", "status": "Closed", "priority": "Trivial", "reporter": "Marco Paga", "assignee": "Aaron Kimball", "labels": [], "created": "2006-06-03T18:49:01.000+0000", "updated": "2012-02-20T12:34:59.000+0000", "description": "Implement a shell that allows the user to work in the dfs like on the local fs.\n\ncd /user/\ncd test\nput text.txt someText.txt\ncat someText.txt\nrm someText.txt\n", "comments": ["Patch implements the interactive shell.", "The new and corrected version", "Added interactive DFS Shell", "i dont understand the motivation behind this since all of this can be done in a bash script. Marco can you explain what would be use case for this ?", "I don't care anymore. Please close this request. Thanks.", "I don't care anymore.", "I still care! :)\n\nAlex Loddengaard and I wrote a simple command shell that uses jline (a readline implementation) to create a simple shell that includes some handy features like tab completion for HDFS. This supports access to the various FsShell commands in an interactive fashion.\n\nThere's a definite need for something like this -- executing several {{hadoop fs ...}} commands in a bash script or in a regular interactive shell is very time-consuming due to the overhead of starting Java for each such command. Putting all the commands into a single JVM instance is a major win.\n\nWhen using Hadoop from outside of Java, programs that wish to interact with\nthe DFS may need to run several commands through the {{hadoop fs ...}} interface, which is\nvery slow.\n\nThis shell:\n* Supports interactive use of HDFS (with a reasonable notion of a \"current directory\", etc)\n* Supports executing scripts of several commands\n\nExternal programs could write a script of several DFS operations and batch them up for execution in a single Java process.\n\nThis changes some of the methods (e.g., {{ls()}}) from FsShell to be public instead of package-public so that their code can be reused here.\n\nWe tested CmdShell by running the various commands locally, as well as testing short scripts of the commands included together. Some basic functionality unit tests are also included in this patch.\n\nIf a committer could please re-open this issue and take a look at the attached code, I'd appreciate it.\n", "Apparently there's no \"reopen\" command any more. Moved to HADOOP-6541."], "derived": {"summary": "Implement a shell that allows the user to work in the dfs like on the local fs. cd /user/\ncd test\nput text.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Add a interactive shell for admistrative access to the DFS - Implement a shell that allows the user to work in the dfs like on the local fs. cd /user/\ncd test\nput text."}, {"q": "What updates or decisions were made in the discussion?", "a": "Apparently there's no \"reopen\" command any more. Moved to HADOOP-6541."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-274", "title": "The new logging framework puts application logs into server directory in hadoop.log", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-06-05T23:26:26.000+0000", "updated": "2006-08-03T17:46:45.000+0000", "description": "The new logging infrastructure puts application logs into the server log directory under hadoop.log. I think it would be less confusing to use the old behavior of writing out to stderr (for applications).\n\nThoughts?", "comments": ["Ok, this patch comes pretty close.\n\nI've made applications log to stdout and simplified their log format.\n\nThe servers still log to their respective files.", "I just committed this.  Thanks, Owen."], "derived": {"summary": "The new logging infrastructure puts application logs into the server log directory under hadoop. log.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "The new logging framework puts application logs into server directory in hadoop.log - The new logging infrastructure puts application logs into the server log directory under hadoop. log."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-275", "title": "log4j changes for hadoopStreaming", "status": "Closed", "priority": "Major", "reporter": "Michel Tourn", "assignee": "Doug Cutting", "labels": [], "created": "2006-06-06T03:59:58.000+0000", "updated": "2009-07-08T17:05:36.000+0000", "description": "A patch for hadoopStreaming. \nUse log4j \nStill Using short UTF8 strings\n\n", "comments": ["I just committed this.  Thanks, Michel."], "derived": {"summary": "A patch for hadoopStreaming. Use log4j \nStill Using short UTF8 strings.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "log4j changes for hadoopStreaming - A patch for hadoopStreaming. Use log4j \nStill Using short UTF8 strings."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Michel."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-276", "title": "No appenders could be found for logger", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-06-06T04:56:07.000+0000", "updated": "2006-08-03T17:46:45.000+0000", "description": "When you start the servers with an old configuration directory without the properties files, you get messages about:\n\nlog4j:WARN No appenders could be found for logger (org.apache.hadoop.conf.Configuration).\nlog4j:WARN Please initialize the log4j system properly.\n\nThe problem is that the property files are not included in the jar file.", "comments": ["This patch adds the property files to the jar.", "I just committed this.  Thanks!"], "derived": {"summary": "When you start the servers with an old configuration directory without the properties files, you get messages about:\n\nlog4j:WARN No appenders could be found for logger (org. apache.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "No appenders could be found for logger - When you start the servers with an old configuration directory without the properties files, you get messages about:\n\nlog4j:WARN No appenders could be found for logger (org. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-277", "title": "Race condition in Configuration.getLocalPath()", "status": "Closed", "priority": "Major", "reporter": "Peter Sutter", "assignee": "Sameer Paranjpye", "labels": [], "created": "2006-06-06T08:11:33.000+0000", "updated": "2006-08-03T17:46:45.000+0000", "description": "(attached: a patch to fix the problem, and a logfile showing the problem occuring twice)\n\nThere is a race condition in Configuration.java:\n\n       Path file = new Path(dirs[index], path);\n       Path dir = file.getParent();\n       if (fs.exists(dir) || fs.mkdirs(dir)) {\n         return file;\n\nIf two threads simultaneously process this code with the same target directory, fs.exists() will return false, but from fs.mkdirs() only one of the two threads will return true. From the Java documentation:\n \"returns: true if and only if the directory was created, along with all necessary parent directories; false otherwise\"\n\nThat is, if the first thread successfully creates the directory, the second will not, and therefore return false, even though the directory exists.\n\nThis was really happening. We use four temporary directories, and we had reducers failing all over the place with  bizarre impossible errors. I modified the ReduceTaskRunner to output the filename that it creates to find the problem, and the log output is below.\n\nHere you can see copies initiated for two files that hash to the same temp directory, simultaneously. map_4.out is created in the correct directory (/data2...), but map_15.out is created in the next directory (/data3...) becuase of this race condition. Minutes later, when the appender tries to locate the file, that race condition does not occur (the directory already exists), and the appender looks for the file map_15.out in the correct directory, where it does not exist.\n\n060605 142414 task_0001_r_000009_1 Copying task_0001_m_000004_0 output from rmr05.\n060605 142414 task_0001_r_000009_1 Copying task_0001_m_000015_0 output from rmr04.\n...\n060605 142416 task_0001_r_000009_1 done copying task_0001_m_000004_0 output from rmr05 into /data2/tmp/mapred/local/task_0001_r_000009_1/map_4.out\n...\n060605 142418 task_0001_r_000009_1 done copying task_0001_m_000015_0 output from rmr04 into /data3/tmp/mapred/local/task_0001_r_000009_1/map_15.out\n...\n060605 142531 task_0001_r_000009_1 0.31808624% reduce > append > /data2/tmp/mapred/local/task_0001_r_000009_1/map_4.out\n...\n060605 142725 task_0001_r_000009_1 java.io.FileNotFoundException: /data2/tmp/mapred/local/task_0001_r_000009_1/map_15.out\n\n\n", "comments": ["This patch is closer to what we did for the routine above it last week. (Sorry about not fixing this one too at the same time. It wasn't biting us, but that was no reason not to fix the obviously parallel code.)  Is there some reason that you need the synchronized block around the mkdirs? File.mkdirs does a File.exists internally, so you don't need to call it yourself.", "By the way, if there is a need for the sync block around the mkdirs, we should go ahead and change the function above it so that next week we don't get a third bug. *smile*", "If we take out the sync block, the code would look like:\n\nif (fs.exists(dir)) {\n  return file;\n}\nfs.mkdirs(dir);\nif (fs.exists(dir)) {\n  return file;\n}\n\nwe added the sync block around the mkdirs because we were not sure exactly how mkdirs() handles race cases. for example if one thread is creating the directory hierarchy, can the other thread return error while the first thread is still creating the hierarchy? if so when the second thread executes fs.exists(), it would return false since the directories are still be created. \n\nbut perhaps this isn't the behavior mkdirs() exhibits. it could return false only if it can't create the final leaf directory in the directory hierarchy (because the other thread just created the leaf). if that's the case then the sync would not be needed. the sun java api doc isn't clear on this.", "The File.mkdirs (based on what I see in eclipse) looks like:\n\n    public boolean mkdirs() {\n        if (exists()) {\n            return false;\n        }\n        if (mkdir()) {\n            return true;\n        }\n        ... <handle recursive mkdirs>...\n    }\n\nin any case, the final mkdir would need to be the last thing done. Without the sync block, I believe your code is functionally identical to my proposal of:\n\nif (fs.mkdirs(dir) || fs.exists(dir)) {\n   return file;\n}\n\nOr am I missing something? If we need to synchronize, we really need to do it everywhere and do it consistently.\n\nOn a side note, the Configuration's getFile roll-over between local directories is problematic. The problem is that readers need to find the file regardless of where it was written. So if the writer can spill over to other directories, there should be a findFile(?) that looks in all of the directories (in the right order) until it finds it. That way readers can find the file regardless of which directory the writer was spilled in to.", "Not sure I understand that, a sync block would still not  protect againt multiple processes trying to create the same hierarchy, and the races would still bite you...\n\nMaybe the safest thing to do is to traverse the hierarchy in getLocalPath(), creating each directory and checking\nfor it's existence.\n  \n\n", "\nSameer -> I think you're right. Ultimately, the synchronize is not good enough, and it seems that you've got the right solution.\n\nOwen ->  here's the case Naveen described: lets say five directory levels are created by the call to mkdirs(), and there are two threads entering mkrdirs() at about the same time. Will one of the two threads exit before the fifth directory is created? I dont have the source code, but it seems to me that it could. If that would happen, that thread will step ahead to the next directory because when it does the exists() check, the directory will not exist. Which leads to Sameer's point that even the synchronize is not good enough.\n\n\n\n", "Below is what mkdirs looks like according to the jad decompiler (File.class from is from Sun JDK 1.5).\n\nIt looks like to me if two processes/threads are trying to create \"/a/b/c/d/e/\" and nothing yet exists, they both try to create \"/a\". One will fail, while the other succeeds. The failing process will return failure early, while the other process continues to create \"b/c/d/e/\". If the failing process after returning from mkdirs() now calls exists(\"/a/b/c/d/e\"), exists() could  return false because the other process is still creating the directories along the path.\n\nSo probably Sameer's suggestion of traversing in getLocalPath() is the best solution.\n\npublic boolean mkdirs()\n    {\n        if(exists())\n            return false;\n        if(mkdir())\n            return true;\n        File file = null;\n        try\n        {\n            file = getCanonicalFile();\n        }\n        catch(IOException ioexception)\n        {\n            return false;\n        }\n        String s = file.getParent();\n        return s != null && (new File(s, fs.prefixLength(s))).mkdirs() && file.mkdir();\n    }\n", "Ok, I see your point. \n\nLet's go with the traversing, but I think it should be done in the FileSystem.mkdirs rather than in Configuration.", "FileSystem.mkdirs is an abstract method. It would have been nice to put the traversal logic there. FSDirectory.mkdirs() already does a traversal like we need, so perhaps that code can be copied and modified for LocalFileSystem.mkdirs() ?", "Sameer is working on a fix for this in LocalFileSystem.mkdirs()", "Attached patch for mkdir issue. LocalFileSystem.mkdirs() now traverses the hierarchy creating each directory along the way. This patch changes the semantics of  'mkdirs' in the FileSystem interface. The semantics are now those of 'mkdir -p', in that existence of the specified directory or any ancestor of it is no longer an error.\n\nAlso updates mkdirs in dfs.FSDirectory so that it has the same behavior.", "This patch conflicts with HADOOP-240, which I just committed.  Can you please resolve this?  Thanks.", "Attached new patch.", "I just committed this.  Thanks, Sameer!"], "derived": {"summary": "(attached: a patch to fix the problem, and a logfile showing the problem occuring twice)\n\nThere is a race condition in Configuration. java:\n\n       Path file = new Path(dirs[index], path);\n       Path dir = file.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Race condition in Configuration.getLocalPath() - (attached: a patch to fix the problem, and a logfile showing the problem occuring twice)\n\nThere is a race condition in Configuration. java:\n\n       Path file = new Path(dirs[index], path);\n       Path dir = file."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Sameer!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-278", "title": "a missing map/reduce input directory does not produce a user-visible error message", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-06-07T00:00:03.000+0000", "updated": "2009-07-08T16:51:46.000+0000", "description": "If map/reduce's input directory is in DFS and does not exist, the user has to find the problem in the jobtracker logs rather than either the launching program or the webapp.", "comments": ["This patch adds a method for InputFormat that checks to see if a list of input paths is valid. InputFormatBase implements the simple check to make sure that each path represents a directory. JobClient checks the validity of each path and stops the job submission with a message to the user if any directories are invalid.", "I just committed this.  Thanks, Owen!"], "derived": {"summary": "If map/reduce's input directory is in DFS and does not exist, the user has to find the problem in the jobtracker logs rather than either the launching program or the webapp.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "a missing map/reduce input directory does not produce a user-visible error message - If map/reduce's input directory is in DFS and does not exist, the user has to find the problem in the jobtracker logs rather than either the launching program or the webapp."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-279", "title": "running without the hadoop script causes warnings about log4j not being configured correctly", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-06-07T03:14:11.000+0000", "updated": "2009-07-08T16:51:46.000+0000", "description": "Anyone who runs hadoop programs without the bin/hadoop script will get a message about log4j not being configured correctly. The problem is that the config file uses the system property \"hadoop.root.logger\", which is set in the script.", "comments": ["This patch sets default values for the three system properties that are used in the default property files.\n\nhadoop.root.logger=INFO,stdout\nhadoop.log.dir=.\nhadoop.log.file=hadoop.log\n\nIt also includes some examples of increasing logging levels.", "I just committed this.  Thanks, Owen."], "derived": {"summary": "Anyone who runs hadoop programs without the bin/hadoop script will get a message about log4j not being configured correctly. The problem is that the config file uses the system property \"hadoop.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "running without the hadoop script causes warnings about log4j not being configured correctly - Anyone who runs hadoop programs without the bin/hadoop script will get a message about log4j not being configured correctly. The problem is that the config file uses the system property \"hadoop."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-280", "title": "AllTestDriver has incorrect class name for DistributedFSCheck test", "status": "Closed", "priority": "Major", "reporter": "Konstantin Shvachko", "assignee": null, "labels": [], "created": "2006-06-07T03:46:44.000+0000", "updated": "2006-08-03T17:46:46.000+0000", "description": "It has TestDFSIO instead.", "comments": ["TestDFSIO. class is replaced by DuistributedFSCheck.class", "I just committed this.  Thanks!"], "derived": {"summary": "It has TestDFSIO instead.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "AllTestDriver has incorrect class name for DistributedFSCheck test - It has TestDFSIO instead."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-281", "title": "dfs.FSDirectory.mkdirs can create sub-directories of a file!", "status": "Closed", "priority": "Major", "reporter": "Sameer Paranjpye", "assignee": "Wendy Chien", "labels": [], "created": "2006-06-07T06:05:21.000+0000", "updated": "2006-09-08T21:19:44.000+0000", "description": "dfs.FSDirectory.mkdirs will merrily adds children to a directory tree node without checking whether it represents a directory. So it is possible to create a subdirectories of a file.\n\n", "comments": ["I have attached a fairly simple patch which essentially checks to ensure that all existing components of the path are directories by checking while 'collecting' the parents in dfs.FSDirectory.mkdirs.\n\nThis patch also necessiated (straightforward) changes to a few places where the return-value of 'mkdirs' wasn't being checked...\n\nthanks,\nArun", "Hi Arun, \nI was simultaneously working on this bug and had a different fix.  Hairong and I think it would be simpler to check the path/parent is a directory in addNode (which is called by unprotectedMkdir which in turn is called by mkdirs).  I think the check in addNode is required regardless of mkdirs. \n\nI have attached my patch file along with test code for the change.  \n\nPlease look at it and let me know what you think.  \n\nThanks,\nWendy\n", "Hi Wendy,\n\n  Your approach looks fine too... I'll just let Doug/Konstantin or someone else with more experience with dfs codebase decide.\n\n  However, a subtle issue: there are some places in the code base (e.g. FileUtil.java) which makes a call to 'mkdirs' and then fail to check the return-value to ensure that it completed successfully, which lead to other bugs. At the very least we will need to patch FileUtil.java (it's a part of my mkdirs.patch). Could you further test your patch using the dfs shell? \n\n  E.g. \n  $ hadoop dfs -mkdir /tmp/test-mkdir\n  $ hadoop dfs -copyFromLocal ./foo.txt /tmp/test-mkdir/foo.txt\n  $ hadoop dfs -mkdir /tmp/test-mkdir/foo.txt/d1/d2\n  $ hadoop dfs -mkdir /tmp/test-mkdir/foo.txt/d1\n  $ hadoop dfs -put ./some_directory /tmp/test-mkdir/foo.txt/some_directory\n\n  Your patch fails only in the last test-case above (due to non-existent return-value check in FileUtil.java). \n  Could you please take a look and maybe include my patch for FileUtil.java too?\n\nthanks,\nArun\n ", "I think addNode() is a good place to check whether the parent is a directory.\nmkdirs() does not need to check all parents in the path, just the last one, since\nall other parents have already been checked before.\nI agree with Arun, the return value of mkdirs() should be checked every time\nit is called to make sure the directories were actually created, including FileUtil\ncopy methods.\nFor mkdirs() I would let the FileNotFoundException exception thrown by addNode()\ngo through, rather than catching it and returning boolean, so that whoever calls\nmkdirs() could decide what to do.\nBut there might be different opinions on that, and this will require more code changes.\n", "Yes, the callers of mkdirs will need to check the return value.  I looked over Arun's changes to FileUtil and they seem fine.  I can make a separate patch which includes those changes once we decide if mkdirs should pass along the FilenotFoundException or only return a boolean.\n\n", "Why not throw an exception on failure?  That seems like the more correct thing to do.  \nChecking return codes for failure is so 20th century.", "throwing an exception was suggested, but rejected in order not to change a public interface that may be (is) used by applications.", "We can still return the correct boolean value to preserve public interface,\nbut throw an exception at the same time. The prototype is in place.\nOr we can use a usual \"trick\" of deprecating the old mkdirs() , and define a\nnew one that is void.\nWhich mkdirs() is called by applications?\nI believe mkdirs() is public since it is a part of the ClientProtocol interface.\nThere are other mkdirs() further in the NameNode data structure implementation.\nThose should not be and are not called by anything outside the dfs project.\nThese mkdirs() don't need to be public at all, imo.\n", "Have we considered this to be a feature. Some newer general purpose file system allow a file to also be a directory (and visa versa). It is an abstraction that nicely folds in the functionality of streams and resource forks. For example, rather than sticking crcs and other attributes for a file in the same directory as a file (which necessitates a bunch of ugly skip logic in the code), we could simply put them in a subdirectory of the file.", "I just committed this.  Thanks, Wendy!\n\nBen: yes, it would be a feature, but a feature with many bugs.  There are lots of places in the current API that assume files are distinct from directories.  So, perhaps, in a separate issue, we could discuss adding this as a feature (and how we'd implement it on linux and windows), but, for now, the most consistent thing to do is treat it as a bug."], "derived": {"summary": "dfs. FSDirectory.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "dfs.FSDirectory.mkdirs can create sub-directories of a file! - dfs. FSDirectory."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Wendy!\n\nBen: yes, it would be a feature, but a feature with many bugs.  There are lots of places in the current API that assume files are distinct from directories.  So, perhaps, in a separate issue, we could discuss adding this as a feature (and how we'd implement it on linux and windows), but, for now, the most consistent thing to do is treat it as a bug."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-282", "title": "the datanode crashes if it starts before the namenode", "status": "Closed", "priority": "Critical", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-06-07T06:07:12.000+0000", "updated": "2009-07-08T16:41:54.000+0000", "description": "If the datanode tries to register before the namenode is offering service, it crashes with a uncaught exception.\n\njava.net.ConnectE\nxception: Connection refused\n        at java.net.PlainSocketImpl.socketConnect(Native Method)\n        at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)\n        at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)\n        at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)\n        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)\n        at java.net.Socket.connect(Socket.java:507)\n        at java.net.Socket.connect(Socket.java:457)\n        at java.net.Socket.<init>(Socket.java:365)\n        at java.net.Socket.<init>(Socket.java:207)\n        at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:112)\n        at org.apache.hadoop.ipc.Client.getConnection(Client.java:351)\n        at org.apache.hadoop.ipc.Client.call(Client.java:289)\n        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:150)\n        at org.apache.hadoop.dfs.$Proxy0.register(Unknown Source)\n        at org.apache.hadoop.dfs.DataNode.register(DataNode.java:176)\n        at org.apache.hadoop.dfs.DataNode.<init>(DataNode.java:109)\n        at org.apache.hadoop.dfs.DataNode.makeInstanceForDir(DataNode.java:892)\n        at org.apache.hadoop.dfs.DataNode.run(DataNode.java:846)\n        at org.apache.hadoop.dfs.DataNode.runAndWait(DataNode.java:862)\n        at org.apache.hadoop.dfs.DataNode.main(DataNode.java:917)", "comments": ["This patch moves the datanode registration into the offerService method, so that if an exception is thrown it will retry again in 5 seconds.", "Ok, this patch fixes the problem without the bad side effects that the old patch had. It passes the regressions and I'm currently installing this on my 200 node cluster. I'll post a message when I can tell how it goes.", "I just committed this.  Thanks!"], "derived": {"summary": "If the datanode tries to register before the namenode is offering service, it crashes with a uncaught exception. java.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "the datanode crashes if it starts before the namenode - If the datanode tries to register before the namenode is offering service, it crashes with a uncaught exception. java."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-283", "title": "Counting of running maps/reduces in tasktrackerstatus", "status": "Closed", "priority": "Minor", "reporter": "Mahadev Konar", "assignee": "Mahadev Konar", "labels": [], "created": "2006-06-07T08:37:26.000+0000", "updated": "2006-12-15T23:02:17.000+0000", "description": "The functions countreducetasks and countmaptasks currently return the total number of maps/reduces on the tasktracker. They should only return the number of running maps/reduces. Because they return the total count, the jobtracker is not able to schedule tasks on these tasktrackers in the current cycle, when a task is finished and the tasktracker is runnning max number of maps/reduces.", "comments": ["resolved with HADOOP-305"], "derived": {"summary": "The functions countreducetasks and countmaptasks currently return the total number of maps/reduces on the tasktracker. They should only return the number of running maps/reduces.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Counting of running maps/reduces in tasktrackerstatus - The functions countreducetasks and countmaptasks currently return the total number of maps/reduces on the tasktracker. They should only return the number of running maps/reduces."}, {"q": "What updates or decisions were made in the discussion?", "a": "resolved with HADOOP-305"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-284", "title": "dfs timeout on open", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-06-07T12:38:18.000+0000", "updated": "2009-07-08T16:41:54.000+0000", "description": "In my sort benchmark, I've started seeing hundred's of maps dying with timeout exceptions in the dfs.open() code:\n\njava.net.SocketTimeoutException: timed out waiting for rpc response at org.apache.hadoop.ipc.Client.call(Client.java:305) at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:153) at org.apache.hadoop.dfs.$Proxy1.open(Unknown Source) at org.apache.hadoop.dfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:457) at org.apache.hadoop.dfs.DFSClient$DFSInputStream.(DFSClient.java:444) at org.apache.hadoop.dfs.DFSClient.open(DFSClient.java:207) at org.apache.hadoop.dfs.DistributedFileSystem.openRaw(DistributedFileSystem.java:90) at org.apache.hadoop.fs.FSDataInputStream$Checker.(FSDataInputStream.java:46) at org.apache.hadoop.fs.FSDataInputStream.(FSDataInputStream.java:228) at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:157) at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:72) at org.apache.hadoop.dfs.DistributedFileSystem.copyToLocalFile(DistributedFileSystem.java:182) at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.localizeTask(TaskTracker.java:535) at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.launchTask(TaskTracker.java:584) at org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:395) at org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:308) at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:424) at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:961)\n", "comments": ["This was fixed already."], "derived": {"summary": "In my sort benchmark, I've started seeing hundred's of maps dying with timeout exceptions in the dfs. open() code:\n\njava.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "dfs timeout on open - In my sort benchmark, I've started seeing hundred's of maps dying with timeout exceptions in the dfs. open() code:\n\njava."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was fixed already."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-285", "title": "Data nodes cannot re-join the cluster once connection is lost", "status": "Closed", "priority": "Blocker", "reporter": "Konstantin Shvachko", "assignee": "Hairong Kuang", "labels": [], "created": "2006-06-07T15:26:00.000+0000", "updated": "2009-07-08T16:41:55.000+0000", "description": "A data node looses connection to a name node and then tries to offerService() again.\nHADOOP-270 changes force it to start dataXceiveServer, which is already started and in this case\nthrows IllegalThreadStateException, which goes on in a loop, and never reaches the heartbeat section.\nSo the data node never re-joins the cluster, while from the out side it looks it's still running.\nThis is another reason why we see missing data, and don't see failed data nodes.\n", "comments": ["This patch starts the data receiver thread in \"run\" instead of \"offerservice\". So it will not be restarted after a connect is lost.", "I just committed this.  Thanks, Owen!"], "derived": {"summary": "A data node looses connection to a name node and then tries to offerService() again. HADOOP-270 changes force it to start dataXceiveServer, which is already started and in this case\nthrows IllegalThreadStateException, which goes on in a loop, and never reaches the heartbeat section.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Data nodes cannot re-join the cluster once connection is lost - A data node looses connection to a name node and then tries to offerService() again. HADOOP-270 changes force it to start dataXceiveServer, which is already started and in this case\nthrows IllegalThreadStateException, which goes on in a loop, and never reaches the heartbeat section."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-286", "title": "copyFromLocal throws LeaseExpiredException", "status": "Closed", "priority": "Major", "reporter": "Runping Qi", "assignee": "Konstantin Shvachko", "labels": [], "created": "2006-06-07T23:50:34.000+0000", "updated": "2009-07-08T16:41:54.000+0000", "description": "\nLoading local files to dfs through hadoop dfs -copyFromLocal failed due to the following exception:\n\ncopyFromLocal: org.apache.hadoop.dfs.LeaseExpiredException: No lease on output_crawled.1.txt\n        at org.apache.hadoop.dfs.FSNamesystem.getAdditionalBlock(FSNamesystem.java:414)\n        at org.apache.hadoop.dfs.NameNode.addBlock(NameNode.java:190)\n        at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:585)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:243)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:231)\n", "comments": ["It looks like the following scenario leads to this exception.\nLEASE_PERIOD = 60 sec is a global constants defining for how long a lease is issued.\nDFSClient.LeaseChecker renews this client leases every 30 sec = LEASE_PERIOD/2.\nIf the renewLease() fails then the client retries to renew every second.\nOne of the most popular reasons the renewLease() fails is because it timeouts SocketTimeoutException.\nThis happens when the namenode is busy, which is not unusual since we lock it for each operation.\nThe socket timeout is defined by the config parameter \"ipc.client.timeout\", which is set to 60 sec in\nhadoop-default.xml That means that the renewLease() can last up to 60 seconds and the lease will\nexpire the next time the client tries to renew it, which could be up to 90 seconds after the lease was\ncreated or renewed last time.\nSo there are 2 simple solutions to the problem:\n1) to increase LEASE_PERIOD\n2) to decrease ipc.client.timeout\n\nA related problem is that DFSClient sends lease renew requests no matter what every 30 seconds \nor less. It looks like the DFSClient has enough information to send renew messages only if it really \nholds a lease. A simple solution would be avoid calling  renewLease() when \nDFSClient.pendingCreates is empty.\nThis could substantially decrease overall net traffic for map/reduce.\n\n", "This is a very simple patch that renews leases only when pendingCreates is not empty.\nThis prevents the client from sending lease renewal messages when the client\nis not writing into dfs, say just reading or doing local stuff.\nThis should make the name node less busy.\n\nI tried to change the ipc.client.timeout from 60 secs to 20 secs.\nOn my 3 node cluster everything worked fine.\nOn a large cluster the timeout was changed only for the DFSClient.\nThe LeaseExpiredException does not appear anymore.\nBut we need more statistics on that, especially with slower networks.\nThe ipc timeout is global for all ipc connections, so if we make it\nsmaller there is a risk that long lasting operations like block transfers\nwill start to timeout. I haven't seen it.\nIf anybody is willing to try 20 sec ipc timeout please post the results.\nFailing early, and retrying might make things faster in general.", "+1 for not requesting a lease unless a write operation is required (i.e. the patch).", "Patch for not renewing leases when pendingCreates is empty.\nThis is a scalability issue.", "I just committed this.  Thanks, Konstantin!"], "derived": {"summary": "Loading local files to dfs through hadoop dfs -copyFromLocal failed due to the following exception:\n\ncopyFromLocal: org. apache.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "copyFromLocal throws LeaseExpiredException - Loading local files to dfs through hadoop dfs -copyFromLocal failed due to the following exception:\n\ncopyFromLocal: org. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Konstantin!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-287", "title": "Speed up SequenceFile sort with memory reduction", "status": "Closed", "priority": "Major", "reporter": "Benjamin Reed", "assignee": "Christopher Douglas", "labels": [], "created": "2006-06-08T04:40:37.000+0000", "updated": "2008-04-17T05:26:38.000+0000", "description": "I replaced the merge sort with a quick sort and it yielded approx 30% improvement in sort time. It also reduced the memory requirement for sorting because the sort is done in place.", "comments": ["This looks good, but it doesn't pass unit tests any longer.  In particular, the following fails for me:\n\nant -Dtestcase=TestSequenceFile test\n\nThis results in something like the following for me:\n\njava.lang.RuntimeException: wrong key at 2838\n\tat org.apache.hadoop.io.TestSequenceFile.checkSort(TestSequenceFile.java:146)\n\tat org.apache.hadoop.io.TestSequenceFile.testSequenceFile(TestSequenceFile.java:53)\n", "My previous patch had two minor typos that gave incorrect results. This patch should work.", "This works for me now.  Thanks!  I'll commit it once we get 0.3.2 out the door.", "What is the status of this patch? I'm running with relatively slow disks, and I could sure use a 30% improvement in sorting time ... ;)", "I have improved it a bit more. It now is guaranteed to only take logN stack space, and I eaked out a bit more performance. Unfortunately, the 30% improvement is for the in-memory sort. For your slow disks you need my other patch which reduces the number of times the data hits the disk. Unfortunately, that patch doesn't apply anymore. I have a newer version that removes one more full disk hit, so that should work even better. I'll try to create a patch for it today. (Unrelated changes break my patches and take a long time to reconcile...", "I just committed this, with one change. Your patch mistakenly removed the 'in' field.\n\nThanks, Ben! ", "This patch broke the build. In particular, the org.apache.hadoop.mapred.TestMiniMRLocalFS no longer finishes. When the job becomes stuck, the reduces are still running. I assume they are stuck in an infinite loop somewhere. ", "I just reverted this.", "Benjamin:\nWould it be possible to look into what broke in the latest patch?", "\n   [[ Old comment, sent by email on Thu, 8 Jun 2006 07:32:09 -0700 ]]\n\nDuh! Sorry Doug. Stupid error. I didn't realize you had unit tests.  \nI've fixed it. I'll run the tests now.\n\nben\n\n\n", "Merged patch w/ latest trunk.", "-1 overall.  Here are the results of testing the latest attachment \nhttp://issues.apache.org/jira/secure/attachment/12374300/287-0.patch\nagainst trunk revision 615723.\n\n    @author +1.  The patch does not contain any @author tags.\n\n    javadoc +1.  The javadoc tool did not generate any warning messages.\n\n    javac +1.  The applied patch does not generate any new javac compiler warnings.\n\n    release audit +1.  The applied patch does not generate any new release audit warnings.\n\n    findbugs -1.  The patch appears to introduce 2 new Findbugs warnings.\n\n    core tests +1.  The patch passed core unit tests.\n\n    contrib tests +1.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1703/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1703/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1703/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1703/console\n\nThis message is automatically generated.", "Fixed findbugs warnings", "+1 overall.  Here are the results of testing the latest attachment \nhttp://issues.apache.org/jira/secure/attachment/12374325/287-1.patch\nagainst trunk revision 615723.\n\n    @author +1.  The patch does not contain any @author tags.\n\n    javadoc +1.  The javadoc tool did not generate any warning messages.\n\n    javac +1.  The applied patch does not generate any new javac compiler warnings.\n\n    release audit +1.  The applied patch does not generate any new release audit warnings.\n\n    findbugs +1.  The patch does not introduce any new Findbugs warnings.\n\n    core tests +1.  The patch passed core unit tests.\n\n    contrib tests +1.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1707/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1707/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1707/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1707/console\n\nThis message is automatically generated.", "Chris, the patch just updates the SequenceFile.java code. However, this will not affect the sort that the Map tasks uses. The SequenceFile's sort is in some sense deprecated but the code is lying around there. Look at org.apache.hadoop.mapred.MapTask.MapOutputBuffer to get an idea of how the sorting infrastructure works. You can plug in quicksort easily (look at org.apache.hadoop.mapred.MergeSorter which is the default sort implementation).", "Deveraj-\nAh, I see. Thanks for the direction. As part of adding QuickSort, would it be worthwhile to explicitly deprecate some of the dead code in SequenceFile? Is anyone aware of other projects that might be using it?", "This simply makes QuickSorter available as a target for map.sort.class.", "-1 overall.  Here are the results of testing the latest attachment \nhttp://issues.apache.org/jira/secure/attachment/12374623/287-2.patch\nagainst trunk revision 616796.\n\n    @author +1.  The patch does not contain any @author tags.\n\n    javadoc +1.  The javadoc tool did not generate any warning messages.\n\n    javac -1.  The applied patch generated 618 javac compiler warnings (more than the trunk's current 617 warnings).\n\n    release audit +1.  The applied patch does not generate any new release audit warnings.\n\n    findbugs +1.  The patch does not introduce any new Findbugs warnings.\n\n    core tests +1.  The patch passed core unit tests.\n\n    contrib tests +1.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1730/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1730/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1730/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1730/console\n\nThis message is automatically generated.", "Removed deprecation warning.", "-1 overall.  Here are the results of testing the latest attachment \nhttp://issues.apache.org/jira/secure/attachment/12374629/287-2.patch\nagainst trunk revision 616796.\n\n    @author +1.  The patch does not contain any @author tags.\n\n    javadoc +1.  The javadoc tool did not generate any warning messages.\n\n    javac +1.  The applied patch does not generate any new javac compiler warnings.\n\n    release audit +1.  The applied patch does not generate any new release audit warnings.\n\n    findbugs +1.  The patch does not introduce any new Findbugs warnings.\n\n    core tests +1.  The patch passed core unit tests.\n\n    contrib tests -1.  The patch failed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1732/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1732/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1732/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1732/console\n\nThis message is automatically generated.", "The current patch effected no measurable improvements for any benchmarks we ran, so I'm pulling it back for now.", "This should be measurable only if the output key set of a map task is big enough. This may happen when the initial Map generates many  {K2,V2} for one input tuple {K1,V1}.\n\nStill I think merge sort in some cases is worth considering (but not for the current implementation)\n\n1. For small data sets,  performance characteristic of merge sort and quick sort should be almost the same in many ways \n- Neglect the fact that merge sort requires more memory which is small in this case\n- Merge sort on index array doesn't incur more objects for GC to be collected\n\n2. For medium-sized data sets (I don't indicate how big is medium), quick sort should be better (assume good pivoting so worst cases are rare).\n\n3. For very large data sets, a good implementation of merge sort which allows  max memory setting and really does multi-pass pipelined merging on the capped memory buffer should yield better performance plus better control over memory usage.", "The difference in time taken by the respective sort algorithms is not significant compared to the overhead incurred in the sort phase, i.e. the time it requires to recreate the BlahSorter/BlahSort objects, recreate and resize all the accounting arrays, and effect the indexed compares/swaps via IntWritables. There is an additional copy for the index array during mergesort, but- again- this is insignificant when considering the cost of the sort/spill. Note that the time taken for each invocation of sort is proportional to the number of keys collected for each partition, which itself is a small fraction of the time spent in that phase; the number of invocations of sort is roughly proportional to the number of reducers, or more specifically: the distribution of output keys effected from the partitioner from a given map within the io.sort.mb limit. Given the current structure of the sort/spill, the particular sort algorithm is unlikely to have a notable impact in the general case. The size of the dataset or the proportion of input to output keys should have little bearing, since the merge code for spilled data is the same, regardless of the sort algorithm selected.", "HADOOP-2919 moved to quick sort."], "derived": {"summary": "I replaced the merge sort with a quick sort and it yielded approx 30% improvement in sort time. It also reduced the memory requirement for sorting because the sort is done in place.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Speed up SequenceFile sort with memory reduction - I replaced the merge sort with a quick sort and it yielded approx 30% improvement in sort time. It also reduced the memory requirement for sorting because the sort is done in place."}, {"q": "What updates or decisions were made in the discussion?", "a": "HADOOP-2919 moved to quick sort."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-288", "title": "RFC: Efficient file caching", "status": "Closed", "priority": "Major", "reporter": "Michel Tourn", "assignee": "Mahadev Konar", "labels": [], "created": "2006-06-08T08:28:11.000+0000", "updated": "2006-10-06T21:48:56.000+0000", "description": "RFC: Efficient file caching \n(on Hadoop Task nodes, for benefit of MapReduce Tasks)\n------------------------------------------------------\n\nWe will start implementing this soon. Please provide feedback and improvements to this plan.\n\nThe header \"Options:\" indicates places where simple choices must be made.\n\n\nProblem:\n-------\no MapReduce tasks require access to additional out-of-band data (\"dictionaries\")\n\nThis out-of-band data is:\n\no in addition to the map/reduce inputs.\no large (1GB+)\no broadcast (same data is required on all the Task nodes)\no changes \"infrequently\", in particular:\noo it is always constant for all the Tasks in a Job. \noo it is often constant for a month at a time \noo it may be shared across team members\no sometimes used by pure-Java MapReduce programs\no sometimes used by non-Java MapReduce programs (using Hadoop-Streaming)\no (future) used by programs that use HDFS and Task-trackers but not MapReduce.\n\nExisting Solutions to the problem:\n---------------------------------\nThese solutions are not good enough. The present proposal is to do Sol 1 with caching.\n\nSol 1: Pure Hadoop: package the out-of-band data in the MapReduce Job jar file.\nSol 2: Non  Hadoop: for each task node run rsync from single source for data.\nSol 3: Non  Hadoop: use BitTorrent, etc.\n\nSol.1 is correct but slow for many reasons:\n The Job submitter must recreate a large jar(tar) file for every Job.\n  (The jar contains both changing programs and stable dictionaries)\n The large Jar file must be propagated from the client to HDFS with \n a large replication factor. \n At the beginning of every Task, the Task tracker gets the job jar from HDFS \n and unjars it in the working directory. This can dominate task execution time.\n \nSol.2 has nice properties but also some problems.\n It does not scale well with large clusters (many concurrent rsync read requests i.e. single-source broadcast)\n It assumes that Hadoop users can upload data using rsync to the cluster nodes. As a policy, this is not allowed.\n It requires rsync.\n \nSol.3 alleviates the rsync scalability problems but \n      It is a dependency on an external system. \n      We want something simpler and more tightly integrated with Hadoop.\n      \n\nStaging (uploading) out-of-band data:\n------------------------------------\nThe out-of-band data will often originate on the local filesystem of a user machine \n (i.e. a MapReduce job submitter)\nNevertheless it makes sense to use HDFS to store the original out-of-band data because:\no HDFS has (wide) replication. This enables scalable broadcast later.\no HDFS is an available channel to move data from clients to all task machines.\no HDFS is convenient as a shared location among Hadoop team members.\n\n\nAccessing (downloading) out-of-band data:\n----------------------------------------\nThe non-Java MapReduce programs do not have or want[1] APIs for HDFS.\nInstead these programs just want to access out-of-band data as \n local files at predefined paths.\n([1] Existing programs should be reusable with no changes. \n This is often possible bec. communication is over stdin/stdout.)\n\n\n\nJob's jar file as a special case:\n--------------------------------\nOne use case is to allow users to make the job jar itself cachable.\n\nThis is only useful in cases where NOTHING changes when a job is resubmitted\n (no MapRed code changes and no changes in shipped data)\nThis situation might occur with an 'extractor' job (gets data from an external source: like Nutch crawler)\n\nCurrently the Hadoop mapred-jar mechanism works in this way:\n the job jar data is unjarred in the \"working directory\" of the Task \n the jar contains both MapRed java code (added to classpath)\n\n\n\nCache synchronization:\n---------------------\n\nThe efficient implementation of the out-of-band data distribution\nis mostly a cache synchronization problem.\nA list of the various aspects where choices must be made follows.\n\n\nCache key:\n---------\nHow do you test that the cached copy is out-of-date?\n\nOptions: \n1. the archive/file timestamp \n2. the MD5 of the archive/file content\n\nComparing source and destination Timestamps is problematic bec. it assumes synchronized clocks.\nAlso there is no last-modif metadata in HDFS (for good reasons, like scalability of metadata ops)\n\nTimestamps stored with the source ('last-propagate-time') do \n not require synchronized clocks, only locally monotonic time. \n(and the worse which can happen at daylight-savings switch is a missed update or an extra-update)\n\nThe cache code could store a copy of the local timestamp \nin the same way that it caches the value of the content hash along with the source data.\n \n\n\nCachable unit:\n-------------\nOptions: individual files or archives or both.\n\nNote:\nAt the API level, directories will be processed recursively \n(and the local FS directories will parallel HDFS directories)\nSo bulk operations are always possible using directories.\nThe question here is whether to handle archives as an additional bulk mechanism.\n\n\nArchives are special because:\no unarchiving occurs transparently as part of the cache sync\no The cache key is computed on the archive and preserved although \n  the archive itself is not preserved.\nSupported archive format will be: tar (maybe tgz or compressed jar)\nArchive detection test: by filename extension \".tar\" or \".jar\"\n\nSuppose we don't handle archives as special files:\nPros:\n o less code, no discussion about which archive formats are supported\n o fine for large dictionary files. And when files are not large, user may as well\n   put them in the Job jar as usual.\n o user code could always check and unarchive specific cached files\n   (as a side-effect of MapRed task initialization)\nCons:\n o handling small files may be inefficient \n  (multiple HDFS operations, multiple hash computation, \n   one 'metadata' hash file along with each small file)\n o It will not be possible to handle the Job's jar file as a special case of caching \n\n\n\nCache isolation: \n---------------\nIn some cases it may be a problem if the cached HDFS files are updated while a Job is in progress:\nThe file may become unavailable for a short period of time and some tasks fail.\nThe file may change (atomically) and different tasks use a different version.\n\nThis isolation problem is not addressed in this proposal.\nStandard solutions to the isolation problem are:\n\no Assume that Jobs and interfering cache updates won't occur concurrently.\n\no Put a version number in the HDFS file paths and refer to a hard-coded version in the Job code.\n\no Before running the MapRed job, run a non-distributed application that tests\n  what is the latest available version of the out-of-band data. \n  Then make this version available to the MapRed job.\n  Two ways to do this. \n  o either set a job property just-in-time:\n    addCachePathPair(\"/mydata/v1234/\", \"localcache/mydata_latest\"); \n    (see Job Configuration for meaning of this)\n  o or publish the decision as an HDFS file containing the version.\n    then rely on user code to read the version, and manually populate the cache:\n    Cache.syncCache(\"/hdfs/path/fileordir\", \"relative/local/fileordir\");\n    (see MapReduce API for meaning of this)\n\n\nCache synchronization stages:\n----------------------------\nThere are two stages: Client-to-HDFS and HDFS-to-TaskTracker\n\no Client-to-HDFS stage.\nOptions: A simple option is to not do anything here, i.e. rely on the user.\n\nThis is a reasonable option given previous remarks on the role of HDFS:\n HDFS is a staging/publishing area and a natural shared location.\nIn particular this means that the system need not track \nwhere the client files come from.\n\n\no HDFS-to-TaskTracker:\nClient-to-HDFS synchronization (if done at all) should happen before this.\nThen HDFS-to-TaskTracker synchronization must happen right before \nthe data is needed on a node.\n\n\n\nMapReduce cache API:\n-------------------\nOptions:\n\n1. No change in MapReduce framework code:\nrequire the user to put this logic in map() (or reduce) function:\n\n in MyMapper constructor (or in map() on first record) user is asked to add:\n \n    Cache.syncCache(\"/hdfs/path/fileordir\", \"relative/local/fileordir\");\n    Cache.syncCache(\"...\"); //etc.\n  \n-----\n\n2. Put this logic in MapReduce framework and use Job properties to\n   communicate the list of pairs (hdfs path; local path)\n \nDirectories are processed recursively.\nIf archives are treated specially then they are unarchived on destination.\n\n \nMapReduce Job Configuration:\n---------------------------\nOptions:\n\nwith No change in MapReduce framework code (see above)\n no special Job configuration: \n   it is up to the MapRed writer to configure and run the cache operations.\n\n---\nwith Logic in MapReduce framework (see above)\n some simple Job configuration\n\nJobConf.addCachePathPair(String, String)\nJobConf.addCachePathPair(\"/hdfs/path/fileordir\", \"relative/local/fileordir\");\n\n", "comments": ["Hadoop proposal: file caching\nupdated description with more details.\n----------\n\nEfficient file caching \n(on Hadoop Task nodes, for benefit of MapReduce Tasks)\n\nOverview\n========\nThis document describes a mechanism for caching files or archives on taskTracker nodes' local file system.\nExporting and extracting large archives from HDFS to local filesystem is expensive.\nAnd is often required by the applications.\nCurrently this would happen at the beginning of every Task of a MapReduce Job.\n\nThe purpose of the Hadoop file cache is to minimize this overhead by\npreserving and reusing the extracted data\n\nDuring a MapRed job there are two kinds of data being uploaded to a Hadoop cluster:\n  Java code and Out-of-band data.\n\nJava code may include libraries so this can easily get large. (megabytes)\n\nOut-of-band data is any data used by the job, in addition to the map input or the reduce input.\nFor example a large dictionary of words. This can also get large (gigabytes)\n\n\nThere are two main kinds of cacheable files:\n1. The MapReduce job jar. \n   This contains Java code and possibly out-of-band data.\n2. Additional archives\n   This contains out-of-band data.\n\nThe proposed solution suggests that\nCacheable files:\nare stored in HDFS, and specified in the JobConf of a MapReduce job.\nA special case is the job jar file, which will get cached by default.\n\nSupported formats for cacheable files are jar, tar and gzip, \nAdditional formats could be added at a future time.\nRegular files are also supported\n\n\nWorkflow:\n========\nFor out-of-band data, the user must first explicitly upload archives to HDFS.\nThis can be done using any HDFS client.\nIt is typical for out-of-band data to be reused across Jobs and users.\n\nThe user specifies the out-of-band data using:\nJobConf.addCachedArchive() or JobConf.addCachedFile()\n\nThe user specifies the job jar as today:\nJobConf.setJar(\"f.jar\") which implicitly has the effect of:\nJobConf.addCachedArchive(\"f.jar\"). \n\nWhen a Job starts, the JobTracker does the following for each cached archive.\nCompute a strong hash of the archive and store the hash in the HDFS.\nTo avoid reading and scanning the archive, the strong hash is based\non the existing HDFS block-CRC codes rather than on the actual content.\n\nWhen a Task starts, the TaskTracker does the following for each cached archive.\nRetrieve the strong hash from HDFS, compare with the hash of the local copy.\nIf the local hash does not exist or is different, then\n  retrieve the archive, unarchive it, update the local hash.\nIf the archive is the job jar, then\n  copy or hard-link the archive contents to the Task working directory.\nThen start the TaskRunner as usual.\n\nOnce the Task is running, the user code may access the cached archive contents.\nThis usually happens at initialization time.\nIf the JobConf added the cached archive: /hdfsdir/path/f.jar\nThen the task can expect to access the archive content at:\n$HADOOP_CACHE/hdfsdir/path/fdir/ffile \nor maybe:\n$HADOOP_CACHE/hdfsdir/path/f_jar/fdir/ffile\nThe second option guarantees that multiple archives \nin the same directory will not clobber each other.\nThe translation of f.jar to f_jar is a convention to ease the distinction of file names and directory names.\n\n\nNote that in the above, the HDFS paths are mirrored on the local filesystem.\nThe intent is to provide namespace protection.\n[i.e. the contents of hdfs1/archive.jar and hdfs2/archive.jar should not collide in the cache]\nThe intent is not to make cache paths interchangeable with HDFS paths. \n\n\nThe variable HADOOP_CACHE is made available to the task as\na JobConf property that is dynamically set by the TaskRunner code.\n\nCache size control:\n------------------\nWe cannot let the cache grow unbounded.\n\nThe cache is always up-to-date at the start of a job.\nSo the configurable parameter should not be the age of the cached data \nbut the total size of the cache. \nThe cache size is a static TaskTracker configuration parameter.\n\nLRU (least recently used) policy:\nOn each Task tracker, the cache manager will measure the total size of the cache\nand expire the oldest cached items. \nWhen a cached item is requested again in a different job, it goes back to the top.\n\nThe cached archive contents are required for the MapReduce task to function.\nSo when the promised cache contents cannot be provided, \nthe cache manager will force a job failure \n\nBefore new files are added to the cache, we do this size test.\nIf the cache size limit WOULD require to expire files...\n1. .. expire files for completed jobs then everything is fine: delete them.\n2. .. expire files for jobs that are already running, then the NEW job fails.\n3. .. expire files for the new job then the new job fails.\n\nNote that a file (archive) may belong to multiple jobs.\n\nIn normal use the cache size is expected to be significantly larger \nthan the files requested by a single job. \nSo the failure modes due to cache overflow should rarely occur.\n\nTHE END.\n", "\nWhy not \n- leave the \"archived\" data in DFS,\n- with its replication level set to infinite, and\n- make a change to the DFS client so that it will replicate blocks of such files locally when they are accessed, and\n- ensure that blocks that are local are accessed through the local file system instead of through DFS\n\nWouldnt that be simpler than having a whole new mechanism?\n\n", "Yes, staying within the context of DFS could be simpler. \nNote however that we have these requirements:\n1. archive files are sometimes used by non-Java non-Hadoop MapReduce programs (using http://wiki.apache.org/lucene-\nhadoop/HadoopStreaming) \n2. avoid repetitive expansion of the job jar and of other archives for each Task in the Job.\n3. In case of many small files, avoid a per-file overhead for DFS and cache operations.\n\nBecause of 1. the files must really be native OS files, not DFS files. \nFor such general tools, the \"common-denominator API\" is only: the base directory for the cache.\n\nToday, unarchiving the job jar occurs in Hadoop, not in the MapRed application. But it is not cached.\nBecause of 2. and 3. the unarchiving process itself must be cacheable.\nSo unarchiving must occur in the Hadoop framework, not the MapRed application.\n", "A problematic choice must be made to implement the following.\n--\nCurrently without caching: in Hadoop the Task working directory contains the expanded contents of the job jar. \nLater with caching: the Task working directory contents should be efficiently created from the filecache contents.\n--\nSo how to synchronize the task working directory job jar data with the file cache?\nOr how to work around the need to do this?\n\nSome options follow, all have problems. \nWhich one is best?\n---------------------------------------\nOption 1. Symbolic links\nThis includes Symbolic links to jar files.\nIt is probably brittle to have classpath elements as symbolic links.\nBut cross-platform support is a little easier than hard links (next)\n--\nOption 2. Hard links\nProblematic for cross-platform support. Both NTFS and UFS can create hard-linked files.\nAt best it works on all platforms but requires launching native/cygwin tools.\nAnd possibly cygwin is not good enough to handle hard links on NTFS.\n--\nOption 2.5 File copy as a fallback mechanism for Option 2.\nProblem is that this is slower and partially defeats the purpose of caching.\n--\nOption 3. \nchange the MapReduce Job jar location.\nThe current convention is \"working directory\" contains MapReduce code jar and resources.\nThe modified convention would be that this directory is a parameter (pointing to a specific directory in the archive cache area)\n This new parameter could be exposed to the MapRed job as JobConf param, as a system property or as environment variable.\nUpside: pure java. \nDownside: not fully backward compatible\n--\n", "Option #3 seems like the simplest to implement, maintain and explain.  \nWould this blow anything up in nutch?  \n\nWe could of course provide a flag that causes the backwards compatible copy for a release or two if really needed.", "I have attached two other files to the patch which are small .jar and .zip files needed for the junit tests.\n\nCaching and job.jars:\n\nTwo parts to the patch:\n1) Unjarring job.jar once for a job\n2) Archiving archives/files locally \n\n1) Unjarring of job.jar\nCurrently the job.jar is unjarred for each task. This patch makes the framework do the unjarring only once for the job. The current working directory for each task if the same directory where the job is unjarred once. \nSo the directory structure now looks like:\n\ntasktracker/jobcache/jod_id/workdir -- the dir where the job is unjarred once\n----------------------------/job_id/task_id/task_specific_job.xml\n\nThe current working dir for each task is the workdir.\n\n2) Archiving of files- \n\ni) Each job can ask for a set of archives/files to be localized. The api for that is \n jobconf.setCacheArchives(comma seperated list of archives)\n or \n jobconf.setCacheFiles(comma seperated list of files).\n The comma seperated list can be specified as absolute path to files/caches (eg. /user/mahadev/test.jar) if they are in the same dfs as the mapred is running on or else they can be specified using urls as in copyfiles ( dfs://hostname:port/path_to_cache )\n There are two apis provided so that users who do not want their archives to be unarchived by the framework or just want to localize a file should use the second api.\n\nii) These archives/files should be present in the specified DFS for localizing.\n    The user makes sure that these archives are present in the DFS before he submits the job else an error will be thrown that these archives are not present in DFS.\n\niii) Localization happens across jobs. So each cache archive/file has a key and the key is the url of the cache (in case of absolute path its the absolute path) \n\niv) Whenever a job is started, the first tasks for these jobs will localize the archives. \n    The archives are stored in mapred.local/tasktracker/archives/hostname_of_dfs/dfs path of the archive.\n    So an archive called /user/mahadev/k.zip  on a dfs running on machine1 would be unarchived in  \n    dir =  mapred.local/tasktracker/archives/machine1/user/mahadev/k.zip/\n    This dir contains the unarchived contents of k.zip.\n    If it is just a file (/user/mahadev/test.txt and not an archive, then it is stored in a directory called \n    mapred.local/tasktracker/archives/machine1/user/mahadev/test.txt/test.txt\n    the local directory name contains test.txt directory just to make it similar to the archive structure.\n\n   if no dfs://hostname:port is specified (eg : setcachefiles(/user/mahadev/test.txt)), in that case it is stored in \n  mapred.local/tasktracker/archives/hostname_of_dfs_usedby_mapred/user/mahadev/test.txt\n\nv) The archives are localized only once and checked for each task if they are fresh and need to be refresed or not.\n   This is done using md5 checksum of the .crc files for the archives.\n   \n   Steps:\n    a) When a job is submitted, the md5 checksums of the required archives/files in dfs are calculated and are written into the \n       jobconf.\n    b) when a task is executing, it matches this md5 to the md5 of the localized cache (stored in memory after it has been localized). If they match its fine to go ahead with this archive.\n       If it does not match then the md5 of the .crc of the file in dfs is calculated. If this does not match then the archives have been changed since the job has been submitted, so the tasks fail with this error. If they do match then the cache is refreshed again. \n\n    c) Two jobs can use the same archives in parallel, but if the second job updates the same archive and tries using the updated archive, then it will fail.\n    \nvi) How to get the localized cache paths\n   An api in the jobconf called jobconf.getLocallizedCacheArchives gives a comma seperated list of localized path of the archives in the same order they had been asked to be localized.\n  Also, you can use names for archives. So you could do something like:\n setcachearchives(x=somearchive)\nand in the maps/reduces do conf.getNamedCache(x) and it will return the localized path of the cache named x.\nvii) Restrictions: \n  Currently only *.zip and *.jar are only supported for archives. \n\nviii) Also, caching across tasktracker going up and down is not supported. So a tasktracker would lose all caching information once it goes down. The caching information can be reconstructed when the task tracker comes up but the support is not available in this patch.\n\nix) When are the caches deleted?\n    A soft limit on the cache directory is a configuration parameter in the hadoop-default set to 10GB. So whenever the cache directory size goes beyond this size the framework will try deleting local caches that are not being used.\n\n\n\n", "The LocalDU class should rather be a method on FileUtils, as should the UnZip utility.\n\nDistributedCache should not be public, nor should the methods you add to TaskTracker.  The only public API for this new feature is the two new JobConf methods, right?\n\n", "You are right Doug. The only public api's should be the jobconf methods. I will incorporate your changes and resubmit the patch. Thanks for your comments.", "Incorporated doug's suggestions in this new patch.", "Jira ate my comments again. Since I didn't realize it for a day, I'll try to reconstruct them.\n\nThey were mostly nits:\n\n1. You add 3 new mini-cluster bring up/tear down cycles in the junit tests. It would be faster to use the same cluster with multiple jobs.\n2. The fields in DistributedCache should be private instead of package local.\n3. The same for the string constants in TaskTracker.\n4. TaskTracker.getCacheSubdir and getJobCacheSubdir should return Path's and be package local. \n5. getJobCacheSubdir should have a String jobId as a parameter and the result should be customized for that job. Furthermore, the result should be pushed through getLocalPath so that they are spread between the local dirs. Therefore, it should be given the server's conf also.\n6. runningJobs should be declared as \"Map runningJobs;\" instead of TreeMap. You might also consider using \"Map<String, RunningJob> runningJobs;\"\n7. appendString should probably be promoted into StringUtils as join(String[], String).\n8. the catch after the calls to launchTaskForJob should probably be rolled into the body of the method rather  than repeated.\n9. MapTask.java just has space changes.\n10. justPaths is public and probably should be package local (and get some java doc). \n11 getCacheArchives is kind of confusing as a name since it gets both archives and files. It should also probably not be public.", "incorporated most of owen's comments, except that of merging all my junit tests into other junit tests. I merged my local/minimr junit test into TestMiniMRLocalFS.  I tried including my caching junit test with DFS and MiniMR in TestMiniMRWIthDFS but that made it longer htan 3 mins. It currently takes arnd 140 seconds. So I have a seperate junit test for DFS and MiniMR.", "The indentation of this patch is non-standard.  Please use 2 spaces per indent level, no tabs.\n\nShould the JobConf setters be adders?  For example, should setCacheFiles(String) instead be named addCacheFile(Path)?  Also, should we use paths instead of strings?", "indented the patch to make it 2 spaces.", "about setcacheFiles(), we could add a addcacheFIles though I have not done it in the patch. Also, we are using URI's (dfs://hostname:port/path) so I dont think we should be using paths instead of strings.", "If we are using URIs then shouldn't the parameters be java.net.URI?", "sorry to have caused confusion. The files are specified as dfs://hostname:port/pathtofile. These are later converted to URI's by the framework to get the dfs and absolute paths. This is all similar to distcp where the input and outfiles can be specified as dfs://......", "The documentation of the primary user API should fully describe the format of the parameters, e.g., that these are strings representing URIs.  This would be simpler if the API used a type other than String.\n\nWe need an addCacheFile() method before we need a setCacheFiles() method, not the other way around.\n\nI'd like to see this as independent of the mapred core as is possible, in order to support things like HADOOP-452.  In particular, the DistributedCache class should probably move to a separate package (filecache)?, and as much of the functionality as is possible should be put in that package.  The TaskTracker and JobTracker should become clients of this facility.\n\nUtilities to store a set of files to cache in a Configuration should arguably move to the new package as well.  There's a temptation to overload JobConf that we need to resist.  So I'd rather see these as static methods like DistributedCache.addFile(Configuration, URI).\n", "\nok, this is what I plan to do. Doug, please comment if you are ok with this -- \n\n1) DistributedCache class will be seperate package - org.apache.hadoop.filecache;\n\n2) To make it idependent of mapreduce -- \nDistributedCache will have methods -- \nPath localizeCacheArchives(URI of archive to localize, conf)\n-- this will return the Path of the localized archive directory\nPath localizeCacheFiles(URI of file to localize, conf)\n-- this will return the Path of the localized file\n\nThe difference between these two methods being that localizeCacheArchives automatically unarchives a zip/jar file while localizeCacheFiles just copies the file locally.\n\n3) DistributedCache maintains a list of localized files/caches so as to copy only once.\n\n4) The TaskRunner is client of DistributedCache asking to cache files locally.\n\n5) JobConf does not have any api's related to setCache/addCache. These will be seperate static methods in DistributedCache to setCaches/files in jobconf. The getCaches can also be impelmented as a static method in DistributedCache.\n\nComments?", "+1  That sounds great!", "Made the suggested changes by doug. I moved all the public methods in jobconf for caching into DistributedCache. Mapreduce is now a client of DistributedCache class. Also, the public api now uses URI to addcachefiles/archives. DistributedCache is seperate package.\n\nSo the api now looks like \nDistrbutedCache.addCacheArchives(uri, jobconf)\n\nand to get the localized Paths \nDistibutedCache.getLocalArchives(Configuration conf) gives an array of  localized path directory of cache archives in the order they were added.  \n", "I just committed this.  Thanks, Mahadev!"], "derived": {"summary": "RFC: Efficient file caching \n(on Hadoop Task nodes, for benefit of MapReduce Tasks)\n------------------------------------------------------\n\nWe will start implementing this soon. Please provide feedback and improvements to this plan.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "RFC: Efficient file caching - RFC: Efficient file caching \n(on Hadoop Task nodes, for benefit of MapReduce Tasks)\n------------------------------------------------------\n\nWe will start implementing this soon. Please provide feedback and improvements to this plan."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Mahadev!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-289", "title": "Datanodes need to catch SocketTimeoutException and UnregisteredDatanodeException", "status": "Closed", "priority": "Major", "reporter": "Konstantin Shvachko", "assignee": "Konstantin Shvachko", "labels": [], "created": "2006-06-08T09:34:01.000+0000", "updated": "2009-07-08T16:41:54.000+0000", "description": "- Datanode needs to catch SocketTimeoutException when registering otherwise it goes down\nthe same way as when the namenode is not available (HADOOP-282).\n- UnregisteredDatanodeException need to be caught for all non-registering requests. The data\nnode should be shutdown in this case. Otherwise it will loop infinitely and consume namenode resources.\n", "comments": ["This patch fixes the two problems described.\nI placed all registration logic inside the DataNode.register(), seems more logical to me.\nThere is also a simple null value checkup included for FSNamesystem,\ndidn't want to create a separate  issue for that.\n", "This patch causes unit tests to fail for me.  For example, TestLocalDFS fails with:\n\n2006-06-08 12:56:54,423 INFO  ipc.Client (Client.java:run(142)) - Client connection to 127.0.0.1:65312: starting\n2006-06-08 12:56:54,432 INFO  ipc.Server (Server.java:run(233)) - Server handler 0 on 65312 call error: org.apache.hadoop.dfs.IncorrectVersionException: Unexpected version of data node reported: 0. Expecting = -2.\norg.apache.hadoop.dfs.IncorrectVersionException: Unexpected version of data node reported: 0. Expecting = -2.\n\tat org.apache.hadoop.dfs.NameNode.verifyVersion(NameNode.java:474)\n\tat org.apache.hadoop.dfs.NameNode.register(NameNode.java:362)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:585)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:243)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:231)\n2006-06-08 12:56:55,370 INFO  conf.Configuration (Configuration.java:loadResource(397)) - parsing file:/home/cutting/src/hadoop/test/conf/hadoop-default.xml\n2006-06-08 12:56:55,390 INFO  conf.Configuration (Configuration.java:loadResource(397)) - parsing file:/home/cutting/src/hadoop/test/src/test/hadoop-site.xml\n2006-06-08 12:56:55,395 WARN  fs.FSNamesystem (FSNamesystem.java:chooseTargets(1646)) - Replication requested of 1 is larger than cluster size (0). Using cluster size.\n2006-06-08 12:56:55,395 WARN  dfs.StateChange (FSNamesystem.java:startFile(388)) - DIR* NameSystem.startFile: failed to create file /user/cutting/somewhat/.random.txt.crc on client hadoop because target-length is 0, below MIN_REPLICATION (1)\n2006-06-08 12:56:55,396 INFO  ipc.Server (Server.java:run(233)) - Server handler 1 on 65312 call error: java.io.IOException: failed to create file /user/cutting/somewhat/.random.txt.crc on client hadoop because target-length is 0, below MIN_REPLICATION (1)\njava.io.IOException: failed to create file /user/cutting/somewhat/.random.txt.crc on client hadoop because target-length is 0, below MIN_REPLICATION (1)\n\tat org.apache.hadoop.dfs.FSNamesystem.startFile(FSNamesystem.java:354)\n\tat org.apache.hadoop.dfs.NameNode.create(NameNode.java:165)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:585)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:243)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:231)\n", "Resubmitted the patch under DatanodeExceptions-2.patch. \nNot failing this time. Sorry.", "Please replace the getLocalizedMessage and implicit toString with calls to StringUtils.stringifyException, which includes both the message and the call stack. The call stack helps a lot in finding and debugging the problem.", "I just committed this.  Thanks, Konstantin."], "derived": {"summary": "- Datanode needs to catch SocketTimeoutException when registering otherwise it goes down\nthe same way as when the namenode is not available (HADOOP-282). - UnregisteredDatanodeException need to be caught for all non-registering requests.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Datanodes need to catch SocketTimeoutException and UnregisteredDatanodeException - - Datanode needs to catch SocketTimeoutException when registering otherwise it goes down\nthe same way as when the namenode is not available (HADOOP-282). - UnregisteredDatanodeException need to be caught for all non-registering requests."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Konstantin."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-290", "title": "Fix Datanode transfer thread logging", "status": "Closed", "priority": "Minor", "reporter": "Dennis Kubes", "assignee": "Dhruba Borthakur", "labels": [], "created": "2006-06-08T11:19:27.000+0000", "updated": "2009-07-08T16:41:55.000+0000", "description": "Under the current datanode the logging of \"Starting thread to transfer block\" doesn't print out the hosts that it is transferring to, it prints out a java array toString.  This patch fixes this and prints out the hostnames and ports for all hosts we are tranferring to.", "comments": ["Patch fixes logging in datanode to print out transfer hosts.", "If you are doing anything other than a simple expression for logging, you should wrap the whole block in a check to see if it will be logged.\n\nif (LOG.isInfoEnabled()) {\n   ...\n   LOG.info(...);\n}\n\nThat way if someone has logging turned off, they don't do the work of constructing the message. ", "Updated patch with LOG.isInfoEnabled().  Thanks for the tip Owen.", "The logging_patch2 is the updated patch with LOG.isInfoEnabled(). ", "merged patch with latest trunk. Tested.", "Merged patch with latest trunk.", "Incorporated Raghu's comments. Changed \"and\" to \",\".", "A space between the datanode port number and the log message.", "+1. finnally this is getting fixed :). There are a few more quirks with Datanode logging. I will file a different jira later.", "+1 overall.  Here are the results of testing the latest attachment \nhttp://issues.apache.org/jira/secure/attachment/12373419/transferLoggingMessage.patch\nagainst trunk revision r613030.\n\n    @author +1.  The patch does not contain any @author tags.\n\n    javadoc +1.  The javadoc tool did not generate any warning messages.\n\n    javac +1.  The applied patch does not generate any new compiler warnings.\n\n    findbugs +1.  The patch does not introduce any new Findbugs warnings.\n\n    core tests +1.  The patch passed core unit tests.\n\n    contrib tests +1.  The patch passed contrib unit tests.\n\nTest results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1632/testReport/\nFindbugs warnings: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1632/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1632/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1632/console\n\nThis message is automatically generated.", "I just committed this."], "derived": {"summary": "Under the current datanode the logging of \"Starting thread to transfer block\" doesn't print out the hosts that it is transferring to, it prints out a java array toString. This patch fixes this and prints out the hostnames and ports for all hosts we are tranferring to.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Fix Datanode transfer thread logging - Under the current datanode the logging of \"Starting thread to transfer block\" doesn't print out the hosts that it is transferring to, it prints out a java array toString. This patch fixes this and prints out the hostnames and ports for all hosts we are tranferring to."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-291", "title": "Hadoop Log Archiver/Analyzer utility", "status": "Closed", "priority": "Major", "reporter": "Arun Murthy", "assignee": null, "labels": [], "created": "2006-06-08T17:40:06.000+0000", "updated": "2007-05-01T20:46:15.000+0000", "description": "Overview of the log archiver/analyzer utility...\n\n1. Input\n  The tool takes as input a list of directory URLs, each url could also we associated with a file-pattern to specify what pattern of files in that directory are to be used.\n  e.g. http://g1015:50030/logs/hadoop-sameer-jobtracker-*\n         file:///export/crawlspace/sanjay/hadoop/trunk/run/logs/haddop-sanjay-namenode-* (local disk on the machine on which the job was submitted)\n\n2. The tool supports 2 main functions:\n\n  a) Archival\n    Archive the logs in the DFS in the following hierarchy:\n   /users/<username>/log-archive/YYYY/mm/dd/HHMMSS.log by default \n   Or a user-specified directory and then: \n   <input-dir>/YYYY/mm/dd/HHMMSS.log\n\n  b) Processing with simple sort/grep primitives\n    Archive the logs as above and then grep for lines with given pattern (e.g. INFO) and then sort with spec e.g. <logger><level><date>. (Note: This is proposed with current log4j based logging in mind... do we need anything more generic?). The sort/grep specs are user-provided; along with directory URLs.\n\n3. Thoughts on implementation...\n\n  a) Archival\n    Current idea is to put a .jsp page (src/webapps) on each of the nodes; which then does a *copyFromLocal* of the log-file into the DFS. The jobtracker will fire n map-tasks which only hit the jsp page as per the directory URLs. The reduce-task is a no-op and only collects statistics on failures (if any).\n\n  b) Processing with sort/grep\n    Here, the tool first archives the files as above and then another set of map-reduce tasks will do the sort/grep on the files in DFS with given specs.\n\n\n                                                                                          - * - * - \n\n Suggestions/corrections welcome...\n\nthanks,\nArun", "comments": ["Duplicate of HADOOP-342"], "derived": {"summary": "Overview of the log archiver/analyzer utility. 1.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Hadoop Log Archiver/Analyzer utility - Overview of the log archiver/analyzer utility. 1."}, {"q": "What updates or decisions were made in the discussion?", "a": "Duplicate of HADOOP-342"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-292", "title": "hadoop dfs commands should not output superfluous data to stdout", "status": "Closed", "priority": "Minor", "reporter": "Yoram Arnon", "assignee": "Owen O'Malley", "labels": [], "created": "2006-06-09T07:46:01.000+0000", "updated": "2009-07-08T16:41:56.000+0000", "description": "running a command such as hadoop dfs -ls /data\nproduces output such as the following:\n06/06/08 17:42:32 INFO conf.Configuration: parsing jar:file: /hadoop/hadoop-0.4-dev/hadoop-0.4-dev.jar!/hadoop-default.xml\n06/06/08 17:42:32 INFO conf.Configuration: parsing file:hadoop/hadoop-site.xml\n06/06/08 17:42:32 INFO dfs.DistributedFileSystem: No FS indicated, using default:kry1200:8020\n06/06/08 17:42:32 INFO ipc.Client: Client connection to 172.30.111.134:8020: starting\nFound 2 items\n/data/a <dir>\n/data/b     <dir>\n\nthe first few lines shouldn't be there.\nit's especially annoying when running -cat into a file or into some post processing program, but in general, the output should be clean.", "comments": ["This patch renames the \"stdout\" log4j appender to \"console\" and sends the log events to stderr instead of stdout. This _partially_ fixes Yoram's problem.", "It is also arguable whether the default logger for applications should be \"WARN,console\" or \"INFO,console\". The default is currently info, which is why you're seeing those messages. The default can be changed via the environment variable HADOOP_ROOT_LOGGER or the log4j.properties file in the config directory.", "I just committed Owen's patch, for inclusion in 0.3.2.  If this fix is not sufficient, please file a new bug.  Thanks!"], "derived": {"summary": "running a command such as hadoop dfs -ls /data\nproduces output such as the following:\n06/06/08 17:42:32 INFO conf. Configuration: parsing jar:file: /hadoop/hadoop-0.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "hadoop dfs commands should not output superfluous data to stdout - running a command such as hadoop dfs -ls /data\nproduces output such as the following:\n06/06/08 17:42:32 INFO conf. Configuration: parsing jar:file: /hadoop/hadoop-0."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed Owen's patch, for inclusion in 0.3.2.  If this fix is not sufficient, please file a new bug.  Thanks!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-293", "title": "map reduce job fail without reporting a reason", "status": "Closed", "priority": "Major", "reporter": "Yoram Arnon", "assignee": "Owen O'Malley", "labels": [], "created": "2006-06-09T07:48:23.000+0000", "updated": "2009-07-08T16:51:49.000+0000", "description": "Often I see in the WI reports of tasks failing without information reported as to the reason of the failure.\nIt makes analysis and fixing the problem much harder.\nThe reason for the failure should always be reported in the WI.", "comments": ["I've had my share of troubles regarding this too. When a task encounters an error, all I see is:\n\nException in thread \"main\" java.io.IOException: Job failed!\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:357)\n       ...\n        <snip useless info>\n\nI attach a preview patch of my suggestion. It is against 0.4, but I'll forward port it to head and integrate it more with the rest of the system, if the approach is generally accepted by the devs. Please consider the patch as a idea-preview, not as a serious stab at the problem.\n\nThe approach is to add a public JobStatus.lastError string, which can be set from any throwable like JobStatus.setLastError(Throwable t). Setting this at relevant places (fx. on errors in mapred.LocalJobRunner.run() as in the patch) is useful for debugging purposes (for me atleast).", "The problem was that the web ui was not looking at the complete list of diagnostics, just the diagnostic that was sent with the last status report. This patch makes it generate the complete list.", "I just committed this.  Thanks, Owen!"], "derived": {"summary": "Often I see in the WI reports of tasks failing without information reported as to the reason of the failure. It makes analysis and fixing the problem much harder.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "map reduce job fail without reporting a reason - Often I see in the WI reports of tasks failing without information reported as to the reason of the failure. It makes analysis and fixing the problem much harder."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-294", "title": "dfs client error retries aren't happening (already being created and not replicated yet)", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-06-10T01:11:48.000+0000", "updated": "2009-07-08T16:41:55.000+0000", "description": "The conditions for catching the dfs error conditions that need to be retried were broken in a previous patch.", "comments": ["There were two missing not operators in the relevant string checks.", "I just committed this.  Thanks, Owen."], "derived": {"summary": "The conditions for catching the dfs error conditions that need to be retried were broken in a previous patch.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "dfs client error retries aren't happening (already being created and not replicated yet) - The conditions for catching the dfs error conditions that need to be retried were broken in a previous patch."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-295", "title": "jobs don't get executed in parallel", "status": "Closed", "priority": "Major", "reporter": "Yoram Arnon", "assignee": "Owen O'Malley", "labels": [], "created": "2006-06-13T07:26:32.000+0000", "updated": "2009-07-08T16:51:47.000+0000", "description": "we expect tasks to be assigned to nodes up to their configured capacity, with separate slots for map and reduce tasks.\nwe're observing jobs queueing up, with the map slots free, as exemplified by this copy-paste (converted to text) of the JT WI page.\n\nCluster Summary\nMaps Reduces Tasks/Node Nodes \n0           205             2                 156 \n--------------------------------------------------------------------------------\nRunning Jobs  \nJobid        User           Name          Map % complete Map total Maps completed Reduce % complete Reduce total Reduces completed \njob_0088 runping  DocUpdater   100.00%                  3597                3597                  99.92%                       256                        253 \njob_0089 murthij   Term Count        0.00%                  243                        0                     0.00%                        200                            0 \njob_0091 zshao     sort                       0.00%                  444                        0                     0.00%                        100                           0 \n\n\n--------------------------------------------------------------------------------\n", "comments": ["I believe this is the same issue. Basically, the padding for the job that is in the reduce phase is keeping the new job from starting maps.", "was a duplicate of Hadoop 299"], "derived": {"summary": "we expect tasks to be assigned to nodes up to their configured capacity, with separate slots for map and reduce tasks. we're observing jobs queueing up, with the map slots free, as exemplified by this copy-paste (converted to text) of the JT WI page.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "jobs don't get executed in parallel - we expect tasks to be assigned to nodes up to their configured capacity, with separate slots for map and reduce tasks. we're observing jobs queueing up, with the map slots free, as exemplified by this copy-paste (converted to text) of the JT WI page."}, {"q": "What updates or decisions were made in the discussion?", "a": "was a duplicate of Hadoop 299"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-296", "title": "Do not assign blocks to a datanode with < x mb free", "status": "Closed", "priority": "Major", "reporter": "Johan Oskarsson", "assignee": "Johan Oskarsson", "labels": [], "created": "2006-06-13T17:58:18.000+0000", "updated": "2009-07-08T16:41:55.000+0000", "description": "We're running a smallish cluster with very different machines, some with only 60 gb harddrives\nThis creates a problem when inserting files into the dfs, these machines run out of space quickly and then they cannot run any map reduce operations\n\nA solution would be to not assign any new blocks once the space is below a certain user configurable threshold\nThis free space could then be used by the map reduce operations instead (if that's on the same disk)", "comments": ["This is a completely untested patch, in fact it's more of an indication of what I mean with this feature request.\n(I don't have the time nor the setup to test this right now)\nJust thought I'd get this submitted before I forget", "If you look further down in FSNamesystem.chooseTarget() there is code that selects nodes that have space \nfor at least MIN_BLOCKS_FOR_WRITE (5 by default) blocks.\nThen, when data nodes calculate remaining disk size (see FSDataset.getRemaining()) they use USABLE_DISK_PCT (98%) \nand the value of the member FSDataset.reserved, which is initially set to 0, and then reflects the amount of space allocated \nfor the ongoing block creates. \n\nI think we should let individual data nodes be in control of the amount of space they need/want to preserve. \nRather than enforcing it on the name node uniformly for all data nodes. \nThis would solve your problem configuring very different machines on the cluster\nwith respect to their disk capacities.\n\nSo I propose to add 2 new configuration parameters for data nodes.\n1) dfs.datanode.du.pct   which is just a configurable variant of USABLE_DISK_PCT.\n2) dfs.datanode.du.reserved   which specifies the amount of space that should always remain on the node.\nThen at startup FSDataset.reserved can be set to dfs.datanode.du.reserved rather than 0, \nand USABLE_DISK_PCT should be replaced by dfs.datanode.du.pct\n", "Thanks for the feedback.\nQuick patch that reads dfs.datanode.du.reserved and dfs.datanode.du.pct from the config.\n\nHope this is what you ment.\n\n/Johan", "managing individual configuration files on each node is exceedingly difficult.\nwhen I make a config change, say, when a new config variable is added, it's usually for the entire cluster(s); I make the change on one node, then distribute to all the other nodes. That would override individual changes made to those files. editing each individual file separately is clearly unmanageable, so for larger cluster it's really important to have a *very* small number of configurations.\n\nAs a rule, less configuration is better. In this particular case, improving the allocation strategy to do a better job automatically, possibly using a single configured strategy variable but better without, would be far better than individual configuration per node.\nAt the end of the day, you're going to select values for these two new config items based on some logic, using real world numbers as a guide. Why not just implement that logic in the code?", "Yoram, as far as I know you use a large cluster of mostly identical machines.\nFor such clusters you need one configuration uniformly distributed to other nodes.\nThe case that Johan describes is different. You can have a uniform config, but should be\nable to correct it for one or two nodes that are different from everything else.\n\nJohan, I think this is good.\nPlease replace 0.98f by USABLE_DISK_PCT_DEFAULT\n+1 after that.\n", "Oh, and you also need to add those properties into hadoop-default.xml", "Added the changes suggested by Konstantin.\n\n/Johan", "Arghl, sorry..\nForgot to save before I did the diff", "Works for me.\nAlthough you don't need \"fs.space.min\" any more.", "Right, removed fs.space.min...\nNote to self, don't post anything when you're in a hurry...", "I think it is reasonable to commit this patch now.\n+1", "Terribly sorry guys, this is a corrected version, disregard the previous ones.\nHow hard can it be to do a five line patch? :)\n\nI'm a bit stressed out by other things, probably shouldn't send patches right now :)", "I just committed this.  Thanks, Johan!"], "derived": {"summary": "We're running a smallish cluster with very different machines, some with only 60 gb harddrives\nThis creates a problem when inserting files into the dfs, these machines run out of space quickly and then they cannot run any map reduce operations\n\nA solution would be to not assign any new blocks once the space is below a certain user configurable threshold\nThis free space could then be used by the map reduce operations instead (if that's on the same disk).", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Do not assign blocks to a datanode with < x mb free - We're running a smallish cluster with very different machines, some with only 60 gb harddrives\nThis creates a problem when inserting files into the dfs, these machines run out of space quickly and then they cannot run any map reduce operations\n\nA solution would be to not assign any new blocks once the space is below a certain user configurable threshold\nThis free space could then be used by the map reduce operations instead (if that's on the same disk)."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Johan!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-298", "title": "nicer reports of progress for distcp", "status": "Closed", "priority": "Minor", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-06-14T02:31:52.000+0000", "updated": "2006-08-03T17:46:47.000+0000", "description": "The unformatted number of bytes in distcp is difficult to read.", "comments": ["This patch adds a percentage complete and a human readable format of the number of bytes copied so far.", "I just committed this.  Thanks, Owen."], "derived": {"summary": "The unformatted number of bytes in distcp is difficult to read.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "nicer reports of progress for distcp - The unformatted number of bytes in distcp is difficult to read."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-299", "title": "maps from second jobs will not run until the first job finishes completely", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-06-14T04:23:34.000+0000", "updated": "2009-07-08T16:51:47.000+0000", "description": "Because of the logic in the JobTracker's pollForNewTask, second jobs will rarely start running maps until the first job finishes completely. The JobTracker leaves room to re-run failed maps from the first job and it reserves the total number of maps for the first job. Thus, if you have more maps in the first job than your cluster capacity, none of the second job maps will ever run.\n\nI propose setting the reserve to 1% of the first job's maps.", "comments": ["This patch does a couple of things:\n  1. It makes it more obvious that the task slot padding is only done on clusters bigger than 3 nodes.\n  2. Remove the dead avgMaps and avgReduces variables.\n  3. Change the default value of PAD_FRACTION from 0.1 to 0.01. (This variable is not in hadoop-default.xml, so the code controls the default value.)\n  4. Fix a typo that was counting all of the running jobs' tasks instead of the the running jobs' running tasks. (totalMaps versus totalNeededMaps and totalReduces versus totalNeededReduces)", "It's the same issue that I reported earlier on the mailing list (e.g. http://www.mail-archive.com/hadoop-dev@lucene.apache.org/msg01510.html, http://www.mail-archive.com/hadoop-dev@lucene.apache.org/msg01524.html and http://www.mail-archive.com/hadoop-dev@lucene.apache.org/msg01557.html).\n\nYour patch, although it improves things, could perhaps go one step further if you have some time to spare... ;) I'm thinking specifically about the following:\n\n* don't schedule reduce tasks from jobs, where map tasks had no chance of running yet. This happens when there are a couple available slots, and map tasks cannot be scheduled yet, but the code in JobTracker:743 still allocates reduce tasks from the next job.\n\n* allow simple priority-based preemption, i.e. jobs with a higher priority (presumably short-lived) could be favored in task allocation over already running jobs.\n\n* alternatively, allow setting limits on the min/max % of cluster capacity the job is willing to accept. This gives some leeway to the scheduler to allocate the flexible portion of remaining tasks to old/new jobs.\n\n* allow setting different priorities for maps and reduces - e.g. in Nutch fetcher job, map tasks are very long running and in many cases need to fit within a specified time-frame (e.g. during the night). However, reduce tasks, which simply reshuffle the data, are not so time-critical.\n", "*smile* I don't have that much free time! I certainly agree with you that the right long term solution would involve:\n  1. Using task killing to make slots available if they were needed.\n  2. Not starting reduces until at least one of the maps feeding it has finished and generated output.\n  3. For bonus points you could prefer to run a reduce that has local input. (In general that won't help, but there is a sub-class of problem where most of the input for each reduce is coming from a small number of maps.)", "I just committed this.  Thanks, Owen!"], "derived": {"summary": "Because of the logic in the JobTracker's pollForNewTask, second jobs will rarely start running maps until the first job finishes completely. The JobTracker leaves room to re-run failed maps from the first job and it reserves the total number of maps for the first job.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "maps from second jobs will not run until the first job finishes completely - Because of the logic in the JobTracker's pollForNewTask, second jobs will rarely start running maps until the first job finishes completely. The JobTracker leaves room to re-run failed maps from the first job and it reserves the total number of maps for the first job."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-301", "title": "the randomwriter example will clobber the output file", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-06-14T06:03:13.000+0000", "updated": "2009-07-08T16:51:47.000+0000", "description": "The randomwriter will automatically clobber the output directory, which is dangerous given the lack of permissions in  dfs.", "comments": ["This checks for the existance of the output directory and aborts with a user message.", "I just committed this.  Thanks, Owen."], "derived": {"summary": "The randomwriter will automatically clobber the output directory, which is dangerous given the lack of permissions in  dfs.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "the randomwriter example will clobber the output file - The randomwriter will automatically clobber the output directory, which is dangerous given the lack of permissions in  dfs."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-302", "title": "class Text (replacement for class UTF8) was: HADOOP-136", "status": "Closed", "priority": "Major", "reporter": "Michel Tourn", "assignee": "Hairong Kuang", "labels": [], "created": "2006-06-14T07:32:27.000+0000", "updated": "2006-08-04T22:22:32.000+0000", "description": "Just to verify, which length-encoding scheme are we using for class Text (aka LargeUTF8) \n\na) The \"UTF-8/Lucene\" scheme? (highest bit of each byte is an extension bit, which I think is what Doug is describing in his last comment) or \nb) the record-IO scheme in o.a.h.record.Utils.java:readInt \n\nEither way, note that: \n\n1. UTF8.java and its successor Text.java need to read the length in two ways: \n  1a. consume 1+ bytes from a DataInput and \n  1b. parse the length within a byte array at a given offset \n(1.b is used for the \"WritableComparator optimized for UTF8 keys\" ). \n\no.a.h.record.Utils only supports the DataInput mode. \nIt is not clear to me what is the best way to extend this Utils code when you need to support both reading modes \n\n2 Methods like UTF8's WritableComparator are to be low overhead, in partic. there should be no Object allocation. \nFor the byte array case, the varlen-reader utility needs to be extended to return both: \n the decoded length and the length of the encoded length. \n (so that the caller can do offset += encodedlength) \n    \n3. A String length does not need (small) negative integers. \n\n4. One advantage of a) is that it is standard (or at least well-known and natural) and there are no magic constants (like -120, -121 -124) ", "comments": ["There are two issues with the current implementation of UTF8.\n\nThe first is that it does not handle over long string. The length of a string is limited to a short, not a int. I'd like to address this problem by storing the length of a string in a variable-length formt. The highest bit of each byte is an extension bit. '1' means that more bytes are followed, while '0' means last byte.\n\nThe second is that the class chooses Java modified UTF8 as the serialized form.  Some argue that we should use the standard UTF8. It seems to me that serializing a string to Java modified UTF8 is quite efficient. But it is Java's internal representation. If we want to support inter-programming-language communication, it makes more sense to use the standard UTF8.\n\nAlso for the name of the class, could I use \"StringWritable\"? It is consistent with other classes that implement WritableComparable, like IntWritable, FloatWritable etc. ", "I think we should use this opportunity to switch to standard UTF-8 for persistent data.  Optimized code should try to avoid conversion of these to Java strings.  For example, comparision can be done on the binary form (since this yeilds the same results as lexicographic unicode comparisons).", "Please see the related Lucene issue.  Note that Marvin has attached a patch that includes optimized code for conversion from standard UTF8 to Java strings.", "+1 on doug's suggestion.  Let's use real UTF8.  Then we can interoperate with more things.\n\nAgreed that we need to use one of the existing variable length encodings.   Inventing another would be counter productive.  My preference would be to use the recordio scheme, since it is already in hadoop.  If we choose to import the lucene version, we should consider using it for recordio too, easy to change now, since it is still new.\n", "If we use standard UTF8, comparison on the binary form does not produce the same results as the string comparison. See the following example provided by Addison:\n\n> Consider the sequence U+D800 U+DC00 (a surrogate pair). In String comparison, this compares as less than U+E000 (since \n> D800 < E000). In UTF-8 byte comparisons it is greater than E000 (because the lead byte of the Unicode character U+10000 \n> encoded by the surrogate pair is 0xF0, which is bigger than lead byte of U+E000, which is 0xEE). \n\nIs it an issue? \n", "If we use the recordio scheme, we need to extend it so that it can read a variable-length integer from a byte array. This is for the support of byte-wise comparison.", "There is support for negative numbers as well in recordio scheme, which is not needed here, thus allowing us to save a few more bits.", "Re String comparison: The bug here is with Java.  Since we wish to keep our persistent data structures language-independent, we should order by UTF-8, not UTF-16.\n\nThe javadoc is confusing:\n\nhttp://java.sun.com/j2se/1.5.0/docs/api/java/lang/String.html#compareTo(java.lang.String)\n\nIt says it compares unicode characters, when in fact it compares UTF-16.\n\nSo any code that orders by Java String and expects things to align with the Hadoop Text class will be buggy when processing text with surrogate pairs.  We should make this clear in the javadoc.\n\nDoes this sound reasonable?", "Sounds great! I believe that ordering by UTF8 is the right way to go.", "This patch extracts the zero-compress integer code in the hadoop record into hadoop io. It also adds functions for comparing serialized integers bytewise.", "This patch includes Text class which stores a string in the standard utf8 format, compares two strings bytewise in the utf8 order, and many other functions.\n\nThis patch also includes a Junit test for the Text class.\n\nMany thanks to Addison Philip for his tim on the design discussion and code review. He also contributed quite a lot of code.", "I just committed this.  Thanks!", "Started using this in my code... looks like there are still some bugs. Here's a testcase that shouldn't fail. (I have no idea how UTF works, or I'd try to offer an actual solution):\n\nIndex: hadoop/src/test/org/apache/hadoop/io/TestText.java\n===================================================================\n--- hadoop/src/test/org/apache/hadoop/io/TestText.java\t(revision 425795)\n+++ hadoop/src/test/org/apache/hadoop/io/TestText.java\t(working copy)\n@@ -87,7 +87,11 @@\n \n \n   public void testCoding() throws Exception {\n-    for (int i = 0; i < NUM_ITERATIONS; i++) {\n+\t  String badString = \"Bad \\t encoding \\t testcase.\";\n+\t  Text testCase = new Text(badString);\n+\t  assertTrue(badString.equals(testCase.toString()));\n+\n+\t  for (int i = 0; i < NUM_ITERATIONS; i++) {\n       try {\n           // generate a random string\n           String before;\n", "Whoops, forgot that inline patches are bad form. The point is, the encoding of \"Bad \\t encoding \\t testcase\" fails the verifyUTF check. It seems to have something to do with the double tabs.", "Here is the patch that should fix Bryan's problem.", "Don't know what the culprits are, but here are two stack traces I got today that killed a 2-hour job. Maybe, for now, validateUTF should be called when first serializing, too. There seem to still be a few bugs in how Text handles content.\n\njava.lang.RuntimeException: java.nio.charset.MalformedInputException: Input length = 3\n\tat org.apache.hadoop.mapred.ReduceTask$ValuesIterator.next(ReduceTask.java:152)\n\tat org.apache.hadoop.mapred.lib.IdentityReducer.reduce(IdentityReducer.java:39)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:272)\n\tat org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1076)\nCaused by: java.nio.charset.MalformedInputException: Input length = 3\n\tat org.apache.hadoop.io.Text.validateUTF(Text.java:439)\n\tat org.apache.hadoop.io.Text.validateUTF8(Text.java:419)\n\tat org.apache.hadoop.io.Text.readFields(Text.java:228)\n\tat org.apache.hadoop.io.ArrayWritable.readFields(ArrayWritable.java:82)\n\tat org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:370)\n\tat org.apache.hadoop.mapred.ReduceTask$ValuesIterator.getNext(ReduceTask.java:183)\n\tat org.apache.hadoop.mapred.ReduceTask$ValuesIterator.next(ReduceTask.java:149)\n\t... 3 more\n\njava.lang.RuntimeException: java.nio.charset.MalformedInputException: Input length = 26\n\tat org.apache.hadoop.mapred.ReduceTask$ValuesIterator.next(ReduceTask.java:152)\n\tat org.apache.hadoop.mapred.lib.IdentityReducer.reduce(IdentityReducer.java:39)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:272)\n\tat org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1076)\nCaused by: java.nio.charset.MalformedInputException: Input length = 26\n\tat org.apache.hadoop.io.Text.validateUTF(Text.java:439)\n\tat org.apache.hadoop.io.Text.validateUTF8(Text.java:419)\n\tat org.apache.hadoop.io.Text.readFields(Text.java:228)\n\tat org.apache.hadoop.io.ArrayWritable.readFields(ArrayWritable.java:82)\n\tat org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:370)\n\tat org.apache.hadoop.mapred.ReduceTask$ValuesIterator.getNext(ReduceTask.java:183)\n\tat org.apache.hadoop.mapred.ReduceTask$ValuesIterator.next(ReduceTask.java:149)\n\t... 3 more\n\nUnlike last time, this content doesn't contain tabs. Contact me off-list for pointers to the dataset this occured in."], "derived": {"summary": "Just to verify, which length-encoding scheme are we using for class Text (aka LargeUTF8) \n\na) The \"UTF-8/Lucene\" scheme? (highest bit of each byte is an extension bit, which I think is what Doug is describing in his last comment) or \nb) the record-IO scheme in o. a.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "class Text (replacement for class UTF8) was: HADOOP-136 - Just to verify, which length-encoding scheme are we using for class Text (aka LargeUTF8) \n\na) The \"UTF-8/Lucene\" scheme? (highest bit of each byte is an extension bit, which I think is what Doug is describing in his last comment) or \nb) the record-IO scheme in o. a."}, {"q": "What updates or decisions were made in the discussion?", "a": "Don't know what the culprits are, but here are two stack traces I got today that killed a 2-hour job. Maybe, for now, validateUTF should be called when first serializing, too. There seem to still be a few bugs in how Text handles content.\n\njava.lang.RuntimeException: java.nio.charset.MalformedInputException: Input length = 3\n\tat org.apache.hadoop.mapred.ReduceTask$ValuesIterator.next(ReduceTask.java:152)\n\tat org.apache.hadoop.mapred.lib.IdentityReducer.reduce(IdentityReducer.java:39)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:272)\n\tat org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1076)\nCaused by: java.nio.charset.MalformedInputException: Input length = 3\n\tat org.apache.hadoop.io.Text.validateUTF(Text.java:439)\n\tat org.apache.hadoop.io.Text.validateUTF8(Text.java:419)\n\tat org.apache.hadoop.io.Text.readFields(Text.java:228)\n\tat org.apache.hadoop.io.ArrayWritable.readFields(ArrayWritable.java:82)\n\tat org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:370)\n\tat org.apache.hadoop.mapred.ReduceTask$ValuesIterator.getNext(ReduceTask.java:183)\n\tat org.apache.hadoop.mapred.ReduceTask$ValuesIterator.next(ReduceTask.java:149)\n\t... 3 more\n\njava.lang.RuntimeException: java.nio.charset.MalformedInputException: Input length = 26\n\tat org.apache.hadoop.mapred.ReduceTask$ValuesIterator.next(ReduceTask.java:152)\n\tat org.apache.hadoop.mapred.lib.IdentityReducer.reduce(IdentityReducer.java:39)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:272)\n\tat org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1076)\nCaused by: java.nio.charset.MalformedInputException: Input length = 26\n\tat org.apache.hadoop.io.Text.validateUTF(Text.java:439)\n\tat org.apache.hadoop.io.Text.validateUTF8(Text.java:419)\n\tat org.apache.hadoop.io.Text.readFields(Text.java:228)\n\tat org.apache.hadoop.io.ArrayWritable.readFields(ArrayWritable.java:82)\n\tat org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:370)\n\tat org.apache.hadoop.mapred.ReduceTask$ValuesIterator.getNext(ReduceTask.java:183)\n\tat org.apache.hadoop.mapred.ReduceTask$ValuesIterator.next(ReduceTask.java:149)\n\t... 3 more\n\nUnlike last time, this content doesn't contain tabs. Contact me off-list for pointers to the dataset this occured in."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-304", "title": "UnregisteredDatanodeException message correction", "status": "Closed", "priority": "Trivial", "reporter": "Konstantin Shvachko", "assignee": "Konstantin Shvachko", "labels": [], "created": "2006-06-15T05:35:05.000+0000", "updated": "2006-08-03T17:46:47.000+0000", "description": null, "comments": ["UnregisteredDatanodeException should report the expected node name rather than its storage id.\n", "I just committed this.  Thanks, Konstantin."], "derived": {"summary": "", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "UnregisteredDatanodeException message correction"}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Konstantin."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-305", "title": "tasktracker waits for 10 seconds for asking for a task.", "status": "Closed", "priority": "Minor", "reporter": "Mahadev Konar", "assignee": "Mahadev Konar", "labels": [], "created": "2006-06-16T05:46:13.000+0000", "updated": "2009-07-08T16:51:49.000+0000", "description": "the tasktracker should ask for a job as soon as a job running on it is finished. ", "comments": ["This patch lets the tasktracker ask for a new task as soon as a task is finished, else it just waits for 10 seconds to get a new task. This is part of Ben's patch submitted to HADOOP-249. This patch also combines the patch to HADOOP-283.\nThanks Ben for your help in integrating the patch.", "I just committed this.  Thanks, Mahadev."], "derived": {"summary": "the tasktracker should ask for a job as soon as a job running on it is finished.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "tasktracker waits for 10 seconds for asking for a task. - the tasktracker should ask for a job as soon as a job running on it is finished."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Mahadev."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-306", "title": "Safe mode and name node startup procedures", "status": "Closed", "priority": "Major", "reporter": "Konstantin Shvachko", "assignee": "Konstantin Shvachko", "labels": [], "created": "2006-06-17T07:57:35.000+0000", "updated": "2009-10-13T21:33:10.000+0000", "description": "This is a proposal to improve DFS cluster startup process.\nThe data node startup procedures were described and implemented in HADOOP-124.\nI'm trying to extend them to the name node here.\nThe main idea is to introduce safe mode, which can be entered manually for administration\npurposes, or automatically when a configurable threshold of active data nodes is breached,\nor at startup when the node stays in safe mode until the minimal limit of active\nnodes is reached.\n\nThis are high level requirements intended to improve the name node and cluster reliability.\n    = The name node safe mode means that the name node is not changing the state of the\n       file system. Meta data is read-only, and block replication / removal is not taking place.\n    = In safe mode the name node accepts data node registrations and\n       processes their block reports.\n    = The name node always starts in safe mode and stays safe until the majority\n        (a configurable parameter: safemode.threshold) of data nodes (or blocks?)\n        is reported.\n    = The name node can also fall into safe mode when the number of non-active\n        (heartbeats stopped coming in) data nodes becomes critical.\n    = The startup \"silent period\", when the name node is in safe mode and is\n        not issuing any block requests to the data nodes, is initially set to a\n        configurable value safemode.timeout.increment. By the end of the timeout\n        the name node checks the safemode.threshold and decides whether to switch\n        to the normal mode or to stay in safe. If the normal mode criteria is not\n        met, then the silent period is extended by incrementing the safemode timeout.\n    = The name node stays in safe mode not longer than a configurable value of\n        safemode.timeout.max, in which case it logs missing data nodes and shuts\n        itself down.\n    = When the name node switches to normal mode it checks whether all required\n        data nodes have actually registered, based on the list of active data storages\n        from the last session. Then it logs missing nodes, if any, and starts\n        replicating and/or deleting blocks as required.\n    = A historical list of data storages (nodes) ever registered with the cluster is\n        persistently stored in the image and log files. The list is used in two ways:\n        a) at startup to verify whether all nodes have registered, and to report\n        missing nodes;\n        b) at runtime if a data node registers with a new storage id the\n        name node verifies that no new blocks are reported from that storage,\n        which would prevent us from accidentally connecting data nodes from a\n        different cluster.\n    = The name node should have an option to run in safe mode. Starting with\n        that option would mean it never leaves safe mode.\n        This is useful for testing the cluster.\n    = Data nodes that can not connect to the name node for a long time (configurable)\n        should shut down themselves.", "comments": ["This patch implements a part of the laid out design.\nIt stores the historical list of datanodes in the image file and logs newly\nregistered nodes in the edits file.\nThe changes are mostly related to the FSNamesystem class.\nSince datanodes are not removed from the datanodeMap when they are considered\nnon responsive, the deadDatanodeMap field becomes redundant. I removed it.\nThere is a change in semantics of the relation between heartbeats and datanodeMap maps.\nheartbeats contains only live nodes while datanodeMap contains both alive and dead nodes.\nSee JavaDoc for more details.\nSo when we are looking for new targets for block replication we should check the heartbeats\nmap rather than the datanodeMap as we did before.\nAlso since the DatanodeDescriptors are not physically removed from datanodeMap\nI had to add their blocks cleanup while processing lost heartbeats.\nSome changes to the FSImage and FSEditLog classes. I removed unnecessary\nparameter to FSDirectory from the previous version. The whole name space can be\naccessed via static method FSNamesystem.getFSNamesystem()\n\n", "Konstantin, since this patch only partially fixes this issue, can you please instead add this patch to a separate issue that this issue depends on?  Thanks!", "I'd like to discuss the meaning of the safemode.threshold parameter, which defines\nwhen the name node can leave safe mode and start replication/deletion of blocks.\nI see at least 2 meanings:\n1) % of storage ids reported so far. Storage id is a unique id of a data node\nthat identifies the storage rather than the data node address.\n2) % of blocks reported so far by all data nodes.\nThey seem to be pretty independent. Even if we have 100% storage ids\nreported that does not mean we have 100% blocks.\nWhat 100% means for blocks? Does it mean the name node has\n2a) at least one copy of each block or\n2b) the complete list of replicas for each block.\nAll three constraints seem reasonable.\n100% in (1) is the best one could physically do, since all storages are\nreported, it is just that they have corrupted or missing blocks.\n100% in (2a) is the minimal requirement for the name node to start\nwithout any data loss, and\n100% in (2b) would be the perfect state of the system.\nSo the question is what do we really want?", "OTOH, I'd want the namenode to be able to move out of safe mode with minimal restrictions, to avoid unnecessary manual intervention.\nOTOH, I want to avoid unnecessary thrash on startup, while data nodes are connecting.\nSo any solution would require both a threshold, and some timer once that threshold is reached, to avoid unnecessary replications the second the threshold is reached, before the last datanodes connect.\n\nI'd go with a configuration requiring:\n* 100% blocks are available, at least one replica\n* allow one more minute for stragglers to connect and report their blocks\n\ntypically this would allow up to two nodes to be missing, but if by some magic more nodes are missing and I still have all my data intact, such as when we implement rack aware data placement and a rack goes down, hallelujah - I wait one minute and start re-replicating.", "I wonder if it makes sense to wait for 100% of blocks to be available. It's possible  for data to go missing because of failed drives, nodes, racks... When some data goes missing it's usually the case that some other (co-located) data becomes under-replicated and the Namenode ought to start replicating the under replicated data. Why do we want safe mode?\n\n* To prevent replication thrash when the Namenode starts\n* To enable administrators to make the file system read only for diagnosis and debugging\n\nNeither of these require that 100% of blocks are present. Maybe we should have a slightly lower threshold for blocks or storage ids.", "Thanks for the comments. Based on what you say I can define the threshold constraint as:\n  **   the percentage of blocks that meet minimal replication requirement (defined by dfs.safemode.replication)\nI think this would be flexible enough.\nAnd I'll keep the dfs.datanode.startupMsec that defines the startup period during which replications are not allowed.\nSo the name node will stay in safe mode at start up until the threshold is reached AND the startup period hasn't expired.", "the timeout should start *after* the threshold is reached.\nIf namenode is started long before datanodes, then as soon as the threshold is reached superfluous replications will occur.\nthe timeout should be short, I'd say less than a minute, and should start as soon as the data/nodes threshold is reached, to allow stragglers to connect and report", "This is the safe mode implementation, which implements everything that was discussed here\nexcept for the automatic entering safe mode when the number of non-active blocks falls below \nthe threshold. This seems to be arguable that rather than replicating as quickly as it can the \nname node falls into safe mode.\nPlease see JavaDoc for complete descriptions of the safe mode concept and its implementation.\n\nOther minor changes\n- Block report interval is configurable, but there was no value for it in hadoop-default.xml. I added it.\n  <name>dfs.blockreport.intervalMsec</name>\n- A typo in hadoop-default.xml\n- There was a memory leak related to that name node never removed blocks from the bloksMap\nso it could grow large if the system works long enough. I fixed this unreported bug.\n- I moved the startup of http servers for both data and name nodes to the end of their constructors\nas discussed in HADOOP-430\n- ClientProtocol version is changed since setSafeMode() method is added\n", "The enum mode should not be passed & returned as an int, but rather as a boolean or enum.\n\nAs a boolean this might look like:\n\nboolean setMode(boolean isSafe);\nboolean getMode();\n\nTo pass an enum over an RPC would require either changes to ObjectWritable or to make the enum class implement Writable.\n", "Making enum implement Writable does not work, since you cannot implement \nreadFields() so that it'd modify fields (which are all final) in enum and change its state. \n\nI made a simple modification to ObjectWritable that treats enumerator type as a s\npecial case like String or Array. This works fine for all simple (traditional) enum types. \n\nIf we will ever want to use internal fields and other \"fancy\" enum features, which are \nconsidered to be confusing for most programmers anyway :-), then we will need to \nintroduce parameters to the WritableFactory.newInstance() that could be used to \nconstruct a correct instance of the enum.\n\nThis patch in addition to what it had before introduces an enum type SafeModeAction \nand a new functionality that lets RPC pass simple enumerator types.\n", "I just committed this.  Thanks, Konstantin!"], "derived": {"summary": "This is a proposal to improve DFS cluster startup process. The data node startup procedures were described and implemented in HADOOP-124.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Safe mode and name node startup procedures - This is a proposal to improve DFS cluster startup process. The data node startup procedures were described and implemented in HADOOP-124."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Konstantin!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-307", "title": "Many small jobs benchmark for MapReduce", "status": "Closed", "priority": "Minor", "reporter": "Sanjay Dahiya", "assignee": "Sanjay Dahiya", "labels": [], "created": "2006-06-18T20:14:33.000+0000", "updated": "2009-07-08T16:51:48.000+0000", "description": "A benchmark that runs many small MapReduce tasks in sequence. A single map reduce implementation is used, it is invoked multiple times with input as the output from previous run. The input to first Map is a TextInputFormat ( a text file with few hundred KBs). Input records are passed to output without much processing. The idea is to benchmark the time taken by initialization of Mapper and Reducer. An initial prototyping on a single machine with 20 MR tasks in sequence took ~47 seconds per task. Looking for suggestions on what else can be included in the benchmark. \n\n", "comments": ["What configuration options does it have?\n\nCan you set an arbitrary number of map and reduce tasks?\nDo you validate that the right bytes get sent through the system?\nSet the number of steps?\n\n", "Configuration options are - \n    input ( local file ), \n    output - DFS Path, \n    times ( no of times to execute the job ) \n    jarFile ( Mapper & Reducer )\n    wordDir ( temp output from intermediate tasks) \n    maps ( num Maps)\n    reduces ( num Reduces) \n    \nI am not yet validating the bytes but I will add that. Also number of map and reduce tasks can be configured, its passed to JobConf . The benchmark sets up multiple MapReduce tasks in sequence and output of each job is passed as input to next execution of same job). Its using a TextInputFormat by default and thats not configurable yet.\n\nI was sick and out so delay in response. I am yet to run on a cluster, by tomorrow I should post the results. ", "Small jobs benchmark - \n\nGenerates input data and runs a number of small jobs repeatedly to estimate launch overhead for jobs. I am putting it in a separate directory from examples because we dont want this to be a part of hadoop distro on all nodes. Part of what we are benchmarking is transfering Mapper and Reducer jar files through HDFS. \n\nQuick usage instructions follows - \n\nBuilding the benchmark. \nbenchmarks/build.xml depends on relative path of hadoop libs, if moving this directory, point the right HADOOP_HOME/lib in build.xml. \nto build - \n$ cd benchmarks \n$ ant\n\nRunning the benchmark\n$ cd benchmarks\n$ bin/run.sh\n\nafter successfully running the benchmark see logs/report.txt for consolidated output of all the runs. \n\nchange this script to configure options. \n\nConfigurable options are - \n\n-inputLines noOfLines \n  no of lines of input to generate. \n\n-inputType (ascending, descending, random)\n  type of input to generate. \n\n-jar jarFilePath \n  Jar file containing Mapper and Reducer implementations in jar file. By default ant build creates MRBenchmark.jar file containing default Mapper and Reducer. \n  \n-times numJobs \nNo of times to run each MapReduce task, time is calculated as average of all runs. \n\n-workDir dfsPath \nDFS path to put output of MR tasks. \n\n-maps numMaps \nNo of maps for wach task \n\n-reduces numReduces \nNo of reduces for each task\n\n-ignoreOutput\nDoesn't copy the output back to local disk. Otherwise it creates the output back to a temp location on local disk. ", "We already have some benchmarks in the examples source tree.  Any reason not to put this there too?  That way it would be compiled by the \"test\" task, which means it will be more likely to be maintained, it would't need it's own build.xml, etc.  Also, we might name this something more informative, like, MRJobBenchmark or something.", "The only reason to keep it separate is we dont want these jar files already in classpath on all nodes. Part of the benchmark's goal is to estimate the overhead in transfering the jar file through HDFS. Also there is bin dir in this for scripts to run the benchmark. If this doesnt conflict with existing examples the we can put it there as well. \n\nUpdating the patch, it now generates excel friendly CSV output to plot graphs etc. ", "> The only reason to keep it separate is we dont want these jar files already in classpath on all nodes.\n\nThen let's put this in src/contrib/small-job-benchmark, ok?  Its build.xml should have \"deploy\", \"test\", and \"clean\" targets.  Michel is working on a patch so that all modules in src/contrib will be compiled and tested by the top-level \"test\" target.", "Updated to reside in src/contrib. with build.xml changes for contrib.", "I just committed this.  Thanks!", "I want to try this on Windows with cygwin and found some problems.\n\n1. $JAVA_HOME in run.sh needs to be \"$JAVA_HOME\" because on windows java is installed in Program Files and there is problem with spaces\n2. also there is some /export/crawlspace/kryptonite/java/jdk/lib/tools.jar in classpath which I believe is not needed.\n\nalso with this I could not get script working with error\n\njava.lang.NoClassDefFoundError: org/apache/hadoop/benchmarks/mapred/MultiJobRunner\n\nIf I have only ./classes in my classpath there is problem with loger. I don't know If there is any problem with classpath definition in windows.\n", "Patch for classpath issues. The benchmark can now be run using hadoop script without having to set any extra classpath - $HADOOP_HOME/bin/hadoop jar <path to MRBenchmark.jar> smallJobsBenchmark <options .. >. See Readme.txt for an example of options. \nbin/run.sh script can be used as an optional helper script if benchmark needs to be run multiple times with different input configurations. \n\nthanks Uros for pointing this out. ", "Can you please create a new issue for this patch?  Since the release of 0.5.0, this issue was closed.\n\nIn general, once a patch is committed and the issue is resolved then subsequent patches should be attached to new issues.", "ok, I will create a new issue for this. "], "derived": {"summary": "A benchmark that runs many small MapReduce tasks in sequence. A single map reduce implementation is used, it is invoked multiple times with input as the output from previous run.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Many small jobs benchmark for MapReduce - A benchmark that runs many small MapReduce tasks in sequence. A single map reduce implementation is used, it is invoked multiple times with input as the output from previous run."}, {"q": "What updates or decisions were made in the discussion?", "a": "ok, I will create a new issue for this."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-308", "title": "Task Tracker does not handle the case of read only local dir  case correctly", "status": "Closed", "priority": "Major", "reporter": "Runping Qi", "assignee": "Owen O'Malley", "labels": [], "created": "2006-06-20T01:05:28.000+0000", "updated": "2009-07-08T16:51:56.000+0000", "description": "In case that the local dir is not writable on a node, the tasks on the  node will fail as expected, with an exception like:\n\n(Read-only file system) at java.io.FileOutputStream.open(Native Method) \nat java.io.FileOutputStream.(FileOutputStream.java:179) \nat java.io.FileOutputStream.(FileOutputStream.java:131) \nat org.apache.hadoop.dfs.DFSClient$DFSOutputStream.(DFSClient.java:723) \nat org.apache.hadoop.dfs.DFSClient.create(DFSClient.java:241) \nat org.apache.hadoop.dfs.DistributedFileSystem.createRaw(DistributedFileSystem.java:96) \nat org.apache.hadoop.fs.FSDataOutputStream$Summer.(FSDataOutputStream.java:44) \nat org.apache.hadoop.fs.FSDataOutputStream.(FSDataOutputStream.java:134) \nat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:224) \nat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:176) \n....\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:265) \nat org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:847)\n\nHowever, the task tracker will continue accept new tasks and continue to fail.\nThe runloop of tasktracker should detect such a problem and exits.\n\n", "comments": ["This still happens. I identified a taskTracker which has 4 disks, of which only one is read-only, and seemingly submits any task to the read-only disk (no job got successfully submitted since Nov 30), although mapred.local.dir in hadoop-site.xml specifies local directories on all 4 disks. This node has 3 good disks, still accepts tasks, but cannot execute any of them, de-facto an unusable node without being detected as such.\n\nThe reason seems to be that the localizeJob method in TaskTracker defines a path localJarFile\nalways on the same disk, because the hash is just based on 'taskTracker/jobcache/', independent on the job id,\nand when the disk has become read-only after that directory got created, then the checking in getLocalPath in Configuration.java does not help to identify the disk as read-only.\n\n\nException(s) look like:\nError initializing task_200712090222_0017_m_000870_0:\njava.io.IOException: Mkdirs failed to create<localDir on disk 2>taskTracker/jobcache/job_200712090222_0017\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:345)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:353)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:260)\n\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:139)\n\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:116)\n\tat org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:853)\n\tat org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:834)\n\tat org.apache.hadoop.mapred.TaskTracker.localizeJob(TaskTracker.java:585)\n\tat org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:1143)\n\tat org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:807)\n\tat org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:1179)\n\tat org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:1880)", "This should be addressed by the patch for HADOOP-2227", "This has been fixed by HADOOP-2227. ", "fixed by HADOOP-2227", "Jiras that are fixed as part of other jiras should be marked duplicates, rather than fixed. "], "derived": {"summary": "In case that the local dir is not writable on a node, the tasks on the  node will fail as expected, with an exception like:\n\n(Read-only file system) at java. io.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Task Tracker does not handle the case of read only local dir  case correctly - In case that the local dir is not writable on a node, the tasks on the  node will fail as expected, with an exception like:\n\n(Read-only file system) at java. io."}, {"q": "What updates or decisions were made in the discussion?", "a": "Jiras that are fixed as part of other jiras should be marked duplicates, rather than fixed."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-309", "title": "NullPointerException in StatusHttpServer", "status": "Closed", "priority": "Minor", "reporter": "Konstantin Shvachko", "assignee": "navychen", "labels": [], "created": "2006-06-20T07:37:19.000+0000", "updated": "2009-07-08T16:51:50.000+0000", "description": "The NullPointerException happens in the constructor of StatusHttpServer because the System property \"hadoop.log.dir\" is undefined.\nI see it trying to start the Namenode directly without using any scripts.\n\nException in thread \"main\" java.lang.NullPointerException\n\tat org.mortbay.util.Resource.newResource(Resource.java:99)\n\tat org.mortbay.http.ResourceCache.setResourceBase(ResourceCache.java:140)\n\tat org.mortbay.http.HttpContext.setResourceBase(HttpContext.java:2207)\n\tat org.apache.hadoop.mapred.StatusHttpServer.<init>(StatusHttpServer.java:66)\n\tat org.apache.hadoop.dfs.FSNamesystem.<init>(FSNamesystem.java:172)\n\tat org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:97)\n\tat org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:88)\n\tat org.apache.hadoop.dfs.NameNode.main(NameNode.java:496)\n\nIn general I think the sources should not rely on the system properties and environment variables defined in hadoop scripts.\n", "comments": ["Even if I get through this I get yet another NullPointerException.\nNow because webapps resource is not defined.\n\nException in thread \"main\" java.lang.NullPointerException\n\tat org.apache.hadoop.mapred.StatusHttpServer.getWebAppsPath(StatusHttpServer.java:108)\n\tat org.apache.hadoop.mapred.StatusHttpServer.<init>(StatusHttpServer.java:71)\n\tat org.apache.hadoop.dfs.FSNamesystem.<init>(FSNamesystem.java:172)\n\tat org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:97)\n\tat org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:88)\n\tat org.apache.hadoop.dfs.NameNode.main(NameNode.java:496)", "As we discussed offline, this is just caused because you didn't define a value for \"hadoop.log.dir\" on your java command line. It would be good to have the server not crash when the property is not defined, but the property should be defined for the servers. The two obvious \"fixes\" are to:\n  1. Remove the \"/logs\" context when no \"hadoop.log.dir\" property is defined. This will be confusing to users.\n  2. See if we can get the value from log4j via the jakarta commons logging api. It looks like there might be routines that will work.\n\nIn any case, I believe this is a pretty low priority bug.", "These were the exact same problems I encountered when I ran the unit tests. I had to define hadoop.log.dir (in the build.xml file) and had to put the \"build\" directory (which contains the webapps) in the CLASSPATH.", "here is my solution, does it okay for this bug?\n\n> 1. Remove the \"/logs\" context when no \"hadoop.log.dir\" property is defined.\n\nI changed StatusHttpServer.java at line 73-79\n     73     // set up the context for \"/logs/\"\n     74     HttpContext logContext = new HttpContext();\n     75     logContext.setContextPath(\"/logs/*\");\n     76     String logDir = System.getProperty(\"hadoop.log.dir\");\n     77     logContext.setResourceBase(logDir);\n     78     logContext.addHandler(new ResourceHandler());\n     79     webServer.addContext(logContext);\nto\n     73     // set up the context for \"/logs/\" if \"hadoop.log.dir\" property is defined.\n     74     String logDir = System.getProperty(\"hadoop.log.dir\");\n     75     if( logDir != null ) {\n     76       HttpContext logContext = new HttpContext();\n     77       logContext.setContextPath(\"/logs/*\");\n     78       logContext.setResourceBase(logDir);\n     79       logContext.addHandler(new ResourceHandler());\n     80       webServer.addContext(logContext);\n     81     }\n\nAnd it works okay, but there has a little impact that user cannot visit \"/logs/\" at web UI.\n\n\n> 2. See if we can get the value from log4j via the jakarta commons logging api. It looks like there might be routines that will work.\n\nI does not find a api to get these value at Jakarta commons logging api documents:\nhttp://jakarta.apache.org/commons/logging/apidocs/index.html \nI am puzzle that how to do this, because log4j.properties is read by log4j lib automatically,\nFor hadoop framework, it don't know or should don't know what log library it use, so is it not good to\nGet value from log4j?\n\n\n> Konstantin: \n1, Even if I get through this I get yet another NullPointerException.Now because webapps resource is not defined.\n\nThis is because he did not add base directory of hadoop that contains \"webapp\" directory into CLASSPATH, \nI insert two lines to StatusHttpServer.java to throw \"not found webapps\" exception to user, Is this a good way to fix it?\n    154   private static String getWebAppsPath() throws IOException {\n    155     URL url = StatusHttpServer.class.getClassLoader().getResource(\"webapps\");\n    156     if( url == null )\n    157       throw new IOException(\"webapps not found in CLASSPATH\");\n    158     String path = url.getPath();\n\nif webapps resource is not defined, It seems that could not set a default value for it or return null, \nbecause webAppContext member of StatusHttpServer should be initialized with webapps resource path for many functions need to access it, \nso maybe throw a intelligible exception to user is a better way.\n\n> 2, trying to start the Namenode directly without using any scripts.\n\nTo resolve this problem and let user can start NameNode without any scripts, \ncan run as these steps in hadoop base directory:\n1, export CLASSPATH=$CLASSPATH:./build/:./build/classes/:./:./conf/\n2, export CLASSPATH=$CLASSPATH:`ls lib/*.jar | cat | tr '\\n' :`\n3, export CLASSPATH=$CLASSPATH:`ls lib/jetty-ext/*.jar | cat | tr '\\n' :`\n4, java org.apache.hadoop.dfs.NameNode\n\n", "this patch will fix the follow bugs: \n1, The NullPointerException happens in the constructor of StatusHttpServer because the System property \"hadoop.log.dir\" is undefined. \n   - to fix this, remove the \"/logs\" context when no \"hadoop.log.dir\" property is defined. \n2, Even if I get through this I get yet another NullPointerException. Now because webapps resource is not defined. \n  - to fix this, just throw a \"not found webapps\" IOException to client ", "I just committed this.  Thanks, navychen!"], "derived": {"summary": "The NullPointerException happens in the constructor of StatusHttpServer because the System property \"hadoop. log.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "NullPointerException in StatusHttpServer - The NullPointerException happens in the constructor of StatusHttpServer because the System property \"hadoop. log."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, navychen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-310", "title": "Additional constructor requested in BytesWritable", "status": "Resolved", "priority": "Minor", "reporter": "Peter Sutter", "assignee": "Brock Noland", "labels": [], "created": "2006-06-21T01:13:19.000+0000", "updated": "2011-06-30T07:24:56.000+0000", "description": "\nIt would be grand if BytesWritable.java had an additional constructor as below. This allows me to use the BytesWritable class without doing a buffer copy, since we have a less-than-fully-utilized byte array holding our key.\n\nThanks!\n \n /**\n   * Create a BytesWritable using the byte array as the initial value.\n   * @param bytes This array becomes the backing storage for the object.\n   */\n  public BytesWritable(byte[] bytes, int size) {\n    this.bytes = bytes;\n    this.size = size;\n  }\n  \n", "comments": ["Perhaps a more general constructor, similar to the ones used in many other places, would be better:\n\n/**\n   * Create a BytesWritable using a part of the byte array as the initial value.\n   */\n  public BytesWritable(byte[] bytes, int offset, int length) {\n    ....\n  }\n\nThis way it would satisfy your requirement, and perhaps help to avoid many other cases of copying ...\n", "\nThat would be even better... but would require a rewrite and retest of the rest of the class (the class already supports a length, but not an offset). ", "I'm ok with either change. Obviously introducing an offset is a bigger change. Clearly if the BytesWritable needs to grow, the offset should go to 0. Equivalently, the offset should not be saved as part of the serialization.", "I think the offset would be ideal, but the extra constructor and a set() method which does the same is a good incremental change. That patch is attached.\n", "Updated one test assert statement with reason for failure.", "Hey Brock, thanks a lot for reviving this issue (after 5 years!) I have a few small concerns about this patch:\n\n# The constructor which only takes a {{byte[]}} can now be implemented in terms of this new constructor you've introduced.\n# We now have {{set(byte[] bytes, int length)}} and {{set(byte[] bytes, int offset, int length)}}, which differ not only in signature, but in their semantics with respect to copying of the input data. This seems like it might lead to confusion. Perhaps we should name the new method something like {{setDirect(...)}}? Or perhaps we should amend the existing {{set(...)}} method to not do a copy? The latter is obviously a trickier and backward-incompatible change,  but probably clearer.", "Great points. The attached removes the set method which would have been confusing. This is what the original JIRA request was for.\n\nI do not see any need for the set method I had previously written. If you compare the cost of creating of an additional object, due to no zero copy set, versus being required to perform a copy to fill the BW, the extra object should be negligible with a byte array of any size.", "+1, the patch looks good to me.\n\nMarking this as patch available so Hudson runs test-patch. I'll commit this pending clean Hudson results.\n\nAlso reassigning this to you, Brock.", "+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12483841/bytes-writable-zero-copy-interface-2.patch\n  against trunk revision 1139947.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://builds.apache.org/job/PreCommit-HADOOP-Build/677//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/677//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://builds.apache.org/job/PreCommit-HADOOP-Build/677//console\n\nThis message is automatically generated.", "I've just committed this.\n\nThanks a lot for the contribution, Brock!", "Integrated in Hadoop-Common-trunk-maven #43 (See [https://builds.apache.org/job/Hadoop-Common-trunk-maven/43/])\n    ", "Integrated in Hadoop-Common-trunk #732 (See [https://builds.apache.org/job/Hadoop-Common-trunk/732/])\n    ", "Integrated in Hadoop-Common-trunk-Commit #668 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/668/])\n    "], "derived": {"summary": "It would be grand if BytesWritable. java had an additional constructor as below.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Additional constructor requested in BytesWritable - It would be grand if BytesWritable. java had an additional constructor as below."}, {"q": "What updates or decisions were made in the discussion?", "a": "Integrated in Hadoop-Common-trunk-Commit #668 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/668/])"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-311", "title": "dfs client timeout on read kills task", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-06-21T03:28:08.000+0000", "updated": "2009-07-08T16:41:54.000+0000", "description": "If a DFS client reads a file and times out, it immediately throws an exception to the client, which will kill the task.", "comments": ["This patch gives the client 2 retries to read the data before throwing the exception out to the client.", "I just committed this.  Thanks, Owen!"], "derived": {"summary": "If a DFS client reads a file and times out, it immediately throws an exception to the client, which will kill the task.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "dfs client timeout on read kills task - If a DFS client reads a file and times out, it immediately throws an exception to the client, which will kill the task."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-312", "title": "Connections should not be cached", "status": "Closed", "priority": "Major", "reporter": "Devaraj Das", "assignee": "Devaraj Das", "labels": [], "created": "2006-06-22T01:55:24.000+0000", "updated": "2006-09-08T21:19:45.000+0000", "description": "Servers and clients (client include datanodes, tasktrackers, DFSClients & tasks) should not cache connections or maybe cache them for very short periods of time. Clients should set up & tear down connections to the servers everytime they need to contact the servers (including the heartbeats). If connection is cached, then reuse the existing connection for a few subsequent transactions until the connection expires. The heartbeat interval should be more so that many more clients (order of  tens of thousands) can be accomodated within 1 heartbeat interval.", "comments": ["The way I have designed this is to have the connection caching configurable. Given any IPC server (like the namenode or jobtracker) the caching can range from fully cached (which is the current behavior) to number of transactions per created connection. The configuration related to caching can be set in the client's hadoop config file like:\n<property>\n  <name>ipc.connection.notcached.address</name>\n  <value>server1:port=num_transactions, server2:port=num_transactions</value>\n  <description>Defines the connections that should not be cached.\n  </description>\n</property>\nThe above means that clients close the connections to server1:port after it has done num_transactions (a transaction is defined as one request-response). Any number of such servers can be specified by a comma-separated list. Connections to servers not explicitly mentioned in the config is cached (though the server will disconnect clients that have not communicated with it for a specified period of time).\nDoing this will limit the number of connections cached significantly. For example, one can always specify in the config that connections to the namenode should not be cached at all (num_transactions = 1) and so on.\nThe patch is ready but I can probably incorporate any inputs you may have before I submit that.", "Modified the datanode Heartbeat logic to allow the datanodes to choose a random heartbeat interval between specified limits (the range, after some calculations on the constants defined in FSconstants.java, is between 3 sec and 103 sec).", "Could we break the datanode heartbeat thing you are doing into another bug?  It seems unrelated to a redesign of our general IPC and this bug is not complete enough on the motivation of your work for me to understand why randomizing heartbeat intervals is a good idea.", "I'm very uncomfortable with this direction.  Turning this into a per installation configuration challenge sounds like a very bad and unscalable idea.\n\n-1", "Had a discussion with Eric/Owen on this subject and decided to do the following:\n* Have time-based connection caching - i.e., on the client side, disconnect those connections that have been idle for a (configurable) period of time (default is 1 sec).\n* Postpone tweaking the heartbeat logic until we see a problem there.", "I have a version of the patch for this but am not able to release it because of the problem described in Hadoop-362.  I use randomwriter and sort as my regular test cases. Strangely enough, the problem described in Hadoop-362 surfaces only when I run the sort benchmark *with* the patch for this issue.", "Attached is the patch. This has been tested independently but it should be commited only after hadoop-362's patch gets commited. During my testing a few days back, I noticed that hadoop-362 surfaces (everytime, almost consistently) with this patch.", "This patch implements the following:\n1) Caching of client - server connections is made optional. Defaults to no-caching.\n2) If no-caching is true, clients will disconnect idle connections to a server after a configured time. The idle time defaults to 1 second.\n\nThe performance hit in this case is that once in a while clients are not able to establish a connection to a server (if the server is too busy to accept incoming connections). I have seen this in the case of TaskTracker -> JobTracker protocol. It happens once in a while. When it happens, the JobTracker assumes that the TaskTracker is lost and then there is a whole set of reruns for the tasks that were running on this \"lost\" tasktracker. This slows down the overall progress of the job. Of course, this also happens in the case where the connections are cached but the difference is that the RPCs timeout as opposed to connect failing.\n\nIf the above doesn't happen, the performance figures with/without caching on a 370 node cluster is nearly the same.", "lost tasktrackers because the jobtracker is temporarily busy is a bad thing.\nwe should have some sort of retry mechanism, or a longer accept queue on the server, or some allowable number of lost heartbeats, or something - to overcome this.\nAs a rule, barring hardware failure and user-code bugs, any time a tasktracker is lost is a bug. ", "I agree with this. In the current code, there is a timeout of 10 minutes and only when a TaskTracker is out of contact for this much amount of time does the JobTracker assume that the TaskTracker is dead. Unfortunately, even with this large timeout, sometimes an unfortunate TaskTracker cannot make it. Yes, the accept queue can be made longer but we will hit the problem sometime later when we have more clients. So,  do you think, in addition to increasing the accept queue size, it makes sense to have a two-way heartbeat here? That is, if a server doesn't receive a heartbeat from a client and the expiry-timeout is about to expire, it schedules a heartbeat to the client and probably invokes a GETSTATUS or some such method on the client and if that method returns a valid response, it keeps the client alive for another expiry-timeout interval and this goes on... We can also look at other approaches - some of them are outlined in hadoop-362.\nBy the way, the patch for hadoop-181 should handle the lost tracker problem but this kind of a problem might turn up for any client-server interaction.", "the current model, where a tasktracker sends a heartbeat every few seconds, and is declared dead only after 10 minutes is fine, as long as the rate of loss of heartbeats is low.\nI'm only concerned, since the size of the accept queue is fairly small by default (~5), that  the probability of missing a new connection will be significantly greater than the probability of losing a message in an open tcp stream (very low). Increasing the size of the queue, or implemeting some form of retries could help.\n\nThere's a tradeoff here between the overhead of many 'accept's per second and the overhead of 'select'ing on many sockets, many of which are idle. Let's compare performance with and without connection caching, and see where we get more lost heartbeats, and better jobtracker performance.", "Increasing the accept queue length and a simple retry mechanism helped very much. Two cases - (1) where idle connections are cached for a max of 1 sec at the client, and (2) where connections are fully cached.\nThe performance of the sort benchmark (total time it takes to complete the run) is, most of the times, better with (1). But with a few tasks failing here and there (in both cases), it's actually hard to conclusively say anything about performance in terms of the time it takes to run the benchmark. Made the accept queue length configurable (since that can be manually set on Linux systems as part of the configurable TCP/IP parameters) with the default being 128.", "When connecting, we should first create an unconnected socket, then attempt to connect with an explicit timeout, and retry the connect if it fails.  Otherwise the retries are subject to whaterver the default timeout is.  Since, with this patch, we're connecting much more, the connect timeout is more significant and should not be left to the system default.", "Also, all connection retries should be logged.", "Incorporated the comments by Doug.", "Several of the logging statements look like:\n\n  if (LOG.isDebugEnabled())\n    LOG.info(getName() + ...);\n\nBut you should log and check the same level. So these should instead be:\n\n  if (LOG.isDebugEnabled())\n    LOG.debug(getName() + ...);\n\nAlso, when connecting, you catch Exception, then check exception types with 'instanceof'.  It would be better to catch those particular exceptions.  You're currently silently trapping other exceptions too.\n", "Sorry. My mistake for having overlooked the things you pointed out.", "A few more issues:\n\nThe IS_CACHING_DISABLED field should not be all caps, since it is not a constant.\n\nWhen connection caching is disabled then the connection threads should exit when there are no outstanding responses.  Then the connections.remove(address) could also be moved to Connection.close(), removing some duplicate logic.\n\nFinally, the check of 'cachingDisabled' could be moved to be inside of incrementRef and decrementRef, rather than around each call, changing these to something like:\n\nprivate void incrementRef() {\n  if (cachingDisabled) {\n    return;\n  }\n  synchronized (this) {\n    ...\n  }\n}", "Actually, I thought that I could save the overhead of destroying/creating threads every so often (could be as frequent as once every second). In the current patch, client connection thread will be created just once for every server and that is destroyed & re-created only when there is an error in receiving server response, etc.\n\nRegarding the check for cachingDisabled, it's outside incrementRef/decrementRef since I wanted to avoid so-many-unnecessary method invocations when caching is disabled.\n\nI agree with the IS_CACHING_DISABLED comment that it should not be all caps.\n\nMakes sense ?", "Meant to say \"Regarding the check for cachingDisabled, it's outside incrementRef/decrementRef since I wanted to avoid so-many-unnecessary method invocations when caching is enabled. \"\n", "> I thought that I could save the overhead of destroying/creating threads [ ...]\n\nThreads should be cheap to create, but keeping too many around can be expensive.  Again, I think the life of the thread should be the same as the life of the connection.\n\n> I wanted to avoid so-many-unnecessary method invocations when caching is disabled.\n\nMethod invocations that check a flag are plenty fast.  This is not an inner loop.  I'd rather have this code be easier to maintain than a few nanoseconds faster.", ">Threads should be cheap to create, but keeping too many \n>around can be expensive. Again, I think the life of the thread \n>should be the same as the life of the connection.\n\nThere are not many threads in the system. It is one per server (in the current version of hadoop, it is a single digit number, maybe, 2 or 3). If connection life is 1 sec and if the thread's life is same as the life of the connection, then a thread is destroyed/created ~once/second per server. I really wanted to avoid creating/destroying threads that often.", "> There are not many threads in the system.\n\nPerhaps, but thread creation is quite cheap. I see two reasons for not keeping threads around.\n\na) The code will be simpler if all objects associated with a server (connection and thread in this case) are created/destroyed together. \nb) The point of not caching connections is to avoid lingering unused resources, that applies to threads as much as it does to connections. If there is a use case where connections are created/destroyed once/second/server then the appropriate course to avoid this thrash would be to enable connection caching, no?", "Incorporated the comments.", "Oops, forgot to remove an unused variable. Attached is the modified patch.", "This disables connection caching by default (and hence, everywhere, since no one yet overrides this new option).  If we really don't think we need connection caching, then the client could be radically simpler: no per-connection threads, etc.\n\nI thought the original plan was not so much to disable all connection caching, but rather limit it to hosts that have been contacted recently.  But I see no provision in the current patch for timing out idle connections.\n\nThe incrementRef & decrementRef methods are each only called once.  In both cases the check for isCachingDisabled can be skipped.  So these can simply become inUse++ and inUse--.\n\nLet's not add a separate config option for the connect timeout, but rather just use the normal io timeout.\n\nFinallly, the socket close and open should not be within a synchronized(connections) block, as, that way, a slow server will block access to all servers.", "This patch should address most of the issues. One issue - that of duplicate code to do with connections.remove(address) is still there. I couldn't find a nice way to remove the duplication of code. I tested this on small clusters (like 80 nodes). Haven't got a chance to test it out on bigger clusters, but in any case, I thought I would get it reviewed...", "By the way, this patch has all the functionality that was planned for originally, less thread-caching.", "+        if (noException) {\n+          synchronized (connections) {\n+            connections.remove(address);\n+          }\n+        }\nThe above should be if (!noException) {  ... }", "If CLOSE_CONNECTION is a constant, then it should be declared 'static final'.  But I think it can be removed altogether, along with shouldCloseConnection.  Why not have the ConnectionCuller simply set running=false, so that Connection.run() exits naturally and removes itself from the connection cache?\n\nThe culler doesn't appear to remove the connection from the cache anyway.  It should, so that new requests are not handed to a connection that is exiting.  And to further safeguard against that, we can check, before calling connections.remove(address) that connections.get(address) == this, rather than keeping a noException boolean.\n\nConnectionCuller's constructor should set its thread name.\n\nMAX_RETRIES should be named 'maxRetries' and should get its value from a config parameter named ipc.client.connect.max.retries.\n\nDo we need a separate nocache config parameter, and code to support it?  Shouldn't this be the same as simply setting maxidletime to 0?  If so, let's get rid of isCachingDisabled.  This would give only one mode of operation, greatly reducing the number of possible bugs.\n\nThe culler should sleep for a minimum time (say 1 second), so it doesn't enter a tight loop.\n\nipc.server.system.somaxconn should be named something like ipc.server.listen.queue.size\n", "> If CLOSE_CONNECTION is a constant, then it should be declared 'static final'. But I think it can be \n> removed altogether, along with shouldCloseConnection. Why not have the ConnectionCuller \n> simply set running=false, so that Connection.run() exits naturally and removes itself from the \n> connection cache? \nIf you are referring to the running boolean field attached to the Client class, the setting that to false will make all the response receiver threads exit. So instead, we can have another boolean field attached to the Connection class to signify that the corresponding response receiver thread should exit.\n\n> The culler doesn't appear to remove the connection from the cache anyway. It should, so that new \n> requests are not handed to a connection that is exiting. \nIt does remove I think. Note there is a i.remove() in the run method of the ConnectionCuller thread.\n\n> Do we need a separate nocache config parameter, and code to support it? Shouldn't this be the \n> same as simply setting maxidletime to 0? If so, let's get rid of isCachingDisabled. This would give \n> only one mode of operation, greatly reducing the number of possible bugs.\nSo this patch will make the Hadoop clients operate in two modes - one with full caching of connections, and another idle-time-based caching (connections are culled when they are idle for some period of time). I am not sure I understood what you mean by one mode of operation. Are you saying that we should remove \"if (isCachingDisabled)\" and instead have the corresponding logic work under \"if (maxIdleTime == 0)\" ?\n\nAlso one more thing - currently, I do a check for isCachingDisabled in a few places and then execute relevant code. But some of the code, although not required for the mode where we don't do idle-time-based caching, doesn't really harm the execution of that mode (like incrementRef) except adding some unecessary code. Does it remain that way or do I remove checks for isCachingDisabled from those \"safe\" places ?", "> If you are referring to the running boolean field attached to the Client class, the setting that to false \n> will make all the response receiver threads exit. \n\nMeant to say: \"If you are referring ... *all* the response receiver threads (connected to the different servers) exit, not just that thread which was idle\".\n", "You're right, Client.running is inapproprate for this use, and the culler does in fact remove connections (not sure how I missed that).\n\nRe modes: right now there are three modes, each with some separate logic: full-caching, timed caching, and no caching.  It would simplify the logic (and hence ease testing and increase reliability) if there were only one mode.  Otherwise, we should add unit tests for all modes.  But the timed cache seems fairly universal.  Setting the timeout to Integer.MAX_VALUE gives full-caching, while setting it to zero could cause most connections to terminate after a single request (although, if some get reused, that should not cause problems).  The default should be a fairly short timeout, so that some kinds of requests will reuse cached connections and some will create new connections per request.  This way the default configuration will test all of the code.  I do not like having modes that might be useful but are not regularly used, since they'll not be tested and may stop working and then generate bugs.\n\n> Are you saying that we should remove \"if (isCachingDisabled)\" and instead have the corresponding logic work under \"if (maxIdleTime == 0)\" ? \n\nI don't think we should have either.  MaxIdleTime==0 is simply a very short idle time.  The culler thread should have a minimum sleep time so that it doesn't busywait.", "Thanks Doug. This patch should address your comments. I have set the time between culler runs to 1000 msec. Please modify it to a more meaningful value if you feel that's not appropriate ( & if you feel the patch is in a form that can be committed :) ).", "I just committed this.  Thanks, Devaraj!"], "derived": {"summary": "Servers and clients (client include datanodes, tasktrackers, DFSClients & tasks) should not cache connections or maybe cache them for very short periods of time. Clients should set up & tear down connections to the servers everytime they need to contact the servers (including the heartbeats).", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Connections should not be cached - Servers and clients (client include datanodes, tasktrackers, DFSClients & tasks) should not cache connections or maybe cache them for very short periods of time. Clients should set up & tear down connections to the servers everytime they need to contact the servers (including the heartbeats)."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Devaraj!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-313", "title": "A stand alone driver for individual tasks", "status": "Closed", "priority": "Major", "reporter": "Michel Tourn", "assignee": "Michel Tourn", "labels": [], "created": "2006-06-22T06:30:19.000+0000", "updated": "2006-08-04T22:22:33.000+0000", "description": "This is a tool to reproduce problems and to run unit tests involving either a map or reduce task.\nYou just give it a reduce directory on the command line.\n\nUsage: java org.apache.hadoop.mapred.StandaloneReduceTask <taskdir> [<limitmaps>]\ntaskdir name encodes: task_<jobid>_r_<partition>_<attempt>\ntaskdir contains job.xml and one or more input files named: map_<dddd>.out\nYou should run with the same -Xmx option as the TaskTracker child JVM\n\n", "comments": ["Shouldn't this go into the \"test\" tree rather than the core?", "Actually, it isn't for testing at all. It is for debugging a reduce crash in place on the production machines. Michel manually caught the reduce inputs, but in general we also need to add a configuration setting that lets a job request that some reduce task's inputs be kept. Maybe something like:\nmapred.keep.reduce.inputs=...\nWhere the legal values are \"all\", \"none\", or a list of integers of which reduce tasks to keep the inputs for. \n", "Why not an option to keep inputs for failed reduces?\nAlso recall HADOOP-91.", "An option to keep failed reduces would be useful, but I've certainly seen cases before where I wanted to run a particular fragment in the debugger, even if it didn't crash.", "This patch does a lot more:\n   1. It moves the jobId and partition fields from ReduceTask up into Task.\n   2. It replaces Michel's stand-alone reduce runner, with a more general map or reduce runner.\n   3. It adds new task localizations into the job.xml.\n       For all tasks:\n          a. map.task.id = the task id\n          b. mapred.task.is.map = is this a map\n          c. mapred.task.partition = numeric task id\n          d. mapred.job.id = job id\n      For maps:\n          a. map.input.file = the file that we are reading\n          b. map.input.start = the offset in the file to start at\n          c. map.input.length = the number of bytes in the split\n      For reduces:\n          a. mapred.map.tasks = correct number of maps\n      These new attributes allow me to reconstruct the MapTask or ReduceTask with just the job.xml.\n   4. A new configuration variable keep.failed.task.files that tells the system to keep files/directories for tasks that fail. This attribute can be set on a particular JobConf.\n     ", "One more comment, the usage is:\n\nbin/hadoop org.apache.hadoop.mapred.IsolationRunner <local-dir>/taskTracker/<taskid>/job.xml\n\n", "This looks good, except I think you forgot to add IsolationRunner.java.", "Here is the updated patch that includes the IsolationRunner.", "I just committed this.  Thanks, Owen."], "derived": {"summary": "This is a tool to reproduce problems and to run unit tests involving either a map or reduce task. You just give it a reduce directory on the command line.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "A stand alone driver for individual tasks - This is a tool to reproduce problems and to run unit tests involving either a map or reduce task. You just give it a reduce directory on the command line."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-314", "title": "remove the append phase in sorting the reduce inputs", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-06-22T22:49:53.000+0000", "updated": "2009-07-08T16:51:49.000+0000", "description": "This patch creates a new interface to SequenceFile.sort that allows both a list of files to be passed as input and specifying whether the inputs should be deleted. The ReduceTask is changed to pass the list of map inputs and to delete them as they are sorted.", "comments": ["I just committed this.  Thanks, Owen!"], "derived": {"summary": "This patch creates a new interface to SequenceFile. sort that allows both a list of files to be passed as input and specifying whether the inputs should be deleted.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "remove the append phase in sorting the reduce inputs - This patch creates a new interface to SequenceFile. sort that allows both a list of files to be passed as input and specifying whether the inputs should be deleted."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-315", "title": "bobo Exception in TestRPC", "status": "Closed", "priority": "Minor", "reporter": "Konstantin Shvachko", "assignee": null, "labels": [], "created": "2006-06-23T01:15:29.000+0000", "updated": "2006-12-15T23:02:18.000+0000", "description": "We are getting a bobo exceptions in TestRPC.\nIt looks like the test should fail in this case after throwing bobo.\nBy the way, does anybody know whether \"bobo\" means anything?\n\nerror: java.io.IOException: bobo\njava.io.IOException: bobo\n    at org.apache.hadoop.ipc.TestRPC$TestImpl.error(TestRPC.java:84)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    at java.lang.reflect.Method.invoke(Method.java:585)\n    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:243)\n    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:467)", "comments": ["This is normal.  This is testing the RPC error handler.  RPC errors have always been logged on the server.  Unless you think we ought to change this, I will resolve this as \"won't fix\".\n\nAlso, \"bobo\" was a pejorative term used at Excite, meaning, roughly, \"messed up\".  I don't know the origin."], "derived": {"summary": "We are getting a bobo exceptions in TestRPC. It looks like the test should fail in this case after throwing bobo.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "bobo Exception in TestRPC - We are getting a bobo exceptions in TestRPC. It looks like the test should fail in this case after throwing bobo."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is normal.  This is testing the RPC error handler.  RPC errors have always been logged on the server.  Unless you think we ought to change this, I will resolve this as \"won't fix\".\n\nAlso, \"bobo\" was a pejorative term used at Excite, meaning, roughly, \"messed up\".  I don't know the origin."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-316", "title": "job tracker has a deadlock", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-06-23T01:27:01.000+0000", "updated": "2009-07-08T16:51:47.000+0000", "description": "The JobTracker has a deadlock in the ExpireLaunchingTasks stuff.\n\nIn particular, it locks the JobTracker and launchingTasks inconsistently.\n\nThis deadlocks has been observed in the wild and causes the JobTracker to stop responding to rpc and http (other than the root page).", "comments": ["Make sure that the JobTracker is locked first.", "I just committed this.  Thanks, Owen!"], "derived": {"summary": "The JobTracker has a deadlock in the ExpireLaunchingTasks stuff. In particular, it locks the JobTracker and launchingTasks inconsistently.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "job tracker has a deadlock - The JobTracker has a deadlock in the ExpireLaunchingTasks stuff. In particular, it locks the JobTracker and launchingTasks inconsistently."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-317", "title": "\"connection was forcibly closed\" Exception in RPC on Windows", "status": "Closed", "priority": "Major", "reporter": "Konstantin Shvachko", "assignee": "Doug Cutting", "labels": [], "created": "2006-06-23T01:51:36.000+0000", "updated": "2006-08-03T17:46:48.000+0000", "description": "I see a lot of exceptions caused by RPC in the nightly build on Windows.\nThe most often is thrown by RPC on the namenode, saying\n\n06/06/21 19:28:36 INFO ipc.Server: Server listener on port 7017: readAndProcess threw exception java.io.IOException: An existing connection was forcibly closed by the remote host. Count of bytes read: 0\njava.io.IOException: An existing connection was forcibly closed by the remote host\n    at sun.nio.ch.SocketDispatcher.read0(Native Method)\n    at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:25)\n    at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:233)\n    at sun.nio.ch.IOUtil.read(IOUtil.java:200)\n    at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:207)\n    at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:374)\n    at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:289)\n    at org.apache.hadoop.ipc.Server$Listener.run(Server.java:210)\n\nI am not sure how serious that is, since the tests do not fail, and the name node does not crash.\nBesides the RPC we should probably also check that the unit tests actually fail if they need to.\n", "comments": ["I just committed a fix to this."], "derived": {"summary": "I see a lot of exceptions caused by RPC in the nightly build on Windows. The most often is thrown by RPC on the namenode, saying\n\n06/06/21 19:28:36 INFO ipc.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "\"connection was forcibly closed\" Exception in RPC on Windows - I see a lot of exceptions caused by RPC in the nightly build on Windows. The most often is thrown by RPC on the namenode, saying\n\n06/06/21 19:28:36 INFO ipc."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed a fix to this."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-318", "title": "Progress in writing a DFS file does not count towards Job progress and can make the task timeout", "status": "Closed", "priority": "Major", "reporter": "Milind Barve", "assignee": "Milind Barve", "labels": [], "created": "2006-06-23T03:14:57.000+0000", "updated": "2009-07-08T16:51:48.000+0000", "description": "When a task writes to DFS file, depending on how busy the cluster is, it can timeout after 10 minutes by default, because the progress towards writing a DFS file does not count as progress of the task. The solution (patch is forthcoming) is to provide a way to callback reporter to report task progress from DFSOutputStream.", "comments": ["Patch for the dfs file-writing progress reporting.", "After svn update, the earlier patch failed to compile.\nI have attached a new patch now.", "This looks good, except it is not back-compatible.  Any user code that implements an OutputFormat will no longer compile after this change is made.  Sigh.  I don't see an easy way around this...", "The only way around it that I can see is if we had:\n\nFSDataOutputStream:\n    setProgressable(Progressable prog)\n\nRecordWriter:\n   setProgressable(Progressable prog)\n\nwhich just pushes the problem down a level.", "I don't see how this can be done without breaking backward-compatibility. Therefore I have made changes so that with minimum porting any other output formats could be incorporated. An additional getRecordWriter method needs to be implemented that takes an additional parameter. This parameter can be passed to fs.create (or even ignored as in the case of local filesystem.)\n", "This is an updated patch for this issue that does not have any errors \"task reported no progress for 600 seconds\" even if there is progress. In fact it is a datanode allocation patch. Each datanode sends an additional load data to namenode that indicates how many bllocks it is currently writing or reading. The namenode, when choosing datanodes for new block takes this load into consideration, and discards datanodes whose load is more than twice that of average.\n\nThiss is in addition to the requirement that the datanode has enough space to store min_num_blocks.\n\nWith this patch, I never see the \"no progress for 600 seconds, killing task\" error. Therefore, on my 240 node cluster, the randomwriter times went down from 3997 seconds to 2404 seconds.\n\nThis patch includes the file-writing progress patch as well. So, please discard  the two patches I submitted earlier.", "Sigh, I also don't see a compatible way to make this change.  So we'll have to upgrade some Nutch InputFormat implementations to define the new method.  Could you please construct a patch for Nutch too?  That would make my life easier.  Thanks.", "Sure. I will provide a patch for nutch tomorrow.", "I have attached the patch for nutch to NUTCH-312.\nAffter the recent commmits, there are conflicts (espcecially in datanodeInfo etc). I am resolving those issues, and will provide  a new patch for this issue soon. In the meanwhile, I am deleting the currrently attached patches.", "this is the correct patch after resolving issues arising from conflicts due to recent commits.", "Sigh.  Now, with HADOOP-321 reverted, this no longer applies.", "I will submit the correct patch again.", "After factoring out the changes introduced by recalling patch for hadoop-320, I have not attached a new patch for this bug. Hopefully this is the last patch.", "I meant reverting hadoop-321, not hadoop-320.", "Any reason not to delete the old getRecordWriter() method?", "Did not even think about it earlier. But  if compiles, go ahead.", "Doug. I have removed the extra getRecordWriter method, added progressable documentation, and param doc, and here is the patch. I have also uploaded corresponding patch to nutch-312.", "I just committed this.  Thanks, Milind."], "derived": {"summary": "When a task writes to DFS file, depending on how busy the cluster is, it can timeout after 10 minutes by default, because the progress towards writing a DFS file does not count as progress of the task. The solution (patch is forthcoming) is to provide a way to callback reporter to report task progress from DFSOutputStream.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Progress in writing a DFS file does not count towards Job progress and can make the task timeout - When a task writes to DFS file, depending on how busy the cluster is, it can timeout after 10 minutes by default, because the progress towards writing a DFS file does not count as progress of the task. The solution (patch is forthcoming) is to provide a way to callback reporter to report task progress from DFSOutputStream."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Milind."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-319", "title": "FileSystem \"close\" does not remove the closed fs from the fs map", "status": "Closed", "priority": "Major", "reporter": "Hairong Kuang", "assignee": "Hairong Kuang", "labels": [], "created": "2006-06-23T03:48:37.000+0000", "updated": "2006-08-03T17:46:48.000+0000", "description": "The close methods of both DistributedFileSystem and LocalFileSystem do not remove the closed file system from the fs map in FileSystem. As a result, a subsequent call to FileSystem.getNamed may return the handle to the closed file system and receive errors if perform any operation on the fs handle.", "comments": ["Here is the patch.", "I am unable to apply this patch:\n\n% patch -p 0 < ~/Desktop/fs_close.patch\npatching file src/java/org/apache/hadoop/fs/FileSystem.java\npatching file src/java/org/apache/hadoop/dfs/DistributedFileSystem.java\npatching file src/java/org/apache/hadoop/fs/LocalFileSystem.java\npatching file src/test/org/apache/hadoop/fs/TestCopyFiles.java\npatch: **** malformed patch at line 51: @@ -171,12 +171,10 @@\n\nDoes it work for you?  Thanks!", "Hope this one works.", "FileSystem.close(String) should not be called by anything other than an implementation, so it should be protected instead of public.  Alternately, one could change FileSystem.close() to call NAME_TO_FS.remove(getName()), then just add a call to super.close() in the implementations (and document that all FileSystem implementations should do this).", "Doug, thanks for the comment. The attached patch takes the 2nd approach that you suggested.", "I just committed this.  Thanks, Hairong."], "derived": {"summary": "The close methods of both DistributedFileSystem and LocalFileSystem do not remove the closed file system from the fs map in FileSystem. As a result, a subsequent call to FileSystem.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "FileSystem \"close\" does not remove the closed fs from the fs map - The close methods of both DistributedFileSystem and LocalFileSystem do not remove the closed file system from the fs map in FileSystem. As a result, a subsequent call to FileSystem."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Hairong."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-320", "title": "bin/hadoop dfs -mv does not mv  source's checksum file if source is a file", "status": "Closed", "priority": "Major", "reporter": "Hairong Kuang", "assignee": "Hairong Kuang", "labels": [], "created": "2006-06-23T06:03:31.000+0000", "updated": "2009-07-08T16:41:55.000+0000", "description": "None of the rename operation of DistributedSystem or DFSClient checks if the source is a file or not. Need also to rename the checksum file accordingly if the source is a file.", "comments": ["Here is the patch.", "FileSystem.rename() already renames the .crc file, no?  That's the primary difference between 'rename' and 'renameRaw'.  Unless I hear otherwise, I will resolve this as \"cannot reproduce\".", "I've heard more reports of this.\n\nCan someone please provide a test case that demonstrates this?", "Here is an example demonstrating this problem.\n\nrlhsiao@csdev1001:~sites>hadoop dfs -ls a\n06/09/01 14:49:37 INFO conf.Configuration: parsing jar:file:/nfs/ystools/vol/ystools/releng/build/Linux_2.6_rh4_i686/tools/hadoop/kryptonite/hadoop-0.5.0.jar!/hadoop-default.xml\n06/09/01 14:49:37 INFO conf.Configuration: parsing file:/nfs/ystools/vol/ystools/releng/build/Linux_2.6_rh4_i686/tools/hadoop/kryptonite/conf/kryptonite2/hadoop-site.xml\n06/09/01 14:49:37 INFO ipc.Client: Client connection to 72.30.117.134:8020: starting\nFound 1 items\n/user/rlhsiao/a/test.txt        <r 3>   1326\n\nrlhsiao@csdev1001:~sites>hadoop dfs -mv adobe/adobe.txt a\n...\n\nrlhsiao@csdev1001:~/sites> hadoop dfs -cat a/adobe.txt |more\n06/09/01 14:44:11 INFO conf.Configuration: parsing jar:file:/nfs/ystools/vol/ystools/releng/build/Linux_2.6_rh4_i686/tools/hadoop/kryptonite/hadoop-0.5.0.jar!/hadoop-default.xml\n06/09/01 14:44:11 INFO conf.Configuration: parsing file:/nfs/ystools/vol/ystools/releng/build/Linux_2.6_rh4_i686/tools/hadoop/kryptonite/conf/kryptonite2/hadoop-site.xml\n06/09/01 14:44:11 INFO ipc.Client: Client connection to 72.30.117.134:8020: starting\n06/09/01 14:44:11 WARN fs.DataInputStream: Problem opening checksum file: a/adobe.txt.  Ignoring exception: org.apache.hadoop.ipc.RemoteException: java.io.IOException: Cannot open filename /user/rlhsiao/a/.adobe.txt.crc\n        at org.apache.hadoop.dfs.NameNode.open(NameNode.java:178)\n        at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:585)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:332)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:468)\n\n        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:159)\n\n[content of the file..] ...", "Ok, figured out what went wrong. The rename method of FileSystem does try to move the checksum file, but it does not check if the destination is a directory or not. So if the dst is a directory, the src checksum file is incorrectly moved to .dst.crc not dst/.src.crc.", "I just committed this.  Thanks, Hairong!"], "derived": {"summary": "None of the rename operation of DistributedSystem or DFSClient checks if the source is a file or not. Need also to rename the checksum file accordingly if the source is a file.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "bin/hadoop dfs -mv does not mv  source's checksum file if source is a file - None of the rename operation of DistributedSystem or DFSClient checks if the source is a file or not. Need also to rename the checksum file accordingly if the source is a file."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Hairong!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-321", "title": "DatanodeInfo refactoring", "status": "Closed", "priority": "Major", "reporter": "Konstantin Shvachko", "assignee": "Konstantin Shvachko", "labels": [], "created": "2006-06-23T08:08:54.000+0000", "updated": "2009-07-08T16:41:55.000+0000", "description": "I'm trying to refactor some name node classes, which seem to be similar.\nSo DatanodeInfo is a public api now for purely external ( to name node) use.\nThe name node class that stores information about data nodes including the\nset of its blocks is called DatanodeDescriptor.\nThe DatanodeReport is removed since it was a variation of DatanodeInfo.\nPreviously DatanodeInfo and DatanodeDescriptor were the same class, and\nDatanodeReport was used for reporting node statistics only.\nThis is a preparation step for HADOOP-306.\n", "comments": ["DatanodeDescriptor should not be public.", "I like this.  However we cannot simply remove DataNodeReport, since it's public, and may be referenced by user code.  Rather, we should change it to a deprecated subclass of DatanodeInfo that defines no methods, and file a bug to remove it from the next release.  Also, DatanodeInfo subclasses the non-public DatanodeID, so that class should probably be made public too.", "I've updated the patch.\n", "DatanodeInfo.lastUpdate() is deprecated.\nDatanodeInfo.getLastUpdate() should be the name.\n", "I just committed this.  Thanks, Konstantin.", "This is the patch that broke the system. Having looked at this code, I don't like the use of inheritance between DatanodeInfo and DatanodeDescriptor. I think that composition is a _much_ better approach. They are _not_ logically the sub/super-type of each other. In particular, DatanodeInfo is intended to be Writable and DatanodeDescriptor is intended NOT to be Writable. Making them inherit from each other not only broke the system, it is a really problematic design.\n\nIn any case, if you take svn head and revert this patch using:\npatch -p0 -R < DatanodeRefactor.patch\nyou'll have a working system again.", "I reverted this.", "Sorry for submitting a bad path. I'm attaching the correct one.\n\nIt look like we have completely different views with Owen on how the inheritance should be used in OOP.\nIn this particular case the derived class DatanodeDescriptor is intended to inherit and reuse all members\nof DatanodeInfo except for the Writable interface (2 methods). So the inheritance seems quite logical to me.\nSaves a lot of source code too.\n", "If you are saying you think an object should inherit methods that don't work, I'm not happy with the idea.  Saving lines of code isn't a good enough reason to do this.\n\nMaybe you need to refactor the base class if what you are trying to do does make sense.", "In fact DatanodeDescriptor needs everything that it inherits from DatanodeInfo\neven the Writable.read() and Writable.write().\nThe last two are going to be used for logging and check pointing of DatanodeDescriptor-s.\nWhat is not used in DatanodeDescriptor is the Writable factory, because descriptors are not\nmeant to be used in RPC communications. That is why the factory is defined for DatanodeInfo\nbut not for the descriptor class.\nSo if we go the composition way we will particularly need to implement Writable for\nDatanodeDescriptor, the only purpose of which would be to call the DatanodeInfo Writable methods.\nWhy? With inheritance it is already there. And the same applies to all other methods.\n\nA few words about the bug that caused this discussion.\nI did not realize that RPC sends classes based on the actual object instances rather than on the type declarations.\nSo DatanodeDescriptor was sent even though the variable was declared as DatanodeInfo.\nThe receiving side then was not able to instantiate the received DatanodeDescriptor class because the factory is\nnot defined for it. DatanodeInfo was expected to be received.\n", "This one is updated in synch with the last release.", "DatanodeDescriptor may in the future be Writable, but the current version in your patch is not. It doesn't implement readFields or write and just inherits the ones from DatanodeInfo, which will ignore the TreeMap in the DatanodeDescriptor.\n\nIn my experience, using inheritance for implementation is extremely brittle. It leads to confusing code that is easy to break. This has been born out both by this bad patch and Milind's very reasonable attempt to fix it that was incorrect because of this design idiom. Another instance when it caused trouble was having the FSNameSystem.FileUnderConstruction inherit from Vector. It was fine, until I tried to make changes to it and the code broke in strange ways. Obviously it can work, I just think create problematic code.\n\nI think that composition is a much better idiom for doing this kind of stuff.", "If class A is supposed to work with each and every method of class B is it the reason to derive A from B or not?", "It took me a while to implement the composition approach.\nBut here it is. What I don't like about it is:\na) the patch is bigger (41K vs 54), and it's not just more changes\nb) it's also the performance, I have to use 2 function calls instead of one,\nwhenever a member is accessed, but copying arrays just to obtain the inner\nclasses is what makes me really sad.\nThe latter can be avoided but will require even more changes.\nI think it's not reasonable to spend more time on this issue.\nBoth versions are fully functional, let's pick one and move on.", "I've trivially updated Konstantin's patch to make it apply to the current sources.\n\nI'm still not wild about having the DataNodeInfo send part of itself via RPC or being Comparable.  There is a lot of confusion still about what should be a DatanodeInfo versus a DatanodeID, but that was present in the original code, not the patch and indicates additional refactorings that are needed.\n\nI ran both the unit tests and a distributed test and it was all good.\n\nThis patch gets rid of some of the redundant code and that is a good thing. I also like the comments saying what the types of the elements in the containers are. I'm looking forward to when we can declare all of the container types. *smile*\n\nA few comments:\n  1. A bunch of the routines have empty javadoc, which isn't useful at all. Although I suspect  those routines were just moved from somewhere else, it makes the patch look bad.\n  2. Namenode.sendHeartbeat creates a throwaway DatanodeDescriptor, including its block set, just to do a lookup on the name. Is there a way to index that table on the DatanodeID instead?\n  3. DatanodeID implements compareTo, but not equals or hashCode. Usually it is a good idea to define all of them.\n  4. Why isn't DatanodeID implementing Writable?\n  5. To write out strings, you don't need to create UTF8 from them, you just need to do:\n      UTF8.writeString(out, name);\n      UTF8.writeString(out, storageId);\n   and reading looks like:\n     name = UTF8.readString(in);\n     storageId = UTF8.readString(in);\n  It works either way, it is just a little easier to read the code with the helper functions.\n", "I'm +1 for commiting this before it goes stale again.", "I just committed this.  Thanks!"], "derived": {"summary": "I'm trying to refactor some name node classes, which seem to be similar. So DatanodeInfo is a public api now for purely external ( to name node) use.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "DatanodeInfo refactoring - I'm trying to refactor some name node classes, which seem to be similar. So DatanodeInfo is a public api now for purely external ( to name node) use."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-322", "title": "Need a job control utility to submit and monitor a group of jobs which have DAG dependency", "status": "Closed", "priority": "Major", "reporter": "Runping Qi", "assignee": "Runping Qi", "labels": [], "created": "2006-06-24T12:15:08.000+0000", "updated": "2006-09-08T21:19:46.000+0000", "description": "\nIn my applications, some jobs depend on the outputs of other jobs. Therefore, job dependency forms a DAG. A job is ready to run if and only if it does not have any dependency or all the jobs it depends are finished successfully. To help schedule and monitor a group of jobs like that, I am thinking of implementing a utility that:\n\t- accept jobs with dependency specification\n      - monitor job status\n      - submit jobs when they are ready\n\nWith such a utility, the application can construct its jobs, specify their dependency and then hand the jobs to the utility class. The utility takes care of the details of job submission.\n\nI'll post my design skech for comments/suggestion.\nEventually, I'll submit a patch for the utility.\n\n\n\n", "comments": ["\nWould Ant solve the problem, if a few extensions were developed? (such as an extentsion to check for completion metadata in a .completed file)?\n\n", "\nMy patch is attached.\n\nIt has two classes: hadoop.jobs.Job and hadoop.jobs.JobControl.\n\nJob class encapsulates a MapReduce job and its dependency. It monitors \nthe states of the depending jobs and updates the state of this job.\nA job starts in the WAITING state. If it does not have any deoending jobs, or\nall of the depending jobs are in SUCCESS state, then the job state will become\nREADY. If any depending jobs fail, the job will fail too. \nWhen in READY state, the job can be submitted to Hadoop for execution, with\nthe state changing into RUNNING state. From RUNNING state, the job can get into \nSUCCESS or FAILED state, depending the status of the jon execution.\n\nJobControl class encapsulates a set of MapReduce jobs and its dependency. It tracks \nthe states of the jobs by placing them into different tables according to their \nstates.  This class provides APIs for the client app to add  jobs to the group and to get \nthe jobs in different states. When a \njob is added, an ID unique to the group is assigned to the job. \nThis class has a thread that submits jobs when they become ready, monitors the\nstates of the running jobs, and updates the states of jobs based on the state changes \nof their depending jobs states. The class provides APIs for suspending/resuming\nthe thread,and for stopping the thread.\n\nA typical use scenarios is as follows:\n\n    create a set of Map/Reduce job confs\n    create a Job object per map/reduce job conf with proper depency \n    create a JobControl object\n    add the Job objects to the JobControl object\n   create a control thread and run it:\n\n                     Thread theController = new Thread(theControl);\n\ttheController.start();\n\twhile (!theControl.allFinished()) {\t\n\t        System.out.println(\"Jobs in waiting state: \" + theControl.getWaitingJobs().size());\n\t        System.out.println(\"Jobs in ready state: \" + theControl.getReadyJobs().size());\n\t        System.out.println(\"Jobs in running state: \" + theControl.getRunningJobs().size());\n\t        System.out.println(\"Jobs in success state: \" + theControl.getSuccessfulJobs().size());\n\t        System.out.println(\"Jobs in failed state: \" + theControl.getFailedJobs().size());\n\t        System.out.println(\"\\n\");\n\t\t\t\n\t        try {\n\t                Thread.sleep(60000);\n\t        } catch (Exception e) {\n\t\t\t\t\n\t        }\n\t}\n\ttheControl.stop();\n\n", "\nFix a bug due to wrong commenting\n", "This looks like a nice addition to Hadoop.  I'd like to see some unit tests added before it is committed.  Also, tabs are used for indentation, when spaces are preferred.", "OK,\n\nI'll re-submit a new patch with a unit test class (may take a few days though).", "\nHere is a new patch.\n\nI've replaced tabs with spaces, and added a unit test class TestJobControl.\n", "The unit test class should extend junit.framework.TestCase and implement at least one public method named testXXX().", "\nTestJobControl nows extends junit.framework.TestCase and implements public void testJobControl() method.\nAlso some comments are added to explain what is going on.\n\n", "I think the 'try' block should be removed from your test.  Otherwise it will never fail, no?  Instead it should be permitted to throw exceptions if it fails, so that unit tests will fail.", "When I remove the 'try' block and add 'throws Exception' where required, the unit test still passes, although all of the jobs fail:\n\n% ant clean test -Dtestcase=TestJobControl\n\ntest:\n    [mkdir] Created dir: /home/cutting/src/hadoop/build/test/data\n    [mkdir] Created dir: /home/cutting/src/hadoop/build/test/logs\n    [junit] Running org.apache.hadoop.jobs.TestJobControl\n    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 74.29 sec\n\nBUILD SUCCESSFUL\nTotal time: 1 minute 22 seconds\n\nYet the log in build/test shows:\n\njava.lang.OutOfMemoryError: Java heap space\nJobs are all done???\nJobs in waiting state: 0\nJobs in ready state: 0\nJobs in running state: 0\nJobs in success state: 0\nJobs in failed state: 4\n\nShouldn't the test fail if any of the jobs fail?\n", "\nDoug, this is a tricky question. The test class is to test the behavior of JobControl class, not the jobs it controls. In your case, I think the JobControl behaved correctly if job 3 and job 4 were failed without being submitted, due to one or more depending job fail. \n\nI can add in such logic to the test class to check the job state consistency.\n", "Job state consistency check is added to TestJobControl class", "\nDoug,\n\nwhat is the status about this patch?\n\n", "\nIs there still a problem in commiting this patch?\nI think the patch adds a good value to Hadoop APIs, and is pretty safe, since it does not affect the server side at all.\n\n", "Sorry.  I lost track of this.  The new \"Patch Available\" status should help to keep this from happening.  I'll review this first thing tomorrow morning.  Thanks for the reminder.", "This looks good.\n\nOne minor nit is that the sleep in the unit test should probably be smaller.  If I change the sleep to 1000ms, then the entire test only takes 37 seconds, 20 seconds longer than the 60 second sleep you have.  I run unit tests a lot and am sensitive to them running slowly.\n\nAlso, this should be in a mapred subpackage, since it is mapred-specific stuff.  I'd suggest org.apache.hadoop.mapred.jobcontrol.\n\nThanks for your patience & persistence!", "\nthe sleep in the unit test is changed to 5 seconds from 60 seconds.\nAlso, UTF8 is replaced with Text.\n\nThe code is now in package org.apache.hadoop.mapred.jobcontrol.\n", "I just committed this.  Thanks Runping!\n\nI also added a package.html."], "derived": {"summary": "In my applications, some jobs depend on the outputs of other jobs. Therefore, job dependency forms a DAG.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Need a job control utility to submit and monitor a group of jobs which have DAG dependency - In my applications, some jobs depend on the outputs of other jobs. Therefore, job dependency forms a DAG."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks Runping!\n\nI also added a package.html."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-323", "title": "IO Exception at LocalFileSystem.renameRaw, when running Nutch nightly builds (0.8-dev).", "status": "Resolved", "priority": "Major", "reporter": "Kuro Kurosaka", "assignee": null, "labels": [], "created": "2006-06-27T00:45:48.000+0000", "updated": "2011-07-16T16:30:25.000+0000", "description": "IO Exception at LocalFileSystem.renameRaw, when running Nutch nightly builds (0.8-dev).\nPlease see the deatil descriptions in:\nhttp://issues.apache.org/jira/browse/NUTCH-266\n\nNot knowing how to reclassify an existing bug, I am opening this new bug under Hadoop.\n\nThe version number is 0.3.3 but because I don't see it in the jira list, I chose the closest matching version.  The Nutch-with-GUI build was running with hadoop-0.2 but stopped running, exhibiting the same symptom with other nightly builds, when switched to use hadoop-0.3.3.\n\nI checked \"fs\" as component but this bug could also be caused by the order in which jobs are scheduled, I suspect.\n\n", "comments": ["Hadoop 0.5 automagically fixes this.\nSee\nhttp://issues.apache.org/jira/browse/NUTCH-266\n", "Was fixed long time ago, but wasn't closed. Doesn't apply today.\n\nI run LJRunner and it never really complains in any run about things like these.\n\nClosing as Invalid (now)."], "derived": {"summary": "IO Exception at LocalFileSystem. renameRaw, when running Nutch nightly builds (0.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "IO Exception at LocalFileSystem.renameRaw, when running Nutch nightly builds (0.8-dev). - IO Exception at LocalFileSystem. renameRaw, when running Nutch nightly builds (0."}, {"q": "What updates or decisions were made in the discussion?", "a": "Was fixed long time ago, but wasn't closed. Doesn't apply today.\n\nI run LJRunner and it never really complains in any run about things like these.\n\nClosing as Invalid (now)."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-324", "title": "\"IOException: No space left on device\" is handled incorrectly", "status": "Closed", "priority": "Major", "reporter": "Konstantin Shvachko", "assignee": "Wendy Chien", "labels": [], "created": "2006-06-27T07:59:17.000+0000", "updated": "2006-09-08T21:19:47.000+0000", "description": "When a data node disk is almost full the name node still assigns blocks to the data node.\nBy the time the data node actually tries to write that data to disk the disk may become full.\nCurrent implementation forces the data node to shutdown after that.\nThe expected behavior is to report the block write failure and continue.\n\nThe Exception looks as follows:\n\njava.io.IOException: No space left on device\nat java.io.FileOutputStream.writeBytes(Native Method)\nat java.io.FileOutputStream.write(FileOutputStream.java:260)\nat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)\nat java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)\nat java.io.DataOutputStream.write(DataOutputStream.java:90)\nat org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:623)\nat org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:410)\nat java.lang.Thread.run(Thread.java:595)\n2006-06-26 08:26:04,751 INFO org.apache.hadoop.dfs.DataNode: Finishing DataNode in: /tmp/hadoop/dfs/data/data\n\n", "comments": ["Perhaps the datanode should behave as if any assigned incomplete (IE write locked) block is expect to become full size and only allow new blocks if it has enough space considering all currently locked blocks?\n\nAs part of its heart beat it and block creation ack it could report if it is ready to take another block.\n\nWe should be able to configure an amount of working space to be left on each device with a home directory too...  Is this currently handled?", "The amount of working space to be left on each device is addressed by HADOOP-296\nI think that if the disk is full we should just try again. In my experience it was the taskTracker\nwho filled the disk to the capacity. There night be other programs that write temporary data\ninto disk and then remove it.\nThe data node should not shutdown itself anyway, it might be useful for something anyways.", "I'm fixing this issue by creating a new exception (DiskOutOfSpaceException) that extends IOException.  In DataNode:writeBlock, a DiskOutOFSpaceException is thrown if while opening local disk out, data.writeToBlock fails due to insufficient space and if while processing the incoming data, the amount we want to write (bytesRead) is greater than the remaining space in the data set.\n\nThis comment serves to claim this bug as mine and as an invitation for comments on this approach.   \n\n", "This patch fixes the problem by checking the message of the IOException to see if it is \"No space left on device\".   If so, then it throws a new DiskOutOfSpaceException and does not shutdown.  If not, then it does what it originally did, shutdown.  \n\nIf there are objections to this way, please let me know.", "I just committed this.  Thanks!"], "derived": {"summary": "When a data node disk is almost full the name node still assigns blocks to the data node. By the time the data node actually tries to write that data to disk the disk may become full.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "\"IOException: No space left on device\" is handled incorrectly - When a data node disk is almost full the name node still assigns blocks to the data node. By the time the data node actually tries to write that data to disk the disk may become full."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-325", "title": "ClassNotFoundException under jvm 1.6", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-06-27T12:57:52.000+0000", "updated": "2006-08-03T17:46:48.000+0000", "description": "We have been having problems with classes that are returned by RPC methods not being loaded/initialized correctly. The work around has been to put in the servers, code of the form:\n\nstatic { new FooBar(); }  // to resolve the ClassNotFoundException for class FooBar.\n\nWhen I tried running under java 1.6, that stopped working because one of the classes had to be instantiated from a package that didn't have visibility to create an instance. So I tracked the problem down to how the classes were being loaded via reflection.", "comments": ["This patch uses the required classloader, but makes sure the class is initialized.\n\nI found and removed the object creation blocks from the 4 servers, but if there are others hiding somewhere, please let me know.", "I just committed this.  Thanks, Owen!"], "derived": {"summary": "We have been having problems with classes that are returned by RPC methods not being loaded/initialized correctly. The work around has been to put in the servers, code of the form:\n\nstatic { new FooBar(); }  // to resolve the ClassNotFoundException for class FooBar.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "ClassNotFoundException under jvm 1.6 - We have been having problems with classes that are returned by RPC methods not being loaded/initialized correctly. The work around has been to put in the servers, code of the form:\n\nstatic { new FooBar(); }  // to resolve the ClassNotFoundException for class FooBar."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-326", "title": "cleanup of dead field (map ouput port)", "status": "Closed", "priority": "Minor", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-06-27T13:10:24.000+0000", "updated": "2009-07-08T16:51:49.000+0000", "description": "The TaskTrackerStatus retains a field that records the port of the map outuput server, even though that server no longer exists in the current code base.", "comments": ["I just committed this.  Thanks, Owen."], "derived": {"summary": "The TaskTrackerStatus retains a field that records the port of the map outuput server, even though that server no longer exists in the current code base.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "cleanup of dead field (map ouput port) - The TaskTrackerStatus retains a field that records the port of the map outuput server, even though that server no longer exists in the current code base."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-327", "title": "ToolBase calls System.exit", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Hairong Kuang", "labels": [], "created": "2006-06-27T13:15:11.000+0000", "updated": "2006-08-04T22:22:34.000+0000", "description": "The new ToolBase class calls System.exit when the main routine finishes. That will break if the application uses threads that need to finish before the jvm exits. The normal semantics is that the program doesn't finish execution until all of the non-daemon threads exit (including the main one) and System.exit should never be called except for critical errors.", "comments": ["The patch removes System.exit from ToolBase and adjusts the interface of method \"run\" to throw IOException.", "I agree with removing the call to exit(), but I don't see the need to change the exception.\n\nTo my thinking, this replaces the standard main() signature:\n\npublic static void main(String[] args) throws Exception {\n  ...\n}\n\nFolks frequently write main() to not throw anything, and then catch exceptions and print the stacktrace.  This just makes the code bigger, since the JVM already prints the stack trace when a main() throws an exception.  It also makes it harder to call the main() programatically (which is ocassionally useful) since you'll have no idea whether it has somehow failed.  So this is why I argue that main() routines should throw Exception, and analagously, why our replacement should do the same.  Otherwise we'll just have folks catching other exceptions and repackaging them in IOException, adding pointless code.\n", "\nI think it is better to be as explicit as possible about which (checked) Exceptions your main() routine is not handling.  It is useful to the reader to know that the only checked exception that can be thrown is an IOException.  This doesn't require any additional code, and in any case readability is more important than saving a line or two.\n\n", "But at issue here is not a particular main(), but an interface for main()-like methods.  So we cannot know which specific exceptions might be thrown.", "\nAgreed that the interface method needs to throw Exception.  I thought that you were disagreeing with the changes to the more specific IOException in the implementing classes also.\n", "Here is the new patch reflecting Doug's suggestion. In addition to removing System.exit, I changed the main function of DFSShell, JobClient, and CopyFiles to throw Exception instead of IOException.", "I just committed this.  Thanks, Hairong.\n\nDavid: you are right, implementations of the Tool interface can throw more precise exceptions."], "derived": {"summary": "The new ToolBase class calls System. exit when the main routine finishes.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "ToolBase calls System.exit - The new ToolBase class calls System. exit when the main routine finishes."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Hairong.\n\nDavid: you are right, implementations of the Tool interface can throw more precise exceptions."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-328", "title": "add a -i option to distcp to ignore read errors of the input files", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-06-27T13:41:35.000+0000", "updated": "2006-08-03T17:46:48.000+0000", "description": "Add an option \"-i\" to ignore problems reading files and just copy what can be copied.", "comments": ["Adds the -i option to distcp, which makes the copy ignore read errors. The -i option must come after the generic options. The failure to copy is only reported back in the web ui.", "I just committed this.  Thanks, Owen!"], "derived": {"summary": "Add an option \"-i\" to ignore problems reading files and just copy what can be copied.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "add a -i option to distcp to ignore read errors of the input files - Add an option \"-i\" to ignore problems reading files and just copy what can be copied."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-329", "title": "ClassCastException in DFSClient", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": null, "labels": [], "created": "2006-06-28T07:07:48.000+0000", "updated": "2009-07-08T16:41:57.000+0000", "description": "I'm getting the following message back to my launching application:\n\nException in thread \"main\" org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.lang.ClassCastException: org.apache.hadoop.dfs.DatanodeInfo cannot be cast to java.lang.Comparable\n        at java.util.TreeMap.getEntry(TreeMap.java:325)\n        at java.util.TreeMap.containsKey(TreeMap.java:209)\n        at java.util.TreeSet.contains(TreeSet.java:217)\n        at org.apache.hadoop.dfs.DFSClient.bestNode(DFSClient.java:373)\n        at org.apache.hadoop.dfs.DFSClient.access$100(DFSClient.java:42)\n        at org.apache.hadoop.dfs.DFSClient$DFSInputStream.blockSeekTo(DFSClient.java:520)\n        at org.apache.hadoop.dfs.DFSClient$DFSInputStream.read(DFSClient.java:638)\n        at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:167)\n        at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)\n        at java.io.BufferedInputStream.read1(BufferedInputStream.java:256)\n        at java.io.BufferedInputStream.read(BufferedInputStream.java:313)\n        at java.io.DataInputStream.readFully(DataInputStream.java:174)\n        at java.io.DataInputStream.readFully(DataInputStream.java:150)\n        at org.apache.hadoop.fs.FSDataInputStream$Checker.<init>(FSDataInputStream.java:55)\n        at org.apache.hadoop.fs.FSDataInputStream.<init>(FSDataInputStream.java:237)\n        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:157)\n        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:72)\n        at org.apache.hadoop.dfs.DistributedFileSystem.copyToLocalFile(DistributedFileSystem.java:182)\n        at org.apache.hadoop.mapred.JobInProgress.<init>(JobInProgress.java:83)\n        at org.apache.hadoop.mapred.JobTracker.submitJob(JobTracker.java:935)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:589)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:243)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:469)\n\n        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:159)\n", "comments": ["This is something that unit tests should have caught.  Perhaps we should add a test that starts a two-datanode DFS system?", "This was fixed when I reverted HADOOP-321."], "derived": {"summary": "I'm getting the following message back to my launching application:\n\nException in thread \"main\" org. apache.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "ClassCastException in DFSClient - I'm getting the following message back to my launching application:\n\nException in thread \"main\" org. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was fixed when I reverted HADOOP-321."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-330", "title": "Raw SequenceFile Input/Output formats", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Christopher Douglas", "labels": [], "created": "2006-06-29T01:01:05.000+0000", "updated": "2009-07-08T16:51:50.000+0000", "description": "We'd like a raw input/output format for Map/Reduce that allows jobs to get the raw bytes for keys/values. This would allow things like the IdentityMap to be much faster because the values would not be decompressed/compressed or serialized/deserialized.", "comments": ["This was fixed as HADOOP-2923 and HADOOP-3460."], "derived": {"summary": "We'd like a raw input/output format for Map/Reduce that allows jobs to get the raw bytes for keys/values. This would allow things like the IdentityMap to be much faster because the values would not be decompressed/compressed or serialized/deserialized.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Raw SequenceFile Input/Output formats - We'd like a raw input/output format for Map/Reduce that allows jobs to get the raw bytes for keys/values. This would allow things like the IdentityMap to be much faster because the values would not be decompressed/compressed or serialized/deserialized."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was fixed as HADOOP-2923 and HADOOP-3460."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-331", "title": "map outputs should be written to a single output file with an index", "status": "Closed", "priority": "Major", "reporter": "Eric Baldeschwieler", "assignee": "Devaraj Das", "labels": [], "created": "2006-06-29T03:50:27.000+0000", "updated": "2009-07-08T16:51:50.000+0000", "description": "The current strategy of writing a file per target map is consuming a lot of unused buffer space (causing out of memory crashes) and puts a lot of burden on the FS (many opens, inodes used, etc).  \n\nI propose that we write a single file containing all output and also write an index file IDing which byte range in the file goes to each reduce.  This will remove the issue of buffer waste, address scaling issues with number of open files and generally set us up better for scaling.  It will also have advantages with very small inputs, since the buffer cache will reduce the number of seeks needed and the data serving node can open a single file and just keep it open rather than needing to do directory and open ops on every request.\n\nThe only issue I see is that in cases where the task output is substantiallyu larger than its input, we may need to spill multiple times.  In this case, we can do a merge after all spills are complete (or during the final spill).\n", "comments": ["\nSounds like a great direction! Could you clarify a little bit? \n\nWould you have one map output file per mapper? Or one map output file per mapping-node (wherein the many mappers that run on an individual node will coalesce their outputs into a single file)?\n\nHow many ranges will each reducer have to read back from the file? one? one per 100 megabytes of map output? one per combiner flush?\n\nWhat's a spill?\n\nHow does this fit with the compression proposal? Will there be a separate compression context per reducer?", "\n.. and further, when considering the plumbing for transferring this data to the reducer, it would be great if the mechanism allows the reducer to receive this data as it is generated by the mapper. in this way, the sorter could begin sorting immediately, and the map output data would only come back off disk in the case of a second execution of a failed reducer (since the first transfer would happen \"for free\" out of the file cache)\n\nThis wouldnt be necessary in the first version, but it would be good if this were considered in the design.\n\n", "I think we should do this in an incremental fashion, so...\n\nEach map task would continue to output its own state only.  Although centralizing this per node is an interesting idea.  I can see how this might be a real benefit in some cases.  Perhaps you should file this enhancement idea.\n\nDitto with the transmition to other nodes.  Interesting, but complicated idea.  Maybe you should file it.  Think that can be taken up at a later date.  Although feedback on how you would enhance this change to support such later work welcome.\n\nA spill is dumping the buffered output to disk when we accumulate enough info.  Yes, something like a 100mB buffer seems right.  (configurable possibly)\n\nI think the goal should be that each reduce only reads a single range.  That will keep the client code simple an will keep us from thrashing as we scale.  This may require some thought, since if you have a small number of reduce tasks, reading from multipule ranges may prove more seek efficient than doing the merge.\n\nIf we do block compression for intermediates, you would need to align those to reduce targets, but I don't think we should try to do that in the first version of this.  Especially given that this data will not be sorted the return to block compression may not be that great.  (yes, I can construct counter examples, but let's deal with this as a seperate project).  Checksums also need to be target aligned.", "\nsounds great. the simpler the better.\n\n+1", "\none more suggestion (plea): could the map-output-spill-extraction take place on the mapper side (perhaps in the http server), such that the reducer be able to retreive fully-formed map outputs as currently?\n\nincidently, we have individual map inputs that are 5-10GB in size, which are compressed so they cant be split, which would yield potentially 50-100 spills per mapper. it would be nice if the reducer (or whatever process is fetching the files) didnt have to do that reassembly, but rather that it happened on the mapper side.", "Instead of one output, what about n outputs, where 1 <= n < # reducers? n can be configurable and is set to a number that can avoid FS thrashing. Each output file contains a disjoint set of partitions so that reducer in need of a partition will later access only one file. Increased locality.\n\nGiven we are thinking of sorting map outputs,  this also allows us to avoid global sorting. i.e. sorting happens on one output file at a time that has only small set of data.", "Don't know that this would have a positive impact, but if this eased implementation in some way I don't yet understand, it would meet my requirements.", "Looking at generating a single map output file per map for now.\nThe plan is to do the following:\nDefine a class called PartKey that will contain two fields - partition number (int) and the actual key (WritableComparable). As we are mapping, the PartKeys and the associated values are written to a buffer. The buffer has a fixed size (configurable via map.output.buffer.size) of 128M and this buffer, when full, is sorted and spilled to disk. We may end up having a couple of these spilled buffers. We do a merge at the end. The sorting takes into account the partition number. Also, the merge emits information about offsets where a particular partition resides in the merged file. The copying phase of reduce strips the partion information contained in the PartKey and feeds the actual map-generated key to the reducer.\nMakes sense?", "Just have to weigh in, as I always do, with the full-disk and multi-disk viewpoint. It would be nice if, either there's awareness of the disk filling up (set spill size down dynamically if disk free space is less than the default spill size), or if, instead of a single output, there was one output written per output directory. Of course, separate spills should be distributed across different output dirs in some way as they are now.\n\nFor huge jobs, every bit of space counts, and avoiding failure through simple means can have a big impact on throughput.", "re: devaraj\n\nI like the approach.  One refinement suggested below:\n\nI don't think you want to store the partkeys inline.  That requires more code change and an on disk format changes and wasted bytes to disk and over the wire.  I think you spill serialized key/values with a side file that maps each partition to a start offset.\n\nIn RAM you spill serialized key/value pairs to your buffer and also keep an array/vector (apply appropriate java class here) of (partition,offset to key).  You can then quicksort the array and spill.  You want to be sure to be able to apply a block compressor to each partition as spilled.  This will be very efficient and simple. So record the compressed lengths (kimoon suggested this on another thread).\n\nMerging would go as you outline.  You could read one line of each sidefile and then merge the next partition from each, so the merge would only consider the keys.  Since it would be per partition.\n\nYou need the sidefile to support efficient access for the reduce readers anyway.\n\n---\nre: brian's comments\n\nI think we should keep maps simple and focus this effort on reduces, which deal with much larger size.\n\nThat said, a corner case with HUGE maps should have a reasonable outcome. I think we need a stripped file abstraction to deal with these cases, where outputs are placed in medium HDFS sized blocks on whichever disk makes the most sense.  This same approach would probably be more used on the reduce side.\n\nBut I think this should come as a second project, rather than burdening this work with it.\nAnyone want to file a bug on it?", "See also HADOOP-603", "It might be useful to have the 'buffer' of key/value pairs parititioned into R buckets, one for each reduce. Then when spilling, one could sort each bucket and reduce the number of comparisons and swaps done considerably. Alternatively, when spilling you could do a couple of linear passes over the buffer and bucket keys by reduce and then sort each bucket.", "Yeah.  Just keeping an array of offset vectors per partition would be simpler.\n\nJim: Don't see a problem with your suggestion (HADOOP-603), but I don't think the two projects relate.  These files will be broken into partitions.  Fitting that into a MapFile might be a bit of a stretch. \n", "> sort each bucket and reduce the number of comparisons and swaps done considerably\n\nIn memory, we only need to sort pointers to buffered items, so there only needs to be a single buffer with the serialized items.  Still, I agree that we can minimize comparisons & swaps by bucketing these pointers, then sorting each buffer prior to dumping the data it points to.  Is that what you meant, Sameer?\n\nEach spill can result in output of the same format as the final output: a <key,value>* file sorted by <part,key> and a <part,offset>* file that indicates the start of each part.  So if there's only one spill then no merging is required.  Unfortunately, since 'part' is implicit, we cannot use the existing SequenceFile merge code.\n", "I would ideally like to use as much of the existing code/tools as possible. So, here is another proposal:\n1) Generate a master index as the buffers are filled with key/value pairs. The master index is an array indexed by partition number and each entry in the array contain values of the form <filename, keyoffset, valoffset>*.  The filename refers to the filename where the containing buffer was spilled. Think of this as each element in the array is an array by itself.\n2) Filled buffers are spilled and NO sorting is done when a spill happens.\n3) At the end, we have a master index and we go through that index by index ( partition by partition). Since we know the filename and the key/value offsets, we can access the key/value data with ease. We create a temporary fixed-sized in-memory buffer for each array element and if we can accomodate all the key/value pairs for the particular array element in that buffer, well and good; we sort that in-memory buffer and append the sorted data to a final output file. If we cannot fit in everything into the buffer, we spill and sort the spills after we spill all the key/value pairs.\n4) While we do (3), we can trivially generate the final index file.\n5) At the end of the step (3), we have a big file with sorted data (by partition) and a single index file. \nThe problems of implicit partition numbers are not there with this approach. Also, we can do parallel sorts (like two at a time or something) of the array elements and generate multiple files and just concatenate the sorted data at the end of the sorts to that single output file. The negative with this approach is that we need another temp buffer (but it may be required in the other approaches also). Think the master index file won't be very big (it will be roughly of the order of number of records processed by a single map).\nThoughts ?", "> Think the master index file won't be very big (it will be roughly of the order of \n> number of records processed by a single map). \nActually meant to say \"master index array/buffer\".", "Unless I misunderstand, your step (3) involves a lot of random disk accesses, something we must avoid.  Seek time dominates for accesses of less than 100k or so.  Optimal performance will only be achieved if all disk operations are reads and writes of larger chunks.\n\nSorting buffers before they are written is also important, since it eliminates a sort pass.  Most map outputs can be sorted in just a few passes, often only one, so each pass is significant.", "I think that the buffer of the map output should be stored as:\n\nclass KeyByteOffset {\n  WritableComparable key;\n  int offset;      // offset of value in a buffer of bytes\n  int length;     // length of value in buffer of bytes\n}\n\nList<KeyByteOffset>[NumReduces]\n\nSo you have a List of records for each reduce.\n\nWhen you fill up on #records or #value bytes, sort and spill. (Combine first, if there is a combiner.)\n\nThe spilled keys should be PartKey's from above so that the standard merge tools on sequence files work. (If the PartKey uses the vint representation, each key will only expand by 1-2 bytes on disk.) \n\nI think we should have a new variant of the sequence file merge that returns an iterator over key/value pairs, which will be useful in reduce also. \n\nWith that, we can easily merge and produce the final partition index & sequence file to local disk.", "Yes, disk accesses bothers me too. So do you think it is better to sort before spill (step (2)) & then updating the master index array to contain the <filename, start-key-val-offset, end-key-val-offset> makes sense? If we do this, then we will have bigger chunks that sort will operate on at the end (basically we will merge the chunks).", "and clearly the servlet opens the file, transmits the sequence file header, and the range of bytes from the index.", "> we should have a new variant of the sequence file merge that returns an iterator over key/value pairs\n\n+1\n\nOwen, shouldn't KeyByteOffset also include the partition #?\n\nUnder this scheme, each spill will result in a separate <PartKey,value> sequence file, right?  And probably each such sequence file should be accompanied by a <part,position> index file.  That way, if there's only one spill, it can be used directly.  Note that block compression complicates random access.  Perhaps we should add a SequenceFile#Writer.flush() method that, when block-compression is used, resets the compressor and emits a sync, creating a seekable offset.", "Before the spill, the partition is implicit in which list the KeyByteOffset is on, so I don't think you need to include it explictly.\n\nWe may want to handle the (standard) 1 spill case separately, so that we can make the final output a sequence file over Key,Value instead of PartKey,Value. Either way will work...\n\n", "Doug:\n\n> Still, I agree that we can minimize comparisons & swaps by bucketing these pointers, then sorting each \n> buffer prior to dumping the data it points to.\n\nYes, this is what I meant.\n---------------------\n\nDon't think KeyByteOffset needs a partition# since partition# is the same as the index of the list in List<KeyByteOffset>[NumReduces] in which the key is stored.\n\n> Under this scheme, each spill will result in a separate <PartKey,value> sequence file, right? And probably \n> each such sequence file should be accompanied by a <part,position> index file. That way, if there's only \n> one spill, it can be used directly. Note that block compression complicates random access. Perhaps we \n> should add a SequenceFile#Writer.flush() method that, when block-compression is used, resets the \n> compressor and emits a sync, creating a seekable offset.\n\n+1\n\nAnother minor optimization could be to only include the partition# in the first key in each partition, populating all succeeding keys with 0.\n", "Owen:\n> the partition is implicit in which list the KeyByteOffset is on\n\nAh, I somehow missed the lists.  Sorry!  You're right, no part is needed in KeyByteOffset.\n\nSameer:\n> only include the partition# in the first key in each partition\n\nBut then we cannot use SequenceFile's merger, since keys wouldn't be independently comparable, right?\n\n", "A modified proposal (after Doug's comments)\n   -  Keep an array (length = # of partitions) whose elements will be lists containing elements of the form <spill#, start-key-val-offset, end-key-val-offset>*. This is obtained after the sort and before the spill of the buffer. The spill# helps in locating the section of the sorted partition data within a spill.\n  - Merge goes over this array element by element and for each element, it looks at the contained list elements and creates a file appending the sections one by one. Then does mergesort with just one file as input with the output set to the final output file (opened in append mode).  An index file is updated to contain <part#, start-offset, compressedlength>\n  - At the end of the above, we have the final output file and the index file.\nThe negative with this approach is that it requires trips to disk for reading/writing the sections before merge. So if we had 10 sections for a particular partition, we would have to read 10 chunks from the sorted spills and write them to disk as a single file. The plus is that the key format doesn't need to change at all (so minimum code change) and the number of comparisons during merge are reduced because we merge chunks of a single partition at a time.\n\nWe can have the interator over keys/values as suggested by Owen and the block compression optimization suggested by Doug. Although the iterator won't be required for the map outputs if we adopt the approach outlined above, but will be required for the reduces, right Owen? ", "Doug:\n> But then we cannot use SequenceFile's merger, since keys wouldn't be independently comparable, right? \n\nYes, you're right, they wouldn't. This optimization is probably not worth the trouble.\n\n-----\nTo summarize, this is what we appear to have ended up with.\n\n- A couple of key variants\nclass KeyByteOffset {\n  WritableComparable key;\n  int offset;\n  int length;\n}\n\nclass PartKey {\n  int partition;\n  WritableComparable key;\n}\n\n- In RAM data structures\nList<KeyByteOffset>[NumReduces]  keyLists; // one list of keys per reduce\nbyte[] valueBuffer; // serialized values are appended to this buffer\n\n- For each <key, value> pair.\nDetermine partition for the key. Append the key to keyLists[partition], append the value to valueBuffer.\nIf #records is greater than #maxRecords or #valueBytes is greater than #maxValueBytes\nOR\nWe have no more records to process.\nSPILL - The spill is a block compressed sequence file of <PartKey, value> pairs, with a <part, offset> index.   \nCompressed blocks in this file must not span partitions. \n\n- At the end, if #numSpills > 1, merge spills\n\n", "+1 to Sameer's summary.\n\nAlso note that two changes to SequenceFile are required:\n1. Expose an iterator-like API to merge, where subsequent next key/value pairs can be extracted from the merge.  This will permit writing an index when merging, and will help optimize reduce.\n2. Add a Writer.sync() method that ends a compression block and creates a sync point.\n", "A couple of other points/questions. In the summary above,\n\nSPILL means:\n- Sort\n- Combine if a Combiner is specified\n- Write to disk\n\nWhat sorting algorithm should we use? Is Collections.sort good enough? It claims to be comparable to a highly optimized quicksort.  Or should we invest in a quicksort implementation.\n\nIs the generic SequenceFile heap merge going to do the right thing when merging spills? Even in the final merge we don't want compressed blocks to span partition boundaries. Or are we going to have to special case it for PartKeys?", "Sounds good.  I think we are converging on something.\n\nA couple of points:\n\n1) When we spill, we don't need to keep any record of the spilled data.  Devaraj maintained operating on these arrays after spills.  We should not do that.  Everything should be cleared out.  (Except possibly the index of partitions to file offsets).\n\n2) I like the idea of extending the block compressed sequence file to directly support flushes at partition boundaries and merges of ranges.  This will be reused in reduce as doug observed.\n\n3) We should not spill based on a number of records.  Don't see any value in that.  We should just spill based on RAM used.\n\n4) Related, we need to track total RAM used, not just for values, but also for keys and arrays.  We don't want the system to blow up in the degenerate cases of huge keys or null values and many, many keys.\n", "A related feature.  Compressing values individually as we serialize (before sort/spill) should be an option.  Even if we are using block compression on output.  Sounds inefficient, but if it reduces the number of spills 3x, this saves total disk passes and compression/decompression passes.  Also clearly good if we choose to just compress by values, not block in the sequence files, since we could then just pass the compressed values about.\n", "> we need to track total RAM used, not just for values, but also for keys and arrays\n\nThen we should change KeyByteOffset to something like:\n\nclass BufferedEntry {\n  int start;\n  int keyLength;\n  int valueLength;\n}\n\nThen the sort code can use WritableComparator.compare(byte[] k1, int s1, int l1, byte[] k2, int s2, int l2).  Most WritableComparables have an optimized implementation of this method.\n\nTo account for arrays we could just add 20 bytes per entry (BufferedEntry+pointer to it), which is probably close enough.", "> Compressing values individually as we serialize (before sort/spill) should be an option.\n\nSounds like an enhancement to be added later.  We should first use whatever compression is specified for map output.\n", "Number of records does impact memory usage precisely because of the KeyByteOffset. I've seen cases where there are very small records (int,int) that use the majority of memory in the objects and their overhead. Since it can become a dominant factor, it needs to be monitored.\n\nI want to keep the Key objects so that performance doesn't suck if the user doesn't define raw comparators. Key objects are typically small and doing the base sort on them will be much faster if we keep them as objects.", "> keep the Key objects so that performance doesn't suck if the user doesn't define raw comparators\n\nNote that merge performance will also suffer when the user doesn't define raw comparators, but not as badly as sort performance.\n\nOne other advantage to using the raw, binary comparator is that we could then share the sort logic with SequenceFile.  It currently has a sort algorithm that, without too much work, could be exposed as something like:\n\npublic static sort(int[] pointers, int[] starts, int[] lengths, byte[] data, WritableComparator order);\n\nThe pointers array indicates which values in starts and lengths should be used.  It is permuted, so that, after sorting, the key at data[starts[pointers[i]]] is always less than data[starts[pointers[i+1]]].  To use this we'd dispense with the KeyByteOffset class altogether and simply keep something like:\n\nbyte[] data;  // all buffered keys and values\nint[] starts;  // the start of each pair\nint[] keyLengths; // the length of each key\n// values run from starts[i]+keyLenghts[i] through starts[i+1].\nList<int[]> partPointers;\n\nThen we could sort each part pointer array and then write it.  Each buffered entry would have a fixed 12-byte overhead, so memory accounting could be exact.", "Ok, I've been convinced that we should start with BufferedEntry as a baseline. I think the overhead/record should be 40 rather than 20 for the size metric, but of course we should use a constant for it anyways:\n\nfinal int BUFFERED_KEY_VALUE_OVERHEAD = 40;\n\nAnd I think BufferedKeyValue is a little more informative than BufferedEntry.\n\n// condition for a spill would be\nbuffer.size() + BUFFERED_KEY_VALUE_OVERHEAD * numKeyValues > conf.getMapOutputBufferSize() \n\nThe records would be stored in an array of ArrayLists:\n\nList<BufferedEntry>[numReduces]\n\nSpills would be written as SequenceFile<PartKey<Key>, Value>\n\nThe spills would be merged (using the iterator output form of merge) to write:\n\nSequenceFile<Key,Value> and partition index\n\nIf there have been no spills, you just write the SequenceFile<Key,Value> and partition index from memory.\n\nIt will give a fixed usage of memory, a single dump to disk in the common case, and a reasonable behavior for large cases.\n\n", ">public static sort(int[] pointers, int[] starts, int[] lengths, byte[] data, \n>WritableComparator order); \n\n+1\n\n>The pointers array indicates which values in starts and lengths should be used. It \n>is permuted, so that, after sorting, the key at data[starts[pointers[i]]] is always less \n>than data[starts[pointers[i+1]]]. To use this we'd dispense with the KeyByteOffset \n>class altogether and simply keep something like: \n\n+1\n\n", "Attached is a txt doc that summarizes the discussions so far as a design doc.", "Attaching a revised design spec.", "This is the first version of the patch for review. While testing is in progress, I thought the review can go on in parallel. The main changes are:\n1) The reduces don't do sort anymore. One change in ReduceTask.java is to do with that - \"sort\" string is replaced by \"merge\" in a couple of places.\n2) The other change in ReduceTask.java is that the class ValuesIterator class has been upgraded to package private static class. This is because, I am using the functionality of this class in Combiner also. Thus, I have two new classes extending from ValuesIterator: ReduceValuesIterator and CombineValuesIterator defined in ReduceTask.java and MapTask.java respectively.\n3) In MapTask.java, I have a couple of other new classes:\n   3.1 MapOutputBuffer: this class does all the work to do with maintaining the buffer, sort, combine and spill. It implements the OutputCollector interface and a new interface called OutputWriter. The intent of having the second interface is for other classes to invoke the methods for writing the partition boundaries (syncs) and doing the final merge of the spills.\n4) The sort has been refactored as an interface SorterBase.java (that extends from RawKeyValueIterator interface). It defines a couple of methods that the MapOutputBuffer invokes during the map phase. The reason for extending from RawKeyValueIterator is to allow easy iteration over the sorted data (during combining/spilling). Also, during iteration the OutputWriter in MapTask.java is notified of partition changes so that it can do things like ending a block boundary by writing a sync (for block compression), etc.\n5) Implemented the SorterBase interface as a class BasicTypeSorterBase that implements the methods and the relevant datastructures as arrays of primitive int (as opposed to object arrays which could be implemented by another class implementing the SorterBase interface). The intent of BasicTypeSorterBase class is to serve all implementations of sort algos that rely on primitive arrays (of offsets/lengths/etc. to a read-only buffer). The sort() method itself is empty here.\n6) MergeSorter.java extends from BasicTypeSorterBase.java and implements the sort method. Basically, it accesses the data structures that BasicTypeSorterBase created and sets up the input arguments for hadoop.util.MergeSort (that actually implements the core of the MergeSort algo). One could have a QuickSorter.java and a corresponding hadoop.util.QuickSort.java and so on. The bridge between the framework and the sort algo is the Comparator that is passed by the framework to the hadoop.util.MergeSort's constructor.\n7) What sort algo to use in the framework can be set by map.sort.class which defaults to MergeSorter.class (for now).\n8) TaskTracker.java has the necessary jsp changes to serve the map output data to the reduces.\n9) SequenceFile.java has two new public APIs: createWriter and sync (this createWriter has slightly different args). Also refactored sort to use the new mergesort from hadoop.util.\n10) A new class hadoop.util.ListOfArrays has been introduced to maintain lists of primitive arrays and handle things like growing of arrays internally.", "Change the name of the class SorterBase to BufferSorter\n\nInstead of \"initialize(JobConf)\"  in the file BasicTypeSorterBase.java, it should implement JobConfigurable.\n\nThe BasicTypeSorterBase should be an abstract class since it itself \ndoesn't implement the sort algorithm\n\nThe sorter should not be concerned about partitions (makes the code \nhard to read). So have one sorter per partition instead of having one sorter \nfor all partitions. In the case, where there is one sorter per partition, we \ndon't require the callback mechanism (the OutputWriter interface in \nMapTask.java). Also, the ListOfArrays class is not required if this is done.\n\nsort method in the SorterBase interface should return a RawKeyValueIterator\nobject rather than deriving from it import DataInputBuffer is not used in\nBasicTypeSorterBase.java \n\nSequenceFile.java has some whitespace-only changes\n\nUse readLong/writeLong instead of LongWritables for reading/writing longs\n\nRemove the empty finally block from MapTask.run\n\nSynchronize the collect methods in MapTask.java (move the 'synchronized'\nfrom the method declaration to inside the method as 'synchronized(this)' )\n\nClose the combiner after iterating a sorted partition while spilling to disk.\nThis is to ensure that in case of streaming, where the combiner is a separate\nprocess, it processes the input key/vals and writes the output key/vals before\nthe spill file is updated with the partition information.\n\nThe occurrences of \"sort\" strings in ReduceTask.java may remain; they need not\nbe replaced with \"merge\" strings (although merge is the more accurate word, but\neffectively we are sorting. Also, there may be code somewhere that relies on\nstatus strings being \"sort\").\n\nIn TestMapRed.java, the check whether map output a compressed file or not is\ncommented out. We need to do the compression check.", "Thanks Owen for the detailed review. Attached is the new patch.", "+1, http://issues.apache.org/jira/secure/attachment/12346642/331.patch applied and successfully tested against trunk revision r483294", "+1", "I just committed this, although I had to increase the heap size used for unit tests or else TestDFSIO would get an out-of-memory error.  It's now 128MB.\n\nThanks, Devaraj!", "Integrated in Hadoop-Nightly #91 (See http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/91/)"], "derived": {"summary": "The current strategy of writing a file per target map is consuming a lot of unused buffer space (causing out of memory crashes) and puts a lot of burden on the FS (many opens, inodes used, etc). I propose that we write a single file containing all output and also write an index file IDing which byte range in the file goes to each reduce.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "map outputs should be written to a single output file with an index - The current strategy of writing a file per target map is consuming a lot of unused buffer space (causing out of memory crashes) and puts a lot of burden on the FS (many opens, inodes used, etc). I propose that we write a single file containing all output and also write an index file IDing which byte range in the file goes to each reduce."}, {"q": "What updates or decisions were made in the discussion?", "a": "Integrated in Hadoop-Nightly #91 (See http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/91/)"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-332", "title": "Implement remote replication of dfs namespace images and transaction logs", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Konstantin Shvachko", "labels": [], "created": "2006-06-29T13:28:03.000+0000", "updated": "2009-07-08T16:41:56.000+0000", "description": "The namespace information (image and transaction logs) needs to be replicated on hosts other than the namenode to prevent data loss if a namenode crashes. In the short term we will add a new protocol to the datanodes to receive and store the namespace information.\n\nIn the long term, it would be nice to have read-only namenodes that could receive the information and serve it up for users that want to read data, but do not need to write to the namespace.", "comments": ["should this issue be closed now that we have a patch committed for HADOOP-90 ?", "Duplicate of HADOOP-90"], "derived": {"summary": "The namespace information (image and transaction logs) needs to be replicated on hosts other than the namenode to prevent data loss if a namenode crashes. In the short term we will add a new protocol to the datanodes to receive and store the namespace information.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Implement remote replication of dfs namespace images and transaction logs - The namespace information (image and transaction logs) needs to be replicated on hosts other than the namenode to prevent data loss if a namenode crashes. In the short term we will add a new protocol to the datanodes to receive and store the namespace information."}, {"q": "What updates or decisions were made in the discussion?", "a": "Duplicate of HADOOP-90"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-333", "title": "we should have some checks that the sort benchmark generates correct outputs", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Arun Murthy", "labels": [], "created": "2006-06-30T00:20:39.000+0000", "updated": "2009-07-08T16:51:55.000+0000", "description": "We should implement some checks of the input versus output of the sort benchmark to get some correctness guarantees:\n\n1. the number of records\n2. the number of bytes\n3. the output records are in fact sorted\n4. the xor of the md5 of each record's key/value pair", "comments": ["Another reasonable check would be to join the input and output directories:\n\ninput dir: key, value -> (key,value), 1\noutput dir: key, value -> (key, value), 2\n\nand ensure that you always have an equal number of 1's and 2's for each key,value pair.", "Here is a set of two utilities to validate the map-reduce framework's 'sort':\na) Checks the records in both the input and the output to the sort b/m and ensures they are consistent.\nb) Checks to ensure that the records in each split are sorted correctly.\n", "I'd still like to see some checksums, bytes, and record information about the input and output directories compared. Otherwise a problem that changed all of the data to 0's would be counted as fine.\n", "Arun, \n\nDo you have the input generator for the patch? \n", "Arun is probably asleep, since he is in Bangalore, but the input to this patch is the randomwriter and sort example programs.\n\nLook at http://wiki.apache.org/lucene-hadoop/Sort", "Here is another patch incorporating Owen's f/b:\na) Validates the no. of bytes and records in sort's input & output.\nb) Validates the xor of the md5's of each key/value pair\nc) Ensures same key/value is present in both input and output.\n\nRun this as:\nhadoop jar hadoop-0.11.2-dev-examples.jar sortvalidator -sortInput /randomwriter/input -sortOutput /randomwriter/output \n", "I forgot to ask for one more check: by looking at the part-[0-9]* filenames, it would be good to have the tool use the default partition function to make sure each key was put into the right partition.", "Incorporated the check for partitioning and also ensures that RecordStatsChecker gets full (sort input/output) files so as to check that full-output-splits are sorted right.", "Moved SortValidator to src/test and it can now be run via the hadoop-test.jar:\n\n$ hadoop jar hadoop-0.11.2-dev-test.jar testmapredsort -sortInput /randomwriter/input -sortOutput /randomwriter/output\n", "+1, because http://issues.apache.org/jira/secure/attachment/12351331/HADOOP-333_20070216_4.patch applied and successfully tested against trunk revision r508345.", "I just committed this.  Thanks, Arun!"], "derived": {"summary": "We should implement some checks of the input versus output of the sort benchmark to get some correctness guarantees:\n\n1. the number of records\n2.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "we should have some checks that the sort benchmark generates correct outputs - We should implement some checks of the input versus output of the sort benchmark to get some correctness guarantees:\n\n1. the number of records\n2."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Arun!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-334", "title": "Redesign the dfs namespace datastructures to be copy on write", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Konstantin Shvachko", "labels": [], "created": "2006-06-30T00:26:50.000+0000", "updated": "2009-07-08T16:41:57.000+0000", "description": "The namespace datastructures should be copy on write so that the namespace does not need to be completely locked down from user changes while the checkpoint is being made.", "comments": ["I was thinking of the different ways to checkpoint the fsimage.\n\nThe simplest and dumb way is to make the checkpointing process acquire the global lock, flush the entire\ncontents to persistent store and then release the lock. This causes all currently running operations to block and is sub-optimal.\n\nAnother simple way is to have a global lock. At the start of the checkpointing process, we make a clone (ie. copy) of the entire FSImage in memory and then release the global lock. The  clone is then lazily written to persistent store. The disadvantage of this approach is that we need lots of memory.\n\nIf we adopt a copy-on-write approach, we might get an optimal solution. One that does not require loads of memory neither does it impact concurrent transactions.\n\nEach node in the tree has a clone member. Each node starts with the clone member set to null.  There is a single global lock associated with the entire tree. No other fine-grain locks are necessary.\n\nAll node modification request holds the global lock (just like in the current code).  Suppose a node modification request finds a node that is not yet cloned. It clones the node and starts using the clone data. If the node is already cloned, it uses the clone data\\and ignores the original data of that node.\n\nThe checkpointing thread traverses the tree and starts writing nodes to persistent store.  No locks are held during this flush to persistent store. The checkpointing thread does not mess around with any clone data (if present). At the end of the checkpointing process, the global lock is reacquired, all data from the clones are copied back to the original nodes, the clone members are nulled and the global lock released.\n\nIn this approach, we keep the simiplicity associated with a single global lock while optimising on memory usage.", "The simplest copy-on-write approach would:\n\n1. Keep a pointer to the root of the tree.\n2. For any change, copy all structures on the path between the root and the change, and reset the root.\n3. To checkpoint, traverse the tree from the root.\n\nSynchronization would be required around changes (single writer, multiple reader).\n\nAllocating something proportional to the path length per change is probably fast enough.  Do we have reason to think this would be too slow?", "Copy on write helps, but the global lock needs to be acquired at the end of the checkpointing process nevertheless. This still has the effect of locking clients out of the namespace while the entire namespace is traversed and the clone pointers are reset.\n\nInstead of copy on write, how about changing the locking model so that for any change:\n1. Acquire read locks on all structures between the root and the change, acquire a write lock on the changed\nnode.\n2. To checkpoint, traverse the namespace acquiring read locks on the path between the root and the node being checkpointed. Serialize each node to a new image file on disk.\n\nThis way we never lock down the whole tree, for any operation. At the start of the checkpointing process, a new edits file is created. Edits that occur while the checkpoint is in progress are sent to the new file. This implies that there will be some overlap between the checkpointed image and the edits file, but this is ok. We require that the union of the image and the edits give us the current state of the namespace but the two do not have to be disjoint. ", "Sameer, you missed the point. You don't need any locks for checkpointing with copy on write other than a quick read lock on the root while you switch to a new log. Once you have started the new log, you can traverse from the old root and see the state at the transition. So the lock is held while you open a single file.", "To do periodic checkpointing we need none of these fancy locking changes. I am putting forward a proposal soon, that would do periodic checkpointing without changing the locking model.", "In Response to Sameer's comments:\n---------------------------------------------------\nRegarding copy-on-write approach, we do not need to traverse the entire namespace to reset the clone pointers at the end of the checkpointing process. We can keep a lookaside list that contains all the nodes that have a clone pointer. But we still have to acquire the global lock at the end of the checkpointing process, traverse this lookaside list of cloned-nodes, and then null-them.\n\nI like the generalized scheme of fine-grain locks (instead of a global lock) while traversing the namespace. It is more efficient once implemented correctly. There are quite a few tricks about lock-hierarchy that one has to play for \"renames\". But it can be done.\n\nThe one thing that I am not clear about is whether we get correct semantics if the imagefile and the editfile overlap.  If x, y and z are three transactions, are you saying that\n   \t\t\tx + y + z is equilvalent to x + y + y +z where y is a single transaction that res ides in the image file as well as the edits file. Are you proposing something like a global transaction number to identify duplicate transactions?\n\nIn response to Doug/Owen' comments:\n-----------------------------------------------------\nI was saying precisely the same thing. The only difference in my proposal was that we do not need to clone all the nodes from a node to its root, we could clone only the node that is being modified.", "> we do not need to clone all the nodes from a node to its root, we could clone only the node that is being modified\n\nI wonder whether this optimization is worthwhile.  It makes things more stateful, for example, we can't have two separate snapshots live at once.  However I don't know that we'll ever need to...", "Dhruba:\n\nYes, we would get correct semantics if the image and edits overlap. While not strictly necessary, we could also add timestamps to entries in the image and edits for validation. Suppose the edits file has the following transactions:\n- x: add file /a/b/c, time=t0\n- y: remove file /b/c, time=t1\n- z: rename directory /a --> /d, time=t2\n\nwhich all overlap with the image. In this case, when the image and edits are being merged the following would happen for each transaction:\n\nx: the image already has file /a/b/c, assert that directory /a/b has timestamp t3 >= t0, we're consistent with the edits, move on\ny: the image does not have the file /b/c, assert that directory /b has timestamp t4 >= t1, we're consistent with the image, move on\nz: the image already has the directory /d, assert that directory /d has timestamp t5 >= t2, we're consistent with the edits, move on\n\nthis sort of logic can be followed for all transactions. So we don't need a transaction number, a timestamp will suffice.\n\n\n", "Correction, in the above comment, transaction 'x' would be handled as:\n\nx: the image does not have directory /a, root has timestamp t3 >= t0, move on", "\nWhy would we have to deal with multiple edit entries in this case? I thought the following operation would be done under globalLock:\n\nat the end of checkpointing we : close the new image file, remove (or move) old image file and old edit logs.\n\nThis is just for my understanding.. I know Milind is working on different proposal.\n", "Could there be a problem if the new file by the same name 'a' gets re-created. e.g.\n\nx: add file /a/b/c, time=t0\n- y: remove file /b/c, time=t1\n- z: rename directory /a --> /d, time=t2 \n+m: create a new directory /a time=t3\n\nSuppose the above transactions were already part of the Image. They also are in the Edits. Now, when the edits are applied, what will happen to transaction x?\n\nOn a different note, isn't it safer to use sequencenumbers rather than timestamps because \n1. there could be two transactions at precisely the same timestamp\n2. an user can rollback the time on a machine, thus causing havoc with DFS metadata", "Yes, a sequence number is more robust in the face of time rollbacks. W.r.t the comment on transaction 'm' and how it affects transaction 'x', if we say that:\n\nA transaction in the edits log applies to the namespace iff:\na. the sequence number on the transaction is higher than the sequence number of the node that it affects OR\nb. if the node does not exist in the namespace the sequence number of the transaction is more recent than that of the closest existing ancestor\n\nthen it should work.", "having to ignore entries in the edits file because of overlap with the image file diminishes the dfs debuggability and robustness. Suddently there's no clear notion of a bad entry in an edits file, opening the door to all sorts of nonsense.\nWhile not incorrect in any way, this approach would IMO make the dfs more vulnerable to problems that are inherently hard to debug.", "I'm not sure that overlapping diminishes debuggability. With a sequence number per node in the image and edits we can completely validate an edits log. There are 3 kinds of node in the image and edits:\n\na. Those that occur in the image but not in the edits. These must have a sequence number lower than the lowest sequence number in the edits.\nb. Those that occur in both, the \"overlapping\" ones. For each such node, the sequence number in the image must match the sequence number of the last reference to the node in the edits\nc. Those that occur in the edits but not in the image. These must occur in rename or delete transactions only.\n\nThis validation is indirect, but I don't think it's incorrect.", "The copy-on-write of namenode data structures is not needed if we do periodic cehckpointing on Secondary namenode. The Secondary Namenode issue is being tracked through hadoop-227."], "derived": {"summary": "The namespace datastructures should be copy on write so that the namespace does not need to be completely locked down from user changes while the checkpoint is being made.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Redesign the dfs namespace datastructures to be copy on write - The namespace datastructures should be copy on write so that the namespace does not need to be completely locked down from user changes while the checkpoint is being made."}, {"q": "What updates or decisions were made in the discussion?", "a": "The copy-on-write of namenode data structures is not needed if we do periodic cehckpointing on Secondary namenode. The Secondary Namenode issue is being tracked through hadoop-227."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-335", "title": "factor out the namespace image/transaction log writing", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Konstantin Shvachko", "labels": [], "created": "2006-06-30T00:30:14.000+0000", "updated": "2006-08-04T22:22:35.000+0000", "description": "Factor the checkpoint/transaction log handling code out of FSNameSystem into its own class.", "comments": ["I factored out two new classes FSImage and FSEditLog from FSDirectory.\nFSImage maintains the dfs image file and checkpointing.\nFSEditLog is responsible for the edits file and the namespace updates logging.\nThus, FSDirectory has only namespace methods and the INode class.\nThis should simplify implementation of checkpointing and logging\nupgrades intended to improve the fs reliability.\nIt should also localize changes related to the namespace versioning.\n\nThe INode probably also needs to be factored out into a separate class,\nsince the type is already referenced in other classes, and it does not have\nany dependencies on the outer class.", "You mix indentation and style changes with functional changes.  Can you please keep these separate?  If you like, make the style changes, but as a separate patch, so that your patch is easier to evaluate.  It's a good practice to preview your patches and make sure that each change in the patch is required by the issue the patch purports to fix.\n", "Sorry, this probably needed more explanation than I provided.\nThis is a pure refactoring patch. It does not introduce new\nfunctionality, or improve performance, or fix a bug. The idea is to\nmake the code modular, easy to understand, and simplify further\nmodifications. So restyling seems rather appropriate in the case.\nBesides, I restyle only the parts that I change. I find it reasonable\nto stick to the same style at least within the same method.\nI believe this patch does a minimal modification to the existing code\nthat achieves the goal stated by Owen in the initial comment to the issue.\nDoug, I do not know what tools you use to evaluate patches.\nDoes it provide \"ignore white spaces\" option? That usually helps to\ndistinguish real changes from restyling.\nFrom your comment I did not realize is there anything I need to\nchange in my code, please clarify.\n", "- I synchronized the patch with the current trunk,\n- and reversed indentation changes.\n", "I just committed this.  Thanks, Konstantin."], "derived": {"summary": "Factor the checkpoint/transaction log handling code out of FSNameSystem into its own class.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "factor out the namespace image/transaction log writing - Factor the checkpoint/transaction log handling code out of FSNameSystem into its own class."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Konstantin."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-336", "title": "The task tracker should track disk space used, and have a configurable cap", "status": "Closed", "priority": "Major", "reporter": "Eric Baldeschwieler", "assignee": null, "labels": [], "created": "2006-07-01T03:21:29.000+0000", "updated": "2009-07-08T16:51:50.000+0000", "description": "We've been having problems where the task tracker is destabilizing HDFS by filling disks and then releasing them.  Various tasks can also interfere with each other this way.\n\nWe should have a configurable max working space for a task (and allow the setting of lower per task limits).  The task tracker should track use against this limit and terminate a job if it overruns it.", "comments": ["Konstantin mentions that tasks filling disks cause HDFS ioExceptions", "this was fixed by HADOOP-27"], "derived": {"summary": "We've been having problems where the task tracker is destabilizing HDFS by filling disks and then releasing them. Various tasks can also interfere with each other this way.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "The task tracker should track disk space used, and have a configurable cap - We've been having problems where the task tracker is destabilizing HDFS by filling disks and then releasing them. Various tasks can also interfere with each other this way."}, {"q": "What updates or decisions were made in the discussion?", "a": "this was fixed by HADOOP-27"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-337", "title": "DFS files should be appendable", "status": "Closed", "priority": "Major", "reporter": "Runping Qi", "assignee": "Sameer Paranjpye", "labels": [], "created": "2006-07-01T04:19:23.000+0000", "updated": "2009-07-08T16:41:56.000+0000", "description": "\nActually two related issues\n\n1. One should be able to open an existing DFS file, to seek to a position and truncate the rest, and to append starting at the end (or where trancation happens) .\n2. One should be able to read the writen data of a DFS file while other is writing/appending to the file\n", "comments": ["\nI'd suggest a strict prioritization of DFS changes (comments encouraged):\n\n(1) Recoverability (not losing data in the presence of failures)\n(2) Scalability\n(3) Availability (staying up 24/7/365)\n(4) Performance\n(5) Features\n\nWherein features is a _distant_ fifth place, not really in the running.\n\nIts worth noting that the elegant simplicity of HDFS is key to (1), (2), and (3) - we have a lot to gain by avoiding any and all complexity in HDFS.\n\n", "\nI don't mind pritorization and agree reliability/scalability/availability are all important. However, that does not mean we have to ban new features, especially some features that are essential for some applications. For example, the feature of of the current issue is a must for one of our applications.\n ", "\nI agree. Beleive me, I love a new feature as much as the next guy. In fact, my temptation was to request multiple concurrent appenders! :)", "Can you describe more about the application that requires this feature?  I wonder if there might be a reasonable workaround.  Previously I proposed using a directory of files instead of a single file for a related issue.", "We generally support Paul's prioritization.  But we are also going to be moving up the tempo and volume of data we manage in the system, \"A LOT\".  If we find a small list of features to be cost effective investments, we'd like to be able to give them back to the community.\n\nIf it makes sense for us to invest significant man-power to produce a patch for this and can validate that said patch does not destabilize large clusters over a significant period of operation, I'd assume this wouldn't be that controversial, since this is a very natural extension of the API that does seem eventually inevitable (along with multiple appenders, which would be very, very helpful).  I think the thing we need to agree on is the testing criteria for acceptance, so we're not constantly derailing the community.\n\nDo folks agree with the above?  If so, let's put more energy into thinking about how we are going to establish a testing / validation regime that will support innovation, rather than trying to kill innovation for safety's sake.  That way lies madness.  We'll be happy to help staff / fund such a testing policy.\n\n(Anyone want to create an \"enhancement\" to discuss HDFS extension validation?)", "\nIf Doug isnt able to talk you out of it, then you must really need it. So give him a chance, but if its OK with him its probably a good thing to do. We'd love to have it, but my intuition is flashing *complexity* when I consider it. Especially with multiple appenders.  Of course, you guys probably have a super elegant approach in mind. \n\nMy last thought: if that \"other\" elegant distributed replicated block-based filesystem has this feature, its probably valuable. If it doesnt, then its probably superfluous.", "Duplicate of HADOOP-1700"], "derived": {"summary": "Actually two related issues\n\n1. One should be able to open an existing DFS file, to seek to a position and truncate the rest, and to append starting at the end (or where trancation happens).", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "DFS files should be appendable - Actually two related issues\n\n1. One should be able to open an existing DFS file, to seek to a position and truncate the rest, and to append starting at the end (or where trancation happens)."}, {"q": "What updates or decisions were made in the discussion?", "a": "Duplicate of HADOOP-1700"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-338", "title": "the number of maps in the JobConf does not match reality", "status": "Resolved", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-07-01T04:56:44.000+0000", "updated": "2009-07-08T16:51:55.000+0000", "description": "The config value JobConf.getNumMapTasks() returns the hint of how many maps should be run rather than the actual number of input splits that were generated.\n\nThe job tracker should fix the job.xml with the actual number of input splits.", "comments": ["Perhaps a different property should be used for the actual number of tasks allocated.", "It certainly surprised me that conf.getNumMapTasks() didn't return the right number and I _should_ have known better. It seems like if we want to leave access to the desired number of maps, we should change it to getDesiredNumMapTasks() or something closer to the real semantics.\n", "This was fixed by HADOOP-933."], "derived": {"summary": "The config value JobConf. getNumMapTasks() returns the hint of how many maps should be run rather than the actual number of input splits that were generated.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "the number of maps in the JobConf does not match reality - The config value JobConf. getNumMapTasks() returns the hint of how many maps should be run rather than the actual number of input splits that were generated."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was fixed by HADOOP-933."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-339", "title": "making improvements to the jobclients to get information on currenlyl running jobs and the jobqueue", "status": "Closed", "priority": "Minor", "reporter": "Mahadev Konar", "assignee": "Mahadev Konar", "labels": [], "created": "2006-07-01T06:37:33.000+0000", "updated": "2006-08-04T22:22:35.000+0000", "description": "adding a few methods to the joblcient and jobsubmission protocol to support queries to the jobtracker for currently running jobs", "comments": ["This patch adds a another method to the jobsubmission client adn jobclient to get the current jobs (to be run or running). Made JobStatus class public to be accessed outside of hadoop.", "This looks good to me, except the new JobSubmissionProtocol method is missing javadoc.  Also, if we make JobStatus public, we should probably improve its javadoc too.", "Here is a new patch. I added javadocs to both the JobStatus and JobSubmissionClient protocol.", "I just committed this.  I made a few minor changes.  I made the JobStatus setters non-public, and also removed a javadoc comment for the new method implementation on JobTracker, since the inherited javadoc from the interface seemed more informative."], "derived": {"summary": "adding a few methods to the joblcient and jobsubmission protocol to support queries to the jobtracker for currently running jobs.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "making improvements to the jobclients to get information on currenlyl running jobs and the jobqueue - adding a few methods to the joblcient and jobsubmission protocol to support queries to the jobtracker for currently running jobs."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  I made a few minor changes.  I made the JobStatus setters non-public, and also removed a javadoc comment for the new method implementation on JobTracker, since the inherited javadoc from the interface seemed more informative."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-340", "title": "Using wildcards in config pathnames", "status": "Closed", "priority": "Minor", "reporter": "Johan Oskarsson", "assignee": "Doug Cutting", "labels": [], "created": "2006-07-02T16:40:00.000+0000", "updated": "2006-08-04T22:22:35.000+0000", "description": "In our cluster there's machines with very different disk setups\nI've solved this by not rsyncing hadoop-site.xml, but as you probably understand this means new settings will not get copied properly.\n\nI'd like to be able to use wildcards in the dfs.data.dir path for example:\n<property>\n  <name>dfs.data.dir</name>\n    <value>/home/hadoop/disk*/dfs/data</value>\n</property>\n\nthen every disk mounted in that directory would be used", "comments": ["Disks listed in dfs.data.dir that do not exist on a host are ignored.  So, instead of a wildcard, you can simply list all possible names used in your cluster, and only those that actually exist on a host will be used.  Similarly for mapred.local.dir.  Does that suffice?", "Ah, I guess I'll just add a massive number of directories to the config then.\nNot as clean, but It'll work. Thanks for the info.\n\nA note about this behaviour in the description field in the xml would be nice.\n\n/Johan", "Okay, I added this to the description of these config parameters."], "derived": {"summary": "In our cluster there's machines with very different disk setups\nI've solved this by not rsyncing hadoop-site. xml, but as you probably understand this means new settings will not get copied properly.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Using wildcards in config pathnames - In our cluster there's machines with very different disk setups\nI've solved this by not rsyncing hadoop-site. xml, but as you probably understand this means new settings will not get copied properly."}, {"q": "What updates or decisions were made in the discussion?", "a": "Okay, I added this to the description of these config parameters."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-341", "title": "Enhance distcp to handle *http* as a 'source protocol'.", "status": "Closed", "priority": "Major", "reporter": "Arun Murthy", "assignee": "Arun Murthy", "labels": [], "created": "2006-07-03T18:49:37.000+0000", "updated": "2013-05-02T02:28:59.000+0000", "description": "Requirements:\n\n  Presently distcp recursively copies a directory from one dfs to another i.e. both source and destination of of the *dfs* protocol.\n  Enhance it to handle *http* as the source protocol i.e. support copying files from arbitrary http-based sources into the dfs.\n\nDesign:\n  \n  Follow distcp's current design: one map task per file which needs to be copied.\n\n  Caveat: distcp handles *recursive* copying by listing sub-directories; this is not as feasible with a http-based source since things like 'fancy-indexing' might not be enabled on the web-server (for all sub-locations recursively too), and even if it is enabled it will mean tedious parsing of the html served to glean the sub-directories etc. Hence the idea is to support an input file (via a -f option) which contains a list of the http-based urls which represent multiple source files.\n", "comments": ["Here's a patch which enables distcp to work with http based source files. \n\nIt also provides a -f option to distcp to provide an input file with urls (arbitrary combinations of dfs & http source paths).", "Forgot to add: the above patch (distcp.patch) does a significant refactoring of CopyFiles.java by providing a base CopyFilesMapper class which is subclassed in DFSCopyFilesMapper (which contains Milind's existing code) and HttpCopyFilesMapper (for http-based sources). In future we can add other protocols (ftp?) by creating new subclasses (FtpCopyFilesMapper).\n\nthanks,\nArun\n\nPS: Apologies for the extra spam.", "The TestCopyFiles unit test fails after I apply this patch.", "Verified patch for the TestCopyFiles junit test.\n\nthanks,\nArun", "I just committed this.  Thanks, Arun.", "I have a further enhancement to distcp i.e. -f option now works with urls of scheme http/dfs/file.  Hence I'm reopening this issue and will submit another patch shortly.\n\nDoug, I'll also update logalyzer (HADOOP-342) to reflect these changes and another patch there will be needed too, please hold off commits there.\n\nthanks,\nArun", "Here's a patch to let distcp take an input uri via the '-f' option. The uri can be of dfs/http/file schemes. \nThe tool will then hit that uri to fetch a list of log-files to be copied over.\n\nthanks,\nArun", "I just committed this.  Thanks!"], "derived": {"summary": "Requirements:\n\n  Presently distcp recursively copies a directory from one dfs to another i. e.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Enhance distcp to handle *http* as a 'source protocol'. - Requirements:\n\n  Presently distcp recursively copies a directory from one dfs to another i. e."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-342", "title": "Design/Implement a tool to support archival and analysis of logfiles.", "status": "Closed", "priority": "Major", "reporter": "Arun Murthy", "assignee": null, "labels": [], "created": "2006-07-03T19:26:17.000+0000", "updated": "2013-05-02T02:28:59.000+0000", "description": "Requirements:\n\n  a) Create a tool support archival of logfiles (from diverse sources) in hadoop's dfs.\n  b) The tool should also support analysis of the logfiles via grep/sort primitives. The tool should allow for fairly generic pattern 'grep's and let users 'sort' the matching lines (from grep) on 'columns' of their choice.\n\n  E.g. from hadoop logs: Look for all log-lines with 'FATAL' and sort them based on timestamps (column x)  and then on column y (column x, followed by column y).\n\n\nDesign/Implementation:\n\n  a) Log Archival\n\n    Archival of logs from diverse sources can be accomplished using the *distcp* tool (HADOOP-341).\n  \n  b) Log analysis\n\n    The idea is to enable users of the tool to perform analysis of logs via grep/sort primitives.\n\n    This can be accomplished via a relatively simple Map-Reduce task where the map does the *grep* for the given pattern via RegexMapper and then the implicit *sort* (reducer) is used with a custom Comparator which performs the user-specified comparision (columns). \n\n    The sort/grep specs can be fairly powerful by letting the user of the tool use java's in-built regex patterns (java.util.regex).\n", "comments": ["Sounds good.  Will the output be one or more DFS files?  stdout?  Will it be text or a sequence file?\nKeeping everything in text seems most appropriate.\n\nContributing a generic sorter sounds very valuable.  Could you spec it in more detail.", "Should have clarified this: the plan is to let the user specify an output directory in which a single text file will contain the output of the 'analysis'.\n\nGeneric Sorter:\n\n  The generic sorter basically lets the user specify a column separator and a spec for priority of columns. \n  The Comparator's *compare* function (implements WritableComparable) then splits each sequence of data based on user specified separator and then compares the 2 data streams on the given priorities. \n\n  E.g. -sortColumnSpec 2,0,1 -separator \\t\n  (0-based columns)\n\n  If there is enough interest, I can push this into mapred.lib. Appreciate any suggestions.\n\nthanks,\nArun\n", "Here's the 'logalyzer' tool.\n\nDoug: I felt that it made sense to create a org.apache.hadoop.tools package for logalyzer and other such tools in the future... let me know if you prefer it to be in some other package and i'll update it accordingly.\n\nthanks,\nArun", "I will look at your patch more closely soon.\n\nI think it would be good, rather than copy the logs into DFS, to use HTTP to retrieve the map input.  Ideally, map tasks would be assigned to nodes where the log data is local.\n\nThis could be implemented as an InputFormat that is parameterized by date.  For example, one might specify something like:\n\njob.setInputFormat(LogInputFormat.class);\njob.set(\"log.input.start\", \"2006-07-13 12:00:00\");\njob.set(\"log.input.end\", \"2006-07-13 15:00:00\");\n\nThe set of hosts can be determined automatically to be all hosts in the cluster.  One could also specify a job id, in which case the job's start and end time would be used, or a start job id and end job id.\n\nWe might implement parts of this by enhancing the web server run on each tasktracker, e.g., to directly support access to logs by date range.\n\nDoes this make sense?", "It does make sense.  It might make sense to do it as a second pass though.\n\nWe've got lots of logs from various sources we want this tool to work on.  In many cases loading them into hadoop is a logical first step.\n\nWe should make sure the loading (or HTTP scanning) is distinct from the query tools.", "I concur with the need for (optional?) HTTP based map input... I'll start on it. \n(I have some ideas about generalising this infrastructure, which I'm in the process of compiling and will send it over to a separate email).\n\nEric: Apologise for not clarifying this earlier: logalyzer (as-is) can be used in either mode independently or together i.e. it can be used either for archival or analysis (assuming logs are already in a given directory) or both.\n\nDoug: Can we get logalyzer as-is into the tree right-away and meanwhile I'll get on to the HTTP-base map input enhancement? There is some interest for using it right-away... hope it isn't too much of a problem.\n\nthanks,\nArun", "Summary of logalyzer usage:\n\nLogalyzer.0.0.1\nUsage: \nLogalyzer [-archive -logs urlsFile>] -archiveDir <archiveDirectory> -grep <pattern> -sort <column1,column2,...> -separator <separator> -analysis <outputDirectory>\n\nUsage Scenarios:\n---------------------------\n\na) Archive only:\n\n$ java org.apache.hadoop.tools.Logalyzer -archive -logs <urlsFile> -archiveDir <archiveDirectory>\n\n Fetch the logs specified in <urlsFile> (arbitrary combination of dfs & http based logs) and archive it in <archiveDirectory> (in the dfs).\n\n  Archival of logs from diverse sources is accomplished using the *distcp* tool (HADOOP-341). \n\n\nb) Analyse only:\n \n $ java org.apache.hadoop.tools.Logalyzer -archiveDir <archiveDirectory> -grep <pattern> -sort <column1,column2,...> -separator <separator> -analysis <outputDirectory>\n\n  Analyse the logs in <archiveDirectory> i.e. grep/sort-with-separator and store the output (as a single textfile) of 'analysis' in <outputDirectory>.\n\n  This is accomplished via a Map-Reduce task where the map does the *grep* for the given pattern via RegexMapper and then the implicit *sort* (reducer) is used with a custom Comparator which performs the user-specified comparision (columns).\n\n\nc) Archive and analyse\n\n  $ java org.apache.hadoop.tools.Logalyzer -archive -logs <urlsFile> -archiveDir <archiveDirectory> -grep <pattern> -sort <column1,column2,...> -separator <separator> -analysis <outputDirectory>\n \n  Perform both a) and b) tasks.\n\n       - * - * -\n\nArun", "This patch requires HADOOP-341.\n", "Here's a new patch for logalyzer incorporating changes in distcp (HADOOP-341).\n\nthanks,\nArun", "I just committed this.  Thanks!"], "derived": {"summary": "Requirements:\n\n  a) Create a tool support archival of logfiles (from diverse sources) in hadoop's dfs. b) The tool should also support analysis of the logfiles via grep/sort primitives.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Design/Implement a tool to support archival and analysis of logfiles. - Requirements:\n\n  a) Create a tool support archival of logfiles (from diverse sources) in hadoop's dfs. b) The tool should also support analysis of the logfiles via grep/sort primitives."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-343", "title": "In case of dead task tracker, the copy mapouts try copying all mapoutputs from this tasktracker", "status": "Closed", "priority": "Major", "reporter": "Mahadev Konar", "assignee": "Sameer Paranjpye", "labels": [], "created": "2006-07-06T01:07:46.000+0000", "updated": "2009-07-08T16:51:50.000+0000", "description": "In case of a dead task tracker, the reduces which do not have the updated map out locations try copygin files from this node and since there are failures on copying, this leads to backoff and slowing down of the copy pahse.", "comments": ["This patch updates its mapoutput locations in case of any failures on copying from the task tracker. So, in case a copy failed from a task tracker, the map outputs corresponding to this node will be marked stale. The ReduceTask will ask for the mapout locations again for these stale map outputs. This patch also fixes a bug wherein the tasks keep polling the job tracker for map otputs \"in a loop\" without sleeping/waiting. With this fix the tasks will poll evry MIN_POLL_INTERVAL before querying the job tracker. ", "I have concerns about this patch. It might have unintended consequences. In particular, if a node is slow you'll drop what you know about that node and put more load on the job tracker. This patch is certainly addressing a real problem, though.", "Don't forget HADOOP-248 .  This could be implemented in such a way that once a single task fails to reach the job all task trackers are notified of the failure.", "I think this needs further attention. The patch is probably out of date at this point, but the problem is real. I think this may also be responsible for the 'long pause' at the end of the shuffle.\n\nIf a tasktracker fails, it's map outputs are lost. However, the other tasktrackers are unaware of this. Towards the end of the shuffle they have all map output locations cached and they keep trying to pull data from the lost tasktracker, one file at a time. Every one of these file transfers fails, each failed transfer also causes the tasktrackers to back off from pulling their remaining outputs. The cumulative effect of all the backoffs is the long pause.", "This patch addresses the concern raised. If a map output transfer from a particular tasktracker fails, other output locations from the tasktracker that are present in the cache are removed. This addresses the problem of repeated attempts and backoffs from a lost tasktracker, which is particularly bad towards the end of a shuffle. Copies can, of course, fail for other reasons, in these cases also output locations are removed. The cost of this removal is fairly low. This is because the number of output locations cached for a specific tasktracker is usually small (3-4), and removing these (multiple times even) results in a handful of extra polls of the jobtracker.", "I just committed this.  Thanks, Sameer!"], "derived": {"summary": "In case of a dead task tracker, the reduces which do not have the updated map out locations try copygin files from this node and since there are failures on copying, this leads to backoff and slowing down of the copy pahse.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "In case of dead task tracker, the copy mapouts try copying all mapoutputs from this tasktracker - In case of a dead task tracker, the reduces which do not have the updated map out locations try copygin files from this node and since there are failures on copying, this leads to backoff and slowing down of the copy pahse."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Sameer!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-344", "title": "TaskTracker passes incorrect file path to DF under cygwin", "status": "Closed", "priority": "Major", "reporter": "Konstantin Shvachko", "assignee": "Konstantin Shvachko", "labels": [], "created": "2006-07-06T05:12:06.000+0000", "updated": "2006-08-04T22:22:37.000+0000", "description": "The path that is passed is OS dependent. The File abstraction should be used in order to make it universal.\nI'm attaching a patch that does that.\nWe might want to change the path parameter for DF. Making it File rather than String should prevent us from \nbugs like that in the future.", "comments": ["This patch is getting stale.\nIs there any reason for not committing this?\nI'm getting FD related exceptions in my windows nightly build.\n", "Wouldn't it be better to change DF to do 'new File(path).getCanonicalPath()', rather than change the caller?", "This would also work. But there are 2 other places that already pass canonical paths.\nSo we will have to change these two callers in order to avoid repetitive parsing of already \ncanonical paths. ", "> there are 2 other places that already pass canonical paths\n\nWouldn't it be better to change these?  That would minimize code replication, a good thing.", "I did two variants.\n- DFConstructor.patch converts path to the canonical form inside the DF constructor.\nI also changed FSDataset() to just simply return the path without canonicalizing it.\n- DFPath2File.patch changes the path parameter type from String to File.\nI String version constructors are deprecated.\nI like this version better for 2 reasons: it makes it clear for the callers what needs to\nbe passed, and it does less File to String conversions.\n", "I just committed this.  Thanks, Konstantin!"], "derived": {"summary": "The path that is passed is OS dependent. The File abstraction should be used in order to make it universal.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "TaskTracker passes incorrect file path to DF under cygwin - The path that is passed is OS dependent. The File abstraction should be used in order to make it universal."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Konstantin!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-345", "title": "JobConf access to name-values", "status": "Closed", "priority": "Major", "reporter": "Michel Tourn", "assignee": "Michel Tourn", "labels": [], "created": "2006-07-06T07:41:37.000+0000", "updated": "2006-08-04T22:22:37.000+0000", "description": "class JobConf (or its base class Configuration) \nshould be extended to enable enumeration of all its key-value pairs.\n( more precisely: the Properties returned by Configuration.getProps() )\n\nThis will be useful to \"export\" all JobConf properties to environment variables.\nWe use env.vars to expose some Hadoop context to non-Java MapReduce applications.\n\nNote that the typed properties are also represented as Strings \n(getInt, getStrings, getClass, etc.)\nSo a single enumeration exposes everything as (untyped) environment variables.\n\nThe proposed escaping rules from JobConf properties to env.var are:\n\n1. values are left as-is.\n2. keys are escaped as follows:\n[A-Za-z0-9] --> unchanged.\nall other chars --> underscore.\n\nFor example\nset(\"mapred.input.key.class\", \"com.example.MyKey\")\nbecomes env.var:\nexport mapred_input_key_class=com.example.MyKey\n\nJustification:\n1. Environment variables are case-sensitive. (Although uppercase is the preferred convention)\n  So no need to uppercase everything.\n2. Some characters are forbidden in env.vars, or at least not shell-friendly:\nFor example period, colon are problematic.\n3. The Hadoop conventions are already hierarchical and provide some namespace protection.\n   This means we don't need an additional prefix as protection.\n   For example all exported environment variables will start with \"mapred.\" , \"dfs.\" , \"ipc.\" etc.\n  This means they will not conflict with standard environemnt variables like PATH, USER, etc.\n  And they will not conflict with standard hadoop env.vars because those are upper-case. (like HADOOP_CONF_DIR)\n\n\n\n\n\n\n\n\n", "comments": [">class JobConf (or its base class Configuration)\n>should be extended to enable enumeration of all its key-value pairs. \n\nSo strictly speaking all that is required is to make this Configuration method public:\n>private synchronized Properties getProps()\n\nis that OK?\n", "Better than exposing the Properties might be adding a method like:\n\npublic Iterator<Map.Entry<String,String>> entries();\n\nThat would provide abstract, read-only access.\n\nDoug", "All right, here is the entries() patch.\n", "I just committed this.  Thanks, Michel."], "derived": {"summary": "class JobConf (or its base class Configuration) \nshould be extended to enable enumeration of all its key-value pairs. ( more precisely: the Properties returned by Configuration.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "JobConf access to name-values - class JobConf (or its base class Configuration) \nshould be extended to enable enumeration of all its key-value pairs. ( more precisely: the Properties returned by Configuration."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Michel."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-347", "title": "Implement HDFS content browsing interface", "status": "Closed", "priority": "Major", "reporter": "Devaraj Das", "assignee": "Devaraj Das", "labels": [], "created": "2006-07-06T17:37:56.000+0000", "updated": "2013-05-02T02:28:58.000+0000", "description": "Implement HDFS content browsing interface over HTTP. Clients would connect to the NameNode and this would send a redirect to a random DataNode. The DataNode, via dfs client, would proxy to namenode for metadata browsing and to other datanodes for content. One can also view the local blocks on any DataNode. Head, Tail will be provided as shorthands for viewing the first block and the last block of a file. \nFor full file viewing, the data displayed per HTTP request will be a block with a PREV/NEXT link. The block size for viewing can be a configurable parameter (the user sets it via the web browser) to the HTTP server (e.g., 256 KB can be the default block size for viewing files).", "comments": ["Sounds good.\n\nCould you provide a little more detail on the UI on the name node?  I image a link on the front page to a page that redirects to a random data node.  That the plan?\n\nAlso you probably want to redirect the reader to a node containing the first block of the file when you get the files contents.\n\nThe block viewing seems like a good option, we should also provide the ability to simply fetch the full file.  Maybe two links on the directory browsing API could provide this option?  Machine clients would be frustrated by the block structure.\n\nLet's be sure to support tail (-f in the future).  The is very often the desired operation.  A third link on the directory browsing page would do nicely.\n\nI like the blocked view idea.  Perhaps all of this can simply be GET arguments appended to the end of the URL?  IE \"HDFS://x.com/myPath?sz=128k&off=512k\"  Negative offsets could be used for tail.\n\nSo you imagine a badge at the top of the block viewed page with <prev next> links?  Might just add some block size options there too and tail.  Keep the whole API stateless (all info in the URL).\n", "Thanks Eric for the inputs! \nRegarding the UI on the front page, what you say makes sense. The other option being the namenode sending a HTTP REDIRECT to the host the namenode chooses. We can take the first approach since that will not involve sending the REDIRECT (thereby saving some data communication overhead).\nI agree in general with the other things like block viewing, etc. you mention. Also, I plan to support tail too (mentioned that when I created this bug).", "looking forward to all this good stuff!", "a typical scenario is for a user to click on a generic link from some front page to browse the dfs, without specifying a file. The namenode will redirect to a datanode, the user browses the directory hierarchy, then views files.\nit's thus impossible to direct the client to the datanode that contains the first block of \"the file\" - it's not known when the client first connects.\n\none way to go about this, is to have the datanode do redirection too when actual contents are requested. In that case we don't need to stop with the first block. Since GET requests are stateless, the client can be redirected every time he needs a new dfs block.\nI suggest pushing this all out for a later version for simplicity. Have the client connect to a random node and keep him there, just load balancing clients.", "I am taking the approach where the user first connects to the namenode's jetty and he gets redirected to a random datanode. The random datanode (having an embedded DFS client) then gets the directory listing (starting at the root) from the namenode (through the regular IPC channel). The user then probably chooses a file to view - the JSP at the backend lists the blocks of the file and he also gets options to do HEAD, TAIL, DOWNLOAD_FULL_FILE, and, BLOCK_VIEW (view the contents of the block in a chunked fashion with a prev, next for navigation). \n\nFor these options he gets redirected to one of the datanodes which has the block that he ought to view. That can be handled since the LocatedBlock array obtained by doing dfs.namenode.open returns an array of blocks  (in the order of the bytes of the file) and the block number & offset within that can be kept track of since the client sends, as part of each HTTP request, the block number he currently is at along with the offsets (start, end) that he wants to view next. If the current datanode finishes serving up all the chunks for the current block the client gets redirected to one of the datanodes that has the next block with chunk offsets (0, <some-preconfigured-value>). The current datanode knows where to send the client to since it can also obtain the LocatedBlock array from the namenode and figure that out from the array. The offset end (<some-preconfigured-value>) can be customized by the user but must be a factor of the DFS block size that the file was created with.", "Implemented this as discussed.", "I just committed this.  Thanks!", "By the way, in order to use this, hit the namenode's jetty (default port 50070). You will see the links from there.", "Fixed a problem whereby negative values of (randomly generated) array indices were not checked. This is an incremental patch over what has already been committed. Sorry for overlooking this.", "This patch is causing problems for me, if a computer have a second dfs data dir in the config it doesn't start properly because of:\n\nException in thread \"main\" java.io.IOException: Problem starting http server\n        at org.apache.hadoop.mapred.StatusHttpServer.start(StatusHttpServer.java:182)\n        at org.apache.hadoop.dfs.DataNode.<init>(DataNode.java:170)\n        at org.apache.hadoop.dfs.DataNode.makeInstanceForDir(DataNode.java:1045)\n        at org.apache.hadoop.dfs.DataNode.run(DataNode.java:999)\n        at org.apache.hadoop.dfs.DataNode.runAndWait(DataNode.java:1015)\n        at org.apache.hadoop.dfs.DataNode.main(DataNode.java:1066)\nCaused by: org.mortbay.util.MultiException[java.net.BindException: Address already in use]\n        at org.mortbay.http.HttpServer.doStart(HttpServer.java:731)\n        at org.mortbay.util.Container.start(Container.java:72)\n        at org.apache.hadoop.mapred.StatusHttpServer.start(StatusHttpServer.java:159)\n        ... 5 more\n\nI noticed there is code in the start method to pick a new port if the MultiException is thrown, however it doesn't seem to work.\nFor now I've just moved this.infoServer.start(); in DataNode.java up one line so the exception is caught and ignored, since I can't use the dfs web interface anyway (dfs nodes are all behind a gateway)", "Yeah this problem is there. Thanks for pointing it out. I will create a separate issue to handle multiple datanodes in a single machine.", "I put the patch for the http port scan over in HADOOP-376. There is a parameter to the StatusHttpServer that tells it to scan for a port, so we just had to change the value to true.", "\n   [[ Old comment, sent by email on Fri, 7 Jul 2006 00:08:28 +0530 ]]\n\nHi Eric,\nI spoke to Marco regarding WebDAV, and he told me that he just got started\non the work, etc. So I am wondering whether I should start off on this work\nwithout WebDAV. Things like block view, etc. may not be possible with WebDAV\nwithout major tweaking. What are your thoughts?\nThanks\nDevaraj.\n\n"], "derived": {"summary": "Implement HDFS content browsing interface over HTTP. Clients would connect to the NameNode and this would send a redirect to a random DataNode.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Implement HDFS content browsing interface - Implement HDFS content browsing interface over HTTP. Clients would connect to the NameNode and this would send a redirect to a random DataNode."}, {"q": "What updates or decisions were made in the discussion?", "a": "[[ Old comment, sent by email on Fri, 7 Jul 2006 00:08:28 +0530 ]]\n\nHi Eric,\nI spoke to Marco regarding WebDAV, and he told me that he just got started\non the work, etc. So I am wondering whether I should start off on this work\nwithout WebDAV. Things like block view, etc. may not be possible with WebDAV\nwithout major tweaking. What are your thoughts?\nThanks\nDevaraj."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-348", "title": "fs.default.name default not working", "status": "Closed", "priority": "Minor", "reporter": "Barry Kaplan", "assignee": "Sameer Paranjpye", "labels": [], "created": "2006-07-07T01:26:41.000+0000", "updated": "2009-07-08T16:41:57.000+0000", "description": "in NameNode:84 and FSNamesystem:176 there is a line:\n\nInetSocketAddress addr = DataNode.createSocketAddr(conf.get(\"fs.default.name\", \"local\"));\n\nThis won't ever work as createSocketAddr first checks to make sure there is a : in the string to get the server name and port. It is not a super simple fix of just putting in a :50000 (or something like that) because the NameNode constructor allows you to pass in a port that would need to match the port the in the conf setting. I think the method public NameNode(File dir, int port, Configuration conf) should simply be deprecated.", "comments": ["This issue is no longer valid."], "derived": {"summary": "in NameNode:84 and FSNamesystem:176 there is a line:\n\nInetSocketAddress addr = DataNode. createSocketAddr(conf.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "fs.default.name default not working - in NameNode:84 and FSNamesystem:176 there is a line:\n\nInetSocketAddress addr = DataNode. createSocketAddr(conf."}, {"q": "What updates or decisions were made in the discussion?", "a": "This issue is no longer valid."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-349", "title": "Allow info server to be turned off/on by conf file", "status": "Closed", "priority": "Major", "reporter": "Barry Kaplan", "assignee": "Doug Cutting", "labels": [], "created": "2006-07-07T01:29:18.000+0000", "updated": "2009-04-23T19:24:57.000+0000", "description": "Since I am using hadoop within my own servlet which is not Jetty, it would be nice to not have to need Jetty to run my servlet. As such I propose adding an if statement/donf setting at FSNamesystem:171 as such         \n\nif(conf.getBoolean(\"dfs.info.active\",true)){\n            this.infoPort = conf.getInt(\"dfs.info.port\", 50070);\n            this.infoServer = new StatusHttpServer(\"dfs\", infoPort, false);\n            this.infoServer.start();\n}", "comments": ["Also see HADOOP-351", "adds config setting for turning off the info server", "this is kind of pushing the opposite direction we're going.  We are adding more functionality to this server and since we want to have a multi-lingual interoperable protocol down the road, something over HTTP makes sense.  I'm imaging the HTTP server will eventually be the only port to reach these servers.\n\nSo a long term solution will need to allow you to use tomcat or run an embedded jeti on another port.  Not just turn of jetty.", "What is the problem with running a Jetty-based daemon from within Tomcat?\n\nIt sounds like you'd like to package daemons as war files.  So, in this scheme, there would be separate namenode, datanode, tasktracker and jobtracker war file, since each exposes a distinct web interface, right?  That's a larger change to Hadoop than simply disabling the jetty daemons, since Hadoop sometimes stores application state in the servlet context, etc.\n\nShort of that, you could simply package the datanode as a war file, but permit it to start its own daemon for it's web interface on a different port.  Is that so horrible?"], "derived": {"summary": "Since I am using hadoop within my own servlet which is not Jetty, it would be nice to not have to need Jetty to run my servlet. As such I propose adding an if statement/donf setting at FSNamesystem:171 as such         \n\nif(conf.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Allow info server to be turned off/on by conf file - Since I am using hadoop within my own servlet which is not Jetty, it would be nice to not have to need Jetty to run my servlet. As such I propose adding an if statement/donf setting at FSNamesystem:171 as such         \n\nif(conf."}, {"q": "What updates or decisions were made in the discussion?", "a": "What is the problem with running a Jetty-based daemon from within Tomcat?\n\nIt sounds like you'd like to package daemons as war files.  So, in this scheme, there would be separate namenode, datanode, tasktracker and jobtracker war file, since each exposes a distinct web interface, right?  That's a larger change to Hadoop than simply disabling the jetty daemons, since Hadoop sometimes stores application state in the servlet context, etc.\n\nShort of that, you could simply package the datanode as a war file, but permit it to start its own daemon for it's web interface on a different port.  Is that so horrible?"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-350", "title": "In standalone mode, 'org.apache.commons.cli cannot be resolved'", "status": "Closed", "priority": "Minor", "reporter": "Michael Stack", "assignee": null, "labels": [], "created": "2006-07-07T03:42:34.000+0000", "updated": "2006-08-04T22:22:38.000+0000", "description": "Standalone works fine in 0.4.0 but if I use TRUNK, 'Last Changed Date: 2006-06-28 14:27:33 -0700 (Wed, 28 Jun 2006)', and pass it my fat job jar, I get following exception.\n\n06/07/06 13:18:02 INFO conf.Configuration: parsing file:/tmp/hadoop-unjar29923/nutch-site.xml\n06/07/06 13:18:02 INFO conf.Configuration: parsing file:/home/stack/workspace/hadoop-local-conf/hadoop-site.xml\nException in thread \"main\" java.lang.Error: Unresolved compilation problems:   \n        The import org.apache.commons.cli cannot be resolved\n        The import org.apache.commons.cli cannot be resolved\n        The import org.apache.commons.cli cannot be resolved\n        The import org.apache.commons.cli cannot be resolved       \n        The import org.apache.commons.cli cannot be resolved\n        The import org.apache.commons.cli cannot be resolved\n        The import org.apache.commons.cli cannot be resolved\n        The import org.apache.commons.cli cannot be resolved\n        The import org.apache.commons.cli cannot be resolved\n        Options cannot be resolved to a type\n        Option cannot be resolved to a type\n        OptionBuilder cannot be resolved\n        Option cannot be resolved to a type\n        OptionBuilder cannot be resolved\n        Option cannot be resolved to a type\n        OptionBuilder cannot be resolved\n        Option cannot be resolved to a type\n        OptionBuilder cannot be resolved\n        Options cannot be resolved to a type\n        Options cannot be resolved to a type\n        CommandLine cannot be resolved to a type\n        Options cannot be resolved to a type\n        The method buildGeneralOptions() is undefined for the type ToolBase\n        CommandLineParser cannot be resolved to a type\n        GnuParser cannot be resolved to a type\n        CommandLine cannot be resolved to a type\n        ParseException cannot be resolved to a type\n        e cannot be resolved\n        HelpFormatter cannot be resolved to a type\n        HelpFormatter cannot be resolved to a type\n\n        at org.apache.hadoop.util.ToolBase.<init>(ToolBase.java:21)\n        at org.apache.hadoop.mapred.JobClient.<init>(JobClient.java:178)\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:320)\n        at org.archive.access.nutch.ImportArcs.importArcs(ImportArcs.java:576)\n        at org.archive.access.nutch.Nutchwax.doImport(Nutchwax.java:159)\n        at org.archive.access.nutch.Nutchwax.doAll(Nutchwax.java:144)\n        at org.archive.access.nutch.Nutchwax.doJob(Nutchwax.java:368)\n        at org.archive.access.nutch.Nutchwax.main(Nutchwax.java:621)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:585)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:128)\n\nI took a bit of a look.  Odd is that command-cli.jar is in hadoop/lib dir so should be found no problem (and we seem to be loading other stuff out of the uncompressed job jar fine -- see mention of nutch-site.xml above).   I tried adding the command-cli to my job jar.  I printed out all that RunJar -- see below -- was adding to its CLASSPATH and saw mention of command-cli but still no workee.\n\nfile:/tmp/hadoop-unjar29923/lib/archive-commons-1.8.0.jar\nfile:/tmp/hadoop-unjar29923/lib/commons-codec-1.3.jar\nfile:/tmp/hadoop-unjar29923/lib/commons-httpclient-3.0-rc3.jar\nfile:/tmp/hadoop-unjar29923/lib/commons-logging-1.0.4.jar\nfile:/tmp/hadoop-unjar29923/lib/dsi.unimi.it-1.2.0.jar\nfile:/tmp/hadoop-unjar29923/lib/commons-lang-2.1.jar\nfile:/tmp/hadoop-unjar29923/lib/commons-cli-2.0-SNAPSHOT.jar\nfile:/tmp/hadoop-unjar29923/lib/lucene-core-1.9.1.jar\nfile:/tmp/hadoop-unjar29923/lib/lucene-misc-1.9.1.jar\nfile:/tmp/hadoop-unjar29923/lib/jakarta-oro-2.0.7.jar\nfile:/tmp/hadoop-unjar29923/lib/xerces-2_6_2-apis.jar\nfile:/tmp/hadoop-unjar29923/lib/xerces-2_6_2.jar\nfile:/tmp/hadoop-unjar29923/lib/concurrent-1.3.4.jar\n06/07/06 13:18:01 INFO conf.Configuration: parsing file:/home/stack/workspace/hadoop-local-conf/hadoop-default.xml \n\nAny ideas?\n\nThanks.", "comments": ["What JVM are you using locally?  1.4?  The cli jar (unfortunately) seems to be a version that requires 1.5.  Could that be the issue?", "Thanks for the tip Doug. JVM 1.4 is the issue. \n\nAlas, I need to stay on Java 1.4 for my hadoop install, so I tried to recomplile the commons-cli jar. Not sure if I am using the right source code (/branches/commons-configuration-integration/), and I had to patch some classes because of the .withArgPattern in /nutch/trunk/hadoop/src/java/org/apache/hadoop/util/ToolBase.java (see http://issues.apache.org/jira/browse/CLI-12).\n\nMaybe Hairong can provide more details...\n\nBest,\nRenaud", "commons cli jar using http://svn.apache.org/repos/asf/jakarta/commons/proper/cli/branches/commons-configuration-integration, patched with CLI-12, compiled with Java 1.4", "Hi Renaud,\n\nI recompiled commons-cli using java 1.4. Here is the new jar.  Also I included the source. I did not check if it is the same as yours. But Millind helped me test it and it works. Thanks for your effort on solving this problem. \n\nHairong", "Thank you Hairong for the fast answer!\n\nI think it would be useful if this jar would be commited to nutch-trunk and hadoop-trunk, WDOT?\n\nBest, Renaud", "I just committed this.  Thanks all!"], "derived": {"summary": "Standalone works fine in 0. 4.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "In standalone mode, 'org.apache.commons.cli cannot be resolved' - Standalone works fine in 0. 4."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks all!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-351", "title": "Remove Jetty dependency", "status": "Closed", "priority": "Major", "reporter": "Barry Kaplan", "assignee": "Devaraj Das", "labels": [], "created": "2006-07-07T05:25:02.000+0000", "updated": "2006-08-04T22:22:38.000+0000", "description": "Somewhat related to HADOOP-349, it would be nice to not have a Jetty dependency for those of us embedding Hadoop within our own web applications. In particular the Server object is using SocketChannelOutputStream from the Jetty code base. It seems to me that for an object like this it would be better to simply have a Hadoop version of a blocking output stream on an nio channel if necessary vs. using the Jetty version requiring a Hadoop user's to package yet another jar file (and all the complications that are associated with that).", "comments": ["Not a problem at all in replicating the class under Hadoop. I will work on it soon.", "Modified Server.java to remove Jetty dependency", "I just committed this.  Thanks!"], "derived": {"summary": "Somewhat related to HADOOP-349, it would be nice to not have a Jetty dependency for those of us embedding Hadoop within our own web applications. In particular the Server object is using SocketChannelOutputStream from the Jetty code base.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Remove Jetty dependency - Somewhat related to HADOOP-349, it would be nice to not have a Jetty dependency for those of us embedding Hadoop within our own web applications. In particular the Server object is using SocketChannelOutputStream from the Jetty code base."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-352", "title": "Portability of hadoop shell scripts for deployment", "status": "Closed", "priority": "Major", "reporter": "Jean-Baptiste Quenot", "assignee": null, "labels": [], "created": "2006-07-07T16:09:20.000+0000", "updated": "2006-08-21T19:35:54.000+0000", "description": "Hadoop shell scripts are based on /bin/bash, which is a \"standard\" shell only on GNU/Linux.  On other Unix systems like FreeBSD however the \"standard\" shell is /bin/sh.  The attached patch addresses these compatiblity issues.  Note that Solaris support is not yet tested.\n\nAlso, the best way to set the HADOOP_HOME variable upon ssh connection in a portable way is to set it in .ssh/environment.  The bash startup script \".bashrc\" is not an option on systems where \"bash\" is not installed.", "comments": ["These scripts were initially declared to use sh, however they didn't work correctly under all versions of sh.  The problem is that most developers of these scripts develop on Linux, where sh and bash are the same program.  Thus it is *very* easy for bash-specific features to creep into these scripts.  So, if we switch them back to sh but don't have folks who are daily testing the trunk on a non-bash-based sh, then we are likely to deliver releases where these fail in strange ways.  Right now they fail in a predictable way if you don't have bash installed, so there's no confusion.  So I am reluctant to make this change unless I am convinced there are several developers who track trunk changes using a non-bash-based sh.\n\nHow hard is it to install bash on FreeBSD?\n", "This is the purpose of compile (and testing) farms at SourceForge (see http://sourceforge.net/docman/display_doc.php?docid=762&group_id=1), is there such a possibility at Apache?\n\nI understand your concern that your changes might introduce new incompatibilities.  But at the contrary, you may prevent users from adopting Hadoop by having very Linux-specific constructs.  FYI I'm working in a group of persons that intend to deploy Hadoop on Linux, FreeBSD, Cygwin and Solaris systems.\n\nThere are simples rules to follow that could ensure compatibility:\n\n* \"> file 2>&1\" instead of \">& file\"\n* \". file\" instead of \"source file\"\n* Not using /proc (Linux-specific)\n\nBest regards.", "We currently do not have unit tests for the scripts, nor does Apache have a FreeBSD compilation server.\n\nI think there were more subtle problems with the scripts than those you mention, although I've forgotten the details.  Have you succeeded, with your patch, to use the scripts to manage a FreeBSD cluster?\n\nI do not wish to discourage in any way your group from using Hadoop.  Is installing bash on FreeBSD difficult?  If it is, and your group will be regular Hadoop users, then perhaps we should commit this patch.\n", "Yes, with my patch, the Hadoop scripts is deployed and runs on FreeBSD.\n\nBTW I didn't mention in the previous comment, \"if [ -a file ]\" constructs are not compatible.\n\nInstalling bash on FreeBSD is straightforward provided that you symlink /bin/bash to /usr/local/bin/bash.  But we must address Solaris deployment soon, and installing bash is more complicated.", "I just committed this.  Thanks, Jean-Baptiste."], "derived": {"summary": "Hadoop shell scripts are based on /bin/bash, which is a \"standard\" shell only on GNU/Linux. On other Unix systems like FreeBSD however the \"standard\" shell is /bin/sh.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Portability of hadoop shell scripts for deployment - Hadoop shell scripts are based on /bin/bash, which is a \"standard\" shell only on GNU/Linux. On other Unix systems like FreeBSD however the \"standard\" shell is /bin/sh."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Jean-Baptiste."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-353", "title": "Run datanode (or other hadoop servers) inside tomcat", "status": "Resolved", "priority": "Major", "reporter": "Eric Baldeschwieler", "assignee": null, "labels": [], "created": "2006-07-07T23:27:49.000+0000", "updated": "2014-07-18T04:58:44.000+0000", "description": "Barry Kaplan is running hadoop data nodes inside tomcat and encountering some issues.\n\nhttp://issues.apache.org/jira/browse/HADOOP-211#action_12419360\n\nI'm filing this bug to capture discussion about the pros and cons of such an approach.  I'd be curious to know what others on the list (who know more about java/tomcat) think about this proposal.", "comments": ["Eric thanks for putting this in... I will help out as much as I can.\n\nTo explain my setup we essentially have a web farm running just tomcat, it makes it simple for deployment and upgrades. Essentially all of our servers are used for utility purposes (indexing, statistic calulations, etc...) or as web servers. Ever web server is also a data node. One of our utility servers (we are working on using a poll method) is picked to be the namenode.\n\nThe beauty of this design for us, is we can deploy a server and install java,tomcat,war file and have a server that is 100% utilized in our environment. Hadoop's DFS allows us to take advantage of the disk space available across our environment with 0 configuration.", "A core issue is that we use JETTY for real work already in the tasktrackers and are actively adding more functionality to the it on the datanodes too.  How does that interact with tomcat?", "Is this still a problem today? If not, we may close this out.", "I'm closing this out as Won't Fix.  At this point, I think there is zero interesting in making Hadoop run inside another app server."], "derived": {"summary": "Barry Kaplan is running hadoop data nodes inside tomcat and encountering some issues. http://issues.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Run datanode (or other hadoop servers) inside tomcat - Barry Kaplan is running hadoop data nodes inside tomcat and encountering some issues. http://issues."}, {"q": "What updates or decisions were made in the discussion?", "a": "I'm closing this out as Won't Fix.  At this point, I think there is zero interesting in making Hadoop run inside another app server."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-354", "title": "All daemons should have public methods to start and stop them", "status": "Closed", "priority": "Major", "reporter": "Barry Kaplan", "assignee": null, "labels": [], "created": "2006-07-08T00:25:02.000+0000", "updated": "2009-07-08T16:41:57.000+0000", "description": "To get tomcat working shutdown working properly I needed to make the NameNode stop() method public as well as the DataNode shutdown() method public. (will attach patch file).\n\nFurthermore I noticed that the RPC object has a static client reference that you have no way of stopping, so I added one. (will attach patch)", "comments": ["modification to NameNode, DataNode, and RPC to allow thread shutdown.", "I wonder if these daemons should implement a Stopable interface?  Is that overkill?\n\nAlso, your patch needs to update the javadoc for NameNode.stop().", "I will add the javadoc. Also in addition to the shutdown on the DataNode, I made a shutdownAll() method that will shutdown all the started datanodes that were created using DatNode.run(conf). I ran into a bit of a head banging issue that manifested itself as a socket timeout because I was starting my DataNodes on my own with offerService, but not starting the DataXceiveServer. I am not sure if there is even a way to start the DataXceiveServer other then dataNode.run(conf).\n\nI will attach the new patch shortly.\n\nI think a stoppable interface would be a bit of overkill right now, but certainly easy enough to add later.", "Patch includes updated javadoc as well as shutdownAll() method on DataNode", "I just committed this.  Thanks, Barry!"], "derived": {"summary": "To get tomcat working shutdown working properly I needed to make the NameNode stop() method public as well as the DataNode shutdown() method public. (will attach patch file).", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "All daemons should have public methods to start and stop them - To get tomcat working shutdown working properly I needed to make the NameNode stop() method public as well as the DataNode shutdown() method public. (will attach patch file)."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Barry!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-355", "title": "hadoopStreaming: fix APIs, -reduce NONE, StreamSequenceRecordReader", "status": "Closed", "priority": "Major", "reporter": "Michel Tourn", "assignee": null, "labels": [], "created": "2006-07-08T04:57:30.000+0000", "updated": "2009-07-08T17:05:35.000+0000", "description": "This patch fixes outstanding problems with streaming:\nfix APIs\noption -reduce NONE\nsupport StreamSequenceRecordReader as an input\n", "comments": ["I just committed this.  Thanks, Michel!"], "derived": {"summary": "This patch fixes outstanding problems with streaming:\nfix APIs\noption -reduce NONE\nsupport StreamSequenceRecordReader as an input.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "hadoopStreaming: fix APIs, -reduce NONE, StreamSequenceRecordReader - This patch fixes outstanding problems with streaming:\nfix APIs\noption -reduce NONE\nsupport StreamSequenceRecordReader as an input."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Michel!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-356", "title": "Build and test hadoopStreaming nightly", "status": "Closed", "priority": "Major", "reporter": "Michel Tourn", "assignee": null, "labels": [], "created": "2006-07-08T05:14:38.000+0000", "updated": "2006-08-04T22:22:41.000+0000", "description": "hadoopStreaming was originally built and tested separately.\n\nIt makes sense to keep building the hadoopStreaming code as a separate jar file:\nthis is all client code that can be uploaded in the Job jar.\nthis allows quick turnaround on production clusters \n(hadoopStreaming code updates do not require to bring down the MapReduce system.\nIf necessary users can use their own modified versions of the hadoopStreaming jar but still run on the production cluster.)\n\nOn the other hand it makes sense to build this code nightly.\nMany recent changes broke either compilation or correctness.\nAll the problems would have been caught if the hadoopStreaming compilation and tests were run nighlty.\n\nConclusion:\nthe updated top-level build.xml adds the following dependencies:\n\n1. Target compile calls target compile in contrib/streaming/build.xml\n2. Target test calls target test in contrib/streaming/build.xml\n\n\n\n\n\n\n\n\n", "comments": ["I think this is a good idea, but I think it should be more structured.  We should rename the \"compile\" and \"test\" targets to be \"compile-core\" and \"test-core\", then add new targets named \"compile\" and \"test\" that depend on both the \"-core\" targets and the \"-contrib\" targets.  Thus the \"compile\" target will compile both core and contrib code, and the \"test\" target will test both core and contrib code.  Does that make sense?  That's the way we've done this in Lucene and Nutch.", "Yes, this target separation is better.\n\nIn order to not break the build for some people:\nI will need to extend and make the regression tests pure-Java, so that they work on non-Linux machines.\n\nSo I will update this patch but only after my next iteration for hadoop streaming.\n", "The patch unblocks HADOOP-307\n\nThe patch modifies the following targets:\n  <target name=\"clean-contrib\"   depends=\"compile\">\n  <target name=\"compile-contrib\" depends=\"compile-core\">\n  <target name=\"compile\"         depends=\"compile-core, compile-contrib\">\n  <target name=\"test\" depends=\"test-core, test-contrib\">\n  <target name=\"test-contrib\">\n--\nHow to add a new \"contrib\" project:\n add a subdirectory to src/contrib/<projectname>\n see src/contrib/streaming/build.xml for an example build.xml\n There are generic ant targets in src/contrib/build.xml and build-contrib.xml\n To disable a contrib target:\n  put an excludes attribute in src/contrib/build.xml (explained inside)\n", "I changed this some and committed it.  Since the old \"compile\" target is now named \"compile-core\", most of the dependencies on it needed to be changed to \"compile-core\" too.  Also, I renamed the \"compile-test\" target to be \"compile-core-test\".\n\nI note that the contrib packages are not included in distributions (the \"tar\" target).  They probably should be.  Michel, would you like to modify \"tar\" to include the contrib code?  This should be done in a separate bug."], "derived": {"summary": "hadoopStreaming was originally built and tested separately. It makes sense to keep building the hadoopStreaming code as a separate jar file:\nthis is all client code that can be uploaded in the Job jar.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Build and test hadoopStreaming nightly - hadoopStreaming was originally built and tested separately. It makes sense to keep building the hadoopStreaming code as a separate jar file:\nthis is all client code that can be uploaded in the Job jar."}, {"q": "What updates or decisions were made in the discussion?", "a": "I changed this some and committed it.  Since the old \"compile\" target is now named \"compile-core\", most of the dependencies on it needed to be changed to \"compile-core\" too.  Also, I renamed the \"compile-test\" target to be \"compile-core-test\".\n\nI note that the contrib packages are not included in distributions (the \"tar\" target).  They probably should be.  Michel, would you like to modify \"tar\" to include the contrib code?  This should be done in a separate bug."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-357", "title": "hadoop doesn't handle 0 reduces", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": null, "labels": [], "created": "2006-07-08T06:25:50.000+0000", "updated": "2009-07-08T16:51:54.000+0000", "description": "We have cases where we want to use maps for submitting parallel jobs and don't need any reduces. Currently, you are required to have a single reducer. It would be nicer if the framework would let you specify that you don't need any reducers.", "comments": ["This was fixed more generally by HADOOP-1216."], "derived": {"summary": "We have cases where we want to use maps for submitting parallel jobs and don't need any reduces. Currently, you are required to have a single reducer.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "hadoop doesn't handle 0 reduces - We have cases where we want to use maps for submitting parallel jobs and don't need any reduces. Currently, you are required to have a single reducer."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was fixed more generally by HADOOP-1216."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-358", "title": "NPE in Path.equals", "status": "Closed", "priority": "Major", "reporter": "Frédéric Bertin", "assignee": "Doug Cutting", "labels": [], "created": "2006-07-10T22:52:31.000+0000", "updated": "2006-08-04T22:22:41.000+0000", "description": "An NPE is raised in Path.equals when testing the method with two unequal pathes and with the first one having no drive.\n\nThis is due to operator precedence: && has a higher priority level than ?:\n\nSee http://java.sun.com/docs/books/tutorial/java/nutsandbolts/expressions.html\n\nSee attached patch (just added some parenthesis and a testcase).", "comments": ["I just committed this.  Thanks, Frédéric."], "derived": {"summary": "An NPE is raised in Path. equals when testing the method with two unequal pathes and with the first one having no drive.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "NPE in Path.equals - An NPE is raised in Path. equals when testing the method with two unequal pathes and with the first one having no drive."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Frédéric."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-359", "title": "add optional compression of map outputs", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-07-12T10:18:47.000+0000", "updated": "2009-07-08T16:51:50.000+0000", "description": "Currently, there is no way to request that the transient data be compressed.", "comments": ["This patch:\n   1. adds mapred.compress.map.outputs variable in the JobConf to request transient data compression.\n   2. extends the unit test to make sure that compression can be turned on or off.\n   3. fixes the local runner to use localizeConfiguration.\n   4. fixes JobConf.newInstance to use setAccessible(true) so that non-public constructors can be called.\n   5. ", "I just committed this.  Thanks, Owen!"], "derived": {"summary": "Currently, there is no way to request that the transient data be compressed.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "add optional compression of map outputs - Currently, there is no way to request that the transient data be compressed."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-360", "title": "hadoop-daemon starts but does not stop servers under cygWin", "status": "Closed", "priority": "Major", "reporter": "Konstantin Shvachko", "assignee": null, "labels": [], "created": "2006-07-12T14:02:50.000+0000", "updated": "2006-08-04T22:22:42.000+0000", "description": "Latest changes HADOOP-352 to the hadoop scripts made them incompatible with cygwin.\nThe servers start fine, but when  I try to stop them the script reports there is nothing to stop. \nWhich in turn messes up consequent server starts.", "comments": ["You should use \"kill -0 `cat $pid`\" instead of \"ps -p `cat $pid`\" in bin/hadoop-daemons.sh", "I just committed the suggested fix.  Konstantin, can you verify whether this works?", "Yes, it works fine now. Thanks.\nDoug, could you also commit HADOOP-344. \nWindows nightly build fails, and not checking other things."], "derived": {"summary": "Latest changes HADOOP-352 to the hadoop scripts made them incompatible with cygwin. The servers start fine, but when  I try to stop them the script reports there is nothing to stop.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "hadoop-daemon starts but does not stop servers under cygWin - Latest changes HADOOP-352 to the hadoop scripts made them incompatible with cygwin. The servers start fine, but when  I try to stop them the script reports there is nothing to stop."}, {"q": "What updates or decisions were made in the discussion?", "a": "Yes, it works fine now. Thanks.\nDoug, could you also commit HADOOP-344. \nWindows nightly build fails, and not checking other things."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-361", "title": "junit with pure-Java hadoopStreaming combiner; remove CRLF in some files", "status": "Closed", "priority": "Major", "reporter": "Michel Tourn", "assignee": null, "labels": [], "created": "2006-07-13T04:29:44.000+0000", "updated": "2009-07-08T17:05:35.000+0000", "description": "junit with pure-Java hadoopStreaming combiner\nremove CRLF in some files\n\nWhen this is committed, the hadoopStreaming tests should be pure-Java.\nThis implies that hadoopStreaming can safely be built nightly without plaftorm dependencies.\n\nAfter I check that this is the case I will update the build.xml in this issue:\nHADOOP-356  \t Build and test hadoopStreaming nightly\n\n", "comments": ["I just committed this.  Thanks, Michel!"], "derived": {"summary": "junit with pure-Java hadoopStreaming combiner\nremove CRLF in some files\n\nWhen this is committed, the hadoopStreaming tests should be pure-Java. This implies that hadoopStreaming can safely be built nightly without plaftorm dependencies.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "junit with pure-Java hadoopStreaming combiner; remove CRLF in some files - junit with pure-Java hadoopStreaming combiner\nremove CRLF in some files\n\nWhen this is committed, the hadoopStreaming tests should be pure-Java. This implies that hadoopStreaming can safely be built nightly without plaftorm dependencies."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Michel!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-362", "title": "tasks can get lost when reporting task completion to the JobTracker has an error", "status": "Closed", "priority": "Major", "reporter": "Devaraj Das", "assignee": "Owen O'Malley", "labels": [], "created": "2006-07-13T05:39:21.000+0000", "updated": "2009-07-08T16:51:50.000+0000", "description": "Basically, the JobTracker used to lose some updates about successful map tasks and it would assume that the tasks are still running (the old progress report is what it used to display in the web page). Now this would cause the reduces to also wait for the map output and they would never receive the output. This would cause the job to appear as if it was hung.\n \nThe following piece of code sends the status of tasks to the JobTracker:\n \n            synchronized (this) {\n                for (Iterator it = runningTasks.values().iterator();\n                     it.hasNext(); ) {\n                    TaskInProgress tip = (TaskInProgress) it.next();\n                    TaskStatus status = tip.createStatus();\n                    taskReports.add(status);\n                    if (status.getRunState() != TaskStatus.RUNNING) {\n                        if (tip.getTask().isMapTask()) {\n                            mapTotal--;\n                        } else {\n                            reduceTotal--;\n                        }\n                        it.remove();\n                    }\n                }\n            }\n \n            //\n            // Xmit the heartbeat\n            //\n           \n            TaskTrackerStatus status =\n              new TaskTrackerStatus(taskTrackerName, localHostname,\n                                    httpPort, taskReports,\n                                    failures);\n            int resultCode = jobClient.emitHeartbeat(status, justStarted);\n \n \nNotice that the completed TIPs are removed from runningTasks data structure. Now, if the emitHeartBeat threw an exception (if it could not communicate with the JobTracker till the IPC timeout expires) then this update is lost. And the next time it sends the hearbeat this completed task's status is missing and hence the JobTracker doesn't know about this completed task. So, one solution to this is to remove the completed TIPs from runningTasks after emitHeartbeat returns. Here is how the new code would look like:\n \n \n            synchronized (this) {\n                for (Iterator it = runningTasks.values().iterator();\n                     it.hasNext(); ) {\n                    TaskInProgress tip = (TaskInProgress) it.next();\n                    TaskStatus status = tip.createStatus();\n                    taskReports.add(status);\n                }\n            }\n \n            //\n            // Xmit the heartbeat\n            //\n \n            TaskTrackerStatus status =\n              new TaskTrackerStatus(taskTrackerName, localHostname,\n                                    httpPort, taskReports,\n                                    failures);\n            int resultCode = jobClient.emitHeartbeat(status, justStarted);\n            synchronized (this) {\n                for (Iterator it = runningTasks.values().iterator();\n                     it.hasNext(); ) {\n                    TaskInProgress tip = (TaskInProgress) it.next();\n                    if (tip.runstate != TaskStatus.RUNNING) {\n                        if (tip.getTask().isMapTask()) {\n                            mapTotal--;\n                        } else {\n                            reduceTotal--;\n                        }\n                        it.remove();\n                    }\n                }\n            }\n ", "comments": ["Your patch has a couple of issues. Here is a trial patch, but it needs a lot more testing. The changes are:\n   1. if the rpc failed after the server updated the status, the status could be updated twice.\n   2. the runningTasks could change from when the status report was generated.", "Thanks Owen for putting this up on Jira! Well the code snippet that I sent you was a quick & dirty hack for the problem I was facing. Of course, yours is a much more elaborate solution. However, with this patch, the problem appears somewhere else - the reduces don't make progress. Even after all maps finish, the reduces remain stuck at 0% progress.\nI haven't yet fully analyzed your patch. I will do that.", "Discovered a minor problem with this patch which caused the status updates to never happen on the job status web page. The call to recomputeProgress for a particular task is conditioned on changedProgress which is true only when at least 0.01 units of progress (in the range 0.0 - 1.0) is seen since the last time progress was reported (by any tasktracker). Changing this figure to 0.00001 solved the problem.", "There is a another aspect/cause of this that we just observed. What we saw was a large job with 5 maps that were not running (all maps were done) and yet all of the reduces were waiting for their output. Closer examination of the log showed that the maps had been logged as complete by the job tracker and were not being run anywhere. The TaskInProgress was showing 100% complete in the web ui. The Task details however was showing 50% and running.\n\nMy best guess as to the failure scenario is:\n\n1. task tracker sends progress of (50%, RUNNING) for map_123 to job tracker\n2. task tracker gets time out on progress message\n3. map_123 finishes\n4. task tracker send progress of (100%, SUCCEED) for map_123 to job tracker\n5. the two messages are taken from the rpc queue and given to separate handler threads\n6. the SUCCEED message thread gets the JobTracker lock and updates the status of the Task and TaskInProgress.\n7. the RUNNING message thread gets the lock and updates the status of the Task\n8. reduces ask for the map output and nothing is available\n\nTherefore, we also need to make sure that Tasks are not allowed to move from SUCCEED or FAILED to RUNNING. That will solve _this_ problem. However, this represents a much deeper and pervasive problem that we will need to address that any two rpc calls from the _same_ thread can be executed in an arbitrary order.", "Will timestamps help for the generic case? So the thread invoking the RPC timestamps the message and when the server handler pulls a job out of the queue for execution, it makes a note of the sender's timestamp. It then looks up a map from client address to client-timestamp and if the current call's timestamp happens to be older than the one found in the map, the server simply ignores that. This means that we don't guarantee  that all calls will be invoked at the server.\nAdditionally, we can have a flag that the client sets forcing the server to execute the call even if it violates the timeliness of the call. This may be required in the DFS operations where a client wants to, lets say, create a file (not absolutely sure whether this is a sensible use-case) but in general this may be helpful.\nBoth of the above can be implemented in the same lines as call-id is handled today (it is a part of each RPC call). In fact, the call-id itself can serve as the timestamp. Makes sense?", "I worry about even relying on timestamps.  In general, in distributed systems, communications can arrive out of order, no?  So it is better to develop protocols that allow for this.", "Yes, that's true. That's why in my proposal (the second part) I allow for this also - the sender of the RPC explicitly flags an RPC message that it should be executed no matter when it is considered for execution. That is, even if a message, m1, is received later than other messages that it sends after sending m1, the server should honor that and execute the RPC.\nSo basically, the client decides whether the server should ignore all (not-yet-executed) RPC requests sent before the current RPC request that the server is executing  OR  the server should execute all RPC requests.\nWe need to serialize the execution of RPC requests based on client addresses (to avoid the problem of multiple requests from the same client getting executed in parallel by different handler threads). This will avoid the race condition for cases like job status updates.\nYes, I agree that this we can implement the above in the protocol itself. I mean each protocol could have a flag signifying either  \"execute all RPCs\" or \"ignore RPC requests with timestamps later than the current RPC's timestamp\". \nSince in the RPC implementation in Hadoop, we have a single client for each protocol, this policy will work I think.", "The issue isn't that messages get handled out of order, but that messages from a single thread are handled out of order. In theory there shouldn't be a second message from the same thread. However, as we observed, it is possible if the first call times out.\n\nSo you wouldn't want to have a global time stamp, just one per a thread on the client. Another approach would be to hash the client thread and only allow a single rpc handler thread to handle rpcs from a single client thread.", "+1 to the second idea (unique hashcode / client thread, forcing serialization the server via mapping it to the same thread ID)\n", "This patch makes the following changes:\n1. Changes the diagnostic info from a Vector to a ArrayList.\n2. Prevents tasks from moving from {FAILED,SUCCEEDED} to RUNNING\n3. Only removes completed tasks from the runningTasks list after it is sure\n     the job tracker got the finished message.\n4. Don't set the job's progress meter to 100% when the job is complete to \n    avoid counting the final reduce's final progress update twice.\n5. Pass the JobTrackerMetrics down into the procedures that actually decide\n    when a task or job is finished to avoid double counting.\n6. Rename some local variables to better reflect their values.", "I just committed this.  Thanks, Owen!"], "derived": {"summary": "Basically, the JobTracker used to lose some updates about successful map tasks and it would assume that the tasks are still running (the old progress report is what it used to display in the web page). Now this would cause the reduces to also wait for the map output and they would never receive the output.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "tasks can get lost when reporting task completion to the JobTracker has an error - Basically, the JobTracker used to lose some updates about successful map tasks and it would assume that the tasks are still running (the old progress report is what it used to display in the web page). Now this would cause the reduces to also wait for the map output and they would never receive the output."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-364", "title": "rpc versioning broke out-of-order server launches", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-07-14T22:10:02.000+0000", "updated": "2006-08-04T22:22:43.000+0000", "description": "The change to check the RPCs broke the ability to bring up the datanodes before the namenode and the tasktracker before the jobtracker.", "comments": ["This patch:\n  1. adds IOException to the getProtocolVersion method. All RPC methods need to declare this exception so that the proxy object can throw IOException if there are communication problems.\n  2. creates a RPC.VersionMismatch exception that can be caught and dealt with in a user friendly way.\n  3. adds IOException to the signature for getProxy.\n  4. adds RPC.waitForProxy that will loop waiting for the server to come up.\n  5. make the TaskTracker and DataNode wait for their masters to come up.", "The patch also extends the mini-mr cluster and mini-dfs cluster classes to take a boolean on the constructor saying to bring up the datanode/tasktracker first. I changed one of the junit tests to bring up the servers backwards to make sure that the fix works.", "I just committed this.  Thanks, Owen!"], "derived": {"summary": "The change to check the RPCs broke the ability to bring up the datanodes before the namenode and the tasktracker before the jobtracker.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "rpc versioning broke out-of-order server launches - The change to check the RPCs broke the ability to bring up the datanodes before the namenode and the tasktracker before the jobtracker."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-365", "title": "datanode crashes on startup with ClassCastException", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-07-14T23:33:16.000+0000", "updated": "2009-07-08T16:41:57.000+0000", "description": "When I bring up a DataNode it gets a ClassCastException:\n\nException in thread \"main\" java.lang.ClassCastException: java.util.HashMap$Entry\n        at org.apache.hadoop.dfs.DataNode.runAndWait(DataNode.java:907)\n        at org.apache.hadoop.dfs.DataNode.main(DataNode.java:954)\n\nThis was introduced by the patch for HADOOP-354.", "comments": ["This fixes the expression to get the Thread out of the MapEntry.", "I just committed this.  Thanks, Owen."], "derived": {"summary": "When I bring up a DataNode it gets a ClassCastException:\n\nException in thread \"main\" java. lang.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "datanode crashes on startup with ClassCastException - When I bring up a DataNode it gets a ClassCastException:\n\nException in thread \"main\" java. lang."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-366", "title": "Should be able to specify more than one jar into a JobConf file", "status": "Closed", "priority": "Major", "reporter": "Thomas Friol", "assignee": "Owen O'Malley", "labels": [], "created": "2006-07-17T12:13:24.000+0000", "updated": "2009-07-08T16:51:50.000+0000", "description": "A job should be able to specify more than one jar file into its JobConf file because sometimes custom Map and Reduce classes or just InputFormat classes uses objects coming from other jar files. For now, we have to build a unique jar to make Hadoop mapreduce operations works.", "comments": ["There is functionality in the system that is close to this functionality. Specifically, you can wrap multiple jars into a single jar in the \"lib\" directory and they will be expanded. So if you have 3 jars map.jar, reduce.jar and format.jar, you could put them together into:\n\n% mkdir lib\n% mv map.jar reduce.jar format.jar lib\n% jar uvf app.jar lib\n\nThat will make all 3 jars (and the rest of your jar) available to the framework. Does that meet your need?", "Owen,\n\nI think you did not understand correctly my explanations. I will try to be more precise.\n\nIn fact, when I am writting a new job using a jobConf.xml file, I am able to define a custom jar using the property 'mapred.jar'. This jar is the uploaded to the DFS and then appended to the Hadoop classpath of each task tracker.\n\nThe exact purpose of my problem is that today, we can specify only one jar and I think it could be great to be able to specify a comma separated list of jars so that we do not have to build a single jar with differents existing jars to give it to each Hadoop task tracker.\n\nAm I more precise now ?\n\nThanks for your help.", "Owen,\n\nMore than one month without news about this bug. What do you think about it ?\n\nThanks for your help.", "In fact Owen's point is to say that you can put jars in the jar.\n\nIs there any reason for this \"hack\" to unpack mapred.jar in TaskRunner and look for a \"lib\" directory containing jars?  Isn't that too much of magic?\n\nWould it be a viable option to allow MapReduce tasks to take a list of jars instead?  Or is there a compelling reason to only give a single Jar?", "A more comprehensive approach to the issue Thomas raises is to use a format that addresses all aspects of Java program execution dependency.   The Java standard has two formats that address this in somewhat different ways:\n\nJNLP  (Java Network Launch Protocol)\nhttp://java.sun.com/j2se/1.4.2/docs/guide/jws/index.html\nUsed by Java Web Start.  Addresses all of the Java platform execution issues, including using native code.\nBut this is probably not the right level for packaing MapReduce modules, but rather should be used to package Hadoop itself.\n\nExtensions \nhttp://java.sun.com/j2se/1.4.2/docs/guide/extensions/index.html\nThis is exactly what Thomas is asking for, and provides the format for included jars and the setting of the classpath.\n\nhttp://java.sun.com/j2se/1.4.2/docs/guide/extensions/spec.html\n\nBundled Optional Packages\n\n The manifest for an application or optional package can specify one or more relative URLs referring to the JAR files and directories for the optional packages (and other libraries) that it needs. These relative URLs will be treated relative to the code base that the containing application or optional package JAR file was loaded from.\n\nAn application (or, more generally, JAR file) specifies the relative URLs of the optional packages (and libraries) that it needs via the manifest attribute Class-Path. This attribute lists the URLs to search for implementations of optional packages (or other libraries) if they cannot be found as optional packages installed on the host Java virtual machine*. These relative URLs may include JAR files and directories for any libraries or resources needed by the application or optional package. Relative URLs not ending with '/' are assumed to refer to JAR files. For example,\n\nClass-Path: servlet.jar infobus.jar acme/beans.jar images/\n\n Multiple Class-Path headers may be specified, and are combined sequentially.\n\n", "I didn't finish that thought...\n\nI also don't think jobs should have this \"external-config-file-plus-jar-file\" structure.  The job should just be a jar file and the configuration contained within it.\n\nThere are several ways to do that.  The one that is the least change to the current implementation is to simply put the JobConf file (sans the mapred.jar property) in the root of the jar file.\n\nPutting a suitable property in the manifest to tag the jar as a Hadoop job file is a good idea too, although it then opens the question of what other properties might belong there as well.\n\n", "The rationale for keeping the config file separate from the jar was that folks might use a single big jar and then submit a sequence of jobs with different parameters, but all of whose code is in the same jar.  The system could be smart about caching the jar on nodes.  Rebuilding a jar for each job submitted seemed heavyweight to me.  In fact, that's what this bug was originally complaining about!\n\nThe lib directory is like that in war files.  So it too has a precedent in Java's standards!", "James you're right, using a Manifest in the JAR is th standard way of doing this, thanks for reminding.  And about parameters, we could just pick the right config file in the JAR by passing a JVM argument:\n\n  java -Dconfig=mybigcluster -jar mapred.jar\n\nOr even using command-line arguments:\n\n  java -jar mapred.jar mybigcluster", "> a Manifest in the JAR is th standard way of doing this\n\nI'd guess there are far more war files with jars in lib directories in the world than there are executable jar files with external jars named in the manifest.  The latter is much more fragile.", "I hear you.  But \"lib\" at the root is not standard WAR, is it?\n\nThe structure is:\n\n{code}\nWEB-INF/\n  |\n  \\__ classes\n  |\n  \\__ lib\n  |  |\n  |  \\__ lib1.jar\n  |  |\n  |  \\__ lib2.jar\n  |\n  \\__ web.xml\n{code}\n\nAnd in web.xml we could define a webapp listener that would be invoked upon webapp startup.", "Sorry, I wasn't arguing that job jar's conform to the war file spec, rather that, if we're looking for sucessful methods of including libraries in jar files, the war file provides a good analogy, one that is more widely used than executable jars.\n\nA job jar is not an executable jar.  It is not standalone: it always gets the hadoop classes externaly, from the environment it is loaded into.  In this way it is like a war file, which always gets servlet runtime from the servlet runner.\n\nA job jar is not a war file: it doesn't specify servlets, include jsp pages, etc.  The reason for the WEB-INF directory in a war file is to hide these files, so that, by default, files in the war are web pages, image files, etc. to be served, unless they're in WEB-INF.\n", "I don't think WAR layout really applies here, and I do think packaging jobs as a JAR make sense.  WAR is for an application, not a library/fragment.  The application packaging I'ld like to see for Hadoop is JNLP (Java Web Start).\n\nFor the situation where common jars are used by multiple job instances, the Extension mechanism supports job jars which are simply a configuration file and dependency(s) on application jars:\n\nhttp://java.sun.com/j2se/1.4.2/docs/guide/extensions/spec.html#deployment\n\nAnd as for job jars being executable or not, they certainly are executables from Hadoop's perspective.  And while Hadoop packaging needs its own issue, I'll point out that putting the dependencies in the job jar to launch Hadoop for the job if it is executed might be a reasonable choice.\n", "Duplicate"], "derived": {"summary": "A job should be able to specify more than one jar file into its JobConf file because sometimes custom Map and Reduce classes or just InputFormat classes uses objects coming from other jar files. For now, we have to build a unique jar to make Hadoop mapreduce operations works.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Should be able to specify more than one jar into a JobConf file - A job should be able to specify more than one jar file into its JobConf file because sometimes custom Map and Reduce classes or just InputFormat classes uses objects coming from other jar files. For now, we have to build a unique jar to make Hadoop mapreduce operations works."}, {"q": "What updates or decisions were made in the discussion?", "a": "I don't think WAR layout really applies here, and I do think packaging jobs as a JAR make sense.  WAR is for an application, not a library/fragment.  The application packaging I'ld like to see for Hadoop is JNLP (Java Web Start).\n\nFor the situation where common jars are used by multiple job instances, the Extension mechanism supports job jars which are simply a configuration file and dependency(s) on application jars:\n\nhttp://java.sun.com/j2se/1.4.2/docs/guide/extensions/spec.html#deployment\n\nAnd as for job jars being executable or not, they certainly are executables from Hadoop's perspective.  And while Hadoop packaging needs its own issue, I'll point out that putting the dependencies in the job jar to launch Hadoop for the job if it is executed might be a reasonable choice."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-367", "title": "Static blocks do not automatically run when a class is loaded in Java 5.0", "status": "Closed", "priority": "Major", "reporter": "Benjamin Reed", "assignee": null, "labels": [], "created": "2006-07-17T22:12:26.000+0000", "updated": "2013-05-02T02:28:59.000+0000", "description": "There seems to be a change that happened between 1.4 and 1.5 with respect to static initializers. I can't find this documented, but I can reproduce with a very simple program. Basically, a static initializer is not called unless a static member/method of the class is accessed or an instance is created. This is actually what the JLS says, but until 1.5 the static initializers ran when the class was loaded. Note that this behavior only occurs when running with the 1.5 JRE AND compiling for 1.5.\n\nFor many Writables this isn't an issue, so the fallback behavior of the WritableFactory works, but Block is package private, so loadEdits fails when called from org.apache.hadoop.io.ArrayWritable.readFields() yielding the following trace:\n\nCaused by: java.lang.RuntimeException: java.lang.IllegalAccessException: Class org.apache.hadoop.io.WritableFactories can not access a member of class org.apache.hadoop.dfs.Block with modifiers \"public\"\n        at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:49)\n        at org.apache.hadoop.io.ArrayWritable.readFields(ArrayWritable.java:81)\n        at org.apache.hadoop.dfs.FSDirectory.loadFSEdits(FSDirectory.java:532)\n        at org.apache.hadoop.dfs.FSDirectory.loadFSImage(FSDirectory.java:470)\n        at org.apache.hadoop.dfs.FSDirectory.<init>(FSDirectory.java:307)\n        at org.apache.hadoop.dfs.FSNamesystem.<init>(FSNamesystem.java:177)\n        at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:91)\n        at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:84)\n        at org.apache.hadoop.dfs.NameNode.main(NameNode.java:491)\n\n\n", "comments": ["Will the setAccessable trick used in JobConf.newInstance() work here too?  I hope it is possible to have non-public classes as parameters in non-public RPC protocols.  For implementation flexibility we'd like to keep some protocols package-protected, so that our only back-compatibility burdens are public, client APIs.", "When would you call setAccessable? It would have to be outside of the class \nsince nothing is initialized at class load time.\n\nOne option would be to put a public static getFactory() method on class that want this non-public behavior. \nNot only would that cause the static blocks to be run when called, but it would allow you \nto get rid of the factory table and just use reflection to call getFactory() \non the class if there is one otherwise fall back to newInstance().", "I was thinking of a hack where we call setAccessible on the constructor before calling it, as is done in JobConf.  We'd bypass the factory altogether.\n\nYour proposal (a public static getFactory method) is better.  Would you like to prepare a patch?", "Arg! It seems that public static methods on non-public classes cannot be called with reflection :( So I took a different approach that is simpler and less code. (This patch removes 132 lines and adds 18.)\n\nI noticed that all of the factories simply created new objects. So now the WritableFactories.newInstance method finds the factory that corresponds to the package of the class and lets that factory create it. This means that there needs to be a WriteableFactory for each package that has Writables. This lowers the number of overall factories while adding one visible factory per package. It also simplifies the Writables since they don't need to worry about factories anymore.", "Oops.  Mistakenly resolved.", "We might as well just use setAccessible.  Let's just add a static util method:\n\npublic Object newInstance(Class aClass);\n\nThis can can be implemented like JobConf.newInstance().  Then JobConf.newInstance() can call this, and we can replace all calls to WritableFactories.newInstance() with calls to this.  If we ever become worried about abuse of this method, we can restrict its use to particular, approved packages or somesuch.\n\nThis would be even less code!", "This patch provides a util function newInstance. It's an incomplete patch to this bug. Still need to clean up all the factories.", "I committed the reflection patch, but am not yet closing this.\n\nHairong, will you provide a patch replacing WritableFactory with ReflectionUtils?", "\n   [[ Old comment, sent by email on Tue, 25 Jul 2006 10:30:37 -0700 ]]\n\nWhen would you call setAccessable? It would have to be outside of the class \nsince nothing is initialized.\n\nOne option would be to put a public static getFactory() method on the class. \nNot only would that cause the static blocks to be run, but it would allow you \nto get rid of the factory table and just use reflection to call getFactory() \non the class if there is one otherwise fall back to newInstance().\n\nben\n\n", "This has been fixed across several patches by using the new ReflectionUtils.newInstance in all contexts. We currently require java 5.0, so clearly this bug isn't present anymore."], "derived": {"summary": "There seems to be a change that happened between 1. 4 and 1.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Static blocks do not automatically run when a class is loaded in Java 5.0 - There seems to be a change that happened between 1. 4 and 1."}, {"q": "What updates or decisions were made in the discussion?", "a": "This has been fixed across several patches by using the new ReflectionUtils.newInstance in all contexts. We currently require java 5.0, so clearly this bug isn't present anymore."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-368", "title": "DistributedFSCheck should cleanup, seek, and report missing files.", "status": "Closed", "priority": "Minor", "reporter": "Konstantin Shvachko", "assignee": "Konstantin Shvachko", "labels": [], "created": "2006-07-18T02:26:33.000+0000", "updated": "2006-08-04T22:22:44.000+0000", "description": null, "comments": ["1) I included calls to cleanup() for DistributedFSCheck and TestDFSIO.\nDistributedFSCheck calls cleanup before and after execution of the main part.\nAfter - in order to restore the files system state.\nBefore - in order to be able to run if the previous run failed.\nTestDFSIO cleans up only within the unit test.\n2) DistributedFSCheck snapshots files at the initial state and then reads them.\nI added correct processing of files that have been snapshot but disappeared after that.\n3) A bug fixed in DistributedFSCheck. The mapper did not seek to the required\noffset when reading files.\n", "I just committed this.  Thanks, Konstantin."], "derived": {"summary": "", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "DistributedFSCheck should cleanup, seek, and report missing files."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Konstantin."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-369", "title": "Added ability to copy all part-files into one output file", "status": "Closed", "priority": "Trivial", "reporter": "Johan Oskarsson", "assignee": "Johan Oskarsson", "labels": [], "created": "2006-07-18T16:39:26.000+0000", "updated": "2009-07-08T16:41:58.000+0000", "description": "Since we use the hadoop output in non-hadoop applications it's nice to be able to merge the part-files into one output file on the local filesystem.\nSo I've added a dfsshell feature that streams from all files in a directory to one output file.", "comments": ["See description above.", "I don't see why this needs to be a new generic FileSystem method.  Can't it simply be a FileUtil method?\n\nAlternately, couldn't we just modify DFSShell.cat(String) to, when it's argument is a directory, recursively descend and copy all contained file content to standard output?  Then one could just 'bin/hadoop dfs cat foo > bar' to append all content under 'foo' into a local file 'bar'.  Would that work for you?", "I've added two patches, one is the requested cat feature (cat whole directory)\n\nHowever, a simple test shows that this is very very much slower then saving it through a filestream.\nWhy I do not know :)\n\nSo I've changed the copymerge patch as suggested and uploaded the new patch.", "*bump*\nAny reason why the revised copymerge patch would not be suitable for commit?\nIf so I'd like to fix it asap, this is a feature we need badly.", "I just committed this.  Thanks, Johan!"], "derived": {"summary": "Since we use the hadoop output in non-hadoop applications it's nice to be able to merge the part-files into one output file on the local filesystem. So I've added a dfsshell feature that streams from all files in a directory to one output file.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Added ability to copy all part-files into one output file - Since we use the hadoop output in non-hadoop applications it's nice to be able to merge the part-files into one output file on the local filesystem. So I've added a dfsshell feature that streams from all files in a directory to one output file."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Johan!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-371", "title": "ant tar should package contrib jars", "status": "Closed", "priority": "Major", "reporter": "Michel Tourn", "assignee": "Nigel Daley", "labels": [], "created": "2006-07-18T18:51:43.000+0000", "updated": "2007-02-03T03:26:57.000+0000", "description": "From Hadoop-356:\n>I note that the contrib packages are not included in distributions (the \"tar\" target).  \n>They probably should be.  Michel, would you like to modify \"tar\" to include the contrib code?  \n>This should be done in a separate bug.\n\nOK.\nThis packaging is done in target deploy-contrib.\nSo I can just add a dependency to the top-level tar target:\n<!-- Make release tarball(s)                                               -->\n<target name=\"tar\" depends=\"package, deploy-contrib\">\n\n\n", "comments": ["I was thinking we should include the contrib jar files and documentation.  The jar files and readme files could go in something like:\n\n contrib/\n    streaming/\n      streaming.jar\n      readme.txt\n\nat the top level in the tar file.\n\nThe javadoc could be built into the main javadoc tree, but in a separate section, like Lucene:\n\nhttp://lucene.apache.org/java/docs/api/overview-summary.html\n\nDoes this sound reasonable?", "OK to keep docs and readme separate in the tarball package.\n\nBut I thought that the way you get all the code on the CLASSPATH in  bin/hadoop:\nis by placing the contrib jar-s along with the main hadoop.jar..\n\n\n", "> I thought that the way you get all the code on the CLASSPATH in bin/hadoop:\n> is by placing the contrib jar-s along with the main hadoop.jar.\n\nThe contrib and example code should not be on the CLASSPATH by default, but rather should be run using the 'bin/hadoop jar ...' command.", "The attached patch to build.xml does 2 things:\n\n1) Modifies the javadoc target to include the contrib javadoc along with the regular javadoc.  \nThe overview-summary.html is created to mimic lucene's: \nhttp://lucene.apache.org/java/docs/api/overview-summary.html\nSuggestion: perhaps the contrib javadoc should be entirely \nseparate from the core javadoc, say in\n     contrib/docs ???\n\n2) Modifies the package target to include the contrib jar files in\n     contrib/streaming/hadoop-streaming.jar\n     contrib/smallJobsBenchmark/MRBenchmark.jar\nSuggestion: perhaps the contrib jar files should be in either\n     contrib/lib/hadoop-streaming.jar\n     contrib/lib/MRBenchmark.jar\nor\n     contrib/hadoop-streaming.jar\n     contrib/MRBenchmark.jar   ???\n\nAny thoughts on the above suggestions or other alternatives?", "I think I'll propose a different solution that expands a bit on the scope of this bug to include consolidating benchmarks and reordering javadoc:\n\n1) consolidate benchmarks into src/test/org/apache/hadoop/benchmark and include them in the test jar file.  This includes:\n    - src/contrib/smallJobsBenchmark\n    - src/examples/org/apache/hadoop/examples/NNBench.java\n\n2) rename PiBenchmark example to PiEstimator and leave it in the examples package\n\n3) build hadoop-streaming.jar into a top-level extensions directory using the package target\n\n4) package the contrib javadoc into docs/contrib and package the example javadoc into docs/examples\n\n5) create a top-level index.html that points to\n    - hadoop api javadoc\n    - examples javadoc\n    - contrib javadoc\n    - existing docs/hadoop-default.html\n    - man pages for our command line utilities (would need to write these)\n    - whitepapers\n    - hadoop wiki\n    - etc.\n\nComments?", "I believe smallJobsBenchmark was initially placed in contrib to keep it in a separate jar, not on the default classpath.  So wherever it is moved, it should retain that property.\n\nI would prefer putting the contrib javadoc together with the main javadoc, as is done in Lucene:\n\nhttp://lucene.apache.org/java/docs/api/overview-summary.html\n\nAs for a top-level index.html, should that really differ from site/index.html?  That's included in releases and links to everything you mention, I think.  Perhaps we should simply add an index.html with a meta-redirect to that page.  Lucene does this:\n\nhttp://svn.apache.org/viewvc/lucene/java/trunk/index.html?view=markup\n\n(Like the last-modified date on that one?)\n", "Here's an updated proposal for restructuring the hadoop packaging: \n\n1) consolidate benchmarks into src/test/org/apache/hadoop and include them in the test jar file: \n    - move src/contrib/smallJobsBenchmark to src/test/org/apache/hadoop/mapred\n    - move src/examples/org/apache/hadoop/examples/NNBench.java to src/test/org/apache/hadoop/dfs\n\n2) rename PiBenchmark example to PiEstimator and leave it in the examples package \n\n3) build hadoop-streaming.jar into a top-level contrib directory using the package target \n\n4) package the contrib and examples javadoc with the core javadoc and provide an overview-summary.html like lucene's: \nhttp://lucene.apache.org/java/docs/api/overview-summary.html\n\n5) include the site docs into the top-level docs directory\n\nMore comments?", "371.patch addresses all issues in my Dec 14 comment except one.  I have not moved src/contrib/smallJobsBenchmark -- I will open a new issue for this item.  This patch also fixes some streaming javadoc comments that were causing warnings.\n\nBefore committing 371.patch, you must perform the following svn commands:\n\nsvn mv src/examples/org/apache/hadoop/examples/NNBench.java src/test/org/apache/hadoop/dfs/NNBench.java\nsvn mv src/examples/org/apache/hadoop/examples/PiBenchmark.java src/examples/org/apache/hadoop/examples/PiEstimator.java\n\nThe patch will then fix up their package and class names.", "-1, because the patch command could not apply the latest attachment (http://issues.apache.org/jira/secure/attachment/12348237/371.patch) as a patch to trunk revision r492365. Please note that this message is automatically generated and may represent a problem with the automation system and not the patch.", "The -1 comment from Hadoop QA is expected as this patch cannot be applied until the noted svn mv commands are run.", "I just committed this.  Thanks, Nigel!\n\nI made two small additional changes: I changed the links to the javadoc from api/ to api/index.html, so that they work offline.  I also added a root index.html file that meta redirects to docs/index.html."], "derived": {"summary": "From Hadoop-356:\n>I note that the contrib packages are not included in distributions (the \"tar\" target). >They probably should be.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "ant tar should package contrib jars - From Hadoop-356:\n>I note that the contrib packages are not included in distributions (the \"tar\" target). >They probably should be."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Nigel!\n\nI made two small additional changes: I changed the links to the javadoc from api/ to api/index.html, so that they work offline.  I also added a root index.html file that meta redirects to docs/index.html."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-372", "title": "should allow to specify different inputformat classes for different input dirs for Map/Reduce jobs", "status": "Closed", "priority": "Major", "reporter": "Runping Qi", "assignee": "Chris Smith", "labels": [], "created": "2006-07-19T18:02:05.000+0000", "updated": "2013-05-02T02:29:00.000+0000", "description": "Right now, the user can specify multiple input directories for a map reduce job. \nHowever, the files under all the directories are assumed to be in the same format, \nwith the same key/value classes. This proves to be  a serious limit in many situations. \nHere is an example. Suppose I have three simple tables: \none has URLs and their rank values (page ranks), \nanother has URLs and their classification values, \nand the third one has the URL meta data such as crawl status, last crawl time, etc. \nSuppose now I need a job to generate a list of URLs to be crawled next. \nThe decision depends on the info in all the three tables.\nRight now, there is no easy way to accomplish this.\n\nHowever, this job can be done if the framework allows to specify different inputformats for different input dirs.\nSuppose my three tables are in the following directory respectively: rankTable, classificationTable. and metaDataTable. \nIf we extend JobConf class with the following method (as Owen suggested to me):\n    addInputPath(aPath, anInputFormatClass, anInputKeyClass, anInputValueClass)\nThen I can specify my job as follows:\n    addInputPath(rankTable, SequenceFileInputFormat.class, UTF8.class, DoubleWritable.class)\n    addInputPath(classificationTable, TextInputFormat.class, UTF8,class, UTF8.class)\n    addInputPath(metaDataTable, SequenceFileInputFormat.class, UTF8.class, MyRecord.class)\nIf an input directory is added through the current API, it will have the same meaning as it is now. \nThus this extension will not affect any applications that do not need this new feature.\n\nIt is relatively easy for the M/R framework to create an appropriate record reader for a map task based on the above information.\nAnd that is the only change needed for supporting this extension.\n\n\n\n\n\n", "comments": ["Can you provide more details?  Is the intent for the mapred.input.format.class property to become multivalued, a parallel list to mapred.input.dir, and when the latter is longer than the former, the first (or last?) input format is used for unmatched entries?  I can imagine how MapTask might create its keys, values and a RecordReader, but how would getSplits() and checkInputDirectories() work?\n\nAnother approach to implementing this is to write an InputFormat that wraps keys and/or values from files of different types in ObjectWritable.  Then map() methods unwrap, introspect and cast.  With your approach map methods still need to introspect and cache, this just adds the wrapper.\n\nTo eliminate the wrapper we'd need to move the getInputKeyClass() and getInputValueClass() methods to RecordReader.  These are only called in MapRunner.java, when a RecordReader is already available, so this would be an easy change, and the default implementation could be back-compatible, accessing the job.\n\nThat's a simpler approach, no?  Just add files with different keys and value types, and let the types in the files drive things rather than having to declare them up front.", "\nDoug,\n\nMy thought is to add a Map object to the JobConf class that keep track the explicit association between input path and the classes (inputformat, key and value). If there is no entry in the Map object for a Path, then the path is associated with the classes set through the current APIs (setInputFormatClass(), setInputKeyClass, setInputValueClass) (or default classes if they are not set). It will be convenient if the JobConf class provides APIs for getting/setting them:\n\n    class getInputFormatClassForPath(Path p) \n    class getInputKeyClassForPath(Path p) \n    class getInputValueClassForPath(Path p) \n\nIt is a good idea to move  the getInputKeyClass() and getInputValueClass() methods to RecordReader (and this should be more logical!).\nThis is easy to achieve since the implementation of the method  of InputFormat Interface \n               getRecordReader FileSystem fs, FileSplit split, JobConf job, Reporter reporter) \nhas enough information to extract the key/value classes for the split.\n\nIt is also convenient to let Split class keep track for the inputformat, key, value class, so that we can get the right RecordReader for a given split.\n\nIn MapTask class, use  the following lines:\n     final RecordReader rawIn =                  // open input\n        split.getInputFormat().getRecordReader\n        (FileSystem.get(job), split, job, reporter);\nto replace\n     final RecordReader rawIn =                  // open input\n        job.getInputFormat().getRecordReader\n        (FileSystem.get(job), split, job, reporter);\n\nFinally, the the getSplits of InputFormatBase class should be changed to a static method, since it is independent of any concrete InputFormat implementation (and this is kind of necessary, since the exact inputformat will not be known prior to creating splits). The initTasks() method of \nJobInProgress class needs to make sure all the inputformat classes are loaded properly, or let getSplits() method to take care of it.\n\nThat should have covered most of the needed changes\n", "\nSecond thought about the stuff related to InputFormat after chatting with Owen. The getSplits should stay as non-static, but should take\na inputPathDir as a parameter. The initTasks should load all the \nclasses and create an InputFormat object per class. For each inputPathDir, the initTasks should call the splits method against the corresponding InputFormat object, and finally union all the splits into one list.\n", "> My thought is to add a Map object to the JobConf class [ ...]\n\nJobConf, underneath, is a Properties instance, mapping String->String.  Adding a map to it is a non-trivial extension to the configuration mechanism.\n\nI didn't hear a response to my suggestion that we instead simply move the getInputKeyClass() and getInputValueClass() methods to RecordReader.  Would that not meet your needs?\n", "Doug,\n\nI did respond your suggestion in my previous comment (it is in the middle of the text, thus may be easy to overlook:):\n\n>It is a good idea to move the getInputKeyClass() and getInputValueClass() methods to RecordReader (and >this should be more logical!). \n>This is easy to achieve since the implementation of the method of InputFormat Interface \n>              getRecordReader FileSystem fs, FileSplit split, JobConf job, Reporter reporter) \n>has enough information to extract the key/value classes for the split. \n\nI think String-->String mapping is enough for encoding the mapping from inputpathdirs to classes (same as how we do with the input key/value classes, except with different attribute names).\n", ">It is a good idea to move the getInputKeyClass() and getInputValueClass() methods to RecordReader\n\nAnd would this change alone not resolve this issue for you?  You could intermix input files with different types in a job, and specify no input key or value classes.  The InputFormat would determine the types of the inputs.  For SequenceFiles, the types are named in the file, for text files they are fixed.  It would not allow you to, e.g., intermix text and SequenceFile inputs, but that's not what you've asked for here, and that could easily be achived through a dispatching InputFormat.", "I like the idea of moving the getInput{Key,Value}Class to the RecordReader, since there is really no need for the user to specify it except as potential error checking.\n\nThe desire is to do joins across different kinds of tables. So if you have two tables that both contain urls, you could write a mapper for each table and generate the join via having the url being the map output key. I agree that you could implement it by using ObjectWritable to wrap all of the input types, but you'll end up using a lot of \"instanceof\" to figure out type dynamically and dispatching to the right kind of map.\n\nI would propose being able to add input directories with specific processing chains:\n\nJobConf conf = new JobConf();\nconf.addInputPath(new Path(\"dir1\");\nconf.addInputPath(new Path(\"dir2\");\nconf.addInputPath(new Path(\"my-input\"), MyInputFormat.class, MyMapper.class);\nconf.addInputPath(new Path(\"other-input\"), OtherInputFormat.class, OtherMapper.class);\n\nSo \"dir1\" and \"dir2\" would be processed with the default InputFormat and Mapper.\n\"my-input\" would be processed with MyInputFormat and MyMapper and \"other-input\" would be processed with OtherInputFormat and OtherMapper.\n\nThis allows it to be backward compatible with a minimum of fuss for users that don't want to use multiple input sources.\n\nClearly, you would want to encode the class names with the paths when you were encoding the strings.", "It seems like a lot of work to avoid instanceof in one's map method.  We could instead add a method like:\n\nMapper getMapper(Class keyClass, Class valueClass);\n\nI also still worry that a static split method won't provide enough flexibility.  If a job is asked to create N splits, how will these be apportioned to the different inputs?  By number of directories, files, bytes, or what?\n", "\nIt would be great if we make it optional for the user to specify key/value classes for the inputs.\nTheses classes, if specified by the user, would be used for validation check only. That way, the framework will get the input key/value class info from the the input files alone. That will be very robust. \n\nAssuming that all records in each split will have the same key/value classes, then the framework can determine the appropriate mapper class upfront, and call the map function of that mapper class, thus the map function does not need to call instanceof method. \n\nThe split will not be static. The only constraint is that a split will not span across multiple files, which is true now.\n  ", "I think this issue is now mostly duplicated by HADOOP-450, which Owen is working on.  If there are no objections, I will close this issue.\n\nPerhaps we should also add a new issue related to supporting different map functions for different input directories.", "HADOOP-450 certainly made this easier, but it doesn't really solve the problem. However, I think that with an implementation of HADOOP-451 (abstract input splits), we could do this all in library code. Something along the lines of:\n\npublic class MultiInputPath {\n  /* must be called after the InputFormat and MapRunner have been set in the JobConf */\n  public static void addHooks(JobConf conf);\n\n  /* add a new input directory and the relevant input format and mapper classes */\n  public static void addInputPath(conf, Path inputDirectory, Class InputFormat, Class Mapper);\n}\n\nThe MultiInputPath will wrap the user's InputFormat, InputSplit, and MapRunner. In particular, the InputSplit will contain the user's InputSplit as well as the InputFormat and Mapper classes.  \n\nThis will pull all of the specialized code into a library instead of the framework, which is a big win.\n\nThoughts?", "\nThe patch for Hadoop-450 laid the foundation for this issue. But the specific aspects of this issue are yet to be addressed, and I started to work on a patch for it. The patch is to:\n        * allow the user to specify different input format classes for different input directories\n        * allow the user to specify different mapper classes for different key/value class pair.\n\nMy thought is to extend the JobConf class with the following methods:\n\n        public void setInputFormatClass(Class theInputFormatClass, Path p) \n\nfor specifying different input format classes\nfor specifying different mapper classes.\n\nThe FileSplit class should be extended to have a method:\n         public Class getInputFormatClass() \n\nThe initTasks method of JobInProgress should make due changes to create file FileSplit objects with the correct input format class information. \n        \n\nFor supporting different mapper classes, we can expend the JobConf class:\n\n        public void setMapperClass(Class theMapperClass,  Class theKeyClass, Class theValueClass) \n        public Class getMapperClass(Class theKeyClass, Class theValueClass) \n\nThe idea is that, for each split, we know the input format class, from there, we know the corresponding record reader, then we know the key/value classes of the input records.\n\nAnother possibility is to allow the user to specify a mapper class per input path, in the same way as for the input format class. To do that, the FileSplit class needs to support the following method:\n        public Class getMapperClass()\n\nThoughts?\n \n", "I really think this should be done outside of the framework without adding 4 new fairly complicated public methods into the JobConf.\n\nI also don't like using types to distinguish which Mapper to use. A typical case will be two directories using TextInputFormats where you want to apply separate Mappers to pull out different fields. I think it is much nicer to define the entire pipeline, rather than using types to infer it.\n\nInputSplit (from an input directory) -> RecordReader (via InputFormat) -> Mapper\n\n", "Ok, let me modify this a bit. How about if we define a new class that defines a map input processing pipeline. Each pipeline is given the input directory and the JobConf when it is created and then gets to pick the appropriate InputFormat and Mapper classes.\n\npublic class InputPipeline {\n  public InputPipeline(Path inputDir, JobConf conf);\n  public Class createInputFormat();\n  public Class getMapper();\n  public int getRequestedNumMaps();\n}\n\nThe JobConf then picks up 2 methods:\n\nJobConf:\n   public void setInputPipelineClass(Class cls);\n   public Class getInputPipelineClass();\n\nThe default will be InputPipeline that just uses the values from the JobConf for all paths.\n\nThe framework changes are pretty light. Just creating the InputPipeline when iterating through the input directories and using that to create the splits. We need to add the InputFormat and Mapper classes to the MapTask and make MapTask.localizeConfiguration set the Mapper class. \n\nTo complete the picture, I'd also add a class in org.apache.hadoop.mapred.lib that looks like:\n\npublic class MultiInputPipeline extends InputPipeline {\n  public static void addInputPipeline(JobConf conf, Path inputDir, Class inputFormat, Class mapper, int numMaps);\n  ...\n}\n\nThat makes this look like the other hooks that are currently in Hadoop and provides the flexibility that the users need.", "I'm having trouble seeing how this is used.  Can you provide some pseudo application code?\n\nWithout understanding it, the API looks ugly.  There's a lot of meta-stuff going on--passing classes around rather than instances.  Your prior proposal was much more straightforward: RecordReader creates the Mapper.  What problem with that are you now trying to solve?\n\n", "\nI think it may be better to make InputPipeline as an interface:\n\npublic interface InputPipeline { \n  public Class createInputFormat(Path inputDir, JobConf conf); \n  public Class getMapper(Path inputDir, JobConf conf); \n  public int getRequestedNumMaps(Path inputDir, JobConf conf); \n} \n\nIn order to customizing input processing pipeline, the user is expected to provide a class\nthat  implements the interface:\n\npublic class MyInputPipeline implements InputPipeline { \n  public MyInputPipeline(); \n  public Class getInputFormat(Path inputDir, JobConf conf); \n  public Class getMapper(Path inputDir, JobConf conf); \n  public int getRequestedNumMaps(Path inputDir, JobConf conf); \n} \n\nThe class provides a argumentless constructor, and implements the logic to \ndetermine an InputFormat class and a mapper class that apply to the data under the \nspecified input directory, and determine\nthe expected number of mappers for the data under the directory.\n\nThe access to this class is through two new methods of JobConf:\n\n   public void setInputPipelineClass(Class cls); \n   public Class getInputPipelineClass(); \n\nTo specify such a class, the user just simply call:\n   myJob.setInputPipelineClass(MyInputPipeline.class)\n\n\nThe initTasks method of JobInProgress class can do something like the following:\n\n       String jobFile = profile.getJobFile();\n\n        JobConf jd = new JobConf(localJobFile);\n        FileSystem fs = FileSystem.get(conf);\n        String userPipelineClassName = jd.get(\"mapred.input.pipeline.class\");\n        InputPipeline  inputPipeline ;\n        if (userPipelineClassName != null && localJarFile != null) {\n          try {\n            ClassLoader loader =\n            new URLClassLoader(new URL[]{ localFs.pathToFile(localJarFile).toURL() });\n            Class inputPipelineClass = Class.forName(userPipelineClassName , true, loader);\n            inputPipeline  = (InputPipeline )inputPipelineClass.newInstance(); \n          } catch (Exception e) {\n            throw new IOException(e.toString());\n          }\n        } else {\n          inputPipeline = jd.getInputPipeline();\n        }\n        ArrayList allSplits;\n        InputFormat inputFormat;\n        InputFormat genericInputFormat;\n        Class mapperClass;\n        Class genericMapperClass;\n        int numMapTasks;\n        // code to get the generic input format, mapper class for the job\n        \n        Path[] dirs = job.getInputPaths();\n        for (int i = 0; i < dirs.length; i++) {\n                inputFormat = genericInputFormat;\n                if (inputPipeline != null) {\n                     Class inputFormatClass = inputPipeline.getInputFormat(dirs[i], jd);\n                \n                     if (inputFormatClass != null) {\n                            inputFormat  = (InputFormat)inputFormatClass.newInstance();\n                    } \n                    mapperClass = inputPipeline.getMapper(dirs[i], jd);\n                    if (mapperClass == null) {\n                          mapperClass = genericMapperClass;\n                    }\n                    int numMapTasks = inputPipeline.getRequestedNumMaps(dirs[i], jd);\n                     \n                }\n                FileSplit[] splits = inputFormat.getSplits(fs, jd, numMapTasks);\n                // add the new splits to allSplits\n                ....\n            }\n\n        FileSplit[] splits = inputFormat.getSplits(fs, jd, numMapTasks);", "\nOne correction on my previous comment: InputFormat should extend with the following method:\n\n    FileSplit[] getSplits(FileSystem fs, JobConf jd, Path p, int numMapTasks); \n\nand the initTasks method should call:\n    FileSplit[] splits = inputFormat.getSplits(fs, jd, dirs[i], numMapTasks); \n                ", "The question is how the user requests multiple InputPipelines. Either they do:\n\njob.setInputPipeline(MyInputPipelineFactory.class);\n\nor \n\njob.addInputPath(myInputDir, MyInputFormat.class, MyMapper.class, numMaps);\n\nSetting a class is more similar to the other hooks to control processing in the JobConf, but the expanded addInputPath is more user friendly. I guess you're right that it is better to be friendly. *smile*\n\nWhether the InputFormat's and Mapper's class is stored in the MapTask or FileSplit is a separate question. I think it makes more sense to put into the MapTask since that is a private type and doesn't change an API. Especially when you consider that we are going to generalize the FileSplit to InputSplit.\n\nFinally, I think that getSplits should take a Path[] rather that just a path so that we can split evenly over a set of input paths. So I propose getSplits(FileSystem, Path[], JobConf, int numMaps).\n\nIn the long run, as we move Path's over to URL's that include their FileSystem we should drop explicit FileSystem parameters like this. But that is another patch. *grin*", "> job.addInputPath(myInputDir, MyInputFormat.class, MyMapper.class, numMaps); \n\nWhy can't this simply be\n\njob.addInputPath(myInputDir, MyInputFormat.class);\n\nand let the mapper be determined by the inputformat?\n\nI also don't think that users should typically be specifying the number of map tasks.  Their code should run unchanged on clusters of various sizes.  The number of map tasks should thus be determined dynamically when splitting.\n\nI also think the above should simply be syntactic sugar for:\n\njob.setInputFormat(MyInputFormat.class)\njob.set(\"my.input.dirs\", \"foo,bar\");\njob.set(\"my.mappers\", \"FooMapper,BarMapper\");\n\nThen implement MyInputFormat to process my.input.dirs and my.mappers.\n\nIn other words, we should foremost first make it possible to do this in user code, then perhaps provide some utilities that simplify things.  So the first patch I'd like to see is mappers from record readers.  Then we can write some InputFormats that use these new features and perhaps promote some of them into the standard public APIs.  Does this make sense?  Am I missing something?", "The problem with that is often I won't want to use a non-standard InputFormat. A very typical case is to have the same input format, but different Mappers.\n\njob.addInputPath(\"foo\", TextInputFormat.class, FooMapper.class);\njob.addInputPath(\"bar\", TextInputFormat.class, BarMapper.class);\n\nThe same applies to SequenceFileInputFormat (or combinations of the two), especially now that the input key/value types are created by the RecordReader. \n\n", "> A very typical case is to have the same input format, but different Mappers\n\nBut, if the mapper is a function of the input format this can instead be:\n\njob.addInputPath(\"foo\", FooInput.class);\njob.addInputPath(\"bar\", BarInput.class);\n\nWhere FooInput is defined with something like:\n\npublic class FooInput extends TextInput {\n  public void map(...) { ... };\n}\n\nIn other words, if you're going to define custom mappers anyway, then it's no more work to define custom Input formats.", "The issue as stated in the title is an important useful feature on it's own.\n\nHowever the actual problem that motivated it, as described in the original post, does not necessarily imply the suggested solution.\nThe problem Runping describes looks like a case of Join to me. \nThe suggested solution would help to implement a work-aorund using MapReduce.  \nBut is this the best way of allocating development resources?  \nImplementing Join  using    MultiJobInput   in Map step  and SplitIterator  in Reduce step  is a temporary work-around solution.\nWould it make sense to define interfaces in terms of actual problem (Join)?   In short term it still may be implemented piggy-backing MapReduce using two-level keys and MultiJobInput.   But the user would be able to think in more appropriate terms.", "Support for \"all-pairs\" joins is a far-reaching requirement that I don't touch here. \n\nBut I agree with Doug's last comment:\n\n>In other words, if you're going to define custom mappers anyway, \n>then it's no more work to define custom Input formats.\n\nMoreover, unless I am missing something, the current APIs already nicely address the requirement in the JIRA issue title.\n\nJobConf.addInputPath(Path dir)\n\nJobConf.setInputFormat(Class theClass)\n\nInputFormat {\n  FileSplit[] getSplits(FileSystem, JobConf, preferredNumSplits)\n  RecordReader getRecordReader(FileSystem fs, FileSplit split,\n                               JobConf job, Reporter reporter)\n  ...\n}\n\nGiven this current API the flow looks like this:\n\nDuring Task execution ( InputFormat.getRecordReader() ):\n\ntaks's FileSplit + job's single InputFormat  -> Path context -> inputDirectory context --> dispatched \"sub\" InputFormat --> getRecordReader --> RecordReader instance.\n\n\nDuring JobTracker splits computation ( InputFormat.getSplits() ):\n\njob's single InputFormat  + job's list of input directories --> list of input dirs/files --> list of sub-InputFormat-s --> dispatch and \"aggregate\" the results from your sub-InputFormat-s .getSplits()\n\n\nThis is enough to implement the special case discussed in the HADOOP-372 title:\n\nInputDirectory  --> InputFormat\n\n\nA framework class or the examples or the Wiki FAQ could demonstrate how one can write such a *generic* dispatching  class:\n\n\nclass DispatchInputFormat(InputFormat[], JobConf)   implements InputFormat\n\n\nIt is generic but not universal.\ndifferent applications will need to use different information to make the InputFormat dispatch / aggregation decisions.\n\nNow three ways to customize this DispatchInputFormat.\n\n1./3. Writing zero java code:\n\nreplace:\n>job.addInputPath(\"foo\", FooInput.class); \nwith:\njob.set(\"DispatchInputFormat.inputdirmap\", \"foo=org.example.FooInput bar=org.example.BarInput\")\n\nIf you want client-side type checking for the classnames, do it in a helper method.\nFor example:\n\nstatic void DispatchInputFormat.addDir(\n  JobConf, Path dir, Class<InputFormat> clazz)\nCall:\nDispatchInputFormat.add(job, new Path(\"foo\"), FooInput.class);\nDispatchInputFormat.add(job, new Path(\"bar\"), BarInput.class);\n\nWhere Class<InputFormat> uses Generics to enforce at compile-time that FooInput implements InputFormat. \n\n\n2./3. code reuse without copy-paste\n\nA few well-placed hooks could allow users to reuse and customize the DispatchInputFormat code without duplicating it:\n\nclass MyInputFormat extends DispatchInputFormat {\n\n  //override\n   protected InputFormat inputDirToFormat(Path inputDir) {\n     ...\n   } \n}\n\n3./3. code reuse with copy-paste\n\nAnd for more complex requirements that do not fit well with inputDirToFormat():\n\none would instead use DispatchInputFormat as a starting point, a source code example.\n\n", "We have a desperate need to be able to specify different inputformat classes, mappers, and partition functions in the same job. Our need roughly corresponds to this issue. However, we do not assume that a given input will need to be processed by only one inputformat, mapper, or partition function.\n\nI think Owen was right about directing the discussion towards HADOOP-451. If HADOOP-451 was fixed, we (meaning my project) would not have any issues in this area. We actually use a similar syntax to the one proposed by Michel: job.set(\"DispatchInputFormat.inputdirmap\", \"foo=org.example.FooInput bar=org.example.BarInput\"), but imagine if we get job.set(\"DispatchInputFormat.inputdirmap\", \"foo=org.example.FooInput foo=org.example.BarInput\"), now foo must be processed by both FooInput and BarInput. When the splits are created and sent to the mappers, the DispatchInputFormat will get {\"foo\", offset, length}, but it has no way of knowing wether to apply FooInput or BarInput. With HADOOP-451 fixed, we could encode the InputFormat to use in the split.", "Ok, Doug, Runping & I chatted offline over a whiteboard and here is a proposal that I believe captures something that will work:\n\npublic class InputHandler {\n  public List<FileSplit> getSplits(JobConf);\n  public RecordReader getRecordReader(JobConf, FileSplit);\n  public Mapper getMapper(JobConf, FileSplit, RecordReader);\n}\n\nwhere InputHandler is a generalization of the current InputFomat. It is clearly possible to write user code then that handles multiple input directories with different RecordReaders and Mappers, but it isn't a huge jump from where we are now.", "I like this proposal. Assuming that the RecordReader in getMapper is the really record reader (ie the one returned from getRecordReader) it would address HADOOP-433.\n\nHow would this interact with the ability to define a MapRunnable?", "> How would this interact with the ability to define a MapRunnable?\n\nGood question.  Perhaps this interface should return a MapRunnable rather than a Mapper?\n\nAlso, note that the uses of FileSplit will be generalized to the more abstract Split by HADOOP-451.\n\nFinally, I'd prefer the name \"JobInput\".  But I wonder: should we change it from InputFormat at all?  Do we intend to rename OutputFormat too?  If not, renaming this might  create more confusion in the API than it removes.", "Because very few users do anything with MapRunnable, I think we should just make the relevant changes to MapRunner.\n\nClearly, the name of the class is totally a judgment call. To me, the input \"format\" is the class of the RecordReader. The scope of this class is naturally much wider.\n\nSo in terms of the user interface, I would propose that the default InputMetaFormat asks the job conf for a RecordReader and a Mapper. So, there should be new JobConf methods:\n\n// defaults to TextRecordReader\nvoid setRecordReaderClass(Class);\nClass getRecordReaderClass();\n\nis that reasonable?", "We don't expect most users to directly implement this API, rather this is the API the system invokes.  So the core API should be MapRunnable, and the library should can provide a standard implementation (MapRunner) whose constructor takes a Mapper class.  Otherwise folks would be unable to use a different MapRunnable for each task and we'd be back where we started.\n\nI don't think we need to add JobConf methods beyond those to access the InputFormat.  The default InputFormat will remain TextInputHandler, for back-compatibility, no?  We don't want JobConf to keep growing arbitrarily, so we should move to using static methods on library classes to access properties specific to those classes.  Thus, if one specifies an InputFormat implementation named MetaInputHandler, then the method to set the record reader would be MetaInputHandler.setRecordReaderClass(Class).\n\nInputFormatBase will get a new method, getMapRunnable, whose default implementation is something like:\n\nMapRunnable getMapRunnable(Split) {\n  return new MapRunner(InputFormatBase.getMapperClass(getConf()));\n}\n\nJobConf.getMapperClass() should be thus deprecated and made to call InputFormatBase.getMapperClass().  So a job might:\n\njob.setInputFormatClass(SequenceFileInputFormat.class);\nSequenceFileInputFormat.setMapperClass(MyMapper.class);\n\n", "A thought: instead of altering existing interfaces, we might create new interfaces and support both for a time.  That would keep things back-compatible.  How much more pain would it be to implement that way?", "Ok, my current thoughts are as follows:\n\n{code}\ninterface JobInput {\n  void validateInput(JobConf conf)\n  List<InputSplit> createSplits(JobConf conf);\n}\n{code}\n\nand the natural implementation over FileSystem files and directories:\n\n{code}\nclass FileSystemJobInput implements JobInput { \n  ...\n  public static void addInputPath(JobConf conf, Path path);\n  public static void addInputPath(JobConf conf, Path path, \n           Class<? extends RecordReader> reader, \n           Class<? extends Mapper> mapper);\n  public static void setDefaultRecordReader(JobConf conf, \n           Class<? extends RecordReader> reader);\n}\n{code}\n\nwhich would be the default and be used by most applications. The other major change is to InputSplits, which get the ability to define their RecordReader and Mapper.\n\n{code}\ninterface InputSplit extends Writable {\n  ...\n  RecordReader createRecordReader();\n  Mapper createMapper();\n}\n{code}\n\nFinally, RecordReader gets a method to set the InputSplit:\n\n{code}\ninterface RecordReader {\n  void initialize(InputSplit split, Progressable progress) throws IOException;\n  ...\n}\n{code}\n\nso that their interface is standard and can be created via ReflectionUtils.newInstance.", "\nThis looks good to me.\n\nShould we also have a method to set the default mapper?\n\nOne note here is that, it is auumed that all the mapper will genenerate \nthe map output values of the same class. I think this is due to the constraint \nimposed by the SequenceFile.\nIt would be nice to relax this constrain to only require the output values are writable.\nThe framework can serialize the mapout values and tag them with their actual class.\nThe value class of the map output files will be simply ByteWritable. \nThe framework can automatically restore the values to their actual class \nbefore they are passed to the reduce function.\n\n ", "bq. The other major change is to InputSplits, which get the ability to define their RecordReader and Mapper.\n\nHmm.  This is a big departure from the previously proposed API:\n{code}\npublic interface JobInput {\n  public List<InputSplit> getSplits(JobConf);\n  public RecordReader getRecordReader(JobConf, InputSplit);\n  public Mapper getMapper(JobConf, InputSplit, RecordReader);\n}\n{code}\n\nYour new API moves the latter two methods to the InputSplit.  Can you motivate this?\n\nI question whether it's a good idea to move such \"policy\" methods to an \"implementation\" class like InputSplit.  It seems to me that we'll want to use inheritance to implement InputSplits, and inheritance can fight with implementation of interfaces.  A typical application will want to be able to orthogonally define its Mapper and its splitter/RecordReader, and we want to make it as simple as possible to paste such independent implementations together.  Splitter and RecordReader implementations will often go together, so it makes sense to have them share implementations.  But Mappers are frequently independent.  How, using the above, would one define a single mapper that operates over inputs in different formats, but that produce compatible keys and values (i.e., merging or joining)?  One should be able to do that by specifying some sort of compound input format, and only a single mapper implementation.  One should be able to extend mappers and splitters independently and then glue them together at the last minute.  Attaching mappers to the split instance seems like it could complicate that.\n\nBefore we agree on these APIs, I'd like to see some APIs for both reusable splitters and record readers as well as sample application code that uses mappers.  Perhaps we should start a wiki page with code examples given various APIs?", "Moving the RecordReader and Mapper into the InputSplit is necessary to support different flavors of joins. In particular, you can do things like:\n\naddInputPath(\"/foo/*_2007/\" , Reader1.class, Mapper1.class);\naddInputPath(\"/foo/march_*/\", Reader2.class, Mapper2. class);\n\nand correctly handle any overlaps that occur.\n\nPart of what has happened recently with RecordReaders shows that they are independent of InputFormats. We have moved almost all of them out of the InputFormats that originally defined them because applications needed to control the splits without changing the RecordReaders.", "> addInputPath(\"/foo/*_2007/\" , Reader1.class, Mapper1.class);\n\nIt might be nice if this were factored to a generic, file-independent interface in the base class, like:\n\nvoid addInput(JobConf job, String name, Reader, Mapper);\n\nThis could just add the names to lists on the job, and could be reused by non-file JobInputs.\n\nAlso the base class for InputSplits should have Reader and Mapper fields, right?  FileSplit should only add path, start and length fields--the file-specific bits.  The generic base class should implement createRecordReader() and createMapper(), right?\n\nA use case to keep in mind is, if we add an HBaseJobInput, how much code from FSJobInput would need to be duplicated?  Hopefully very little.\n\n", "Another thought: we ought to use context objects (/HADOOP-1230) wherever possible here, to simplify future evolution.  For example, instead of RecordReader#initialize(InputSplit split, Progressable progress) we should probably have RecordReader#initialize(InputSplit split, MapContext context), with MapContext including a 'getProgress()' method.  That way we can add more methods to the system-implemented context without altering the user-implemented API.\n", "I'll also rename the JobInput to InputSplitter and create a class InputSplitBase that has the generic code for handling the record readers and mappers.", "Here's a first cut at providing this feature. Note that it doesn't require any framework changes - it's all in the library classes, as Owen pointed out above.\n\nFor a concrete example, consider:\n\n{code}\nExtendedJobConf conf = new ExtendedJobConf(MultiInputFormat.class);\nconf.setMapperClass(KeyMapper.class);\nconf.setReducerClass(IdentityReducer.class);\n\nconf.setOutputPath(new Path(\"output\"));\n\nconf.addInputPath(new Path(\"input/multiinput/1\"),\n        TextInputFormat.class);\nconf.addInputPath(new Path(\"input/multiinput/2\"),\n        SequenceFileInputFormat.class);\n{code}\n\nIt's also possible to specialize the mappers by input path:\n\n{code}\nExtendedJobConf conf = new ExtendedJobConf(MultiInputFormatWithMapper.class);\nconf.setReducerClass(IdentityReducer.class);\n\nconf.setOutputPath(new Path(\"output\"));\n\nconf.addInputPath(new Path(\"input/multiinput/1\"),\n        TextInputFormat.class, IdentityMapper.class);\nconf.addInputPath(new Path(\"input/multiinput/3\"),\n        KeyValueTextInputFormat.class, ValueMapper.class);\n{code}\n\nNote that this patch uses ExtendedJobConf - are the new addInputPath methods useful enough to be added to JobConf?", "I've just tried this out, with a few minor changes to make it run on our 0.15.2 cluster.\nThis makes writing a lot of our programs a lot easier and the code a lot cleaner, great work Tom!\n\nPersonally I'd like to see the methods in JobConf instead of an extended class, but either way works.", "bq. Personally I'd like to see the methods in JobConf \n\nMe too - I'll make that change before submitting the patch for inclusion.", "\nHi, that sounds cool.\nAfter sitting there in open state for almost 2 years, it's nice to see a  patch is available!\n\nThanks a lot, Tom.\n\nRunping\n", "We've unfortunately run into an issue with this patch, if we add a lot of input directories we get a huge number of map tasks.\nIn this case almost all of the directories have the same mappers and input formats.\n\nI'm guessing that the issue is that our input format's getSplits will be called separately on each directory instead of over all of the directories at once. Meaning that if we have a directory with for example one 10mb file it will be split up into many map jobs. Normally it would be a part of many files and probably end up into just one map job.\n\nPerhaps it would be possible to merge all directories using the same input format and mapper into one input format getSplits call?", "Attached is a modified version of Tom's patch.\n\nChanges:\n  * Updated to work against the trunk\n  * Moved methods from ExtendedJobConf to static methods in FileInputFormat\n  * Fixed issue described by Johan above (multiple paths with the same InputFormat and Mappers are now grouped and split together, rather than separately, so you don't get excessive numbers of map jobs)\n  * Generally tidied up and documented the code\n  * Added a few unit tests", "Trying to assign this to Chris Smith since he's actively working on it.\nUnfortunately he's not in the Assignee list, so I'll unassigne this first.", "Seems Chris can't assign it to himself or submit the patch to hudson, so I'll let someone with greater jira permissions help him out :)", "Thanks for picking this up Chris. Generally looks good. There seem to be some problems with tabs, and there's a System.out.println in the test that doesn't need to be there. ", "New patch, without tabs or superfluous System.out.printlns", "-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12385878/hadoop-372.patch\n  against trunk revision 676069.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 6 new or modified tests.\n\n    -1 patch.  The patch command could not apply the patch.\n\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2846/console\n\nThis message is automatically generated.", "-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12385966/hadoop-372.patch\n  against trunk revision 676069.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 6 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    -1 findbugs.  The patch appears to cause Findbugs to fail.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed core unit tests.\n\n    -1 contrib tests.  The patch failed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2856/testReport/\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2856/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2856/console\n\nThis message is automatically generated.", "-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12385985/hadoop-372.patch\n  against trunk revision 676772.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 6 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2860/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2860/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2860/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2860/console\n\nThis message is automatically generated.", "The unit test failure (a timeout in hdfs.TestFileCreation.testClientTriggeredLeaseRecovery) wasn't caused by this patch. The tests pass locally, and nothing in the patch will even be used without either specifying the Delegating{Mapper,InputFormat} manually or using the new methods in FileInputFormat, which that test case obviously doesn't do.", "I've just committed this. Thanks Chris!", "I find this feature useful but I'm not sure about how it has been implemented (apologies for the late comment, just become aware of it when seeing the Jira Resolved email).\n\n*{{FileInputFormat}} API changes:*\n\nAll the methods added to the {{FileInputFormat}} are not related to {{InputFormat}} functionality.\n\nIn my opinion they should be in a separate class (something like {{MultipleInputs}} )\n\n*Package for all the patch classes:*\n\nAll these classes should be in {{org.apache.hadoop.mapred.lib}} thus keeping the core {{mapred}} with exactly that, the core, this is a useful extension but still is an extension.\n\n*{{JobConf}} for the different mappers:*\n\nThe current code forces all mappers to have the same {{JobConf}}, this means that mappers have to be written with knowledge of the other mappers to avoid collision in the configuration keys.\n\nHow about passing along a {{JobConf}} instance when adding an input path with its own {{Mapper}}, something like HADOOP-3702 is doing when adding a mapper to the chain?\n", "Alejandro,\n\nThanks for the feedback. I've created HADOOP-3853 to cover your first two points. The third (JobConf for the different mappers) is an enhancement that should probably go in another issue.", "Integrated in Hadoop-trunk #581 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/581/])"], "derived": {"summary": "Right now, the user can specify multiple input directories for a map reduce job. However, the files under all the directories are assumed to be in the same format, \nwith the same key/value classes.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "should allow to specify different inputformat classes for different input dirs for Map/Reduce jobs - Right now, the user can specify multiple input directories for a map reduce job. However, the files under all the directories are assumed to be in the same format, \nwith the same key/value classes."}, {"q": "What updates or decisions were made in the discussion?", "a": "Integrated in Hadoop-trunk #581 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/581/])"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-373", "title": "Some calls to mkdirs do not check return value", "status": "Closed", "priority": "Major", "reporter": "Wendy Chien", "assignee": "Wendy Chien", "labels": [], "created": "2006-07-19T23:25:36.000+0000", "updated": "2009-07-08T16:41:58.000+0000", "description": "Some calls to mkdirs do not check if mkdirs failed.  \n\nArun noticed this while looking at HADOOP-281.  Since this issue is actually a separate issue from the mkdirs implementation issue of HADOOP-281, I created a new bug for it.   ", "comments": ["I've changed calls which did not check the return value.", "I can't apply this patch:\n\n% patch -p 0 < ~/Desktop/hadoop-373.patch\npatching file src/java/org/apache/hadoop/io/MapFile.java\npatching file src/java/org/apache/hadoop/fs/FileUtil.java\npatch: **** malformed patch at line 31: @@ -130,7 +132,9 @@\n\nCan you please re-generate it?  Thanks!", "regenerated patch to check return value of mkdirs calls", "I just committed this.  Thanks, Wendy!"], "derived": {"summary": "Some calls to mkdirs do not check if mkdirs failed. Arun noticed this while looking at HADOOP-281.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Some calls to mkdirs do not check return value - Some calls to mkdirs do not check if mkdirs failed. Arun noticed this while looking at HADOOP-281."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Wendy!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-374", "title": "native support for gzipped text files", "status": "Closed", "priority": "Major", "reporter": "Yoram Arnon", "assignee": null, "labels": [], "created": "2006-07-20T18:14:44.000+0000", "updated": "2009-07-08T16:51:50.000+0000", "description": "in many cases it is convenient to store text files in dfs as gzip compressed files.\nIt would be good to have built in support for processing these files in a mapreduce job.\n\nThe getSplits implementation should return a single split per input file, ignoring the numSplits parameter.\nOne can probably subclass InputFormatBase, and the getSplits method can simply call listPaths() \nand then construct and return a single split per path returned.\n\nThe code for reading would look something like (courtesy of Vijay Murthy):\n\n   public RecordReader getRecordReader(FileSystem fs, FileSplit split,\n                                       JobConf job, Reporter reporter)\n     throws IOException {\n     final BufferedReader in =\n       new BufferedReader(new InputStreamReader\n         (new GZIPInputStream(fs.open(split.getPath()))));\n     return new RecordReader() {\n         long position;\n         public synchronized boolean next(Writable key, Writable value)\n           throws IOException {\n           String line = in.readLine();\n           if (line != null) {\n             position += line.length();\n             ((UTF8)value).set(line);\n             return true;\n           }\n           return false;\n         }\n         public synchronized long getPos() throws IOException {\n           return position;\n         }\n        public synchronized void close() throws IOException {\n           in.close();\n         }\n       };\n   }\n\n", "comments": [], "derived": {"summary": "in many cases it is convenient to store text files in dfs as gzip compressed files. It would be good to have built in support for processing these files in a mapreduce job.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "native support for gzipped text files - in many cases it is convenient to store text files in dfs as gzip compressed files. It would be good to have built in support for processing these files in a mapreduce job."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-375", "title": "Introduce a way for datanodes to register their HTTP info ports with the NameNode", "status": "Closed", "priority": "Major", "reporter": "Devaraj Das", "assignee": "Devaraj Das", "labels": [], "created": "2006-07-21T10:48:37.000+0000", "updated": "2009-07-08T16:41:58.000+0000", "description": "If we have multiple datanodes within a single machine the Jetty servers (other than the first one) won't be able to bind to the fixed HTTP port. So, one solution is to have the datanodes pick a free port (starting from a configured port value) and then inform namenode about it so that the namenode can then do redirects, etc.\n\nJohan Oskarson reported this problem. \n\nIf a computer have a second dfs data dir in the config it doesn't start properly because of:\n\nException in thread \"main\" java.io.IOException: Problem starting http server\n        at org.apache.hadoop.mapred.StatusHttpServer.start(StatusHttpServer.java:182)\n        at org.apache.hadoop.dfs.DataNode.<init>(DataNode.java:170)\n        at org.apache.hadoop.dfs.DataNode.makeInstanceForDir(DataNode.java:1045)\n        at org.apache.hadoop.dfs.DataNode.run(DataNode.java:999)\n        at org.apache.hadoop.dfs.DataNode.runAndWait(DataNode.java:1015)\n        at org.apache.hadoop.dfs.DataNode.main(DataNode.java:1066)\nCaused by: org.mortbay.util.MultiException[java.net.BindException: Address already in use]\n        at org.mortbay.http.HttpServer.doStart(HttpServer.java:731)\n        at org.mortbay.util.Container.start(Container.java:72)\n        at org.apache.hadoop.mapred.StatusHttpServer.start(StatusHttpServer.java:159)\n        ... 5 more\n", "comments": ["The way I am thinking of doing this is to add a new field in DatanodeRegistration class called infoPort. The infoPort is set to the value that the jetty could bind to (after retrials if applicable). This becomes known to the namenode as a part of the registration process. The namenode puts the port in a (new) field of the DatanodeInfo object that it creates for every datanode. Later on, anyone wishing to contact the jetty on a specific datanode (say, for redirecting the user to a datanode containing a particular data block) can do so by looking at the port number obtained from the DatanodeInfo object (for e.g., LocatedBlock contains an array of DatanodeInfo objects where a given block can be found). Makes sense?", "it would be good to have a unit test for multiple datanodes in a machine", "Testcase for multiple datanodes is not included in this patch. Maybe, that should be a separate issue.", "Since the patch to Hadoop-376 made this patch go out of sync, resubmitting it again with a one line change.", "I'm confused by the 'started' field added to StatusHttpServer.  It is set at the end of the 'start' method.  Then in DataNode, you add the code:\n\n         this.infoServer.start();\n+        while (!this.infoServer.started()) {\n+          try {\n+            wait(1);\n+          } catch (InterruptedException ie){}\n+        }\n+        infoPort = this.infoServer.getPort();\n\nWon't started() always be true here?  Why the loop?  Why call wait(1)?  Wait() is used with notify(), and there are no calls to notify(), and a 1ms wait makes for a rather tight loop.  Did you mean Thread.sleep(1000)?\n\n", "The 'started' field is there to just make sure that we do getPort() only after the webserver starts up. Since the webserver starts in a new thread, we should wait before doing getPort() (to avoid timing issues). Note that 'started' is set to false in the field initialization in StatusHttpServer. 'started' is set to true only when the webserver has successfully started up (towards the end of the 'start' method in StatusHttpServer). Note that if there was an exception while starting the webserver, it would throw an exception and 'started' would not be touched. Only after the exceptions are handled (e.g. a new port is found, etc.) will 'started' be set to true (after it 'breaks' out of the outermost 'while' loop inside 'start' method).\n\nThe wait(1) is can be replaced with Thread.sleep(1). I meant that and no issues with replacing that. The loop seems tight but I expect the webserver to start soon enough that it actually makes this loop not that tight (but in any case it could be replaced by Thread.sleep(100) or something).\n\nMakes sense?", "Some more points:\nI think the timing issue requires to be handled when we allow for port roll-overs. The webserver may end up binding to a port other than the default port and unless the webserver starts up successfully, we won't be able to know which port it bound to. If we do a getPort() before the webserver starts up fully, then the port that is returned is the port value at which the webserver is currently *trying* to bind to (but it is not guaranteed that the attempt will be successful). \nIf we don't allow port roll-overs (the last argument to StatusHttpServer's constructor is false), then the timing issue isn't there since getPort() will always (& supposed to) return the default port (in the absence of any exception).", "Perhaps I'm missing something, but I still don't see how this works.  The loop retrying ports is in StatusHttpServer().start().  When this method returns, the port is known.  There's nothing more to wait for, is there?", "Well!!! \nI seem to have had an impression that StatusHttpServer is started in a thread by itself!! (that start method led me astray) :-(\nReally sorry about this. That part of the patch is not needed.", "Sorry for spamming the dev list with comments/mails on this issue. Hopefully, I won't make such blunders any more ..", "Sorry, this has conflicts with HADOOP-321, which I just committed.  Can you please regenerate this?  Thanks.", "Attached the modified patch", "I just committed this.  Thanks!"], "derived": {"summary": "If we have multiple datanodes within a single machine the Jetty servers (other than the first one) won't be able to bind to the fixed HTTP port. So, one solution is to have the datanodes pick a free port (starting from a configured port value) and then inform namenode about it so that the namenode can then do redirects, etc.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Introduce a way for datanodes to register their HTTP info ports with the NameNode - If we have multiple datanodes within a single machine the Jetty servers (other than the first one) won't be able to bind to the fixed HTTP port. So, one solution is to have the datanodes pick a free port (starting from a configured port value) and then inform namenode about it so that the namenode can then do redirects, etc."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-376", "title": "Datanode does not scan for an open http port", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-07-21T15:24:57.000+0000", "updated": "2013-05-02T02:28:58.000+0000", "description": "The DataNode does not scan for an open http port. Only the singleton servers are allowed to have required port assignments.", "comments": ["This patch enables the port scan.", "I just committed this.  Thanks, Owen."], "derived": {"summary": "The DataNode does not scan for an open http port. Only the singleton servers are allowed to have required port assignments.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Datanode does not scan for an open http port - The DataNode does not scan for an open http port. Only the singleton servers are allowed to have required port assignments."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-377", "title": "Configuration does not handle URL", "status": "Closed", "priority": "Major", "reporter": "Jean-Baptiste Quenot", "assignee": null, "labels": [], "created": "2006-07-21T17:49:33.000+0000", "updated": "2006-08-04T22:22:46.000+0000", "description": "Current Configuration allows:\n\n* <String> pointing to a resource in the classpath\n* <Path> local path on the file system\n\nThe attached patch handles java.net.URL.  We use it to load hadoop-client.xml from a JAR.\n\nThanks in advance!", "comments": ["Patch against hadoop 0.4.0", "Can you say more about how you use this?  One can already name config files in jar files on the CLASSPATH, or at an absolute place in the filesystem.  Are you specifying a jar that's accessed from a jar not on the CLASSPATH?", "Here are the details: we use an abstraction class that returns either a File or an URL.  An URL is returned by getClass().getResource(\"hadoop-client.xml\") so it would be great if Hadoop could handle that.", "So the URL is a reference to something on the CLASSPATH?  Couldn't you simply add the String \"hadoop-client-xml\", since that would do the same thing?", "You're right in this particular case I could do that.  But basically allowing to pass an URL is much more flexible: that way the config file can be in a JAR  or ZIP file (even outside of the classpath), at an HTTP(S) URL, etc.  The java.net.URL architecture even allows you to implement easily a custom protocol handler (this information is from http://java.sun.com/developer/onlineTraining/protocolhandlers/).\n\nAnother point: we are using Spring to instantiate the Hadoop configuration.  And in Spring you can specify custom editors for the properties you set.  We pass something like conf://hadoop-client.xml and this is resolved to a Spring Resource with a ResourceEditor (a custom editor that resolves to a org.springframework.core.io.Resource), which is holding either a File or an URL.  Creating a String editor to resolve a path is a bit awkward since a String is not a concept that is appropriate to represent a file location in the realm of OOP, it is just too generic.  An URL clearly represents the location of some piece of data, in our case the Hadoop Client configuration file.\n\nTo sum up, I think providing URL support to Hadoop's Configuration gives more choice to the end developer.", "I just committed this, minus the changes to the imports.  Thanks!"], "derived": {"summary": "Current Configuration allows:\n\n* <String> pointing to a resource in the classpath\n* <Path> local path on the file system\n\nThe attached patch handles java. net.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "Configuration does not handle URL - Current Configuration allows:\n\n* <String> pointing to a resource in the classpath\n* <Path> local path on the file system\n\nThe attached patch handles java. net."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this, minus the changes to the imports.  Thanks!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-378", "title": "too many files are distributed to slaves nodes", "status": "Resolved", "priority": "Trivial", "reporter": "Yoram Arnon", "assignee": null, "labels": [], "created": "2006-07-21T19:08:22.000+0000", "updated": "2011-07-16T16:46:48.000+0000", "description": "currently all the svn tree is sync'ed to the slaves on startup, excluding only .svn files.\nthat includes the example jar file, sources, and other files that are not required.\nIt would  be better to pick and choose the files that get sync'ed. It will improve sync times on startup, and can prevent some name conflicts, since all the jar files are also included in the classpath by the hadoop script.", "comments": ["(This one might have gone stale over time, and perhaps does not apply anymore)\n\nThis one needs to explain more about what its talking about - deployment of Hadoop via HADOOP_MASTER which uses rsync (and thereby does not pass out incrementals)?\n\nThis would be resolved by better packaging, which is gonna be addressed in future via Bigtop and the like, hopefully. A package for examples, a package for test, a package for clients. With mavenized builds, this is very easy to do and is on its way.\n\nResolving as Incomplete, needs more desc."], "derived": {"summary": "currently all the svn tree is sync'ed to the slaves on startup, excluding only. svn files.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "too many files are distributed to slaves nodes - currently all the svn tree is sync'ed to the slaves on startup, excluding only. svn files."}, {"q": "What updates or decisions were made in the discussion?", "a": "(This one might have gone stale over time, and perhaps does not apply anymore)\n\nThis one needs to explain more about what its talking about - deployment of Hadoop via HADOOP_MASTER which uses rsync (and thereby does not pass out incrementals)?\n\nThis would be resolved by better packaging, which is gonna be addressed in future via Bigtop and the like, hopefully. A package for examples, a package for test, a package for clients. With mavenized builds, this is very easy to do and is on its way.\n\nResolving as Incomplete, needs more desc."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-380", "title": "The reduce tasks poll for mapoutputs in a loop", "status": "Closed", "priority": "Major", "reporter": "Mahadev Konar", "assignee": "Mahadev Konar", "labels": [], "created": "2006-07-21T23:18:22.000+0000", "updated": "2009-07-08T16:51:51.000+0000", "description": "The Reduce tasks poll for the mapoutputs in a loop. The polling thread should be sleeping for 5 seconds before polling again but there is a bug in updating the timestamps which make the reduce task poll in a loop without sleeping.", "comments": ["Just a one line change that fixes the lastpolltime.", "I just committed this.  Thanks, Mahadev!"], "derived": {"summary": "The Reduce tasks poll for the mapoutputs in a loop. The polling thread should be sleeping for 5 seconds before polling again but there is a bug in updating the timestamps which make the reduce task poll in a loop without sleeping.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "The reduce tasks poll for mapoutputs in a loop - The Reduce tasks poll for the mapoutputs in a loop. The polling thread should be sleeping for 5 seconds before polling again but there is a bug in updating the timestamps which make the reduce task poll in a loop without sleeping."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Mahadev!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-381", "title": "keeping files for tasks that match regex on task id", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-07-22T00:03:20.000+0000", "updated": "2009-07-08T16:51:51.000+0000", "description": "For debugging map/reduce jobs, if a single task is producing bad results, but *not* failing, it is hard to debug the problem. This patch lets you set a pattern for task ids that will keep their files from being deleted when the task and job complete. This allows the developer to run the task in the IsolationRunner under the debugger.", "comments": ["1. adds the keep.failed.task.files and keep.task.files.pattern variables to hadoop-default.xml\n2. adds set/getKeepTaskFilesPattern to JobConf\n3. modifies the TaskTracker to keep the files at Task completion if the task name matches the regex.\n4. adds a word count test to the junit tests.\n5. adds a test to make sure that task trackers clean up normally\n6. also checks that the keep.task.files.pattern keeps the directories\n7. the mini-mr cluster keeps track of the local dir for each task tracker\n8. the mini-mr cluster adds a waitUntilIdle() method that waits for the cluster to be stable\n9. task tracker adds isIdle to test whether a given task tracker is idle\n10. add the examples to the classpath for the junit tests", "I just committed this.  Thanks, Owen!"], "derived": {"summary": "For debugging map/reduce jobs, if a single task is producing bad results, but *not* failing, it is hard to debug the problem. This patch lets you set a pattern for task ids that will keep their files from being deleted when the task and job complete.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "keeping files for tasks that match regex on task id - For debugging map/reduce jobs, if a single task is producing bad results, but *not* failing, it is hard to debug the problem. This patch lets you set a pattern for task ids that will keep their files from being deleted when the task and job complete."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-382", "title": "add a unit test for multiple datanodes in a machine", "status": "Closed", "priority": "Major", "reporter": "Yoram Arnon", "assignee": "Milind Barve", "labels": [], "created": "2006-07-24T18:41:00.000+0000", "updated": "2009-07-08T16:41:59.000+0000", "description": "recently we saw a bug present itself only when multiple data nodes are started on a single machine.\nA unit test that starts multiple data nodes would expose such bugs before they happen in a real installation.", "comments": ["This patch modifies the MiniDFSCluster to optionally specify starting multiple datanodes in separate threads.", "Patch submitted.", "I just committed this.  Thanks, Milind!"], "derived": {"summary": "recently we saw a bug present itself only when multiple data nodes are started on a single machine. A unit test that starts multiple data nodes would expose such bugs before they happen in a real installation.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "add a unit test for multiple datanodes in a machine - recently we saw a bug present itself only when multiple data nodes are started on a single machine. A unit test that starts multiple data nodes would expose such bugs before they happen in a real installation."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Milind!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-383", "title": "unit tests fail on windows", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Michel Tourn", "labels": [], "created": "2006-07-24T20:48:56.000+0000", "updated": "2009-07-08T17:05:35.000+0000", "description": "When I run the \"ant test\" target under Windows, I get the following exception in the logs for the streaming unit tests. The unit tests need to run under Windows, if it is going to be a supported platform. So we either need to remove the streaming unit tests or make them work.\n\nTestsuite: org.apache.hadoop.streaming.TestStreaming\nTests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.172 sec\n\nTestcase: testCommandLine took 0.172 sec\n        FAILED\njava.lang.RuntimeException: Operating system Windows XP not supported by this class\n        at org.apache.hadoop.streaming.Environment.<init>(Environment.java:47)\n        at org.apache.hadoop.streaming.StreamJob.init(StreamJob.java:68)\n        at org.apache.hadoop.streaming.StreamJob.go(StreamJob.java:55)\n        at org.apache.hadoop.streaming.TestStreaming.testCommandLine(Unknown Source)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:585)\n        at junit.framework.TestCase.runTest(TestCase.java:154)\n        at junit.framework.TestCase.runBare(TestCase.java:127)\n        at junit.framework.TestResult$1.protect(TestResult.java:106)\n        at junit.framework.TestResult.runProtected(TestResult.java:124)\n        at junit.framework.TestResult.run(TestResult.java:109)\n        at junit.framework.TestCase.run(TestCase.java:118)\n        at junit.framework.TestSuite.runTest(TestSuite.java:208)\n        at junit.framework.TestSuite.run(TestSuite.java:203)\n        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:2\n97)\n        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.jav\na:672)\n        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:\n567)\n\n", "comments": ["The Environment code used was a little out-of-date:\n-      if (OS.equals(\"Windows NT\")) {\n+      if (OS.indexOf(\"Windows\") > -1) {\n\nWith this patch,\nI successfully ran ant test-contrib on Windows XP.\n", "I just committed this.  Thanks, Michel!"], "derived": {"summary": "When I run the \"ant test\" target under Windows, I get the following exception in the logs for the streaming unit tests. The unit tests need to run under Windows, if it is going to be a supported platform.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "unit tests fail on windows - When I run the \"ant test\" target under Windows, I get the following exception in the logs for the streaming unit tests. The unit tests need to run under Windows, if it is going to be a supported platform."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Michel!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-384", "title": "improved error messages for file checksum errors", "status": "Closed", "priority": "Minor", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-07-24T21:43:47.000+0000", "updated": "2006-08-04T22:22:48.000+0000", "description": "Improves the messages on a couple of the failures we've been seeing to try and get enough information to identifiy the problem.", "comments": ["I just committed this.  Thanks, Owen!"], "derived": {"summary": "Improves the messages on a couple of the failures we've been seeing to try and get enough information to identifiy the problem.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "improved error messages for file checksum errors - Improves the messages on a couple of the failures we've been seeing to try and get enough information to identifiy the problem."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-385", "title": "rcc does not generate correct Java code for the field of a record type", "status": "Closed", "priority": "Major", "reporter": "Hairong Kuang", "assignee": "Milind Barve", "labels": [], "created": "2006-07-25T00:38:00.000+0000", "updated": "2006-08-04T22:22:48.000+0000", "description": "If  a field of a Jute record  is also a Jute record and the target language is Java, rcc does not generate correct field name, correct code for deserializing the field. The access modifier \"private\" of the validation method causes problem for validating nested records.\n\nIn addition, rcc can not handle the case when a comment started with \"//\" also contains the string \"//\".", "comments": ["Fixed bugs in code generation for embedded records.\n", "I just committed this.  Thanks, Milind!"], "derived": {"summary": "If  a field of a Jute record  is also a Jute record and the target language is Java, rcc does not generate correct field name, correct code for deserializing the field. The access modifier \"private\" of the validation method causes problem for validating nested records.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "rcc does not generate correct Java code for the field of a record type - If  a field of a Jute record  is also a Jute record and the target language is Java, rcc does not generate correct field name, correct code for deserializing the field. The access modifier \"private\" of the validation method causes problem for validating nested records."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Milind!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-386", "title": "Periodically move blocks from full nodes to those with space", "status": "Closed", "priority": "Major", "reporter": "Johan Oskarsson", "assignee": "Johan Oskarsson", "labels": [], "created": "2006-07-25T10:48:55.000+0000", "updated": "2009-07-08T16:41:58.000+0000", "description": "I'm still having *a lot* of problems with some nodes filling up quickly and others hardly being touched, mostly because of the hardware being\nvery different.\n\nAs someone suggested, there should be a thread that periodically checks the dfs for nodes with little or no free space and schedules blocks\nto be moved off that node.", "comments": ["As a first step, one could make chooseExcessReplicates in FSNamesystem.java space-aware.\nSo that if a block are over replicated it is removed from the machine with least percentage free space.\n\nThat way, all we would have to do is to copy the block to one extra node with lots of space and the rest would solve itself.\n", "I suggest rather than \"percent free space\" we use the metric \"absolute free space\". This would improve interaction with the new code which lets a tasktracker withdraw from the workload when its space gets low.... and, especially, improve the odds that machines with much smaller drives can contribute productively. I have a few really old machines in my cluster, with 15gb drives. Running datanodes and tasktrackers on them makes them usually end up being (small) datanodes, and doing very little computation. The reverse would probably be more useful.\n\nAnother thing that would be nice would be to first consider rebalancing within local drives. It costs no network bandwidth to move files between local drives. Plus, you can add a new big drive to one of those old nodes I mention above, and have it become productive even more easily.", "This patch removes excess replicates from the machine with least absolute space first, as suggested by Bryan.\n\nWould be helpful to have this, no matter what we decide when it comes to redistributing blocks to nodes with more free space.", "I just committed this.  Thanks, Johan.  This does not fully address the issue of this patch, so perhaps someone should start a new bugs for further aspects."], "derived": {"summary": "I'm still having *a lot* of problems with some nodes filling up quickly and others hardly being touched, mostly because of the hardware being\nvery different. As someone suggested, there should be a thread that periodically checks the dfs for nodes with little or no free space and schedules blocks\nto be moved off that node.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Periodically move blocks from full nodes to those with space - I'm still having *a lot* of problems with some nodes filling up quickly and others hardly being touched, mostly because of the hardware being\nvery different. As someone suggested, there should be a thread that periodically checks the dfs for nodes with little or no free space and schedules blocks\nto be moved off that node."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Johan.  This does not fully address the issue of this patch, so perhaps someone should start a new bugs for further aspects."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-387", "title": "LocalJobRunner assigns duplicate mapid's", "status": "Closed", "priority": "Major", "reporter": "Sami Siren", "assignee": null, "labels": [], "created": "2006-07-25T18:39:30.000+0000", "updated": "2009-07-08T16:51:50.000+0000", "description": "While hunting down nutch issue NUTCH-266 i discovered that id's are generated with following fragment of code:\n\n    private String newId() {\n      return Integer.toString(Math.abs(new Random().nextInt()),36);\n    }\n\nand the related Javadoc:\n\"\npublic Random()\n\n    Creates a new random number generator. Its seed is initialized to a value based on the current time:\n\n         public Random() { this(System.currentTimeMillis()); }\n\n    Two Random objects created within the same millisecond will have the same sequence of random numbers.\n\n\"\n\nit appears that in this case there are more than one Random pobject generated at the same millisecond and id's are\nno longer unique.\n", "comments": ["fix for hadoop 0.4", "fix for trunk", "One more note, there were couiple of other places in code where the random was used in the same way, perhaps someone with better understanding of the internals should check them out too.", "I just committed this.  Thanks, Sami!"], "derived": {"summary": "While hunting down nutch issue NUTCH-266 i discovered that id's are generated with following fragment of code:\n\n    private String newId() {\n      return Integer. toString(Math.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "LocalJobRunner assigns duplicate mapid's - While hunting down nutch issue NUTCH-266 i discovered that id's are generated with following fragment of code:\n\n    private String newId() {\n      return Integer. toString(Math."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Sami!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-388", "title": "the hadoop-daemons.sh fails with \"no such file or directory\" when used from a relative path", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-07-25T21:10:10.000+0000", "updated": "2006-10-02T06:29:18.000+0000", "description": "The new shell scripts fail with a relative directory:\n\n% current/bin/hadoop-daemons.sh start datanode\nnode1: current/bin/..: No such file or directory\nnode2: current/bin/..: No such file or directory\n\nThe problem is that HADOOP_HOME is set to a relative directory and hadoop-daemons does a cd, breaking the other scripts.", "comments": ["This patch makes hadoop-config.sh create an absolute path name for HADOOP_HOME.", "I just committed this.  Thanks, Owen."], "derived": {"summary": "The new shell scripts fail with a relative directory:\n\n% current/bin/hadoop-daemons. sh start datanode\nnode1: current/bin/.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "the hadoop-daemons.sh fails with \"no such file or directory\" when used from a relative path - The new shell scripts fail with a relative directory:\n\n% current/bin/hadoop-daemons. sh start datanode\nnode1: current/bin/."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-389", "title": "MiniMapReduce tests get stuck because of some timing issues with initialization of tasktrackers.", "status": "Closed", "priority": "Major", "reporter": "Mahadev Konar", "assignee": "Mahadev Konar", "labels": [], "created": "2006-07-26T18:50:15.000+0000", "updated": "2009-07-08T16:51:51.000+0000", "description": "The MiniMapReduce tests sometimes calls shutdown before the tasktrackers have been initialized. This makes the TestMiniMRBringup run for ever. ", "comments": ["This patch waits for the tasktrackers to be initialized before shutdown can be called. This patch also fixes a bug in build.xml where in a dependency has been introduced between compile-test-core and examples (in TestMiniMRWithDfs.java).", "I divided the original test into 2 independent patches.\nMiniMRCluster.patch - is the MiniMR cluster part of the original patch.\nBuildXML.patch - fixes some target dependencies in the ant script.\nThe original patch fixes the bug in build.xml but introduces unnecessary\ndependencies on the compile targets.\n", "I just committed this.  Thanks!"], "derived": {"summary": "The MiniMapReduce tests sometimes calls shutdown before the tasktrackers have been initialized. This makes the TestMiniMRBringup run for ever.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "MiniMapReduce tests get stuck because of some timing issues with initialization of tasktrackers. - The MiniMapReduce tests sometimes calls shutdown before the tasktrackers have been initialized. This makes the TestMiniMRBringup run for ever."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-390", "title": "compile-core-test target depends on compile-examples", "status": "Closed", "priority": "Major", "reporter": "Konstantin Shvachko", "assignee": "Konstantin Shvachko", "labels": [], "created": "2006-07-26T19:03:03.000+0000", "updated": "2006-08-04T22:22:50.000+0000", "description": "Compiling with compile-core-test target returns the following error:\n    [javac] hadoop\\src\\test\\org\\apache\\hadoop\\mapred\\TestMiniMRWithDFS.java:28: package org.apache.hadoop.examples does not exist\n    [javac] import org.apache.hadoop.examples.WordCount;\n    [javac]                                   ^\nThis target should depend on the compile-examples target.", "comments": ["A modification of build.xml to fix the problem.\n", "This was fixed by the patch in HADOOP-389."], "derived": {"summary": "Compiling with compile-core-test target returns the following error:\n    [javac] hadoop\\src\\test\\org\\apache\\hadoop\\mapred\\TestMiniMRWithDFS. java:28: package org.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "compile-core-test target depends on compile-examples - Compiling with compile-core-test target returns the following error:\n    [javac] hadoop\\src\\test\\org\\apache\\hadoop\\mapred\\TestMiniMRWithDFS. java:28: package org."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was fixed by the patch in HADOOP-389."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-391", "title": "test-contrib with spaces in classpath (Windows)", "status": "Closed", "priority": "Major", "reporter": "Michel Tourn", "assignee": null, "labels": [], "created": "2006-07-27T01:46:17.000+0000", "updated": "2006-08-04T22:22:51.000+0000", "description": "On Windows, ant test-contrib (streaming) with fail if classpath contains spaces.\n-    vargs.add(System.getProperty(\"java.class.path\"));\n+    vargs.add(\"\\\"\" + System.getProperty(\"java.class.path\") + \"\\\"\");\n", "comments": ["I just committed this.  Thanks!"], "derived": {"summary": "On Windows, ant test-contrib (streaming) with fail if classpath contains spaces. -    vargs.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "test-contrib with spaces in classpath (Windows) - On Windows, ant test-contrib (streaming) with fail if classpath contains spaces. -    vargs."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-392", "title": "Improve the UI for DFS content browsing", "status": "Closed", "priority": "Major", "reporter": "Devaraj Das", "assignee": null, "labels": [], "created": "2006-07-27T08:23:37.000+0000", "updated": "2009-07-08T16:41:59.000+0000", "description": "A couple of improvements requested:\n\n* change the title of the namenode page to include namenode\n    Hadoop Administration\n    -> Hadoop NameNode (<host>:<port>)\n\n* Move all the info except the per node info from the dfshealth.jsp page to the home page.  No reason to make folks go through that page to get summary info or to find local logs.\n\n* It would  be good if the health status information were sorted by node name, rather than the (random) order that -report returns\n\n* Looking at an xml file copied to the dfs, can't view/tail it in the browser. Can download the file and view it locally.\n\n* Viewing a java source file, the newlines don't come out right. Again, downloading and viewing locally is fine.\n\n* When clicking on a file, it would be good to view its contents (or some subset of it) immediately, with the advanced options appearing at the bottom (block locations [just choose one], number of blocks view/tail/download/chunk size).\n\n* Fix the filename problem (browser related) [e.g. /user/yarnon/build.xml gets translated to -user-yarnon-build.xml]\n\n\n", "comments": ["This patch implements most of what has been requested for.", "I just committed this.  Thanks, Devaraj!"], "derived": {"summary": "A couple of improvements requested:\n\n* change the title of the namenode page to include namenode\n    Hadoop Administration\n    -> Hadoop NameNode (<host>:<port>)\n\n* Move all the info except the per node info from the dfshealth. jsp page to the home page.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Improve the UI for DFS content browsing - A couple of improvements requested:\n\n* change the title of the namenode page to include namenode\n    Hadoop Administration\n    -> Hadoop NameNode (<host>:<port>)\n\n* Move all the info except the per node info from the dfshealth. jsp page to the home page."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Devaraj!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-393", "title": "The validateUTF function of class Text throws MalformedInputException for valid UTF8 code containing ascii chars", "status": "Closed", "priority": "Major", "reporter": "Hairong Kuang", "assignee": "Hairong Kuang", "labels": [], "created": "2006-07-27T17:38:51.000+0000", "updated": "2006-08-04T22:22:52.000+0000", "description": "The validateUTF function does not handle ascii chars right therefore causing MIE exception to be thrown.", "comments": ["This patch fixes the validation problem.\n\nI also changed the junit test TestText so it reports failure when any exception occurs.\n\nSince issue 302 is closed, I included the patch to Bryan's problem to this patch. So we do not need to reopen issue 302.\n\nThank Bryan for being the first one that uses Text. Let me know if you have any other problem.\n\nHairong\n\n", "I committed this.  I reverted your previous patch, then applied this one.  Thanks!"], "derived": {"summary": "The validateUTF function does not handle ascii chars right therefore causing MIE exception to be thrown.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "The validateUTF function of class Text throws MalformedInputException for valid UTF8 code containing ascii chars - The validateUTF function does not handle ascii chars right therefore causing MIE exception to be thrown."}, {"q": "What updates or decisions were made in the discussion?", "a": "I committed this.  I reverted your previous patch, then applied this one.  Thanks!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-394", "title": "MiniDFSCluster shudown order", "status": "Closed", "priority": "Minor", "reporter": "Konstantin Shvachko", "assignee": "Konstantin Shvachko", "labels": [], "created": "2006-07-27T18:29:05.000+0000", "updated": "2009-07-08T16:41:59.000+0000", "description": "I propose to reverse the order of shutdown in MiniDFSCluster.\nCurrently it shutdowns the name node first and then the data node,\nwhich causes the data node throwing unnecessary Exceptions in unit test.", "comments": ["With this patch the data node will shutdown before the name node.", "This is resolved. Patch was applied on 08/01/06.\n", "Forgot to resolve this earlier.  Thanks, Konstantin!"], "derived": {"summary": "I propose to reverse the order of shutdown in MiniDFSCluster. Currently it shutdowns the name node first and then the data node,\nwhich causes the data node throwing unnecessary Exceptions in unit test.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "MiniDFSCluster shudown order - I propose to reverse the order of shutdown in MiniDFSCluster. Currently it shutdowns the name node first and then the data node,\nwhich causes the data node throwing unnecessary Exceptions in unit test."}, {"q": "What updates or decisions were made in the discussion?", "a": "Forgot to resolve this earlier.  Thanks, Konstantin!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-395", "title": "infoPort field should be a DatanodeID member", "status": "Closed", "priority": "Major", "reporter": "Konstantin Shvachko", "assignee": "Devaraj Das", "labels": [], "created": "2006-07-27T22:39:38.000+0000", "updated": "2006-08-04T22:22:52.000+0000", "description": "I have a couple of comments on HADOOP-375. Sorry missed that discussion when the patch was under construction.\n1) As it is done now infoPort is a member of two classes DatanodeInfo and DatanodeRegistration, which in fact have a\ncommon base DatanodeID. It seems more logical to place infoPort into DatanodeID. This will let us handle the port\nassignments inside the constructors rather than outside, and will prevent from unsafe \"dynamic casts\" like the one\nfound in FSNamesystem.gotHeartbeat( DatanodeID nodeID, ... )\n            nodeinfo.infoPort = ((DatanodeRegistration)nodeID).infoPort;\n2) Also, should we make infoPort short rather than integer?\nSince it is a part of a heartbeat message we might want keeping it small.\n3) Member getters should start with get<MemberName>\n- public int infoPort() { return infoPort; }\n+ public int getInfoPort() { return infoPort; }\n", "comments": ["Attached is the patch.", "Noticed now that Konstantin's patch for Hadoop-396 will make this patch go stale. So I will submit another patch after Konstantin's patch is committed. Please don't commit this patch for now.", "The patch for HADOOP-396 fixes this.", "This was fixed as a part of HADOOP-396."], "derived": {"summary": "I have a couple of comments on HADOOP-375. Sorry missed that discussion when the patch was under construction.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "infoPort field should be a DatanodeID member - I have a couple of comments on HADOOP-375. Sorry missed that discussion when the patch was under construction."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was fixed as a part of HADOOP-396."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-396", "title": "Writable DatanodeID", "status": "Closed", "priority": "Major", "reporter": "Konstantin Shvachko", "assignee": "Konstantin Shvachko", "labels": [], "created": "2006-07-28T04:37:08.000+0000", "updated": "2009-07-08T16:41:58.000+0000", "description": "This patch implements Owen's comments to HADOOP-321. Namely,\n- Namenode.sendHeartbeat does not create a throwaway DatanodeDescriptor. DatanodeID is sufficient in this case.\n- DatanodeID is made Writable, and it also implements equals() and hashCode().\n- DatanodeInfo is updated to call base class methods in its implementation of Writable interface.\n- I changed the way strings are written/read using UTF8.writeString and UTF8.readString.\n- Some comments are updated.\n", "comments": ["Shouldn't DatanodeRegistration's readFields/write methods invoke the DatanodeID's corresponding methods (like you did for DatanodeInfo) ?", "Yes, thank you Devaraj, I missed that.\nI updated my patch to incorporate changes to the DatanodeID and derived classes\naddressed by both issues HADOOP-395 and HADOOP-396.\nIn addition to what was said in these issues DatanodeID2.patch\n- makes DatanodeRegistration call super.readFields/write for the Writable methods\n- simplifies constructors\n- removes redundant DataNode.infoPort, since DataNode.dnRegistration already contains it.\n- upgrades ipc version for DatanodeProtocol and ClientProtocol, since protocol structures are affected by infoPort member.\n", "I think you missed removing the lines that stores the infoPort after DatanodeDescriptor is created in gotHeartBeat and registerDatanode from FSNameSystem.java. With your current patch we don't need to explicitly store the infoPort anymore.", "I removed the two lines mentioned by Devaraj.", "And removed a redundant variable in registerDatanode().", "I just committed this.  Thanks, Konstantin!", "And Devaraj! That was a joint patch."], "derived": {"summary": "This patch implements Owen's comments to HADOOP-321. Namely,\n- Namenode.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Writable DatanodeID - This patch implements Owen's comments to HADOOP-321. Namely,\n- Namenode."}, {"q": "What updates or decisions were made in the discussion?", "a": "And Devaraj! That was a joint patch."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-397", "title": "c/c++ record io library does not use autoconf", "status": "Resolved", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Vivek Ratan", "labels": [], "created": "2006-07-28T20:11:45.000+0000", "updated": "2011-07-16T19:29:08.000+0000", "description": "It would be more convenient, if the C/C++ portion of record io used autoconf to configure the location of the required libraries and generate make files.", "comments": ["It should also use the same targets as the rest of Hadoop, like pipes does, so that \n\n{code}\nant -Dcompile.c++=yes tar \n{code}\n\ncompiles hadoop along with pipes, c++ utils, and librecordio into the release.\n", "Resolving as Won't Fix, since the whole recordio component is now deprecated in favor of Avro (and technically ought to be removed in 0.22/0.23). Please see https://issues.apache.org/jira/browse/HADOOP-6155"], "derived": {"summary": "It would be more convenient, if the C/C++ portion of record io used autoconf to configure the location of the required libraries and generate make files.", "classification": "configuration", "qna": [{"q": "What is the issue described?", "a": "c/c++ record io library does not use autoconf - It would be more convenient, if the C/C++ portion of record io used autoconf to configure the location of the required libraries and generate make files."}, {"q": "What updates or decisions were made in the discussion?", "a": "Resolving as Won't Fix, since the whole recordio component is now deprecated in favor of Avro (and technically ought to be removed in 0.22/0.23). Please see https://issues.apache.org/jira/browse/HADOOP-6155"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-399", "title": "the javadoc currently generates lot of warnings about bad fields", "status": "Closed", "priority": "Trivial", "reporter": "Owen O'Malley", "assignee": "Nigel Daley", "labels": [], "created": "2006-07-28T20:26:26.000+0000", "updated": "2006-11-03T22:40:11.000+0000", "description": "When building the \"tar\" target, I get the following warnings:\n\n  [javadoc] /home/oom/work/eclipse/hadoop-trunk/src/java/org/apache/hadoop/dfs/DFSck.java:475: warning - @return tag has no arguments.\n  [javadoc] /home/oom/work/eclipse/hadoop-trunk/src/java/org/apache/hadoop/dfs/DatanodeProtocol.java:46: warning - Tag @link: reference not found: DatanodeRegistration\n  [javadoc] /home/oom/work/eclipse/hadoop-trunk/src/java/org/apache/hadoop/dfs/DatanodeProtocol.java:46: warning - Tag @see: reference not found: FSNamesystem#registerDatanode(DatanodeRegistration)\n  [javadoc] /home/oom/work/eclipse/hadoop-trunk/src/java/org/apache/hadoop/io/BytesWritable.java:63: warning - @return tag has no arguments.\n  [javadoc] /home/oom/work/eclipse/hadoop-trunk/src/java/org/apache/hadoop/io/SequenceFile.java:252: warning - Tag @link: can't find Reader(FileSystem,Path,Configuration) in org.apache.hadoop.io.SequenceFile.Reader\n  [javadoc] /home/oom/work/eclipse/hadoop-trunk/src/java/org/apache/hadoop/io/SequenceFile.java:81: warning - Tag @link: can't find Writer(FileSystem,Path,Class,Class) in org.apache.hadoop.io.SequenceFile.Writer\n  [javadoc] /home/oom/work/eclipse/hadoop-trunk/src/java/org/apache/hadoop/io/Text.java:104: warning - @returns is an unknown tag.\n  [javadoc] /home/oom/work/eclipse/hadoop-trunk/src/java/org/apache/hadoop/io/Text.java:413: warning - @param argument \"utf8:\" is not a parameter name.\n  [javadoc] /home/oom/work/eclipse/hadoop-trunk/src/java/org/apache/hadoop/io/WritableUtils.java:338: warning - @param argument \"i:\" is not a parameter name.\n  [javadoc] /home/oom/work/eclipse/hadoop-trunk/src/java/org/apache/hadoop/mapred/JobConf.java:77: warning - @param argument \"conf\" is not a parameter name.\n  [javadoc] /home/oom/work/eclipse/hadoop-trunk/src/examples/org/apache/hadoop/examples/ExampleDriver.java:29: warning - Tag @author cannot be used in method documentation.  It can only be used in the following types of documentation: overview, package, class/interface.\n  [javadoc] /home/oom/work/eclipse/hadoop-trunk/src/examples/org/apache/hadoop/examples/ExampleDriver.java:29: warning - @date is an unknown tag.\n  [javadoc] /home/oom/work/eclipse/hadoop-trunk/src/examples/org/apache/hadoop/examples/PiBenchmark.java:62: warning - @param argument \"value\" is not a parameter name.\n  [javadoc] /home/oom/work/eclipse/hadoop-trunk/src/examples/org/apache/hadoop/examples/PiBenchmark.java:106: warning - @ is an unknown tag.\n", "comments": ["This patch fixes all the javadoc warnings.  Before committing any new patches, please ensure your patch doesn't generate any javadoc warnings. \n\nThis patch touches a large number of files, specifically these files:\n\nsrc/examples/org/apache/hadoop/examples/ExampleDriver.java\nsrc/examples/org/apache/hadoop/examples/PiBenchmark.java\nsrc/java/org/apache/hadoop/conf/Configuration.java\nsrc/java/org/apache/hadoop/dfs/DFSShell.java\nsrc/java/org/apache/hadoop/dfs/DFSck.java\nsrc/java/org/apache/hadoop/dfs/DatanodeProtocol.java\nsrc/java/org/apache/hadoop/dfs/DistributedFileSystem.java\nsrc/java/org/apache/hadoop/dfs/NamenodeFsck.java\nsrc/java/org/apache/hadoop/fs/FileSystem.java\nsrc/java/org/apache/hadoop/fs/Path.java\nsrc/java/org/apache/hadoop/io/BytesWritable.java\nsrc/java/org/apache/hadoop/io/SequenceFile.java\nsrc/java/org/apache/hadoop/io/Text.java\nsrc/java/org/apache/hadoop/io/WritableUtils.java\nsrc/java/org/apache/hadoop/io/compress/CompressionInputStream.java\nsrc/java/org/apache/hadoop/mapred/FileSplit.java\nsrc/java/org/apache/hadoop/mapred/JobConf.java\nsrc/java/org/apache/hadoop/mapred/JobHistory.java\nsrc/java/org/apache/hadoop/mapred/SequenceFileInputFilter.java\nsrc/java/org/apache/hadoop/mapred/StatusHttpServer.java\nsrc/java/org/apache/hadoop/mapred/TaskTracker.java\nsrc/java/org/apache/hadoop/util/ReflectionUtils.java\nsrc/java/org/apache/hadoop/util/StringUtils.java\nsrc/java/overview.html\nsrc/test/org/apache/hadoop/mapred/EmptyInputFormat.java\n", "I just committed this.  Thanks, Nigel!"], "derived": {"summary": "When building the \"tar\" target, I get the following warnings:\n\n  [javadoc] /home/oom/work/eclipse/hadoop-trunk/src/java/org/apache/hadoop/dfs/DFSck. java:475: warning - @return tag has no arguments.", "classification": "documentation", "qna": [{"q": "What is the issue described?", "a": "the javadoc currently generates lot of warnings about bad fields - When building the \"tar\" target, I get the following warnings:\n\n  [javadoc] /home/oom/work/eclipse/hadoop-trunk/src/java/org/apache/hadoop/dfs/DFSck. java:475: warning - @return tag has no arguments."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Nigel!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-400", "title": "the job tracker re-runs failed tasks on the same node", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-07-28T20:33:32.000+0000", "updated": "2009-07-08T16:51:51.000+0000", "description": "The job tracker tries not to run tasks that have previously failed on a node on that node again, but it doesn't strictly prevent it.\n\nI propose to change the rule so that when pollForNewTask is called by a TaskTracker, the JobTracker will only assign it a task that has failed on that TaskTracker, if and only if it has already failed on the entire cluster. Thus, for \"normal\" clusters with more than 4 TaskTrackers, you will be guaranteed that it will run on 4 different TaskTrackers. For small clusters, it will run on every TaskTracker in the cluster at least once.\n\nDoes that sound reasonable to everyone?", "comments": ["I do see one possible hole.  If a machine loses its TaskTracker, it gets a new one.  Can we arrange for the new TaskTracker to inherit the task failures from its predecessor?  That would be a bit hard ... but for this to work at all the tasks have to know what TaskTrackers they've flunked on.  All the TaskTracker has to know is who its predecessors are to refuse tasks that have flunked on its TaskTracker site [usually, on its machine].\n\n-dk\n", "This patch does:\n  1. It limits each TaskTracker to running\n        min(tasksPerTracker, ceil(tasksLeftToRun/numTaskTrackers))\n      this will prevent the problem that we saw where the last 2 reduces scheduled were put on the same node rather than different empty ones\n  2. It refactors obtainNewMapTask and obtainNewReduceTask to call a common utility function. It also replaces the two parallel loops with one.\n  3. Only allowed tasks that have failed on this task tracker to run if we have exhausted the cluster.", "I just committed this.  Thanks, Owen!"], "derived": {"summary": "The job tracker tries not to run tasks that have previously failed on a node on that node again, but it doesn't strictly prevent it. I propose to change the rule so that when pollForNewTask is called by a TaskTracker, the JobTracker will only assign it a task that has failed on that TaskTracker, if and only if it has already failed on the entire cluster.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "the job tracker re-runs failed tasks on the same node - The job tracker tries not to run tasks that have previously failed on a node on that node again, but it doesn't strictly prevent it. I propose to change the rule so that when pollForNewTask is called by a TaskTracker, the JobTracker will only assign it a task that has failed on that TaskTracker, if and only if it has already failed on the entire cluster."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-402", "title": "TaskTracker should shutdown when it receives the VersionMismatch exception.", "status": "Closed", "priority": "Major", "reporter": "Konstantin Shvachko", "assignee": null, "labels": [], "created": "2006-07-28T23:56:09.000+0000", "updated": "2009-07-08T16:51:47.000+0000", "description": "Like Datanodes TaskTracker should shutdown when it receives the VersionMismatch exception rather than\ninfinitely retrying to re-connect. The result will be the same.\n", "comments": [], "derived": {"summary": "Like Datanodes TaskTracker should shutdown when it receives the VersionMismatch exception rather than\ninfinitely retrying to re-connect. The result will be the same.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "TaskTracker should shutdown when it receives the VersionMismatch exception. - Like Datanodes TaskTracker should shutdown when it receives the VersionMismatch exception rather than\ninfinitely retrying to re-connect. The result will be the same."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-403", "title": "close method in a Mapper should be provided with OutputCollector and a Reporter", "status": "Closed", "priority": "Major", "reporter": "Milind Barve", "assignee": null, "labels": [], "created": "2006-07-31T22:09:38.000+0000", "updated": "2009-07-08T16:51:51.000+0000", "description": "For mappers with side-effects, or mappers that work as aggregators (i.e. no output on individual key-value pairs, but an aggregate output at the end of all key-value pairs), output should be performed in the close method. For this purpose, we need to supply output collector and reporter to the close method of Mapper. This involves interface change, though. Thoughts ?", "comments": ["+1 on this one. Reporter or Progressable objects should be passed into *any* call in this project, but I've also run into the need to output \"after all maps\", with no hook currently available to do it.", "I just checked that this can be done without changing the mapper interface. We can hang the Reporter and OutputCollector off of JobConf which is passed to configure method. I will attach a patch soon.", "I'm very uncomfortable with passing the Reporter and OutputCollector via the JobConf.\n\nIt does two bad things:\n  1. It passes \"real\" Java objects around in the JobConf, which breaks the assumption that the JobConf can be serialized successfully. (In this case, it is ok because it won't cross the process boundary, but it breaks the developers expectations.)\n  2. It hides the information that application writers need in a very hidden place. If I look at Mapper or Reducer, I won't see the information that I have available. Only if I scan through the HUGE JobConf API will I see the fact that they are avaiable.\n\nI strongly suggest that we just take the hit and extend the Closeable interface. I'd propose:\n  1. Making the Closeable.close() method depricated.\n  2. Add a new Closeable.close(OutputCollector, Reporter) method.\n  3. In MapReduceBase provide a default implementation that calls the close() method.\n\nThat should minimize the breakage in user code and still make the intended interface clear.", "\nIn my app, I added an OutputCollector variable (and a Reporter variable if necessary) in my Map/Reduce classes so that the close method can access them. Those variables are assigned only once my the first call to Map/Reduce method.\n\nIf the framework should offer help here, we can introduce to Closable a method setCollector(OutputCollector) and setReporter(Reporter). When the framework creates a Mapper/Reducer object, it calls these methods. \n\n", "I don't think we should make all future io.Closeable implementations deal with a mapred.OutputCollector.  I would prefer any such methods were added to a new interface, perhaps mapred.Output.\n\nOne can easily workaround this problem by specifying an alternate MapRunnable.  Is this issue really so common that this workaround, or Runping's, do not suffice?", "The upcoming task context eliminates the need for this."], "derived": {"summary": "For mappers with side-effects, or mappers that work as aggregators (i. e.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "close method in a Mapper should be provided with OutputCollector and a Reporter - For mappers with side-effects, or mappers that work as aggregators (i. e."}, {"q": "What updates or decisions were made in the discussion?", "a": "The upcoming task context eliminates the need for this."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-404", "title": "Regression tests are not working.", "status": "Closed", "priority": "Major", "reporter": "Mahadev Konar", "assignee": null, "labels": [], "created": "2006-08-01T01:41:48.000+0000", "updated": "2006-08-04T22:22:53.000+0000", "description": "TestMiniMRLocalFS gets stuck on running the reduce. THe reduce never seems to return. The test kept on running for more than an hour and the reduce never returned. I tried ant test with revision 427236\n which is just before the patch for HADOOP-362, and the tests seemed to work fine. I have not yet diagnozed the problem.", "comments": ["This was fixed by the revert of HADOOP-287."], "derived": {"summary": "TestMiniMRLocalFS gets stuck on running the reduce. THe reduce never seems to return.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Regression tests are not working. - TestMiniMRLocalFS gets stuck on running the reduce. THe reduce never seems to return."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was fixed by the revert of HADOOP-287."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-405", "title": "Duplicate browseDirectory.jsp", "status": "Closed", "priority": "Minor", "reporter": "Konstantin Shvachko", "assignee": "navychen", "labels": [], "created": "2006-08-01T02:24:43.000+0000", "updated": "2009-07-08T16:41:58.000+0000", "description": "I found 2 identical files\nsrc\\webapps\\datanode\\browseDirectory.jsp\nand\nsrc\\webapps\\dfs\\browseDirectory.jsp\n\nAnd there is a line in build.xml that makes the copy\n    <copy file=\"${src.webapps}/datanode/browseDirectory.jsp\" todir=\"${src.webapps}/dfs/\"/>\n\nThat does not seem right. If one of them is a copy of another, then only one should be in svn.\nAnother thing is that while copying files to the build/ directory is OK, copying files around within\nthe source directories makes things complicated. Is there a way to avoid that?", "comments": ["Yes, the file browseDirectory.jsp need not be there in src\\webapps\\dfs directory. It is anyway overwritten with a copy from src\\webapps\\datanode during the build process. Only one copy need to be there in svn.\nI think the *.jsp files have to be there in the src\\webapps\\* directories for the jsp compilation to complete successfully. Not sure whether we can avoid that copying.", "I agree with Konstantin: the build should not modify anything in the src tree.\n\nInstead of the copy, you can add <jspcompile jspfiles=\"../browseDirectory.jsp\" .../>.\n\nhttp://tomcat.apache.org/tomcat-5.5-doc/jasper/docs/api/org/apache/jasper/JspC.html\n", "we checked these two copies of browseDirectory.jsp and found these difference:\n\n1.  \"dfs/browseDirectory.jsp\":  if the input is a document, the page will be sent to \"browseBlock.jsp\" directly.  Otherwise, the links of sub-files will be displayed.\n   \"datanode/browseDirectory.jsp\":  if the input is a document, the page will be returned.  Otherwise, the links of sub-files will be displayed.\n2.  All the inputs of the two pages are the same.\n3.  All the pages work well in our cluster. We use \"dfs/browseDirectory.jsp\" in fact.\n\n4, \"dfs/browseDirectory.jsp\" is a newest version than \"datanode/browseDirectory.jsp\"\n5, \"datanode/browseDirectory.jsp\" is not changed from 0.7.x or early to 0.10.x\n6, \"dfs/browseDirectory.jsp\" is change from 0.9 and before 0.9 version of hadoop it is completeness same as \"datanode/browseDirectory.jsp\"\n7, hadoop actually build the \"dfs/browseDirectory.jsp\" to \"build/src/org/apache/hadoop/dfs/browseDirectory_jsp.java\", and dosen't compile \"datanode/browseDirectory.jsp\" yet\n8, so we may remove \"datanode/browseDirectory.jsp\"\n\nHere is our solution for HADOOP-405\n1, remove \"src/webapps/datanode/browseDirectory.jsp\" file\n2, remove line \"<copy file=\"${src.webapps}/datanode/browseDirectory.jsp\" todir=\"${src.webapps}/dfs/\"/>\" in build.xml\n\nWe checked it worked okay, maybe remove file has a little risk, but I think it maybe a good way for 405, isn't it?\n\nby navychen & weilei\n", "1, there are 2 identical files:\nsrc\\webapps\\datanode\\browseDirectory.jsp\nand\nsrc\\webapps\\dfs\\browseDirectory.jsp\n2, \"dfs/browseDirectory.jsp\" is a newest version than \"datanode/browseDirectory.jsp\"\n3, hadoop actually build the \"dfs/browseDirectory.jsp\" to \"build/src/org/apache/hadoop/dfs/browseDirectory_jsp.java\", and dosen't compile \"datanode/browseDirectory.jsp\" yet\n\nso the solution is\n1, remove \"src/webapps/datanode/browseDirectory.jsp\" file \n2, remove line \"<copy file=\"${src.webapps}/datanode/browseDirectory.jsp\" todir=\"${src.webapps}/dfs/\"/>\" in build.xml ", "+1, because http://issues.apache.org/jira/secure/attachment/12350097/405.patch applied and successfully tested against trunk revision r502030.", "I just committed this.  Thanks, navychen!"], "derived": {"summary": "I found 2 identical files\nsrc\\webapps\\datanode\\browseDirectory. jsp\nand\nsrc\\webapps\\dfs\\browseDirectory.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Duplicate browseDirectory.jsp - I found 2 identical files\nsrc\\webapps\\datanode\\browseDirectory. jsp\nand\nsrc\\webapps\\dfs\\browseDirectory."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, navychen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-406", "title": "Tasks launched by tasktracker in separate JVM can't generate log output", "status": "Closed", "priority": "Major", "reporter": "Chris Schneider", "assignee": null, "labels": [], "created": "2006-08-01T16:22:32.000+0000", "updated": "2009-07-08T16:51:52.000+0000", "description": "Child JVM's don't have access to logging config system properties. When the child JVM gets launched, it doesn't inherit the Java system properties hadoop.log.dir and hadoop.log.file (which are actually based on the Bash environment variables $HADOOP_LOG_DIR and $HADOOP_LOGFILE). This means that you get no log messages from the actual map/reduce tasks that are executing.\n\nStefan Groschupf reported this problem a while back:\n\n-------------------------------------------------------------------------\nTo: hadoop-dev@lucene.apache.org\nFrom: Stefan Groschupf <sg@media-style.com>\nSubject: tasks can't log bug?\nDate: Tue, 25 Jul 2006 19:26:17 -0700\nX-Virus-Checked: Checked by ClamAV on apache.org\n\nHi Hadoop developers,\n\nI'm confused about the way logging works within map or reduce tasks.\nSince tasks are launched in a new JVM  the java system properties \"hadoop.log.dir\" and \"hadoop.log.file\" are not passed to the new JVM.\nThis prevents the child process from logging properly. In fact you get:\n\n java.io.FileNotFoundException: / (Is a directory)\n  at java.io.FileOutputStream.openAppend(Native Method)\n  at java.io.FileOutputStream.<init>(FileOutputStream.java:177)\n  at java.io.FileOutputStream.<init>(FileOutputStream.java:102)\n  at org.apache.log4j.FileAppender.setFile(FileAppender.java:289)\n  at org.apache.log4j.RollingFileAppender.setFile(RollingFileAppender.java:165)\n  at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:163)\n  at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:256)\n  at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:132)\n  at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:96)\n  at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:654)\n  at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:612)\n  at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.j\n2006-07-25 15:59:07,553 INFO  mapred.TaskTracker (TaskTracker.java:main(993)) - Child\n  at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:415)\n  at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:441)\n  at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:4\n  at org.apache.log4j.LogManager.<clinit>(LogManager.java:122)\n  at org.apache.log4j.Logger.getLogger(Logger.java:104)\n  at org.apache.commons.logging.impl.Log4JLogger.getLogger(Log4JLogger.java:229)\n  at org.apache.commons.logging.impl.Log4JLogger.<init>(Log4JLogger.java:65)\n  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImp\n  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAcc\n  at java.lang.reflect.Constructor.newInstance(Constructor.java:494)\n  at org.apache.commons.logging.impl.LogFactoryImpl.newInstance(LogFactoryImpl.java:529\n  at org.apache.commons.logging.impl.LogFactoryImpl.getInstance(LogFactoryImpl.java:235\n  at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:370)\n  at org.apache.hadoop.mapred.TaskTracker.<clinit>(TaskTracker.java:44)\n  at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:993)\n\nWe see several ways to solve this problem. First retrieve the properties \"hadoop.log.dir\" and \"hadoop.log.file\" from the mother JVM and then pass them to the child JVM as within the args parameter.\nSecond would be to  access the environment variables \"$HADOOP_LOG_DIR\" and \"$HADOOP_LOGFILE\" using System.getEnv (java 1.5).\nThird there would be a more general solution. Taskrunner would resolve any environment variables it found in \"mapred.child.java.opts\" by lookup the value using System.getEnv().\nEg:\nunix:\nexport MAX_MEMORY = 200\nhadoop-site.xml:\n<name>mapred.child.java.opts</name>\n<value>-Xmx${MAX_MEMORY}</value>\n", "comments": ["Here's a patch that provides one solution to the problem.\n\nI modified TaskRunner.java so that it supports elements like -Dhadoop.log.dir=@property[hadoop.log.dir]@ within the mapred.child.java.opts Hadoop configuration property (replacing @property[hadoop.log.dir]@ with this property value).\n\nIf others agree that this is the appropriate way to solve the problem, then we should probably modify mapred.child.java.opts in hadoop-default.xml to include all 5 of the items in the default $HADOOP_OPTS (hadoop.log.dir, hadoop.log.file, hadoop.home.dir, hadoop.id.str, and hadoop.root.logger), using this method. A few notes, though:\n\n1) This creates yet another dependency on the contents of $HADOOP_OPTS (i.e., anyone that adds items to this should probably add them to mapred.child.java.opts as well.) This is less than elegant.\n\n2) It doesn't seem like hadoop.home.dir, hadoop.id.str, and hadoop.root.logger are actually being used currently by the code. Both hadoop.home.dir and hadoop.id.str are loaded by LogFormatter.initFileHandler(), but that method doesn't seem to be used by anyone. It doesn't seem like anyone is loading hadoop.root.logger at all.", "This issue bit me as well, and it took me some time to unravel the mystery. It would be nice to fix it in the main distribution!\n\nI'm agnostic about what the long-term \"right\" solution is, but Chris's patch certainly works for me. (Thanks, Chris!)\n\nDoug", "The way this is currently supposed to work is that child processes log to standard output, and standard output is logged by the parent process (the task tracker).  Is that not working for you?\n\nLonger-term, we probably do not want child processes opening log files directly.  Rather, they should be configured with a logger that, for each line logged, makes an RPC to the parent process, passing the level and log message.  Then the tasktracker can log things accordingly.\n", "Hi, Doug.\n\nThe task trackers were definitely not logging the fetcher output until I applied and enabled this patch. Perhaps there is another bug, or something I simply misconfigured (I am rather new at this). I am running on a single MP machine; don't know if that makes any difference.\n\nAs I said, I'm agnostic about the solution, but given discussions about this on the Nutch forum,  there seem to be others who see the same problem.\n", "Since I don't see this, it is hard for me to guess what's going on.  Perhaps you can tell more about your installation: OS, JVM, config, etc.  Thanks.", "Sorry it took me so long to get back to this. I have just integrated Hadoop 0.5 and was about to back out my changes and install a more standard configuration when I noticed what I believe is the problem. The log4j.properties file we use doesn't default to the console logger like the standard log4j.properties does.\n\nOur log4j.properties file:\n\n  # RootLogger - DailyRollingFileAppender\n  log4j.rootLogger=INFO,DRFA\n\nVanilla log4j.properties file:\n\n  hadoop.root.logger=INFO,console\n\nThus, when the child JVM is launched, it's going to try to use our DRFA logger, not send its log output to standard output. Note that before Owen fixed HADOOP-279, running without the hadoop script broke logging (probably in a similar way), and the child JVM is launched without the hadoop script.\n\nOwen should probably comment on this, but it seems that the \"child processes log to standard output, and standard output is logged by the parent process\" design requires that every log4j.properties file must have console in its hadoop.root.logger.", "I ran into this problem with hadoop 0.9.2, too. Upgrading log4j to version 1.2.14 seems to have solved the problem.\n\nrlog4j:ERROR setFile(null,true) call failed.\njava.io.FileNotFoundException: / (Invalid argument)\n\tat java.io.FileOutputStream.openAppend(Native Method)\n\tat java.io.FileOutputStream.(FileOutputStream.java:177)\n\tat java.io.FileOutputStream.(FileOutputStream.java:102)\n\tat org.apache.log4j.FileAppender.setFile(FileAppender.java:289)\n\tat org.apache.log4j.FileAppender.activateOptions(FileAppender.java:163)\n\tat org.apache.log4j.DailyRollingFileAppender.activateOptions(DailyRollingFileAppender.java:215)\n\tat org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:256)\n\tat org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:132)\n\tat org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:96)\n\tat org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:654)\n\tat org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:612)\n\tat org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:509)\n\tat org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:415)\n\tat org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:441)\n\tat org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:468)\n\tat org.apache.log4j.LogManager.(LogManager.java:122)\n\tat org.apache.log4j.Logger.getLogger(Logger.java:104)\n\tat org.apache.commons.logging.impl.Log4JLogger.getLogger(Log4JLogger.java:229)\n\tat org.apache.commons.logging.impl.Log4JLogger.(Log4JLogger.java:65)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:494)\n\tat org.apache.commons.logging.impl.LogFactoryImpl.newInstance(LogFactoryImpl.java:529)\n\tat org.apache.commons.logging.impl.LogFactoryImpl.getInstance(LogFactoryImpl.java:235)\n\tat org.apache.commons.logging.LogFactory.getLog(LogFactory.java:370)\n\tat org.apache.hadoop.mapred.TaskTracker.(TaskTracker.java:57)\n\tat org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1367)\nlog4j:ERROR Either File or DatePattern options are not set for appender [DRFA].\n\n", "Please ignore my previous comment. It worked after the upgrade only because there were no mapred errors that had to be logged. I should have read the all comments to figure out that the log appender causes this problem.", "This was resolved by giving the child jvm a special logger that captures it as part of the job and provides it via http."], "derived": {"summary": "Child JVM's don't have access to logging config system properties. When the child JVM gets launched, it doesn't inherit the Java system properties hadoop.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "Tasks launched by tasktracker in separate JVM can't generate log output - Child JVM's don't have access to logging config system properties. When the child JVM gets launched, it doesn't inherit the Java system properties hadoop."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was resolved by giving the child jvm a special logger that captures it as part of the job and provides it via http."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-407", "title": "the SequenceFile sorter should take a Progressable that should be used in map/reduce sort", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-08-01T17:37:48.000+0000", "updated": "2009-07-08T16:51:51.000+0000", "description": "Currently, when the framework is sorting it creates a side thread that sends fake progress reports back to the task tracker once a second. This can cause problems if there is a problem in the sort getting stuck. It would be far better to have the sort take a Progressable that can report that progress is being made in the sort.", "comments": ["This was addressed in HADOOP-1462"], "derived": {"summary": "Currently, when the framework is sorting it creates a side thread that sends fake progress reports back to the task tracker once a second. This can cause problems if there is a problem in the sort getting stuck.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "the SequenceFile sorter should take a Progressable that should be used in map/reduce sort - Currently, when the framework is sorting it creates a side thread that sends fake progress reports back to the task tracker once a second. This can cause problems if there is a problem in the sort getting stuck."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was addressed in HADOOP-1462"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-408", "title": "Unit tests take too long to run.", "status": "Closed", "priority": "Minor", "reporter": "Doug Cutting", "assignee": null, "labels": [], "created": "2006-08-01T17:59:19.000+0000", "updated": "2006-09-08T21:19:48.000+0000", "description": "Several of the unit tests take over a minute to run.  I suspect most of this time is waiting for daemons to stop.  Perhaps we can restructure daemon shutdown in the test frameworks so that all daemons are first asked to shutdown, and then wait for the actual shutdown of all daemons in the TestCase's tearDown() method.  Also, we might try to combine some unit tests that require the same daemons under a single TestCase caller, so that they share setup/tearDown costs.  Or we could try to simply make daemon shutdowns faster.", "comments": ["I just committed a partial patch for this.  With these changes, the total time for unit tests is down from over nine minutes to under six minutes."], "derived": {"summary": "Several of the unit tests take over a minute to run. I suspect most of this time is waiting for daemons to stop.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "Unit tests take too long to run. - Several of the unit tests take over a minute to run. I suspect most of this time is waiting for daemons to stop."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed a partial patch for this.  With these changes, the total time for unit tests is down from over nine minutes to under six minutes."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-409", "title": "expose JobConf properties as environment variables", "status": "Closed", "priority": "Major", "reporter": "Michel Tourn", "assignee": null, "labels": [], "created": "2006-08-01T18:00:35.000+0000", "updated": "2006-08-04T22:22:53.000+0000", "description": "expose JobConf properties as environment variables (for the benefit of hadoopStreaming apps)\n\nThis patch depends on  HADOOP-345\n", "comments": ["I just committed this.  Thanks, Michel!"], "derived": {"summary": "expose JobConf properties as environment variables (for the benefit of hadoopStreaming apps)\n\nThis patch depends on  HADOOP-345.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "expose JobConf properties as environment variables - expose JobConf properties as environment variables (for the benefit of hadoopStreaming apps)\n\nThis patch depends on  HADOOP-345."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Michel!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-410", "title": "Using HashMap instead of TreeMap for some maps in Namenode yields 17% performance improvement", "status": "Closed", "priority": "Major", "reporter": "Milind Barve", "assignee": "Milind Barve", "labels": [], "created": "2006-08-01T21:10:14.000+0000", "updated": "2009-07-08T16:41:59.000+0000", "description": "For blocksMap in FSNameSystem and activeBlocks map in FSDirectory, if we use HashMap instead of TreeMap, it yields 17% performance improvement (without significant difference in memory consumption).\nI am attaching a patch, alongwith a namenode benchmark.", "comments": ["patch attached.", "I just committed this.  Thanks, Milind!"], "derived": {"summary": "For blocksMap in FSNameSystem and activeBlocks map in FSDirectory, if we use HashMap instead of TreeMap, it yields 17% performance improvement (without significant difference in memory consumption). I am attaching a patch, alongwith a namenode benchmark.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "Using HashMap instead of TreeMap for some maps in Namenode yields 17% performance improvement - For blocksMap in FSNameSystem and activeBlocks map in FSDirectory, if we use HashMap instead of TreeMap, it yields 17% performance improvement (without significant difference in memory consumption). I am attaching a patch, alongwith a namenode benchmark."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Milind!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-411", "title": "junit test for HADOOP-59: support generic command-line options", "status": "Closed", "priority": "Major", "reporter": "Hairong Kuang", "assignee": "Hairong Kuang", "labels": [], "created": "2006-08-01T22:00:50.000+0000", "updated": "2009-07-08T16:41:59.000+0000", "description": "This issue will proivde Junit test cases for HADOOP-59.", "comments": ["I just committed this.  Thanks, Hairong!", "This writes files in the conf directory that it does not remove.  It would be better to write these in the build directory."], "derived": {"summary": "This issue will proivde Junit test cases for HADOOP-59.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "junit test for HADOOP-59: support generic command-line options - This issue will proivde Junit test cases for HADOOP-59."}, {"q": "What updates or decisions were made in the discussion?", "a": "This writes files in the conf directory that it does not remove.  It would be better to write these in the build directory."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-412", "title": "provide an input format that fetches a subset of sequence file records", "status": "Closed", "priority": "Major", "reporter": "Hairong Kuang", "assignee": "Hairong Kuang", "labels": [], "created": "2006-08-02T00:07:08.000+0000", "updated": "2013-05-02T02:28:59.000+0000", "description": "Sometimes a map/red job only wants to work on a subset of input data for the needs of its apllication or at the debugging phase. It would be convenient if an input format transparently handles this. It should provide an API that allows a programmer to specify a filtering criteria.", "comments": ["This patch provides class SequenceFileInputFilter that can feed a subset of sequence file records to map tasks.  It provides a class method setFilter that defines a flltering criteria.\n\nThe patch provides three Filters: RegexFilter, PercentFilter, and MD5Filter. But a programmer may define its own filter. Any user-defined filter should either implements interface Filter or extend from FilterBase.\n\nA junit test is also included.", "The patch to issue 412 uses the method newInstance in ReflectionUtils.", "I just committed this, along with the patch from HADOOP-376 that it requires.  Thanks, Hairong!"], "derived": {"summary": "Sometimes a map/red job only wants to work on a subset of input data for the needs of its apllication or at the debugging phase. It would be convenient if an input format transparently handles this.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "provide an input format that fetches a subset of sequence file records - Sometimes a map/red job only wants to work on a subset of input data for the needs of its apllication or at the debugging phase. It would be convenient if an input format transparently handles this."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this, along with the patch from HADOOP-376 that it requires.  Thanks, Hairong!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-413", "title": "streaming: replace class UTF8 with class Text", "status": "Closed", "priority": "Major", "reporter": "Michel Tourn", "assignee": "Hairong Kuang", "labels": [], "created": "2006-08-02T00:27:35.000+0000", "updated": "2009-07-08T17:05:35.000+0000", "description": "need to support long lines in hadoopStreaming.\nSo replace class UTF8 with class Text or with line-oriented binary I/O\n", "comments": ["This was fixed as part of HADOOP-499"], "derived": {"summary": "need to support long lines in hadoopStreaming. So replace class UTF8 with class Text or with line-oriented binary I/O.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "streaming: replace class UTF8 with class Text - need to support long lines in hadoopStreaming. So replace class UTF8 with class Text or with line-oriented binary I/O."}, {"q": "What updates or decisions were made in the discussion?", "a": "This was fixed as part of HADOOP-499"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-414", "title": "replace class UTF8 with class Text", "status": "Open", "priority": "Major", "reporter": "Hairong Kuang", "assignee": "Owen O'Malley", "labels": [], "created": "2006-08-02T01:22:16.000+0000", "updated": "2014-07-18T05:00:34.000+0000", "description": "Since class UTF8 is deprecated, all references of UTF8 in hadoop should be replaced with class Text if the change does not break the system. ", "comments": ["I think we should target this for 0.6, and just deprecate UTF8 in 0.5.  Does that make sense?", "I agree, it seems reasonable practice to deprecate the class in 0.5 and give people enough time to prepare for removal.\n\n\n\n", "This is a partial patch to the problem including changes to packages mapred, ipc, and test.\n\nThe dfs is kept untouched. It is not trival to keep dfs  backwards compatible because writables are not versioned. Will work on it later on.", "It would be nice if RPC.Invocation were versioned, since you've changed its format.  Old clients talking to new servers (and vice versa) will fail.  It would be nice if these failures were accompanied by version-mismatch exceptions.\n\nWhat if we start sending #xFF at the start of an Invocation as the version?  The first thing read by the old code is the method name, a UTF8.  So if old client code talks to a new daemon, then, unless the method name has 2^16-1 characters, we could log a version-mismatch and close the connection.  With a bit more work, we could even construct a version-mismatch error (using UTF8) that the client could read.\n\nIf new client code talks to an old server, the old server would try to read a method name with 2^16-1 bytes, log an EOF error and close the connection.  Is that acceptable?\n\nOr should we punt on this?\n", "There is a worse problem in that none of the hadoop.io.* Writable classes are versioned.\n\nIn particular, we would like to move to the new method of writing Strings (Text.writeString), but can't very easily because even if the file versions and the rpc protocol versions change, that doesn't get passed down to readFields.\n\nWe really need to find a story that works for versioning Writables. I guess we could move everything over to VersionedWritable and that would work at the cost of 1 byte/Writable.\n\nThoughts?", "> we would like to move to the new method of writing Strings (Text.writeString), but can't very easily because even if the file versions and the rpc protocol versions change, that doesn't get passed down to readFields\n\nI'm not sure what you're saying.  If we, in a single commit, change from UTF8 to Text  in RPC parameter classes and in classes written to disk, and increase the version numbers of the file formats and RPC protocols involved, then we'd be safe, no?  The problem is that if we change from UTF8 to Text in the RPC system, when transmitting class names, since this has no version.  It would be good to get a version number here.\n\nMore generally, it would be useful to have a per-Writable class version number.  This could be used when setting up RPC connections, and stored in file formats, etc.  But that would not solve the current problem with the RPC system.\n\nMaybe we should punt on the RPC system for this patch for now, and just make the changes that are easy, leaving that for another day?", "Only versioning file formats and RPC protocols does not completely solve the problem, for example, when a Writable class contains a field of String. Because the field is deserialied by readField and readField does not have the version information, so we have no way to find out if we should use UTF8.readString or Text.readString. I guess either we should support Writable versioning or support a method readField(DataInput in, int version).", "We seem to be talking at cross purposes.  Are we still talking about the patch attached to this bug?  Of course, if one changes a Writable from using UTF8 to Text, and makes no other changes, then one will have problems reading old data and/or with protocol compatibility.  Folks who wish to be able to do things like this in user data should use VersionedWritable.  But the question I was raising was about migrating internal uses of UTF8 to Text.  For most of those, incrementing protocol version numbers and container format version numbers will suffice.  The only exception I know of is the one I discussed above, namely the names of parameter classes used in RPC.  That's the issue I was trying to raise here, not the general issue of version compatiblity for Writable.\n\nUsers who have data files that use UTF8 may wish to upgrade their data to use Text.  That's outside the scope of this bug, I think.  Here we should be concerned with replacing internal uses.", "I was not talking about user data. Instead I was concerned about replacing data internal uses. The following is an example that illustreates my point.\n\nWhen we load the edit log, if the operation name is OP_DATANODE_ADD, we need to read a DataNodeDescriptor, which is a Writable, by calling DataNodeDescriptor.readField. The class DataNodeDescriptor has a field \"name\" with type String. Although FSEditLog has version info, but DataNodeDescriptor does not. So the readFiied method does not know how to read the data node name, whether to use UTF8.readString or Text.readString. So either we should support Writable versioning or we need to pass version number to readField.", "Perhaps we should add a new abstract base class:\n\npublic abstract class ImplicitVersionedWritable implements Writable {\n  public abstract readFields(DataInputStream in, int version);\n  public abstract int currentVersion();\n  public readFields(DataInputStream in) {\n    readFields(in, currentVersion());\n  }\n}\n\nThen change DatanodeID to extend this:\n\npublic class DatanodeID\n  implements WritableComparable\n  extends ImplicitVersionedWritable {\n\n  public int currentVersion() { return FSConstants.DFS_CURRENT_VERSION; }\n  public void readFields(DataInputStream in, int version) {\n    if (version >= -3) {\n      name = UTF8.readString();\n      ...\n    } else {\n      name = Text.readString();\n      ...\n    }\n  }\n\nWould that work?", "Added a vote to this issue.  HBase spends a bunch of time rpc'ing.  A loads of our time is spent in UTF8 class doing reading and writing on the wire.  A primitive test has the Text.readString/writeString being a good bit faster than the UTF8 equivs so would be great if RPC moved to off UTF8 and on to Text.", "+1\n", "On adding a base abstract ImplicitVersionedWritable, I can see how injection of application version can be done easily for application-specific Comparables -- as is done above for the DFS-specific DatanodeID class -- but I'm wondering how would the version be injected when an application uses  generic Comparables such as BytesWritable?  Would an application have to pass its version on every instance creation? E.g. 'new BytesWritable(byte [] b, int version)'?", "My vote also on this issue. From some profiling I did, it looks like the NameNode is spending 2% of the CPU time on UTF8, and it could save about 40% of it by using Text. ", "At this point, is it even possible to replace UTF8 with Text? Or has it already been done?"], "derived": {"summary": "Since class UTF8 is deprecated, all references of UTF8 in hadoop should be replaced with class Text if the change does not break the system.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "replace class UTF8 with class Text - Since class UTF8 is deprecated, all references of UTF8 in hadoop should be replaced with class Text if the change does not break the system."}, {"q": "What updates or decisions were made in the discussion?", "a": "At this point, is it even possible to replace UTF8 with Text? Or has it already been done?"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-415", "title": "DFSNodesStatus() should sort data nodes.", "status": "Closed", "priority": "Major", "reporter": "Konstantin Shvachko", "assignee": null, "labels": [], "created": "2006-08-02T01:38:36.000+0000", "updated": "2009-07-08T16:41:58.000+0000", "description": "HADOOP-392 introduced a new TreeMap member (datanodeMapByName) in the name node.\nIt is used solely for reporting the data nodes in the UI sorted by their names.\nI think it is quite inefficient both time and space-wise to support an excessive\ndata structure just for that.\nSuppose the UI will also require sorting by last heartbeat and/or by the available space....\nI think DFSNodesStatus() should just sort the original datanodeMap before returning the list.\n", "comments": ["Actually, I think that it would be even better to make DFSNodesStatus() a member of the JspHelper  class.\nAnd sorting should be called directly in the jsp file.\nIn general it would be good to keep all data collection/processing in the JspHelper class,\nand let the jsp-files just output information.", "Agree with this. Attached is the patch. In dfshealth.jsp, I also changed the DatanodeInfo references to DatanodeDescriptor to reflect the current code.", "DatanodeDescriptor is supposed to be used only be the namenode internally.\nThe class for external use is DatanodeInfo. See HADOOP-321.\nThat is why DatanodeInfo is public and DatanodeDescriptor is not.\nSo using DatanodeInfo in dfshealth.jsp was actually right.\nI should add some comments in JavaDoc on that.", "Well, that should be okay since the jsp file actually belongs to the dfs package. But in any case, I have attached a new patch with the DatanodeDescriptor changes reverted.", "I just committed this.  Thanks, Devaraj!", "I mistakenly submitted the same patch file that I submitted the first time. Sorry. Thanks to Konstantin for pointing it out. I have now placed the patch with just the DatanodeDescriptor -> DatanodeInfo changes. This will apply cleanly to the current trunk.", "\n   [[ Old comment, sent by email on Mon, 07 Aug 2006 10:32:18 -0700 ]]\n\nThis issue is closed now.\nYou probably need to create a new issue and attach your patch there.\nI don't think it is possible to reopen closed issues.\n--Konstantin\n\n\n"], "derived": {"summary": "HADOOP-392 introduced a new TreeMap member (datanodeMapByName) in the name node. It is used solely for reporting the data nodes in the UI sorted by their names.", "classification": "feature_request", "qna": [{"q": "What is the issue described?", "a": "DFSNodesStatus() should sort data nodes. - HADOOP-392 introduced a new TreeMap member (datanodeMapByName) in the name node. It is used solely for reporting the data nodes in the UI sorted by their names."}, {"q": "What updates or decisions were made in the discussion?", "a": "[[ Old comment, sent by email on Mon, 07 Aug 2006 10:32:18 -0700 ]]\n\nThis issue is closed now.\nYou probably need to create a new issue and attach your patch there.\nI don't think it is possible to reopen closed issues.\n--Konstantin"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-417", "title": "Replace Jetty5 with Jetty6", "status": "Closed", "priority": "Major", "reporter": "Marco Nicosia", "assignee": "Owen O'Malley", "labels": [], "created": "2006-08-02T02:51:45.000+0000", "updated": "2009-07-08T16:51:51.000+0000", "description": "Possibly a solution for HADOOP-401?\n", "comments": ["I realize it may be premature to file this, there are newer versions of Jetty5 we could try first. But it seems from the mortbay site, that even though Jetty6 has only gone GA pretty recently, they're really pushing it. I expect we'll have to upgrade sooner or later, hence the bug.\n", "Warning, jetty6 has not been released as a production version, and still contains some bugs.", "Duplicate of HADOOP-1650."], "derived": {"summary": "Possibly a solution for HADOOP-401?.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Replace Jetty5 with Jetty6 - Possibly a solution for HADOOP-401?."}, {"q": "What updates or decisions were made in the discussion?", "a": "Duplicate of HADOOP-1650."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-418", "title": "hadoopStreaming test jobconf -> env.var. mapping", "status": "Closed", "priority": "Major", "reporter": "Michel Tourn", "assignee": null, "labels": [], "created": "2006-08-02T14:59:24.000+0000", "updated": "2006-08-04T22:22:55.000+0000", "description": "The subprocess loads environment variables and tests for expected JobConf properties.\n\n", "comments": ["I just committed this.  Thanks, Michel!"], "derived": {"summary": "The subprocess loads environment variables and tests for expected JobConf properties.", "classification": "test", "qna": [{"q": "What is the issue described?", "a": "hadoopStreaming test jobconf -> env.var. mapping - The subprocess loads environment variables and tests for expected JobConf properties."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Michel!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-419", "title": "libdfs doesn't work with application threads", "status": "Closed", "priority": "Major", "reporter": "Christian Kunz", "assignee": "Owen O'Malley", "labels": [], "created": "2006-08-02T19:33:01.000+0000", "updated": "2009-07-08T17:05:34.000+0000", "description": "Using libdfs from C++ using threads I get a null pointer for the ThreadClassLoader, which throws an exception.", "comments": ["This patch, pulls the classloader stuff out of JobConf and puts it into ReflectionUtils. It also fixes this bug by using the default class loader if there isn't a thread specific class loader.", "Previously different configurations and threads were permitted to use different classloaders.  After this patch, all use the same classloader.  I'm not sure that's always right.  Ideally, shouldn't Configuration.getResource() and Configuration.getClass(\"foo.class\") be able to refer to configuration-specific resources, like the job jar?\n", "ok, after chatting with doug, here is a different version.\n\nIt leaves the class loader in the Configuration, but it exposes a getClassByName method that allows a class lookup by class name rather than by attribute name.\n\nIf the thread does not have a class loader, it will use the default, which fixes the precise bug that Christian was hitting.\n\nIt also pulls all of the other places that were using the thread's class loader to find classes into the same code, based on the config. Note that some of the writeObject methods needed to have their signatures changed to pass in the config object so that it was clear what class loader should be used.\n", "I just committed this.  I changed NullWritable to extend Configured instead of implenting Configurable, saving a few lines of code.  Thanks, Owen!"], "derived": {"summary": "Using libdfs from C++ using threads I get a null pointer for the ThreadClassLoader, which throws an exception.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "libdfs doesn't work with application threads - Using libdfs from C++ using threads I get a null pointer for the ThreadClassLoader, which throws an exception."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  I changed NullWritable to extend Configured instead of implenting Configurable, saving a few lines of code.  Thanks, Owen!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-420", "title": "Accumulate bytes & records statistics at the job level via haoop.metrics.", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Milind Barve", "labels": [], "created": "2006-08-02T20:15:29.000+0000", "updated": "2007-09-27T18:37:23.000+0000", "description": "We'd like to accumulate the statistics (and get graphs) at the job level rather than the job level, because it will provide a lot more useful information. This likely implies that they need to be rolled up in the TaskStatus reported up to the JobTracker.", "comments": ["This is already done."], "derived": {"summary": "We'd like to accumulate the statistics (and get graphs) at the job level rather than the job level, because it will provide a lot more useful information. This likely implies that they need to be rolled up in the TaskStatus reported up to the JobTracker.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "Accumulate bytes & records statistics at the job level via haoop.metrics. - We'd like to accumulate the statistics (and get graphs) at the job level rather than the job level, because it will provide a lot more useful information. This likely implies that they need to be rolled up in the TaskStatus reported up to the JobTracker."}, {"q": "What updates or decisions were made in the discussion?", "a": "This is already done."}]}}
{"project": "HADOOP", "issue_id": "HADOOP-421", "title": "replace String in hadoop record io with the new Text class", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Milind Barve", "labels": [], "created": "2006-08-02T20:30:43.000+0000", "updated": "2006-09-08T21:19:49.000+0000", "description": "The record io in Java is currently using String and should be using the new Text class.", "comments": ["Attached patch that replaces String with Text in hadoop-record I/O. Also changes xml-encoding of strings. Earlier it used url encoding to encode and decode strings. Now it uses xml 1.0 compliant character entities.", "Earlier patch had a bug in xml escaping of special chars. I have removed that patch from the bug reports, and have added this fixed patch.", "I just committed this.  Thanks, Milind!"], "derived": {"summary": "The record io in Java is currently using String and should be using the new Text class.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "replace String in hadoop record io with the new Text class - The record io in Java is currently using String and should be using the new Text class."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Milind!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-422", "title": "ant test is failing with TestLocalDFS and TestMiniMRWithDFS failing", "status": "Closed", "priority": "Major", "reporter": "Mahadev Konar", "assignee": null, "labels": [], "created": "2006-08-03T01:50:48.000+0000", "updated": "2006-12-15T23:02:19.000+0000", "description": "org.apache.hadoop.ipc.RPC$VersionMismatch: Protocol org.apache.hadoop.dfs.ClientProtocol version mismatch. (client = 1, server = 2)\n        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:250)\n        at org.apache.hadoop.dfs.DFSClient.<init>(DFSClient.java:103)\n        at org.apache.hadoop.dfs.DistributedFileSystem.<init>(DistributedFileSystem.java:47)\n        at org.apache.hadoop.fs.FileSystem.getNamed(FileSystem.java:101)\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:86)\n        at org.apache.hadoop.dfs.MiniDFSCluster.getFileSystem(MiniDFSCluster.java:146)\n        at org.apache.hadoop.dfs.TestLocalDFS.testWorkingDirectory(TestLocalDFS.java:42)\n\nThis is the error I am getting when running ant test on TestLocalDFS.\nIts the same error (version mismatch) for TestMiniMRWithDFS.", "comments": ["try doing ant clean, before ant test"], "derived": {"summary": "org. apache.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "ant test is failing with TestLocalDFS and TestMiniMRWithDFS failing - org. apache."}, {"q": "What updates or decisions were made in the discussion?", "a": "try doing ant clean, before ant test"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-423", "title": "file paths are not normalized", "status": "Closed", "priority": "Major", "reporter": "Christian Kunz", "assignee": "Wendy Chien", "labels": [], "created": "2006-08-03T20:25:53.000+0000", "updated": "2009-07-08T16:42:00.000+0000", "description": "Paths containing  '.' or '..' are not normalized -- the corresponding directories actually seem to be created. This is inconsistent and unexpected behavior, and  is particularly painful when running from a working directory, using relative paths that start with ...", "comments": ["To fix this, we'll detect and normalize relative paths in the Path class.  For backwards compatibility, we'll allow people to rename or remove existing files/dirs that contain '.' or '..' .  Or do people think that's not necessary because it's unlikely anyone has been using '.' and '..' in their paths intentionally?  (Though I suppose if they did unintentionally, it might be nice to let them remove them.)\n      ", "The patch also includes changes in Path.java as well as added tests in TestPath.java.", "I think, as a safeguard, we should probably also disallow \".\" and \"..\" as file or directory names at the namenode, throwing an exception for attempts to create directories or files with these names.\n\nWhile we're at it, should we also prohibit \"/\" and \":\" in names?  I think so.  Currently it is impossible to create a Path with a slash in a name, although colons are permitted.", "I can add a check to disallow \".\" and \"..\" (and prohibit \":\") as file or directory names.  Should this be done in DFSClient or the namenode?  We can save an rpc by putting it in the client.", "I think we want it as a sanity check in the namenode, so that we're sure they never enter the filesystem.  I don't think it's worth trying to optimize this.", "This patch also contains the check in the namenode. ", "I think the check in the NameNode isn't quite right.  We should never permit \".\" or \"..\" at any point in a path here, since paths should always be fully qualifited and normalized by the time they reach the namenode, right?\n\nAlso, it seems a shame to have to call Path.toString().  Perhaps we should add a Path method like:\n\n  public String element(int i) { return elements[i]; }\n\nThen the path checker could be implemented with something like:\n\nif (!path.isAbsolute()) {\n  throw ... \n}\nfor (int i = 0; i < path.depth(); i++) {\n  String element = path.element(i);\n  if (element.equals(\"..\") || element.equals(\".\") || element.indexOf(\"/\") >= 0) {\n    throw ...\n  }\n}\n", "You're right, I've changed the logic in the namenode check.\n\nI don't understand where you don't want to call Path.toString(), in DFSClient?  The namenode only gets a String, which I parse for the offending sequences.    Do you mean we should convert it to a Path there?", "> I don't understand where you don't want to call Path.toString(), in DFSClient?\n\nYou're right.  I mistakenly assumed that src.toString() was used to convert a Path to a String, when it's really used to convert a UTF8 to a String.  Sorry!", "I just committed this.  Thanks, Wendy!"], "derived": {"summary": "Paths containing  '. ' or '.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "file paths are not normalized - Paths containing  '. ' or '."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks, Wendy!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-424", "title": "mapreduce jobs fail when no split is returned via inputFormat.getSplits", "status": "Closed", "priority": "Major", "reporter": "Frédéric Bertin", "assignee": null, "labels": [], "created": "2006-08-04T15:05:57.000+0000", "updated": "2009-07-08T16:51:52.000+0000", "description": "I'm using a MapReduce job to process some data logged and timestamped into files.\nWhen the job runs, it does not process the whole data, but filters only the data that has been logged since the last job run.\n\nHowever, when no new data has been logged, the job fails because the getSplits method of InputFormat returns no split. Thus the number of map tasks is 0. This is not intercepted, and the job fails at reduce step because it seems it does not find any data to process:\n\njava.io.FileNotFoundException: /local/home/hadoop/var/mapred/local/task_0030_r_000000_3/all.2 at org.apache.hadoop.fs.LocalFileSystem.openRaw(LocalFileSystem.java:121) at org.apache.hadoop.fs.FSDataInputStream$Checker.(FSDataInputStream.java:47) at org.apache.hadoop.fs.FSDataInputStream.(FSDataInputStream.java:221) at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:150) at org.apache.hadoop.io.SequenceFile$Reader.(SequenceFile.java:259) at org.apache.hadoop.io.SequenceFile$Reader.(SequenceFile.java:253) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:241) at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1013)\n\nWhat should be Hadoop's behaviour in such a case?\n\nIMHO, the job should be considered as successful. Indeed, this is not a job failure, but just a lack of input data. WDYT?\n\n", "comments": ["I agree that this should not fail.  Can you please modify one of the mini-mr unit tests to test for this case and submit that as a patch?", "ok, here is a testcase (in a separate class). Indeed I tried to include it in TestMiniMRWithDFS class, but tests freezed at some point, don't know exactly why.", "here is a patch which fixes this issue. I don't have enough knowledge of the whole Hadoop codebase to determine if this is the best place/way to fix this problem, but it works.", "sorry, the patch was badly generated, this one seems better.", "+1 this patch looks fine.\n\nWe could move the new code and the code that corrects the number of map tasks above sorting the splits, but it isn't a big deal since sorting an empty array is pretty fast. *smile*", "I just committed this.  Thanks Frédéric!"], "derived": {"summary": "I'm using a MapReduce job to process some data logged and timestamped into files. When the job runs, it does not process the whole data, but filters only the data that has been logged since the last job run.", "classification": "bug", "qna": [{"q": "What is the issue described?", "a": "mapreduce jobs fail when no split is returned via inputFormat.getSplits - I'm using a MapReduce job to process some data logged and timestamped into files. When the job runs, it does not process the whole data, but filters only the data that has been logged since the last job run."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Thanks Frédéric!"}]}}
{"project": "HADOOP", "issue_id": "HADOOP-425", "title": "a python word count example that runs under jython", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "labels": [], "created": "2006-08-04T16:20:24.000+0000", "updated": "2009-07-08T16:51:52.000+0000", "description": "I translated the word count example into python and convert it into a jar using jython. It provides a nice example of writing Hadoop map/reduce programs in python.", "comments": ["+1 on this !!!!!", "I just committed this.  Since it poses no dangers, I'm including it in 0.5.\n\nThanks, Owen!"], "derived": {"summary": "I translated the word count example into python and convert it into a jar using jython. It provides a nice example of writing Hadoop map/reduce programs in python.", "classification": "other", "qna": [{"q": "What is the issue described?", "a": "a python word count example that runs under jython - I translated the word count example into python and convert it into a jar using jython. It provides a nice example of writing Hadoop map/reduce programs in python."}, {"q": "What updates or decisions were made in the discussion?", "a": "I just committed this.  Since it poses no dangers, I'm including it in 0.5.\n\nThanks, Owen!"}]}}
